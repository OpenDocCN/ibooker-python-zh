<html><head></head><body><section data-pdf-bookmark="Chapter 6. Writing Web Crawlers" data-type="chapter" epub:type="chapter"><div class="chapter" id="c-6">&#13;
<h1><span class="label">Chapter 6. </span>Writing Web Crawlers</h1>&#13;
&#13;
<p>So far, you’ve seen single static pages with somewhat artificial canned examples. In this chapter, you’ll start looking at real-world problems, with scrapers traversing multiple pages and even multiple sites.</p>&#13;
&#13;
<p><em>Web crawlers</em> are called such <a contenteditable="false" data-primary="web crawlers" data-type="indexterm" id="id454"/>because they crawl across the web. At their core is an element of recursion. They must retrieve page contents for a URL, examine that page for other URLs, and retrieve <em>those</em> pages, ad infinitum.</p>&#13;
&#13;
<p>Beware, however: just because you can crawl the web doesn’t mean that you always should. The scrapers used in previous examples work great in situations where all the data you need is on a single page. With web crawlers, you must be extremely conscientious of how much bandwidth you are using and make every effort to determine whether there’s a way to make the target server’s load easier.</p>&#13;
&#13;
<section data-pdf-bookmark="Traversing a Single Domain" data-type="sect1"><div class="sect1" id="id41">&#13;
<h1>Traversing a Single Domain</h1>&#13;
&#13;
<p>Even if you haven’t heard of Six Degrees of Wikipedia, you <a contenteditable="false" data-primary="Six Degrees of Wikipedia" data-type="indexterm" id="id455"/><a contenteditable="false" data-primary="web crawlers" data-secondary="single domain" data-type="indexterm" id="id456"/><a contenteditable="false" data-primary="Six Degrees of Kevin Bacon" data-type="indexterm" id="id457"/>may have heard of its namesake, Six Degrees of Kevin Bacon.<sup><a data-type="noteref" href="ch06.html#id458" id="id458-marker">1</a></sup> In both games, the goal is to link two unlikely subjects (in the first case, Wikipedia articles that link to each other, and in the second case, actors appearing in the same film) by a chain containing no more than six total (including the two original subjects).</p>&#13;
&#13;
<p>For example, Eric Idle appeared in <em>Dudley Do-Right</em> with Brendan Fraser, who appeared in <em>The Air I Breathe</em> with Kevin Bacon.<sup><a data-type="noteref" href="ch06.html#id459" id="id459-marker">2</a></sup> In this case, the chain from Eric Idle to Kevin Bacon is only three subjects long.</p>&#13;
&#13;
<p>In this section, you’ll begin a project that will become a Six Degrees of Wikipedia solution finder: you’ll be able to take <a href="https://en.wikipedia.org/wiki/Eric_Idle">the Eric Idle page</a> and find the fewest number of link clicks that will take you to <a href="https://en.wikipedia.org/wiki/Kevin_Bacon">the Kevin Bacon page</a>.</p>&#13;
&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id460">&#13;
<h1>But What About Wikipedia’s Server Load?</h1>&#13;
&#13;
<p>According to the Wikimedia Foundation (the parent organization behind Wikipedia), the site’s web <a contenteditable="false" data-primary="Wikipedia" data-secondary="server load" data-type="indexterm" id="wkpdsvl"/><a contenteditable="false" data-primary="Six Degrees of Wikipedia" data-secondary="Wikipedia server load" data-type="indexterm" id="sxdwkk"/>properties receive approximately 2,500 hits per <em>second</em>, with more than 99% of them to the Wikipedia domain (see <a href="https://meta.wikimedia.org/wiki/Wikimedia_in_figures_-_Wikipedia#Traffic_volume">the “Traffic Volume” section of the “Wikimedia in Figures” page</a>). Because of the sheer volume of traffic, your web scrapers are unlikely to have any noticeable impact on Wikipedia’s server load. However, if you run the code samples in this book extensively or create your own projects that scrape Wikipedia, I encourage you to make <a href="https://wikimediafoundation.org/wiki/Ways_to_Give">a tax-deductible donation to the Wikimedia Foundation</a>—not just to offset your server load but also to help make education resources available for everyone else.</p>&#13;
&#13;
<p>Also keep in mind that if you plan on doing a large project involving data from Wikipedia, you should check to make sure that data isn’t already available from the <a href="https://www.mediawiki.org/wiki/API:Main_page">Wikipedia API</a>. Wikipedia is <a contenteditable="false" data-primary="APIs (application programming interfaces)" data-secondary="Wikipedia" data-type="indexterm" id="id461"/><a contenteditable="false" data-primary="Wikipedia API" data-type="indexterm" id="id462"/>often used as a website to demonstrate scrapers and crawlers because it has a simple HTML structure and is relatively stable. However, its APIs often make this same data more efficiently accessible.</p>&#13;
</div></aside>&#13;
&#13;
<p>You should already know how to write a Python script that retrieves an arbitrary Wikipedia page and produces a list of links on that page:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">urllib.request</code> <code class="kn">import</code> <code class="n">urlopen</code>&#13;
<code class="kn">from</code> <code class="nn">bs4</code> <code class="kn">import</code> <code class="n">BeautifulSoup</code> &#13;
&#13;
<code class="n">html</code> <code class="o">=</code> <code class="n">urlopen</code><code class="p">(</code><code class="s1">'http://en.wikipedia.org/wiki/Kevin_Bacon'</code><code class="p">)</code>&#13;
<code class="n">bs</code> <code class="o">=</code> <code class="n">BeautifulSoup</code><code class="p">(</code><code class="n">html</code><code class="p">,</code> <code class="s1">'html.parser'</code><code class="p">)</code>&#13;
<code class="k">for</code> <code class="n">link</code> <code class="ow">in</code> <code class="n">bs</code><code class="o">.</code><code class="n">find_all</code><code class="p">(</code><code class="s1">'a'</code><code class="p">):</code>&#13;
    <code class="k">if</code> <code class="s1">'href'</code> <code class="ow">in</code> <code class="n">link</code><code class="o">.</code><code class="n">attrs</code><code class="p">:</code>&#13;
        <code class="nb">print</code><code class="p">(</code><code class="n">link</code><code class="o">.</code><code class="n">attrs</code><code class="p">[</code><code class="s1">'href'</code><code class="p">])</code>&#13;
</pre>&#13;
&#13;
<p>If you look at the list of links produced, you’ll notice that all the articles you’d expect are there: <em>Apollo 13</em>, <em>Philadelphia</em>, <em>Primetime Emmy Award</em>, and other films that Kevin Bacon appeared in. However, there are some things that you may not want as well:</p>&#13;
&#13;
<pre data-code-language="text" data-type="programlisting">&#13;
//foundation.wikimedia.org/wiki/Privacy_policy&#13;
//en.wikipedia.org/wiki/Wikipedia:Contact_us</pre>&#13;
&#13;
<p>In fact, Wikipedia is full of sidebar, footer, and header links that appear on every page, along with links to the category pages, talk pages, and other pages that do not contain different articles:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
/wiki/Category:All_articles_with_unsourced_statements&#13;
/wiki/Talk:Kevin_Bacon</pre>&#13;
&#13;
<p>Recently, a friend of mine, while working on a similar Wikipedia-scraping project, mentioned he had written a large filtering function, with more than 100 lines of code, to determine whether an internal Wikipedia link was an article page. Unfortunately, he had not spent much time up-front trying to find patterns between “article links” and “other links,” or he might have discovered the trick. If you examine the links that point to article pages (as opposed to other internal pages), you’ll see that they all have three things in common:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>They reside within the <code>div</code> with the <code>id</code> set to <code>bodyContent</code>.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>The URLs do not contain colons.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>The URLs begin with <em>/wiki/</em>.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>You can use these rules to revise the code slightly to retrieve only the desired article links by using the regular expression <code>^(/wiki/)((?!:).)*$</code>:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">urllib.request</code> <code class="kn">import</code> <code class="n">urlopen</code> &#13;
<code class="kn">from</code> <code class="nn">bs4</code> <code class="kn">import</code> <code class="n">BeautifulSoup</code> &#13;
<code class="kn">import</code> <code class="nn">re</code>&#13;
&#13;
<code class="n">html</code> <code class="o">=</code> <code class="n">urlopen</code><code class="p">(</code><code class="s1">'http://en.wikipedia.org/wiki/Kevin_Bacon'</code><code class="p">)</code>&#13;
<code class="n">bs</code> <code class="o">=</code> <code class="n">BeautifulSoup</code><code class="p">(</code><code class="n">html</code><code class="p">,</code> <code class="s1">'html.parser'</code><code class="p">)</code>&#13;
<code class="k">for</code> <code class="n">link</code> <code class="ow">in</code> <code class="n">bs</code><code class="o">.</code><code class="n">find</code><code class="p">(</code><code class="s1">'div'</code><code class="p">,</code> <code class="p">{</code><code class="s1">'id'</code><code class="p">:</code><code class="s1">'bodyContent'</code><code class="p">})</code><code class="o">.</code><code class="n">find_all</code><code class="p">(</code>&#13;
    <code class="s1">'a'</code><code class="p">,</code> <code class="n">href</code><code class="o">=</code><code class="n">re</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="s1">'^(/wiki/)((?!:).)*$'</code><code class="p">)):</code>&#13;
    <code class="nb">print</code><code class="p">(</code><code class="n">link</code><code class="o">.</code><code class="n">attrs</code><code class="p">[</code><code class="s1">'href'</code><code class="p">])</code>&#13;
</pre>&#13;
&#13;
<p>Running this, you should see a list of all article URLs that the Wikipedia article on Kevin Bacon links to.</p>&#13;
&#13;
<p class="pagebreak-before">Of course, having a script that finds all article links in one, hardcoded Wikipedia article, while interesting, is fairly useless in practice. You need to be able to take this code and transform it into something more like the following:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>A single function, <code>getLinks</code>, that takes in a Wikipedia article URL of the form <span class="keep-together"><code>/wiki/&lt;Article_Name&gt;</code></span> and returns a list of all linked article URLs in the same form.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>A main function that calls <code>getLinks</code> with a starting article, chooses a random article link from the returned list, and calls <code>getLinks</code> again, until you stop the program or until no article links are found on the new page.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>Here is the complete code that accomplishes this:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">urllib.request</code> <code class="kn">import</code> <code class="n">urlopen</code>&#13;
<code class="kn">from</code> <code class="nn">bs4</code> <code class="kn">import</code> <code class="n">BeautifulSoup</code>&#13;
<code class="kn">import</code> <code class="nn">datetime</code>&#13;
<code class="kn">import</code> <code class="nn">random</code>&#13;
<code class="kn">import</code> <code class="nn">re</code>&#13;
&#13;
<code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="n">datetime</code><code class="o">.</code><code class="n">datetime</code><code class="o">.</code><code class="n">now</code><code class="p">())</code>&#13;
<code class="k">def</code> <code class="nf">getLinks</code><code class="p">(</code><code class="n">articleUrl</code><code class="p">):</code>&#13;
    <code class="n">html</code> <code class="o">=</code> <code class="n">urlopen</code><code class="p">(</code><code class="s1">'http://en.wikipedia.org</code><code class="si">{}</code><code class="s1">'</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">articleUrl</code><code class="p">))</code>&#13;
    <code class="n">bs</code> <code class="o">=</code> <code class="n">BeautifulSoup</code><code class="p">(</code><code class="n">html</code><code class="p">,</code> <code class="s1">'html.parser'</code><code class="p">)</code>&#13;
    <code class="k">return</code> <code class="n">bs</code><code class="o">.</code><code class="n">find</code><code class="p">(</code><code class="s1">'div'</code><code class="p">,</code> <code class="p">{</code><code class="s1">'id'</code><code class="p">:</code><code class="s1">'bodyContent'</code><code class="p">})</code><code class="o">.</code><code class="n">find_all</code><code class="p">(</code><code class="s1">'a'</code><code class="p">,</code>&#13;
         <code class="n">href</code><code class="o">=</code><code class="n">re</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="s1">'^(/wiki/)((?!:).)*$'</code><code class="p">))</code>&#13;
&#13;
<code class="n">links</code> <code class="o">=</code> <code class="n">getLinks</code><code class="p">(</code><code class="s1">'/wiki/Kevin_Bacon'</code><code class="p">)</code>&#13;
<code class="k">while</code> <code class="nb">len</code><code class="p">(</code><code class="n">links</code><code class="p">)</code> <code class="o">&gt;</code> <code class="mi">0</code><code class="p">:</code>&#13;
    <code class="n">newArticle</code> <code class="o">=</code> <code class="n">links</code><code class="p">[</code><code class="n">random</code><code class="o">.</code><code class="n">randint</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="nb">len</code><code class="p">(</code><code class="n">links</code><code class="p">)</code><code class="o">-</code><code class="mi">1</code><code class="p">)]</code><code class="o">.</code><code class="n">attrs</code><code class="p">[</code><code class="s1">'href'</code><code class="p">]</code>&#13;
    <code class="nb">print</code><code class="p">(</code><code class="n">newArticle</code><code class="p">)</code>&#13;
    <code class="n">links</code> <code class="o">=</code> <code class="n">getLinks</code><code class="p">(</code><code class="n">newArticle</code><code class="p">)</code>&#13;
</pre>&#13;
&#13;
<p>The first thing the program does, after importing the needed libraries, is set the random-number generator <a contenteditable="false" data-primary="random-number generator" data-type="indexterm" id="id463"/>seed with the current system time. This practically ensures a new and interesting random path <a contenteditable="false" data-primary="Wikipedia" data-secondary="server load" data-startref="wkpdsvl" data-type="indexterm" id="id464"/><a contenteditable="false" data-primary="Six Degrees of Wikipedia" data-secondary="Wikipedia server load" data-startref="sxdwkk" data-type="indexterm" id="id465"/>through Wikipedia articles every time the program is run.</p>&#13;
&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id466">&#13;
<h1>Pseudorandom Numbers and Random Seeds</h1>&#13;
&#13;
<p>The previous example used Python’s random-number <a contenteditable="false" data-primary="pseudorandom numbers" data-type="indexterm" id="psdrndm"/><a contenteditable="false" data-primary="random-number generator" data-secondary="seeds" data-type="indexterm" id="rdmmg"/>generator to select an article at random on each page in order to continue a random traversal of Wikipedia. However, random numbers should be used with caution.</p>&#13;
&#13;
<p>Although computers are great at calculating correct answers, they’re terrible at making things up. For this reason, random numbers can be a challenge. Most random-number algorithms strive to produce an evenly distributed and hard-to-predict sequence of numbers, but a “seed” number is needed to give these algorithms something to work with initially. The exact same seed will produce the exact same sequence of “random” numbers every time, so for this reason I’ve used the system clock as a starter for producing new sequences of random numbers, and thus, new sequences of random articles. This makes the program a little more exciting to run.</p>&#13;
&#13;
<p>For the curious, the Python pseudorandom-number <a contenteditable="false" data-primary="Mersenne Twister algorithm" data-type="indexterm" id="id467"/>generator is powered by the <em>Mersenne Twister algorithm</em>. While it produces random numbers that are difficult to predict and uniformly distributed, it is slightly processor intensive. Random numbers this good don’t come cheap!</p>&#13;
</div></aside>&#13;
&#13;
<p>Next, the program defines the <code>getLinks</code> function, which takes in an article URL of the form <span class="keep-together"><code>/wiki/...</code></span>, prepends the Wikipedia domain name, <code>http://en.wikipedia.org</code>, and retrieves the <code>BeautifulSoup</code> object for the HTML at that domain. It then extracts a list of article link tags, based on the parameters discussed previously, and returns them.</p>&#13;
&#13;
<p>The main body of the program begins with setting a list of article link tags (the <code>links</code> variable) to the list of links in the initial page: <em>https://en.wikipedia.org/wiki/Kevin_Bacon</em>. It then goes into a loop, finding a random article link tag in the page, extracting the <code>href</code> attribute from it, printing the page, and getting a new list of links from the extracted URL.</p>&#13;
&#13;
<p>Of course, there’s a bit more to solving a Six Degrees of Wikipedia problem than building a scraper that goes from page to page. You <a contenteditable="false" data-primary="pseudorandom numbers" data-startref="psdrndm" data-type="indexterm" id="id468"/><a contenteditable="false" data-primary="random-number generator" data-secondary="seeds" data-startref="rdmmg" data-type="indexterm" id="id469"/>also must be able to store and analyze the resulting data. For a continuation of the solution to this problem, see <a data-type="xref" href="ch09.html#c-9">Chapter 9</a>.</p>&#13;
&#13;
<div data-type="warning" epub:type="warning">&#13;
<h1>Handle Your Exceptions!</h1>&#13;
&#13;
<p>Although these code examples omit most <a contenteditable="false" data-primary="Six Degrees of Wikipedia" data-secondary="exception handling" data-type="indexterm" id="id470"/><a contenteditable="false" data-primary="exception handling" data-type="indexterm" id="id471"/>exception handling for the sake of brevity, be aware that many potential pitfalls could arise. What if Wikipedia changes the name of the <code>bodyContent</code> tag, for example? When the program attempts to extract the text from the tag, it throws an <code>AttributeError</code>.</p>&#13;
&#13;
<p>So although these scripts might be fine to run as closely watched examples, autonomous production code requires far more exception handling than can fit into this book. Look back to <a data-type="xref" href="ch04.html#c-4">Chapter 4</a> for more information about this.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Crawling an Entire Site" data-type="sect1"><div class="sect1" id="id42">&#13;
<h1>Crawling an Entire Site</h1>&#13;
&#13;
<p>In the previous section, you took a <a contenteditable="false" data-primary="web crawlers" data-secondary="entire site" data-type="indexterm" id="wbwts"/>random walk through a website, going from link to link. But what if you need to systematically catalog or search every page on a site? Crawling an entire site, especially a large one, is a memory-intensive process that is best suited to applications for which a database to store crawling results is readily available. However, you can explore the behavior of these types of applications without running them full-scale. To learn more about running these applications by using a database, see <a data-type="xref" href="ch09.html#c-9">Chapter 9</a>.</p>&#13;
&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id472">&#13;
<h1>The Dark and Deep Webs</h1>&#13;
&#13;
<p>You’ve likely heard the terms <em>deep web</em>, <em>dark web</em>, or <em>hidden web</em> being thrown <a contenteditable="false" data-primary="dark web" data-type="indexterm" id="dkwb"/><a contenteditable="false" data-primary="deep web" data-type="indexterm" id="dpwb"/>around a lot, especially in the media lately. What do they mean?</p>&#13;
&#13;
<p>The <em>deep web</em> is any part of the web that’s not part of the <em>surface web.<sup><a data-type="noteref" href="ch06.html#id473" id="id473-marker">3</a></sup></em>  The surface is part of the internet that is indexed by search engines. Estimates vary widely, but the deep web almost certainly makes up about 90% of the internet. Because Google can’t do things like submit forms, find pages that haven’t been linked to by a top-level domain, or investigate sites where <em>robots.txt</em> prohibits it, the surface web stays relatively small.</p>&#13;
&#13;
<p>The <em>dark web</em>, also known as <a contenteditable="false" data-primary="darknet" data-type="indexterm" id="id474"/>the <em>darknet</em>, is another beast entirely.<sup><a data-type="noteref" href="ch06.html#id475" id="id475-marker">4</a></sup> It is run over the existing network hardware infrastructure but uses Tor, or another client, with an application <a contenteditable="false" data-primary="Tor" data-secondary="dark web" data-type="indexterm" id="id476"/>protocol that runs on top of HTTP, providing a secure channel to exchange information. Although it is possible to scrape the dark web, just as you’d scrape any other website, doing so is outside the scope of this book.</p>&#13;
&#13;
<p>Unlike the dark web, the deep web is relatively easy to scrape. Many tools in this book will teach you how to crawl and scrape information from many places that Googlebots can’t go.</p>&#13;
</div></aside>&#13;
&#13;
<p>When might crawling an entire website be useful, and when might it be harmful? Web scrapers that traverse an entire site are good for many things, including:</p>&#13;
&#13;
<dl>&#13;
	<dt><em>Generating a site map</em></dt>&#13;
	<dd>A few years ago, I was faced with a <a contenteditable="false" data-primary="site map generation" data-type="indexterm" id="id477"/>problem: an important client wanted an estimate for a website redesign but did not want to provide my company with access to the internals of their current content management system and did not have a publicly available site map. I was able to use a crawler to cover the entire site, gather all internal links, and organize the pages into the actual folder structure used on the site. This allowed me to quickly find sections of the site I wasn’t even aware existed and accurately count how many page designs would be required and how much content would need to be migrated. </dd>&#13;
	<dt><em>Gathering data</em></dt>&#13;
	<dd>Another client wanted to gather articles (stories, blog posts, news articles, etc.) in order to create a working prototype of a specialized search platform. Although these website crawls didn’t need to be exhaustive, they did need to be fairly expansive (we were interested in getting data from only a few sites). I was able to create crawlers that recursively traversed each site and collected only data found on article pages.</dd>&#13;
</dl>&#13;
&#13;
<p>The general approach to an exhaustive site crawl is to start with a top-level page (such as the home page) and search for a list of all internal links on that page. Every one of those links is then crawled, and additional lists of links are found on each one of them, triggering another round of crawling.</p>&#13;
&#13;
<p>Clearly, this is a situation that can blow up quickly. If every page has 10 internal links, and a website is 5 pages deep (a fairly typical depth for a medium-size website), then the number of pages you need to crawl is 10<sup>5</sup>, or 100,000 pages, before you can be sure that you’ve exhaustively covered the website. Strangely enough, although “5 pages deep and 10 internal links per page” are fairly typical dimensions for a website, very few websites have 100,000 or more pages. The reason, of course, is that the vast majority of internal links are duplicates.</p>&#13;
&#13;
<p>To avoid crawling the same page twice, it is extremely important that all internal links discovered are formatted consistently and kept in a running set for easy lookups, while the program is running. A <em>set</em> is similar to a list, but elements do not have a specific order, and only unique elements are stored, which is ideal for our needs. Only links that are “new” should be crawled and searched for additional links:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">urllib.request</code> <code class="kn">import</code> <code class="n">urlopen</code>&#13;
<code class="kn">from</code> <code class="nn">bs4</code> <code class="kn">import</code> <code class="n">BeautifulSoup</code>&#13;
<code class="kn">import</code> <code class="nn">re</code>&#13;
&#13;
<code class="n">pages</code> <code class="o">=</code> <code class="nb">set</code><code class="p">()</code>&#13;
<code class="k">def</code> <code class="nf">getLinks</code><code class="p">(</code><code class="n">pageUrl</code><code class="p">):</code>&#13;
    <code class="n">html</code> <code class="o">=</code> <code class="n">urlopen</code><code class="p">(</code><code class="s1">'http://en.wikipedia.org</code><code class="si">{}</code><code class="s1">'</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">pageUrl</code><code class="p">))</code>&#13;
    <code class="n">bs</code> <code class="o">=</code> <code class="n">BeautifulSoup</code><code class="p">(</code><code class="n">html</code><code class="p">,</code> <code class="s1">'html.parser'</code><code class="p">)</code>&#13;
    <code class="k">for</code> <code class="n">link</code> <code class="ow">in</code> <code class="n">bs</code><code class="o">.</code><code class="n">find_all</code><code class="p">(</code><code class="s1">'a'</code><code class="p">,</code> <code class="n">href</code><code class="o">=</code><code class="n">re</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="s1">'^(/wiki/)'</code><code class="p">)):</code>&#13;
        <code class="k">if</code> <code class="s1">'href'</code> <code class="ow">in</code> <code class="n">link</code><code class="o">.</code><code class="n">attrs</code><code class="p">:</code>&#13;
            <code class="k">if</code> <code class="n">link</code><code class="o">.</code><code class="n">attrs</code><code class="p">[</code><code class="s1">'href'</code><code class="p">]</code> <code class="ow">not</code> <code class="ow">in</code> <code class="n">pages</code><code class="p">:</code>&#13;
                <code class="c1">#We have encountered a new page</code>&#13;
                <code class="n">newPage</code> <code class="o">=</code> <code class="n">link</code><code class="o">.</code><code class="n">attrs</code><code class="p">[</code><code class="s1">'href'</code><code class="p">]</code>&#13;
                <code class="nb">print</code><code class="p">(</code><code class="n">newPage</code><code class="p">)</code>&#13;
                <code class="n">pages</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">newPage</code><code class="p">)</code>&#13;
                <code class="n">getLinks</code><code class="p">(</code><code class="n">newPage</code><code class="p">)</code>&#13;
<code class="n">getLinks</code><code class="p">(</code><code class="s1">''</code><code class="p">)</code>&#13;
</pre>&#13;
&#13;
<p>To show you the full effect of how this web crawling business works, I’ve relaxed the standards of what constitutes an internal link (from previous examples). Rather than limit the scraper to article pages, it looks for all links that begin with <em>/wiki/</em>, regardless of where they are on the page, and regardless of whether they contain colons. Remember: article pages do not contain colons, but file-upload pages, talk pages, and the like do contain colons in the URL.</p>&#13;
&#13;
<p>Initially, <code>getLinks</code> is called with an empty URL. This is translated as “the front page of Wikipedia” as soon as the empty URL is prepended with <code>http://en.wikipedia.org</code> inside the function. Then, each link on the first page is iterated through and a check is made to see whether it is in the set of pages that the script has encountered already. If not, it is added to the list, printed to the screen, and the <code>getLinks</code> function is called recursively <a contenteditable="false" data-primary="web crawlers" data-secondary="entire site" data-startref="wbwts" data-type="indexterm" id="id478"/><a contenteditable="false" data-primary="dark web" data-startref="dkwb" data-type="indexterm" id="id479"/><a contenteditable="false" data-primary="deep web" data-startref="dpwb" data-type="indexterm" id="id480"/>on it.</p>&#13;
&#13;
<div data-type="warning" epub:type="warning">&#13;
<h1>A Warning About Recursion</h1>&#13;
&#13;
<p>This is a warning rarely seen in <a contenteditable="false" data-primary="recursion" data-type="indexterm" id="id481"/>software books, but I thought you should be aware: if left running long enough, the preceding program will almost certainly crash.</p>&#13;
&#13;
<p>Python has a <a contenteditable="false" data-primary="Python" data-secondary="recursion limit" data-type="indexterm" id="id482"/>default recursion limit (the number of times a program can recursively call itself) of 1,000. Because Wikipedia’s network of links is extremely large, this program will eventually hit that recursion limit and stop, unless you put in a recursion counter or something to prevent that from happening.</p>&#13;
&#13;
<p>For “flat” sites that are fewer than 1,000 links deep, this method usually works well, with a few unusual exceptions. For instance, I once encountered a bug in a dynamically generated URL that depended on the address of the current page to write the link on that page. This resulted in infinitely repeating paths like <em>/blogs/blogs.../blogs/blog-post.php</em>.</p>&#13;
&#13;
<p>For the most part, however, this recursive technique should be fine for any typical website you’re likely to encounter.</p>&#13;
</div>&#13;
&#13;
<section data-pdf-bookmark="Collecting Data Across an Entire Site" data-type="sect2"><div class="sect2" id="id43">&#13;
<h2>Collecting Data Across an Entire Site</h2>&#13;
&#13;
<p>Web crawlers would be fairly boring if all they <a contenteditable="false" data-primary="data collection, web crawlers" data-type="indexterm" id="dclww"/>did was hop from one page to the other. To make them useful, you need to be able to do something on the page while you’re there. Let’s look at how to build a scraper that collects the title, the first paragraph of content, and the link to edit the page (if available).</p>&#13;
&#13;
<p>As always, the first step to determine how best to do this is to look at a few pages from the site and determine a pattern. By looking at a handful of Wikipedia pages (both articles and nonarticle pages such as the privacy policy page), the following things should be clear:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>All titles (on all pages, regardless of their status as an article page, an edit history page, or any other page) have titles under <code>h1</code> → <code>span</code> tags, and these are the only <code>h1</code> tags on the page.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>As mentioned before, all body text lives under the <code>div#bodyContent</code> tag. However, if you want to get more specific and access just the first paragraph of text, you might be better off using <code>div#mw-content-text</code> → <code>​p</code> (selecting the first paragraph tag only). This is true for all content pages except file pages (for example, <a href="https://en.wikipedia.org/wiki/File:Orbit_of_274301_Wikipedia.svg"><em>https://en.wikipedia.org/wiki/File:Orbit_of_274301_Wikipedia.svg</em></a>), which do not have sections of content text.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Edit links occur only on article pages. If they occur, they will be found in the <code>li#ca-edit</code> tag, under <code>li#ca-edit</code> →​ <code>span</code> →​ <code>a</code>.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>By modifying your basic crawling code, you can create a combination crawler/data-gathering (or at least, data-printing) program:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">urllib.request</code> <code class="kn">import</code> <code class="n">urlopen</code>&#13;
<code class="kn">from</code> <code class="nn">bs4</code> <code class="kn">import</code> <code class="n">BeautifulSoup</code>&#13;
<code class="kn">import</code> <code class="nn">re</code>&#13;
&#13;
<code class="n">pages</code> <code class="o">=</code> <code class="nb">set</code><code class="p">()</code>&#13;
<code class="k">def</code> <code class="nf">getLinks</code><code class="p">(</code><code class="n">pageUrl</code><code class="p">):</code>&#13;
    <code class="n">html</code> <code class="o">=</code> <code class="n">urlopen</code><code class="p">(</code><code class="s1">'http://en.wikipedia.org</code><code class="si">{}</code><code class="s1">'</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">pageUrl</code><code class="p">))</code>&#13;
    <code class="n">bs</code> <code class="o">=</code> <code class="n">BeautifulSoup</code><code class="p">(</code><code class="n">html</code><code class="p">,</code> <code class="s1">'html.parser'</code><code class="p">)</code>&#13;
    <code class="k">try</code><code class="p">:</code>&#13;
        <code class="nb">print</code><code class="p">(</code><code class="n">bs</code><code class="o">.</code><code class="n">h1</code><code class="o">.</code><code class="n">get_text</code><code class="p">())</code>&#13;
        <code class="nb">print</code><code class="p">(</code><code class="n">bs</code><code class="o">.</code><code class="n">find</code><code class="p">(</code><code class="nb">id</code> <code class="o">=</code><code class="s1">'mw-content-text'</code><code class="p">)</code><code class="o">.</code><code class="n">find_all</code><code class="p">(</code><code class="s1">'p'</code><code class="p">)[</code><code class="mi">0</code><code class="p">])</code>&#13;
        <code class="nb">print</code><code class="p">(</code><code class="n">bs</code><code class="o">.</code><code class="n">find</code><code class="p">(</code><code class="nb">id</code><code class="o">=</code><code class="s1">'ca-edit'</code><code class="p">)</code><code class="o">.</code><code class="n">find</code><code class="p">(</code><code class="s1">'span'</code><code class="p">)</code>&#13;
             <code class="o">.</code><code class="n">find</code><code class="p">(</code><code class="s1">'a'</code><code class="p">)</code><code class="o">.</code><code class="n">attrs</code><code class="p">[</code><code class="s1">'href'</code><code class="p">])</code>&#13;
    <code class="k">except</code> <code class="ne">AttributeError</code><code class="p">:</code>&#13;
        <code class="nb">print</code><code class="p">(</code><code class="s1">'This page is missing something! Continuing.'</code><code class="p">)</code>&#13;
    &#13;
    <code class="k">for</code> <code class="n">link</code> <code class="ow">in</code> <code class="n">bs</code><code class="o">.</code><code class="n">find_all</code><code class="p">(</code><code class="s1">'a'</code><code class="p">,</code> <code class="n">href</code><code class="o">=</code><code class="n">re</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="s1">'^(/wiki/)'</code><code class="p">)):</code>&#13;
        <code class="k">if</code> <code class="s1">'href'</code> <code class="ow">in</code> <code class="n">link</code><code class="o">.</code><code class="n">attrs</code><code class="p">:</code>&#13;
            <code class="k">if</code> <code class="n">link</code><code class="o">.</code><code class="n">attrs</code><code class="p">[</code><code class="s1">'href'</code><code class="p">]</code> <code class="ow">not</code> <code class="ow">in</code> <code class="n">pages</code><code class="p">:</code>&#13;
                <code class="c1">#We have encountered a new page</code>&#13;
                <code class="n">newPage</code> <code class="o">=</code> <code class="n">link</code><code class="o">.</code><code class="n">attrs</code><code class="p">[</code><code class="s1">'href'</code><code class="p">]</code>&#13;
                <code class="nb">print</code><code class="p">(</code><code class="s1">'-'</code><code class="o">*</code><code class="mi">20</code><code class="p">)</code>&#13;
                <code class="nb">print</code><code class="p">(</code><code class="n">newPage</code><code class="p">)</code>&#13;
                <code class="n">pages</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">newPage</code><code class="p">)</code>&#13;
                <code class="n">getLinks</code><code class="p">(</code><code class="n">newPage</code><code class="p">)</code>&#13;
<code class="n">getLinks</code><code class="p">(</code><code class="s1">''</code><code class="p">)</code> &#13;
</pre>&#13;
&#13;
<p>The <code>for</code> loop in this program is essentially the same as it was in the original crawling program (with the addition of printed dashes for clarity, separating the printed <span class="keep-together">content)</span>.</p>&#13;
&#13;
<p class="pagebreak-before">Because you can never be entirely sure that all the data is on each page, each <code>print</code> statement is arranged in the order that it is likeliest to appear on the site. That is, the <code>h1</code> title tag appears on every page (as far as I can tell, at any rate), so you attempt to get that data first. The text content appears on most pages (except for file pages), so that is the second piece of data retrieved. The Edit button appears only on pages in which both titles and <a contenteditable="false" data-primary="data collection, web crawlers" data-startref="dclww" data-type="indexterm" id="id483"/>text content already exist, but it does not appear on all of those pages.</p>&#13;
&#13;
<div data-type="warning" epub:type="warning">&#13;
<h1>Different Patterns for Different Needs</h1>&#13;
&#13;
<p>Obviously, some dangers are involved <a contenteditable="false" data-primary="web crawlers" data-secondary="patterns" data-type="indexterm" id="id484"/><a contenteditable="false" data-primary="patterns, web crawlers" data-type="indexterm" id="id485"/>with wrapping multiple lines in an exception handler. You cannot tell which line threw the exception, for one thing. Also, if for some reason a page contains an Edit button but no title, the Edit button would never be logged. However, it suffices for many instances in which there is an order of likeliness of items appearing on the site, and inadvertently missing a few data points or keeping detailed logs is not a problem.</p>&#13;
</div>&#13;
&#13;
<p>You might notice that in this and all the previous examples, you haven’t been “collecting” data so much as “printing” it. Obviously, data in your terminal is hard to manipulate. For more on storing information and creating databases, see <a data-type="xref" href="ch09.html#c-9">Chapter 9</a>.</p>&#13;
&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id486">&#13;
<h1>Handling Redirects</h1>&#13;
&#13;
<p>Redirects allow a web server to point <a contenteditable="false" data-primary="web servers, redirects" data-type="indexterm" id="id487"/><a contenteditable="false" data-primary="redirects" data-type="indexterm" id="id488"/>one domain name or URL to a piece of content at a different location. There are two types of redirects:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>Server-side redirects, where the URL is changed <a contenteditable="false" data-primary="server-side redirects" data-type="indexterm" id="id489"/>before the page is loaded</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Client-side redirects, sometimes seen <a contenteditable="false" data-primary="client-side redirects" data-type="indexterm" id="id490"/>with a “You will be redirected in 10 seconds” type of message, where the page loads before redirecting to the new one</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>With server-side redirects, you usually don’t have to worry. If you’re using the urllib library with Python 3.x, it handles redirects automatically! If you’re using the Requests library, make sure to set the <code>allow_redirects</code> flag to <code>True</code>:</p>&#13;
&#13;
<pre>&#13;
<span class="n">r</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>'http://github.com'<span class="p">,</span> <span class="n">allow_redirects</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></pre>&#13;
&#13;
<p>Just be aware that, occasionally, the URL of the page you’re crawling might not be exactly the URL that you entered the page on.</p>&#13;
&#13;
<p>For more information on client-side redirects, which are performed using JavaScript or HTML, see <a class="xref" data-type="xref" href="ch14.html#c-14">Chapter 14</a>.</p>&#13;
</div></aside>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Crawling Across the Internet" data-type="sect1"><div class="sect1" id="id44">&#13;
<h1>Crawling Across the Internet</h1>&#13;
&#13;
<p>Whenever I give a talk on web scraping, someone inevitably asks, “How do you build Google?” My answer is always twofold: “First, you get many billions of dollars so that you can buy the world’s largest data warehouses and place them in hidden locations all around the world. Second, you build a web crawler.”</p>&#13;
&#13;
<p>When Google started in 1996, it was <a contenteditable="false" data-primary="web crawlers" data-secondary="Google and" data-type="indexterm" id="id491"/><a contenteditable="false" data-primary="Google, history of" data-type="indexterm" id="id492"/>just two Stanford graduate students with an old server and a Python web crawler. Now that you know how to scrape the web, you’re just some VC funding away from becoming the next tech multibillionaire!</p>&#13;
&#13;
<p>In all seriousness, web crawlers are at the heart of what drives many modern web technologies, and you don’t necessarily need a large data warehouse to use them. To do any cross-domain data analysis, you do need to build crawlers that can interpret and store data across the myriad of pages on the internet.</p>&#13;
&#13;
<p>Just as in the previous example, the web crawlers you are going to build will follow links from page to page, building out a map of the web. But this time, they will not ignore external links; they will follow them.</p>&#13;
&#13;
<div data-type="warning" epub:type="warning">&#13;
<h1>Unknown Waters Ahead</h1>&#13;
&#13;
<p>Keep in mind that the code from <a contenteditable="false" data-primary="web crawlers" data-secondary="undesirable results" data-type="indexterm" id="id493"/>the next section can go <em>anywhere</em> on the internet. If we’ve learned anything from Six Degrees of Wikipedia, it’s that it’s entirely possible to go from a site such as <a href="http://www.sesamestreet.org/"><em>http://www.sesamestreet.org/</em></a> to something less savory in just a few hops.</p>&#13;
&#13;
<p>Kids, ask your parents before running this code. For those with sensitive constitutions or with religious restrictions that might prohibit seeing text from a prurient site, follow along by reading the code examples but be careful when running them.</p>&#13;
</div>&#13;
&#13;
<p>Before you start writing a crawler that follows all outbound links willy-nilly, you should ask yourself a few questions:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>What data am I trying to gather? Can this be accomplished by scraping just a few predefined websites (almost always the easier option), or does my crawler need to be able to discover new websites I might not know about?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>When my crawler reaches a particular website, will it immediately follow the next outbound link to a new website, or will it stick around for a while and drill down into the current website?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Are there any conditions under which I would not want to scrape a particular site? Am I interested in non-English content?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>How am I protecting myself against legal action if <a contenteditable="false" data-primary="web crawlers" data-secondary="legal protections" data-type="indexterm" id="id494"/>my web crawler catches the attention of a webmaster on one of the sites it runs across? (Check out <a data-type="xref" href="ch02.html#c-2">Chapter 2</a> for more information on this subject.)</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>A flexible set of Python functions that can be combined to perform a variety of types of web scraping can be easily written in fewer than 60 lines of code. Here, I’ve omitted the library imports for brevity and broken the code up into multiple sections for the purposes of discussion. However, a full working version can be found in the <a href="https://github.com/REMitchell/python-scraping">GitHub repository</a> for this book:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="c1">#Retrieves a list of all Internal links found on a page</code>&#13;
<code class="k">def</code> <code class="nf">getInternalLinks</code><code class="p">(</code><code class="n">bs</code><code class="p">,</code> <code class="n">url</code><code class="p">):</code>&#13;
    <code class="n">netloc</code> <code class="o">=</code> <code class="n">urlparse</code><code class="p">(</code><code class="n">url</code><code class="p">)</code><code class="o">.</code><code class="n">netloc</code>&#13;
    <code class="n">scheme</code> <code class="o">=</code> <code class="n">urlparse</code><code class="p">(</code><code class="n">url</code><code class="p">)</code><code class="o">.</code><code class="n">scheme</code>&#13;
    <code class="n">internalLinks</code> <code class="o">=</code> <code class="nb">set</code><code class="p">()</code>&#13;
    <code class="k">for</code> <code class="n">link</code> <code class="ow">in</code> <code class="n">bs</code><code class="o">.</code><code class="n">find_all</code><code class="p">(</code><code class="s1">'a'</code><code class="p">):</code>&#13;
        <code class="k">if</code> <code class="ow">not</code> <code class="n">link</code><code class="o">.</code><code class="n">attrs</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s1">'href'</code><code class="p">):</code>&#13;
            <code class="k">continue</code>&#13;
        <code class="n">parsed</code> <code class="o">=</code> <code class="n">urlparse</code><code class="p">(</code><code class="n">link</code><code class="o">.</code><code class="n">attrs</code><code class="p">[</code><code class="s1">'href'</code><code class="p">])</code>&#13;
        <code class="k">if</code> <code class="n">parsed</code><code class="o">.</code><code class="n">netloc</code> <code class="o">==</code> <code class="s1">''</code><code class="p">:</code>&#13;
            <code class="n">l</code> <code class="o">=</code> <code class="sa">f</code><code class="s1">'</code><code class="si">{</code><code class="n">scheme</code><code class="si">}</code><code class="s1">://</code><code class="si">{</code><code class="n">netloc</code><code class="si">}</code><code class="s1">/</code><code class="si">{</code><code class="n">link</code><code class="o">.</code><code class="n">attrs</code><code class="p">[</code><code class="s2">"href"</code><code class="p">]</code><code class="o">.</code><code class="n">strip</code><code class="p">(</code><code class="s2">"/"</code><code class="p">)</code><code class="si">}</code><code class="s1">'</code>&#13;
            <code class="n">internalLinks</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">l</code><code class="p">)</code>&#13;
        <code class="k">elif</code> <code class="n">parsed</code><code class="o">.</code><code class="n">netloc</code> <code class="o">==</code> <code class="n">internal_netloc</code><code class="p">:</code>&#13;
            <code class="n">internalLinks</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">link</code><code class="o">.</code><code class="n">attrs</code><code class="p">[</code><code class="s1">'href'</code><code class="p">])</code>&#13;
    <code class="k">return</code> <code class="nb">list</code><code class="p">(</code><code class="n">internalLinks</code><code class="p">)</code>&#13;
</pre>&#13;
&#13;
<p>The first function is <code>getInternalLinks</code>. This takes, as arguments, a BeautifulSoup object and the URL of the page. This URL is used only to identify the <code>netloc</code> (network location) and <code>scheme</code> (usually <code>http</code> or <code>https</code>) of the internal site, so it’s important to note that any internal URL for the target site can be used here—it doesn’t need to be the exact URL of the BeautifulSoup object passed in.</p>&#13;
&#13;
<p>This function creates a set called <code>internalLinks</code> used to track all internal links found on the page. It checks all anchor tags for an <code>href</code> attribute that either doesn’t contain a <code>netloc</code> (is a relative URL like “/careers/”) or has a <code>netloc</code> that matches that of the URL passed in:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="c1">#Retrieves a list of all external links found on a page</code>&#13;
<code class="k">def</code> <code class="nf">getExternalLinks</code><code class="p">(</code><code class="n">bs</code><code class="p">,</code> <code class="n">url</code><code class="p">):</code>&#13;
    <code class="n">internal_netloc</code> <code class="o">=</code> <code class="n">urlparse</code><code class="p">(</code><code class="n">url</code><code class="p">)</code><code class="o">.</code><code class="n">netloc</code>&#13;
    <code class="n">externalLinks</code> <code class="o">=</code> <code class="nb">set</code><code class="p">()</code>&#13;
    <code class="k">for</code> <code class="n">link</code> <code class="ow">in</code> <code class="n">bs</code><code class="o">.</code><code class="n">find_all</code><code class="p">(</code><code class="s1">'a'</code><code class="p">):</code>&#13;
        <code class="k">if</code> <code class="ow">not</code> <code class="n">link</code><code class="o">.</code><code class="n">attrs</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s1">'href'</code><code class="p">):</code>&#13;
            <code class="k">continue</code>&#13;
        <code class="n">parsed</code> <code class="o">=</code> <code class="n">urlparse</code><code class="p">(</code><code class="n">link</code><code class="o">.</code><code class="n">attrs</code><code class="p">[</code><code class="s1">'href'</code><code class="p">])</code>&#13;
        <code class="k">if</code> <code class="n">parsed</code><code class="o">.</code><code class="n">netloc</code> <code class="o">!=</code> <code class="s1">''</code> <code class="ow">and</code> <code class="n">parsed</code><code class="o">.</code><code class="n">netloc</code> <code class="o">!=</code> <code class="n">internal_netloc</code><code class="p">:</code>&#13;
            <code class="n">externalLinks</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">link</code><code class="o">.</code><code class="n">attrs</code><code class="p">[</code><code class="s1">'href'</code><code class="p">])</code>&#13;
    <code class="k">return</code> <code class="nb">list</code><code class="p">(</code><code class="n">externalLinks</code><code class="p">)</code>&#13;
&#13;
</pre>&#13;
&#13;
<p>The function <code>getExternalLinks</code> works similarly to <code>getInternalLinks</code>. It examines all anchor tags with an <code>href</code> attribute and looks for those that have a populated <code>netloc</code> that does <em>not</em> match that of the URL passed in:</p>&#13;
&#13;
<pre class="pre drop-element-attached-top drop-element-attached-center drop-target-attached-bottom drop-target-attached-center" data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="k">def</code> <code class="nf">getRandomExternalLink</code><code class="p">(</code><code class="n">startingPage</code><code class="p">):</code>&#13;
    <code class="n">bs</code> <code class="o">=</code> <code class="n">BeautifulSoup</code><code class="p">(</code><code class="n">urlopen</code><code class="p">(</code><code class="n">startingPage</code><code class="p">),</code> <code class="s1">'html.parser'</code><code class="p">)</code>&#13;
    <code class="n">externalLinks</code> <code class="o">=</code> <code class="n">getExternalLinks</code><code class="p">(</code><code class="n">bs</code><code class="p">,</code> <code class="n">startingPage</code><code class="p">)</code>&#13;
    <code class="k">if</code> <code class="ow">not</code> <code class="nb">len</code><code class="p">(</code><code class="n">externalLinks</code><code class="p">):</code>&#13;
        <code class="nb">print</code><code class="p">(</code><code class="s1">'No external links, looking around the site for one'</code><code class="p">)</code>&#13;
        <code class="n">internalLinks</code> <code class="o">=</code> <code class="n">getInternalLinks</code><code class="p">(</code><code class="n">bs</code><code class="p">,</code> <code class="n">startingPage</code><code class="p">)</code>&#13;
        <code class="k">return</code> <code class="n">getRandomExternalLink</code><code class="p">(</code><code class="n">random</code><code class="o">.</code><code class="n">choice</code><code class="p">(</code><code class="n">internalLinks</code><code class="p">))</code>&#13;
    <code class="k">else</code><code class="p">:</code>&#13;
        <code class="k">return</code> <code class="n">random</code><code class="o">.</code><code class="n">choice</code><code class="p">(</code><code class="n">externalLinks</code><code class="p">)</code>&#13;
</pre>&#13;
&#13;
<p>The function <code>getRandomExternalLink</code> uses the function <code>getExternalLinks</code> to get a list of all external links on the page. If at least one link is found, it picks a random link from the list and returns it:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="k">def</code> <code class="nf">followExternalOnly</code><code class="p">(</code><code class="n">startingSite</code><code class="p">):</code>&#13;
    <code class="n">externalLink</code> <code class="o">=</code> <code class="n">getRandomExternalLink</code><code class="p">(</code><code class="n">startingSite</code><code class="p">)</code>&#13;
    <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Random external link is: </code><code class="si">{</code><code class="n">externalLink</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>&#13;
    <code class="n">followExternalOnly</code><code class="p">(</code><code class="n">externalLink</code><code class="p">)</code>&#13;
</pre>&#13;
&#13;
<p>The function <code>followExternalOnly</code> uses <code>getRandomExternalLink</code> and then recursively traverses across the internet. You can call it like this:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="n">followExternalOnly</code><code class="p">(</code><code class="s1">'https://www.oreilly.com/'</code><code class="p">)</code>&#13;
</pre>&#13;
&#13;
<p>This program starts at <a href="http://oreilly.com"><em>http://oreilly.com</em></a> and randomly hops from external link to external link. Here’s an example of the output it produces:</p>&#13;
&#13;
<pre data-code-language="text" data-type="programlisting">&#13;
http://igniteshow.com/&#13;
http://feeds.feedburner.com/oreilly/news&#13;
http://hire.jobvite.com/CompanyJobs/Careers.aspx?c=q319&#13;
http://makerfaire.com/</pre>&#13;
&#13;
<p>External links are not always guaranteed to <a contenteditable="false" data-primary="external links, location" data-type="indexterm" id="id495"/>be found on the first page of a website. To find external links in this case, a method similar to the one used in the previous crawling example is employed to recursively drill down into a website until it finds an external link.</p>&#13;
&#13;
<p><a data-type="xref" href="#flowchart_crawl">Figure 6-1</a> illustrates the operation as a flowchart.</p>&#13;
&#13;
<figure><div class="figure" id="flowchart_crawl"><img alt="Alt Text" class="iimagesweb_crawling_flowchart_-_new_pagepng" src="assets/wsp3_0601.png"/>&#13;
<h6><span class="label">Figure 6-1. </span>Flowchart for the script that crawls through sites on the internet</h6>&#13;
</div></figure>&#13;
&#13;
<div data-type="warning" epub:type="warning">&#13;
<h1>Don’t Put Example Programs into Production</h1>&#13;
&#13;
<p>I keep bringing this up, but for the sake of space and readability, the example programs in this book do not always contain the necessary checks and exception handling required for production-ready code. For example, if an external link is not found anywhere on a site that this crawler encounters (unlikely, but it’s bound to happen at some point if you run it long enough), this program will keep running until it hits Python’s recursion limit.</p>&#13;
&#13;
<p>One easy way to increase the robustness of this crawler would be to combine it with the connection exception-handling code in <a data-type="xref" href="ch04.html#c-4">Chapter 4</a>. This would allow the code to choose a different URL to go to if an HTTP error or server exception was encountered when retrieving the page.</p>&#13;
&#13;
<p>Before running this code for any serious purpose, make sure that you are putting checks in place to handle potential pitfalls.</p>&#13;
</div>&#13;
&#13;
<p>The nice thing about breaking up tasks into simple functions such as “find all external links on this page” is that the code can later be refactored to perform a different crawling task. For example, if your goal is to crawl an entire site for external links and make a note of each one, you can add the following function:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="c1"># Collects a list of all external URLs found on the site</code>&#13;
<code class="n">allExtLinks</code> <code class="o">=</code> <code class="p">[]</code>&#13;
<code class="n">allIntLinks</code> <code class="o">=</code> <code class="p">[]</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">getAllExternalLinks</code><code class="p">(</code><code class="n">url</code><code class="p">):</code>&#13;
    <code class="n">bs</code> <code class="o">=</code> <code class="n">BeautifulSoup</code><code class="p">(</code><code class="n">urlopen</code><code class="p">(</code><code class="n">url</code><code class="p">),</code> <code class="s1">'html.parser'</code><code class="p">)</code>&#13;
    <code class="n">internalLinks</code> <code class="o">=</code> <code class="n">getInternalLinks</code><code class="p">(</code><code class="n">bs</code><code class="p">,</code> <code class="n">url</code><code class="p">)</code>&#13;
    <code class="n">externalLinks</code> <code class="o">=</code> <code class="n">getExternalLinks</code><code class="p">(</code><code class="n">bs</code><code class="p">,</code> <code class="n">url</code><code class="p">)</code>&#13;
    <code class="k">for</code> <code class="n">link</code> <code class="ow">in</code> <code class="n">externalLinks</code><code class="p">:</code>&#13;
        <code class="k">if</code> <code class="n">link</code> <code class="ow">not</code> <code class="ow">in</code> <code class="n">allExtLinks</code><code class="p">:</code>&#13;
            <code class="n">allExtLinks</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">link</code><code class="p">)</code>&#13;
            <code class="nb">print</code><code class="p">(</code><code class="n">link</code><code class="p">)</code>&#13;
&#13;
    <code class="k">for</code> <code class="n">link</code> <code class="ow">in</code> <code class="n">internalLinks</code><code class="p">:</code>&#13;
        <code class="k">if</code> <code class="n">link</code> <code class="ow">not</code> <code class="ow">in</code> <code class="n">allIntLinks</code><code class="p">:</code>&#13;
            <code class="n">allIntLinks</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">link</code><code class="p">)</code>&#13;
            <code class="n">getAllExternalLinks</code><code class="p">(</code><code class="n">link</code><code class="p">)</code>&#13;
&#13;
&#13;
<code class="n">allIntLinks</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="s1">'https://oreilly.com'</code><code class="p">)</code>&#13;
<code class="n">getAllExternalLinks</code><code class="p">(</code><code class="s1">'https://www.oreilly.com/'</code><code class="p">)</code>&#13;
</pre>&#13;
&#13;
<p>This code can be thought of as two loops—one gathering internal links, one gathering external links—working in conjunction with each other. The flowchart looks something like <a data-type="xref" href="#flow_diagram_crawl">Figure 6-2</a>.</p>&#13;
&#13;
<figure><div class="figure" id="flow_diagram_crawl"><img alt="Alt Text" class="iimagesflowchart_internalexternalpng" src="assets/wsp3_0602.png"/>&#13;
<h6><span class="label">Figure 6-2. </span>Flow diagram for the website crawler that collects all external links</h6>&#13;
</div></figure>&#13;
&#13;
<p>Jotting down or making diagrams of what the code should do before you write the code itself is a fantastic habit to get into, and one that can save you a lot of time and frustration as your crawlers get more complicated.</p>&#13;
</div></section>&#13;
<div data-type="footnotes"><p data-type="footnote" id="id458"><sup><a href="ch06.html#id458-marker">1</a></sup> A popular parlor game created in the 1990s, <a href="https://en.wikipedia.org/wiki/Six_Degrees_of_Kevin_Bacon"><em>https://en.wikipedia.org/wiki/Six_Degrees_of_Kevin_Bacon</em></a>.</p><p data-type="footnote" id="id459"><sup><a href="ch06.html#id459-marker">2</a></sup> Thanks to <a href="http://oracleofbacon.org">The Oracle of Bacon</a> for satisfying my curiosity about this particular chain.</p><p data-type="footnote" id="id473"><sup><a href="ch06.html#id473-marker">3</a></sup> See <a href="http://nyti.ms/2pohZmu">“Exploring a ‘Deep Web’ That Google Can’t Grasp”</a> by Alex Wright. </p><p data-type="footnote" id="id475"><sup><a href="ch06.html#id475-marker">4</a></sup> See <a href="http://bit.ly/2psIw2M">“Hacker Lexicon: What Is the Dark Web?”</a> by Andy Greenberg.</p></div></div></section></body></html>