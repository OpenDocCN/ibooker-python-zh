- en: 'Appendix B. Scalable DataFrames: A Comparison and Some History'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录 B. 可扩展数据框架：比较和一些历史
- en: Dask’s distributed pandas-like DataFrame is, in our opinion, one of its key
    features. Various approaches exist to provide scalable DataFrame-like functionality.
    One of the big things that made Dask’s DataFrames stand out is the high level
    of support of the pandas APIs, which other projects are rapidly trying to catch
    up on. This appendix compares some of the different current and historical DataFrame
    libraries.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Dask的分布式类似于pandas的DataFrame，在我们看来是其关键特性之一。存在各种方法提供可扩展的类似DataFrame的功能。使得Dask的DataFrame脱颖而出的一个重要因素是对pandas
    API的高度支持，其他项目正在迅速赶上。本附录比较了一些不同的当前和历史数据框架库。
- en: To understand the differences, we will look at a few key factors, some of which
    are similar to techniques we suggest in [Chapter 8](ch08.xhtml#ch08). The first
    one is what the API looks like, and how much of your existing skills and code
    using pandas can be transferred. Then we’ll look at how much work is forced to
    happen on a single thread, on the driver/head node, and then on a single worker
    node.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解这些差异，我们将看几个关键因素，其中一些与我们在[第 8 章](ch08.xhtml#ch08)中建议的技术类似。首先是API的外观，以及使用pandas的现有技能和代码可以转移多少。然后我们将看看有多少工作被强制在单个线程、驱动程序/主节点上进行，然后在单个工作节点上进行。
- en: Scalable DataFrames does not have to mean distributed, although distributed
    scaling often allows for affordable handling of larger datasets than the single-machine
    options—and at truly massive scales, it’s the only practical option.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展数据框架并不一定意味着分布式，尽管分布式扩展通常允许处理比单机选项更大的数据集更经济实惠，并且在真正大规模的情况下，这是唯一的实际选择。
- en: Tools
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工具
- en: One of the common dependencies you’ll see in many of the tools is that they
    are built on top of ASF Arrow. While Arrow is a fantastic project, and we hope
    to see its continued adoption, it has some [type differences](https://oreil.ly/VPyAL),
    especially with respect to nullability.^([1](app02.xhtml#id1044)) These differences
    mean that most of the systems built using Arrow share some common restrictions.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 许多工具中常见的一个依赖是它们建立在ASF Arrow之上。虽然Arrow是一个很棒的项目，我们希望看到它持续被采纳，但它在[类型差异](https://oreil.ly/VPyAL)方面有些差异，特别是在可空性方面。^([1](app02.xhtml#id1044))
    这些差异意味着大多数使用Arrow构建的系统共享一些共同的限制。
- en: Open Multi-Processing (OpenMP) and Open Message Passing Interface (OpenMPI)
    are two other common dependencies many of these tools depend on. Despite their
    similar acronyms, by which you’ll see them referred to most commonly, they take
    fundamentally different approaches to parallelism. OpenMP is a single-machine
    tool focused on shared memory (with potentially non-uniform access). OpenMPI supports
    multiple machines and instead of shared memory uses message passing (conceptually
    similar to Dask’s actor system) for parallelization.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 开放多处理（OpenMP）和开放消息传递接口（OpenMPI）是许多这些工具依赖的另外两个常见依赖项。尽管它们有类似的缩写，你通常会看到它们被称为，但它们采用了根本不同的并行化方法。OpenMP是一个专注于共享内存的单机工具（可能存在非均匀访问）。OpenMPI支持多台机器，而不是共享内存，使用消息传递（在概念上类似于Dask的Actor系统）进行并行化。
- en: One Machine Only
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 仅限单机
- en: The one-machine scalable DataFrames focus on either parallelizing computation
    or allowing data to not all reside in memory at the same time (e.g., some can
    reside on disk). To a certain extent, this “data can reside on disk” approach
    can be solved with swap files at the OS level, but in practice having the library
    do intelligent paging in and out of elements has its benefits.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 单机可扩展数据框架专注于并行化计算或允许数据不同时驻留在内存中（例如，一些可以驻留在磁盘上）。在某种程度上，这种“数据可以驻留在磁盘上”的方法可以通过操作系统级别的交换文件来解决，但实际上，让库在元素的智能页面进出中进行智能分页也具有其优点。
- en: Pandas
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Pandas
- en: It may seem silly to mention pandas in a section on scaling DataFrames, but
    it’s useful to remember what the baseline is that we’re comparing against. Pandas
    is, generally, single threaded and requires that all of the data fits in memory
    on a single machine. There are various tricks that you can use to handle larger
    datasets in pandas, such as creating huge swap files or serially processing smaller
    chunks. It’s good to note that many of these techniques are incorporated in the
    tools for scaling pandas, so if you need to do that, it’s probably time to start
    exploring the options to scale. On the other hand, if everything is working fine
    in pandas, you get 100% pandas API compatibility by using pandas itself, something
    none of the other options are able to guarantee. Also, [pandas is a direct requirement
    more than any of the scalable pandas tools are](https://oreil.ly/IzYDb).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论缩放 DataFrame 的部分中提到 pandas 可能看起来有些愚蠢，但记住我们比较的基准是什么是有用的。总体而言，Pandas 是单线程的，要求所有数据都适合单台机器的内存。可以使用各种技巧来处理
    pandas 中更大的数据集，如创建大交换文件或逐个处理较小的块。需要注意的是，许多这些技术都已纳入用于扩展 pandas 的工具中，因此如果您需要这样做，现在可能是开始探索扩展选项的时候了。另一方面，如果在
    pandas 中一切正常运行，通过使用 pandas 本身可以获得 100% 的 pandas API 兼容性，这是其他选项无法保证的。另外，[pandas
    是直接要求，而不是可扩展 pandas 工具之一](https://oreil.ly/IzYDb)。
- en: H2O’s DataTable
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: H2O 的 DataTable
- en: DataTable is a single-machine DataFrame-like attempt to scale processing up
    to 100 GB (while the project authors describe this as “big data,” we view it as
    more along the lines of medium-sized data). Despite being for Python, DataTable,
    instead of copying the pandas APIs, aims to inherit much of R’s `data.table` APIs.
    This can make it a great choice for a team coming from R, but for dedicated pandas
    users it is likely less appealing. DataTable is also a single-company open source
    project, residing under H2O’s GitHub rather than in a foundation or on its own.
    At the time of this writing, it has a [relatively concentrated location of developer
    activity](https://oreil.ly/8vgA5). It has active CI (being run on incoming PRs),
    which we believe suggests higher-quality software. DataTable can use OpenMP to
    parallelize computation on a single machine, but it does not require OpenMP.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: DataTable 是一个类似于 DataFrame 的单机尝试，旨在扩展处理能力达到 100 GB（尽管项目作者将其描述为“大数据”，我们认为它更接近中等规模数据）。尽管是为
    Python 设计的，DataTable 并没有简单复制 pandas 的 API，而是致力于继承很多 R 的 `data.table` API。这使得它对于来自
    R 的团队来说可能是一个很好的选择，但对于专注于 pandas 的用户来说可能不太吸引人。DataTable 也是一个单公司开源项目，存放在 H2O 的 GitHub
    上，而不是在某个基金会或自己的平台上。在撰写本文时，它的[开发活动相对集中](https://oreil.ly/8vgA5)。它有积极的持续集成（在 PR
    进来时运行），我们认为这表明它是高质量的软件。DataTable 可以使用 OpenMP 在单台机器上并行计算，但不要求使用 OpenMP。
- en: Polars
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Polars
- en: Polars is another single-machine scalable DataFrame, but it takes the approach
    of writing its core functionality in Rust instead of C/C++ or Fortran. Like many
    of the distributed DataFrame tools, polars uses the ASF’s Arrow project for storing
    the DataFrames. Similarly, polars uses lazy evaluation to pipeline operations
    and internally partition/chunk the DataFrame, so (most of the time) it needs to
    have only a subset of the data in memory at any one time. Polars has one of the
    [largest developer communities among all single-machine scalable DataFrames](https://oreil.ly/zxoFJ).
    Polars links to benchmarks from its main page, showing it to be substantially
    faster than many of the distributed tools—but this comparison makes sense only
    when the distributed tools are constrained to a single machine, which is unlikely.
    It achieves its parallelism by using all of the cores in a single machine. Polars
    has [extensive documentation](https://oreil.ly/QW5s2), and it also has an explicit
    section on what to expect when coming from regular pandas. Not only does it have
    CI, but it has also integrated benchmark testing as part of each PR and tests
    against multiple versions of Python and environments.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Polars 是另一个单机可扩展的 DataFrame，但它采用的方法是在 Rust 中编写其核心功能，而不是 C/C++ 或 Fortran。与许多分布式
    DataFrame 工具类似，Polars 使用 ASF 的 Arrow 项目来存储 DataFrame。同样，Polars 使用惰性评估来管道化操作，并在内部分区/分块
    DataFrame，因此（大部分时间）只需在任一时间内内存中保留数据的子集。Polars 在所有单机可扩展 DataFrame 中拥有[最大的开发者社区](https://oreil.ly/zxoFJ)。Polars
    在其主页上链接到基准测试，显示其比许多分布式工具快得多，但仅当将分布式工具约束为单机时才有意义，这是不太可能的。它通过使用单台机器中的所有核心来实现其并行性。Polars
    拥有[详尽的文档](https://oreil.ly/QW5s2)，并且还有一个明确的章节，介绍从常规 pandas 迁移时可以期待的内容。它不仅具有持续集成，而且还将基准测试集成为每个
    PR 的一部分，并针对多个版本的 Python 和环境进行测试。
- en: Distributed
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分布式
- en: The majority of tools for scaling DataFrames are distributed in nature, since
    all of the fancy tricks on a single machine can get you only so far.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展 DataFrame 的大多数工具都具有分布式的特性，因为在单个机器上的所有花哨技巧只能带来有限的效果。
- en: ASF Spark DataFrame
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ASF Spark DataFrame
- en: Spark started out with what it called a resilient distributed dataset (RDD)
    and then quickly added a more DataFrame-like API called DataFrames. This caused
    much excitement, but many folks interpreted it to mean “pandas-like,” whereas
    Spark’s (initial) DataFrames was more akin to “SQL-like” DataFrames. Spark is
    written primarily in Scala and Java, both of which run on the Java Virtual Machine
    (JVM). While Spark has a Python API, it involves substantial data transfer between
    the JVM and Python, which can be slow and can increase memory requirements. Spark
    DataFrames was created before ASF Arrow, and so it has its own in-memory storage
    format, but it has since added support for using Arrow for communication between
    the JVM and Python.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 最初以所谓的弹性分布式数据集（RDD）起步，然后迅速添加了更类似于 DataFrame 的 API，称为 DataFrames。这引起了很多兴奋，但许多人误解它是指“类似于
    pandas”，而 Spark 的（最初的）DataFrames 更类似于“类似于 SQL 的”DataFrames。Spark 主要用 Scala 和 Java
    编写，两者都运行在 Java 虚拟机（JVM）上。虽然 Spark 有 Python API，但它涉及 JVM 和 Python 之间大量数据传输，这可能很慢，并且可能增加内存需求。Spark
    DataFrames 在 ASF Arrow 之前创建，因此具有其自己的内存存储格式，但后来添加了对 Arrow 在 JVM 和 Python 之间通信的支持。
- en: PySpark errors are especially difficult to debug, since when anything goes wrong
    you get a Java exception along with a Python exception.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 要调试 PySpark 错误尤其困难，因为一旦出错，你会得到一个 Java 异常和一个 Python 异常。
- en: SparklingPandas
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SparklingPandas
- en: Since Holden co-wrote SparklingPandas, it is the one library we can confidently
    say not to use without having to worry about people being upset.^([2](app02.xhtml#id1063))
    SparklingPandas is built on top of ASF Spark’s RDD and DataFrame APIs to provide
    a more Python-like API, but as the logo is a panda eating bamboo on a sticky note,
    you can see that we didn’t get all the way. SparklingPandas did show it was possible
    to provide a pandas-like experience by reusing parts of pandas itself.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Holden 共同编写了 SparklingPandas，我们可以自信地说不要使用这个库，而不必担心会有人不高兴。SparklingPandas
    建立在 ASF Spark 的 RDD 和 DataFrame API 之上，以提供更类似于 Python 的 API，但由于其标志是一只熊猫在便签纸上吃竹子，你可以看到我们并没有完全成功。SparklingPandas
    确实表明通过重用 pandas 的部分内容可以提供类似 pandas 的体验。
- en: For embarrassingly parallel types of operations, adding each function from the
    pandas API by using `map` to delegate the Python code on each DataFrame was very
    fast. Some operations, like dtypes, were evaluated on just the first DataFrame.
    Grouped and window operations were more complicated.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于尴尬并行类型的操作，通过使用 `map` 将 pandas API 的每个函数添加到每个 DataFrame 上，Python 代码的委托非常快速。一些操作，如
    dtypes，仅在第一个 DataFrame 上评估。分组和窗口操作则更为复杂。
- en: Since the initial co-authors had day jobs with other focus areas, the project
    failed to move beyond proof-of-concept.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 由于最初的合著者有其他重点领域的日常工作，项目未能超越概念验证阶段。
- en: Spark Koalas/Spark pandas DataFrames
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Spark Koalas / Spark pandas DataFrames
- en: The Koalas project, which was integrated into Spark 3.2, initially came out
    of Databricks. Koalas follows a similar approach of chunking pandas DataFrames,
    but these DataFrames are represented as Spark DataFrames rather than Arrow DataFrames.
    Like most of the systems, the DataFrames are lazily evaluated to allow for pipelining.
    Arrow is used to transfer data to and from the JVM, so you still have all of the
    type restrictions of Arrow. This project benefits from being part of a large community
    and being interoperable with much of the traditional big data stack. This comes
    from being a part of the JVM and Hadoop ecosystem, which also comes with some
    downsides for performance. At present, moving data between the JVM and Python
    increases overhead, and in general, Spark is focused on supporting heavier-weight
    tasks.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Koalas 项目最初源自 Databricks，并已整合到 Spark 3.2 中。Koalas 采用类似的分块 pandas DataFrames
    方法，但这些 DataFrames 表示为 Spark DataFrames 而不是 Arrow DataFrames。像大多数系统一样，DataFrames
    被延迟评估以允许流水线处理。Arrow 用于将数据传输到 JVM 并从中传输数据，因此您仍然具有 Arrow 的所有类型限制。这个项目受益于成为一个庞大社区的一部分，并与传统的大数据堆栈大部分互通。这源自于作为
    JVM 和 Hadoop 生态系统的一部分，但这也会带来性能上的一些不利影响。目前，在 JVM 和 Python 之间移动数据会增加开销，而且总体上，Spark
    专注于支持更重的任务。
- en: Grouped operations on Spark Koalas/Spark pandas DataFrames do not yet support
    partial aggregations. This means that all the data for one key must fit on one
    node.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark Koalas / Spark pandas DataFrames 上的分组操作尚不支持部分聚合。这意味着一个键的所有数据必须适合一个节点。
- en: Cylon
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Cylon
- en: Cylon’s home page is very focused on benchmarks, but the benchmark it has chosen
    (comparing Cylon to Spark on a single machine) is one that is easy to meet, since
    Spark is designed for distributed usage instead of single-machine usage. Cylon
    uses PyArrow for storage along with OpenMPI for managing its task parallelism.
    Cylon also has a GPU backend called GCylon. PyClon’s documentation has a lot of
    room for growth, and the link to its API documentation is currently broken.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Cylon 的主页非常专注于基准测试，但它选择的基准测试（将 Cylon 与 Spark 在单机上进行比较）很容易达到，因为 Spark 是设计用于分布式使用而不是单机使用。Cylon
    使用 PyArrow 进行存储，并使用 OpenMPI 管理其任务并行性。Cylon 还有一个名为 GCylon 的 GPU 后端。PyClon 的文档还有很大的改进空间，并且当前的
    API 文档链接已经失效。
- en: The Cylon community seems to have ~30 messages per year, and attempting to find
    any open source users of the DataFrame library [comes up empty](https://oreil.ly/uroxr).
    The [contributor file](https://oreil.ly/dWC16) and LinkedIn show the majority
    of contributors all share a common university.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Cylon 社区似乎每年有约 30 条消息，试图找到任何使用 DataFrame 库的开源用户 [没有结果](https://oreil.ly/uroxr)。[贡献者文件](https://oreil.ly/dWC16)
    和 LinkedIn 显示大多数贡献者都来自同一所大学。
- en: The project follows several software engineering best practices, like having
    CI enabled. That being said, the comparatively small (visibly active) community
    and lack of clear documentation mean that, in our mind, depending on Cylon would
    be more involved than some other options.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 该项目遵循几个软件工程的最佳实践，如启用 CI。尽管如此，相对较小（明显活跃）的社区和缺乏清晰的文档意味着，在我们看来，依赖 Cylon 可能比其他选项更复杂。
- en: Ibis
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Ibis
- en: The Ibis project [promises](https://oreil.ly/9OL2f) “the flexibility of Python
    analytics with the scale and performance of modern SQL.” It compiles your somewhat
    pandas-like code (as much as possible) into SQL. This is convenient, as not only
    do many big data systems (like Hive, Spark, BigQuery, etc.) support SQL, but it
    is also the de facto query language for the majority of databases out there. Unfortunately,
    SQL is not uniformly implemented, so moving between backend engines may result
    in breakages, but Ibis does a great job of [tracking which APIs work with which
    backends](https://oreil.ly/g2E_W). Of course, this design limits you to the kinds
    of expressions that can be expressed in SQL.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[Ibis 项目](https://oreil.ly/9OL2f) 承诺“结合 Python 分析的灵活性和现代 SQL 的规模与性能”。它将你的代码编译成类似
    pandas 的 SQL 代码（尽可能），这非常方便，因为许多大数据系统（如 Hive、Spark、BigQuery 等）支持 SQL，而且 SQL 是目前大多数数据库的事实标准查询语言。不幸的是，SQL
    的实现并不统一，因此在不同后端引擎之间移动可能会导致故障，但 Ibis 在 [跟踪哪些 API 适用于哪些后端引擎](https://oreil.ly/g2E_W)
    方面做得很好。当然，这种设计限制了你可以在 SQL 中表达的表达式类型。'
- en: Modin
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Modin
- en: Like Ibis, Modin is slightly different from many of the other tools in that
    it has multiple distributed backends, including Ray, Dask, and OpenMPI. Modin
    has the stated goal of handling from 1 MB to 1+ TB, which is a wide range to attempt
    to cover. [Modin’s home page](https://modin.org) also makes a claim to “Scale
    your pandas workflows by changing a single line of code,” which, while catchy,
    in our opinion overpromises on the API compatibility and knowledge required to
    take advantage of parallel and distributed systems.^([3](app02.xhtml#id1074))
    In our opinion, Modin is very exciting since it seems silly for each distributed
    computing engine to have its own re-implementation of the pandas APIs. Modin has
    a very active developer community, with core developers from multiple companies
    and backgrounds. On the other hand, we feel that the current documentation does
    not do a good enough job of setting users up for success with understanding the
    limitations of Modin. Thankfully, much of the intuition you will have developed
    around Dask DataFrames still applies to Modin. We think Modin is ideal for individuals
    who need to move between different computation engines.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Ibis 类似，Modin 与许多其他工具略有不同，它具有多个分布式后端，包括 Ray、Dask 和 OpenMPI。Modin 的宣称目标是处理从
    1 MB 到 1+ TB 的数据，这是一个广泛的范围。[Modin 的主页](https://modin.org) 还声称可以“通过更改一行代码扩展您的 pandas
    工作流”，虽然这种说法有吸引力，但在我们看来，它对 API 兼容性和利用并行和分布式系统所需的知识要求做出了过多的承诺。^([3](app02.xhtml#id1074))
    在我们看来，Modin 很令人兴奋，因为每个分布式计算引擎都有自己重新实现 pandas API 的需求看起来很愚蠢。Modin 有一个非常活跃的开发者社区，核心开发者来自多个公司和背景。另一方面，我们认为当前的文档并没有很好地帮助用户理解
    Modin 的局限性。幸运的是，您对 Dask DataFrames 的大部分直觉在 Modin 中仍然适用。我们认为 Modin 对需要在不同计算引擎之间移动的个人用户来说是理想选择。
- en: Warning
  id: totrans-35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Unlike the other systems, Modin is eagerly evaluated, meaning it can’t take
    advantage of automatic pipelining of your computation.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他系统不同，Modin 被积极评估，这意味着它不能利用自动流水线处理您的计算。
- en: Vanilla Dask DataFrame
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Vanilla Dask DataFrame
- en: We are biased here, but we think that Dask’s DataFrame library does an excellent
    job of striking a balance between being an easy on-ramp and being clear about
    its limitations. Dask’s DataFrames have a large number of contributors from a
    variety of different companies. Dask DataFrames also have a relatively high level
    of parallelism, including for grouped operations, not found in many of the other
    systems.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里有偏见，但我们认为 Dask 的 DataFrame 库在平衡易于入门和明确其限制方面做得非常好。Dask 的 DataFrames 拥有来自多家不同公司的大量贡献者。Dask
    DataFrames 还具有相对高水平的并行性，包括对分组操作的支持，在许多其他系统中找不到。
- en: cuDF
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: cuDF
- en: cuDF extends Dask DataFrame to add support for GPUs. It is, however, primarily
    a single-company project, from NVIDIA. This makes sense since NVIDIA wants to
    sell you more GPUs, but it also does mean it is unlikely to, say, add support
    for AMD GPUs anytime soon. This project is likely to be maintained if NVIDIA continues
    to see a future in selling more GPUs for data analytics as best served with pandas-like
    interfaces.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: cuDF 扩展了 Dask DataFrame，以支持 GPU。然而，它主要是一个单一公司项目，来自 NVIDIA。这是有道理的，因为 NVIDIA 希望卖更多的
    GPU，但这也意味着它不太可能很快为 AMD GPU 添加支持。如果 NVIDIA 继续认为为数据分析销售更多 GPU 是最佳选择的话，该项目可能会得到维护，并保持类似
    pandas 的接口。
- en: cuDF not only has CI but also has a strong culture of code review with per-area
    responsibilities.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: cuDF 不仅具有 CI，而且具有区域责任的强大代码审查文化。
- en: Conclusion
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In an ideal world, there would be a clear winner, but as you can see, the different
    scalable DataFrame libraries serve different purposes, and except those already
    abandoned, all have potential uses. We think all of these libraries have their
    place, depending on your exact needs.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在理想的世界中，会有一个明确的赢家，但正如你所见，不同的可扩展 DataFrame 库为不同目的提供服务，除了那些已经被放弃的，所有都有潜在的用途。我们认为所有这些库都有其位置，取决于您的确切需求。
- en: ^([1](app02.xhtml#id1044-marker)) Arrow allows all data types to be null. Pandas
    does not allow integer columns to contain nulls. When reading Arrow files as pandas,
    if an Int column does not contain nulls, it will be read as Int in the pandas
    DataFrame, but if at runtime it encounters a null, the entire column will be read
    as a float.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](app02.xhtml#id1044-marker)) Arrow 允许所有数据类型为 null。Pandas 不允许整数列包含 null。当将
    Arrow 文件读取为 pandas 时，如果一个整数列不包含 null，它将被读取为整数在 pandas DataFrame 中，但如果在运行时遇到 null，则整个列将被读取为浮点数。
- en: ^([2](app02.xhtml#id1063-marker)) Besides ourselves, and if you’re reading this
    you’ve likely helped Holden buy a cup of coffee and that’s enough. :)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](app02.xhtml#id1063-marker)) 除了我们自己之外，如果你正在阅读这篇文章，你可能已经帮助 Holden 买了一杯咖啡，那就足够了。:)
- en: ^([3](app02.xhtml#id1074-marker)) For example, see the confusion around the
    limitation with groupBy + apply, which is not otherwise documented besides a [GitHub
    issue](https://oreil.ly/rIeam).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '^([3](app02.xhtml#id1074-marker)) 例如，看看关于 groupBy + apply 的限制混乱，除了 [GitHub
    问题](https://oreil.ly/rIeam) 外，没有其他文档。 '
