- en: 'Chapter 3\. How Dask Works: The Basics'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you’ve run your first few tasks with Dask, it’s time to learn a little
    bit about what’s happening behind the scenes. Depending on whether you are using
    Dask locally or in a distributed fashion, the behavior can be a little different.
    While Dask does a good job of abstracting away many of the details of running
    on multiple threads or servers, having a solid grasp of how Dask is working will
    help you better decide both how and when to use it.
  prefs: []
  type: TYPE_NORMAL
- en: 'To be familiar with Dask, you need to understand:'
  prefs: []
  type: TYPE_NORMAL
- en: The deployment framework that Dask is able to run on, and its strengths and
    weaknesses
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The types of data that Dask is able to read, and how you can interact with the
    data types in Dask
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The computational pattern of Dask, and how to turn your ideas into Dask code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to monitor and troubleshoot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will introduce each of these concepts, and we will expand
    upon them in the rest of the book.
  prefs: []
  type: TYPE_NORMAL
- en: Execution Backends
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dask has many different execution backends, but we find it easiest to think
    about them in two groups, local and distributed. With local backends, you are
    limited in scale to what a single computer can handle. Local backends also have
    advantages like the avoidance of network overhead, simpler library management,
    and a lower dollar cost.^([1](ch03.xhtml#id374)) Dask’s distributed backend has
    many options for deployment, from cluster managers such as Kubernetes to job queue–like
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: Local Backends
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dask’s three local backends are single-process, multi-threaded, and multi-process.
    The single-process backend has no parallelism and is mostly useful for validating
    that a problem is caused by concurrency. Multi-threaded and multi-process backends
    are ideal for problems in which the data is small or the cost of copying it would
    be higher than computation time.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you don’t configure a specific local backend, Dask will pick the backend
    based on the library you are working with.
  prefs: []
  type: TYPE_NORMAL
- en: The local multi-threaded scheduler is able to avoid having to serialize data
    and interprocess communication costs. The multi-threaded backend is suited for
    tasks in which the majority of the computation is happening in native code, outside
    of Python. This is the case for many numeric libraries, such as pandas and NumPy.
    If that is the case for you, you can configure Dask to use multi-threading, as
    shown in [Example 3-1](#configure_dask_multithreading_ch03_1686241430957).
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-1\. Configuring Dask to use multi-threading
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The local multi-process backend, shown in [Example 3-2](#configuring_dask_multiprocess_ch03_1686241509022),
    has some additional overhead over multi-threaded, although it can be decreased
    on Unix and Unix-like systems.^([2](ch03.xhtml#id375)) The multi-process backend
    is able to avoid Python’s global interpreter lock by launching separate processes.
    Launching a new process is more expensive than a new thread, and Dask needs to
    serialize data that moves between processes.^([3](ch03.xhtml#id376))
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-2\. Configuring Dask to use the multi-process backend
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: If you are running on a Unix system, you can use the *forkserver*, shown in
    [Example 3-3](#configuring_dask_forkserver_ch03_1686241608423), which will [reduce
    the overhead of starting each Python interpreter](https://oreil.ly/U9voe). Using
    the forkserver will not reduce the communication overhead.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-3\. Configuring Dask to use the multi-process forkserver
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This optimization is generally not available on Windows.
  prefs: []
  type: TYPE_NORMAL
- en: Dask’s local backends are designed for performance rather than for testing that
    your code will work on a distributed scheduler. To test that your code will run
    remotely, you should use Dask’s distributed scheduler with a LocalCluster instead.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed (Dask Client and Scheduler)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While Dask can work well locally, its true power comes with the distributed
    scheduler, with which you can scale your problem to multiple machines. Since there
    are physical and financial limits to how much computing power, storage, and memory
    can be put into one machine, using multiple machines is often the most cost-efficient
    solution (and it is sometimes the only solution). Distributed computing is not
    without its drawbacks; as Leslie Lamport famously said, “A distributed system
    is one in which the failure of a computer you didn’t even know existed can render
    your own computer unusable.” While Dask does much to limit these failures (see
    [“Fault Tolerance”](#fault_tolerance_ch03_1688739701115)), you accept some increase
    in complexity when moving to a distributed system.
  prefs: []
  type: TYPE_NORMAL
- en: Dask has one distributed scheduler backend, and it talks to many different types
    of clusters, including a LocalCluster. Each type of cluster is supported in its
    own library, which schedules the scheduler^([4](ch03.xhtml#id383)) that the Dask
    client then connects to. Using the distributed abstraction `dask.distributed`
    gives you portability between any types of cluster you may be using at the moment,
    including local. If you don’t use `dask​.dis⁠tributed`, Dask can still run perfectly
    well on a local computer, in which case you are using a default single-machine
    scheduler provided in the Dask library.
  prefs: []
  type: TYPE_NORMAL
- en: The Dask client is your entry point into the Dask distributed scheduler. In
    this chapter, we will be using Dask with a Kubernetes cluster; if you have another
    type of cluster or want details, please see [Chapter 12](ch12.xhtml#ch12).
  prefs: []
  type: TYPE_NORMAL
- en: Auto-scaling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With auto-scaling, Dask can increase or decrease the computers/resources being
    used, based on the tasks you have asked it to run.^([5](ch03.xhtml#id386)) For
    example, if you have a program that computes complex aggregations using many computers
    but then mostly operates on the aggregated data, the number of computers you need
    could decrease by a large amount post-aggregation. Many workloads, including machine
    learning, do not need the same amount of resources/computers the entire time.
  prefs: []
  type: TYPE_NORMAL
- en: Some of Dask’s cluster backends, including Kubernetes, support auto-scaling,
    which Dask calls *adaptive deployments*. Auto-scaling is useful mostly in situations
    of shared cluster resources, or when running on cloud providers where the underlying
    resources are paid for by the hour.
  prefs: []
  type: TYPE_NORMAL
- en: Important limitations with the Dask client
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dask’s client is not fault tolerant, so while Dask can handle the failures of
    its workers, if the connection between the client and the scheduler is broken,
    your application will fail. A common workaround for this is scheduling the client
    within the same environment as the scheduler, although this does somewhat reduce
    the usefulness of having the client and scheduler as separate components.
  prefs: []
  type: TYPE_NORMAL
- en: Libraries and dependencies in distributed clusters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Part of why Dask is so powerful is the Python ecosystem that it is in. While
    Dask will pickle, or serialize (see [“Serialization and Pickling”](#ser_pick_dtl)),
    and send our code to the workers, this doesn’t include the libraries we use.^([6](ch03.xhtml#id400))
    To take advantage of that ecosystem, you need to be able to use additional libraries.
    During the exploration phase, it is common to install packages at runtime as you
    discover that you need them.
  prefs: []
  type: TYPE_NORMAL
- en: The `PipInstall` worker plug-in takes a list of packages and installs them at
    runtime on all of the workers. Looking back at [Example 2-4](ch02.xhtml#web_crawler_ch02_1688747981454),
    to install bs4 you would call `distributed.diagnostics.plugin.PipInstall(["bs4"])`.
    Any new workers that are launched by Dask then need to wait for the package to
    be installed. The `Pip​Install` plug-in is ideal for quick prototyping when you
    are discovering which packages you need. You can think of `PipInstall` as the
    replacement for `!pip install` in a notebook over having a virtualenv.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid the slowness of having to install packages each time a new worker is
    launched, you should try to pre-install your libraries. Each cluster manager (e.g.,
    YARN, Kubernetes, Coiled, Saturn, etc.) has its own methods for managing dependencies.
    This can happen at runtime or at setup where the packages are pre-installed. The
    specifics for the different cluster managers are covered in [Chapter 12](ch12.xhtml#ch12).
  prefs: []
  type: TYPE_NORMAL
- en: With Kubernetes, for example, the default startup script checks for the presence
    of some key environment variables (`EXTRA_APT_PACKAGES`, `EXTRA_CONDA_PACKAGES`,
    and `EXTRA_PIP_PACKAGES`), which, in conjunction with customized worker specs,
    can be used to add dependencies at runtime. Some of them, like Coiled and Kubernetes,
    allow for adding dependencies when building an image for our workers. Others,
    like YARN, use preallocated conda/virtual environment packing.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It is very important to have the same versions of Python and libraries installed
    on all of the workers and your client. Different versions of libraries can lead
    to outright failures or subtler data correctness issues.
  prefs: []
  type: TYPE_NORMAL
- en: Dask’s Diagnostics User Interface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of your first stops in understanding what your program is doing should be
    Dask’s Diagnostics UI. The UI allows you to see what Dask is executing, the number
    of worker threads/processes/computers, memory utilization information, and much
    more. If you are running Dask locally, you will likely find the UI at *http:​//localhost:8787*.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re using the Dask client to connect to a cluster, the UI will be running
    on the scheduler node. You can get the link to the dashboard from `client.dashboard_link`.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For remote notebook users, the hostname of the scheduler node may not be reachable
    directly from your computer. One option is to use the Jupyter proxy; for example,
    one might go to `http://jupyter.example.com/user/username/proxy/dask-head-4c81d51e-3.jhub:8787/status`
    to access the endpoint `dask-head-4c81d51e-3.jhub:8787/status`.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 3-1](#fig_dask_ui) shows the Dask UI during the running of the examples
    in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: '![spwd 0301](Images/spwd_0301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-1\. The Dask UI ([digital, color version](https://oreil.ly/PuWRN))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The UI allows you to see what Dask is doing and what’s being stored on the workers
    and to explore the execution graph. We will revisit the execution graph in [“visualize”](#graph_visualize).
  prefs: []
  type: TYPE_NORMAL
- en: Serialization and Pickling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Distributed and parallel systems depend on serialization, sometimes called *pickling*
    in Python, to share data and functions/code between processes. Dask uses a mixture
    of serialization techniques to match the use case and provides hooks to extend
    by class when the defaults don’t meet your needs.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We most often think of serialization when it fails (with an error), but equally
    important can be situations where we end up serializing more data than we need—or
    when the amount of data that needs to be transferred is so large that distributing
    the work is no longer beneficial.
  prefs: []
  type: TYPE_NORMAL
- en: Cloudpickle serializes the functions and the generic Python types in Dask. Most
    Python code doesn’t depend on serializing functions, but cluster computing often
    does. Cloudpickle is a project designed for cluster computing and is able to serialize
    and deserialize more functions than Python’s built-in pickle.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Dask has its own ability to extend serialization, but the registry methods are
    not automatically sent to the workers, and it’s not always used.^([7](ch03.xhtml#id416))
  prefs: []
  type: TYPE_NORMAL
- en: Dask has built-in special handling for NumPy arrays, sparse, and cuPY. These
    serializations tend to be more space efficient than the default serializers. When
    you make a class that contains one of these types and does not require any special
    initialization, you should call `register_generic(YourClass)` from `dask.distributed.protocol`
    to take advantage of Dask’s special handling.
  prefs: []
  type: TYPE_NORMAL
- en: If you have a class that is not serializable, as in [Example 3-4](#fail_to_ser),
    you can wrap it to add serialization functions to it, as shown in [Example 3-5](#custom_ser_with_pickle).
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-4\. Dask fails to serialize
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Example 3-5\. Custom serialization
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: If you control the original class, you can also directly add the `getstate`/`setstate`
    methods instead of wrapping it.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Dask automatically attempts to compress serialized data, which generally improves
    performance. You can disable this by setting `distributed.comm.compression` to
    `None`.
  prefs: []
  type: TYPE_NORMAL
- en: Partitioning/Chunking Collections
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Partitioning gives you the ability to control the number of tasks used to process
    your data. If you have billions of rows, using one task for each row would mean
    you spend more time scheduling the tasks than doing the work itself. Understanding
    partitioning is key to being able to make the most efficient use of Dask.
  prefs: []
  type: TYPE_NORMAL
- en: Dask uses slightly different terminology for partitioning in each of its collections.
    In Dask, partitioning impacts how data is located on your cluster, and the right
    partitioning for your problem can make order-of-magnitude improvements. Partitioning
    has a few different aspects, like the size of each partition, the number of partitions,
    and optional properties such as partition key and sorted versus unsorted.
  prefs: []
  type: TYPE_NORMAL
- en: The number and size of partitions are closely related and impact the maximum
    parallelism. Partitions that are too small or too great in number mean that Dask
    will spend more time scheduling the tasks than running them. A general sweet spot
    for partition size is around 100 MB to 1 GB, but if your computation per element
    is very expensive, smaller partition sizes can perform better.
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, your partitions should be similar in size to avoid stragglers. A situation
    in which you’ve got partitions of different sizes is called *skewed*. There are
    many different sources of skew, ranging from input file sizes to key skew (when
    keyed). When your data gets too skewed, you will need to repartition the data.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The Dask UI is a great place to see if you might have stragglers.
  prefs: []
  type: TYPE_NORMAL
- en: Dask Arrays
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dask arrays’ partitions are called *chunks* and represent the number of elements.
    Although Dask always knows the number of chunks, when you apply a filter or load
    data, Dask is unaware of the size of each chunk. Indexing or slicing a Dask array
    requires that Dask know the chunk sizes so it can find the chunk(s) with the desired
    elements. Depending on how your Dask array was created, Dask may or may not know
    the size of each chunk. We talk about this more in [Chapter 5](ch05.xhtml#ch05).
    If you want to index into an array where Dask does not know the chunk sizes, you
    will need to first call `compute_chunk_sizes()` on the array. When creating a
    Dask array from a local collection, you can specify the target chunk size, as
    shown in [Example 3-6](#custom_array_chunk_size).
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-6\. Custom array chunk size
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Partitions/chunking doesn’t have to be static, and the `rechunk` function allows
    you to change the chunk size of a Dask array.
  prefs: []
  type: TYPE_NORMAL
- en: Dask Bags
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dask bags’ partitions are called *partitions*. Unlike with Dask arrays, since
    Dask bags do not support indexing, Dask does not track the number of elements
    in each partition. When you use `scatter`, Dask will try to partition the data
    as well as possible, but subsequent iterations can change the number of elements
    inside each partition. Similar to Dask arrays, when creating from a local collection,
    you can specify the number of partitions of a bag, except the parameter is called
    `npartitions` instead of `chunks`.
  prefs: []
  type: TYPE_NORMAL
- en: You can change the number of partitions in a bag by calling `repartition` with
    either `npartitions` (for a fixed number of partitions) or `partition_size` (for
    a target size of each partition). Specifying `partition_size` is more expensive
    since Dask needs to do some extra computation to determine what the matching number
    of partitions would be.
  prefs: []
  type: TYPE_NORMAL
- en: You can think of data as keyed when there is an index or when the data can be
    looked up by a value, such as in a hashtable. While bags implement keyed operations
    like `groupBy`, where values with the same key are combined, their partitioning
    does not have any idea of key and instead keyed operations always operate on all
    partitions.^([8](ch03.xhtml#id430))
  prefs: []
  type: TYPE_NORMAL
- en: Dask DataFrames
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DataFrames have the most options for partitioning. DataFrames can have partitions
    of different sizes, as well as known or unknown partitioning. With unknown partitioning,
    the data is distributed, but Dask is unable to determine which partition holds
    a particular key. Unknown partitioning happens often, as any operation that could
    change the value of a key results in unknown partitioning. The `known_divisions`
    property on a DataFrame allows you to see whether Dask knows the partitioning,
    and the `index` property shows the splits used and the column.
  prefs: []
  type: TYPE_NORMAL
- en: If a DataFrame has the right partitioning, operations like `groupBy`, which
    would normally involve a lot of internode communication, can be executed with
    less communication. Accessing rows by ID requires that the DataFrame is partitioned
    on that key. If you want to change the column that your DataFrame is partitioned
    on, you can call `set_index` to change the index. Setting the index, like all
    the repartitioning operations, involves copying the data between workers, known
    as a *shuffle*.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The “right” partitioner for a dataset depends on not only the data but also
    your operations.
  prefs: []
  type: TYPE_NORMAL
- en: Shuffles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Shuffling refers to transferring data between different workers to repartition
    the data. Shuffling can be the result of an explicit operation, like calling `repartition`,
    or an implicit one, like grouping data together by key or performing an aggregation.
    Shuffles tend to be relatively expensive, so it’s useful to minimize how often
    they are needed and also to reduce the amount of data they move.
  prefs: []
  type: TYPE_NORMAL
- en: The most straightforward case to understand shuffles is when you explicitly
    ask Dask to repartition your data. In those cases you generally see many-to-many
    worker communication, with the majority of the data needing to be moved over the
    network. This is naturally more expensive than situations in which data is able
    to be processed locally, as the network is much slower than RAM.
  prefs: []
  type: TYPE_NORMAL
- en: Another important way that you can trigger shuffles is implicitly through a
    reduction/aggregation. In such cases, if parts of the reduction or aggregation
    can be applied prior to moving the data around, Dask is able to transfer less
    data over the network, making for a faster shuffle.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Sometimes you’ll see things referred to as *map-side* and *reduce-side*; this
    just means before and after the shuffle.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll explore more about how to minimize the impact of shuffles in the next
    two chapters, where we introduce aggregations.
  prefs: []
  type: TYPE_NORMAL
- en: Partitions During Load
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far you’ve seen how to control partitions when creating from a local collection,
    as well as how to change the partitioning of an existing distributed collection.
    Partitioning during the creation of a collection from delayed tasks is generally
    1:1, with each delayed task being its own partition. When loading data from files,
    partitioning becomes a bit more complicated, involving file layout and compression.
    Generally speaking, it is good practice to look at the partitioning of data you
    have loaded by calling `npartitions` for bags, `chunks` for arrays, or `index`
    for DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: Tasks, Graphs, and Lazy Evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tasks are the building blocks that Dask uses to implement `dask.delayed`, futures,
    and operations on Dask’s collections. Each task represents a small piece of computation
    that Dask cannot break down any further. Tasks are often fine-grained, and when
    computing a result Dask will try to combine multiple tasks into a single execution.
  prefs: []
  type: TYPE_NORMAL
- en: Lazy Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most of Dask is lazily evaluated, with the exception of Dask futures. Lazy evaluation
    shifts the responsibility for combining computations from you to the scheduler.
    This means that Dask will, when it makes sense, combine multiple function calls.
    Not only that, but if only some parts of a structure are needed, Dask is sometimes
    able to optimize by evaluating just the relevant parts (like `head` or `tail`
    calls).^([9](ch03.xhtml#id439)) Implementing lazy evaluation requires Dask to
    construct a task graph. This task graph is also reused for fault tolerance.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike most of Dask, futures are eagerly evaluated, which limits the optimizations
    available when chaining them together, as the scheduler has a less complete view
    of the world when it starts executing the first future. Futures still create task
    graphs, and you can verify this by visualizing them in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the rest of Dask, futures are eagerly evaluated, which limits the optimizations
    available when chaining them together, as the scheduler has a less complete view
    of the world when it starts executing the first future. Futures still create task
    graphs, and you can verify this by visualizing them, as we’ll see in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Task Dependencies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to nested tasks, as seen in [“Nested tasks”](ch02.xhtml#nested_tasks),
    you can also use a `dask.delayed` object as input to another delayed computation
    (see [Example 3-7](#task_dependencies_example_ch03_1686946028901)), and Dask’s
    `submit`/`compute` function will construct a task graph for you.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-7\. Task dependencies
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now when you go to compute the final combined value, Dask will compute all of
    the other values that are needed for the final function using its implicit task
    graph.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You don’t need to pass around real values. For example, if one function updates
    a database and you want to run another function after that, you can use it as
    a parameter even if you don’t actually need its Python return value.
  prefs: []
  type: TYPE_NORMAL
- en: By passing delayed objects into other delayed function calls, you allow Dask
    to re-use shared nodes in the task graph and potentially reduce network overhead.
  prefs: []
  type: TYPE_NORMAL
- en: visualize
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Visualizing the task graph is an excellent tool for you to use while learning
    about task graphs and debugging in the future. The `visualize` function is defined
    both in the Dask library and on all Dask objects. Instead of calling `.visualize`
    separately on multiple objects, you should call `dask.visualize` with the list
    of objects you are planning to compute to see how Dask combines the task graph.
  prefs: []
  type: TYPE_NORMAL
- en: You should try this out now by visualizing Examples [2-6](ch02.xhtml#make_bag_of_crawler)
    through [2-9](ch02.xhtml#wc_func). When you call `dask.visualize` on `words_bag​.fre⁠quen⁠cies()`,
    you should get a result that looks something like [Figure 3-2](#vis_ex).
  prefs: []
  type: TYPE_NORMAL
- en: '![spwd 0302](Images/spwd_0302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-2\. Visualized word count task graph (redrawn output)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The Dask UI also shows visualized representations of the task graph, without
    the need to modify your code.
  prefs: []
  type: TYPE_NORMAL
- en: Intermediate Task Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Intermediate task results are generally removed as soon as the dependent task
    has started to execute. This can be less than optimal when we need to perform
    multiple computations on the same piece of data. One solution to this is combining
    all our execution together into one call to `dask.compute`, so that Dask is able
    to keep the data around as needed. This breaks down in both the interactive case,
    where we don’t know in advance what our computation is going to be, and iterative
    cases. In those cases, some form of caching or persistence can be beneficial.
    You will learn about how to apply caching later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Task Sizing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dask uses a centralized scheduler, which is a common technique for many systems.
    It does mean, however, that while the general task scheduling overhead is 1 ms,
    as the number of tasks in a system increases the scheduler can become a bottleneck,
    and the overhead can grow. Counterintuitively, this means that as our system grows
    we may benefit from larger, coarser-grained tasks.
  prefs: []
  type: TYPE_NORMAL
- en: When Task Graphs Get Too Large
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes the task graph itself can become too much for Dask to handle. This
    issue can show up as an out-of-memory exception on the client or scheduler or,
    more commonly, as jobs that slow down with iterations. Most frequently this occurs
    with recursive algorithms. One common example of a situation in which the graph
    can become too expensive to keep is distributed alternating least squares.
  prefs: []
  type: TYPE_NORMAL
- en: The first step when encountering a situation with a too-large task graph is
    to see if you can reduce the parallelism by using larger chunks of work or by
    switching the algorithm. For example, if we think of Fibonacci numbers computed
    with recursion, a better option would be using a dynamic programming or memoized
    solution instead of trying to distribute the computation task with Dask.
  prefs: []
  type: TYPE_NORMAL
- en: If you have an iterative algorithm, and there isn’t a better way to accomplish
    what you want, you can help Dask out by periodically writing out the intermediate
    work and re-loading it.^([10](ch03.xhtml#id454)) By doing this, Dask does not
    have to keep track of all of the steps involved in creating the data, but instead
    just needs to remember where the data is. The next two chapters will look at how
    to write and load data efficiently for these and other purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In Spark the equivalent idea is expressed as *checkpointing*.
  prefs: []
  type: TYPE_NORMAL
- en: Combining Computation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To take the most advantage of Dask’s graph optimization, it’s important to submit
    your work in larger batches. First off, when you’re blocking on the result with
    `dask.compute`, small batches can limit the parallelism. If you have a shared
    parent—say, two results on the same data—submitting the computations together
    allows Dask to share the computation of the underlying data. You can verify whether
    Dask is able to share a common node by calling `visualize` on your list of tasks
    (e.g., if you take Examples [2-8](ch02.xhtml#wc_freq) and [2-9](ch02.xhtml#wc_func)
    and visualize both the tasks together, you’ll see the shared node in [Figure 3-2](#vis_ex)).
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes you can’t submit the computations together, but you still know that
    you want to reuse some data. In those cases you should explore persistence.
  prefs: []
  type: TYPE_NORMAL
- en: Persist, Caching, and Memoization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Persistence allows you to keep specified Dask collections in memory on the cluster.
    To persist a collection for future reuse, just call `dask.persist` on the collection.
    If you choose persistence, you will be responsible for telling Dask when you are
    done with a distributed collection. Unlike Spark, Dask does not have an easy `unpersist`
    equivalent; instead, you need to release the underlying futures for each partition
    as shown in [Example 3-8](#manual_persist).
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-8\. Manual persistence and memory management with Dask
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A common mistake is to persist and cache things that are used only once or are
    inexpensive to compute.
  prefs: []
  type: TYPE_NORMAL
- en: Dask’s local mode has a best-effort caching system based on cachey. Since this
    only works in local mode, we won’t go into the details, but if you are running
    in local mode, you can take a look at [the local cache documentation](https://oreil.ly/VFSVQ).
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Dask does not raise an error when you attempt to use Dask caching in a distributed
    fashion; it just won’t work. So when migrating code from local to distributed,
    make sure to check for usage of Dask’s local caching.
  prefs: []
  type: TYPE_NORMAL
- en: Fault Tolerance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In distributed systems like Dask, *fault tolerance* generally refers to how
    a system handles computer, network, or program failures. Fault tolerance becomes
    increasingly important the more computers you use. When you are using Dask on
    a single computer, the concept of fault tolerance is less important, since if
    your one computer fails, there is nothing to recover. However, when you have hundreds
    of machines, the odds of a machine failing go up. Dask’s task graph is used to
    provide its fault tolerance.^([11](ch03.xhtml#id465)) There are many different
    kinds of failures in a distributed system, but thankfully many of them can be
    handled in the same way.
  prefs: []
  type: TYPE_NORMAL
- en: Dask automatically retries tasks when the scheduler loses connection to the
    worker. This retry is accomplished by using the same graph of computation Dask
    uses for lazy evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The Dask client is *not* fault tolerant to network issues connecting to the
    scheduler. One mitigation technique you can use is to run your client in the same
    network as the scheduler.
  prefs: []
  type: TYPE_NORMAL
- en: Machine failure is a fact of life with distributed systems. When a worker fails,
    Dask will treat it in the same way as a network failure, retrying any necessary
    tasks. However, Dask cannot recover from failures of the scheduler of your client
    code.^([12](ch03.xhtml#id466)) This makes it important that, when you are running
    in a shared environment, you run your client and scheduler nodes at a high priority
    to avoid preemption.
  prefs: []
  type: TYPE_NORMAL
- en: Dask automatically retries software failures that exit or crash the worker.
    Much like machine failure, from Dask’s point of view a worker exiting and a network
    failing look the same.
  prefs: []
  type: TYPE_NORMAL
- en: IOError and OSError exceptions are the only two classes of exceptions Dask will
    retry. If your worker process raises one of these errors, the exception is pickled
    and transferred over to the scheduler. Dask’s scheduler then retries the task.
    If your code encounters an IOError that should not be retried (e.g., a web page
    doesn’t exist), you’ll need to wrap it in another exception to keep Dask from
    retrying it.
  prefs: []
  type: TYPE_NORMAL
- en: Since Dask retries failed computation, it’s important to be careful with side
    effects or changing values. For example, if you have a Dask bag of transactions
    and were to update a database as part of a `map`, Dask might re-execute some of
    the operations on that bag multiple times, resulting in the update to the database
    happening more than once. If we think of a withdrawal from an ATM, we can see
    how this would result in some unhappy customers and incorrect data. Instead, if
    you need to mutate small bits of data, you can bring them back to a local collection.
  prefs: []
  type: TYPE_NORMAL
- en: If your program encounters any other exceptions, Dask will return the exception
    to your main thread.^([13](ch03.xhtml#id472))
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After this chapter you should have a good grasp of how Dask is able to scale
    your Python code. You should now understand the basics of partitioning, why this
    matters, task sizing, and Dask’s approach to fault tolerance. This will hopefully
    set you up well for deciding when to apply Dask, and for the next few chapters,
    where we do a deeper dive into Dask’s collection libraries. In the next chapter
    we’ll focus on Dask’s DataFrames, as they are the most full-featured of Dask’s
    distributed collections.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch03.xhtml#id374-marker)) Unless you work for a cloud provider and computers
    are close to free. If you do work for a cloud provider, please send us cloud credits.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch03.xhtml#id375-marker)) Including OS X and Linux.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch03.xhtml#id376-marker)) This also involves having to store a second
    copy of any object that is in the driver thread and then used in a worker. Since
    Dask shards its collections, this doesn’t generally blow up as quickly as it would
    with normal multi-processing.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch03.xhtml#id383-marker)) Say that five times fast.
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch03.xhtml#id386-marker)) As in many real-world situations, it’s easier
    to grow and harder to shrink the number of Dask nodes
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch03.xhtml#id400-marker)) Automatically picking up and shipping the libraries
    would be very hard and also slow, although it can be done under certain circumstances.
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch03.xhtml#id416-marker)) See Dask distributed GitHub issues [5561](https://oreil.ly/DmFxp)
    and [2953](https://oreil.ly/RxzGS).
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch03.xhtml#id430-marker)) Coming from databases, you can think of this
    as a “full-scan” or “full-shuffle” for Spark folks with `groupBy`.
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch03.xhtml#id439-marker)) When Dask can optimize evaluation here is complicated,
    but remember that a task is the fundamental unit of computation and Dask cannot
    break down compute any further inside the task. So a DataFrame created from many
    individual tasks that you call `head` on is a great candidate for Dask to optimize,
    but for a single task making a large DataFrame, Dask is unable to break “inside.”
  prefs: []
  type: TYPE_NORMAL
- en: ^([10](ch03.xhtml#id454-marker)) You could also collect and scatter if the dataset
    is small enough.
  prefs: []
  type: TYPE_NORMAL
- en: ^([11](ch03.xhtml#id465-marker)) The same technique used in Spark.
  prefs: []
  type: TYPE_NORMAL
- en: ^([12](ch03.xhtml#id466-marker)) This is common for most systems like this.
    Spark does have limited ability to recover from head node failure, but it has
    many restrictions and is not frequently used.
  prefs: []
  type: TYPE_NORMAL
- en: ^([13](ch03.xhtml#id472-marker)) For those migrating from Spark, this retry
    behavior is different. Spark will retry most exceptions, whereas Dask will only
    retry errors resulting in a worker exiting, or an IOError or OSError.
  prefs: []
  type: TYPE_NORMAL
