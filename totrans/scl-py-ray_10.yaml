- en: Chapter 9\. Advanced Data with Ray
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Despite, or perhaps because of, data ecosystems’ rapid advances, you will likely
    end up needing to use multiple tools as part of your data pipeline. Ray Datasets
    allows data sharing among tools in the data and ML ecosystems. This allows you
    to switch tools without having to copy or move data. Ray Datasets supports Spark,
    Modin, Dask, and Mars and can also be used with ML tools like TensorFlow. You
    can also use Arrow with Ray to allow more tools to work on top of Datasets, such
    as R or even MATLAB. Ray Datasets act as a common format for all steps of your
    ML pipeline, simplifying legacy pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'It all boils down to this: you can use the same dataset in multiple tools without
    worrying about the details. Internally, many of these tools have their own formats,
    but Ray and Arrow manage the translations transparently.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition to simplifying your use of different tools, Ray also has a growing
    collection of built-in operations for Datasets. These built-in operations are
    being actively developed and are not intended to be as full-featured as those
    of the data tools built on top of Ray.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As covered in [“Ray Objects”](ch05.html#ray_objects), Ray Datasets’ default
    behavior may be different than you expect. You can enable object recovery by setting
    `enable_object_reconstruction=True` in `ray.init` to make Ray Datasets more resilient.
  prefs: []
  type: TYPE_NORMAL
- en: Ray Datasets continues to be an area of active development, including large
    feature additions between minor releases, and more functionality likely will be
    added by the time you are reading this chapter. Regardless, the fundamental principles
    of partitioning and multitool interoperability will remain the same.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and Saving Ray Datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you saw in [Example 2-9](ch02.html#ds_hello), you can create datasets from
    local collections by calling `ray.data.from_items`. However, local collections
    naturally limit the scope of data that you can handle, so Ray supports many other
    options.
  prefs: []
  type: TYPE_NORMAL
- en: Ray uses [Arrow](https://oreil.ly/GY0at) to load external data into Datasets,
    which support multiple file formats and filesystems. The formats, at present,
    are CSV, JSON, Parquet, NumPy, text, and raw binary. The functions for loading
    data follow the `read_[*format*]` pattern and are in the `ray.data` module, as
    shown in [Example 9-1](#load_csv_local_fs).
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-1\. [Loading local data](https://oreil.ly/HP05n)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: When loading, you can specify a target `parallelism`, but Ray may be limited
    by the number of files being loaded. Picking a good value for your target parallelism
    is complicated and depends on numerous factors. You want to ensure that your data
    can fit easily in memory and take advantage of all of the machines in your cluster,
    while also not picking a number so high that the overhead of launching individual
    tasks exceeds the benefits. Generally, parallelism resulting in splits between
    hundreds of megabytes to tens of gigabytes is often considered a sweet spot.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you wish to customize the way Arrow loads your data, you can pass additional
    arguments, like `compression` or `buffer_size`, to Arrow through the `arrow_open_stream_args`
    parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Arrow has built-in native (fast) support for S3, HDFS, and regular filesystems.
    Ray automatically selects the correct built-in filesystem driver based on the
    path.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When loading from a local filesystem, it is up to you to ensure that the file
    is available on all of the workers when running in distributed mode.
  prefs: []
  type: TYPE_NORMAL
- en: Arrow, and by extension Ray, also uses [`fsspec`](https://oreil.ly/Tz32F), which
    supports a wider array of filesystems, including HTTPS (when aiohttp is installed).
    Unlike with the “built-in” filesystems, you need to manually specify the filesystem,
    as shown in [Example 9-2](#ex_load_from_https).
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-2\. [Loading data over HTTPS](https://oreil.ly/HP05n)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: At present, the protocol is incorrectly stripped off, so you need to put it
    in twice. For example, when loading data from an HTTPS website, you would load
    from `https://https://[someurlhere].com`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ray has the ability to write in all the formats it can read from. The writing
    functions, like the reading functions, follow a pattern of `write_[*format*]`.
    A few minor differences exist between the read path and the write path. Instead
    of taking in a `parallelism` parameter, the write path always writes with the
    parallelism of the input dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If Ray does not have I/O support for your desired format or filesystem, you
    should check to see whether any of the other tools that Ray supports does. Then,
    as covered in the next section, you can convert your dataset from/to the desired
    tool.
  prefs: []
  type: TYPE_NORMAL
- en: Using Ray Datasets with Different Tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ray has built-in tooling to share data among the various data tools running
    on Ray. Most of these tools have their own internal representations of the data,
    but Ray handles converting the data as needed. Before you first use a dataset
    with Spark or Dask, you need to run a bit of setup code so that they delegate
    their execution to Ray, as shown in Examples [9-3](#ex_setup_dask) and [9-4](#ex_setup_spark).
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-3\. [Setting up Dask on Ray](https://oreil.ly/HP05n)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Example 9-4\. [Setting up Dask on Spark](https://oreil.ly/HP05n)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As with functions for reading and loading datasets, transfer-to-Ray functions
    are defined on the `ray.data` module and follow the `from_[*x*]` pattern, where
    `[*x*]` is the tool name. Similar to writing data, we convert datasets to a tool
    with a `to_[*x*]` function defined on the dataset, where `[*x*]` is the tool name.
    [Example 9-5](#raydataset0905) shows how to use this pattern to convert a Ray
    dataset into a Dask DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Datasets do not use Ray’s runtime environments for dependencies, so you must
    have your desired tools installed in your worker image; see [Appendix B](app02.html#appB).
    This is more involved for Spark, as it requires the Java Virtual Machine (JVM)
    and other non-Python components.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-5\. [Ray dataset in Dask](https://oreil.ly/HP05n)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: You are not limited to the tools that are built into Ray. If you have a new
    tool that supports Arrow, and you are using [Arrow-supported types](https://oreil.ly/qNbFA),
    `to_arrow_refs` gives you a zero-copy Arrow representation of your dataset. You
    can then use this list of Ray Arrow objects to pass into your tool, whether for
    model training or any other purpose. You will learn more about this in [“Using
    Built-in Ray Dataset Operations”](#builtinRayDSops).
  prefs: []
  type: TYPE_NORMAL
- en: 'Many tools and languages can be connected with Arrow and Ray, including:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Apache Spark](https://oreil.ly/o2m4s)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Dask](https://oreil.ly/tqbJY)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Apache Parquet](https://oreil.ly/Mj0N8)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Modin](https://oreil.ly/uSMt5)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[pandas](https://oreil.ly/oX5FO)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[TensorFlow](https://oreil.ly/7sweI)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[R](https://oreil.ly/41btG)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[JSON](https://oreil.ly/shYH5)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[MATLAB](https://oreil.ly/tqqR2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Dask and Spark both have non-DataFrame collections—bags, arrays, and resilient
    distributed datasets (RDDs)—that cannot be converted with these APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Using Tools on Ray Datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section assumes you have a good understanding of the data-wrangling tools
    you’re going to use with Ray—either pandas or Spark. Pandas is ideal for users
    scaling Python analytics, and Spark is well suited for users connecting to big
    data tools. If you’re not familiar with the pandas APIs, you should check out
    [*Python for Data Analysis*](https://oreil.ly/IoSNu) by Wes McKinney (O’Reilly).
    New Spark users should check out [*Learning Spark*](https://oreil.ly/wmypt) by
    Jules Damji et al. (O’Reilly). If you want to go super deep, Holden recommends
    [*High Performance Spark*](https://oreil.ly/lyZ99) by Holden and Rachel Warren
    (O’Reilly).^([1](ch09.html#idm45354767432992))
  prefs: []
  type: TYPE_NORMAL
- en: pandas-like DataFrames with Dask
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Dask](https://oreil.ly/ylNqR) on Ray is an excellent choice for data preparation
    for ML, or scaling existing pandas code. Many initial Dask developers also worked
    on pandas, leading to a comparatively solid distributed pandas interface.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Portions of this section are based on the DataFrame chapter in [*Scaling Python
    with Dask*](https://oreil.ly/Fk0I6).
  prefs: []
  type: TYPE_NORMAL
- en: Dask on Ray benefits from using Ray’s per node/container shared memory storage
    for data. This is especially important when doing operations like broadcast joins;
    in Dask the same data will need to be stored in each worker process.^([2](ch09.html#idm45354767410480))
    However, in Ray, it needs to be stored only once per node or container.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Unlike Ray, Dask is generally lazy, meaning it does not evaluate data until
    forced. This can make debugging a challenge as errors may appear several lines
    removed from their root cause.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the distributed components of Dask’s DataFrames use the three core building
    blocks `map_partitions`, `reduction`, and `rolling`. You mostly won’t need to
    call these functions directly; instead, you will use higher-level APIs, but understanding
    them and how they work is important to understanding how Dask works. `shuffle`
    is a critical building block of distributed DataFrames for reorganizing your data.
    Unlike the other building blocks, you may use it directly more frequently as Dask
    is unable to abstract away partitioning.
  prefs: []
  type: TYPE_NORMAL
- en: Indexing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Indexing into a DataFrame is one of the powerful features of pandas, but comes
    with some restrictions when moving into a distributed system like Dask. Since
    Dask does not, by default, track the size of each partition, positional indexing
    by row is not supported. You can use positional indexing into columns, as well
    as label indexing for columns or rows.
  prefs: []
  type: TYPE_NORMAL
- en: Indexing is frequently used to filter data to have only the components you need.
    We did this for San Francisco COVID-19 data by looking at just the case rates
    for people of all vaccine statuses, as shown in [Example 9-6](#index_covid_data).
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-6\. [Dask DataFrame indexing](https://oreil.ly/IJaQ2)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: If you truly need positional indexing by row, you can implement your own by
    computing the size of each partition and using this to select the desired partition
    subsets. This is very inefficient, so Dask avoids implementing directly so you
    make an intentional choice before doing this.
  prefs: []
  type: TYPE_NORMAL
- en: Shuffles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned in the previous chapter, shuffles are expensive. The primary causes
    of the expensive nature of shuffles are the comparative slowness of network speed
    (relative to to reading data from memory) and serialization overhead. These costs
    scale as the amount of data being shuffled increases, so Dask has techniques to
    reduce the amount of data being shuffled. These techniques depend on certain data
    properties or on the operation being performed.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: While understanding shuffles is important for performance, feel free to skip
    this section if your code is working well enough.
  prefs: []
  type: TYPE_NORMAL
- en: Rolling windows and map_overlap
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One situation that can trigger the need for a shuffle is a rolling window, where
    at the edges of a partition your function needs some records from its neighbors.
    A Dask DataFrame has a special `map_overlap` function in which you can specify
    a *look-after* window (also called a *look-ahead* window) and a *look-before*
    window (also called a *look-back* window) of rows to transfer (either an integer
    or a time delta). The simplest example taking advantage of this is a rolling average,
    shown in [Example 9-7](#rolling_date_ex).
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-7\. [Dask DataFrame rolling average](https://oreil.ly/IJaQ2)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Using `map_overlap` allows Dask to transfer only the data needed. For this implementation
    to work correctly, your minimum partition size needs to be larger than your largest
    window.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Dask’s rolling windows will not cross multiple partitions. If your DataFrame
    is partitioned in such a way that the look-after or look-back is greater than
    the length of the neighbor’s partition, the results will either fail or be incorrect.
    Dask validates this for timedelta look-afters, but no such checks are performed
    for look-backs or integer look-afters.
  prefs: []
  type: TYPE_NORMAL
- en: Aggregations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Aggregations are another special case that can reduce the amount of data that
    needs to be transferred over the network. Aggregations are functions that combine
    records. If you are coming from a map/reduce or Spark background, `reduceByKey`
    is the classic aggregation. Aggregations can either be *by key* or global across
    an entire DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: To aggregate by key, you first need to call `groupby` with the column(s) representing
    the key, or the keying function to aggregate on. For example, calling `df.groupby("PostCode")`
    groups your DataFrame by postal code, or calling `df.groupby(["PostCode", "SicCodes"])`
    uses a combination of columns for grouping. Function-wise, many of the same pandas
    aggregates are available, but the performance of aggregates in Dask are very different
    than with local pandas DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you’re aggregating by partition key, Dask can compute the aggregation without
    needing a shuffle.
  prefs: []
  type: TYPE_NORMAL
- en: The first way to speed up your aggregations is to reduce the columns that you
    are aggregating on, since the fastest data to process is no data. Finally, when
    possible, doing multiple aggregations at the same time reduces the number of times
    the same data needs to be shuffled. Therefore, you need to compute the average
    and the max, you should compute both at the same time, as shown in [Example 9-8](#max_mean).
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-8\. [Computing a Dask DataFrame max and mean together](https://oreil.ly/IJaQ2)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: For distributed systems like Dask, if an aggregation can be partially evaluated
    and then merged, you can potentially combine some records pre-shuffle. Not all
    partial aggregations are created equal. What matters with partial aggregations
    is the amount of data reduced when merging values with the same key, as compared
    to the storage space used by the original multiple values.
  prefs: []
  type: TYPE_NORMAL
- en: The most efficient aggregations take a sublinear amount of space regardless
    of the number of records. Some of these can take constant space such as sum, count,
    first, minimum, maximum, mean, and standard deviation. More complicated tasks,
    like quantiles and distinct counts, also have sublinear approximation options.
    These approximation options can be great, as exact answers can require linear
    growth in storage.
  prefs: []
  type: TYPE_NORMAL
- en: Some aggregation functions are not sublinear in growth, but “tend to” or “might”
    not grow too quickly. Counting the distinct values is in this group, but if all
    your values are unique, there is no space-saving.
  prefs: []
  type: TYPE_NORMAL
- en: To take advantage of efficient aggregations, you need to use a built-in aggregation
    from Dask, or write your own using Dask’s aggregation class. Whenever you can,
    use a built-in. Built-ins not only require less effort but also are often faster.
    Not all of the pandas aggregates are directly supported in Dask, so sometimes
    your only choice is to write your own aggregate.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you choose to write your own aggregate, you have three functions to define:
    `chunk` for handling each group-partition/chunk, `agg` to combine the results
    of `chunk` between partitions, and (optionally) `finalize` to take the result
    of `agg` and produce a final value.'
  prefs: []
  type: TYPE_NORMAL
- en: The fastest way to understand how to use partial aggregation is by looking at
    an example that uses all three functions. Using weighted average in [Example 9-9](#weight_avg)
    can help you think of what is needed for each function. The first function needs
    to compute the weighted values and the weights. The `agg` function combines these
    by summing each side part of the tuple. Finally, the `finalize` function divides
    the total by the weights.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-9\. [Dask custom aggregate](https://oreil.ly/IJaQ2)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In some cases, like a pure summation, you don’t need to do any post-processing
    on `agg`’s output, so you can skip the `finalize` function.
  prefs: []
  type: TYPE_NORMAL
- en: Not all aggregations must be by key; you can also compute aggregations across
    all rows. Dask’s custom aggregation interface, however, is only exposed with by-key
    operations.
  prefs: []
  type: TYPE_NORMAL
- en: Dask’s built-in full DataFrame aggregations use a lower-level interface called
    `apply_contact_apply`, for partial aggregations. Rather than learn two different
    APIs for partial aggregations, we prefer to do a static `groupby` by providing
    a constant grouping function. This way, we have to know only one interface for
    aggregations. You can use this to find the aggregate COVID-19 numbers across the
    DataFrame, as shown in [Example 9-10](#agg_entire).
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-10\. [Aggregating the entire DataFrame](https://oreil.ly/IJaQ2)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'If built-in aggregations exist, they will likely be better than anything we
    may be able to write. Sometimes a partial aggregation is partially implemented,
    as in the case of Dask’s HyperLogLog: it is implemented only for full DataFrames.
    You can often translate simple aggregations by using `apply_contact_apply` or
    `aca` by copying the `chunk` function, using the `combine` parameter for `agg`,
    and using the `aggregate` parameter for `finalize`. This is shown via porting
    Dask’s HyperLogLog implementation in [Example 9-11](#custom_agg_hyperloglog).'
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-11\. [Wrapping Dask’s HyperLogLog in `dd.Aggregation`](https://oreil.ly/IJaQ2)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Slow/inefficient aggregations (or those likely to cause an out-of-memory exception)
    use storage proportional to the records being aggregated. Examples from this slow
    group include making a list and naively computing exact quantiles.^([3](ch09.html#idm45354766947536))
    With these slow aggregates, using Dask’s aggregation class has no benefit over
    the `apply` API, which you may wish to use for simplicity. For example, if you
    just wanted a list of employer IDs by postal code, rather than having to write
    three functions, you could use a one-liner like `df.groupby("PostCode")["EmployerId"].apply(lambda
    g: list(g))`. Dask implements the `apply` function as a full shuffle, which is
    covered in the next section.'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Dask is unable to apply partial aggregations when you use the `apply` function.
  prefs: []
  type: TYPE_NORMAL
- en: Full shuffles and partitioning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sorting is inherently expensive in distributed systems because it most often
    requires a *full shuffle*. Full shuffles are sometimes an unavoidable part of
    working in Dask. Counterintuitively, while full shuffles are themselves slow,
    you can use them to speed up future operations that are all happening on the same
    grouping key(s). As mentioned in the aggregation section, one of the ways a full
    shuffle is triggered is by using the `apply` method when partitioning is not aligned.
  prefs: []
  type: TYPE_NORMAL
- en: Partitioning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You will most commonly use full shuffles to repartition your data. It’s important
    to have the right partitioning when dealing with aggregations, rolling windows,
    or look-ups/indexing. As discussed in [“Rolling windows and map_overlap”](#rollingWindowsMapOL),
    Dask cannot do more than one partition’s worth of look-ahead or look-behind, so
    having the right partitioning is required to get correct results. For most other
    operations, having incorrect partitioning will slow down your job.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dask has three primary methods for controlling the partitioning of a DataFrame:
    `set_index`, `repartition`, and `shuffle`. You use `set_index` when the partitioning
    is being changed to a new key/index. `repartition` keeps the same key/index but
    changes the splits. `repartition` and `set_index` take similar parameters, with
    `repartition` not taking an index key name. `shuffle` is a bit different since
    it does not produce a “known” partitioning scheme that operations like `groupby`
    can take advantage of.'
  prefs: []
  type: TYPE_NORMAL
- en: The first step of getting the right partitioning for your DataFrame is to decide
    whether you want an index. Indexes are useful for pretty much any by-key type
    of operation, including filtering data, grouping, and, of course, indexing. One
    such by-key operation would be a `groupby`; the column being grouped on could
    be a good candidate for the key. If you use a rolling window over a column, that
    column must be the key, which makes choosing the key relatively easy. Once you’ve
    decided on an index, you can call `set_index` with the column name of the index
    (e.g., `set_index("PostCode")`). This will, under most circumstances, result in
    a shuffle, so it’s a good time to size your partitions.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you’re unsure of the current key used for partitioning, you can check the
    `index` property to see the partitioning key.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you’ve chosen your key, the next question is how to size your partitions.
    The advice in [“Partitioning”](#basic_partitioning) generally applies here: shoot
    for enough partitions to keep each machine busy, but keep in mind the general
    sweet spot of 100 MB to 1 GB. Dask generally computes pretty even splits if you
    give it a target number of partitions.^([4](ch09.html#idm45354766886064)) Thankfully,
    `set_index` will also take `npartitions`. To repartition the data by postal code,
    with 10 partitions, you would add `set_index("PostCode", npartitions=10)`; otherwise,
    Dask will default to the number of input partitions.'
  prefs: []
  type: TYPE_NORMAL
- en: If you plan to use rolling windows, you will likely need to ensure that you
    have the right size (in terms of key range) covered in each partition. To do this
    as part of `set_index`, you would need to compute your own divisions to ensure
    that each partition has the right range of records present. Divisions are specified
    as a list starting from the minimal value of the first partition up to the maximum
    value of the last partition. Each value in between is a “cut” point for between
    the pandas DataFrames that make up the Dask DataFrame. To make a DataFrame with
    `[0, 100) [100, 200), (300, 500]`, you would write `df.set_index("Num​Employees",
    divisions=[0, 100, 200, 300, 500])`. Similarly for the date range, to support
    a rolling window of up to seven days, from the start of the pandemic to this writing,
    is shown in [Example 9-12](#set_index_with_rolling_window).
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-12\. [Dask DataFrame rolling window with `set_index`](https://oreil.ly/IJaQ2)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Dask, including for rolling time windows, assumes that your partition index
    is *monotonically increasing*—strictly increasing with no repeated values (e.g.,
    1, 4, 7 is monotically increasing, but 1, 4, 4, 7 is not).
  prefs: []
  type: TYPE_NORMAL
- en: So far, you’ve had to specify the number of partitions, or the specific divisions,
    but you might be wondering if Dask can just figure this out itself. Thankfully,
    Dask’s repartition function has the ability to pick divisions from a target size.
    However, doing this is a nontrivial cost as Dask must evaluate the DataFrame as
    well as the repartition itself. [Example 9-13](#repartition_ex) shows how to have
    Dask calculate the divisions from a desired partition size in bytes.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-13\. [Dask DataFrame automatic partitioning](https://oreil.ly/IJaQ2)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Dask’s `set_index` has a similar `partition_size` parameter, but as of the writing,
    it does not work.
  prefs: []
  type: TYPE_NORMAL
- en: As you’ve seen at the start of this chapter when writing DataFrames, each partition
    is given its own file, but sometimes this can result in files that are too big
    or too small. Some tools can accept only one file as input, so you need to repartition
    everything into a single partition. Other times, the data storage system is optimized
    for certain file sizes, like the Hadoop Distributed File System (HDFS) default
    block size of 128 MB. The good news is you can use `repartition` or `set_index`
    to get your desired output structure.
  prefs: []
  type: TYPE_NORMAL
- en: Embarrassingly Parallel Operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dask’s `map_partitions` function applies a function to each of the partitions
    underlying pandas DataFrames, and the result is also a pandas DataFrame. Functions
    implemented with `map_partitions` are embarrassingly parallel since they don’t
    require any inter-worker transfer of data. In [embarrassingly parallel problems](https://oreil.ly/NFYHB),
    the overhead of distributed computing and communication is low.
  prefs: []
  type: TYPE_NORMAL
- en: '`map_partitions` implements `map`, and many row-wise operations. If you want
    to use a row-wise operation that you find missing, you can implement it yourself,
    as shown in [Example 9-14](#filna_ex).'
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-14\. [Dask DataFrame `fillna`](https://oreil.ly/IJaQ2)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: You aren’t limited to calling pandas built-ins as in this example. Provided
    that your function takes and returns a DataFrame, you can do pretty much anything
    you want inside `map_partitions`.
  prefs: []
  type: TYPE_NORMAL
- en: The full pandas API is too long to cover in this chapter, but if a function
    can operate on a row-by-row basis without any knowledge of the rows before or
    after, it may already be implemented in Dask DataFrames using `map_partitions`.
    If not, you can also implement it yourself using the pattern from [Example 9-14](#filna_ex).
  prefs: []
  type: TYPE_NORMAL
- en: When using `map_partitions` on a DataFrame, you can change anything about each
    row, including the key that it is partitioned on. If you *are* changing the values
    in the partition key, you *must* either clear the partitioning information on
    the resulting DataFrame with `clear_divisions` *or* specify the correct indexing
    with `set_index`, which you’ll learn more about in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Incorrect partitioning information can result in incorrect results, not just
    exceptions, as Dask may miss relevant data.
  prefs: []
  type: TYPE_NORMAL
- en: Working with Multiple DataFrames
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: pandas and Dask have four common functions for combining DataFrames. At the
    root is the `concat` function, which allows joining DataFrames on any axis. Concatenating
    DataFrames is generally slower in Dask since it involves inter-worker communication.
    The other three functions are `join`, `merge`, and `append`, which all implement
    special cases for common situations on top of `concat`, and have slightly different
    performance considerations. Having good divisions/partitioning, in terms of key
    selection and number of partitions, makes a huge difference when working on multiple
    DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: Dask’s `join` and `merge` functions take most of the standard pandas arguments
    along with an extra, optional, one. `npartitions` specifies a target number of
    output partitions, but is used for only hash-based joins (which you’ll learn about
    in [“Multi-DataFrame internals”](#multiDFI)). Both `join` and `merge` automatically
    repartition your input DataFrames if needed. This is great, as you might not know
    the partitioning, but since repartitioning can be slow, explicitly using the lower-level
    `concat` function when you don’t expect any partitioning changes to be needed
    can help catch performance problems early. Dask’s join takes only more than two
    DataFrames at a time when doing a left or outer join type.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Dask has special logic to speed up multi-DataFrame joins, so in most cases,
    rather than do `a.join(b).join(c).join(d).join(e)`, you will benefit from doing
    `a.join([b, c, d, e])`. However, if you are performing a left join with a small
    dataset, the first syntax may be more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: When you combine (via `concat`) DataFrames by row (similar to a SQL UNION) the
    performance depends on whether divisions of the DataFrames being combined are
    well ordered. We call the divisions of a series of DataFrames *well ordered* if
    all the divisions are known, and the highest division of the previous DataFrame
    is below that of the lowest division of the next. If any input has an unknown
    division, Dask will produce an output without known partitioning. With all known
    partitions, Dask treats row-based concatenations as a metadata-only change and
    will not perform any shuffle. This requires that no overlap between the divisions
    exists. In addition, an extra `interleave_partitions` parameter will change the
    join type for row-based combinations to one without the input partitioning restriction
    and will result in a known partitioner.
  prefs: []
  type: TYPE_NORMAL
- en: Dask’s column-based `concat` (similar to a SQL JOIN) also has restrictions around
    the divisions/partitions of the DataFrames it is combining. Dask’s version of
    concat supports only inner or full outer joins, not left or right. Column-based
    joins require that all inputs have known partitioners and also result in a DataFrame
    with known partitioning. Having a known partitioner can be useful for subsequent
    joins.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Don’t use Dask’s `concat` when operating by row on a DataFrame with unknown
    divisions, as it will likely return incorrect results. Dask assumes indices are
    aligned no indices are present.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-DataFrame internals
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dask uses four techniques—hash, broadcast, partitioned, and `stack_partitions`—to
    combine DataFrames, and each results in very different performance. Dask chooses
    the technique based on the indexes, divisions, and requested join type (e.g.,
    outer/left/inner). The three column-based join techniques are hash joins, broadcast,
    and partitioned joins. When doing row-based combinations (e.g., `append`), Dask
    has a special technique called `stack_partitions` that is extra fast. It’s important
    that you understand the performance of each of these techniques and the conditions
    that will cause Dask to pick which approach.
  prefs: []
  type: TYPE_NORMAL
- en: '*Hash* joins are the default that Dask uses when no other join technique is
    suit­able. Hash joins shuffle the data for all the input DataFrames to partition
    on the target key. Hash joins use the hash values of keys, which results in a
    DataFrame that is not in any particular order. As such, the result of hash joins
    do not have any known divisions.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Broadcast* joins are ideal for joining large DataFrames with small DataFrames.
    In a broadcast join, Dask takes the smaller DataFrame and distributes it to all
    of the workers. This means that the smaller DataFrame must be able to fit in memory.
    To tell Dask that a DataFrame is a good candidate for broadcasting, you make sure
    it is all stored in one partition—for example, call `repartition(npartitions=1)`.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Partitioned* joins happen when combining DataFrames along an index where the
    partitions are known for all the DataFrames. Since the input partitions are known,
    Dask is able to align the partitions between the DataFrames, involving less data
    transfer as each output partition has a smaller than full set of inputs.'
  prefs: []
  type: TYPE_NORMAL
- en: Since partition and broadcast joins are faster, doing some work to help Dask
    can be worthwhile. For example, concatenating several DataFrames with known and
    aligned partitions, and one DataFrame which is unaligned, will result in an expensive
    hash join. Instead, try to either set the index and partition on the remaining
    DataFrame, or join the less expensive DataFrames first and then perform the expensive
    join after.
  prefs: []
  type: TYPE_NORMAL
- en: Using `stack_partitions` is different from all of the other options since it
    doesn’t involve any movement of data. Instead, the resulting DataFrame partitions
    list is a union of the upstream partitions from the input DataFrames. Dask uses
    `stack​_partiti⁠ons` for most row-based combinations except when all of the input
    DataFrame divisions are known and they are not well ordered and you ask Dask to
    `interleave_partitions`. The `stack_partitions` function is able to provide only
    known partitioning in its output when the input divisions are known and well ordered.
    If all the divisions are known but not well ordered, and you set `interleave_partitions`,
    Dask will use a partitioned join instead. While this approach is compa⁠r­atively
    inexpensive, it is not free and can result in an excessively large number of partitions
    requiring you to repartition anyway.
  prefs: []
  type: TYPE_NORMAL
- en: Missing functionality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Not all multi-DataFrame operations are implemented; `compare` is one such operation,
    which leads into the next section about the limitations of Dask DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: What Does Not Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dask’s DataFrame implements most, but not all, of the pandas DataFrame API.
    Some of the pandas API is not implemented in Dask because of the development time
    involved. Other parts are not used in order to avoid exposing an API that would
    be unexpectedly slow.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes the API is just missing small parts, as both pandas and Dask are under
    active development. An example is the `split` function. In local pandas, instead
    of doing `split().explode`, you could have called `split(expand=true)`. Some of
    these can be excellent places to get involved and [contribute to the Dask project](https://oreil.ly/OHPqQ)
    if you are interested.
  prefs: []
  type: TYPE_NORMAL
- en: Some libraries do not parallelize as well as others. In these cases, a common
    approach is to try to filter or aggregate the data down enough that it can be
    represented locally and then apply the local libraries to the data. For example,
    with graphing, it’s common to pre-aggregate the counts or take a random sample
    and graph the result.
  prefs: []
  type: TYPE_NORMAL
- en: While much of the pandas DataFrame API will work out of the box, before you
    swap in Dask DataFrame, it’s important to make sure you have good test coverage
    to catch the situations where it does not.
  prefs: []
  type: TYPE_NORMAL
- en: What’s Slower
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Usually, using Dask DataFrames will improve performance, but not always. Generally,
    smaller datasets will perform better in local pandas. As discussed, anything involving
    shuffles is generally slower in a distributed system than in a local one. Iterative
    algorithms can also produce large graphs of operations, which are slow to evaluate
    in Dask compared to traditional greedy evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Some problems are generally unsuitable for data-parallel computing. For example,
    writing out to a data store with a single lock that has more parallel writers
    will increase the lock contention and may make it slower than if a single thread
    was doing the writing. In these situations, you can sometimes repartition your
    data or write individual partitions to avoid lock contention.
  prefs: []
  type: TYPE_NORMAL
- en: Handling Recursive Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dask’s lazy evaluation, powered by its lineage graph, is normally beneficial,
    allowing it to combine steps automatically. However, when the graph gets too large,
    Dask can struggle to manage it, which often shows up as a slow driver process
    or notebook, and sometimes an out-of-memory exception. Thankfully, you can work
    around this by writing out your DataFrame and reading it back in. Generally, Parquet
    is the best format for doing this as it is space-efficient and self-describing,
    so no schema inference is required.
  prefs: []
  type: TYPE_NORMAL
- en: What Other Functions Are Different
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For performance reasons, various parts of Dask DataFrames behave a little differently
    from local DataFrames:'
  prefs: []
  type: TYPE_NORMAL
- en: '`reset_index`'
  prefs: []
  type: TYPE_NORMAL
- en: The index will start back over at zero on each partition.
  prefs: []
  type: TYPE_NORMAL
- en: '`kurtosis`'
  prefs: []
  type: TYPE_NORMAL
- en: Does not filter out not-a-number (NaN) values and uses SciPy defaults.
  prefs: []
  type: TYPE_NORMAL
- en: '`concat`'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of coercing category types, each category type is expanded to the union
    of all of the categories it is concatenated with.
  prefs: []
  type: TYPE_NORMAL
- en: '`sort_values`'
  prefs: []
  type: TYPE_NORMAL
- en: Dask supports only single-column sorts.
  prefs: []
  type: TYPE_NORMAL
- en: Joins
  prefs: []
  type: TYPE_NORMAL
- en: When joining more than two DataFrames at the same time, the join type must be
    either outer or left.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you are interested in going deeper with Dask, several Dask-focused books
    are in active development. Much of the material in this chapter is based on [*Scaling
    Python with Dask*](https://oreil.ly/Fk0I6).
  prefs: []
  type: TYPE_NORMAL
- en: pandas-like DataFrames with Modin
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Modin](https://oreil.ly/KR1wT), like Dask DataFrames, is designed to largely
    be a plug-in replacement for pandas DataFrames. Modin DataFrames follow the same
    general performance as Dask DataFrames, with a few caveats. Modin offers less
    control over internals, which can limit performance for some applications. Since
    Modin and Dask DataFrames are sufficiently similar, we won’t cover it here except
    to say it’s another option if Dask doesn’t meet your needs.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*Modin* is a new library designed to accelerate pandas by automatically distributing
    the computation across all of the system’s available CPU cores. Modin claims to
    be able to get nearly linear speedup to the number of CPU cores on your system
    for pandas DataFrames of any size.'
  prefs: []
  type: TYPE_NORMAL
- en: Since Modin on Ray is so similar to Dask DataFrames, we’ve decided to skip repeating
    the examples from Dask on Ray as they would not change substantially.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When you look at Dask and Modin’s documentation side by side, you may get the
    impression that Dask is earlier in its development cycle. In our opinion, this
    is not the case; rather, the Dask documentation takes a more conservative approach
    to marking features as ready.
  prefs: []
  type: TYPE_NORMAL
- en: Big Data with Spark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you’re working with an existing big data infrastructure (such as Apache Hive,
    Iceberg, or HBase), Spark is an excellent choice. Spark has optimizations like
    filter push-down, which can dramatically improve performance. Spark has a more
    traditional, big data DataFrame interface.
  prefs: []
  type: TYPE_NORMAL
- en: Spark’s strong suit is in the data ecosystem of which it is a part. As a Java-based
    tool, with a Python API, Spark plugs into much of the traditional big data ecosystem.
    Spark supports the widest array of formats and filesystems, making it an excellent
    choice for the initial stages of many pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'While Spark continues to add more pandas-like functionality, its DataFrames
    started from more of a SQL-inspired design. You have several options to learn
    about Spark, including some O’Reilly books: [*Learning Spark*](https://oreil.ly/LearningSpark2)
    by Jules Damji, [*High Performance Spark*](https://oreil.ly/highperfSpark) by
    Holden and Rachel Warren, and [*Spark: The Definitive Guide*](https://oreil.ly/sparkTDG)
    by Bill Chambers and Matei Zaharia.'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Unlike Ray, Spark is generally lazy, meaning it does not evaluate data until
    forced. This can make debugging a challenge as errors may appear several lines
    removed from their root cause.
  prefs: []
  type: TYPE_NORMAL
- en: Working with Local Tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some tools are not well suited to distributed operation. Thankfully, provided
    your dataset is filtered down small enough, you can convert it into a variety
    of local in-process formats. If the entire dataset can fit in memory, `to_pandas`
    and `to_arrow` are the simplest ways to convert a dataset to a local object. For
    larger objects, where each partition may fit in memory but the entire dataset
    may not, `iter_batches` will give you a generator/iterator to consume one partition
    at a time. The `iter_batches` function takes a `batch_format` parameter to switch
    between `pandas` or `pyarrow`. If possible, `pyarrow` is generally more efficient
    than `pandas`.
  prefs: []
  type: TYPE_NORMAL
- en: Using Built-in Ray Dataset Operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to allowing you to move data among various tools, Ray also has some
    built-in operations. Ray Datasets does not attempt to match any particular existing
    API, but rather expose basic building blocks you can use when the existing libraries
    do not meet your needs.
  prefs: []
  type: TYPE_NORMAL
- en: Ray Datasets has support for basic data operations. Ray Datasets does not aim
    to expose a pandas-like API; rather, it focuses on providing basic primitives
    to build on top of. The Dataset API is functionally inspired, along with partition-oriented
    functions. Ray also recently added `groupBy`s and aggregates.
  prefs: []
  type: TYPE_NORMAL
- en: The core building block of most of the dataset operations is `map_batches`.
    By default, `map_batches` executes the function you provide across the blocks
    or batches that make up a dataset and uses the results to make a new dataset.
    The `map_batches` function is used to implement `filter`, `flat_map`, and `map`.
    You can see the flexibility of `map_batches` by looking at the word-count example
    rewritten to directly use `map_batches` as well as drop any word that shows up
    only once, as shown in [Example 9-15](#ray_wordcount_on_ds_filter_only_once_with_batches).
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-15\. [Ray word count with `map_batches`](https://oreil.ly/HP05n)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The `map_batches` function takes parameters to customize its behavior. For stateful
    operations, you can change the `compute` strategy to `actors` from its default
    `tasks`. The previous example uses the default format, which is Ray’s internal
    format, but you can also convert the data to `pandas` or `pyarrow`. You can see
    this in [Example 9-16](#batch_op_on_pandas) in which Ray converts the data to
    pandas for us.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-16\. [Using Ray `map_batches` with pandas to update a column](https://oreil.ly/HP05n)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The result you return must be a list, `pandas`, or `pyarrow`, and it does not
    need to match the same type that you take in.
  prefs: []
  type: TYPE_NORMAL
- en: Ray Datasets does not have a built-in way to specify additional libraries to
    be installed. You can use `map_batches` along with a task to accomplish this,
    as shown in [Example 9-17](#more_awesome_wordcount_with_batches), which installs
    extra libraries to parse the HTML.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-17\. [Using Ray `map_batches` with extra libraries](https://oreil.ly/HP05n)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: For operations needing shuffles, Ray has `GroupedDataset`, which behaves a bit
    differently. Unlike the rest of the Datasets API, `groupby` is lazily evaluated
    in Ray. The `groupby` function takes a column name or function, where records
    with the same value will be aggregated together. Once you have the `GroupedDataset`,
    you can then pass in multiple aggregates to the `aggregate` function. Ray’s `AggregateFn`
    class is conceptually similar to Dask’s `Aggregation` class except it operates
    by row. Since it operates by row, you need to provide an `init` function for when
    a new key value is found. Instead of `chunk` for each new chunk, you provide `accumulate`
    to add each new element. You still provide a method of combining the aggregators,
    called `merge` instead of `agg`, and both have the same optional `finalize`. To
    understand the differences, we rewrote the Dask weighted average example to Ray
    in [Example 9-18](#ray_basic_agg).
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-18\. [Ray weighted average aggregation](https://oreil.ly/HP05n)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Full dataset aggregation is implemented using `None` since all records then
    have the same key.
  prefs: []
  type: TYPE_NORMAL
- en: Ray’s parallelism control does not have the same flexibility as indexes in Dask
    or partitioning in Spark. You can control the target number of partitions—but
    not the way the data is spread out.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Ray does not currently take advantage of the concept of known partitioning to
    minimize shuffles.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Ray Datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ray datasets are built using the tools you have been working with in the previous
    chapters. Ray splits each dataset into many smaller components. These smaller
    components are called both *blocks* and *partitions* inside the Ray code. Each
    partition contains an Arrow dataset representing a slice of the entire Ray dataset.
    Since Arrow does not support all of the types from Ray, if you have unsupported
    types, each partition also contains a list of the unsupported types.
  prefs: []
  type: TYPE_NORMAL
- en: The data inside each dataset is stored in the standard Ray object store. Each
    partition is stored as a separate object, since Ray is not able to split up individual
    objects. This also means that you can use the underlying Ray objects as parameters
    to Ray remote functions and actors. The dataset contains references to these objects
    as well as schema information.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Since the dataset contains the schema information, loading a dataset blocks
    on the first partition so that the schema information can be determined. The remainder
    of the partitions are eagerly loaded, but in a nonblocking fashion like the rest
    of Ray’s operations.
  prefs: []
  type: TYPE_NORMAL
- en: In keeping with the rest of Ray, datasets are immutable. When you want to do
    an operation on a dataset, you apply a transformation, like `filter`, `join`,
    or `map`, and Ray returns a new dataset with the result.
  prefs: []
  type: TYPE_NORMAL
- en: Ray datasets can use either tasks (aka remote functions) or actors for processing
    transformations. Some libraries built on top of Ray datasets, like Modin, depend
    on using actor processing so they can implement certain ML tasks involving state.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ray’s transparent handling of moving data among tools makes it an excellent
    choice for building end-to-end ML pipelines when compared with traditional techniques
    where the communication barrier between tools is much higher. Two separate frameworks,
    Modin and Dask, both offer a pandas-like experience on top of Ray Datasets, making
    it easy to scale existing data science workflows. Spark on Ray (known as *RayDP*)
    provides an easy integration path for those working in organizations with existing
    big-data tools.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you learned to effectively process data with Ray to power your
    ML and other needs. In the next chapter, you will learn to use Ray to power ML.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch09.html#idm45354767432992-marker)) This is like a Ford dealer recommending
    a Ford, so take this advice with a grain of salt.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch09.html#idm45354767410480-marker)) Operations in native code can avoid
    this problem in Dask by using multithreading, but the details are beyond the scope
    of this book.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch09.html#idm45354766947536-marker)) Alternate algorithms for exact quantiles
    depend on more shuffles to reduce the space overhead.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch09.html#idm45354766886064-marker)) Key-skew can make this impossible
    for a known partitioner.
  prefs: []
  type: TYPE_NORMAL
