["```py\n$ cat docker-compose.yaml\nversion: \"3\"\nservices:\n  app:\n    image: \"griggheo/flask-by-example:v1\"\n    command: \"manage.py runserver --host=0.0.0.0\"\n    ports:\n      - \"5000:5000\"\n    environment:\n      APP_SETTINGS: config.ProductionConfig\n      DATABASE_URL: postgresql://wordcount_dbadmin:$DBPASS@db/wordcount\n      REDISTOGO_URL: redis://redis:6379\n    depends_on:\n      - db\n      - redis\n  worker:\n    image: \"griggheo/flask-by-example:v1\"\n    command: \"worker.py\"\n    environment:\n      APP_SETTINGS: config.ProductionConfig\n      DATABASE_URL: postgresql://wordcount_dbadmin:$DBPASS@db/wordcount\n      REDISTOGO_URL: redis://redis:6379\n    depends_on:\n      - db\n      - redis\n  migrations:\n    image: \"griggheo/flask-by-example:v1\"\n    command: \"manage.py db upgrade\"\n    environment:\n      APP_SETTINGS: config.ProductionConfig\n      DATABASE_URL: postgresql://wordcount_dbadmin:$DBPASS@db/wordcount\n    depends_on:\n      - db\n  db:\n    image: \"postgres:11\"\n    container_name: \"postgres\"\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - dbdata:/var/lib/postgresql/data\n  redis:\n    image: \"redis:alpine\"\n    ports:\n      - \"6379:6379\"\nvolumes:\n  dbdata:\n```", "```py\n$ kompose convert\nINFO Kubernetes file \"app-service.yaml\" created\nINFO Kubernetes file \"db-service.yaml\" created\nINFO Kubernetes file \"redis-service.yaml\" created\nINFO Kubernetes file \"app-deployment.yaml\" created\nINFO Kubernetes file \"db-deployment.yaml\" created\nINFO Kubernetes file \"dbdata-persistentvolumeclaim.yaml\" created\nINFO Kubernetes file \"migrations-deployment.yaml\" created\nINFO Kubernetes file \"redis-deployment.yaml\" created\nINFO Kubernetes file \"worker-deployment.yaml\" created\n```", "```py\n$ rm docker-compose.yaml\n```", "```py\n$ minikube start --kubernetes-version v1.15.0\n minikube v1.2.0 on darwin (amd64)\n Creating virtualbox VM (CPUs=2, Memory=2048MB, Disk=20000MB) ...\n Configuring environment for Kubernetes v1.15.0 on Docker 18.09.6\n Downloading kubeadm v1.15.0\n Downloading kubelet v1.15.0\n Pulling images ...\n Launching Kubernetes ...\n Verifying: apiserver proxy etcd scheduler controller dns\n Done! kubectl is now configured to use \"minikube\"\n\n```", "```py\n$ kubectl config use-context minikube\nSwitched to context \"minikube\".\n```", "```py\n$ kubectx minikube\nSwitched to context \"minikube\".\n```", "```py\nsource \"/usr/local/opt/kube-ps1/share/kube-ps1.sh\"\nPS1='$(kube_ps1)'$PS1\n```", "```py\n$ kubectl get nodes\nNAME       STATUS   ROLES    AGE     VERSION\nminikube   Ready    master   2m14s   v1.15.0\n```", "```py\n$ cat dbdata-persistentvolumeclaim.yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  creationTimestamp: null\n  labels:\n    io.kompose.service: dbdata\n  name: dbdata\nspec:\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 100Mi\nstatus: {}\n```", "```py\n$ kubectl create -f dbdata-persistentvolumeclaim.yaml\npersistentvolumeclaim/dbdata created\n```", "```py\n$ kubectl get pvc\nNAME     STATUS   VOLUME                                     CAPACITY\nACCESS MODES   STORAGECLASS   AGE\ndbdata   Bound    pvc-39914723-4455-439b-a0f5-82a5f7421475   100Mi\nRWO            standard       1m\n```", "```py\n$ cat db-deployment.yaml\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  annotations:\n    kompose.cmd: kompose convert\n    kompose.version: 1.16.0 (0c01309)\n  creationTimestamp: null\n  labels:\n    io.kompose.service: db\n  name: db\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        io.kompose.service: db\n    spec:\n      containers:\n      - image: postgres:11\n        name: postgres\n        ports:\n        - containerPort: 5432\n        resources: {}\n        volumeMounts:\n        - mountPath: /var/lib/postgresql/data\n          name: dbdata\n      restartPolicy: Always\n      volumes:\n      - name: dbdata\n        persistentVolumeClaim:\n          claimName: dbdata\nstatus: {}\n```", "```py\n$ kubectl create -f db-deployment.yaml\ndeployment.extensions/db created\n```", "```py\n$ kubectl get deployments\nNAME     READY   UP-TO-DATE   AVAILABLE   AGE\ndb       1/1     1            1           1m\n\n$ kubectl get pods\nNAME                  READY   STATUS    RESTARTS   AGE\ndb-67659d85bf-vrnw7   1/1     Running   0          1m\n```", "```py\n$ kubectl exec -it db-67659d85bf-vrnw7 -- psql -U postgres\npsql (11.4 (Debian 11.4-1.pgdg90+1))\nType \"help\" for help.\n\npostgres=# create database wordcount;\nCREATE DATABASE\npostgres=# \\q\n\n$ kubectl exec -it db-67659d85bf-vrnw7 -- psql -U postgres wordcount\npsql (11.4 (Debian 11.4-1.pgdg90+1))\nType \"help\" for help.\n\nwordcount=# CREATE ROLE wordcount_dbadmin;\nCREATE ROLE\nwordcount=# ALTER ROLE wordcount_dbadmin LOGIN;\nALTER ROLE\nwordcount=# ALTER USER wordcount_dbadmin PASSWORD 'MYPASS';\nALTER ROLE\nwordcount=# \\q\n```", "```py\n$ cat db-service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    kompose.cmd: kompose convert\n    kompose.version: 1.16.0 (0c01309)\n  creationTimestamp: null\n  labels:\n    io.kompose.service: db\n  name: db\nspec:\n  ports:\n  - name: \"5432\"\n    port: 5432\n    targetPort: 5432\n  selector:\n    io.kompose.service: db\nstatus:\n  loadBalancer: {}\n```", "```py\n  labels:\n    io.kompose.service: db\n```", "```py\n$ kubectl create -f db-service.yaml\nservice/db created\n```", "```py\n$ kubectl get services\nNAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE\ndb           ClusterIP   10.110.108.96   <none>        5432/TCP   6s\nkubernetes   ClusterIP   10.96.0.1       <none>        443/TCP    4h45m\n```", "```py\n$ cat redis-deployment.yaml\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  annotations:\n    kompose.cmd: kompose convert\n    kompose.version: 1.16.0 (0c01309)\n  creationTimestamp: null\n  labels:\n    io.kompose.service: redis\n  name: redis\nspec:\n  replicas: 1\n  strategy: {}\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        io.kompose.service: redis\n    spec:\n      containers:\n      - image: redis:alpine\n        name: redis\n        ports:\n        - containerPort: 6379\n        resources: {}\n      restartPolicy: Always\nstatus: {}\n\n$ kubectl create -f redis-deployment.yaml\ndeployment.extensions/redis created\n\n$ kubectl get pods\nNAME                    READY   STATUS    RESTARTS   AGE\ndb-67659d85bf-vrnw7     1/1     Running   0          37m\nredis-c6476fbff-8kpqz   1/1     Running   0          11s\n\n$ kubectl create -f redis-service.yaml\nservice/redis created\n\n$ cat redis-service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    kompose.cmd: kompose convert\n    kompose.version: 1.16.0 (0c01309)\n  creationTimestamp: null\n  labels:\n    io.kompose.service: redis\n  name: redis\nspec:\n  ports:\n  - name: \"6379\"\n    port: 6379\n    targetPort: 6379\n  selector:\n    io.kompose.service: redis\nstatus:\n  loadBalancer: {}\n\n$ kubectl get services\nNAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE\ndb           ClusterIP   10.110.108.96   <none>        5432/TCP   84s\nkubernetes   ClusterIP   10.96.0.1       <none>        443/TCP    4h46m\nredis        ClusterIP   10.106.44.183   <none>        6379/TCP   10s\n```", "```py\n$ echo MYPASS | base64\nMYPASSBASE64\n```", "```py\n$ sops --pgp E14104A0890994B9AC9C9F6782C1FF5E679EFF32 secrets.yaml.enc\n```", "```py\napiVersion: v1\nkind: Secret\nmetadata:\n  name: fbe-secret\ntype: Opaque\ndata:\n  dbpass: MYPASSBASE64\n```", "```py\n$ sops -d secrets.yaml.enc\napiVersion: v1\nkind: Secret\nmetadata:\n  name: fbe-secret\ntype: Opaque\ndata:\n  dbpass: MYPASSBASE64\n```", "```py\n$ sops -d secrets.yaml.enc | kubectl create -f -\nsecret/fbe-secret created\n```", "```py\n$ kubectl get secrets\nNAME                  TYPE                                  DATA   AGE\ndefault-token-k7652   kubernetes.io/service-account-token   3      3h19m\nfbe-secret            Opaque                                1      45s\n\n$ kubectl describe secret fbe-secret\nName:         fbe-secret\nNamespace:    default\nLabels:       <none>\nAnnotations:  <none>\n\nType:  Opaque\n\nData\ndbpass:  12 bytes\n```", "```py\n$ kubectl get secrets fbe-secret -ojson | jq -r \".data.dbpass\"\nMYPASSBASE64\n```", "```py\n$ kubectl get secrets fbe-secret -ojson | jq -r \".data.dbpass\" | base64 -D\nMYPASS\n```", "```py\n$ kubectl get secrets fbe-secret -ojson | jq -r \".data.dbpass\" | base64 -d\nMYPASS\n```", "```py\n$ cat worker-deployment.yaml\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  annotations:\n    kompose.cmd: kompose convert\n    kompose.version: 1.16.0 (0c01309)\n  creationTimestamp: null\n  labels:\n    io.kompose.service: worker\n  name: worker\nspec:\n  replicas: 1\n  strategy: {}\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        io.kompose.service: worker\n    spec:\n      containers:\n      - args:\n        - worker.py\n        env:\n        - name: APP_SETTINGS\n          value: config.ProductionConfig\n        - name: DBPASS\n          valueFrom:\n            secretKeyRef:\n              name: fbe-secret\n              key: dbpass\n        - name: DATABASE_URL\n          value: postgresql://wordcount_dbadmin:${DBPASS}@db/wordcount\n        - name: REDISTOGO_URL\n          value: redis://redis:6379\n        image: griggheo/flask-by-example:v1\n        name: worker\n        resources: {}\n      restartPolicy: Always\nstatus: {}\n```", "```py\n$ kubectl create -f worker-deployment.yaml\ndeployment.extensions/worker created\n```", "```py\n$ kubectl get pods\nNAME                      READY   STATUS              RESTARTS   AGE\ndb-67659d85bf-vrnw7       1/1     Running             1          21h\nredis-c6476fbff-8kpqz     1/1     Running             1          21h\nworker-7dbf5ff56c-vgs42   0/1     Init:ErrImagePull   0          7s\n```", "```py\n$ kubectl describe pod worker-7dbf5ff56c-vgs42 | tail -10\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type     Reason     Age                  From               Message\n  ----     ------     ----                 ----               -------\n  Normal   Scheduled  2m51s                default-scheduler\n  Successfully assigned default/worker-7dbf5ff56c-vgs42 to minikube\n\n  Normal   Pulling    76s (x4 over 2m50s)  kubelet, minikube\n  Pulling image \"griggheo/flask-by-example:v1\"\n\n  Warning  Failed     75s (x4 over 2m49s)  kubelet, minikube\n  Failed to pull image \"griggheo/flask-by-example:v1\": rpc error:\n  code = Unknown desc = Error response from daemon: pull access denied for\n  griggheo/flask-by-example, repository does not exist or may require\n  'docker login'\n\n  Warning  Failed     75s (x4 over 2m49s)  kubelet, minikube\n  Error: ErrImagePull\n\n  Warning  Failed     62s (x6 over 2m48s)  kubelet, minikube\n  Error: ImagePullBackOff\n\n  Normal   BackOff    51s (x7 over 2m48s)  kubelet, minikube\n  Back-off pulling image \"griggheo/flask-by-example:v1\"\n```", "```py\n$ sops --pgp E14104A0890994B9AC9C9F6782C1FF5E679EFF32 \\\ncreate_docker_credentials_secret.sh.enc\n```", "```py\nDOCKER_REGISTRY_SERVER=docker.io\nDOCKER_USER=Type your dockerhub username, same as when you `docker login`\nDOCKER_EMAIL=Type your dockerhub email, same as when you `docker login`\nDOCKER_PASSWORD=Type your dockerhub pw, same as when you `docker login`\n\nkubectl create secret docker-registry myregistrykey \\\n--docker-server=$DOCKER_REGISTRY_SERVER \\\n--docker-username=$DOCKER_USER \\\n--docker-password=$DOCKER_PASSWORD \\\n--docker-email=$DOCKER_EMAIL\n```", "```py\n$ sops -d create_docker_credentials_secret.sh.enc | bash -\nsecret/myregistrykey created\n```", "```py\n$ kubectl get secrets myregistrykey -oyaml\napiVersion: v1\ndata:\n  .dockerconfigjson: eyJhdXRocyI6eyJkb2NrZXIuaW8iO\nkind: Secret\nmetadata:\n  creationTimestamp: \"2019-07-17T22:11:56Z\"\n  name: myregistrykey\n  namespace: default\n  resourceVersion: \"16062\"\n  selfLink: /api/v1/namespaces/default/secrets/myregistrykey\n  uid: 47d29ffc-69e4-41df-a237-1138cd9e8971\ntype: kubernetes.io/dockerconfigjson\n```", "```py\n      imagePullSecrets:\n      - name: myregistrykey\n```", "```py\n     restartPolicy: Always\n```", "```py\n$ kubectl delete -f worker-deployment.yaml\ndeployment.extensions \"worker\" deleted\n\n$ kubectl create -f worker-deployment.yaml\ndeployment.extensions/worker created\n```", "```py\n$ kubectl get pods\nNAME                      READY   STATUS    RESTARTS   AGE\ndb-67659d85bf-vrnw7       1/1     Running   1          22h\nredis-c6476fbff-8kpqz     1/1     Running   1          21h\nworker-7dbf5ff56c-hga37   1/1     Running   0          4m53s\n```", "```py\n$ kubectl logs worker-7dbf5ff56c-hga37\n20:43:13 RQ worker 'rq:worker:040640781edd4055a990b798ac2eb52d'\nstarted, version 1.0\n20:43:13 *** Listening on default...\n20:43:13 Cleaning registries for queue: default\n```", "```py\n      initContainers:\n      - args:\n        - manage.py\n        - db\n        - upgrade\n        env:\n        - name: APP_SETTINGS\n          value: config.ProductionConfig\n        - name: DATABASE_URL\n          value: postgresql://wordcount_dbadmin:@db/wordcount\n        image: griggheo/flask-by-example:v1\n        name: migrations\n        resources: {}\n\n$ rm migrations-deployment.yaml\n```", "```py\n$ cat app-deployment.yaml\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  annotations:\n    kompose.cmd: kompose convert\n    kompose.version: 1.16.0 (0c01309)\n  creationTimestamp: null\n  labels:\n    io.kompose.service: app\n  name: app\nspec:\n  replicas: 1\n  strategy: {}\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        io.kompose.service: app\n    spec:\n      initContainers:\n      - args:\n        - manage.py\n        - db\n        - upgrade\n        env:\n        - name: APP_SETTINGS\n          value: config.ProductionConfig\n        - name: DBPASS\n          valueFrom:\n            secretKeyRef:\n              name: fbe-secret\n              key: dbpass\n        - name: DATABASE_URL\n          value: postgresql://wordcount_dbadmin:${DBPASS}@db/wordcount\n        image: griggheo/flask-by-example:v1\n        name: migrations\n        resources: {}\n      containers:\n      - args:\n        - manage.py\n        - runserver\n        - --host=0.0.0.0\n        env:\n        - name: APP_SETTINGS\n          value: config.ProductionConfig\n        - name: DBPASS\n          valueFrom:\n            secretKeyRef:\n              name: fbe-secret\n              key: dbpass\n        - name: DATABASE_URL\n          value: postgresql://wordcount_dbadmin:${DBPASS}@db/wordcount\n        - name: REDISTOGO_URL\n          value: redis://redis:6379\n        image: griggheo/flask-by-example:v1\n        name: app\n        ports:\n        - containerPort: 5000\n        resources: {}\n      restartPolicy: Always\nstatus: {}\n```", "```py\n$ kubectl create -f app-deployment.yaml\ndeployment.extensions/app created\n\n$ kubectl get pods\nNAME                      READY   STATUS    RESTARTS   AGE\napp-c845d8969-l8nhg       1/1     Running   0          7s\ndb-67659d85bf-vrnw7       1/1     Running   1          22h\nredis-c6476fbff-8kpqz     1/1     Running   1          21h\nworker-7dbf5ff56c-vgs42   1/1     Running   0          4m53s\n```", "```py\n$ cat app-service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    kompose.cmd: kompose convert\n    kompose.version: 1.16.0 (0c01309)\n  creationTimestamp: null\n  labels:\n    io.kompose.service: app\n  name: app\nspec:\n  ports:\n  - name: \"5000\"\n    port: 5000\n    targetPort: 5000\n  type: LoadBalancer\n  selector:\n    io.kompose.service: app\nstatus:\n  loadBalancer: {}\n```", "```py\n  labels:\n    io.kompose.service: app\n```", "```py\n$ kubectl create -f app-service.yaml\nservice/app created\n\n$ kubectl get services\nNAME         TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE\napp          LoadBalancer   10.99.55.191    <pending>     5000:30097/TCP   2s\ndb           ClusterIP      10.110.108.96   <none>        5432/TCP         21h\nkubernetes   ClusterIP      10.96.0.1       <none>        443/TCP          26h\nredis        ClusterIP      10.106.44.183   <none>        6379/TCP         21h\n```", "```py\n$ minikube service app\n```", "```py\n$ mkdir pulumi_gke\n$ cd pulumi_gke\n```", "```py\n$ gcloud init\nWelcome! This command will take you through the configuration of gcloud.\n\nSettings from your current configuration [default] are:\ncore:\n  account: grig.gheorghiu@gmail.com\n  disable_usage_reporting: 'True'\n  project: pulumi-gke-testing\n\nPick configuration to use:\n [1] Re-initialize this configuration [default] with new settings\n [2] Create a new configuration\nPlease enter your numeric choice:  2\n\nEnter configuration name. Names start with a lower case letter and\ncontain only lower case letters a-z, digits 0-9, and hyphens '-':\npythonfordevops-gke-pulumi\nYour current configuration has been set to: [pythonfordevops-gke-pulumi]\n\nPick cloud project to use:\n [1] pulumi-gke-testing\n [2] Create a new project\nPlease enter numeric choice or text value (must exactly match list\nitem):  2\n\nEnter a Project ID. pythonfordevops-gke-pulumi\nYour current project has been set to: [pythonfordevops-gke-pulumi].\n```", "```py\n$ gcloud auth login\n```", "```py\n$ gcloud auth application-default login\n```", "```py\n$ pulumi new\nPlease choose a template: gcp-python\nA minimal Google Cloud Python Pulumi program\nThis command will walk you through creating a new Pulumi project.\n\nEnter a value or leave blank to accept the (default), and press <ENTER>.\nPress ^C at any time to quit.\n\nproject name: (pulumi_gke_py) pythonfordevops-gke-pulumi\nproject description: (A minimal Google Cloud Python Pulumi program)\nCreated project 'pythonfordevops-gke-pulumi'\n\nstack name: (dev)\nCreated stack 'dev'\n\ngcp:project: The Google Cloud project to deploy into: pythonfordevops-gke-pulumi\nSaved config\n\nYour new project is ready to go! \n\nTo perform an initial deployment, run the following commands:\n\n   1\\. virtualenv -p python3 venv\n   2\\. source venv/bin/activate\n   3\\. pip3 install -r requirements.txt\n\nThen, run 'pulumi up'.\n\n```", "```py\n$ ls -la\nls -la\ntotal 40\ndrwxr-xr-x  7 ggheo  staff  224 Jul 16 15:08 .\ndrwxr-xr-x  6 ggheo  staff  192 Jul 16 15:06 ..\n-rw-------  1 ggheo  staff   12 Jul 16 15:07 .gitignore\n-rw-r--r--  1 ggheo  staff   50 Jul 16 15:08 Pulumi.dev.yaml\n-rw-------  1 ggheo  staff  107 Jul 16 15:07 Pulumi.yaml\n-rw-------  1 ggheo  staff  203 Jul 16 15:07 __main__.py\n-rw-------  1 ggheo  staff   34 Jul 16 15:07 requirements.txt\n```", "```py\n$ cp ~/pulumi-examples/gcp-py-gke/*.py .\n$ cp ~/pulumi-examples/gcp-py-gke/requirements.txt .\n```", "```py\n$ pulumi config set gcp:project pythonfordevops-gke-pulumi\n$ pulumi config set gcp:zone us-west1-a\n$ pulumi config set password --secret PASS_FOR_KUBE_CLUSTER\n```", "```py\n$ virtualenv -p python3 venv\n$ source venv/bin/activate\n$ pip3 install -r requirements.txt\n$ pulumi up\n```", "```py\n$ pulumi stack output kubeconfig > kubeconfig.yaml\n$ export KUBECONFIG=./kubeconfig.yaml\n```", "```py\n$ kubectl get nodes\nNAME                                                 STATUS   ROLES    AGE\n   VERSION\ngke-gke-cluster-ea17e87-default-pool-fd130152-30p3   Ready    <none>   4m29s\n   v1.13.7-gke.8\ngke-gke-cluster-ea17e87-default-pool-fd130152-kf9k   Ready    <none>   4m29s\n   v1.13.7-gke.8\ngke-gke-cluster-ea17e87-default-pool-fd130152-x9dx   Ready    <none>   4m27s\n   v1.13.7-gke.8\n```", "```py\n$ kubectl create -f redis-deployment.yaml\ndeployment.extensions/redis created\n\n$ kubectl get pods\nNAME                             READY   STATUS    RESTARTS   AGE\ncanary-aqw8jtfo-f54b9749-q5wqj   1/1     Running   0          5m57s\nredis-9946db5cc-8g6zz            1/1     Running   0          20s\n\n$ kubectl create -f redis-service.yaml\nservice/redis created\n\n$ kubectl get service redis\nNAME    TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE\nredis   ClusterIP   10.59.245.221   <none>        6379/TCP   18s\n```", "```py\n$ kubectl create -f dbdata-persistentvolumeclaim.yaml\npersistentvolumeclaim/dbdata created\n\n$ kubectl get pvc\nNAME     STATUS   VOLUME                                     CAPACITY\ndbdata   Bound    pvc-00c8156c-b618-11e9-9e84-42010a8a006f   1Gi\n   ACCESS MODES   STORAGECLASS   AGE\n   RWO            standard       12s\n```", "```py\n$ kubectl create -f db-deployment.yaml\ndeployment.extensions/db created\n\n$ kubectl get pods\nNAME                             READY   STATUS             RESTARTS  AGE\ncanary-aqw8jtfo-f54b9749-q5wqj   1/1     Running            0         8m52s\ndb-6b4fbb57d9-cjjxx              0/1     CrashLoopBackOff   1         38s\nredis-9946db5cc-8g6zz            1/1     Running            0         3m15s\n\n$ kubectl logs db-6b4fbb57d9-cjjxx\n\ninitdb: directory \"/var/lib/postgresql/data\" exists but is not empty\nIt contains a lost+found directory, perhaps due to it being a mount point.\nUsing a mount point directly as the data directory is not recommended.\nCreate a subdirectory under the mount point.\n```", "```py\n$ kubectl delete -f db-deployment.yaml\ndeployment.extensions \"db\" deleted\n```", "```py\n$ cat pvc-inspect.yaml\nkind: Pod\napiVersion: v1\nmetadata:\n  name: pvc-inspect\nspec:\n  volumes:\n    - name: dbdata\n      persistentVolumeClaim:\n        claimName: dbdata\n  containers:\n    - name: debugger\n      image: busybox\n      command: ['sleep', '3600']\n      volumeMounts:\n        - mountPath: \"/data\"\n          name: dbdata\n\n$ kubectl create -f pvc-inspect.yaml\npod/pvc-inspect created\n\n$ kubectl get pods\nNAME                             READY   STATUS    RESTARTS   AGE\ncanary-aqw8jtfo-f54b9749-q5wqj   1/1     Running   0          20m\npvc-inspect                      1/1     Running   0          35s\nredis-9946db5cc-8g6zz            1/1     Running   0          14m\n```", "```py\n$ kubectl exec -it pvc-inspect -- sh\n/ # cd /data\n/data # ls -la\ntotal 24\ndrwx------    3 999      root          4096 Aug  3 17:57 .\ndrwxr-xr-x    1 root     root          4096 Aug  3 18:08 ..\ndrwx------    2 999      root         16384 Aug  3 17:57 lost+found\n/data # rm -rf lost\\+found/\n/data # exit\n```", "```py\n$ kubectl delete pod pvc-inspect\npod \"pvc-inspect\" deleted\n```", "```py\n$ kubectl create -f db-deployment.yaml\ndeployment.extensions/db created\n\n$ kubectl get pods\nNAME                             READY   STATUS    RESTARTS   AGE\ncanary-aqw8jtfo-f54b9749-q5wqj   1/1     Running   0          23m\ndb-6b4fbb57d9-8h978              1/1     Running   0          19s\nredis-9946db5cc-8g6zz            1/1     Running   0          17m\n\n$ kubectl logs db-6b4fbb57d9-8h978\nPostgreSQL init process complete; ready for start up.\n\n2019-08-03 18:12:01.108 UTC [1]\nLOG:  listening on IPv4 address \"0.0.0.0\", port 5432\n2019-08-03 18:12:01.108 UTC [1]\nLOG:  listening on IPv6 address \"::\", port 5432\n2019-08-03 18:12:01.114 UTC [1]\nLOG:  listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5432\"\n2019-08-03 18:12:01.135 UTC [50]\nLOG:  database system was shut down at 2019-08-03 18:12:01 UTC\n2019-08-03 18:12:01.141 UTC [1]\nLOG:  database system is ready to accept connections\n```", "```py\n$ kubectl exec -it db-6b4fbb57d9-8h978 -- psql -U postgres\npsql (11.4 (Debian 11.4-1.pgdg90+1))\nType \"help\" for help.\n\npostgres=# create database wordcount;\nCREATE DATABASE\npostgres=# \\q\n\n$ kubectl exec -it db-6b4fbb57d9-8h978 -- psql -U postgres wordcount\npsql (11.4 (Debian 11.4-1.pgdg90+1))\nType \"help\" for help.\n\nwordcount=# CREATE ROLE wordcount_dbadmin;\nCREATE ROLE\nwordcount=# ALTER ROLE wordcount_dbadmin LOGIN;\nALTER ROLE\nwordcount=# ALTER USER wordcount_dbadmin PASSWORD 'MYNEWPASS';\nALTER ROLE\nwordcount=# \\q\n```", "```py\n$ kubectl create -f db-service.yaml\nservice/db created\n```", "```py\n$ kubectl describe service db\nName:              db\nNamespace:         default\nLabels:            io.kompose.service=db\nAnnotations:       kompose.cmd: kompose convert\n                   kompose.version: 1.16.0 (0c01309)\nSelector:          io.kompose.service=db\nType:              ClusterIP\nIP:                10.59.241.181\nPort:              5432  5432/TCP\nTargetPort:        5432/TCP\nEndpoints:         10.56.2.5:5432\nSession Affinity:  None\nEvents:            <none>\n```", "```py\n$ echo MYNEWPASS | base64\nMYNEWPASSBASE64\n\n$ sops secrets.yaml.enc\n\napiVersion: v1\nkind: Secret\nmetadata:\n  name: fbe-secret\ntype: Opaque\ndata:\n  dbpass: MYNEWPASSBASE64\n\n$ sops -d secrets.yaml.enc | kubectl create -f -\nsecret/fbe-secret created\n\nkubectl describe secret fbe-secret\nName:         fbe-secret\nNamespace:    default\nLabels:       <none>\nAnnotations:  <none>\n\nType:  Opaque\n\nData\n===\ndbpass:  21 bytes\n```", "```py\n$ sops -d create_docker_credentials_secret.sh.enc | bash -\nsecret/myregistrykey created\n```", "```py\n$ kubectl create -f worker-deployment.yaml\ndeployment.extensions/worker created\n```", "```py\n$ kubectl get pods\nNAME                             READY   STATUS    RESTARTS   AGE\ncanary-aqw8jtfo-f54b9749-q5wqj   1/1     Running   0          39m\ndb-6b4fbb57d9-8h978              1/1     Running   0          16m\nredis-9946db5cc-8g6zz            1/1     Running   0          34m\nworker-8cf5dc699-98z99           1/1     Running   0          35s\nworker-8cf5dc699-9s26v           1/1     Running   0          35s\nworker-8cf5dc699-v6ckr           1/1     Running   0          35s\n\n$ kubectl logs worker-8cf5dc699-98z99\n18:28:08 RQ worker 'rq:worker:1355d2cad49646e4953c6b4d978571f1' started,\n version 1.0\n18:28:08 *** Listening on default...\n```", "```py\n$ kubectl create -f app-deployment.yaml\ndeployment.extensions/app created\n```", "```py\n$ kubectl get pods\nNAME                             READY   STATUS    RESTARTS   AGE\napp-7964cff98f-5bx4s             1/1     Running   0          54s\napp-7964cff98f-8n8hk             1/1     Running   0          54s\ncanary-aqw8jtfo-f54b9749-q5wqj   1/1     Running   0          41m\ndb-6b4fbb57d9-8h978              1/1     Running   0          19m\nredis-9946db5cc-8g6zz            1/1     Running   0          36m\nworker-8cf5dc699-98z99           1/1     Running   0          2m44s\nworker-8cf5dc699-9s26v           1/1     Running   0          2m44s\nworker-8cf5dc699-v6ckr           1/1     Running   0          2m44s\n```", "```py\n$ kubectl create -f app-service.yaml\nservice/app created\n```", "```py\n$ kubectl describe service app\nName:                     app\nNamespace:                default\nLabels:                   io.kompose.service=app\nAnnotations:              kompose.cmd: kompose convert\n                          kompose.version: 1.16.0 (0c01309)\nSelector:                 io.kompose.service=app\nType:                     LoadBalancer\nIP:                       10.59.255.31\nLoadBalancer Ingress:     34.83.242.171\nPort:                     5000  5000/TCP\nTargetPort:               5000/TCP\nNodePort:                 5000  31305/TCP\nEndpoints:                10.56.1.6:5000,10.56.2.12:5000\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:\nType    Reason                Age   From                Message\n----    ------                ----  ----                -------\nNormal  EnsuringLoadBalancer  72s   service-controller  Ensuring load balancer\nNormal  EnsuredLoadBalancer   33s   service-controller  Ensured load balancer\n```", "```py\n$ kubectl -n kube-system create sa tiller\n\n$ kubectl create clusterrolebinding tiller \\\n  --clusterrole cluster-admin \\\n  --serviceaccount=kube-system:tiller\n\n$ kubectl patch deploy --namespace kube-system \\\ntiller-deploy -p  '{\"spec\":{\"template\":{\"spec\":{\"serviceAccount\":\"tiller\"}}}}'\n```", "```py\n$ helm init\n```", "```py\n$ kubectl create namespace monitoring\nnamespace/monitoring created\n```", "```py\n$ helm install --name prometheus --namespace monitoring stable/prometheus\nNAME:   prometheus\nLAST DEPLOYED: Tue Aug 27 12:59:40 2019\nNAMESPACE: monitoring\nSTATUS: DEPLOYED\n```", "```py\n$ kubectl get pods -nmonitoring\nNAME                                             READY   STATUS    RESTARTS AGE\nprometheus-alertmanager-df57f6df6-4b8lv          2/2     Running   0        3m\nprometheus-kube-state-metrics-564564f799-t6qdm   1/1     Running   0        3m\nprometheus-node-exporter-b4sb9                   1/1     Running   0        3m\nprometheus-node-exporter-n4z2g                   1/1     Running   0        3m\nprometheus-node-exporter-w7hn7                   1/1     Running   0        3m\nprometheus-pushgateway-56b65bcf5f-whx5t          1/1     Running   0        3m\nprometheus-server-7555945646-d86gn               2/2     Running   0        3m\n\n$ kubectl get services -nmonitoring\nNAME                            TYPE        CLUSTER-IP    EXTERNAL-IP  PORT(S)\n   AGE\nprometheus-alertmanager         ClusterIP   10.0.6.98     <none>       80/TCP\n   3m51s\nprometheus-kube-state-metrics   ClusterIP   None          <none>       80/TCP\n   3m51s\nprometheus-node-exporter        ClusterIP   None          <none>       9100/TCP\n   3m51s\nprometheus-pushgateway          ClusterIP   10.0.13.216   <none>       9091/TCP\n   3m51s\nprometheus-server               ClusterIP   10.0.4.74     <none>       80/TCP\n   3m51s\n\n$ kubectl get configmaps -nmonitoring\nNAME                      DATA   AGE\nprometheus-alertmanager   1      3m58s\nprometheus-server         3      3m58s\n```", "```py\n$ export PROMETHEUS_POD_NAME=$(kubectl get pods --namespace monitoring \\\n-l \"app=prometheus,component=server\" -o jsonpath=\"{.items[0].metadata.name}\")\n\n$ echo $PROMETHEUS_POD_NAME\nprometheus-server-7555945646-d86gn\n\n$ kubectl --namespace monitoring port-forward $PROMETHEUS_POD_NAME 9090\nForwarding from 127.0.0.1:9090 -> 9090\nForwarding from [::1]:9090 -> 9090\nHandling connection for 9090\n```", "```py\n$ helm install --name grafana --namespace monitoring stable/grafana\nNAME:   grafana\nLAST DEPLOYED: Tue Aug 27 13:10:02 2019\nNAMESPACE: monitoring\nSTATUS: DEPLOYED\n```", "```py\n$ kubectl get pods -nmonitoring | grep grafana\ngrafana-84b887cf4d-wplcr                         1/1     Running   0\n\n$ kubectl get services -nmonitoring | grep grafana\ngrafana                         ClusterIP   10.0.5.154    <none>        80/TCP\n\n$ kubectl get configmaps -nmonitoring | grep grafana\ngrafana                   1      99s\ngrafana-test              1      99s\n\n$ kubectl get secrets -nmonitoring | grep grafana\ngrafana                                     Opaque\ngrafana-test-token-85x4x                    kubernetes.io/service-account-token\ngrafana-token-jw2qg                         kubernetes.io/service-account-token\n```", "```py\n$ kubectl get secret --namespace monitoring grafana \\\n-o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo\n\nSOMESECRETTEXT\n```", "```py\n$ export GRAFANA_POD_NAME=$(kubectl get pods --namespace monitoring \\\n-l \"app=grafana,release=grafana\" -o jsonpath=\"{.items[0].metadata.name}\")\n\n$ kubectl --namespace monitoring port-forward $GRAFANA_POD_NAME 3000\nForwarding from 127.0.0.1:3000 -> 3000\nForwarding from [::1]:3000 -> 3000\n```", "```py\n$ helm list\nNAME        REVISION  UPDATED                   STATUS    CHART\n    APP VERSION NAMESPACE\ngrafana     1         Tue Aug 27 13:10:02 2019  DEPLOYED  grafana-3.8.3\n    6.2.5       monitoring\nprometheus. 1         Tue Aug 27 12:59:40 2019  DEPLOYED  prometheus-9.1.0\n    2.11.1      monitoring\n\n```", "```py\n$ mkdir charts\n$ cd charts\n$ helm fetch stable/prometheus\n$ helm fetch stable/grafana\n$ ls -la\ntotal 80\ndrwxr-xr-x   4 ggheo  staff    128 Aug 27 13:59 .\ndrwxr-xr-x  15 ggheo  staff    480 Aug 27 13:55 ..\n-rw-r--r--   1 ggheo  staff  16195 Aug 27 13:55 grafana-3.8.3.tgz\n-rw-r--r--   1 ggheo  staff  23481 Aug 27 13:54 prometheus-9.1.0.tgz\n```", "```py\n$ tar xfz prometheus-9.1.0.tgz; rm prometheus-9.1.0.tgz\n$ tar xfz grafana-3.8.3.tgz; rm grafana-3.8.3.tgz\n```", "```py\n## Enable persistence using Persistent Volume Claims\n## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/\n##\npersistence:\n  enabled: true\n  # storageClassName: default\n  accessModes:\n    - ReadWriteOnce\n  size: 10Gi\n  # annotations: {}\n  finalizers:\n    - kubernetes.io/pvc-protection\n  # subPath: \"\"\n  # existingClaim:\n```", "```py\n$ helm upgrade grafana grafana/\nRelease \"grafana\" has been upgraded. Happy Helming!\n```", "```py\nkubectl describe pvc grafana -nmonitoring\nName:        grafana\nNamespace:   monitoring\nStorageClass:standard\nStatus:      Bound\nVolume:      pvc-31d47393-c910-11e9-87c5-42010a8a0021\nLabels:      app=grafana\n             chart=grafana-3.8.3\n             heritage=Tiller\n             release=grafana\nAnnotations: pv.kubernetes.io/bind-completed: yes\n             pv.kubernetes.io/bound-by-controller: yes\n             volume.beta.kubernetes.io/storage-provisioner:kubernetes.io/gce-pd\nFinalizers:  [kubernetes.io/pvc-protection]\nCapacity:    10Gi\nAccess Modes:RWO\nMounted By:  grafana-84f79d5c45-zlqz8\nEvents:\nType    Reason                 Age   From                         Message\n----    ------                 ----  ----                         -------\nNormal  ProvisioningSucceeded  88s   persistentvolume-controller  Successfully\nprovisioned volume pvc-31d47393-c910-11e9-87c5-42010a8a0021\nusing kubernetes.io/gce-pd\n```", "```py\n  ## Prometheus data retention period (default if not specified is 15 days)\n  ##\n  retention: \"30d\"\n```", "```py\n$ helm upgrade prometheus prometheus\nRelease \"prometheus\" has been upgraded. Happy Helming!\n```", "```py\n$ kubectl get pods -nmonitoring\nNAME                                            READY   STATUS   RESTARTS   AGE\ngrafana-84f79d5c45-zlqz8                        1/1     Running  0          9m\nprometheus-alertmanager-df57f6df6-4b8lv         2/2     Running  0          87m\nprometheus-kube-state-metrics-564564f799-t6qdm  1/1     Running  0          87m\nprometheus-node-exporter-b4sb9                  1/1     Running  0          87m\nprometheus-node-exporter-n4z2g                  1/1     Running  0          87m\nprometheus-node-exporter-w7hn7                  1/1     Running  0          87m\nprometheus-pushgateway-56b65bcf5f-whx5t         1/1     Running  0          87m\nprometheus-server-779ffd445f-4llqr              2/2     Running  0          3m\n\n$ kubectl describe pod prometheus-server-779ffd445f-4llqr -nmonitoring\nOUTPUT OMITTED\n      Args:\n      --storage.tsdb.retention.time=30d\n      --config.file=/etc/config/prometheus.yml\n      --storage.tsdb.path=/data\n      --web.console.libraries=/etc/prometheus/console_libraries\n      --web.console.templates=/etc/prometheus/consoles\n      --web.enable-lifecycle\n```", "```py\n$ pulumi destroy\n\nPreviewing destroy (dev):\n\n     Type                            Name                            Plan\n -   pulumi:pulumi:Stack             pythonfordevops-gke-pulumi-dev  delete\n -   ├─ kubernetes:core:Service      ingress                         delete\n -   ├─ kubernetes:apps:Deployment   canary                          delete\n -   ├─ pulumi:providers:kubernetes  gke_k8s                         delete\n -   ├─ gcp:container:Cluster        gke-cluster                     delete\n -   └─ random:index:RandomString    password                        delete\n\nResources:\n    - 6 to delete\n\nDo you want to perform this destroy? yes\nDestroying (dev):\n\n     Type                            Name                            Status\n -   pulumi:pulumi:Stack             pythonfordevops-gke-pulumi-dev  deleted\n -   ├─ kubernetes:core:Service      ingress                         deleted\n -   ├─ kubernetes:apps:Deployment   canary                          deleted\n -   ├─ pulumi:providers:kubernetes  gke_k8s                         deleted\n -   ├─ gcp:container:Cluster        gke-cluster                     deleted\n -   └─ random:index:RandomString    password                        deleted\n\nResources:\n    - 6 deleted\n\nDuration: 3m18s\n```"]