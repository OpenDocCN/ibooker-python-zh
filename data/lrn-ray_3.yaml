- en: Chapter 4\. Reinforcement Learning with Ray RLlib
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 4 章：使用 Ray RLlib 的强化学习
- en: In the last chapter you’ve built a Reinforcement Learning (RL) environment,
    a simulation to play out some games, an RL algorithm, and the code to parallelize
    the training of the algorithm - all completely from scratch. It’s good to know
    how to do all that, but in practice the only thing you really want to do when
    training RL algorithms is the first part, namely specifying your custom environment,
    the “game”^([1](ch04.xhtml#idm44990030106000)) you want to play. Most of your
    efforts will then go into selecting the right algorithm, setting it up, finding
    the best parameters for the problem, and generally focusing on training a well-performing
    algorithm.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你从头开始构建了一个强化学习（RL）环境，一个模拟来执行一些游戏，一个 RL 算法，以及并行化训练算法的代码。知道如何做这一切是很好的，但实际上在训练
    RL 算法时，你真正想做的只是第一部分，即指定你的自定义环境，“游戏”^([1](ch04.xhtml#idm44990030106000))。然后大部分精力将会放在选择正确的算法上，设置它，为问题找到最佳参数，并且总体上专注于训练一个表现良好的算法。
- en: Ray RLlib is an industry-grade library for building RL algorithms at scale.
    You’ve seen a first example of the RLlib in [Chapter 1](ch01.xhtml#chapter_01)
    already, but in this chapter we’ll go into much more depth. The great thing about
    RLlib is that it’s a mature library for developers and comes with good abstractions
    to work with. As you will see, many of these abstractions you already know from
    the last chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Ray RLlib 是一个工业级库，用于大规模构建 RL 算法。你已经在 [第 1 章](ch01.xhtml#chapter_01) 中看到了 RLlib
    的一个示例，但在本章中我们将深入探讨。RLlib 的优点在于它是一个成熟的开发库，并提供了良好的抽象以便开发者使用。正如你将看到的，许多这些抽象你已经从上一章中了解过了。
- en: We start out this chapter by first giving you an overview of RLlib’s capabilities.
    Then we quickly revisit the maze game from [Chapter 3](ch03.xhtml#chapter_03)
    and show you how to tackle it both with the RLlib command line interface (CLI)
    and the RLlib Python API in a few lines of code. You’ll see how easy RLlib is
    to get started with before learning about its key concepts, such as RLlib environments,
    algorithms, and trainers.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先通过概述 RLlib 的能力来开始本章。然后我们迅速回顾一下 [第 3 章](ch03.xhtml#chapter_03) 中的迷宫游戏，并展示如何在几行代码中使用
    RLlib 命令行界面（CLI）和 RLlib Python API 来处理它。你将看到 RLlib 的易用性在开始学习它的关键概念之前是多么简单。
- en: We’ll also take a closer look at some advanced RL topics that are extremely
    useful in practice, but are not often properly supported in other RL libraries.
    For instance, you will learn how to create a learning curriculum for your RL agents
    so that they can learn simple scenarios first, before moving on to more complex
    ones. You will also see how RLlib deals with having multiple agents in a single
    environment, and how to leverage experience data that you’ve collected outside
    your current application to improve your agent’s performance.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将更详细地研究一些在实践中非常有用但通常在其他 RL 库中得不到良好支持的高级 RL 主题。例如，你将学习如何为你的 RL 代理创建一个学习课程，以便它们可以先学习简单的场景，然后再转向更复杂的场景。你还将看到
    RLlib 如何处理单个环境中有多个代理，并如何利用在当前应用之外收集的经验数据来提高你的代理性能。
- en: An Overview of RLlib
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RLlib 概述
- en: Before we dive into some examples, let’s take a quick overview of what RLlib
    is and what it can do. As part of the Ray ecosystem, RLlib inherits all the performance
    and scalability benefits of Ray. In particular, RLlib is distributed by default,
    so you can scale out your RL training to as many nodes as you want. Other RL libraries
    can potentially scale out experiments, but it’s usually not straightforward to
    do so.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入一些示例之前，让我们快速概述一下 RLlib 的功能。作为 Ray 生态系统的一部分，RLlib 继承了 Ray 的所有性能和可扩展性优势。特别是，RLlib
    默认是分布式的，因此你可以将 RL 训练扩展到任意数量的节点。其他 RL 库可能也能够扩展实验，但通常并不简单明了。
- en: Another benefit of being built on top of Ray is that RLlib integrates tightly
    with other Ray libraries. For instance, all RLlib algorithms can be tuned with
    Ray Tune, as we will see in [Chapter 5](ch05.xhtml#chapter_05), and you can seamlessly
    deploy your RLlib models with Ray Serve, as we will discuss in [Chapter 8](ch08.xhtml#chapter_08).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 Ray 构建的另一个好处是，RLlib 与其他 Ray 库紧密集成。例如，所有 RLlib 算法都可以通过 Ray Tune 进行调优，正如我们将在
    [第 5 章](ch05.xhtml#chapter_05) 中看到的，你也可以使用 Ray Serve 无缝部署你的 RLlib 模型，正如我们将在 [第
    8 章](ch08.xhtml#chapter_08) 中讨论的那样。
- en: What’s extremely useful is that RLlib works with both of the predominant deep
    learning frameworks at the time of this writing, namely PyTorch and TensorFlow.
    You can use either one of them as your backend and can easily switch between them,
    often by just changing one line of code. That’s a huge benefit, as companies are
    often locked into their underlying deep learning framework and can’t afford to
    switch to another system and rewrite their code.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 极其有用的是，RLlib在撰写本文时与两种主要的深度学习框架都兼容，即PyTorch和TensorFlow。您可以将其中任何一个用作后端，并且可以轻松地在它们之间切换，通常只需更改一行代码。这是一个巨大的优势，因为公司通常被锁定在其底层深度学习框架中，无法承担切换到另一个系统并重写其代码的成本。
- en: RLlib also has a track record of solving real-world problems and is a mature
    library used by many companies to bring their RL workloads to production. I often
    recommend RLlib to engineers, because its API tends to appeal to them. One of
    the reasons for that is that the RLlib API offers the right level of abstraction
    for many applications, while still being flexible enough to be extended, if necessary.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: RLlib还解决了许多现实世界的问题，并且是许多公司将其RL工作负载推向生产的成熟库。我经常向工程师推荐RLlib，因为它的API往往对他们具有吸引力。其中一个原因是RLlib
    API为许多应用程序提供了正确的抽象级别，同时仍然足够灵活，可以在必要时进行扩展。
- en: Apart from these more general benefits, RLlib has a lot of RL specific features
    that we will cover in this chapter. In fact, RLlib is so feature rich that it
    would deserve a book on its own, so we can only touch on some aspects of it here.
    For instance, RLlib has a rich library of advanced RL algorithms to choose from.
    In this chapter we will only focus on a few select ones, but you can track the
    growing list of options on the [RLlib algorithms page](https://docs.ray.io/en/latest/rllib-algorithms.xhtml).
    RLlib also has many options for specifying RL environments and is very flexible
    in handling them during training, see [for an overview of RLlib environments](https://docs.ray.io/en/latest/rllib-env.xhtml).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些更一般的好处之外，RLlib还具有许多RL特定的功能，我们将在本章中介绍。事实上，RLlib功能如此丰富，以至于它本身都应该有一本书来介绍，所以我们只能在这里涉及一些方面。例如，RLlib具有丰富的高级RL算法库可供选择。在本章中，我们将只专注于其中的一些选择，但您可以在[RLlib算法页面](https://docs.ray.io/en/latest/rllib-algorithms.xhtml)上跟踪不断增长的选项列表。RLlib还有许多用于指定RL环境的选项，并且在训练过程中处理这些选项非常灵活，详见[RLlib环境概述](https://docs.ray.io/en/latest/rllib-env.xhtml)。
- en: Getting Started With RLlib
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用RLlib入门
- en: 'To use RLlib, make sure you have installed it on your computer:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用RLlib，请确保已在计算机上安装了它：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As with every chapter in this book, if you don’t feel like following along by
    typing the code yourself, you can check out the accompanying [notebook for this
    chapter](https://github.com/maxpumperla/learning_ray/blob/main/notebooks/ch_04_rllib.ipynb).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 与本书中的每一章一样，如果您不想自己键入代码来跟进，可以查看本章的[附带笔记本](https://github.com/maxpumperla/learning_ray/blob/main/notebooks/ch_04_rllib.ipynb)。
- en: Every RL problem starts with having an interesting environment to investigate.
    In [Chapter 1](ch01.xhtml#chapter_01) we already looked at the classical pendulum
    balancing problem. Recall that we didn’t implement this pendulum environment,
    it came out of the box with RLlib.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 每个RL问题都始于拥有一个有趣的环境来研究。在[第1章](ch01.xhtml#chapter_01)中，我们已经看过了经典的摆动平衡问题。请回忆一下，我们没有实现这个摆动环境，它是由RLlib自带的。
- en: In contrast, in [Chapter 3](ch03.xhtml#chapter_03) we implemented a simple maze
    game on our own. The problem with this implementation is that we can’t directly
    use it with RLlib, or any other RL library for that matter. The reason is that
    in RL you have ubiquitous standards for environments. Your environments need to
    implement certain interfaces. The best known and most widely used library for
    RL environments is `gym`, an [open-source Python project](https://gym.openai.com/)
    from OpenAI.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，在[第3章](ch03.xhtml#chapter_03)中，我们自己实现了一个简单的迷宫游戏。这个实现的问题是我们不能直接将其与RLlib或任何其他RL库一起使用。原因是在RL中，您必须遵循环境的普遍标准。您的环境需要实现某些接口。用于RL环境的最知名且最广泛使用的库是`gym`，这是一个来自OpenAI的[开源Python项目](https://gym.openai.com/)。
- en: Let’s have a look at what `gym` is and how to make our maze `Environment` from
    the last chapter a `gym` environment compatible with RLlib.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看`gym`是什么，以及如何将上一章的迷宫`Environment`转换为与RLlib兼容的`gym`环境。
- en: Building A Gym Environment
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个Gym环境
- en: If you look at the well-documented and easy to read `gym.Env` environment interface
    [on GitHub](https://github.com/openai/gym/blob/master/gym/core.py#L17), you’ll
    notice that an implementation of this interface has two mandatory class variables
    and three methods that subclasses need to implement. You don’t have to check the
    source code, but I do encourage you to have a look. You might just be surprised
    by how much you already know about `gym` environments.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看过在 GitHub 上很好文档化且易于阅读的`gym.Env`环境接口 [链接](https://github.com/openai/gym/blob/master/gym/core.py#L17)，你会注意到这个接口的实现具有两个必需的类变量和三个子类需要实现的方法。你不必查看源代码，但我鼓励你看一看。你可能会对你已经了解到的关于`gym`环境的知识感到惊讶。
- en: 'In short, the interface of a gym environment looks like the following pseudo-code:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，gym 环境的接口看起来像以下的伪代码：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[![1](assets/1.png)](#co_reinforcement_learning_with_ray_rllib_CO1-1)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_reinforcement_learning_with_ray_rllib_CO1-1)'
- en: The `gym.Env` interface has an action and an observation space.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '`gym.Env`接口具有动作和观察空间。'
- en: '[![2](assets/2.png)](#co_reinforcement_learning_with_ray_rllib_CO1-2)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_reinforcement_learning_with_ray_rllib_CO1-2)'
- en: The `Env` can run a `step` and returns a tuple of observations, reward, done
    condition, and further info.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '`Env`可以运行`step`并返回一个包含观测、奖励、完成状态和其他信息的元组。'
- en: '[![3](assets/3.png)](#co_reinforcement_learning_with_ray_rllib_CO1-3)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_reinforcement_learning_with_ray_rllib_CO1-3)'
- en: An `Env` can `reset` itself, which will return the current observations
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`Env`可以通过`reset`方法重置自身，并返回当前的观测结果。
- en: '[![4](assets/4.png)](#co_reinforcement_learning_with_ray_rllib_CO1-4)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_reinforcement_learning_with_ray_rllib_CO1-4)'
- en: We can `render` an `Env` for different purposes, like for human display or as
    string representation.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以为不同目的`render`一个`Env`，比如用于人类显示或作为字符串表示。
- en: If you’ve read [Chapter 3](ch03.xhtml#chapter_03) carefully, you’ll notice that
    this is very similar to the interface of the maze `Environment` we built there.
    In fact, `gym` has a so-called `Discrete` space implemented in `gym.spaces`, which
    means that we can make our maze `Environment` a `gym.Env` as follows. We assume
    that you store this code in a file called `maze_gym_env.py` and that the code
    for the `Discrete` space and the `Environment` from [Chapter 3](ch03.xhtml#chapter_03)
    is either located at the top of that file (or is imported there).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细阅读了[第三章](ch03.xhtml#chapter_03)，你会注意到这与我们在那里构建的迷宫`Environment`接口非常相似。事实上，`gym`中有一个所谓的`Discrete`空间在`gym.spaces`中实现，这意味着我们可以将我们的迷宫`Environment`作为`gym.Env`来使用。我们假设你将这段代码存储在一个名为`maze_gym_env.py`的文件中，并且`Discrete`空间和来自[第三章](ch03.xhtml#chapter_03)的`Environment`代码要么位于文件的顶部（或者被导入到那里）。
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[![1](assets/1.png)](#co_reinforcement_learning_with_ray_rllib_CO2-1)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_reinforcement_learning_with_ray_rllib_CO2-1)'
- en: We override our own `Discrete` implementation with that of `gym`.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用我们自己的`Discrete`实现覆盖`gym`的实现。
- en: '[![2](assets/2.png)](#co_reinforcement_learning_with_ray_rllib_CO2-2)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_reinforcement_learning_with_ray_rllib_CO2-2)'
- en: We then simply make our `GymEnvironment` implement a `gym.Env`. The interface
    is essentially the same as before.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们简单地使我们的`GymEnvironment`实现`gym.Env`。接口本质上与之前相同。
- en: Of course, we could have made our original `Environment` implement `gym.Env`
    by simply inheriting from it in the first place. But the point is that the `gym.Env`
    interface comes up so naturally in the context of RL that it is a good exercise
    to implement it without having to resort to external libraries.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们本来可以通过直接从中继承的方式使我们的原始`Environment`实现`gym.Env`。但重点在于，在强化学习的背景下，`gym.Env`接口如此自然地显现出来，所以不依赖外部库实现它是个不错的练习。
- en: Notably, the `gym.Env` interface also comes with helpful utility functionality
    and many interesting example implementations. For instance, the `Pendulum-v1`
    environment we used in [Chapter 1](ch01.xhtml#chapter_01) is an example from `gym`,
    and there are [many other environments](https://gym.openai.com/envs) available
    to test your RL algorithms.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，`gym.Env`接口还提供了有用的实用功能和许多有趣的示例实现。例如，我们在[第一章](ch01.xhtml#chapter_01)中使用的`Pendulum-v1`环境就是`gym`的一个例子，还有[许多其他环境](https://gym.openai.com/envs)可用于测试你的强化学习算法。
- en: Running the RLlib CLI
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行 RLlib CLI
- en: Now that we have our `GymEnvironment` implemented as a `gym.Env`, here’s how
    you can use it with RLlib. You’ve seen the RLlib CLI in action in [Chapter 1](ch01.xhtml#chapter_01)
    before, but this time the situation is a bit different. In the first chapter we
    simply referenced the `Pendulum-v1` environment from by *name* in a YAML file,
    along with other RL training configuration. This time around we want to bring
    our own `gym` environment class, namely the class `GymEnvironment` that we defined
    in `maze_gym_env.py`. To specify this class in Ray RLlib, you use the full qualifying
    name of the class from where you’re referencing it, i.e. in our case `maze_gym_env.GymEnvironment`.
    If you had a more complicated Python project and your environment is stored in
    another module, you’d simply add the module name accordingly.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将我们的`GymEnvironment`实现为`gym.Env`，以下是您如何在RLlib中使用它。您在[第1章](ch01.xhtml#chapter_01)中已经看到了RLlib
    CLI的运行情况，但这次情况有些不同。在第一章中，我们仅通过*名称*在一个YAML文件中引用了`Pendulum-v1`环境，以及其他RL训练配置。这次我们想要使用我们自己的`gym`环境类，即我们在`maze_gym_env.py`中定义的`GymEnvironment`类。为了在Ray
    RLlib中指定这个类，您需要使用从引用它的地方的类的完整限定名称，即在我们的情况下是`maze_gym_env.GymEnvironment`。如果您有一个更复杂的Python项目，并且您的环境存储在另一个模块中，您只需相应地添加模块名。
- en: The following YAML file specifies the minimal configuration needed to train
    an RLlib algorithm on the `GymEnvironment` class. To align as closely as possible
    with our experiment from [Chapter 3](ch03.xhtml#chapter_03), in which we used
    Q-learning, we use `DQN` as the algorithm for our training `run`. Also, to make
    sure we can control the time of training, we set an explicit `stop` condition,
    namely by setting `timesteps_total` to `10000`.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 以下的YAML文件指定了在`GymEnvironment`类上训练RLlib算法所需的最小配置。为了尽可能地与我们在[第3章](ch03.xhtml#chapter_03)中的实验保持一致，我们使用Q-learning时，选择了`DQN`作为我们训练的算法。此外，为了确保我们可以控制训练的时间，我们设置了显式的停止条件，即通过将`timesteps_total`设置为`10000`。
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[![1](assets/1.png)](#co_reinforcement_learning_with_ray_rllib_CO3-1)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_reinforcement_learning_with_ray_rllib_CO3-1)'
- en: We specify the relative Python path to our environment class here.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里指定了相对Python路径到我们的环境类。
- en: '[![2](assets/2.png)](#co_reinforcement_learning_with_ray_rllib_CO3-2)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_reinforcement_learning_with_ray_rllib_CO3-2)'
- en: We store checkpoints of our model after each training iteration.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在每次训练迭代后存储我们模型的检查点。
- en: '[![3](assets/3.png)](#co_reinforcement_learning_with_ray_rllib_CO3-3)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_reinforcement_learning_with_ray_rllib_CO3-3)'
- en: We can also specify a stopping condition for training, here a maximum of 10000
    steps.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以为训练指定一个停止条件，这里是最大10000步。
- en: 'Assuming you store this configuration in a file called `maze.yml` you can now
    kick off an RLlib training run by running the following `train` command:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您将此配置存储在一个名为`maze.yml`的文件中，您现在可以通过运行以下`train`命令来启动RLlib训练运行：
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This single line of code basically takes care of everything we did in [Chapter 3](ch03.xhtml#chapter_03),
    but better. It runs a more sophisticated version of Q-Learning for us (DQN), takes
    care of scaling out to multiple workers under the hood, and even creates checkpoints
    of the algorithm automatically for us.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这一行代码基本上处理了我们在[第3章](ch03.xhtml#chapter_03)中所做的一切，但更好。它为我们运行了一个更复杂的Q-Learning版本（DQN），在底层处理多个工作者的扩展，并且甚至自动为我们创建算法的检查点。
- en: 'From the output of that training script you should see that Ray will write
    training results to a `logdir` directory located at `~/ray_results/maze_env`.
    Within that folder you’ll find another directory that starts with `DQN_maze_gym_env.GymEnvironment_`
    and contains both an identifier for this experiment (`0ae8d` in my case) and the
    current date and time. Within that directory you should find several other subdirectories
    starting with a `checkpoint` prefix. For the training run on my computer there
    are a total of `10` checkpoints available and we’re using the last one (`checkpoint_000010/checkpoint-10`)
    to evaluate our trained RLlib algorithm with it. With the folders and checkpoints
    generated on my machine the `rllib evaluate` command you can use reads as follows
    (adapt the checkpoint path to what you see on your machine):'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 从训练脚本的输出中，您应该看到Ray会将训练结果写入位于`~/ray_results/maze_env`的`logdir`目录。在那个文件夹中，您会找到另一个以`DQN_maze_gym_env.GymEnvironment_`开头的目录，其中包含这个实验的标识符（在我的情况下是`0ae8d`）和当前的日期和时间。在那个目录中，您应该会找到几个以`checkpoint`前缀开头的其他子目录。对于我电脑上的训练运行，总共有`10`个检查点可用，并且我们正在使用最后一个（`checkpoint_000010/checkpoint-10`）来评估我们训练过的RLlib算法。通过在我的机器上生成的文件夹和检查点，您可以使用`rllib
    evaluate`命令读取如下（根据您的机器调整检查点路径）：
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The algorithm used in `--run` and the environment specified with `--env` have
    to match the ones used in the training run, and we evaluate the trained algorithm
    for a total of 100 `steps`. This should lead to output of the following form:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`--run`中使用的算法和`--env`中指定的环境必须与训练运行中使用的算法和环境匹配，我们评估了经过训练的算法共计100步。这应该会导致以下形式的输出：'
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: It should not come as a big surprise that the `DQN` algorithm from RLlib gets
    the maximum reward of `1` for the simple maze environment we tasked it with every
    single time.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们给`DQN`算法设置的简单迷宫环境中，RLlib算法每次都能获得最大奖励`1`，这应该不会让人感到意外。
- en: Before moving on to the Python API of RLlib, it should be noted that the `train`
    and `evaluate` CLI commands can come in handy even for more complex environments.
    The YAML configuration can take any parameter the Python API would, so in that
    sense there is no limit for training your experiments on the command line^([2](ch04.xhtml#idm44990029718176)).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在转向RLlib的Python API之前，应该注意`train`和`evaluate`CLI命令即使对于更复杂的环境也很方便。YAML配置可以接受Python
    API会接受的任何参数，因此从这个意义上说，在命令行上训练您的实验没有限制^([2](ch04.xhtml#idm44990029718176))。
- en: Using the RLlib Python API
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用RLlib Python API
- en: Having said that, you will likely spend most of your time coding your reinforcement
    learning experiments in Python. In the end the RLlib CLI is merely a wrapper around
    the underlying Python library that we’re going to look at now.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，您可能会大部分时间在Python中编写您的强化学习实验。最终，RLlib CLI只是我们现在要看的基础Python库的一个包装器。
- en: To run RL workloads with RLlib from Python, your main entrypoint is that of
    the `Trainer` class. Specifically, for the algorithm of your choice you want to
    use the corresponding `Trainer` of it. In our case, since we decided to use Deep
    Q-Learning (DQN) for demonstration purposes, we’ll use the `DQNTrainer` class.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 要从Python运行RLlib的RL工作负载，您的主入口点是`Trainer`类。具体而言，对于您选择的算法，您希望使用相应的`Trainer`。在我们的案例中，由于我们决定使用Deep
    Q-Learning (DQN)进行演示，我们将使用`DQNTrainer`类。
- en: Training RLlib models
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练 RLlib 模型
- en: RLlib has good defaults for all its `Trainer` implementations, meaning that
    you can initialize them without having to tweak any configuration parameters for
    these trainers^([3](ch04.xhtml#idm44990029687616)). For instance, to generate
    a DQN trainer you can simply use `DQNTrainer(env=GymEnvironment)`. That said,
    it’s worth noting that RLlib trainers are highly configurable, as you will see
    in the following example. Specifically, we pass a `config` dictionary to the `Trainer`
    constructor and tell it to use four workers in total. What that means is that
    the `DQNTrainer` will spawn four Ray actors, each using a CPU kernel, to train
    our DQN algorithm in parallel.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: RLlib为其所有`Trainer`实现提供了良好的默认值，这意味着您可以初始化它们而不必调整这些训练器的任何配置参数^([3](ch04.xhtml#idm44990029687616))。例如，要生成一个DQN训练器，您可以简单地使用`DQNTrainer(env=GymEnvironment)`。但值得注意的是，RLlib训练器是高度可配置的，正如您将在以下示例中看到的。具体来说，我们向`Trainer`构造函数传递了一个`config`字典，并告诉它总共使用四个工作进程。这意味着`DQNTrainer`将生成四个Ray
    actor，每个使用一个CPU内核，以并行方式训练我们的DQN算法。
- en: 'After you’ve initialized your trainer with the `env` you want to train on,
    and pass in the `config` you want, you can simply call the `train` method. Let’s
    use this method to train the algorithm for ten iterations in total:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在您使用所需的`env`初始化您的训练器，并传入您想要的`config`之后，您可以简单地调用`train`方法。让我们使用这种方法来对算法进行总共十次迭代的训练：
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[![1](assets/1.png)](#co_reinforcement_learning_with_ray_rllib_CO4-1)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_reinforcement_learning_with_ray_rllib_CO4-1)'
- en: We use the `DQNTrainer` from RLlib to use Deep-Q-Networks (DQN) for training,
    using 4 parallel workers (Ray actors).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用RLlib中的`DQNTrainer`来使用Deep-Q-Networks (DQN)进行训练，使用4个并行工作者（Ray actors）。
- en: '[![2](assets/2.png)](#co_reinforcement_learning_with_ray_rllib_CO4-2)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_reinforcement_learning_with_ray_rllib_CO4-2)'
- en: Each `Trainer` has a complex default configuration.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 每个`Trainer`都有一个复杂的默认配置。
- en: '[![3](assets/3.png)](#co_reinforcement_learning_with_ray_rllib_CO4-3)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_reinforcement_learning_with_ray_rllib_CO4-3)'
- en: We can then simply call the `train` method to train the agent for ten iterations.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后可以简单地调用`train`方法来对代理进行十次迭代的训练。
- en: '[![4](assets/4.png)](#co_reinforcement_learning_with_ray_rllib_CO4-4)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_reinforcement_learning_with_ray_rllib_CO4-4)'
- en: With the `pretty_print` utility we can generate human-readable output of the
    training results.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`pretty_print`实用程序，我们可以生成训练结果的易读输出。
- en: Note that the number `10` training iterations has no special meaning, but it
    should be enough for the algorithm to learn to solve the maze problem adequately.
    The example just goes to show you that you have full control over the training
    process.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`10` 次训练迭代并没有特殊意义，但应足以使算法充分学习如何解决迷宫问题。这个例子只是向您展示，您完全可以控制训练过程。
- en: 'From printing the `config` dictionary, you can verify that the `num_workers`
    parameter is set to 4^([4](ch04.xhtml#idm44990029518816)). Similarly, If you run
    this training script, the `result` contains detailed information about the state
    of the `Trainer` and the training results that’s too verbose to put here. The
    part that’s most relevant for us right now is information about the reward of
    the algorithm, which hopefully indicates that the algorithm learned to solve the
    maze problem. You should see output of the following form:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 从打印的 `config` 字典中，您可以验证 `num_workers` 参数设置为 4^([4](ch04.xhtml#idm44990029518816))。同样地，如果您运行此训练脚本，则
    `result` 包含了关于 `Trainer` 状态和训练结果的详细信息，这些信息过于冗长，无法在此处列出。对我们目前最相关的部分来说，有关算法奖励的信息应当表明算法已学会解决迷宫问题。您应该看到以下形式的输出：
- en: '[PRE8]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In particular, this output shows that the minimum reward attained on average
    per episode is `1.0`, which in turn means that the agent always reached the goal
    and collected the maximum reward (`1.0`).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，这个输出显示每一集平均达到的最小奖励为 `1.0`，这意味着代理程序总是能够达到目标并收集到最大奖励（`1.0`）。
- en: Saving, loading, and evaluating RLlib models
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 保存、加载和评估 RLlib 模型
- en: Reaching the goal for this simple example isn’t too hard, but let’s see if evaluating
    the trained algorithm confirms that the agent can also do so in an optimal way,
    namely by only taking the minimum number of eight steps to reach the goal.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个简单示例来说，达到目标并不太难，但我们来看看评估训练过的算法是否确认代理也可以以最优的方式达到目标，即仅需最少的八步即可。
- en: 'To do so, we utilize another mechanism that you’ve already seen from the RLlib
    CLI, namely *checkpointing*. Creating model checkpoints is very useful to ensure
    you can recover your work in case of a crash, or simply to track training progress
    persistently. You can simply create a checkpoint of your RLlib trainers at any
    point in the training process by calling `trainer.save()`. Once you have a checkpoint,
    you can easily `restore` your `Trainer` with it. And evaluating a model is as
    simple as calling `trainer.evaluate(checkpoint)` with the checkpoint you created.
    Here’s how that looks like if you put it all together:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们利用了您已经从 RLlib CLI 中看到的另一种机制，即 *checkpointing*。创建模型检查点非常有用，以确保您在崩溃时可以恢复工作，或者简单地持续跟踪训练进度。您可以通过调用
    `trainer.save()` 在训练过程的任何时候创建 RLlib 训练器的检查点。一旦您有了检查点，您可以轻松地通过调用 `trainer.evaluate(checkpoint)`
    和您创建的检查点来 `restore` 您的 `Trainer`。整个过程如下所示：
- en: '[PRE9]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[![1](assets/1.png)](#co_reinforcement_learning_with_ray_rllib_CO5-1)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_reinforcement_learning_with_ray_rllib_CO5-1)'
- en: You can `save` trainers to create checkpoints.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 `save` 来创建训练器的检查点。
- en: '[![2](assets/2.png)](#co_reinforcement_learning_with_ray_rllib_CO5-2)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_reinforcement_learning_with_ray_rllib_CO5-2)'
- en: RLlib trainers can be evaluated at your checkpoints.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: RLlib 的训练器可以在您的检查点处进行评估。
- en: '[![3](assets/3.png)](#co_reinforcement_learning_with_ray_rllib_CO5-3)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_reinforcement_learning_with_ray_rllib_CO5-3)'
- en: And you can also `restore` any `Trainer` from a given checkpoint.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以从给定的检查点 `restore` 任何 `Trainer`。
- en: 'I should mention that you can also just call `trainer.evaluate()` without creating
    a checkpoint first, but it’s usually good practice to use checkpoints anyway.
    Looking at the output, we can now confirm that the trained RLlib algorithm did
    indeed converge to a good solution for the maze problem, as indicated by episodes
    of length `8` in evaluation:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我应该提到，您也可以直接调用 `trainer.evaluate()` 而无需先创建检查点，但通常最好还是使用检查点。现在看看输出，我们可以确认，经过训练的
    RLlib 算法确实收敛到了迷宫问题的良好解决方案，这是通过评估中的长度为 `8` 的集数表明的：
- en: '[PRE10]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Computing actions
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算动作
- en: 'RLlib trainers have much more functionality than just the `train`, `evaluate`,
    `save` and `restore` methods we’ve seen so far. For example, you can directly
    compute actions given the current state of an environment. In [Chapter 3](ch03.xhtml#chapter_03)
    we implemented episode rollouts by stepping through an environment and collecting
    rewards. We can easily do the same with RLlib for our `GymEnvironment` as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: RLlib 训练器拥有比我们迄今为止看到的`train`、`evaluate`、`save`和`restore`方法更多的功能。例如，您可以直接根据环境的当前状态计算动作。在[第三章](ch03.xhtml#chapter_03)中，我们通过在环境中进行步进并收集奖励来实现了回合轧制。我们可以通过以下方式轻松地为我们的`GymEnvironment`使用
    RLlib 实现相同的功能：
- en: '[PRE11]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[![1](assets/1.png)](#co_reinforcement_learning_with_ray_rllib_CO6-1)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_reinforcement_learning_with_ray_rllib_CO6-1)'
- en: To compute actions for given `observations` use `compute_single_action`.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 要为给定的`observations`计算动作，请使用`compute_single_action`。
- en: In case you should need to compute many actions at once, not just a single one,
    you can use the `compute_actions` method instead, which takes dictionaries of
    observations as input and produces dictionaries of actions with the same dictionary
    keys as output.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要一次计算多个动作，而不仅仅是单个动作，您可以使用`compute_actions`方法，该方法以字典形式的观察结果作为输入，并产生具有相同字典键的动作字典作为输出。
- en: '[PRE12]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Accessing policy and model states
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 访问策略和模型状态
- en: Remember that each reinforcement learning algorithm is based on a *policy* that
    chooses next actions given the current observations the agent has of the environment.
    Each policy is in turn based on an underlying *model*.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，每个强化学习算法都基于一个*策略*，该策略根据智能体对环境的当前观察选择下一个动作。每个策略又基于一个底层*模型*。
- en: In the case of vanilla Q-Learning that we discussed in [Chapter 3](ch03.xhtml#chapter_03)
    the model was a simple look-up table of state-action values, also called Q-values.
    And that policy used this model for predicting next actions in case it decided
    to *exploit* what the model had learned so far, or to *explore* the environment
    with random actions otherwise.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 就我们在[第三章](ch03.xhtml#chapter_03)中讨论的基本 Q 学习而言，模型是一个简单的状态-动作值查找表，也称为 Q 值。该策略根据所学到的模型决定是否*利用*，或者通过随机动作*探索*环境来预测下一个动作。
- en: When using Deep Q-Learning, the underlying model of the policy is a neural network
    that, loosely speaking, maps observations to actions. Note that for choosing next
    actions in an environment, we’re ultimately we’re not interested in the concrete
    values of the approximated Q-values, but rather in the *probabilities* of taking
    each action. The probability distribution over all possible actions is called
    an *action distribution*. In the maze example we’re using here as a running examples
    we can move up, right, down or left, so in that case an action distribution is
    a vector of four probabilities, one for each action.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用深度 Q 学习时，策略的基础模型是一个神经网络，它大致将观察结果映射到动作。请注意，为了选择环境中的下一个动作，我们最终对近似的 Q 值的具体值不感兴趣，而是对每个动作的*概率*感兴趣。所有可能动作的概率分布称为*动作分布*。在我们这里使用的迷宫示例中，我们可以向上、向右、向下或向左移动，因此在这种情况下，动作分布是一个包含四个概率的向量，每个概率对应一个动作。
- en: 'To make things concrete, let’s have a look at how you access policies and models
    in RLlib:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了具体说明，让我们看看如何在 RLlib 中访问策略和模型：
- en: '[PRE13]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Both `policy` and `model` have many useful methods to explore. In this example
    we use `get_weights` to inspect the parameters of the model underlying the policy
    (which are called “weights” by standard convention).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`policy`和`model`都有许多有用的方法可供探索。在这个示例中，我们使用`get_weights`来检查策略底层模型的参数（标准惯例称为“权重”）。'
- en: 'To convince you that there is in fact not just one model at play here, but
    in fact a collection of four models that we trained on separate Ray workers, we
    can access all the workers we used in training - and then ask each worker’s policy
    for their weights like this:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让您相信这里实际上不只是一个模型在起作用，而实际上是我们在单独的 Ray 工作器上训练的四个模型集合，我们可以访问我们在训练中使用的所有工作器，然后像这样询问每个工作器的策略权重：
- en: '[PRE14]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In this way, you can access every method available on a `Trainer` instance on
    each of your workers. In principle, you can use this to *set* model parameters
    as well, or otherwise configure your workers. RLlib workers are ultimately Ray
    actors, so you can alter and manipulate them in almost any way you like.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，您可以在每个工作器上访问`Trainer`实例的每个可用方法。原则上，您也可以使用这种方法*设置*模型参数，或以其他方式配置您的工作器。RLlib
    工作器最终是 Ray actor，因此您几乎可以按照任何方式更改和操作它们。
- en: 'We haven’t talked about the specific implementation of Deep Q-Learning used
    in `DQNTrainer`, but the `model` used is in fact a bit more complex than what
    I’ve described so far. Every RLlib `model` obtained from a policy has a `base_model`
    that has a neat `summary` method to describe itself:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尚未讨论在`DQNTrainer`中使用的具体深度Q学习实现，但是实际上使用的`model`比我到目前为止描述的要复杂一些。从任何从策略获得的RLlib`model`都有一个`base_model`，它具有一个`summary`方法来描述自身：
- en: '[PRE15]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As you can see from the output below, this model takes in our `observations`.
    The shape of these `observations` is a bit strangely annotated as `[(None, 25)]`,
    but essentially this just means we have the expected `5*5` maze grid values correctly
    encoded. The model follows up with two so-called `Dense` layers and predicts a
    single value at the end.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在下面的输出中所见，此模型接收我们的`observations`。这些`observations`的形状有点奇怪地标注为`[(None, 25)]`，但本质上这只是表示我们正确编码了预期的`5*5`迷宫网格值。该模型接着使用两个所谓的`Dense`层，并在最后预测一个单一值。
- en: '[PRE16]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Note that it’s perfectly possible to customize this model for your RLlib experiments.
    If your environment is quite complex and has a big observation space, for instance,
    you might need a bigger model to capture that complexity. However, doing so requires
    in-depth knowledge of the underlying neural network framework (in this case TensorFlow),
    which we don’t assume you have^([5](ch04.xhtml#idm44990029092880)).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您完全可以为您的RLlib实验定制此模型。例如，如果您的环境非常复杂并且具有较大的观察空间，您可能需要一个更大的模型来捕捉该复杂性。但是，这需要对底层神经网络框架（在这种情况下为TensorFlow）有深入的了解，我们并不假设您已具备^([5](ch04.xhtml#idm44990029092880))。
- en: Next, let’s see if we can take some observations from our environment and pass
    them to the `model` we just extracted from our `policy`. This part is a bit technically
    involved, because models are a bit more difficult to access directly in RLlib.
    The reason is that normally you would only interface with a `model` through your
    `policy`, which takes care of preprocessing the observations and passing them
    to the model (among other things).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看我们是否可以从环境中获取一些观察结果，并将它们传递给我们刚刚从我们的策略中提取出来的`model`。这部分内容有点技术性，因为在RLlib中直接访问模型有点困难。原因是通常情况下，您只会通过您的`policy`与一个`model`进行接口，它会负责预处理观察结果并将其传递给模型（以及其他操作）。
- en: 'Luckily, we can simply access the preprocessor used by the policy, `transform`
    the observations from our environment, and then pass them to the model:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们可以简单地访问策略使用的预处理器，从我们的环境中`transform`观察结果，然后将其传递给模型：
- en: '[PRE17]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[![1](assets/1.png)](#co_reinforcement_learning_with_ray_rllib_CO7-1)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_reinforcement_learning_with_ray_rllib_CO7-1)'
- en: You can use `get_processor` to access the preprocessor used by the policy.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`get_processor`来访问策略使用的预处理器。
- en: '[![2](assets/2.png)](#co_reinforcement_learning_with_ray_rllib_CO7-2)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_reinforcement_learning_with_ray_rllib_CO7-2)'
- en: For any `observations` obtained from your `env` you can use `transform` them
    to the format expected by the model. Note that we need to reshape the observations,
    too.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 对于从您的`env`获取的任何`observations`，您都可以使用`transform`将其转换为模型所期望的格式。请注意，我们也需要重新调整这些观察结果的形状。
- en: '[![3](assets/3.png)](#co_reinforcement_learning_with_ray_rllib_CO7-3)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_reinforcement_learning_with_ray_rllib_CO7-3)'
- en: You get the model output by using the `from_batch` method of the model on a
    preprocessed observation dictionary.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用模型的`from_batch`方法，在预处理后的观察字典上获取模型输出。
- en: 'Having computed our `model_output`, we can now both access the Q-values, as
    well as the action distribution of the model for this output like this:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 计算了我们的`model_output`之后，我们现在既可以访问Q值，也可以访问模型对于该输出的动作分布，如下所示：
- en: '[PRE18]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[![1](assets/1.png)](#co_reinforcement_learning_with_ray_rllib_CO8-1)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_reinforcement_learning_with_ray_rllib_CO8-1)'
- en: The `get_q_value_distributions` method is specific to `DQN` models only.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_q_value_distributions`方法仅适用于`DQN`模型。'
- en: '[![2](assets/2.png)](#co_reinforcement_learning_with_ray_rllib_CO8-2)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_reinforcement_learning_with_ray_rllib_CO8-2)'
- en: By accessing `dist_class` we get the policy’s action distribution class.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 通过访问`dist_class`，我们可以获取策略的动作分布类。
- en: '[![3](assets/3.png)](#co_reinforcement_learning_with_ray_rllib_CO8-3)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_reinforcement_learning_with_ray_rllib_CO8-3)'
- en: Action distributions can be sampled from.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 可以从动作分布中进行抽样。
- en: Configuring RLlib Experiments
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置RLlib实验
- en: Now that you’ve seen the basic Python training API of RLlib in an example, let’s
    take a step back and discuss in more depth how to configure and run RLlib experiments.
    By now you know that your `Trainer` takes a `config` argument, which so far we’ve
    only used to set the number of Ray workers to 4.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经在示例中看到了RLlib的基本Python训练API，让我们退一步，更深入地讨论如何配置和运行RLlib实验。到目前为止，您知道您的`Trainer`接受一个`config`参数，到目前为止，我们只用它来设置Ray工作器的数量为4。
- en: If you want to alter the behaviour of your RLlib training run, the way to do
    this is to change the `config` argument of your `Trainer`. This is at the same
    time relatively simple, as you can add configuration properties quickly, and a
    bit tricky, as you have to know which key-words the `config` dictionary expects.
    Finding and tweaking the right configuration properties becomes easier once you
    have a good grasp of what’s available and what to expect.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想要更改RLlib训练运行的行为，方法是更改您的`Trainer`的`config`参数。这既相对简单，因为您可以快速添加配置属性，也有点棘手，因为您必须知道`config`字典期望哪些关键字。一旦您掌握了可用的内容和期望的内容，找到并调整正确的配置属性就会变得更容易。
- en: RLlib configuration splits in two parts, namely algorithm-specific and common
    configuration. We’ve used `DQN` as our algorithm in the examples so far, which
    has certain properties that are only available to this choice^([6](ch04.xhtml#idm44990028855872)).
    Algorithm-specific configuration only becomes more relevant once you’ve settled
    on an algorithm and want to squeeze it for performance, but in practice RLlib
    provides you with good defaults to get started. You can look up configuration
    arguments in the [API reference for RLlib algorithms](https://docs.ray.io/en/latest/rllib-algorithms.xhtml).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: RLlib的配置分为两部分，即特定于算法和通用配置。到目前为止，在示例中我们使用了`DQN`作为我们的算法，它具有某些仅适用于此选择的属性^([6](ch04.xhtml#idm44990028855872))。一旦您选择了算法并希望为性能进行优化，特定于算法的配置就变得更加相关，但在实践中，RLlib为您提供了很好的默认值以便开始使用。您可以在[RLlib算法的API参考](https://docs.ray.io/en/latest/rllib-algorithms.xhtml)中查找配置参数。
- en: The common configuration of algorithms can be further split into the following
    types.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的通用配置可以进一步分为以下类型。
- en: Resource Configuration
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源配置
- en: 'Whether you use Ray RLlib locally or on a cluster, you can specify the resources
    used for the training process. Here are the most important options to consider:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您是在本地使用还是在集群上使用Ray RLlib，您都可以指定用于训练过程的资源。以下是考虑的最重要选项：
- en: '`num_gpus`'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '`num_gpus`'
- en: Specify the number of GPUs to use for training. It’s important to check whether
    your algorithm of choice supports GPUs first. This value can also be fractional.
    For example, if using four rollout workers in `DQN` (`num_workers` = 4), you can
    set `num_gpus=0.25` to pack all four workers on the same GPU, so that all trainers
    benefit from the potential speedup.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 指定用于训练的GPU数量。首先检查您选择的算法是否支持GPU是很重要的。此值也可以是分数。例如，如果在`DQN`中使用四个回滚工作器（`num_workers`=
    4），则可以设置`num_gpus=0.25`以将所有四个工作器放置在同一个GPU上，以便所有训练器都能从潜在的加速中受益。
- en: '`num_cpus_per_worker`'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`num_cpus_per_worker`'
- en: Set the number of CPUs to use for each worker.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 设置每个工作器使用的CPU数。
- en: Debugging and Logging Configuration
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调试和日志配置
- en: Debugging your applications is crucial for any project, and machine learning
    is no exception. RLlib allows you to configure the way it logs information and
    how you can access it.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何项目来说，调试您的应用程序都至关重要，机器学习也不例外。RLlib允许您配置它记录信息的方式以及您如何访问它。
- en: '`log_level`'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '`log_level`'
- en: Set the level of logging to use. This can be either `DEBUG`, `INFO`, `WARN`,
    or `ERROR` and defaults to `WARN`. You should experiment with the different levels
    to see what suits your needs best in practice.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 设置要使用的日志记录级别。这可以是`DEBUG`、`INFO`、`WARN`或`ERROR`，默认为`WARN`。您应该尝试不同的级别，看看在实践中哪个最适合您的需求。
- en: '`callbacks`'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`callbacks`'
- en: You can specify custom *callback functions* to be called at various points during
    training. We will take a closer look at this topic in [Chapter 5](ch05.xhtml#chapter_05).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以指定自定义*回调函数*，在训练过程中的各个时点调用。我们将在[第五章](ch05.xhtml#chapter_05)中更详细地讨论这个话题。
- en: '`ignore_worker_failures`'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`ignore_worker_failures`'
- en: For testing it might be useful to ignore worker failures by setting this property
    to `True` (defaults to `False`).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 对于测试来说，将此属性设置为`True`可能会忽略工作器的失败（默认为`False`）。
- en: '`logger_config`'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`logger_config`'
- en: You can specify a custom logger configuration, passed in as a nested dictionary.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以指定一个自定义的日志配置，作为一个嵌套字典传递。
- en: Rollout Worker and Evaluation Configuration
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回滚工作器和评估配置
- en: Of course, you can also specify how many workers are used for rollouts during
    training and evaluation.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你也可以指定训练和评估期间用于回放的工作者数量。
- en: '`num_workers`'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`num_workers`'
- en: You’ve seen this one already. It’s used to specify the number of Ray workers
    to use.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经见过这个选项了。它用于指定要使用的 Ray 工作者数量。
- en: '`num_envs_per_worker`'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '`num_envs_per_worker`'
- en: Specify the number of environments to evaluate per worker. This setting allows
    you to “batch” evaluation of environments. In particular, if your models take
    a long time to evaluate, grouping environments like this can speed up training.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 指定每个工作者要评估的环境数量。这一设置允许你对环境进行“批量”评估。特别是，如果你的模型评估时间很长，将环境分组可以加速训练。
- en: '`create_env_on_driver`'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '`create_env_on_driver`'
- en: If you’ve set `num_workers` at least to 1, then the driver process does not
    need to create an environment, since there are rollout workers for that. If you
    set this property to `True` you create an additional environment on the driver.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你至少设置了 `num_workers` 为 1，那么驱动进程不需要创建环境，因为有回放工作者。如果你将此属性设置为 `True`，你将在驱动器上创建一个额外的环境。
- en: '`explore`'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '`explore`'
- en: Set to `True` by default, this property allows you to turn off exploration,
    for instance during evaluation of your algorithms.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 默认设置为 `True`，此属性允许你关闭探索，例如在评估算法时。
- en: '`evaluation_num_workers`'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`evaluation_num_workers`'
- en: Specify the number of parallel evaluation workers to use, which defaults to
    0.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 指定要使用的并行评估工作者数量，默认值为 0。
- en: Environment Configuration
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 环境配置
- en: '`env`'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '`env`'
- en: Specify the environment you want to use for training. This can either be a string
    of an environment known to Ray RLlib, such as any `gym` environment, or the class
    name of a custom environment you’ve implemented. There’s also a way to *register*
    your environments so that you can refer to them by name, but this requires using
    Ray Tune. We will learn about this feature in [Chapter 5](ch05.xhtml#chapter_05).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 指定你想用于训练的环境。这可以是 Ray RLlib 已知的环境的字符串，如任何 `gym` 环境，或者你实现的自定义环境的类名。还有一种方法是 *注册*
    你的环境，以便你可以通过名称引用它们，但这需要使用 Ray Tune。我们将在 [第 5 章](ch05.xhtml#chapter_05) 中学习这个功能。
- en: '`observation_space` and `action_space`'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`observation_space` 和 `action_space`'
- en: You can specify the observation and action spaces of your environment. If you
    don’t specify them, they will be inferred from the environment.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以指定环境的观察空间和动作空间。如果不指定，它们将从环境中推断出来。
- en: '`env_config`'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '`env_config`'
- en: You can optionally specify a dictionary of configuration options for your environment
    that will be passed to the environment constructor.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以选择指定一个环境配置选项的字典，这些选项将传递给环境构造函数。
- en: '`render_env`'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '`render_env`'
- en: '`False` by default, this property allows you to turn on rendering of the environment,
    which requires you to implement the `render` method of your environment.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 默认设置为 `False`，此属性允许你打开环境渲染，这需要你实现环境的 `render` 方法。
- en: Note that we left out many available configuration options for each of the types
    we listed. On top of that, there’s a class of common configuration options to
    modify the behavior of the RL training procedure, like modifying the underlying
    model to use. These properties are the most important in a sense, while at the
    same time require the most specific knowledge of reinforcement learning. For this
    introduction to RLlib, we can’t go into any more details. But the good news is
    that if you’re a regular user of RL software, you will have no trouble identifying
    the relevant training configuration options.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们省略了列出的每种类型的许多可用配置选项。此外，还有一类通用配置选项，用于修改强化学习训练过程的行为，比如修改要使用的底层模型。这些属性在某种意义上是最重要的，同时也需要对强化学习有最具体的了解。对于这篇关于
    RLlib 的介绍，我们无法再深入讨论更多细节。但好消息是，如果你是 RL 软件的常规用户，你将很容易识别相关的训练配置选项。
- en: Working With RLlib Environments
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 RLlib 环境
- en: So far we’ve only introduced you to `gym` environments, but RLlib supports a
    wide variety of environments. After giving you a quick overview of all available
    options, we’ll show you two concrete examples of advanced RLlib environments in
    action.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只向你介绍了 `gym` 环境，但 RLlib 支持多种环境。在快速概述所有可用选项后，我们将向你展示两个具体的高级 RLlib 环境示例。
- en: An Overview of RLlib Environments
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RLlib 环境概述
- en: All available RLlib Environments extend a common `BaseEnv` class. If you want
    to work with several copies of the same `gym.Env` environment, you can use RLlib’s
    `VectorEnv` wrapper. Vectorized environments are useful, but also straightforward
    generalizations of what you’ve seen already. The two other types of environments
    available in RLlib are more interesting and deserve more attention.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 所有可用的 RLlib 环境都扩展自一个通用的 `BaseEnv` 类。如果您想要使用多个相同的 `gym.Env` 环境副本，可以使用 RLlib 的
    `VectorEnv` 包装器。矢量化环境非常有用，但也是对您已经见过的内容的直接推广。RLlib 提供的另外两种类型的环境更有趣，值得更多的关注。
- en: The first is called `MultiAgentEnv`, which allows you to train a model with
    *multiple agents*. Working with multiple agents can be tricky, because you have
    to take care of defining your agents within your environment with a suitable interface
    and account for the fact that each agent might have a completely different way
    of interacting with its environment. What’s more is that agents might interact
    with each other, and have to respect each other’s actions. In more advanced setting
    there might even be a *hierarchy* of agents, which explicitly depend on each other.
    In short, running multi-agent RL experiments is difficult, and we’ll see how RLlib
    handles this in the next example.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个称为 `MultiAgentEnv`，允许您训练具有 *多个智能体* 的模型。与多个智能体一起工作可能会很棘手，因为您必须确保在环境中定义您的智能体具有适当的接口，并考虑每个智能体可能有完全不同的与其环境交互方式。更重要的是，智能体可能会相互交互，并且必须尊重彼此的行动。在更高级的设置中，甚至可能存在智能体的
    *层次结构*，这些结构明确依赖于彼此。简而言之，运行多智能体强化学习实验很困难，我们将在下一个示例中看到 RLlib 如何处理这一问题。
- en: The other type of environment we will look at is called `ExternalEnv`, which
    can be used to connect external simulators to RLlib. For instance, imagine our
    simple maze problem from earlier was a simulation of an actual robot navigating
    a maze. It might not be suitable in such scenarios to co-locate the robot (or
    its simulation, implemented in a different software stack) with RLlib’s learning
    agents. To account for that, RLlib provides you with a simple client-server architecture
    for communicating with external simulators, which allows communication over a
    REST API.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看看的另一种环境类型称为 `ExternalEnv`，它可以用于将外部模拟器连接到 RLlib。例如，想象一下我们早期简单迷宫问题是一个实际机器人导航迷宫的模拟。在这种情况下，将机器人（或其模拟，实现在不同的软件堆栈中）与
    RLlib 的学习代理共同定位可能并不适合。为此，RLlib 为您提供了一个简单的客户端-服务器架构，用于通过 REST API 与外部模拟器进行通信。
- en: 'In figure [Figure 4-1](#fig_envs) we summarize all available RLlib environments
    for you:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 [Figure 4-1](#fig_envs) 中，我们总结了所有可用的 RLlib 环境：
- en: '![RLlib envs](assets/rllib_envs.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![RLlib envs](assets/rllib_envs.png)'
- en: Figure 4-1\. An overview of all available RLlib environments
  id: totrans-178
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 4-1\. 所有可用的 RLlib 环境概述
- en: Working with Multiple Agents
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多智能体工作
- en: The basic idea of defining multi-agent environments in RLlib is simple. Whatever
    you define as a single value in a gym environment, you now define as a dictionary
    with values for each agent, and each agent has its unique key. Of course, the
    details are a little more complicated than that in practice. But once you have
    defined an environment hosting several agents, what’s necessary is to define how
    these agents should learn.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在 RLlib 中定义多智能体环境的基本思想很简单。无论您在 gym 环境中定义为单个值的内容，现在您都可以定义为具有每个智能体值的字典，并且每个智能体都有其独特的键。当然，实际上细节比这更复杂。但一旦您定义了一个承载多个智能体的环境，必须定义这些智能体应该如何学习。
- en: 'In a single-agent environment there’s one agent and one policy to learn. In
    a multi-agent environment there are multiple agents that might map to one or several
    policies. For instance, if you have a group of homogenous agents in your environment,
    then you could define a single policy for all of them. If they all *act* the same
    way, then their behavior can be learnt the same way. In contrast, you might have
    situations with heterogeneous agents in which each of them has to learn a separate
    policy. Between these two extremes, there’s a spectrum of possibilities displayed
    in figure [Figure 4-2](#fig_policy_mapping):'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在单智能体环境中，有一个智能体和一个策略需要学习。在多智能体环境中，可能会有多个智能体映射到一个或多个策略。例如，如果您的环境中有一组同质智能体，则可以为所有智能体定义一个单一策略。如果它们都以相同的方式
    *行动*，那么它们的行为可以以相同的方式学习。相反，您可能会遇到异构智能体的情况，其中每个智能体都必须学习单独的策略。在这两个极端之间，图 [Figure 4-2](#fig_policy_mapping)
    中显示了一系列可能性：
- en: '![Mapping envs](assets/mapping_envs.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![Mapping envs](assets/mapping_envs.png)'
- en: Figure 4-2\. Mapping agents to policies in multi-agent reinforcement learning
    problems
  id: totrans-183
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-2。在多代理强化学习问题中将代理映射到策略
- en: 'We continue to use our maze game as a running example for this chapter. This
    way you can check for yourself how the interfaces differ in practice. So, to put
    the ideas we just outlined into code, let’s define a multi-agent version of the
    `GymEnvironment` class. Our `MultiAgentEnv` class will have precisely two agents,
    which we encode in a Python dictionary called `agents`, but in principle this
    works with any number of agents. We start be initializing and resetting our new
    environment:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续使用迷宫游戏作为本章的运行示例。这样，您可以亲自检查接口在实践中的差异。因此，为了将我们刚刚概述的思想转化为代码，让我们定义一个`GymEnvironment`类的多代理版本。我们的`MultiAgentEnv`类将精确地有两个代理，我们将它们编码在一个名为`agents`的Python字典中，但原则上，这也适用于任意数量的代理。我们开始通过初始化和重置我们的新环境：
- en: '[PRE19]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[![1](assets/1.png)](#co_reinforcement_learning_with_ray_rllib_CO9-1)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_reinforcement_learning_with_ray_rllib_CO9-1)'
- en: We now have two seekers with `(0, 4)` and `(4, 0)` starting positions in an
    `agents` dictionary.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有两个搜索者在一个名为`agents`的字典中具有`(0, 4)`和`(4, 0)`的起始位置。
- en: '[![2](assets/2.png)](#co_reinforcement_learning_with_ray_rllib_CO9-2)'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_reinforcement_learning_with_ray_rllib_CO9-2)'
- en: For the `info` object we’re using agent IDs as keys.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`info`对象，我们使用代理ID作为键。
- en: '[![3](assets/3.png)](#co_reinforcement_learning_with_ray_rllib_CO9-3)'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_reinforcement_learning_with_ray_rllib_CO9-3)'
- en: Action and observation spaces stay exactly the same as before.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 动作和观察空间与之前完全相同。
- en: '[![4](assets/4.png)](#co_reinforcement_learning_with_ray_rllib_CO9-4)'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_reinforcement_learning_with_ray_rllib_CO9-4)'
- en: Observations are now per-agent dictionaries.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 观察现在是每个代理的字典。
- en: Notice that compared to the single-agent situation we had to modify neither
    action nor observation spaces, since we’re using two essentially identical agents
    here that can use the same spaces. In more complex situations you’d have to account
    for the fact that the actions and observations might look different for some agents.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，与单一代理情况相比，我们既不需要修改动作空间也不需要修改观察空间，因为我们在这里使用的是两个本质上相同的代理，它们可以使用相同的空间。在更复杂的情况下，您需要考虑这样一个事实，即对于某些代理，动作和观察可能看起来不同。
- en: To continue, let’s generalize our helper methods `get_observation`, `get_reward`,
    and `is_done` to work with multiple agents. We do this by passing in an `action_id`
    to their signatures and handling each agent the same way as before.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们将我们的辅助方法`get_observation`、`get_reward`和`is_done`泛化为适用于多个代理的方法。我们通过将`action_id`传递给它们的签名，并像以前一样处理每个代理来实现这一点。
- en: '[PRE20]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[![1](assets/1.png)](#co_reinforcement_learning_with_ray_rllib_CO10-1)'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_reinforcement_learning_with_ray_rllib_CO10-1)'
- en: Getting a specific agent from its ID.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 根据其ID获取特定代理。
- en: '[![2](assets/2.png)](#co_reinforcement_learning_with_ray_rllib_CO10-2)'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_reinforcement_learning_with_ray_rllib_CO10-2)'
- en: Redefining each helper method to work per-agent.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 重新定义每个辅助方法，以便每个代理都可以使用。
- en: Next, to port the `step` method to our multi-agent setup, you have to know that
    `MultiAgentEnv` now expects the `action` passed to a `step` to be a dictionary
    with keys corresponding to the agent IDs, too. We define a step by looping through
    all available agents and acting on their behalf^([7](ch04.xhtml#idm44990028443184)).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，要将`step`方法移植到我们的多代理设置中，您必须知道`MultiAgentEnv`现在期望`step`传递给一个动作是一个字典，其键也是代理ID。我们通过循环遍历所有可用代理并代表它们行动来定义一个步骤^([7](ch04.xhtml#idm44990028443184))。
- en: '[PRE21]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[![1](assets/1.png)](#co_reinforcement_learning_with_ray_rllib_CO11-1)'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_reinforcement_learning_with_ray_rllib_CO11-1)'
- en: Actions in a `step` are now per-agent dictionaries.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '`step`中的动作现在是每个代理的字典。'
- en: '[![2](assets/2.png)](#co_reinforcement_learning_with_ray_rllib_CO11-2)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_reinforcement_learning_with_ray_rllib_CO11-2)'
- en: After applying the correct action for each seeker, we set the correct states
    of all `agents`.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在为每个搜索者应用正确的动作之后，我们设置所有`agents`的正确状态。
- en: '[![3](assets/3.png)](#co_reinforcement_learning_with_ray_rllib_CO11-3)'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_reinforcement_learning_with_ray_rllib_CO11-3)'
- en: '`observations`, `rewards`, and `dones` are also dictionaries with agent IDs
    as keys.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '`observations`、`rewards` 和 `dones` 也是以代理 ID 为键的字典。'
- en: '[![4](assets/4.png)](#co_reinforcement_learning_with_ray_rllib_CO11-4)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_reinforcement_learning_with_ray_rllib_CO11-4)'
- en: Additionally, RLlib needs to know when all agents are done.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，RLlib需要知道所有代理何时完成。
- en: The last step is to modify rendering the environment, which we do by denoting
    each agent by its ID when printing the maze to the screen.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是修改环境的渲染方式，我们通过在屏幕上打印迷宫时用其ID表示每个代理来实现这一点。
- en: '[PRE22]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Randomly rolling out an episode until *one* of the agents reaches the goal
    can for instance be done by the following code:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，可以通过以下代码随机执行一个情节，直到*一个*代理到达目标：
- en: '[PRE23]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Note how we have to make sure to pass two random samples by means of a Python
    dictionary into the `step` method, and how we check if any of the agents are `done`
    yet. We use this `break` condition for simplicity, as it’s highly unlikely that
    both seekers find their way to the goal at the same time by chance. But of course
    we’d like both agents to complete the maze eventually.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们如何确保通过Python字典的方式传递两个随机样本到`step`方法，并检查任何代理是否已经完成了`done`。为了简单起见，我们使用了这个`break`条件，因为两个寻找者同时偶然找到目标的可能性非常小。但是当然，我们希望两个代理最终能够完成迷宫。
- en: In any case, equipped with our `MultiAgentMaze`, training an RLlib `Trainer`
    works *exactly* the same way as before.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，配备我们的`MultiAgentMaze`，训练一个RLlib的`Trainer`的方法与以前完全相同。
- en: '[PRE24]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This covers the most simple case of training a multi-agent reinforcement learning
    (MARL) problem. But if you remember what we said earlier, when using multiple
    agents there’s always a mapping between agents and policies. By not specifying
    such a mapping, both of our seekers were implicitly assigned to the same policy.
    This can be changed by modifying the `multiagent` dictionary in our trainer `config`
    as follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这涵盖了训练多智能体强化学习（MARL）问题的最简单情况。但是，如果你还记得我们之前说的，当使用多个代理时，代理和策略之间总是有一个映射关系。如果不指定这样的映射，我们的两个寻找者都会隐式地分配到相同的策略。可以通过修改我们训练器配置中的`multiagent`字典来改变这一点，如下所示：
- en: Example 4-1\.
  id: totrans-219
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-1\.
- en: '[PRE25]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[![1](assets/1.png)](#co_reinforcement_learning_with_ray_rllib_CO12-1)'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_reinforcement_learning_with_ray_rllib_CO12-1)'
- en: We first define multiple policies for our agents.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先为我们的代理定义多个策略。
- en: '[![2](assets/2.png)](#co_reinforcement_learning_with_ray_rllib_CO12-2)'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_reinforcement_learning_with_ray_rllib_CO12-2)'
- en: Each agent can then be mapped to a policy with a custom `policy_mapping_fn`.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 每个代理随后可以通过自定义的`policy_mapping_fn`映射到一个策略。
- en: As you can see, running multi-agent RL experiments is a first-class citizen
    of RLlib, and there’s a lot more that could be said about it. The support of MARL
    problems is one of RLlib’s strongest features.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，运行多智能体RL实验是RLlib的一等公民，并且有很多更多的内容可以讨论。支持MARL问题是RLlib最强大的功能之一。
- en: Working with Policy Servers and Clients
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用策略服务器和客户端
- en: For the last example in this section on environments, let’s assume our original
    `GymEnvironment` can only be simulated on a machine that can’t run RLlib, for
    instance because it doesn’t have enough resources available. We can run the environment
    on a `PolicyClient` that can ask a respective *server* for suitable next actions
    to apply to the environment. The server, in turn, does not know about the environment.
    It only knows how to ingest input data from a `PolicyClient`, and it is responsible
    for running all RL related code, in particular it defines an RLlib `config` object
    and trains a `Trainer`.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本节中环境的最后一个例子，假设我们原始的`GymEnvironment`只能在一个不能运行RLlib的机器上模拟，例如因为它没有足够的可用资源。我们可以在一个`PolicyClient`上运行环境，该客户端可以向一个相应的*服务器*询问适当的下一步动作以应用于环境。反过来，服务器不知道环境的情况。它只知道如何从`PolicyClient`摄取输入数据，并负责运行所有RL相关的代码，特别是定义一个RLlib的`config`对象并训练一个`Trainer`。
- en: Defining a server
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义一个服务器
- en: 'Let’s start by defining the server-side of such an application first. We define
    a so-called `PolicyServerInput` that runs on `localhost` on port `9900`. This
    policy input is what the client will provide later on. With this `policy_input`
    defined as `input` to our trainer configuration, we can define yet another `DQNTrainer`
    to run on the server:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先定义这种应用程序的服务器端。我们定义一个名为`PolicyServerInput`的内容，在`localhost`的`9900`端口上运行。这个策略输入是客户端稍后将提供的。将这个`policy_input`定义为我们训练器配置的`input`，我们可以定义另一个在服务器上运行的`DQNTrainer`：
- en: '[PRE26]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[![1](assets/1.png)](#co_reinforcement_learning_with_ray_rllib_CO13-1)'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_reinforcement_learning_with_ray_rllib_CO13-1)'
- en: The `policy_input` function returns a `PolicyServerInput` object running on
    localhost on port 9900.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '`policy_input`函数返回一个在localhost上运行在端口9900上的`PolicyServerInput`对象。'
- en: '[![2](assets/2.png)](#co_reinforcement_learning_with_ray_rllib_CO13-2)'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_reinforcement_learning_with_ray_rllib_CO13-2)'
- en: We explicitly set the `env` to `None` because this server does not need one.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们明确将`env`设置为`None`，因为这个服务器不需要环境。
- en: '[![3](assets/3.png)](#co_reinforcement_learning_with_ray_rllib_CO13-3)'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_reinforcement_learning_with_ray_rllib_CO13-3)'
- en: To make this work, we need to feed our `policy_input` into the experiment’s
    `input`.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 要使这个工作起来，我们需要将我们的 `policy_input` 输入到实验的 `input` 中。
- en: 'With this `trainer` defined ^([8](ch04.xhtml#idm44990027406144)), we can now
    start a training session on the server like so:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个`trainer`的定义 ^([8](ch04.xhtml#idm44990027406144))，我们现在可以像这样在服务器上开始一个训练会话：
- en: '[PRE27]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[![1](assets/1.png)](#co_reinforcement_learning_with_ray_rllib_CO14-1)'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_reinforcement_learning_with_ray_rllib_CO14-1)'
- en: We train for a maximum of 100 iterations and store checkpoints after each iteration.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最多训练 100 次迭代，并在每次迭代后保存检查点。
- en: '[![2](assets/2.png)](#co_reinforcement_learning_with_ray_rllib_CO14-2)'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_reinforcement_learning_with_ray_rllib_CO14-2)'
- en: If training surpasses 10.000 time steps, we stop the training.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 如果训练超过 10,000 个时间步，我们停止训练。
- en: In what follows we assume that you store the last two code snippets in a file
    called `policy_server.py`. If you want to, you can now start this policy server
    on your local machine by running `python policy_server.py` in a terminal.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的内容中，我们假设你将最后两个代码片段存储在一个名为 `policy_server.py` 的文件中。如果愿意，你现在可以在本地终端上运行 `python
    policy_server.py` 来启动这个策略服务器。
- en: Defining a client
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义一个客户端
- en: Next, to define the corresponding client-side of the application, we define
    a `PolicyClient` that connects to the server we just started. Since we can’t assume
    that you have several computers at home (or available in the cloud), contrary
    to what we said prior, we will start this client on the same machine. In other
    words, the client will connect to `http://localhost:9900`, but if you can run
    the server on different machine, you could replace `localhost` with the IP address
    of that machine, provided it’s available in the network.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，为了定义应用程序的相应客户端端，我们定义一个`PolicyClient`，它连接到刚刚启动的服务器。由于我们不能假设您家里或云端有多台计算机，与之前相反，我们将在同一台机器上启动此客户端。换句话说，客户端将连接到
    `http://localhost:9900`，但如果您可以在不同的机器上运行服务器，可以用该机器的 IP 地址替换 `localhost`，只要它在网络中可用。
- en: Policy clients have a fairly lean interface. They can trigger the server to
    start or end an episode, get next actions from it, and log reward information
    to it (that it would otherwise not have). With that said, here’s how you define
    such a client.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 策略客户端具有相当简洁的接口。它们可以触发服务器开始或结束一个 episode，从中获取下一个动作，并将奖励信息记录到其中（否则不会有）。说到这里，这是如何定义这样一个客户端的方法。
- en: '[PRE28]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[![1](assets/1.png)](#co_reinforcement_learning_with_ray_rllib_CO15-1)'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_reinforcement_learning_with_ray_rllib_CO15-1)'
- en: We start a policy client on the server address with `remote` inference mode.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在服务器地址上启动一个策略客户端，以 `remote` 推理模式运行。
- en: '[![2](assets/2.png)](#co_reinforcement_learning_with_ray_rllib_CO15-2)'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_reinforcement_learning_with_ray_rllib_CO15-2)'
- en: Then we tell the server to start an episode.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们告诉服务器开始一个 episode。
- en: '[![3](assets/3.png)](#co_reinforcement_learning_with_ray_rllib_CO15-3)'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_reinforcement_learning_with_ray_rllib_CO15-3)'
- en: For given environment observations, we can get the next action from the server.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的环境观察，我们可以从服务器获取下一个动作。
- en: '[![4](assets/4.png)](#co_reinforcement_learning_with_ray_rllib_CO15-4)'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_reinforcement_learning_with_ray_rllib_CO15-4)'
- en: It’s mandatory for the `client` to log reward information to the server.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '`client`必须向服务器记录奖励信息。'
- en: '[![5](assets/5.png)](#co_reinforcement_learning_with_ray_rllib_CO15-5)'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_reinforcement_learning_with_ray_rllib_CO15-5)'
- en: If a certain condition is reached, we can stop the client process.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 如果达到某个特定条件，我们可以停止客户端进程。
- en: '[![6](assets/6.png)](#co_reinforcement_learning_with_ray_rllib_CO15-6)'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](assets/6.png)](#co_reinforcement_learning_with_ray_rllib_CO15-6)'
- en: If the environment is `done`, we have to inform the server about episode completion.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 如果环境处于 `done` 状态，我们必须通知服务器 episode 已经完成。
- en: Assuming you store this code under `policy_client.py` and start it by running
    `python policy_client.py`, then the server that we started earlier will start
    learning with environment information solely obtained from the client.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你把这段代码存储在 `policy_client.py` 中，并通过运行 `python policy_client.py` 启动它，之后我们之前启动的服务器将仅仅从客户端获取环境信息进行学习。
- en: Advanced Concepts
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级概念
- en: So far we’ve been working with simple environments that were easy enough to
    tackle with the most basic RL algorithm settings in RLlib. Of course, in practice
    you’re not always that lucky and might have to come up with other ideas to tackle
    harder environments. In this section we’re going to introduce a slightly harder
    version of the maze environment and discuss some advanced concepts that help you
    solve this environment.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在处理简单的环境，这些环境足够简单，可以用RLlib中最基本的RL算法设置来解决。当然，在实践中，您并不总是那么幸运，可能需要想出其他方法来解决更难的环境问题。在本节中，我们将介绍迷宫环境的稍微更难的版本，并讨论一些帮助您解决这个环境的高级概念。
- en: Building an Advanced Environment
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建高级环境
- en: 'Let’s make our maze `GymEnvironment` a bit more challenging. First, we increase
    its size from a `5x5` to a `11x11` grid. Then we introduce obstacles in the maze
    that the agent can pass through, but only by incurring a penalty, a negative reward
    of `-1`. This way our seeker agent will have to learn to avoid obstacles, while
    still finding the goal. Also, we randomize the starting position of the agent.
    All of this makes the RL problem harder to solve. Let’s have a look at the initialization
    of this new `AdvancedEnv` first:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们让我们的迷宫`GymEnvironment`变得更具挑战性。首先，我们将其大小从`5x5`增加到`11x11`网格。然后我们在迷宫中引入障碍物，代理可以穿过，但会受到惩罚，即负奖励`-1`。这样，我们的搜索代理将不得不学会避开障碍物，同时找到目标。另外，我们随机化代理的起始位置。所有这些都使得RL问题更难解决。让我们首先看看这个新`AdvancedEnv`的初始化：
- en: '[PRE29]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[![1](assets/1.png)](#co_reinforcement_learning_with_ray_rllib_CO16-1)'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_reinforcement_learning_with_ray_rllib_CO16-1)'
- en: We can now set the `seeker` position upon initialization.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以在初始化时设置`seeker`的位置。
- en: '[![2](assets/2.png)](#co_reinforcement_learning_with_ray_rllib_CO16-2)'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_reinforcement_learning_with_ray_rllib_CO16-2)'
- en: We introduce `punish_states` as obstacles for the agent.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`punish_states`引入作为代理的障碍物。
- en: Next, when resetting the environment, we want to make sure to reset the agent’s
    position to a random state. We also increase the positive reward for reaching
    the goal to `5`, to offset the negative reward for passing through an obstacle
    (which will happen a lot before the RL trainer picks up on the obstacle locations).
    Balancing rewards like this is a crucial task in calibrating your RL experiments.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在重置环境时，我们希望确保将代理的位置重置为随机状态。我们还将达到目标的正奖励增加到`5`，以抵消通过障碍物时的负奖励（在RL训练器探测到障碍位置之前，这种情况会经常发生）。像这样平衡奖励在校准RL实验中是至关重要的任务。
- en: '[PRE30]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: There are many other ways you could make this environment harder, like making
    it much bigger, introducing a negative reward for every step the agent takes in
    a certain direction, or punishing the agent for trying to walk off the grid. By
    now you should understand the problem setting well enough to customize the maze
    yourself further.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他方法可以使这个环境变得更难，比如将其扩大许多倍，对代理在某个方向上的每一步引入负奖励，或者惩罚代理试图走出网格。到现在为止，您应该已经足够了解问题设置，可以自定义迷宫了。
- en: While you might have success training this environment, this is a good opportunity
    to introduce some advanced concepts that you can apply to other RL problems.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管您可能已成功训练了这个环境，但这是一个介绍一些高级概念的好机会，您可以应用到其他RL问题中。
- en: Applying Curriculum Learning
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用课程学习
- en: One of the most interesting features of RLlib is to provide a `Trainer` with
    a *curriculum* to learn from. What that means is that, instead of letting the
    trainer learn from arbitrary environment setups, we cherry pick states that are
    much easier to learn from and then slowly but surely introduce more difficult
    states. Building a learning curriculum this way is a great way to make your experiments
    converge on solutions quicker. The only thing you need to apply curriculum learning
    is a view on which starting states are easier than others. For many environments
    that can actually be a challenge, but it’s easy to come up with a simple curriculum
    for our advanced maze. Namely, the distance of the seeker from the goal can be
    used as a measure of difficulty. The distance measure we’ll use for simplicity
    is the sum of the absolute distance of both seeker coordinates from the goal to
    define a `difficulty`.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: RLlib 最有趣的特性之一是为 `Trainer` 提供一个*课程*来学习。这意味着，我们不是让训练器从任意的环境设置中学习，而是挑选出容易学习的状态，然后慢慢地引入更困难的状态。通过这种方式构建学习课程是使你的实验更快收敛于解决方案的好方法。唯一需要应用课程学习的是一个关于哪些起始状态比其他状态更容易的观点。对于许多环境来说，这实际上可能是一个挑战，但对于我们的高级迷宫来说，很容易想出一个简单的课程。即，追逐者距离目标的距离可以用作难度的度量。我们将使用的距离度量是追逐者坐标与目标之间的绝对距离之和，以定义一个`difficulty`。
- en: 'To run curriculum learning with RLlib, we define a `CurriculumEnv` that extends
    both our `AdvancedEnv` and a so-called `TaskSettableEnv` from RLLib. The interface
    of `TaskSettableEnv` is very simple, in that you only have to define how get the
    current difficulty (`get_task`) and how to set a required difficulty (`set_task`).
    Here’s the full definition of this `CurriculumEnv`:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 RLlib 中运行课程学习，我们定义一个 `CurriculumEnv`，它扩展了我们的 `AdvancedEnv` 和来自 RLLib 的所谓
    `TaskSettableEnv`。`TaskSettableEnv` 的接口非常简单，你只需定义如何获取当前难度（`get_task`）以及如何设置所需难度（`set_task`）。下面是这个
    `CurriculumEnv` 的完整定义：
- en: '[PRE31]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'To use this environment for curriculum learning, we need to define a curriculum
    function that tells the trainer when and how to set the task difficulty. We have
    many options here, but we use a schedule that simply increases the difficulty
    by one every `1000` time steps trained:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 要在课程学习中使用这个环境，我们需要定义一个课程函数，告诉训练器何时以及如何设置任务难度。我们在这里有很多选择，但我们使用的是一个简单的调度，每 `1000`
    个训练时间步骤增加一次难度：
- en: '[PRE32]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: To test this curriculum function, we need to add it to our RLlib trainer `config`,
    namely by setting the `env_task_fn` property to our `curriculum_fn`. Note that
    before training a `DQNTrainer` for `15` iterations, we also set an `output` folder
    in our config. This will store experience data of our training run to the specified
    temp folder.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 要测试这个课程函数，我们需要将它添加到我们的 RLlib 训练器 `config` 中，即通过将 `env_task_fn` 属性设置为我们的 `curriculum_fn`。请注意，在对
    `DQNTrainer` 进行`15`次迭代训练之前，我们还在配置中设置了一个 `output` 文件夹。这将把我们的训练运行的经验数据存储到指定的临时文件夹中。
- en: '[PRE33]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Running this trainer, you should see how the task difficulty increases over
    time, thereby giving the trainer easy examples to start with so that in can learn
    from them and progress to more difficult tasks as it progresses.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这个训练器，你应该看到任务难度随着时间的推移而增加，从而为训练器提供了容易开始的例子，以便它从中学习，并在进展中转移到更困难的任务。
- en: Curriculum learning is a great technique to be aware of and RLlib allows you
    to easily incorporate it into your experiments through the curriculum API we just
    discussed.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 课程学习是一个很棒的技术，RLlib 允许你通过我们刚刚讨论过的课程 API 轻松地将其纳入你的实验中。
- en: Working with Offline Data
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用离线数据
- en: In our previous curriculum learning example we stored training data to a temporary
    folder. What’s interesting is that you already know from [Chapter 3](ch03.xhtml#chapter_03)
    that in Q-learning you can collect experience data first, and decide when to use
    it in a training step later. This separation of data collection and training opens
    up many possibilities. For instance, maybe you have a good heuristic that can
    solve your problem in an imperfect, yet reasonable manner. Or you have records
    of human interaction with your environment, demonstrating how to solve the problem
    by example.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的课程学习示例中，我们将训练数据存储到了一个临时文件夹中。有趣的是，你已经从[第 3 章](ch03.xhtml#chapter_03)知道，在
    Q 学习中，你可以先收集经验数据，然后决定在后续的训练步骤中何时使用它。数据收集与训练的分离打开了许多可能性。例如，也许你有一个良好的启发式算法，可以以一种不完美但合理的方式解决你的问题。或者你有人与环境互动的记录，演示了如何通过示例解决问题。
- en: The topic of collecting experience data for later training is often discussed
    as working with *offline data*. It’s called offline, as it’s not directly generated
    by a policy interacting online with the environment. Algorithms that don’t rely
    on training on their own policy output are called off-policy algorithms, and Q-Learning,
    respectively DQN, is just one such example. Algorithms that don’t share this property
    are accordingly called on-policy algorithms. In other words, off-policy algorithms
    can be used to train on offline data^([9](ch04.xhtml#idm44990026103120)).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 为了以后的训练收集经验数据的主题通常被称为*离线数据*。之所以称为离线数据，是因为它不是直接由与环境在线交互的策略生成的。不依赖于自己策略输出进行训练的算法称为离策略算法，Q学习和DQN就是其中的一个例子。不具备这种属性的算法相应地称为在策略算法。换句话说，离策略算法可用于训练离线数据^([9](ch04.xhtml#idm44990026103120))。
- en: To use the data we stored in the `/tmp/env-out` folder before, we can create
    a new training configuration that takes this folder as `input`. Note how we set
    `exploration` to `False` in the following configuration, since we simply want
    to exploit the data previously collected for training - the algorithm will not
    explore according to its own policy.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用我们之前存储在`/tmp/env-out`文件夹中的数据，我们可以创建一个新的训练配置，将该文件夹作为`input`。请注意，在以下配置中，我们将`exploration`设置为`False`，因为我们只想利用以前收集的数据进行训练
    - 该算法不会根据自己的策略进行探索。
- en: '[PRE34]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Using this `input_config` for training works exactly as before, which we demonstrate
    by training an agent for `10` iterations and evaluating it:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个`input_config`进行训练的方式与以前完全相同，我们通过训练一个代理进行`10`次迭代并评估它来证明这一点：
- en: '[PRE35]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Note that we called the trainer `imitation_trainer`. That’s because this training
    procedure intends to *imitate* the behavior reflected in the data we collected
    before. This type of learning by demonstration in RL is therefore often called
    *imitation learning* or *behavior cloning*.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们称之为`imitation_trainer`的训练器。这是因为这种训练过程旨在*模仿*我们之前收集的数据中反映的行为。因此，在RL中这种通过示范学习的类型通常被称为*模仿学习*或*行为克隆*。
- en: Other Advanced Topics
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他高级主题
- en: 'Before concluding this chapter, let’s have a look at a few other advanced topics
    that RLlib has to offer. You’ve already seen how flexible RLlib is, from working
    with a range of different environments, to configuring your experiments, training
    on a curriculum, or running imitation learning. To give you a taste of what else
    is possible, you can also do the following things with RLlib:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在结束本章之前，让我们看看RLlib还提供了一些其他高级主题。您已经看到RLlib有多么灵活，从使用各种不同环境到配置您的实验、在课程上进行训练或运行模仿学习。为了让您了解还有什么可能性，您还可以通过RLlib做以下几件事情：
- en: You can completely customize the models and policies used under the hood. If
    you’ve worked with deep learning before, you know how important it can be to have
    a good model architecture in place. In RL this is often not as crucial as in supervised
    learning, but still a vital part of running advanced experiments successfully.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以完全定制在幕后使用的模型和策略。如果您之前有过深度学习的经验，您就知道在RL中拥有一个良好的模型架构是多么重要。在RL中，这通常不像在监督学习中那样关键，但仍然是成功运行高级实验的重要部分。
- en: You can change the way your observations are preprocessed by providing custom
    preprocessors. For our simple maze examples there was nothing to preprocess, but
    when working with image or video data, preprocessing is often a crucial step.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以通过提供自定义预处理器来更改观察数据的预处理方式。对于我们简单的迷宫示例，没有什么需要预处理的，但在处理图像或视频数据时，预处理通常是一个关键步骤。
- en: In our `AdvancedEnv` we introduced states to avoid. Our agents had to learn
    to do this, but RLlib has a feature to automatically avoid them through so-called
    *parametric action spaces*. Loosely speaking, what you can do is to “mask out”
    all undesired actions from the action space for each point in time.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在我们的`AdvancedEnv`中，我们介绍了要避免的状态。我们的代理需要学会这样做，但是RLlib有一个功能可以通过所谓的*参数化动作空间*自动避免它们。粗略地说，您可以在每个时间点上从动作空间中“屏蔽掉”所有不需要的动作。
- en: In some cases it can also be necessary to have variable observation spaces,
    which is also fully supported by RLlib.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在某些情况下，具有可变观察空间可能也是必要的，RLlib也完全支持这一点。
- en: We only briefly touched on the topic of offline data. Rllib has a full-fledged
    Python API for reading and writing experience data that can be used in various
    situation.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们只是简要触及了离线数据的主题。Rllib具有完整的Python API，用于读写经验数据，可在各种情况下使用。
- en: Lastly, I want to stress again that we have solely worked with `DQNTrainer`
    here for simplicity, but RLlib has an impressive range of training algorithms.
    To name just one, the MARWIL algorithm is a complex hybrid algorithm with which
    you can run imitation learning from offline data, while also mixing in regular
    training on data generated “online”.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我要再次强调，我们这里仅仅使用了`DQNTrainer`来简化操作，但 RLlib 拥有令人印象深刻的训练算法范围。举一个例子，MARWIL 算法是一种复杂的混合算法，您可以使用它从离线数据中进行模仿学习，同时还可以混合“在线”生成的常规训练数据。
- en: Summary
  id: totrans-300
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: To summarize, you’ve seen a selection of interesting features of RLlib in this
    chapter. We covered training multi-agent environments, working with offline data
    generated by another agent, setting up a client-server architecture to split simulations
    from RL training, and using curriculum learning to specify increasingly difficult
    tasks.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，您在本章中看到了 RLlib 的一些有趣特性。我们涵盖了训练多智能体环境，处理由另一个智能体生成的离线数据，设置客户端-服务器架构以将模拟与
    RL 训练分离，并使用课程学习来指定越来越困难的任务。
- en: We’ve also given you a quick overview of the main concepts underlying RLlib,
    and how to use its CLI and Python API. In particular, we’ve shown how to configure
    your RLlib trainers and environments to your needs. As we’ve only covered a small
    part of the possibilities of RLlib, we encourage you to read its [documentation
    and explore its API](https://docs.ray.io/en/master/rllib/index.xhtml).
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还为您快速概述了 RLlib 的主要概念，以及如何使用其 CLI 和 Python API。特别是，我们展示了如何根据您的需求配置 RLlib 训练器和环境。由于我们只覆盖了
    RLlib 的一小部分可能性，我们鼓励您阅读其[文档并探索其 API](https://docs.ray.io/en/master/rllib/index.xhtml)。
- en: In the next chapter you’re going to learn how to tune the hyperparameters of
    your RLlib models and policies with Ray Tune.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，您将学习如何使用 Ray Tune 调整 RLlib 模型和策略的超参数。
- en: ^([1](ch04.xhtml#idm44990030106000-marker)) We simply used a simple game to
    illustrate the process of RL. There is a multitude of interesting industry applications
    of RL that are not games.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch04.xhtml#idm44990030106000-marker)) 我们仅仅使用了一个简单的游戏来说明强化学习的过程。强化学习有许多有趣的工业应用，并非仅限于游戏。
- en: ^([2](ch04.xhtml#idm44990029718176-marker)) We should mention that the RLlib
    CLI uses Ray Tune under the hood, among many other things for checkpointing models.
    You will learn more about this integration in [Chapter 5](ch05.xhtml#chapter_05)
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch04.xhtml#idm44990029718176-marker)) 我们需要提及的是，RLlib CLI 在内部使用 Ray Tune，用于诸如模型的检查点等许多其他功能。您将在[第
    5 章](ch05.xhtml#chapter_05)中更多了解这种集成。
- en: ^([3](ch04.xhtml#idm44990029687616-marker)) Of course, configuring your models
    is a crucial part of RL experiments. We will discuss configuration of RLlib trainers
    in more detail in the next section.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch04.xhtml#idm44990029687616-marker)) 当然，配置你的模型是强化学习实验的关键部分。我们将在下一节详细讨论
    RLlib 训练器的配置。
- en: ^([4](ch04.xhtml#idm44990029518816-marker)) If you set `num_workers` to `0`,
    only the local worker on the head node will be created, and all training is done
    there. This is particularly useful for debugging, as no additional Ray actor processes
    are spawned.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch04.xhtml#idm44990029518816-marker)) 如果将`num_workers`设置为`0`，只会创建本地节点上的本地工作进程，并且所有训练都在此进行。这在调试时非常有用，因为不会生成额外的
    Ray actor 进程。
- en: ^([5](ch04.xhtml#idm44990029092880-marker)) If you want to learn more about
    customizing your RLlib models, check out [the guide to custom models](https://docs.ray.io/en/latest/rllib-models.xhtml#custom-models-implementing-your-own-forward-logic)
    on the Ray documentation.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch04.xhtml#idm44990029092880-marker)) 如果您想了解更多关于定制化您的 RLlib 模型的信息，请查看
    Ray 文档中关于[自定义模型的指南](https://docs.ray.io/en/latest/rllib-models.xhtml#custom-models-implementing-your-own-forward-logic)。
- en: '^([6](ch04.xhtml#idm44990028855872-marker)) For the experts, our DQNs are dueling
    double Q-learning models via the `"dueling": True` and `"double_q": True` default
    arguments, for example.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '^([6](ch04.xhtml#idm44990028855872-marker)) 对于专家来说，我们的 DQNs 是通过`"dueling":
    True`和`"double_q": True`的默认参数实现的双 Q 学习模型。'
- en: ^([7](ch04.xhtml#idm44990028443184-marker)) Note how this can lead to issues
    like deciding which agent gets to act first. In our simple maze problem the order
    of actions is irrelevant, but in more complex scenarios this becomes a crucial
    part of modeling the RL problem correctly.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch04.xhtml#idm44990028443184-marker)) 注意，这可能会导致诸如决定哪个代理程序首先行动之类的问题。在我们的简单迷宫问题中，行动顺序不重要，但在更复杂的场景中，这成为正确建模
    RL 问题的关键部分。
- en: ^([8](ch04.xhtml#idm44990027406144-marker)) For technical reasons we do have
    to specify observation and action spaces here, which might not be necessary in
    future iterations of this project, as it leaks environment information. Also note
    that we need to set `input_evaluation` to an empty list to make this server work.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch04.xhtml#idm44990027406144-marker)) 由于技术原因，我们必须在这里指定观察和行动空间，这在项目的未来迭代中可能不再需要，因为它泄漏了环境信息。同时请注意，我们需要将`input_evaluation`设置为空列表才能使此服务器正常工作。
- en: ^([9](ch04.xhtml#idm44990026103120-marker)) Note that RLlib has a wide range
    of on-policy algorithms like `PPO` as well.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch04.xhtml#idm44990026103120-marker)) 请注意，RLlib还具有诸如`PPO`之类的广泛的在线策略算法。
