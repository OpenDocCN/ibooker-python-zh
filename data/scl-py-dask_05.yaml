- en: Chapter 5\. Dask’s Collections
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 5 章。Dask 的集合
- en: So far you’ve seen the basics of how Dask is built as well as how Dask uses
    these building blocks to support data science with DataFrames. This chapter explores
    where Dask’s bag and array interfaces—often overlooked, relative to DataFrames—are
    more appropriate. As mentioned in [“Hello Worlds”](ch02.xhtml#hello_worlds), Dask
    bags implement common functional APIs, and Dask arrays implement a subset of NumPy
    arrays.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经看到了 Dask 是如何构建的基础知识，以及 Dask 如何利用这些构建模块支持数据科学与数据帧。本章探讨了 Dask 的 bag 和
    array 接口的地方——相对于数据帧而言，这些接口经常被忽视更合适。正如在[“Hello Worlds”](ch02.xhtml#hello_worlds)中提到的，Dask
    袋实现了常见的函数式 API，而 Dask 数组实现了 NumPy 数组的一个子集。
- en: Tip
  id: totrans-2
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Understanding partitioning is important for understanding collections. If you
    skipped [“Partitioning/Chunking Collections”](ch03.xhtml#basic_partitioning),
    now is a good time to head back and take a look.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 理解分区对于理解集合非常重要。如果您跳过了[“分区/分块集合”](ch03.xhtml#basic_partitioning)，现在是回头看一看的好时机。
- en: Dask Arrays
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dask 数组
- en: Dask arrays implement a subset of the NumPy ndarray interface, making them ideal
    for porting code that uses NumPy to run on Dask. Much of your understanding from
    the previous chapter with DataFrames carries over to Dask arrays, as well as much
    of your understanding of ndarrays.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 数组实现了 NumPy ndarray 接口的一个子集，使它们非常适合用于将使用 NumPy 的代码移植到 Dask 上运行。你从上一章对数据帧的理解大部分都适用于
    Dask 数组，以及你对 ndarrays 的理解。
- en: Common Use Cases
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 常见用例
- en: 'Some common use cases for Dask arrays include:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 数组的一些常见用例包括：
- en: Large-scale imaging and astronomy data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大规模成像和天文数据
- en: Weather data
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 天气数据
- en: Multi-dimensional data
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多维数据
- en: Similar to Dask DataFrames and pandas, if you wouldn’t use an nparray for the
    problem at a smaller scale, a Dask array may not be the right solution.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Dask 数据帧和 pandas 类似，如果在较小规模问题上不使用 nparray，那么 Dask 数组可能不是正确的解决方案。
- en: When Not to Use Dask Arrays
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不适用 Dask 数组的情况
- en: If your data fits in memory on a single computer, using Dask arrays is unlikely
    to give you many benefits over nparrays, especially compared to local accelerators
    like Numba. Numba is well suited to vectorizing and parallelizing local tasks
    with and without Graphics Processing Units (GPUs). You can use Numba with or without
    Dask, and we’ll look at how to further speed up Dask arrays using Numba in [Chapter 10](ch10.xhtml#ch10).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的数据适合在单台计算机的内存中，使用 Dask 数组不太可能比 nparrays 带来太多好处，特别是与像 Numba 这样的本地加速器相比，Numba
    适用于使用和不使用图形处理单元（GPUs）的本地任务的向量化和并行化。你可以使用 Numba 与或不使用 Dask，并且我们将看看如何在[第 10 章](ch10.xhtml#ch10)中进一步加速
    Dask 数组使用 Numba。
- en: Dask arrays, like their local counterpart, require that data is all of the same
    type. This means that they cannot be used for semi-structured or mixed-type data
    (think strings and ints).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 数组与其本地对应物一样，要求数据都是相同类型的。这意味着它们不能用于半结构化或混合类型数据（例如字符串和整数）。
- en: Loading/Saving
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载/保存
- en: As with Dask DataFrames, loading and writing functions start with `to_` or `read_`
    as the prefixes. Each format has its own configuration, but in general, the first
    positional argument is the location of the data to be read. The location can be
    a wildcard path of files (e.g., *s3://test-bucket/magic/**), a list of files,
    or a regular file location.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Dask 数据帧一样，加载和写入函数以 `to_` 或 `read_` 作为前缀开始。每种格式都有自己的配置，但一般来说，第一个位置参数是要读取数据的位置。位置可以是文件的通配符路径（例如
    *s3://test-bucket/magic/**），文件列表或常规文件位置。
- en: 'Dask arrays support reading the following formats:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 数组支持读取以下格式：
- en: zarr
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: zarr
- en: npy stacks (only local disk)
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: npy 堆栈（仅本地磁盘）
- en: 'And reading from and writing to:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 以及读取和写入：
- en: hdf5
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: hdf5
- en: zarr
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: zarr
- en: tiledb
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: tiledb
- en: npy stacks (local disk only)
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: npy 堆栈（仅本地磁盘）
- en: In addition, you can convert Dask arrays to/from Dask bags and DataFrames (provided
    the types are compatible). As you may have noted, Dask does not support reading
    arrays from as many formats as you might expect, which provides the opportunity
    for an excellent use of bags (covered in the next section).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，您可以将 Dask 数组转换为/从 Dask 袋和数据帧（如果类型兼容）。正如您可能已经注意到的那样，Dask 不支持从许多格式读取数组，这为使用袋提供了一个绝佳的机会（在下一节中介绍）。
- en: What’s Missing
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有何缺失
- en: While Dask arrays implement a large amount of the ndarray APIs, it is not a
    complete set. As with Dask DataFrames, some of the omissions are intentional (e.g.,
    `sort`, much of `linalg`, etc., which would be slow), and other parts are just
    missing because no one has had the time to implement them yet.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Dask 数组实现了大量的 ndarray API，但并非完整集合。与 Dask 数据框一样，部分省略是有意的（例如，`sort`，大部分 `linalg`
    等，这些操作会很慢），而其他部分则是因为还没有人有时间来实现它们。
- en: Special Dask Functions
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特殊的 Dask 函数
- en: 'Since, as with distributed DataFrames, the partitioned nature of Dask arrays
    makes performance a bit different, there are some unique Dask array functions
    not found in `numpy.linalg`:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 与分布式数据框一样，Dask 数组的分区性质使得性能略有不同，因此有一些在 `numpy.linalg` 中找不到的独特 Dask 数组函数：
- en: '`map_overlap`'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '`map_overlap`'
- en: You can use this for any windowed view of the data, as with `map_overlap` on
    Dask DataFrames.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将此用于数据的任何窗口视图，例如在 Dask 数据框上的 `map_overlap`。
- en: '`map_blocks`'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`map_blocks`'
- en: This is similar to Dask’s DataFrames `map_partitions`, and you can use it for
    implementing embarrassingly parallel operations that the standard Dask library
    has not yet implemented, including new element-wise functions in NumPy.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这类似于 Dask 的 DataFrames `map_partitions`，您可以用它来实现尚未在标准 Dask 库中实现的尴尬并行操作，包括 NumPy
    中的新元素级函数。
- en: '`topk`'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`topk`'
- en: This returns the topk elements of the array in place of fully sorting it (which
    is much more expensive).^([1](ch05.xhtml#id640))
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回数组的前 k 个元素，而不是完全排序它（后者要显著更昂贵）。^([1](ch05.xhtml#id640))
- en: '`compute_chunk_sizes`'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`compute_chunk_sizes`'
- en: Dask needs to know the chunk sizes to support indexing; if an array has unknown
    chunk sizes, you can call this function.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 需要知道块的大小来支持索引；如果一个数组具有未知的块大小，您可以调用此函数。
- en: These special functions are not present on the underlying regular collections,
    as they do not offer the same performance savings in non-parallel/non-distributed
    environments.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这些特殊函数在基础常规集合上不存在，因为它们在非并行/非分布式环境中无法提供相同的性能节省。
- en: Dask Bags
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dask Bags
- en: To continue to draw parallels to Python’s internal data structures, you can
    think of bags as slightly different lists or sets. Bags are like lists except
    without the concept of order (so there are no indexing operations). Alternatively,
    if you think of bags like sets, the difference between them is that bags allow
    duplicates. Dask’s bags have the least number of restrictions on what they contain
    and similarly have the smallest API. In fact, Examples [2-6](ch02.xhtml#make_bag_of_crawler)
    through [2-9](ch02.xhtml#wc_func) covered most of the core of the APIs for bags.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 继续与 Python 内部数据结构类比，您可以将 bags 视为稍有不同的列表或集合。 Bags 类似于列表，但没有顺序的概念（因此没有索引操作）。或者，如果您将
    bags 视为集合，则它们与集合的区别在于 bags 允许重复。 Dask 的 bags 对它们包含的内容没有太多限制，并且同样具有最小的 API。 实际上，示例
    [2-6](ch02.xhtml#make_bag_of_crawler) 到 [2-9](ch02.xhtml#wc_func) 涵盖了 bags 的核心
    API 大部分内容。
- en: Tip
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: For users coming from Apache Spark, Dask bags are most closely related to Spark’s
    RDDs.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于从 Apache Spark 转来的用户，Dask bags 与 Spark 的 RDDs 最为接近。
- en: Common Use Cases
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 常见用例
- en: 'Bags are an excellent choice when the structure of the data is unknown or otherwise
    not consistent. Some of the common use cases are as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据的结构未知或不一致时，bags 是一个很好的选择。 一些常见用例包括：
- en: Grouping together a bunch of `dask.delayed` calls—for example, for loading “messy”
    or unstructured (or unsupported) data.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将一堆 `dask.delayed` 调用分组在一起，例如，用于加载混乱或非结构化（或不支持的）数据。
- en: “Cleaning” (or adding structure to) unstructured data (like JSON).
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “清理”（或为其添加结构）非结构化数据（如 JSON）。
- en: Parallelizing a group of tasks over a fixed range—for example, if you want to
    call an API 100 times but you are not picky about the details.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在固定范围内并行化一组任务，例如，如果您想调用 API 100 次，但不关心细节。
- en: 'Catch-all: if the data doesn’t fit in any other collection type, bags are your
    friend.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总括来说：如果数据不适合任何其他集合类型，bags 是您的朋友。
- en: We believe that the most common use case for Dask bags is loading messy data
    or data that Dask does not have built-in support for.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为 Dask bags 最常见的用例是加载混乱数据或 Dask 没有内置支持的数据。
- en: Loading and Saving Dask Bags
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载和保存 Dask Bags
- en: Dask bags have built-in readers for text files, with `read_text`, and avro files,
    with `read_avro`. Similarly, you can write Dask bags to text files and avro files,
    although the results must be serializable. Bags are commonly used when Dask’s
    built-in tools for reading data are not enough, so the next section will dive
    into how to go beyond these two built-in formats.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Dask Bag 内置了用于文本文件的读取器，使用 `read_text`，以及 Avro 文件的读取器，使用 `read_avro`。同样，您也可以将
    Dask Bag 写入文本文件和 Avro 文件，尽管结果必须可序列化。当 Dask 的内置工具无法满足读取数据需求时，通常会使用 Bags，因此接下来的部分将深入讲解如何超越这两种内置格式。
- en: Loading Messy Data with a Dask Bag
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Dask Bag 加载混乱数据
- en: Normally, the goal when loading messy data is to get it into a structured format
    for further processing, or at least to extract the components that you are interested
    in. While your data formats will likely be a bit different, this section will
    look at loading some messy JSON and then extracting some relevant fields. Don’t
    worry—we call out places where different formats or sources may require different
    techniques.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在加载混乱数据时的目标是将其转换为结构化格式以便进一步处理，或者至少提取您感兴趣的组件。虽然您的数据格式可能略有不同，但本节将介绍如何加载一些混乱的
    JSON 数据，并提取一些相关字段。不用担心——我们会指出不同格式或来源可能需要不同技术的地方。
- en: For messy textual data, which is a common occurrence with JSON, you can save
    yourself some time by loading the data using bags’ `read_text` function. The `read_text`
    function defaults to splitting up the records by line; however, many formats cannot
    be processed by line. To get each whole file in a whole record rather than it
    being split up, you can set the `linedelimiter` parameter to one not found. Often
    REST APIs will return the results as a subcomponent, so in [Example 5-1](#preprocess_json),
    we load the [US Food and Drug Administration (FDA) recall dataset](https://oreil.ly/3Xmd_)
    and strip it down to the part we care about. The FDA recall dataset is a wonderful
    real-world example of nested datasets often found in JSON data, which can be hard
    to process directly in DataFrames.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对于混乱的文本数据（在 JSON 中很常见），您可以通过使用 bags 的 `read_text` 函数节省一些时间。`read_text` 函数默认按行分割记录；然而，许多格式不能通过行来处理。为了获取每个完整文件作为一个整体记录而不是被分割开，您可以将
    `linedelimiter` 参数设置为找不到的值。通常 REST API 会返回结果作为一个子组件，因此在 [示例 5-1](#preprocess_json)
    中，我们加载 [美国食品药品管理局（FDA）召回数据集](https://oreil.ly/3Xmd_) 并将其剥离到我们关心的部分。FDA 召回数据集是
    JSON 数据中经常遇到的嵌套数据集的一个精彩现实世界示例，直接在 DataFrame 中处理这类数据集可能会很困难。
- en: Example 5-1\. Pre-processing JSON
  id: totrans-55
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-1\. 预处理 JSON
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If you need to load data from an unsupported source (like a custom storage system)
    or a binary format (like protocol buffers or Flexible Image Transport System),
    you’ll need to use lower-level APIs. For binary files that are still stored in
    an FSSPEC-supported filesystem like S3, you can try the pattern in [Example 5-2](#custom_load).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要从不受支持的源（如自定义存储系统）或二进制格式（如协议缓冲区或灵活图像传输系统）加载数据，您需要使用较低级别的 API。对于仍存储在像 S3
    这样的 FSSPEC 支持的文件系统中的二进制文件，您可以尝试 [示例 5-2](#custom_load) 中的模式。
- en: Example 5-2\. Loading PDFs from an FSSPEC-supported filesystem
  id: totrans-58
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-2\. 从 FSSPEC 支持的文件系统加载 PDF
- en: '[PRE1]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If you are not using a FSSPEC-supported filesystem, you can still load the data
    as illustrated in [Example 5-3](#custom_load_nonfs).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您没有使用 FSSPEC 支持的文件系统，您仍然可以按照 [示例 5-3](#custom_load_nonfs) 中所示的方式加载数据。
- en: Example 5-3\. Loading data using a purely custom function
  id: totrans-61
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-3\. 使用纯定制函数加载数据
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Loading data in this fashion requires that each file be able to fit inside a
    worker/executor. If that is not the case, things get much more complicated. Implementing
    splittable data readers is beyond the scope of this book, but you can take a look
    at Dask’s internal IO libraries (text is the easiest) to get some inspiration.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式加载数据要求每个文件能够适合一个 worker/executor。如果不符合该条件，情况会变得更加复杂。实现可分割的数据读取器超出了本书的范围，但您可以查看
    Dask 的内部 IO 库（文本是最简单的）以获取一些灵感。
- en: Sometimes with nested directory structures, creating the list of files can take
    a long time. In that case, it’s worthwhile to parallelize the listing of files
    as well. There are a number of different techniques to parallelize file listing,
    but for simplicity, we show parallel recursive listing in [Example 5-4](#parallel_list).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，对于嵌套的目录结构，创建文件列表可能需要很长时间。在这种情况下，将文件列表并行化是值得的。有许多不同的技术可以并行化文件列表，但为了简单起见，我们展示了在
    [示例 5-4](#parallel_list) 中递归并行列出的方式。
- en: Example 5-4\. Listing the files in parallel (recursively)
  id: totrans-66
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-4\. 并行列出文件（递归）
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Tip
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: You don’t always have to do the directory listing yourself. It can be worthwhile
    to check whether there is a metastore, such as Hive or Iceberg, which can give
    you the list of files without all of these slow API calls.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你并不总是需要自己进行目录列表。检查一下是否有元数据存储（例如 Hive 或 Iceberg）可能会有所帮助，它可以提供文件列表，而不需要进行所有这些慢速
    API 调用。
- en: 'This approach has some downsides: namely, all the filenames come back to a
    single point—but this is rarely an issue. However, if even just the list of your
    files is too big to fit in memory, you’ll want to try a recursive algorithm for
    directory discovery, followed by an iterative algorithm for file listing that
    keeps the names of the files in the bag.^([2](ch05.xhtml#id652)) The code becomes
    a bit more complex, as shown in [Example 5-5](#parallel_list_large), so this last
    approach is rarely used.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有一些缺点：即所有的文件名都回到一个单一的点——但这很少是个问题。然而，如果你的文件列表甚至只是太大而无法在内存中容纳，你可能会尝试使用递归算法来进行目录发现，然后采用迭代算法来列出文件，保持文件名在袋子里。[^2](ch05.xhtml#id652)
    代码会变得稍微复杂一些，如[示例 5-5](#parallel_list_large)所示，所以这种最后的方法很少被使用。
- en: Example 5-5\. Listing the files in parallel without collecting to the driver
  id: totrans-71
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-5\. 并行列出文件而不收集到驱动程序
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: A fully iterative algorithm with FSSPEC would not be any faster than the naive
    listing, since FSSPEC does not support querying just for directories.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一个完全迭代的 FSSPEC 算法不会比天真的列表快，因为 FSSPEC 不支持仅查询目录。
- en: Limitations
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: Dask bags are not well suited to most reduction or shuffling operations, as
    their core `reduction` function reduces results down to one partition, requiring
    that all of the data fit on a single machine. You can reasonably use aggregations
    that are purely constant space, such as mean, min, and max. However, most of the
    time you find yourself trying to aggregate your data, you should consider transforming
    your bag into a DataFrame with `bag.Bag.to_dataframe`.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Dask bags 不太适合大多数的缩减或洗牌操作，因为它们的核心 `reduction` 函数将结果缩减到一个分区，需要所有的数据都能适应单台机器。你可以合理地使用纯粹的常量空间的聚合，例如平均值、最小值和最大值。然而，大多数情况下，你会发现自己尝试对数据进行聚合，你应该考虑将你的
    bag 转换为 DataFrame，使用 `bag.Bag.to_dataframe`。
- en: Tip
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: All three Dask data types (bag, array, and DataFrame) have methods for being
    converted to other data types. However, some conversions require special attention.
    For example, when converting a Dask DataFrame to a Dask array, the resulting array
    will have `NaN` if you look at the shape it generates. This is because Dask DataFrame
    does not keep track of the number of rows in each DataFrame chunk.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 所有三种 Dask 数据类型（bag、array 和 DataFrame）都有被转换为其他数据类型的方法。然而，有些转换需要特别注意。例如，当将 Dask
    DataFrame 转换为 Dask array 时，生成的数组将具有 `NaN`，如果你查看它生成的形状。这是因为 Dask DataFrame 不会跟踪每个
    DataFrame 块中的行数。
- en: Conclusion
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: While Dask DataFrames get the most use, Dask arrays and bags have their place.
    You can use Dask arrays to speed up and parallelize large multi-dimensional array
    processes. Dask bags allow you to work with data that doesn’t fit nicely into
    a DataFrame, like PDFs or multi-dimensional nested data. These collections get
    much less focus and active development than Dask DataFrames but may still have
    their place in your workflows. In the next chapter, you will see how you can add
    state to your Dask programs, including with operations on Dask’s collections.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Dask DataFrames 得到了最多的使用，但 Dask arrays 和 bags 也有它们的用处。你可以使用 Dask arrays 来加速和并行化大型多维数组处理。Dask
    bags 允许你处理不太适合 DataFrame 的数据，比如 PDF 或多维嵌套数据。这些集合比 Dask DataFrames 得到的关注和积极的开发要少得多，但可能仍然在你的工作流程中有它们的位置。在下一章中，你将看到如何向你的
    Dask 程序中添加状态，包括对 Dask 集合的操作。
- en: ^([1](ch05.xhtml#id640-marker)) [`topk`](https://oreil.ly/vUjgv) extracts the
    topk elements of each partition and then only needs to shuffle the k elements
    out of each partition.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch05.xhtml#id640-marker)) [`topk`](https://oreil.ly/vUjgv) 提取每个分区的前 k
    个元素，然后只需要将 k 个元素从每个分区洗牌出来。
- en: ^([2](ch05.xhtml#id652-marker)) Iterative algorithms involve using constructs
    like *while* or *for* instead of recursive calls to the same function.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch05.xhtml#id652-marker)) 迭代算法涉及使用 *while* 或 *for* 这样的结构，而不是对同一函数的递归调用。
