["```py\ndef get_one(name: str) -> Explorer:\n    qry = \"select * from explorer where name=:name\"\n    params = {\"name\": name}\n    curs.execute(qry, params)\n    return row_to_model(curs.fetchone())\n```", "```py\nfrom sqlalchemy import Metadata, Table, Column, Text\nfrom sqlalchemy import connect, insert\n\nconn = connect(\"sqlite:///cryptid.db\")\nmeta = Metadata()\nexplorer_table = Table(\n    \"explorer\",\n    meta,\n    Column(\"name\", Text, primary_key=True),\n    Column(\"country\", Text),\n    Column(\"description\", Text),\n    )\ninsert(explorer_table).values(\n    name=\"Beau Buffette\",\n    country=\"US\",\n    description=\"...\")\n```", "```py\nfrom faker import Faker\nfrom time import perf_counter\n\ndef load():\n    from error import Duplicate\n    from data.explorer import create\n    from model.explorer import Explorer\n\n    f = Faker()\n    NUM = 100_000\n    t1 = perf_counter()\n    for row in range(NUM):\n        try:\n            create(Explorer(name=f.name(),\n                country=f.country(),\n                description=f.description))\n        except Duplicate:\n            pass\n    t2 = perf_counter()\n    print(NUM, \"rows\")\n    print(\"write time:\", t2-t1)\n\ndef read_db():\n    from data.explorer import get_all\n\n    t1 = perf_counter()\n    _ = get_all()\n    t2 = perf_counter()\n    print(\"db read time:\", t2-t1)\n\ndef read_api():\n    from fastapi.testclient import TestClient\n    from main import app\n\n    t1 = perf_counter()\n    client = TestClient(app)\n    _ = client.get(\"/explorer/\")\n    t2 = perf_counter()\n    print(\"api read time:\", t2-t1)\n\nload()\nread_db()\nread_db()\nread_api()\n```", "```py\n$ python test_load.py\n100000 rows\nwrite time: 14.868232927983627\ndb read time: 0.4025074450764805\ndb read time: 0.39750714192632586\napi read time: 2.597553930943832\n```", "```py\n$ pip install torch torchvision\n$ pip install transformers\n```", "```py\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\nfrom transformers import (AutoTokenizer,\n    AutoModelForSeq2SeqLM, GenerationConfig)\nmodel_name = \"google/flan-t5-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\nconfig = GenerationConfig(max_new_tokens=200)\n\n@app.get(\"/ai\")\ndef prompt(line: str) -> str:\n    tokens = tokenizer(line, return_tensors=\"pt\")\n    outputs = model.generate(**tokens,\n        generator_config=config)\n    result = tokenizer.batch_decode(outputs,\n        skip_special_tokens=True)\n    return result[0]\n```", "```py\n$ http -b localhost:8000/ai line==\"What are you?\"\n\"a sailor\"\n```"]