- en: Chapter 8\. Pytest for DevOps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Continuous integration, continuous delivery, deployments, and any pipeline workflow
    in general with some thought put into it will be filled with validation. This
    *validation* can happen at every step of the way and when achieving important
    objectives.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if in the middle of a long list of steps to produce a deployment,
    a `curl` command is called to get an all-important file, do you think the build
    should continue if it fails? Probably not! `curl` has a flag that can be used
    to produce a nonzero exit status (`--fail`) if an HTTP error happens. That simple
    flag usage is a form of validation: ensure that the request succeeded, otherwise
    fail the build step. The *key word* is to *ensure* that something succeeded, and
    that is at the core of this chapter: validation and testing strategies that can
    help you build better infrastructure.'
  prefs: []
  type: TYPE_NORMAL
- en: Thinking about validation becomes all the more satisfying when Python gets in
    the mix, harnessing testing frameworks like `pytest` to handle the verification
    of systems.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter reviews some of the basics associated with testing in Python using
    the phenomenal `pytest` framework, then dives into some advanced features of the
    framework, and finally goes into detail about the *TestInfra* project, a plug-in
    to `pytest` that can do system verification.
  prefs: []
  type: TYPE_NORMAL
- en: Testing Superpowers with pytest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can’t say enough good things about the `pytest` framework. Created by Holger
    Krekel, it is now maintained by quite a few people that do an incredible job at
    producing a high-quality piece of software that is usually part of our everyday
    work. As a full-featured framework, it is tough to narrow down the scope enough
    to provide a useful introduction without repeating the project’s complete documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The `pytest` project has lots of information, examples, and feature details
    [in its documentation](https://oreil.ly/PSAu2) that are worth reviewing. There
    are always new things to learn as the project continues to provide new releases
    and different ways to improve testing.
  prefs: []
  type: TYPE_NORMAL
- en: When Alfredo was first introduced to the framework, he was struggling to write
    tests, and found it cumbersome to adhere to Python’s built-in way of testing with
    `unittest` (this chapter goes through the differences later). It took him a couple
    of minutes to get hooked into `pytest`’s magical reporting. It wasn’t forcing
    him to move away from how he had written his tests, and it worked right out of
    the box with no modifications! This flexibility shows throughout the project,
    and even when things might not be possible today, you can extend its functionality
    via plug-ins or its configuration files.
  prefs: []
  type: TYPE_NORMAL
- en: By understanding how to write more straightforward test cases, and by taking
    advantage of the command-line tool, reporting engine, plug-in extensibility, and
    framework utilities, you will want to write more tests that will undoubtedly be
    better all around.
  prefs: []
  type: TYPE_NORMAL
- en: Getting Started with pytest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In its simplest form, `pytest` is a command-line tool that discovers Python
    tests and executes them. It doesn’t force a user to understand its internals,
    which makes it easy to get started with. This section demonstrates some of the
    most basic features, from writing tests to laying out files (so that they get
    automatically discovered), and finally, looking at the main differences between
    it and Python’s built-in testing framework, `unittest`.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Most *Integrated Development Environments* (IDEs), such as PyCharm and Visual
    Studio Code, have built-in support for running `pytest`. If using a text editor
    like Vim, there is support via the [`pytest.vim`](https://oreil.ly/HowKu) plug-in.
    Using `pytest` from the editor saves time and makes debugging failures easier,
    but be aware that not every option or plug-in is supported.
  prefs: []
  type: TYPE_NORMAL
- en: Testing with pytest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Make sure that you have `pytest` installed and available in the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a file called *test_basic.py*; it should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'If `pytest` runs without any arguments, it should show a pass and a failure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The output is beneficial from the start; it displays how many tests were collected,
    how many passed, and which one failed—including its line number.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The default output from `pytest` is handy, but it might be too verbose. You
    can control the amount of output with configuration, reducing it with the `-q`
    flag.
  prefs: []
  type: TYPE_NORMAL
- en: There was no need to create a class to include the tests; functions were discovered
    and ran correctly. A test suite can have a mix of both, and the framework works
    fine in such an environment.
  prefs: []
  type: TYPE_NORMAL
- en: Layouts and conventions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When testing in Python, there are a few conventions that `pytest` follows implicitly.
    Most of these conventions are about naming and structure. For example, try renaming
    the *test_basic.py* file to *basic.py* and run `pytest` to see what happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: No tests ran because of the convention of prefixing test files with *test_*.
    If you rename the file back to *test_basic.py*, it should be automatically discovered
    and tests should run.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Layouts and conventions are helpful for automatic test discovery. It is possible
    to configure the framework to use other naming conventions or to directly test
    a file that has a unique name. However, it is useful to follow through with basic
    expectations to avoid confusion when tests don’t run.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are conventions that will allow the tool to discover tests:'
  prefs: []
  type: TYPE_NORMAL
- en: The testing directory needs to be named *tests*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test files need to be prefixed with *test*; for example, *test_basic.py*, or
    suffixed with *test.py*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test functions need to be prefixed with `test_`; for example, `def test``simple():`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test classes need to be prefixed with `Test`; for example, `class TestSimple`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test methods follow the same conventions as functions, prefixed with `test_`;
    for example, `def test_method(self):`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because prefixing with `test_` is a requirement for automatic discovery and
    execution of tests, it allows introducing helper functions and other nontest code
    with different names, so that they get excluded automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Differences with unittest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Python already comes with a set of utilities and helpers for testing, and they
    are part of the `unittest` module. It is useful to understand how `pytest` is
    different and why it is highly recommended.
  prefs: []
  type: TYPE_NORMAL
- en: The `unittest` module forces the use of classes and class inheritance. For an
    experienced developer who understands object-oriented programming and class inheritance,
    this shouldn’t be a problem, but for beginners, *it is an obstacle*. Using classes
    and inheritance shouldn’t be a requisite for writing basic tests!
  prefs: []
  type: TYPE_NORMAL
- en: 'Part of forcing users to inherit from `unittest.TestCase` is that you are required
    to understand (and remember) most of the assertion methods that are used to verify
    results. With `pytest`, there is a single assertion helper that can do it all:
    `assert`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'These are a few of the assert methods that can be used when writing tests with
    `unittest`. Some of them are easy to grasp, while others are very confusing:'
  prefs: []
  type: TYPE_NORMAL
- en: '`self.assertEqual(a, b)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`self.assertNotEqual(a, b)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`self.assertTrue(x)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`self.assertFalse(x)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`self.assertIs(a, b)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`self.assertIsNot(a, b)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`self.assertIsNone(x)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`self.assertIsNotNone(x)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`self.assertIn(a, b)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`self.assertNotIn(a, b)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`self.assertIsInstance(a, b)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`self.assertNotIsInstance(a, b)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`self.assertRaises(exc, fun, *args, **kwds)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`self.assertRaisesRegex(exc, r, fun, *args, **kwds)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`self.assertWarns(warn, fun, *args, **kwds)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`self.assertWarnsRegex(warn, r, fun, *args, **kwds)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`self.assertLogs(logger, level)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`self.assertMultiLineEqual(a, b)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`self.assertSequenceEqual(a, b)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`self.assertListEqual(a, b)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`self.assertTupleEqual(a, b)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`self.assertSetEqual(a, b)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`self.assertDictEqual(a, b)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`self.assertAlmostEqual(a, b)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`self.assertNotAlmostEqual(a, b)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`self.assertGreater(a, b)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`self.assertGreaterEqual(a, b)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`self.assertLess(a, b)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`self.assertLessEqual(a, b)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`self.assertRegex(s, r)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`self.assertNotRegex(s, r)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`self.assertCountEqual(a, b)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pytest` allows you to use `assert` exclusively and does not force you to use
    any of the above. Moreover, it *does allow* you to write tests using `unittest`,
    and it even executes them. We strongly advise against doing that and suggest you
    concentrate on just using plain asserts.'
  prefs: []
  type: TYPE_NORMAL
- en: Not only is it easier to use plain asserts, but `pytest` also provides a rich
    comparison engine on failures (more on this in the next section).
  prefs: []
  type: TYPE_NORMAL
- en: pytest Features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Aside from making it easier to write tests and execute them, the framework provides
    lots of extensible options, such as hooks. Hooks allow you to interact with the
    framework internals at different points in the runtime. If you want to alter the
    collection of tests, for example, a hook for the collection engine can be added.
    Another useful example is if you want to implement a nicer report when a test
    fails.
  prefs: []
  type: TYPE_NORMAL
- en: 'While developing an HTTP API, we found that sometimes the failures in the tests
    that used HTTP requests against the application weren’t beneficial: an assertion
    failure would be reported because the expected response (an HTTP 200) was an HTTP
    500 error. We wanted to know more about the request: to what URL endpoint? If
    it was a POST request, did it have data? What did it look like? These are things
    that were already available in the HTTP response object, so we wrote a hook to
    poke inside this object and include all these items as part of the failure report.'
  prefs: []
  type: TYPE_NORMAL
- en: Hooks are an advanced feature of `pytest` that you might not need at all, but
    it is useful to understand that the framework can be flexible enough to accommodate
    different requirements. The next sections cover how to extend the framework, why
    using `assert` is so valuable, how to parametrize tests to reduce repetition,
    how to make helpers with `fixtures`, and how to use the built-in ones.
  prefs: []
  type: TYPE_NORMAL
- en: conftest.py
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Most software lets you extend functionality via plug-ins (web browsers call
    them *extensions*, for example); similarly, `pytest` has a rich API for developing
    plug-ins. The complete API is not covered here, but its simpler approach is: the
    *conftest.py* file. In this file, the tool can be extended *just like a plug-in
    can*. There is no need to fully understand how to create a separate plug-in, package
    it, and install it. If a *conftest.py* file is present, the framework will load
    it and consume any specific directives in it. It all happens automatically!'
  prefs: []
  type: TYPE_NORMAL
- en: Usually, you will find that a *conftest.py* file is used to hold hooks, fixtures,
    and helpers for those fixtures. Those *fixtures* can then be used within tests
    if declared as arguments (that process is described later in the fixture section).
  prefs: []
  type: TYPE_NORMAL
- en: It makes sense to add fixtures and helpers to this file when more than one test
    module will use it. If there is only a single test file, or if only one file is
    going to make use of a fixture or hook, there is no need to create or use a *conftest.py*
    file. Fixtures and helpers can be defined within the same file as the test and
    behave the same.
  prefs: []
  type: TYPE_NORMAL
- en: The only condition for loading a *conftest.py* file is to be present in the
    *tests* directory and match the name correctly. Also, although this name is configurable,
    we advise against changing it and encourage you to follow the default naming conventions
    to avoid potential issues.
  prefs: []
  type: TYPE_NORMAL
- en: The Amazing assert
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When we have to describe how great the `pytest` tooling is, we start by describing
    the important uses of the `assert` statement. Behind the scenes, the framework
    is inspecting objects and providing a rich comparison engine to better describe
    errors. This is usually met with resistance because a bare `assert` in Python
    is terrible at describing errors. Compare two long strings as an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Where is the difference? It is hard to tell without spending some time looking
    at those two long lines closely. This will cause people to recommend against it.
    A small test shows how `pytest` augments when reporting the failure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Can you tell where the error is? This is *tremendously easier*. Not only does
    it tell you it fails, but it points to exactly *where* the failure is. The example
    is a simple assert with a long string, but the framework handles other data structures
    like lists and dictionaries without a problem. Have you ever compared very long
    lists in tests? It is impossible to easily tell what items are different. Here
    is a small snippet with long lists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: After informing the user that the test failed, it points exactly to the index
    number (index four or fifth item), and finally, it says that one list has one
    extra item. Without this level of introspection, debugging failures would take
    a very long time. The bonus in reporting is that, by default, it omits very long
    items when making comparisons, so that only the relevant portion shows in the
    output. After all, what you want is to know not only that the lists (or any other
    data structure) are different but *exactly where they* are different.
  prefs: []
  type: TYPE_NORMAL
- en: Parametrization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Parametrization is one of the features that can take a while to understand
    because it doesn’t exist in the `unittest` module and is a feature unique to the
    pytest framework. It can be clear once you find yourself writing very similar
    tests that had minor changes in the inputs but are testing the same thing. Take,
    for example, this class that is testing a function that returns `True` if a string
    is implying a truthful value. The `string_to_bool` is the function under test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'See how all these tests are evaluating the same result from similar inputs?
    This is where parametrization shines because it can group all these values and
    pass them to the test; it can effectively reduce them to a single test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: There are a couple of things happening here. First `pytest` is imported (the
    framework) to use the `pytest.mark.parametrize` module, then `true_values` is
    defined as a (list) variable of all the values to use that should evaluate the
    same, and finally, it replaces all the test methods to a single one. The test
    method uses the `parametrize` decorator, which defines two arguments. The first
    is a string, `*value*`, and the second is the name of the list defined previously.
    This can look a bit odd, but it is telling the framework that `*value*` is the
    name to use for the argument in the test method. That is where the `value` argument
    comes from!
  prefs: []
  type: TYPE_NORMAL
- en: 'If the verbosity is increased when running, the output will show exactly what
    value was passed in. It almost looks like the single test got cloned into every
    single iteration passed in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The output includes the values used in each iteration of the *single test* in
    brackets. It is reducing the very verbose test class into a single test method,
    thanks to `parametrize`. The next time you find yourself writing tests that seem
    very similar and that assert the same outcome with different inputs, you will
    know that you can make it simpler with the `parametrize` decorator.
  prefs: []
  type: TYPE_NORMAL
- en: Fixtures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We think of [`pytest` fixtures](https://oreil.ly/gPoM5) like little helpers
    that can get injected into a test. Regardless of whether you are writing a single
    test function or a bunch of test methods, fixtures can be used in the same way.
    If they aren’t going to be shared among other test files, it is fine to define
    them in the same test file; otherwise they can go into the *conftest.py* file.
    Fixtures, just like helper functions, can be almost anything you need for a test,
    from simple data structures that get pre-created to more complex ones like setting
    a database for a web application.
  prefs: []
  type: TYPE_NORMAL
- en: These helpers can also have a defined *scope*. They can have specific code that
    cleans up for every test method, class, and module, or even allows setting them
    up once for the whole test session. By defining them in a test method (or test
    function), you are effectively getting the fixture injected at runtime. If this
    sounds a bit confusing, it will become clear through examples in the next few
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: Getting Started
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fixtures are so easy to define and use that they are often abused. We know we’ve
    created a few that could have been simple helper methods! As we’ve mentioned already,
    there are many different use cases for fixtures—from simple data structures to
    more complex ones, such as setting up whole databases for a single test.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently, Alfredo had to test a small application that parses the contents
    of a particular file called a *keyring file*. It has some structure similar to
    an INI file, with some values that have to be unique and follow a specific format.
    The file structure can be very tedious to recreate on every test, so a fixture
    was created to help. This is how the keyring file looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The fixture is a function that returns the contents of the keyring file. Let’s
    create a new file called test_keyring.py with the contents of the fixture, and
    a small test function that verifies the default key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The fixture is using a nested function that does the heavy lifting, allows using
    a *default* key value, and returns the nested function in case the caller wants
    to have a randomized key. Inside the test, it receives the fixture by declaring
    it part of the argument of the test function (`mon_keyring` in this case), and
    is calling the fixture with `default=True` so that the default key is used, and
    then verifying it is generated as expected.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In a real-world scenario, the generated contents would be passed to the parser,
    ensuring expected behavior after parsing and that no errors happen.
  prefs: []
  type: TYPE_NORMAL
- en: The production code that used this fixture eventually grew to do other kinds
    of testing, and at some point, the test wanted to verify that the parser could
    handle files in different conditions. The fixture was returning a string, so it
    needed extending. Existing tests already made use of the `mon_keyring` fixture,
    so to extend the functionality without altering the current fixture, a new one
    was created that used a feature from the framework. Fixtures can *request* other
    fixtures! You define the required fixture as an argument (like a test function
    or test method would), so the framework injects it when it gets executed.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is how the new fixture that creates (and returns) the file looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Going line by line, the `pytest.fixture` decorator tells the framework that
    this function is a fixture, then the fixture is defined, asking for *two fixtures*
    as arguments: `mon_keyring` and `tmpdir`. The first is the one created previously
    in the *test_keyring.py* file earlier, and the second one is a built-in fixture
    from the framework (more on built-in fixtures in the next section). The `tmpdir`
    fixture allows you to use a temporary directory that gets removed after the test
    completes, then the *keyring* file is created, and the text generated by the `mon_keyring`
    fixture is written, passing the `default` argument. Finally, it returns the absolute
    path of the new file created so that the test can use it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is how the test function would use it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: You should now have a good idea of what fixtures are, where can you define them,
    and how to consume them in tests. The next section goes through a few of the most
    useful built-in fixtures that are part of the framework.
  prefs: []
  type: TYPE_NORMAL
- en: Built-in Fixtures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The previous section briefly touched on one of the many built-in fixtures that
    `pytest` has to offer: the `tmpdir` fixture. The framework provides a few more
    fixtures. To verify the full list of available fixtures, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'There are two fixtures that we use a lot: `monkeypatch` and `capsys`, and they
    are in the list produced when the above command is run. This is the brief description
    you will see in the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '`capsys` captures any `stdout` or `stderr` produced in a test. Have you ever
    tried to verify some command output or logging in a unit test? It is challenging
    to get right and is something that requires a separate plug-in or library to *patch*
    Python’s internals and then inspect its contents.'
  prefs: []
  type: TYPE_NORMAL
- en: 'These are two test functions that verify the output produced on `stderr` and
    `stdout`, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The `capsys` fixture handles all the patching, setup, and helpers to retrieve
    the `stderr` and `stdout` produced in the test. The content is reset for every
    test, which ensures that the variables populate with the correct output.
  prefs: []
  type: TYPE_NORMAL
- en: '`monkeypatch` is probably the fixture that we use the most. When testing, there
    are situations where the code under test is out of our control, and *patching*
    needs to happen to override a module or function to have a specific behavior.
    There are quite a few *patching* and *mocking* libraries (*mocks* are helpers
    to set behavior on patched objects) available for Python, but `monkeypatch` is
    good enough that you might not need to install a separate library to help out.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following function runs a system command to capture details from a device,
    then parses the output, and returns a property (the `ID_PART_ENTRY_TYPE` as reported
    by `blkid`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'To test it, set the desired behavior on the `check_output` attribute of the
    `subprocess` module. This is how the test function looks using the `monkeypatch`
    fixture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The `setattr` call *sets the attribute* on the patched callable (`check_output`
    in this case). It *patches* it with a lambda function that returns the one interesting
    line. Since the `subprocess.check_output` function is not under our direct control,
    and the `get_part_entry_type` function doesn’t allow any other way to inject the
    values, patching is the only way.
  prefs: []
  type: TYPE_NORMAL
- en: We tend to favor using other techniques like injecting values (known as *dependency
    injection*) before attempting to patch, but sometimes there is no other way. Providing
    a library that can patch and handle all the cleanup on testing is one more reason
    `pytest` is a joy to work with.
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure Testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section explains how to do infrastructure testing and validation with the
    [Testinfra project](https://oreil.ly/e7Afx). It is a `pytest` plug-in for infrastructure
    that relies heavily on fixtures and allows you to write Python tests as if testing
    code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The previous sections went into some detail on `pytest` usage and examples,
    and this chapter started with the idea of verification at a system level. The
    way we explain infrastructure testing is by asking a question: *How can you tell
    that the deployment was successful?* Most of the time, this means some manual
    checks, such as loading a website or looking at processes, which is insufficient;
    it is error-prone and can get tedious if the system is significant.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Although you can initially get introduced to `pytest` as a tool to write and
    run Python unit tests, it can be advantageous to repurpose it for infrastructure
    testing. A few years ago Alfredo was tasked to produce an installer that exposed
    its features over an HTTP API. This installer was to create a [Ceph cluster](https://ceph.com),
    involving many machines. During the QA portion of launching the API, it was common
    to get reports where the cluster wouldn’t work as expected, so he would get the
    credentials to log in to these machines and inspect them. There is a multiplier
    effect once you have to debug a distributed system comprising several machines:
    multiple configuration files, different hard drives, network setups, anything
    and everything can be different even if they appear to be similar.'
  prefs: []
  type: TYPE_NORMAL
- en: Every time Alfredo had to debug these systems, he had an ever-growing list of
    things to check. Is the configuration the same on all servers? Are the permissions
    as expected? Does a specific user exist? He would eventually forget something
    and spend time trying to figure out what he was missing. It was an unsustainable
    process. *What if I could write simple test cases against the cluster?* Alfredo
    wrote a few simple tests to verify the items on the list to execute them against
    the machines making up the cluster. Before he knew it, he had a good set of tests
    that took a few seconds to run that would identify all kinds of issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'That was an incredible eye-opener for improving the delivery process. He could
    even execute these (functional) tests while developing the installer and catch
    things that weren’t quite right. If the QA team caught any issues, he could run
    the same tests against their setup. Sometimes tests caught environmental issues:
    a drive was *dirty* and caused the deployment to fail; a configuration file from
    a different cluster was left behind and caused issues. Automation, granular tests,
    and the ability to run them often made the work better and alleviated the amount
    of work the QA team had to put up with.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The TestInfra project has all kinds of fixtures to test a system efficiently,
    and it includes a complete set of backends to connect to servers; regardless of
    their deployment type: Ansible, Docker, SSH, and Kubernetes are some of the supported
    connections. By supporting many different connection backends, you can execute
    the same set of tests regardless of infrastructure changes.'
  prefs: []
  type: TYPE_NORMAL
- en: The next sections go through different backends and get into examples of a production
    project.
  prefs: []
  type: TYPE_NORMAL
- en: What Is System Validation?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'System validation can happen at different levels (with monitoring and alert
    systems) and at different stages in the life cycle of an application, such as
    during pre-deployment, at runtime, or during deployment. An application that Alfredo
    recently put into production needed to handle client connections gracefully without
    any disruption, even when restarted. To sustain traffic, the application is load
    balanced: when the system is under heavy loads, new connections get sent to other
    servers with a lighter load.'
  prefs: []
  type: TYPE_NORMAL
- en: When a new release gets deployed, the application *has to be restarted*. Restarting
    means that clients experience an odd behavior at best, or a very broken experience
    at the worst. To avoid this, the restart process waits for all client connections
    to terminate, the system refuses new connections, allowing it to finish work from
    existing clients, and the rest of the system picks up the work. When no connections
    are active, the deployment continues and stops services to get the newer code
    in.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is validation at every step of the way: before the deployment to tell
    the balancer to stop sending new clients and later, verifying that no new clients
    are active. If that workflow converts to a test, the title could be something
    like: `make sure that no clients are currently running`. Once the new code is
    in, another validation step checks whether the balancer has acknowledged that
    the server is ready to produce work once again. Another test here could be: `balancer
    has server as active`. Finally, it makes sure that the server is receiving new
    client connections—yet another test to write!'
  prefs: []
  type: TYPE_NORMAL
- en: Throughout these steps, verification is in place, and tests can be written to
    verify this type of workflow.
  prefs: []
  type: TYPE_NORMAL
- en: System validation can also be tied to monitoring the overall health of a server
    (or servers in a clustered environment) or be part of the continuous integration
    while developing the application and testing functionally. The basics of validation
    apply to these situations and anything else that might benefit from status verification.
    It shouldn’t be used exclusively for testing, although that is a good start!
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Testinfra
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Writing unit tests against infrastructure is a powerful concept, and having
    used Testinfra for over a year, we can say that it has improved the quality of
    production applications we’ve had to deliver. The following sections go into specifics,
    such as connecting to different nodes and executing validation tests, and explore
    what type of fixtures are available.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a new virtual environment, install `pytest`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Install `testinfra`, ensuring that version `2.1.0` is used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '`pytest` fixtures provide all the test functionality offered by the Testinfra
    project. To take advantage of this section, you will need to know how they work.'
  prefs: []
  type: TYPE_NORMAL
- en: Connecting to Remote Nodes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Because different backend connection types exist, when the connection is not
    specified directly, Testinfra defaults to certain ones. It is better to be explicit
    about the connection type and define it in the command line.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are all the connection types that Testinfra supports:'
  prefs: []
  type: TYPE_NORMAL
- en: local
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Paramiko (an SSH implementation in Python)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SSH
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Salt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ansible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes (via kubectl)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: WinRM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LXC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A `testinfra` section appears in the help menu with some context on the flags
    that are provided. This is a neat feature from `pytest` and its integration with
    Testinfra. The help for both projects comes from the same command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'There are two servers up and running. To demonstrate the connection options,
    let’s check if they are running CentOS 7 by poking inside the */etc/os-release*
    file. This is how the test function looks (saved as `test_remote.py`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: It is a single test function that accepts the `host` fixture, which runs against
    all the nodes specified.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `--hosts` flag accepts a list of hosts with a connection scheme (SSH would
    use `*ssh://hostname*` for example), and some other variations using globbing
    are allowed. If testing against more than a couple of remote servers at a time,
    passing them on the command line becomes cumbersome. This is how it would look
    to test against two servers using SSH:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The increased verbosity (with the `-v` flag) shows that Testinfra is executing
    the one test function in the two remote servers specified in the invocation.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When setting up the hosts, it is important to have a passwordless connection.
    There shouldn’t be any password prompts, and if using SSH, a key-based configuration
    should be used.
  prefs: []
  type: TYPE_NORMAL
- en: 'When automating these types of tests (as part of a job in a CI system, for
    example), you can benefit from generating the hosts, determining how they connect,
    and any other special directives. Testinfra can consume an SSH configuration file
    to determine what hosts to connect to. For the previous test run, [Vagrant](https://www.vagrantup.com)
    was used, which created these servers with special keys and connection settings.
    Vagrant can generate an ad-hoc SSH config file for the servers it has created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Exporting the contents of the output to a file and then passing that to Testinfra
    offers greater flexibility if using more than one host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Using `--hosts=default` avoids having to specify them directly in the command
    line, and the engine feeds from the SSH configuration. Even without Vagrant, the
    SSH configuration tip is still useful if connecting to many hosts with specific
    directives.
  prefs: []
  type: TYPE_NORMAL
- en: '[Ansible](https://www.ansible.com) is another option if the nodes are local,
    SSH, or Docker containers. The test setup can benefit from using an inventory
    of hosts (much like the SSH config), which can group the hosts into different
    sections. The host groups can also be specified so that you can single out hosts
    to test against, instead of executing against all.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For `node1` and `node2` used in the previous example, this is how the inventory
    file is defined (and saved as `hosts`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'If executing against all of them, the command changes to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'If defining other hosts in the inventory that need an exclusion, a group can
    be specified as well. Assuming that both nodes are web servers and are in the
    `nginx` group, this command would run the tests on only that one group:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A lot of system commands require superuser privileges. To allow escalation of
    privileges, Testinfra allows specifying `--sudo` or `--sudo-user`. The `--sudo`
    flag makes the engine use `sudo` when executing the commands, while the `--sudo-user`
    command allows running with higher privileges as a different user.The fixture
    can be used directly as well.
  prefs: []
  type: TYPE_NORMAL
- en: Features and Special Fixtures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far, the `host` fixture is the only one used in examples to check for a
    file and its contents. However, this is deceptive. The `host` fixture is an *all-included*
    fixture; it contains all the other powerful fixtures that Testinfra provides.
    This means that the example has already used the `host.file`, which has lots of
    extras packed in it. It is also possible to use the fixture directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The all-in-one `host` fixture makes use of the extensive API from Testinfra,
    which loads everything for each host it connects to. The idea is to write a single
    test that gets executed against different nodes, all accessible from the same
    `host` fixture.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are a [couple dozen](https://oreil.ly/2_J-o) attributes available. These
    are some of the most used ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '`host.ansible`'
  prefs: []
  type: TYPE_NORMAL
- en: Provides full access to any of the Ansible properties at runtime, such as hosts,
    inventory, and vars
  prefs: []
  type: TYPE_NORMAL
- en: '`host.addr`'
  prefs: []
  type: TYPE_NORMAL
- en: Network utilities, like checks for IPV4 and IPV6, is host reachable, is host
    resolvable
  prefs: []
  type: TYPE_NORMAL
- en: '`host.docker`'
  prefs: []
  type: TYPE_NORMAL
- en: Proxy to the Docker API, allows interacting with containers, and checks if they
    are running
  prefs: []
  type: TYPE_NORMAL
- en: '`host.interface`'
  prefs: []
  type: TYPE_NORMAL
- en: Helpers for inspecting addresses from a given interface
  prefs: []
  type: TYPE_NORMAL
- en: '`host.iptables`'
  prefs: []
  type: TYPE_NORMAL
- en: Helpers for verifying firewall rules as seen by `host.iptables`
  prefs: []
  type: TYPE_NORMAL
- en: '`host.mount_point`'
  prefs: []
  type: TYPE_NORMAL
- en: Check mounts, filesystem types as they exist in paths, and mount options
  prefs: []
  type: TYPE_NORMAL
- en: '`host.package`'
  prefs: []
  type: TYPE_NORMAL
- en: '*Very useful* to query if a package is installed and at what version'
  prefs: []
  type: TYPE_NORMAL
- en: '`host.process`'
  prefs: []
  type: TYPE_NORMAL
- en: Check for running processes
  prefs: []
  type: TYPE_NORMAL
- en: '`host.sudo`'
  prefs: []
  type: TYPE_NORMAL
- en: Allows you to execute commands with `host.sudo` or as a different user
  prefs: []
  type: TYPE_NORMAL
- en: '`host.system_info`'
  prefs: []
  type: TYPE_NORMAL
- en: All kinds of system metadata, such as distribution version, release, and codename
  prefs: []
  type: TYPE_NORMAL
- en: '`host.check_output`'
  prefs: []
  type: TYPE_NORMAL
- en: Runs a system command, checks its output if runs successfully, and can be used
    in combination with `host.sudo`
  prefs: []
  type: TYPE_NORMAL
- en: '`host.run`'
  prefs: []
  type: TYPE_NORMAL
- en: Runs a command, allows you to check the return code, `host.stderr`, and `host.stdout`
  prefs: []
  type: TYPE_NORMAL
- en: '`host.run_expect`'
  prefs: []
  type: TYPE_NORMAL
- en: Verifies that the return code is as expected
  prefs: []
  type: TYPE_NORMAL
- en: Examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A frictionless way to start developing system validation tests is to do so while
    creating the actual deployment. Somewhat similar to *Test Driven Development*
    (TDD), any progress warrants a new test. In this section, a web server needs to
    be installed and configured to run on port 80 to serve a static landing page.
    While making progress, tests will be added. Part of writing tests is understanding
    failures, so a few problems will be introduced to help us figure out what to fix.
  prefs: []
  type: TYPE_NORMAL
- en: 'With a *vanilla* Ubuntu server, start by installing the Nginx package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a new test file called *test_webserver.py* for adding new tests after
    making progress. After Nginx installs, let’s create another test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Reduce the verbosity in `pytest` output with the `-q` flag to concentrate on
    failures. The remote server is called `node4` and SSH is used to connect to it.
    This is the command to run the first test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Progress! The web server needs to be up and running, so a new test is added
    to verify that behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Running again *should* work once again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Some Linux distributions do not allow packages to start the services when they
    get installed. Moreover, the test has caught the Nginx service not running, as
    reported by `systemd` (the default unit service). Starting Nginx manually and
    running the test should make everything pass once again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'As mentioned at the beginning of this section, the web server should be serving
    a static landing page on port 80\. Adding another test (in *test_webserver.py*)
    to verify the port is the next step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'This test is more involved and needs attention to some details. It opts to
    check for TCP connections on port `80` on *any IP in the server*. While this is
    fine for this test, if the server has multiple interfaces and is configured to
    bind to a specific address, then a new test would have to be added. Adding another
    test that checks if port `80` is listening on a given address might seem like
    overkill, but if you think about the reporting, it helps explain what is going
    on:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Test `nginx` listens on port `80` : PASS'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Test `nginx` listens on address `192.168.0.2` and port `80`: FAIL'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The above tells us that Nginx is binding to port `80`, *just not to the right
    interface*. An extra test is an excellent way to provide granularity (at the expense
    of extra verbosity).
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the newly added test again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'No address has anything listening on port `80`. Looking at the configuration
    for Nginx reveals that it is set to listen on port `8080` using a directive in
    the default site that configures the port:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'After changing it back to port `80` and restarting the `nginx` service, the
    tests pass again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Since there isn’t a built-in fixture to handle HTTP requests to an address,
    the final test uses the `wget` utility to retrieve the contents of the running
    website and make assertions on the output to ensure that the static site renders:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Running *test_webserver.py* once more verifies that all our assumptions are
    correct:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Understanding the concepts of testing in Python, and repurposing those for system
    validation, is incredibly powerful. Automating test runs while developing applications
    or even writing and running tests on existing infrastructure are both excellent
    ways to simplify day-to-day operations that can become error-prone. pytest and
    Testinfra are great projects that can help you get started, and make it easy when
    extending is needed. Testing is a *level up* on skills.
  prefs: []
  type: TYPE_NORMAL
- en: Testing Jupyter Notebooks with pytest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One easy way to introduce big problems into your company is to forget about
    applying software engineering best practices when it comes to data science and
    machine learning. One way to fix this is to use the `nbval` plug-in for pytest
    that allows you to test your notebooks. Take a look at this `Makefile`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The key item is the `--nbval` flag that also allows the notebook in the repo
    to be tested by the build server.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Name at least three conventions needed so that `pytest` can discover a test.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the *conftest.py* file for?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain parametrization of tests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is a fixture and how can it be used in tests? Is it convenient? Why?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain how to use the `monkeypatch` fixture.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Case Study Question
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Create a test module to use `testinfra` to connect to a remote server. Test
    that Nginx is installed, is running with `systemd`, and the server is binding
    to port 80\. When all tests pass, try to make them fail by configuring Nginx to
    listen on a different port.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
