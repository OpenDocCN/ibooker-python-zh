["```py\nfrom urllib.request import urlretrieve, urlopen\nfrom bs4 import BeautifulSoup\n\nhtml = urlopen('http://www.pythonscraping.com')\nbs = BeautifulSoup(html, 'html.parser')\nimageLocation = bs.find('img', {'alt': 'python-logo'})['src']\nurlretrieve (imageLocation, 'logo.jpg')\n\n```", "```py\nimport os\nfrom urllib.request import urlretrieve, urlopen\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\n\ndownloadDir = 'downloaded'\nbaseUrl = 'https://pythonscraping.com/'\nbaseNetloc = urlparse(baseUrl).netloc\n\ndef getAbsoluteURL(source):\n    if urlparse(baseUrl).netloc == '':\n        return baseUrl + source\n    return source\n\ndef getDownloadPath(fileUrl):\n    parsed = urlparse(fileUrl)\n    netloc = parsed.netloc.strip('/')\n    path = parsed.path.strip('/')\n    localfile = f'{downloadDir}/{netloc}/{path}'\n\n    # Remove the filename from the path in order to \n    # make the directory structure leading up to it\n    localpath = '/'.join(localfile.split('/')[:-1])\n    if not os.path.exists(localpath):\n        os.makedirs(localpath)\n    return localfile\n\nhtml = urlopen(baseUrl)\nbs = BeautifulSoup(html, 'html.parser')\ndownloadList = bs.findAll(src=True)\n\nfor download in downloadList:\n    fileUrl = getAbsoluteURL(download['src'])\n    if fileUrl is not None:\n        try:\n            urlretrieve(fileUrl, getDownloadPath(fileUrl))\n            print(fileUrl)\n        except Exception as e:\n            print(f'Could not retrieve {fileUrl} Error: {e}')\n\n```", "```py\nfruit,cost\napple,1.00\nbanana,0.30\npear,1.25\n```", "```py\nimport csv\n\ncsvFile = open('test.csv', 'w+')\ntry:\n    writer = csv.writer(csvFile)\n    writer.writerow(('number', 'number plus 2', 'number times 2'))\n    for i in range(10):\n        writer.writerow( (i, i+2, i*2))\nfinally:\n    csvFile.close()\n```", "```py\nnumber,number plus 2,number times 2\n0,2,0\n1,3,2\n2,4,4\n...\n```", "```py\nimport csv\nfrom urllib.request import urlopen\nfrom bs4 import BeautifulSoup\n\nhtml = urlopen('https://en.wikipedia.org/wiki/\n       List_of_countries_with_McDonald%27s_restaurants')\nbs = BeautifulSoup(html, 'html.parser')\n# The main comparison table is currently the first table on the page\ntable = bs.find('table',{'class':'wikitable'})\nrows = table.findAll('tr')\ncsvFile = open('countries.csv', 'wt+')\nwriter = csv.writer(csvFile)\ntry:\n    for row in rows:\n        csvRow = []\n        for cell in row.findAll(['td', 'th']):\n            csvRow.append(cell.get_text().strip())\n        writer.writerow(csvRow)\nfinally:\n    csvFile.close()\n\n```", "```py\nSELECT * FROM users WHERE firstname = \"Ryan\"\n```", "```py\n$ sudo apt-get install mysql-server\n```", "```py\n$ cd /usr/local/mysql\n$ sudo ./bin/mysqld_safe\n```", "```py\n$ brew install mysql\n\n```", "```py\n$ mysql -u root -p\n\n```", "```py\n> CREATE DATABASE scraping;\n```", "```py\n> USE scraping;\n```", "```py\n> CREATE TABLE pages;\n```", "```py\nERROR 1113 (42000): A table must have at least 1 column\n```", "```py\n> CREATE TABLE pages (id BIGINT(7) NOT NULL AUTO_INCREMENT,\ntitle VARCHAR(200), content VARCHAR(10000),\ncreated TIMESTAMP DEFAULT CURRENT_TIMESTAMP, PRIMARY KEY(id));\n```", "```py\n> DESCRIBE pages;\n+---------+----------------+------+-----+-------------------+----------------+\n| Field   | Type           | Null | Key | Default           | Extra          |\n+---------+----------------+------+-----+-------------------+----------------+\n| id      | bigint(7)      | NO   | PRI | NULL              | auto_increment |\n| title   | varchar(200)   | YES  |     | NULL              |                |\n| content | varchar(10000) | YES  |     | NULL              |                |\n| created | timestamp      | NO   |     | CURRENT_TIMESTAMP |                |\n+---------+----------------+------+-----+-------------------+----------------+\n4 rows in set (0.01 sec)\n\n```", "```py\n> INSERT INTO pages (title, content) VALUES (\"Test page title\",\n\"This is some test page content. It can be up to 10,000 characters\nlong.\");\n```", "```py\n> INSERT INTO pages (id, title, content, created) VALUES (3, \n\"Test page title\",\n\"This is some test page content. It can be up to 10,000 characters\nlong.\", \"2014-09-21 10:25:32\");\n```", "```py\n> SELECT * FROM pages WHERE id = 2;\n```", "```py\n> SELECT * FROM pages WHERE title LIKE \"%test%\";\n```", "```py\n> SELECT id, title FROM pages WHERE content LIKE \"%page content%\";\n```", "```py\n> DELETE FROM pages WHERE id = 1;\n```", "```py\n> UPDATE pages SET title=\"A new title\",\ncontent=\"Some new content\" WHERE id=2;\n```", "```py\n$ pip install PyMySQL\n\n```", "```py\nimport pymysql\nconn = pymysql.connect(\n    host='127.0.0.1',\n​    unix_socket='/tmp/mysql.sock',\n​    user='root',\n​    passwd=None,\n​    db='mysql'\n)\ncur = conn.cursor()\ncur.execute('USE scraping')\ncur.execute('SELECT * FROM pages WHERE id=1')\nprint(cur.fetchone())\ncur.close()\nconn.close()\n```", "```py\nALTER DATABASE scraping CHARACTER SET = utf8mb4 COLLATE = utf8mb4_unicode_ci;\nALTER TABLE pages CONVERT TO CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;\nALTER TABLE pages CHANGE title title VARCHAR(200) CHARACTER SET utf8mb4 COLLATE \nutf8mb4_unicode_ci;\nALTER TABLE pages CHANGE content content VARCHAR(10000) CHARACTER SET utf8mb4 \nCOLLATE utf8mb4_unicode_ci;\n```", "```py\nconn = pymysql.connect(host='127.0.0.1', unix_socket='/tmp/mysql.sock',\n                       user='root', passwd=None, db='mysql', charset='utf8')\ncur = conn.cursor()\ncur.execute('USE scraping')\n\nrandom.seed(datetime.datetime.now())\n\ndef store(title, content):\n    cur.execute('INSERT INTO pages (title, content) VALUES '\n        '(\"%s\", \"%s\")', (title, content))\n    cur.connection.commit()\n\ndef getLinks(articleUrl):\n    html = urlopen('http://en.wikipedia.org'+articleUrl)\n    bs = BeautifulSoup(html, 'html.parser')\n    title = bs.find('h1').get_text()\n    content = bs.find('div', {'id':'mw-content-text'}).find('p')\n        .get_text()\n    store(title, content)\n    return bs.find('div', {'id':'bodyContent'}).find_all('a',\n        href=re.compile('^(/wiki/)((?!:).)*$'))\n\nlinks = getLinks('/wiki/Kevin_Bacon')\ntry:\n    while len(links) > 0:\n         newArticle = links[random.randint(0, len(links)-1)].attrs['href']\n         print(newArticle)\n         links = getLinks(newArticle)\nfinally:\n    cur.close()\n    conn.close()\n\n```", "```py\n>SELECT * FROM dictionary WHERE definition=\"A small furry animal that says meow\";\n+------+-------+-------------------------------------+\n| id   | word  | definition                          |\n+------+-------+-------------------------------------+\n|  200 | cat   | A small furry animal that says meow |\n+------+-------+-------------------------------------+\n1 row in set (0.00 sec)\n```", "```py\nCREATE INDEX definition ON dictionary (id, definition(16));\n```", "```py\n+--------+--------------+------+-----+---------+----------------+\n| Field  | Type         | Null | Key | Default | Extra          |\n+--------+--------------+------+-----+---------+----------------+\n| id     | int(11)      | NO   | PRI | NULL    | auto_increment |\n| url    | varchar(200) | YES  |     | NULL    |                |\n| phrase | varchar(200) | YES  |     | NULL    |                |\n+--------+--------------+------+-----+---------+----------------+\n```", "```py\n>DESCRIBE phrases\n+--------+--------------+------+-----+---------+----------------+\n| Field  | Type         | Null | Key | Default | Extra          |\n+--------+--------------+------+-----+---------+----------------+\n| id     | int(11)      | NO   | PRI | NULL    | auto_increment |\n| phrase | varchar(200) | YES  |     | NULL    |                |\n+--------+--------------+------+-----+---------+----------------+\n\n >DESCRIBE urls\n +-------+--------------+------+-----+---------+----------------+\n| Field | Type         | Null | Key | Default | Extra          |\n+-------+--------------+------+-----+---------+----------------+\n| id    | int(11)      | NO   | PRI | NULL    | auto_increment |\n| url   | varchar(200) | YES  |     | NULL    |                |\n+-------+--------------+------+-----+---------+----------------+\n\n>DESCRIBE foundInstances\n+-------------+---------+------+-----+---------+----------------+\n| Field       | Type    | Null | Key | Default | Extra          |\n+-------------+---------+------+-----+---------+----------------+\n| id          | int(11) | NO   | PRI | NULL    | auto_increment |\n| urlId       | int(11) | YES  |     | NULL    |                |\n| phraseId    | int(11) | YES  |     | NULL    |                |\n| occurrences | int(11) | YES  |     | NULL    |                |\n+-------------+---------+------+-----+---------+----------------+\n```", "```py\nCREATE DATABASE wikipedia;\nUSE wikipedia;\n\nCREATE TABLE wikipedia.pages (\n  id INT NOT NULL AUTO_INCREMENT,\n  url VARCHAR(255) NOT NULL,\n  created TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n  PRIMARY KEY (id)\n);\n\nCREATE TABLE wikipedia.links (\n  id INT NOT NULL AUTO_INCREMENT,\n  fromPageId INT NULL,\n  toPageId INT NULL,\n  created TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n  PRIMARY KEY (id)\n);\n\n```", "```py\nfrom urllib.request import urlopen\nfrom bs4 import BeautifulSoup\nimport re\nimport pymysql\nfrom random import shuffle\n\nconn = pymysql.connect(host='127.0.0.1', unix_socket='/tmp/mysql.sock',\n                       user='root', passwd='password', db='mysql', \n                             charset='utf8')\ncur = conn.cursor()\ncur.execute('USE wikipedia')\n\ndef insertPageIfNotExists(url):\n    cur.execute('SELECT id FROM pages WHERE url = %s LIMIT 1', (url))\n    page = cur.fetchone()\n    if not page:\n        cur.execute('INSERT INTO pages (url) VALUES (%s)', (url))\n        conn.commit()\n        return cur.lastrowid\n    else:\n        return page[0]\n\ndef loadPages():\n    cur.execute('SELECT url FROM pages')\n    return [row[0] for row in cur.fetchall()]\n\ndef insertLink(fromPageId, toPageId):\n    cur.execute(\n        'SELECT EXISTS(SELECT 1 FROM links WHERE fromPageId = %s\\\n AND toPageId = %s)'\n        ,(int(fromPageId),\n        int(toPageId))\n    )\n    if not cur.fetchone()[0]:\n        cur.execute('INSERT INTO links (fromPageId, toPageId) VALUES (%s, %s)', \n                    (int(fromPageId), int(toPageId)))\n        conn.commit()\n\ndef pageHasLinks(pageId):\n    cur.execute(\n        'SELECT EXISTS(SELECT 1 FROM links WHERE fromPageId = %s)'\n        , (int(pageId))\n    )\n    return cur.fetchone()[0]\n\ndef getLinks(pageUrl, recursionLevel, pages):\n    if recursionLevel > 4:\n        return\n\n    pageId = insertPageIfNotExists(pageUrl)\n    html = urlopen(f'http://en.wikipedia.org{pageUrl}')\n    bs = BeautifulSoup(html, 'html.parser')\n    links = bs.findAll('a', href=re.compile('^(/wiki/)((?!:).)*$'))\n    links = [link.attrs['href'] for link in links]\n\n    for link in links:\n        linkId = insertPageIfNotExists(link)\n        insertLink(pageId, linkId)\n        if not pageHasLinks(linkId):\n            print(f'Getting {link}')\n            pages.append(link)\n            getLinks(link, recursionLevel+1, pages)\n        else:\n            print(f'Already fetched {link}')\n\ngetLinks('/wiki/Kevin_Bacon', 0, loadPages()) \ncur.close()\nconn.close()\n\n```", "```py\nimport smtplib\nfrom email.mime.text import MIMEText\n\nmsg = MIMEText('The body of the email is here')\n\nmsg['Subject'] = 'An Email Alert'\nmsg['From'] = 'ryan@pythonscraping.com'\nmsg['To'] = 'webmaster@pythonscraping.com'\n\ns = smtplib.SMTP('localhost')\ns.send_message(msg)\ns.quit()\n\n```", "```py\nimport smtplib\nfrom email.mime.text import MIMEText\nfrom bs4 import BeautifulSoup\nfrom urllib.request import urlopen\nimport time\n\ndef sendMail(subject, body):\n    msg = MIMEText(body)\n    msg['Subject'] = subject\n    msg['From'] ='christmas_alerts@pythonscraping.com'\n    msg['To'] = 'ryan@pythonscraping.com'\n\n    s = smtplib.SMTP('localhost')\n    s.send_message(msg)\n    s.quit()\n\nbs = BeautifulSoup(urlopen('https://isitchristmas.com/'), 'html.parser')\nwhile(bs.find('a', {'id':'answer'}).attrs['title'] == 'NO'):\n    print('It is not Christmas yet.')\n    time.sleep(3600)\n    bs = BeautifulSoup(urlopen('https://isitchristmas.com/'), 'html.parser')\n\nsendMail('It\\'s Christmas!', \n         'According to http://itischristmas.com, it is Christmas!')\n\n```"]