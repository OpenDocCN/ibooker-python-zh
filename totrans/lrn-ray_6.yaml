- en: Chapter 7\. Data Processing with Ray
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 7 章. 使用 Ray 进行数据处理
- en: Edward Oakes
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Edward Oakes
- en: 'In the previous chapter, you learned how to scale machine learning training
    using Ray Train. Of course, the key component to applying machine learning in
    practice is data. In this chapter, we’ll explore the core set of data processing
    capabilities on Ray: Ray Data.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，您学习了如何使用 Ray 训练来扩展机器学习训练。当然，将机器学习应用于实践的关键组件是数据。在本章中，我们将探讨 Ray 上的核心数据处理能力集：Ray
    数据。
- en: While not meant to be a replacement for more general data processing systems
    such as Apache Spark or Apache Hadoop, Ray Data offers basic data processing capabilities
    and a standard way to load, transform, and pass data to different parts of a Ray
    application. This enables an ecosystem of libraries on Ray to all speak the same
    language so users can mix and match functionality in a framework-agnostic way
    to meet their needs.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然并不意味着取代 Apache Spark 或 Apache Hadoop 等更通用的数据处理系统，但 Ray 数据提供了基本的数据处理功能和一种标准方法，用于在
    Ray 应用程序的不同部分加载、转换和传递数据。这使得 Ray 上的库生态系统能够以一种框架不可知的方式说同一种语言，用户可以混合和匹配功能，以满足他们的需求。
- en: The central component of the Ray Data ecosystem is the aptly-named “Datasets,”
    which offers the core abstractions for loading, transforming, and passing references
    to data in a Ray cluster. Datasets are the “glue” that enable different libraries
    to interoperate on top of Ray. You’ll see this in action later in the chapter,
    as we show how you can do dataframe processing using the full expressiveness of
    the Dask API using Dask on Ray and transform the result into a Dataset, as well
    as how a Dataset can be used to efficiently do distributed deep learning training
    at scale using Ray Train and Ray Tune.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Ray 数据生态系统的核心组件是名副其实的“数据集”，它提供了在 Ray 集群中加载、转换和传递数据的核心抽象。数据集是使不同库能够在 Ray 之上相互操作的“粘合剂”。在本章中，您将看到这一点的实际操作，因为我们将展示如何使用
    Dask on Ray 进行数据帧处理，并使用 Ray 训练和 Ray 调整将结果转换为数据集，以及如何使用数据集有效地进行大规模分布式深度学习训练。
- en: 'The main benefits of Datasets are:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的主要优势包括：
- en: Flexibility
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 灵活性
- en: Datasets support a wide range of data formats, work seamlessly with library
    integrations like Dask on Ray, and can be passed between Ray tasks and actors
    without copying data.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集支持各种数据格式，在 Ray 上与 Dask on Ray 等库集成无缝，可以在 Ray 任务和角色之间传递而不复制数据。
- en: Performance for ML workloads
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习工作负载性能
- en: Datasets offers important features like accelerator support, pipelining, and
    global random shuffles that accelerate ML training and inference workloads.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集提供重要功能，如加速器支持、流水线处理和全局随机洗牌，加速机器学习训练和推理工作负载。
- en: This chapter is intended to get you familiar with the core concepts for doing
    data processing on Ray and to help you understand how to accomplish common patterns
    and why you would choose to use different pieces to accomplish a task. The chapter
    assumes a basic familiarity with data processing concepts such as map, filter,
    groupby, and partitions, but it’s not to be a tutorial on data science in general
    nor a deep dive into the internals of how these operations are implemented. Readers
    with a limited data processing / data science background should not have a problem
    following along.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章旨在使您熟悉在 Ray 上进行数据处理的核心概念，并帮助您了解如何完成常见模式以及为什么选择使用不同的部分来完成任务。本章假定您对诸如映射、过滤、分组和分区等数据处理概念有基本了解，但不是一般数据科学的教程，也不是对这些操作的内部实现进行深入挖掘。具有有限数据处理/数据科学背景的读者不应该在跟进时遇到问题。
- en: 'We’ll start by giving a basic introduction to the core building block: Ray
    Datasets. This will cover the architecture, basics of the API, and an example
    how Datasets can be used to making building complex data-intensive applications
    easy. Then, we’ll briefly cover external library integrations on Ray, focusing
    on Dask on Ray. Finally, we’ll bring it all together by building a scalable end-to-end
    machine learning pipeline in a single Python script.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从对核心构建模块 Ray 数据集进行基本介绍开始。这将涵盖架构、API 基础知识以及示例，展示如何使用数据集来简化构建复杂的数据密集型应用程序。然后，我们将简要介绍
    Ray 上的外部库集成，重点放在 Dask on Ray 上。最后，我们将通过在单个 Python 脚本中构建可扩展的端到端机器学习管道来将所有内容汇总。
- en: The notebook for this chapter [is also available online](https://github.com/maxpumperla/learning_ray/blob/main/notebooks/ch_07_data_processing.ipynb).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的笔记本[也可在线获得](https://github.com/maxpumperla/learning_ray/blob/main/notebooks/ch_07_data_processing.ipynb)。
- en: Ray Datasets
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Ray 数据集
- en: An Overview of Datasets
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集概述
- en: The main goal of Datasets is to support a scalable, flexible abstraction for
    data processing on Ray. Datasets are intended to be the standard way to read,
    write, and transfer data across the full ecosystem of Ray libraries. One of the
    most powerful uses of Datasets is acting as the data ingest and preprocessing
    layer for machine learning workloads, allowing you to efficiently scale up training
    using Ray Train and Ray Tune. This will be explored in more detail in the last
    section of the chapter.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Datasets 的主要目标是支持在 Ray 上进行数据处理的可扩展、灵活的抽象。Datasets 旨在成为 Ray 库完整生态系统中读取、写入和传输数据的标准方式。Datasets
    最强大的用途之一是作为机器学习工作负载的数据摄入和预处理层，使您能够通过 Ray Train 和 Ray Tune 高效扩展训练。这将在本章的最后一节中详细探讨。
- en: If you’ve worked with other distributed data processing APIs such as Apache
    Spark’s RDDs in the past, you will find the Datasets API very familiar. The core
    of the API leans on functional programming and offers standard functionality such
    as reading/writing many different data sources, performing basic transformations
    like map, filter, and sort, as well as some simple aggregations such as groupby.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你之前使用过其他分布式数据处理 API，比如 Apache Spark 的 RDDs，那么你会觉得 Datasets API 非常熟悉。该 API
    的核心依赖于函数式编程，并提供标准功能，如读取/写入多种不同的数据源，执行诸如映射、过滤和排序等基本转换，以及一些简单的聚合操作，比如分组。
- en: Under the hood, Datasets implements Distributed [Apache Arrow](https://arrow.apache.org/).
    Apache Arrow is a unified columnar data format for data processing libraries and
    applications, so integrating with it means that Datasets get interoperability
    with many of the most popular processing libraries such as NumPy and pandas out
    of the box.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在底层，Datasets 实现了分布式[Apache Arrow](https://arrow.apache.org/)。Apache Arrow 是一个统一的列式数据格式，用于数据处理库和应用程序，因此与之集成意味着
    Datasets 能够与诸如 NumPy 和 pandas 等最受欢迎的处理库直接进行交互。
- en: A Dataset consists of a list of Ray object references, each of which points
    at a “block” of data. These blocks are either Arrow tables or Python lists (for
    data that isn’t supported by the Arrow format) in Ray’s shared memory object store,
    and compute over the data such as for map or filter operations happens in Ray
    tasks (and sometimes actors).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 Dataset 包含一系列 Ray 对象引用，每个引用指向数据的“块”。这些块可以是 Arrow 表格，也可以是 Python 列表（对于不支持
    Arrow 格式的数据），存储在 Ray 的共享内存对象存储中，数据的计算（如映射或过滤操作）发生在 Ray 任务（有时是 actor）中。
- en: 'Because Datasets relies on the core Ray primitives of tasks and objects in
    the shared memory object store, it inherits key benefits of Ray: scalability to
    hundreds of nodes, efficient memory usage due to sharing memory across processes
    on the same node, and object spilling + recovery to gracefully handle failures.
    Additionally, because Datasets are just lists of object references, they can also
    be passed between tasks and actors efficiently without needing to make a copy
    of the data, which is crucial for making data-intensive applications and libraries
    scalable.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Datasets 依赖于 Ray 的任务和共享内存对象存储的核心原语，它继承了 Ray 的关键优势：可扩展到数百个节点、由于在同一节点上的进程之间共享内存而具有高效的内存使用率，以及对象溢出和恢复以优雅处理故障。此外，由于
    Datasets 只是对象引用列表，它们还可以在任务和 actor 之间高效传递，而无需复制数据，这对于使数据密集型应用程序和库具有可扩展性至关重要。
- en: Datasets Basics
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Datasets 基础
- en: This section will give a basic overview of Ray Datasets, covering how to get
    started reading, writing, and transforming datasets. This is not meant to be a
    comprehensive reference, but rather to introduce you to the basic concepts so
    we can build up to some interesting examples of what makes Ray Datasets powerful
    in later sections. For up-to-date information on what’s supported and exact syntax,
    see the [Datasets documentation](https://docs.ray.io/en/latest/data/dataset.xhtml).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将基本概述 Ray Datasets，涵盖如何开始读取、写入和转换数据集。这并不意味着是全面参考，而是为了介绍基本概念，以便我们在后面的部分中构建一些有趣的示例，展示
    Ray Datasets 的强大之处。有关支持的最新信息和确切语法，请参阅[Datasets 文档](https://docs.ray.io/en/latest/data/dataset.xhtml)。
- en: 'To follow along with the examples in this section, please make sure you have
    Ray Data installed locally:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 若要按照本节示例操作，请确保在本地安装了 Ray Data：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Creating a dataset
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建数据集
- en: 'First, let’s create a simple Dataset and perform some basic operations on it:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们创建一个简单的数据集并对其执行一些基本操作：
- en: Example 7-1\.
  id: totrans-26
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-1\.
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here we created a Dataset containing the numbers from 0 to 10000, then printed
    some basic information about it: the total number of records, a few samples, and
    the schema (we will discuss this in more detail later).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个包含从 0 到 10000 的数字的数据集，然后打印了一些关于它的基本信息：记录总数，一些样本以及架构（稍后我们将详细讨论这一点）。
- en: Reading from and writing to storage
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从存储中读取和写入
- en: 'Of course, for real workloads you’ll often want to read from and write to persistent
    storage to load your data and write the results. Writing and reading Datasets
    is simple; for example, to write a Dataset to a CSV file and then load it back
    into memory, we just need to use the builtin write_csv and read_csv utilities:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，对于真实的工作负载，您通常会希望从持久存储中读取数据并写入结果。写入和读取数据集非常简单；例如，要将数据集写入 CSV 文件，然后将其加载回内存，我们只需使用内置的
    `write_csv` 和 `read_csv` 实用程序即可：
- en: Example 7-2\.
  id: totrans-31
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-2\.
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Datasets supports a number of common serialization formats such as CSV, JSON,
    and Parquet and can read from or write to local disk as well as remote storage
    like HDFS or AWS S3.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集支持多种常见的序列化格式，如 CSV、JSON 和 Parquet，并且可以从/写入到本地磁盘以及像 HDFS 或 AWS S3 这样的远程存储。
- en: In the example above, we provided just a local file path (`"local_dir"`) so
    the Dataset was written to a directory on the local machine. If we wanted to write
    to and read from S3 instead, we would instead provide a path like `"s3://my_bucket/"`
    and Datasets would automatically handle efficiently reading/writing remote storage,
    parallelizing the requests across many tasks to improve throughput.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的例子中，我们仅提供了一个本地文件路径（`"local_dir"`），因此数据集被写入到本地机器上的一个目录中。如果我们想要改为写入和读取 S3，我们需要提供类似
    `"s3://my_bucket/"` 的路径，数据集将自动处理有效地读写远程存储，并通过多任务并行化请求以提高吞吐量。
- en: Note that Datasets also supports custom datasources that you can use to write
    to any external data storage system that isn’t supported out-of-the-box.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，数据集还支持自定义数据源，您可以使用它们来写入到任何不受开箱即用支持的外部数据存储系统。
- en: Built-in transformations
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内置转换
- en: 'Now that we understand the basic APIs around how to create and inspect Datasets,
    let’s take a look at some of the builtin operations we can do on them. The code
    sample below shows three basic operations that Datasets support: - First, we `union`
    two Datasets together. The result is a new Dataset that contains all of the records
    of both. - Then, we `filter` the elements of a Dataset to only include even integers
    by providing a custom filter function. - Finally, we `sort` the Dataset.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何创建和检查数据集的基本 API，让我们来看一些可以在其上执行的内置操作。下面的代码示例展示了数据集支持的三个基本操作：- 首先，我们`union`两个数据集。结果是一个包含两者所有记录的新数据集。-
    然后，我们通过提供自定义过滤函数，`filter`数据集中的元素仅包括偶数整数。- 最后，我们对数据集进行`sort`排序。
- en: Example 7-3\.
  id: totrans-38
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-3\.
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In addition to the operations above, Datasets also support common aggregations
    you might expect such as `groupby`, `sum`, `min`, etc. You can also pass a user-defined
    function for custom aggregations.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述操作外，数据集还支持您可能期望的常见聚合，例如 `groupby`、`sum`、`min` 等。您还可以传递用户定义的函数进行自定义聚合。
- en: Blocks and repartitioning
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 块和重新分区
- en: One of the important things to keep in mind when using Datasets is the concept
    of *blocks* discussed earlier in the section. Blocks are the underlying chunks
    of data that make up a Dataset; operations are applied to the underlying data
    one block at a time. If the number of blocks in a Dataset is too high, each block
    will be small and there will be a lot of overhead for each operation. If the number
    of blocks is too small, operations won’t be able to be parallelized as efficiently.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 使用数据集时要牢记的一件重要事情是*块*的概念，在本节中之前已经讨论过。块是构成数据集的基本数据块，操作是逐块应用到底层数据的。如果数据集中的块数过多，每个块将很小，并且每次操作都会有很多开销。如果块数太少，则操作无法高效并行化。
- en: 'If we take a peek under the hood from the example above, we can see that the
    initial datasets we created each had 200 blocks by default and when we combined
    them, the resulting Dataset had 400 blocks. In this case, we may want to repartition
    the Dataset to bring it back to the original 200 blocks that we started with:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们从上面的例子中查看一下，我们可以看到我们创建的初始数据集默认每个有 200 个块，当我们将它们组合时，结果数据集有 400 个块。在这种情况下，我们可能希望重新分区数据集，将其恢复到最初的
    200 个块：
- en: Example 7-4\.
  id: totrans-44
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-4\.
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Blocks also control the number of files that are created when we write a Dataset
    to storage (so if you want all of the data to be coalesced into a single output
    file, you should call `.repartition(1)` before writing it).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 块还控制了在将Dataset写入存储时创建的文件数（因此，如果希望所有数据合并到单个输出文件中，应在写入之前调用`.repartition(1)`）。
- en: Schemas and data formats
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模式和数据格式
- en: Up until this point, we’ve been operating on simple Datasets made up only of
    integers. However, for more complex data processing we often want to have a schema,
    allowing us to more easily comprehend the data and enforce types on each column.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在处理仅由整数组成的简单Dataset。然而，对于更复杂的数据处理，我们经常希望有一个模式，这样可以更轻松地理解数据并强制每列的类型。
- en: Given that datasets are meant to be the point of interoperation for applications
    and libraries on Ray, they are designed to be agnostic to a specific datatype
    and offer flexibility to read, write, and convert between many popular data formats.
    Datasets by supporting Arrow’s columnar format, which enables converting between
    different types of structured data such as Python dictionaries, DataFrames, and
    serialized parquet files.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于数据集旨在成为Ray上应用程序和库的交互点，它们被设计为对特定数据类型不可知，并提供灵活性以在许多流行的数据格式之间读取、写入和转换。通过支持Arrow的列格式，Dataset能够在Python字典、DataFrames和序列化parquet文件等不同类型的结构化数据之间转换。
- en: 'The simplest way to create a Dataset with a schema is to create it from a list
    of Python dictionaries:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 创建带有模式的Dataset的最简单方法是从Python字典列表创建它：
- en: Example 7-5\.
  id: totrans-51
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-5。
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In this case, the schema was inferred from the keys in the dictionaries we
    passed in. We can also convert to/from data types from popular libraries such
    as pandas:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，模式是从我们传递的字典键中推断出来的。我们还可以将数据类型与流行库（如pandas）的数据类型进行转换：
- en: Example 7-6\.
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-6。
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here we went from a Dataset to a pandas DataFrame, but this also works in reverse:
    if you create a Dataset from a dataframe, it will automatically inherit the schema
    from the DataFrame.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们从Dataset转换为pandas DataFrame，但反向操作也适用：如果您从DataFrame创建Dataset，它将自动继承DataFrame的模式。
- en: Computing Over Datasets
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Dataset上计算
- en: In the section above, we introduced some of the functionality that comes built
    in with Ray Datasets such as filtering, sorting, and unioning. However, one of
    the most powerful parts of Datasets is that they allow you to harness the flexible
    compute model of Ray and perform computations efficiently over large amounts of
    data.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的部分中，我们介绍了Ray Dataset提供的一些功能，如过滤、排序和合并。然而，Dataset最强大的部分之一是它们允许您利用Ray的灵活计算模型，并高效地处理大量数据。
- en: 'The primary way to perform a custom transformation on a Dataset is using `.map()`.
    This allows you to pass a custom function that will be applied to the records
    of a Dataset. A basic example might be to square the records of a Dataset:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 执行Dataset上的自定义转换的主要方法是使用`.map()`。这允许您传递一个自定义函数，该函数将应用于Dataset的记录。一个基本的例子可能是将Dataset的记录平方：
- en: Example 7-7\.
  id: totrans-60
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-7。
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In this example, we passed a simple lambda function and the data we operated
    on was integers, but we could pass any function here and operate on structured
    data that supports the Arrow format.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们传递了一个简单的lambda函数，我们操作的数据是整数，但我们可以在这里传递任何函数，并在支持Arrow格式的结构化数据上操作。
- en: We can also choose to map batches of data instead of individual records using
    `.map_batches()`. There are some types of computations that are much more efficient
    when they’re *vectorized*, meaning that they use an algorithm or implementation
    that is more efficient operating on a set of items instead of one at a time.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以选择使用`.map_batches()`来映射数据批次，而不是单个记录。有些类型的计算在进行*向量化*时效率更高，这意味着它们使用一种更高效的算法或实现，可以同时操作一组项目。
- en: 'Revisiting our simple example of squaring the values in the Dataset, we can
    rewrite it to be performed in batches and use the `numpy.square` optimized implementation
    instead of the naive Python implemenation:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 重新访问我们简单的平方Dataset值的例子，我们可以重写它以批处理执行，并使用优化的`numpy.square`实现，而不是朴素的Python实现：
- en: Example 7-8\.
  id: totrans-65
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-8。
- en: '[PRE8]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Vectorized computations are especially useful on GPUs when performing deep learning
    training or inference. However, generally performing computations on GPUs also
    has significant fixed cost due to needing to load model weights or other data
    into the GPU RAM. For this purpose, Datasets supports mapping data using Ray actors.
    Ray actors are long lived and can hold state, as opposed to stateless Ray tasks,
    so we can cache expensive operations costs by running them in the actor’s constructor
    (such as loading a model onto a GPU).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行深度学习训练或推理时，矢量化计算在 GPU 上尤其有用。然而，通常在 GPU 上执行计算也具有显著的固定成本，因为需要将模型权重或其他数据加载到
    GPU 内存中。为此，Datasets 支持使用 Ray actors 对数据进行映射。Ray actors 寿命长且可以保存状态，与无状态的 Ray tasks
    相反，因此我们可以通过在 actor 的构造函数中运行它们（例如将模型加载到 GPU 上）来缓存昂贵的操作成本。
- en: To perform batch inference using Datasets, we need to pass a class instead of
    a function, specify that this computation should run using actors, and use `.map_batches()`
    so we can perform vectorized inference. If we want this to run on a GPU, we would
    also pass `num_gpus=1`, which specifies that the actors running the map function
    each require a GPU. Datasets will automatically autoscale a group of actors to
    perform the map operation.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 Datasets 执行批处理推理，我们需要传递一个类而不是一个函数，指定此计算应使用 actors 运行，并使用 `.map_batches()`，以便我们可以执行矢量化推理。如果我们希望这在
    GPU 上运行，还会传递`num_gpus=1`，这指定了运行 map 函数的 actor 每个都需要一个 GPU。Datasets 将自动为一组 actor
    进行自动缩放，以执行映射操作。
- en: Example 7-9\.
  id: totrans-69
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-9\.
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Dataset Pipelines
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集管道
- en: 'By default, Dataset operations are blocking, meaning they run synchronously
    from start to finish and there is only a single operation happening at a time.
    This pattern can be very inefficient for some workloads, however. For example,
    consider the following set of Dataset transformations that might be used to do
    batch inference for a machine learning model:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Dataset 操作是阻塞的，这意味着它们从开始到结束同步运行，一次只有一个操作发生。然而，对于某些工作负载，这种模式可能非常低效。例如，考虑以下一组
    Dataset 转换，这些转换可能用于对机器学习模型进行批处理推理：
- en: Example 7-10\.
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-10\.
- en: '[PRE10]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'There are five stages to this pipeline, and each of them stresses different
    parts of the system: - Reading from remote storage requires ingress bandwidth
    to the cluster and may be limited by the throughput of the storage system. - Preprocessing
    the inputs requires CPU resources. - Vectorized inference on the model requires
    GPU resources. - Repartitioning requires network bandwidth within the cluster.
    - Writing to remote storage requires egress bandwidth from the cluster and may
    be limited by the throughput of storage once again.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 此流水线共有五个阶段，每个阶段都强调系统的不同部分： - 从远程存储中读取需要集群入口带宽，并可能受到存储系统吞吐量的限制。 - 对输入进行预处理需要
    CPU 资源。 - 在模型上进行矢量化推理需要 GPU 资源。 - 重新分区需要集群内部的网络带宽。 - 写入远程存储需要集群出口带宽，并可能再次受到存储吞吐量的限制。
- en: '![Inefficient Dataset Computation](assets/data_pipeline_1.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![低效的数据集计算](assets/data_pipeline_1.png)'
- en: Figure 7-1\. A naive Dataset computation, leading to idle resources between
    stages
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-1\. 一个天真的 Dataset 计算，导致阶段之间的空闲资源
- en: In this scenario, it would likely be much more efficient to instead *pipeline*
    the the stages and allow them to overlap. This means that as soon as some data
    has been read from storage, it is fed into the preprocessing stage, then to the
    inference stage, and so on.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，相对于顺序执行阶段并允许它们重叠，很可能效率会更高。这意味着一旦从存储中读取了一些数据，它就被馈送到预处理阶段，然后到推理阶段，依此类推。
- en: '![Optimized DatasetPipeline](assets/data_pipeline_2.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![优化的 DatasetPipeline](assets/data_pipeline_2.png)'
- en: Figure 7-2\. An optimized DatasetPipeline that enables overlapping compute between
    stages and reduces idle resources
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-2\. 一个优化的 DatasetPipeline，实现了阶段之间的重叠计算并减少了空闲资源
- en: This pipelining will improve the overall resource usage of the end-to-end workload,
    improving throughput and therefore decreasing the cost it takes to run the computation
    (fewer idle resources is better!).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这种流水线将改善端到端工作负载的整体资源使用情况，提高吞吐量，从而减少运行计算所需的成本（更少的空闲资源更好！）。
- en: Datasets can be converted to [DatasetPipelines](https://docs.ray.io/en/latest/data/dataset-pipeline.xhtml)
    using `ds.window()`, enabling the pipelining behavior that we want in this scenario.
    A window specifies the number of blocks that will be passed through a stage in
    the pipeline before being passed to the next stage in the pipeline. This behavior
    can be tuned using the `blocks_per_window` parameter, which defaults to 10.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `ds.window()` 可以将数据集转换为 [DatasetPipelines](https://docs.ray.io/en/latest/data/dataset-pipeline.xhtml)，从而实现我们在这种场景中希望的管道化行为。窗口指定在通过管道中的一个阶段之前将通过多少个块传递到下一个阶段。可以使用
    `blocks_per_window` 参数进行调整，默认为 10。
- en: 'Let’s rewrite the inefficient pseudocode above to use a DatasetPipeline instead:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重写上面低效伪代码，改用 DatasetPipeline：
- en: Example 7-11\.
  id: totrans-84
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-11\.
- en: '[PRE11]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The only modification made was the addition of a `.window()` call after `read_parquet`
    and before the preprocessing stage. Now the Dataset has been converted to a DatasetPipeline
    and its stages will proceed in parallel in 5-block windows, decreasing idle resources
    and improving efficiency.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一的修改是在 `read_parquet` 之后和预处理阶段之前添加了 `.window()` 调用。现在 Dataset 已经转换为 DatasetPipeline，并且其阶段将以
    5 块窗口并行进行，减少空闲资源并提高效率。
- en: DatasetPipelines can also be created using `ds.repeat()` to repeat stages in
    a pipeline a finite or infinite number of times. This will be explored further
    in the next section, where we’ll use it for a training workload. Of course, pipelining
    can be equally beneficial for performance for training in addition to inference.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `ds.repeat()` 可以创建 DatasetPipelines，以有限次数或无限次数重复管道中的阶段。在下一节中我们将进一步探讨这一点，我们将在训练工作负载中使用它。当然，除了推理之外，管道化对于训练的性能同样有益。
- en: 'Example: Parallel SGD from Scratch'
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例：从头开始的并行 SGD
- en: One of the key benefits of Datasets is that they can be passed between tasks
    and actors. In this section, we’ll explore how this can be used to write efficient
    implementations of complex distributed workloads like distributed hyperparameter
    tuning and machine learning training.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的一个关键优点是它们可以在任务和参与者之间传递。在本节中，我们将探讨如何利用这一点来编写复杂分布式工作负载的高效实现，如分布式超参数调整和机器学习训练。
- en: 'As discussed in [Chapter 5](ch05.xhtml#chapter_05), a common pattern in machine
    learning training is to explore a range of “hyperparameters” to find the ones
    that result in the best model. We may want to run across a wide range of hyperparameters,
    and doing this naively could be very expensive. Datasets allows us to easily share
    the same in-memory data across a range of parallel training runs in a single Python
    script: we can load and preprocess the data once, then pass a reference to it
    to many downstream actors who can read the data from shared memory.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [第 5 章](ch05.xhtml#chapter_05) 中讨论的，机器学习训练中的一个常见模式是探索一系列“超参数”，以找到产生最佳模型的超参数。我们可能希望在广泛的超参数范围内运行，而这样做可能非常昂贵。数据集使我们能够在单个
    Python 脚本中轻松地跨多个并行训练运行中共享相同的内存中数据：我们可以加载和预处理数据一次，然后将其引用传递给许多下游参与者，这些参与者可以从共享内存中读取数据。
- en: 'Additionally, sometimes when working with very large data sets it can be infeasible
    to load the full training data into memory in a single process or on a single
    machine. It’s common in distributed training to instead shard the data across
    many different workers to train in parallel and combining the results either synchronously
    or asynchronously using a parameter server. There are some important considerations
    that can make this tricky: 1\. Many distributed training algorithms take a *synchronous*
    approach, requiring the workers to synchronize their weights after each training
    epoch. This means there needs to be some coordination between the workers to maintain
    consistency beween which batch of data they are operating on. 2\. It’s important
    that each worker gets a random sample of the data during each epoch. A global
    random shuffle has been shown to perform better than local shuffle or no shuffle.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，有时在处理非常大的数据集时，将完整的训练数据加载到单个进程或单台机器的内存中是不可行的。在分布式训练中，通常将数据分片到许多不同的工作节点上进行并行训练，并使用参数服务器同步或异步地合并结果。有一些重要的考虑因素可能会使这变得棘手：1\.
    许多分布式训练算法采用同步方法，要求工作节点在每个训练 epoch 后同步其权重。这意味着需要一些协调来维护工作节点在操作的数据批次之间的一致性。2\. 在每个
    epoch 中，每个工作节点获取数据的随机样本是重要的。全局随机洗牌已被证明比本地洗牌或不洗牌效果更好。
- en: Let’s walk through an example of how we can implement this type of pattern using
    Ray Datasets. In the example, we will training multiple copies of an SDG classifier
    using different hyperparameters across different workers in parallel. This isn’t
    exactly the same, but it is a similar pattern that focuses on the flexibility
    and power of Ray Datasets for ML training workloads.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个示例来演示如何使用Ray Datasets实现这种类型的模式。在示例中，我们将使用不同的超参数并行地训练多个SDG分类器的副本。虽然这不完全相同，但它是一个类似的模式，专注于Ray
    Datasets在机器学习训练工作负载中的灵活性和强大性。
- en: We’ll be training a scikit-learn `SGDClassifier` on a generated binary classification
    dataset and the hyperparameter we’ll tune is the regularization term (alpha value).
    The actual details of the ML task and model aren’t too important to this example,
    you could replace the model and data with any number of examples. The main thing
    to focus on here is how we orchestrate the data loading and computation using
    Datasets.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在生成的二分类数据集上训练一个scikit-learn `SGDClassifier`，我们将调整的超参数是正则化项（alpha值）。实际的机器学习任务和模型的详细信息对于本示例并不重要，你可以用任何示例替换模型和数据。这里的重点是我们如何使用Datasets来编排数据加载和计算过程。
- en: 'First, let’s define our `TrainingWorker` that will train a copy of the classifier
    on the data:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们定义我们的`TrainingWorker`，它将在数据上训练分类器的一个副本：
- en: Example 7-12\.
  id: totrans-95
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-12。
- en: '[PRE12]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'There are a few important things to note about the `TrainingWorker`: - It’s
    a simple wrapper around the `SGDClassifier` and instantiates it with a given alpha
    value. - The main training function happens in the `train` method. For each epoch,
    it trains the classifier on the data available. - We also have a `test` method
    that can be used to run the trained model against a testing set.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 关于`TrainingWorker`有几点重要的事项需要注意：- 它只是一个简单的`SGDClassifier`包装器，并使用给定的alpha值实例化它。-
    主要的训练功能在`train`方法中进行。每个epoch，它都会在可用的数据上对分类器进行训练。- 我们还有一个`test`方法，可以用来针对测试集运行训练好的模型。
- en: 'Now, let’s instantiate a number of `TrainingWorker`s with different hyperparameters
    (alpha values):'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们用不同的超参数（alpha值）实例化几个`TrainingWorker`：
- en: Example 7-13\.
  id: totrans-99
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-13。
- en: '[PRE13]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Next, we generate training and validation data and convert the training data
    to a Dataset. Here, we’re using `.repeat()` to create a DatasetPipeline. This
    defines the number of epochs that our training will run for. In each epoch, the
    subsequent operations will be applied to the Dataset and the workers will be able
    to iterate over the resulting data. We also shuffle the data randomly and shard
    it to be passed to the training workers, each getting an equal chunk.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们生成训练和验证数据，并将训练数据转换为Dataset。在这里，我们使用`.repeat()`来创建一个DatasetPipeline。这定义了我们训练将运行的epoch数量。在每个epoch中，后续操作将应用于Dataset，并且工作者将能够迭代处理结果数据。我们还随机打乱数据并将其分片以传递给训练工作者，每个工作者获得一个相等的块。
- en: Example 7-14\.
  id: totrans-102
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-14。
- en: '[PRE14]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To run the training on the workers, we invoke their `train` method and pass
    in one shard of the DatasetPipeline to each. We then block waiting for training
    to complete across all of the workers. To summarize what happens during this phase:
    - Each epoch, each worker gets a random shard of the data. - The worker trains
    its local model on the shard of data assigned to it. - Once a worker has finished
    training on the current shard, it blocks until the other workers have finished.
    - The above repeats for the remaining epochs (in this case, 10 total).'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 要在工作者上运行训练，我们调用它们的`train`方法，并将DatasetPipeline的一个shard传递给每个工作者。然后，我们阻塞等待所有工作者的训练完成。总结这个阶段发生的事情如下：-
    每个epoch，每个工作者都会获得随机的数据shard。- 工作者在分配给它的数据片段上训练其本地模型。- 一旦工作者完成了当前shard的训练，它会阻塞直到其他工作者也完成。-
    上述过程在剩余的epochs中重复（在本例中总共为10个epochs）。
- en: Example 7-15\.
  id: totrans-105
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-15。
- en: '[PRE15]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Finally, we can test out the trained models from each worker on some test data
    to determine which alpha value produced the most accurate model.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以在一些测试数据上测试每个工作者训练出的模型，以确定哪个alpha值产生了最准确的模型。
- en: Example 7-16\.
  id: totrans-108
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-16。
- en: '[PRE16]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: While this not be a real-world ML task and in reality you should likely reach
    for Ray Tune or Ray Train, this example conveys the power of Ray Datasets, especially
    for machine learning workloads. In just a few 10s of lines of Python code, we
    were able to implement a complex distributed hyperparameter tuning and training
    workflow that could easily be scaled up to 10s or 100s of machines and is agnostic
    to any framework or specific ML task.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这可能不是一个真实的机器学习任务，实际上你可能应该选择 Ray Tune 或 Ray Train，但这个例子传达了 Ray 数据集的强大之处，特别是对于机器学习工作负载。仅用几十行
    Python 代码，我们就能实现一个复杂的分布式超参数调整和训练工作流程，可以轻松扩展到数十台或数百台机器，并且不受任何框架或特定机器学习任务的限制。
- en: External Library Integrations
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 外部库集成
- en: Overview
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: While Ray Datasets supports a number of common data processing functionalities
    out of the box, as mentioned above it’s not a replacement for full data processing
    systems. Instead, it’s more focused on performing “last mile” processing such
    as basic data loading, cleaning, and featurization before ML training or inference.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Ray 数据集支持一些常见的数据处理功能，但正如上文所述，它并不是完整数据处理系统的替代品。相反，它更侧重于执行“最后一英里”处理，如基本数据加载、清理和特征化，然后进行机器学习训练或推断。
- en: '![Ray for last-mile data processing](assets/data_positioning_1.svg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![用于最后一英里数据处理的 Ray](assets/data_positioning_1.svg)'
- en: 'Figure 7-3\. A typical workflow using Ray for machine learning: use external
    systems for primary data processing and ETL, use Ray Datasets for last-mile preprocessing'
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-3\. 使用 Ray 进行机器学习的典型工作流程：使用外部系统进行主要数据处理和 ETL，使用 Ray 数据集进行最后一英里预处理
- en: 'However, there are a number of other more fully-featured DataFrame and relational
    data processing systems that integrate with Ray: - Dask on Ray - RayDP (Spark
    on Ray) - Modin (Pandas on Ray) - Mars on Ray'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，还有许多其他功能更完备的 DataFrame 和关系型数据处理系统与 Ray 集成：- Dask on Ray - RayDP（Spark on
    Ray） - Modin（Pandas on Ray） - Mars on Ray
- en: These are all standalone data processing libraries that you may be familiar
    with outside the context of Ray. Each of these tools has an integration with Ray
    core that enables more expressive data processing than comes with the built-in
    Datasets while still using Ray’s deployment tooling, scalable scheduling, and
    shared memory object store for exchanging data.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都是独立的数据处理库，你可能在 Ray 的环境之外也很熟悉它们。每个工具都与 Ray 核心集成，可以实现比内置数据集更具表现力的数据处理，同时利用
    Ray 的部署工具、可扩展的调度和共享内存对象存储交换数据。
- en: '![Ray with ecosystem integrations](assets/data_positioning_1.svg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![Ray 与生态系统集成](assets/data_positioning_1.svg)'
- en: Figure 7-4\. The benefit of Ray data ecosystem integrations, enabling more expressive
    data processing on Ray. These libraries integrate with Ray Datasets to feed into
    downstream libraries such as Ray Train.
  id: totrans-119
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-4\. Ray 数据生态系统集成的好处，实现在 Ray 上更具表现力的数据处理。这些库与 Ray 数据集集成，以提供给下游库，如 Ray Train。
- en: For the purposes of this book, we’ll explore Dask on Ray in slightly more depth
    to give you a feel for what these integrations look like. If you’re interested
    in the details of a specific integration, please see the latest [Ray documentation](https://docs.ray.io/en/latest/data/dask-on-ray.xhtml)
    for up-to-date information.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为了本书的目的，我们将稍微深入探讨一下 Dask on Ray，让你对这些集成有所了解。如果你对特定集成的详细信息感兴趣，请参阅最新的 [Ray 文档](https://docs.ray.io/en/latest/data/dask-on-ray.xhtml)
    获取最新信息。
- en: Dask on Ray
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dask on Ray
- en: 'To follow along with the examples in this section, please install Ray and Dask:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟着本节的示例进行操作，请安装 Ray 和 Dask：
- en: '[PRE17]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[Dask]([*https://dask.org/*](https://dask.org/)) is a Python library for parallel
    computing that is specifically target at scaling analytics and scientific computing
    workloads to a cluster. One of the most popular features of Dask is [Dask DataFrames](https://docs.dask.org/en/stable/dataframe.xhtml),
    which offers a subset of the pandas DataFrame API that can be scaled to a cluster
    of machines in cases where processing in memory on a single node is not feasible.
    DataFrames work by creating a *task graph* that is submitted to a scheduler for
    execution. The most typical way to execute Dask DataFrames operations is using
    the Dask Distributed scheduler, but there is also a pluggable API that allows
    other schedulers to execute these task graphs as well.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[Dask]([*https://dask.org/*](https://dask.org/)) 是一个专门针对将分析和科学计算工作负载扩展到集群的并行计算
    Python 库。Dask 最受欢迎的功能之一是 [Dask DataFrames](https://docs.dask.org/en/stable/dataframe.xhtml)，它提供了
    pandas DataFrame API 的一个子集，在处理单节点内存不可行的情况下可以扩展到一组机器上。DataFrames 通过创建一个*任务图*来工作，该图提交给调度器执行。执行
    Dask DataFrames 操作的最典型方式是使用 Dask 分布式调度器，但也有一个可插拔的 API，允许其他调度器执行这些任务图。'
- en: 'Ray comes packaged with a Dask scheduler backend, allowing Dask DataFrame task
    graphs to be executed as Ray tasks and therefore make use of the Ray scheduler
    and shared memory object store. This doesn’t require modifying the core DataFrames
    code at all; instead, in order to run using Ray all you need to do is first connect
    to a running Ray cluster (or run Ray locally) and then enable the Ray scheduler
    backend:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Ray 自带一个 Dask 调度器后端，允许 Dask DataFrame 任务图作为 Ray 任务执行，并因此利用 Ray 调度器和共享内存对象存储。这完全不需要修改核心
    DataFrames 代码；相反，为了使用 Ray 运行，您只需首先连接到一个运行中的 Ray 集群（或在本地运行 Ray），然后启用 Ray 调度器后端：
- en: Example 7-17\.
  id: totrans-126
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-17\.
- en: '[PRE18]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Now we can run regular Dask DataFrames code and have it scaled across the Ray
    cluster. For example, we might want to do some time series analysis using standard
    DataFrame operations like filter, groupby, and computing the standard deviation
    (example taken from Dask documentation).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以运行常规的 Dask DataFrames 代码，并让它跨 Ray 集群进行扩展。例如，我们可能想使用标准的 DataFrame 操作进行一些时间序列分析，如过滤、分组和计算标准偏差（示例摘自
    Dask 文档）。
- en: Example 7-18\.
  id: totrans-129
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-18\.
- en: '[PRE19]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: If you’re used to pandas or other DataFrame libraries, you might wonder why
    we need to call `df.compute()`. This is because Dask is *lazy* by default, and
    will only compute results on demand, allowing it to optimize the task graph that
    will be executed across the cluster.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您习惯于 pandas 或其他 DataFrame 库，您可能会想知道为什么我们需要调用 `df.compute()`。这是因为 Dask 默认是*延迟执行*的，只会在需要时计算结果，从而优化将在整个集群上执行的任务图。
- en: 'One of the most powerful aspects of the is that it integrates very nicely with
    Ray Datasets. We can convert a Ray Dataset to a Dask DataFrame and vice versa
    using built-in utilities:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这个功能中最强大的一个方面是它与 Ray 数据集非常好地集成在一起。我们可以使用内置工具将 Ray 数据集转换为 Dask DataFrame，反之亦然：
- en: Example 7-19\.
  id: totrans-133
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-19\.
- en: '[PRE20]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This simple example might not look too impressive because we’re able to compute
    the standard deviation using either Dask DataFrames or Ray Datasets. However,
    as you’ll see in the next section when we build an end-to-end ML pipeline, this
    enables really powerful workflows. For example, we can use the full expressiveness
    of DataFrames to do our featurization and preprocessing, then pass the data directly
    into downstream operations such as distributed training or inference while keeping
    everything in memory. This highlights how Datasets enables a wide range of use
    cases on top of Ray, and how integrations like Dask on Ray make the ecosystem
    even more powerful.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的例子可能看起来不太令人印象深刻，因为我们能够使用 Dask DataFrames 或 Ray 数据集计算标准偏差。然而，正如您将在下一节看到的那样，当我们构建一个端到端的
    ML pipeline 时，这使得工作流程非常强大。例如，我们可以利用 DataFrame 的全部表现力来进行特征化和预处理，然后直接将数据传递到诸如分布式训练或推断等下游操作，同时保持所有数据在内存中。这突显了
    DataFrames 如何在 Ray 上实现广泛的用例，并且像 Dask on Ray 这样的集成使生态系统变得更加强大。
- en: Building an ML Pipeline
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建一个 ML Pipeline
- en: Although we were able to we build a simple distributed training application
    from scratch in the previous section, there were many edge cases, opportunities
    for performance optimization, and usability features that we would want to address
    to build a real-world application. As you’ve learned in the previous chapters
    about Ray RLlib, Ray Tune, and Ray Train, Ray has an ecosystem of libraries that
    enable us to build production-ready ML applications. In this section, we’ll explore
    how to use Datasets as the “glue layer” to build an ML pipeline end-to-end.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们能够在前一节从头开始构建一个简单的分布式训练应用程序，但还有许多边缘情况、性能优化机会和我们想要解决的可用性功能，以构建一个真实的应用程序。正如您在前几章关于Ray
    RLlib、Ray Tune和Ray Train中学到的，Ray拥有一系列库，使我们能够构建可投入生产的ML应用程序。在本节中，我们将探讨如何使用数据集作为“粘合层”来端到端构建ML管道。
- en: Background
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 背景
- en: 'To successfully productionize a machine learning model, one first needs to
    collect and catalog data using standard ETL processes. However, that’s not the
    end of the story: in order to train a model, we also often need to do featurization
    of the data before feeding into our training process, and how we feed the data
    into training can make a big impact on cost and performance. After training a
    model, we’ll also want to run inference across many different datasets — that’s
    the whole point of training the model after all! This end-to-end process is summarized
    in the figure below.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 要成功将机器学习模型投入生产，首先需要使用标准的ETL流程收集和编目数据。然而，这还不是故事的结束：为了训练模型，我们还经常需要对数据进行特征化，然后再将其馈送到我们的训练过程中，而我们如何将数据馈送到训练中会对成本和性能产生很大影响。训练完模型后，我们还想要在许多不同的数据集上运行推断，毕竟这正是训练模型的全部意义！这个端到端的过程在下图中概述。
- en: 'Though this might look like just a chain of steps, in practice the data processing
    workflow for machine learning is an iterative process of experimentation to define
    the right set of features and train a high-performing model on them. Efficiently
    loading, transforming, and feeding the data into training and inference is also
    crucial for performance, which translates directly to cost for compute-intensive
    models. Often times, implementing these ML pipelines means stitching together
    multiple different systems and materializing intermediate results to remote storage
    between the stages. This has two major downsides: 1\. First, it requires orchestrating
    many different systems and programs for a single workflow. This can be a lot to
    handle for any ML practictioner, so many people reach to workflow orchestration
    systems like [Apache Airflow](https://airflow.apache.org/). While Airflow has
    some great benefits, it’s also a lot of complexity to introduce (especially in
    development). 2\. Second, running our ML workflow across multiple different systems
    means we need to read from and write to storage between each stage. This incurs
    significant overhead and cost due to data transfer and serialization.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这可能看起来只是一系列步骤，但在实践中，机器学习的数据处理工作流程是一个迭代的实验过程，以定义正确的特征集并在其上训练高性能模型。高效加载、转换和将数据输入训练和推断也对性能至关重要，这直接转化为计算密集型模型的成本。通常情况下，实现这些ML流水线意味着将多个不同的系统连接在一起，并在阶段之间将中间结果实现到远程存储。这有两个主要缺点：1\.
    首先，它需要为单个工作流程编排许多不同的系统和程序。这对于任何ML从业者来说可能是很难处理的，因此许多人会借助工作流程编排系统，如[Apache Airflow](https://airflow.apache.org/)。虽然Airflow有一些很好的好处，但引入它也意味着引入了许多复杂性（特别是在开发中）。2\.
    其次，我们在多个不同的系统上运行我们的ML工作流程意味着我们需要在每个阶段之间读取和写入存储。这会因数据传输和序列化而产生重大的开销和成本。
- en: In contrast, using Ray we are able to build a complete machine learning pipeline
    as a single application that can be run as a single Python script. The ecosystem
    of built-in and third party libraries make it possible to mix-and-match the right
    functionality for a gien use case and build scalable, production-ready pipelines.
    Ray Datasets acts as the glue layer, enabling us to efficiently load, preprocess,
    and compute over the data while avoiding expensive serialization costs and keeping
    intermediate data in shared memory.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，使用Ray，我们能够将完整的机器学习流水线构建为一个单独的应用程序，可以作为单个Python脚本运行。内置和第三方库的生态系统使得可以混合匹配适合特定用例的正确功能，并构建可扩展的、可投入生产的管道。Ray数据集充当粘合层，使我们能够在避免昂贵的序列化成本和保持中间数据在共享内存中的情况下，高效地加载、预处理和计算数据。
- en: 'End-to-End Example: Predicting Big Tips in Nyc Taxi Rides'
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 端到端示例：在纽约出租车乘车中预测大费用
- en: 'This section walks through a practical, end-to-end example of building a deep
    learning pipeline using Ray. We will build a binary classification model to predict
    if a taxi ride will result in a big tip (>20% of the fare) using the public [New
    York City Taxi and Limousine Commission (TLC) Trip Record Data](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page).
    Our workflow will closely resemble that of a typical ML practitioner: - First,
    we will load the data, do some basic preprocessing, and compute features we’ll
    use in our model. - Then, we will define a neural network and train it using distributed
    data-parallel training. - Finally, we will apply the trained neural network to
    a fresh batch of data.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将通过一个实际的端到端示例演示如何使用 Ray 构建深度学习流水线。我们将建立一个二元分类模型，以预测出租车行程是否会有较大的小费（超过车费的20%），使用公共的[纽约市出租车和豪华车委员会（TLC）出行记录数据](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page)。我们的工作流程将与典型的机器学习从业者非常相似：-
    首先，我们将加载数据，进行基本预处理，并计算我们模型中将使用的特征。- 然后，我们将定义一个神经网络，并使用分布式数据并行训练它。- 最后，我们将把训练好的神经网络应用于新的数据批次。
- en: 'The example will use Dask on Ray and train a PyTorch neural network, but note
    that nothing here is specific to either of those libraries, Ray Datasets and Ray
    Train can be used with a wide range of popular machine learning tools. To follow
    along with the example code in this section, please install Ray, PyTorch, and
    Dask:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 本例将使用 Ray 上的 Dask 并训练一个 PyTorch 神经网络，但请注意，这里没有具体限定于这两个库，Ray Dataset 和 Ray Train
    可以与许多流行的机器学习工具一起使用。要在本节中按照示例代码操作，请安装 Ray、PyTorch 和 Dask：
- en: '[PRE21]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In the examples below, we’ll be loading the data from local disk to make it
    easy to run the examples on your machine. You can download the data to your local
    machine from the [AWS Registry of Open Data](https://registry.opendata.aws/nyc-tlc-trip-records-pds/)
    using the AWS CLI:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，我们将从本地磁盘加载数据，以便在您的机器上轻松运行示例。您可以使用 AWS CLI 从[AWS 开放数据注册表](https://registry.opendata.aws/nyc-tlc-trip-records-pds/)下载数据到您的本地机器上：
- en: '[PRE22]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: If you’d like to try loading the data directly from cloud storage, simply replace
    the local paths in the examples with the corresponding S3 URL.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想直接从云存储加载数据，请将示例中的本地路径替换为相应的 S3 URL。
- en: Loading, preprocessing, and featurizing with dask on Ray
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Ray 上的 Dask 进行加载、预处理和特征化
- en: The first step in training our model is to load and preprocess it. To do this,
    we’ll be using Dask on Ray, which as discussed above gives us a convenient DataFrames
    API and the ability to scale up the preprocessing across a cluster and efficiently
    pass it into our training and inference operations.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 训练我们的模型的第一步是加载和预处理数据。为此，我们将使用 Ray 上的 Dask，正如上文所讨论的，它为我们提供了一个方便的 DataFrame API，并且可以跨集群扩展预处理并有效地传递到我们的训练和推断操作中。
- en: 'Below is our code for preprocessing and featurization:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们的预处理和特征化代码：
- en: Example 7-20\.
  id: totrans-152
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-20。
- en: '[PRE23]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This involves basic data loading and cleaning (dropping nulls and outliers)
    as well as transforming some columns into a format that can be used as features
    in our machine learning model. For instance, we transform the pickup and dropoff
    datetimes, which are provided as a string, into three numerical features: `trip_duration`,
    `hour`, and `day_of_week`. This is made easy by Dask’s built-in support for [Python
    datetime utilites](https://docs.python.org/3/library/datetime.xhtml). If this
    data is going to be used for training, we also need to compute the label column
    (whether the tip was more or less than 20% of the fare amount).'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这涉及到基本的数据加载和清洗（删除空值和异常值），以及将一些列转换为可以用作机器学习模型特征的格式。例如，我们将提供为字符串的上车和下车日期时间转换为三个数值特征：`trip_duration`、`hour`和`day_of_week`。这在
    Dask 内置支持的[Python datetime utilites](https://docs.python.org/3/library/datetime.xhtml)的帮助下变得非常简单。如果这些数据将用于训练，我们还需要计算标签列（小费是否超过车费的20%）。
- en: Finally, once we’ve computed our preprocessed Dask DataFrame, we transform it
    into a Ray Dataset so we can pass it into our training and inference processes
    later.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一旦我们计算出预处理后的 Dask DataFrame，我们将其转换为 Ray Dataset，以便稍后传递到我们的训练和推断过程中。
- en: Defining a PyTorch model
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义一个 PyTorch 模型
- en: Now that we’ve cleaned and prepared the data, we need to define a model architecture
    that we’ll use for the model. In practice, this would likely be an iterative process
    and involve researching the state of the art for similar problems. For the sake
    of our example, we’ll keep things simple and use use a basic PyTorch neural network.
    The neural network has three linear transformations starting with the dimension
    of our feature vector and then outputs a value between 0 and 1 using a Sigmoid
    activation function. This output value will be rounded to produce the binary prediction
    of if the ride will result in a big tip or not.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经清理和准备好数据，我们需要定义一个模型架构，我们将用于模型。实际上，这可能是一个迭代过程，并涉及研究类似问题的最新技术。为了我们的示例，我们将保持简单，并使用基本的PyTorch神经网络。神经网络有三个线性变换，从我们的特征向量维度开始，然后使用Sigmoid激活函数输出一个值在0到1之间。此输出值将四舍五入以产生是否会有较大小费的二进制预测。
- en: Example 7-21\.
  id: totrans-158
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-21。
- en: '[PRE24]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Distributed training with Ray Train
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Ray Train进行分布式训练
- en: Now that we’ve defined the neural network architecture, we need a way to efficiently
    train this on our data. This data set is very large (hundreds of gigabytes in
    total), so our best bet is probably to perform distributed data-parallel training.
    We will use Ray Train, which you learned about in [Chapter 6](ch06.xhtml#chapter_06),
    to define a scalable training process that will use [PyTorch DataParallel](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.xhtml)
    under the hood.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了神经网络架构，我们需要一种有效地在我们的数据上训练它的方式。这个数据集非常庞大（总共数百GB），因此我们最好的选择可能是执行分布式数据并行训练。我们将使用Ray
    Train，在[第6章](ch06.xhtml#chapter_06)中学习到的内容中定义可扩展的训练过程，这将在底层使用[PyTorch DataParallel](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.xhtml)。
- en: The first thing we need to do is define the logic that will happen to train
    on a batch of data on each worker in each epoch. This will take in a local shard
    of the full dataset, run it through the local copy of the model, and perform backpropagation.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要定义逻辑，以便在每个工作节点的每个时期训练数据批次。这将接收完整数据集的本地片段，通过本地模型副本运行，并执行反向传播。
- en: Example 7-22\.
  id: totrans-163
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-22。
- en: '[PRE25]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Next, we also need to define the logic for each worker to validate its current
    copy of the model in each epoch. This will run a local batch of data through the
    model, compare the predictions to the actual label values, and compute the subsequent
    loss using a provided loss function.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们还需要为每个工作节点定义验证其当前模型副本的逻辑。这将通过模型运行本地数据批次，将预测与实际标签值进行比较，并使用提供的损失函数计算随后的损失。
- en: Example 7-23\.
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-23。
- en: '[PRE26]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Finally, we define the core training logic. This will take in a variety of configuration
    options (such as a batch size and other model hyperparameters), instantiate the
    model, loss function, and optimizer, and then run the core training loop. In each
    epoch, each worker will get its shard of the training and validation datasets,
    convert it to a local [PyTorch Dataset](https://pytorch.org/docs/stable/data.xhtml),
    and run the validation and training code we defined above. After each epoch, the
    worker will using Ray Train utilities to report the result and save the current
    model weights for use later.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们定义核心训练逻辑。这将接收各种配置选项（例如批量大小和其他模型超参数），实例化模型、损失函数和优化器，然后运行核心训练循环。在每个时期，每个工作节点将获取其训练和验证数据集的片段，将其转换为本地的[PyTorch数据集](https://pytorch.org/docs/stable/data.xhtml)，并运行上述定义的验证和训练代码。每个时期结束后，工作节点将使用Ray
    Train工具报告结果并保存当前模型权重以供以后使用。
- en: Example 7-24\.
  id: totrans-169
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-24。
- en: '[PRE27]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Now that the full training process has been defined, we need to load the training
    and validation data to feed into our training workers. Here, we call the `load_dataset`
    function we defined earlier that will do preprocessing and featurization, then
    split the dataset into a training and validation Dataset^([1](ch07.xhtml#idm44990019385840)).
    Finally, we want to convert both Datasets into [Dataset Pipelines](https://docs.ray.io/en/latest/data/dataset-pipeline.xhtml)
    for efficiency and make sure that the training dataset is globally shuffled between
    all of the workers in each epoch.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在已经定义了完整的训练过程，我们需要加载训练和验证数据以供训练工作节点使用。在这里，我们调用之前定义的`load_dataset`函数，该函数将进行预处理和特征化，然后将数据集分割为训练和验证数据集^[1](ch07.xhtml#idm44990019385840)。最后，我们希望将这两个数据集都转换为[数据集流水线](https://docs.ray.io/en/latest/data/dataset-pipeline.xhtml)，以提高效率，并确保在每个时期将训练数据集在所有工作节点之间全局洗牌。
- en: Example 7-25\.
  id: totrans-172
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-25。
- en: '[PRE28]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Everything is set, and it’s time to run our distributed training process! All
    that’s left is to create a Trainer, pass in our Datasets, and let the training
    scale up and run across the configured number of workers. After training has completed,
    we fetch the latest model weights using the checkpoint API.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 一切准备就绪，现在是运行我们的分布式训练过程的时候了！剩下的就是创建一个Trainer，传入我们的Datasets，并让训练在配置的工作节点上扩展和运行。训练完成后，我们使用检查点API获取最新的模型权重。
- en: Example 7-26\.
  id: totrans-175
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-26。
- en: '[PRE29]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Distributed batch inference with Ray datasets
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Ray数据集进行分布式批量推理
- en: Once we’ve trained a model and gotten the best accuracy that we can, the next
    step is to actually apply it in practice. Sometimes this means powering a low-latency
    service, which we’ll explore in [Chapter 8](ch08.xhtml#chapter_08), but often
    the task is to apply the model across batches of data as they come in.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们训练了一个模型并获得了最佳准确性，下一步就是实际应用它。有时这意味着提供低延迟服务，我们将在[第8章](ch08.xhtml#chapter_08)中探讨这一点，但通常任务是在数据批次到达时跨批次应用模型。
- en: 'Let’s use the trained model weights from the training process above and apply
    them across a new batch of data (in this case, it’ll just be another chunk of
    the same public data set). To do this, first we need to load, preprocess, and
    featurize the data in the same way we did for training. Then we will load our
    model and `map` it across the whole data set. As discussed in the section above,
    Datasets allows us to do this efficiently with Ray Actors, even using GPUs just
    by changing one parameter. We simply wrap our trained model weights in a class
    that will load them and configure a model for inference, then call `map_batches`
    and pass in the inference model class:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用上述训练过程中的训练模型权重，并将其应用于新的数据批次（在本例中，只是同一公共数据集的另一块）。为此，首先需要以与训练相同的方式加载、预处理和特征化数据。然后我们将加载我们的模型，并在整个数据集上进行`map`操作。如上节所述，Datasets允许我们通过Ray
    Actors高效地执行此操作，甚至只需通过更改一个参数即可使用GPU。我们只需将训练好的模型权重封装在一个类中，该类将加载并配置一个推理模型，然后调用`map_batches`并传入推理模型类：
- en: Example 7-27\.
  id: totrans-180
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-27。
- en: '[PRE30]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: ^([1](ch07.xhtml#idm44990019385840-marker)) The code only loads a subset of
    the data for testing, to test at scale use all partitions of the data when calling
    `load_dataset` and increase `num_workers` when training the model.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch07.xhtml#idm44990019385840-marker)) 代码仅加载数据的一个子集进行测试，若要进行规模化测试，请在调用`load_dataset`时使用所有数据分区，并在训练模型时增加`num_workers`。
