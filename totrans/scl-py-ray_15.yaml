- en: Appendix B. Installing and Deploying Ray
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The power of Ray is in its support for various deployment models, ranging from
    a single-node deployment—​allowing you to experiment with Ray locally—​to clusters
    containing thousands of machines. The same code developed on the local Ray installation
    can run on the entire spectrum of Ray’s installations. In this appendix, we will
    show some of the installation options that we evaluated while writing this book.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Ray Locally
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The simplest Ray installation is done locally with `pip`. Use the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This command installs all the code required to run local Ray programs or launch
    programs on a Ray cluster (see [“Using Ray Clusters”](#sec-using-ray-clusters)).
    The command installs the latest official release. In addition, it is possible
    to install Ray from [daily releases](https://oreil.ly/2VzQD) or a [specific commit](https://oreil.ly/f9k7H).
    It is also possible to install Ray inside the [Conda environment](https://oreil.ly/1TsIZ).
    Finally, you can build Ray from the source by following the instructions in the
    [Ray documentation](https://oreil.ly/rjane).
  prefs: []
  type: TYPE_NORMAL
- en: Using Ray Docker Images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In addition to natively installing on your local machine, Ray provides an option
    for running the provided [Docker image](https://oreil.ly/zrvoq). The Ray project
    provides a wealth of [Docker images](https://oreil.ly/0qv77) built for various
    versions of Python and hardware options. These images can be used to execute Ray’s
    code by starting a corresponding Ray image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here `<*shm-size*>` is the memory that Ray uses internally for its object store.
    A good estimate for this value is to use roughly 30% of your available memory;
    `<*image name*>` is the name of the image used.
  prefs: []
  type: TYPE_NORMAL
- en: Once this command is executed, you will get back a command-line prompt and can
    enter any Ray code.
  prefs: []
  type: TYPE_NORMAL
- en: Using Ray Clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although a local Ray installation is extremely useful for experimenting and
    initial debugging, the real power of Ray is its ability to run and scale on clusters
    of machines.
  prefs: []
  type: TYPE_NORMAL
- en: Ray *cluster nodes* are logical nodes based on Docker images. Docker images
    provided by the Ray project contain all the code required for running logical
    nodes, but not necessarily all the code required to run user applications. The
    issue here is that the user’s code might need specific Python libraries, which
    are not part of Ray’s Docker images.
  prefs: []
  type: TYPE_NORMAL
- en: To overcome this problem, Ray allows the installation of specific libraries
    to the nodes as part of the cluster installation, which is great for initial testing
    but can significantly impact the node’s creation performance. As a result, in
    production installs, it is typically recommended to use custom images derived
    from Ray-provided ones and add required libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ray provides two main options for installation: installation directly on the
    hardware nodes or cloud provider’s VMs and installation on Kubernetes. Here we
    will discuss Ray’s installation on cloud providers and Kubernetes. For information
    on Ray’s installation on hardware nodes, refer to the [Ray documentation](https://oreil.ly/3hYV0).'
  prefs: []
  type: TYPE_NORMAL
- en: The official [documentation](https://oreil.ly/mrThY) describes Ray’s installation
    on several cloud providers, including AWS, Azure, Google Cloud, Alibaba, and custom
    clouds. Here we will discuss installation on AWS (as it is the most popular) and
    IBM Cloud (as one of the coauthors works at IBM, which takes a unique approach).^([1](app02.html#idm45354756650320))
  prefs: []
  type: TYPE_NORMAL
- en: Installing Ray on AWS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AWS cloud installation leverages the Boto3 AWS SDK for Python and requires configuring
    your AWS credentials in the *~/.aws/credentials* file.^([2](app02.html#idm45354756645408))
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the credentials are created and Boto3 is installed, you can use the [*ray-aws.yaml*
    file](https://oreil.ly/zkodJ), which was adapted from the [Ray GitHub repository](https://oreil.ly/h0UnW),
    to install Ray on AWS via the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This command creates the cluster. It also provides a set of useful commands
    that you can use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note that the IP addresses that you’ll see will be different from those shown
    here. When the cluster is created, it uses a firewall that allows only a Secure
    Shell (SSH) connection to the cluster. If you want to access the cluster’s dashboard,
    you need to open port 8265, and for gRPC access, use port 10001\. To do this,
    find your node in the Amazon Elastic Compute Cloud (EC2) dashboard, click the
    Security tab, choose the security group, and modify the inbound rules. [Figure B-1](#fig-appb-1)
    shows a new rule allowing any instance port access from anywhere. For more information
    on inbound rule configuration, refer to the [AWS documentation](https://oreil.ly/MRfib).
  prefs: []
  type: TYPE_NORMAL
- en: '![spwr ab01](assets/spwr_ab01.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure B-1\. Instances view in the AWS console
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As requested by your YAML file, you can see only a head, and the worker nodes
    will be created to satisfy the execution requirements of submitted jobs. To verify
    that the cluster is running correctly, you can use the code in [*localPython.py*
    on GitHub](https://oreil.ly/OzOQN), which verifies that it can connect to the
    cluster and its nodes.
  prefs: []
  type: TYPE_NORMAL
- en: An alternative approach to using Docker images for installation is [installing
    Ray directly on a VM](https://oreil.ly/k733p). The advantage of this approach
    is the ability to easily add additional software to the VM, which can be useful
    in real life. An obvious use case is managing Python libraries. You can do this
    with Docker-based installation, but you will then need to build Docker images
    for each library configuration. In the VM-based approach, there is no need to
    create and manage Docker images; just do appropriate `pip` installs. Additionally,
    you can install applications on VMs to leverage them in the Ray execution (see
    [“Wrapping Custom Programs with Ray”](ch12.html#sec-wrapping-custom-programs)).
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Installing Ray on a VM requires a lot of setup commands, and as a result, it
    can take a significant amount of time for the Ray node to start. A recommended
    approach is to start the Ray cluster once, create a new image, and then use this
    image and remove additional setup commands.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Ray on IBM Cloud
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'IBM Cloud installation is based on the [Gen2 connector](https://oreil.ly/tIF6Y)
    that enables the Ray cluster to be deployed on IBM’s Gen2 cloud infrastructure.
    As with Ray on AWS, you’ll start with creating the cluster specification in a
    YAML file. You can use Lithopscloud to do this interactively if you don’t want
    to manually create the YAML file. You install Lithopscloud with `pip` as normal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: To use Lithopscloud, you first need to either create an [API key](https://oreil.ly/ZO9Nv)
    or reuse the existing one. With your API key, you can run `lithopscloud -o cluster.yaml`
    to generate a *cluster.yaml* file. Once you start Lithopscloud, follow the questions
    to generate a file (you’ll need to use the up and down arrows to make your selections).
    You can find an example of the generated file on [GitHub](https://oreil.ly/rQNOx).
  prefs: []
  type: TYPE_NORMAL
- en: 'The limitation of the autogenerated file is that it uses the same image type
    for both head and worker nodes, which is not always ideal. You often may want
    to provide different types for these nodes. To do this, you can modify the autogenerated
    [*cluster.yaml* file](https://oreil.ly/LqpIl) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here you define two types of nodes: the default head node and default worker
    node (you can define multiple worker node types with a max number of workers per
    time). Therefore, you can now have a relatively small head node (running all the
    time) and much larger worker nodes that will be created just in time.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you take a look at the generated YAML file, you will notice that it has a
    lot of setup commands, and as a result, it can take a significant amount of time
    for the Ray node to start. A recommended approach is to start the Ray cluster
    once, create a new image, and then use this image and remove the setup commands.
  prefs: []
  type: TYPE_NORMAL
- en: Once the YAML file is generated, you can install Gen2-connector to be able to
    use it. Run `pip3 install gen2-connector`. You can then create your cluster by
    running `ray up cluster.yaml`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to installing Ray on AWS, this installation displays a list of useful
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: To be able to access the cluster, be sure to open the required ports following
    [IBM Cloud documentation](https://oreil.ly/8oTDR) ([Figure B-2](#fig-appB-2)).
  prefs: []
  type: TYPE_NORMAL
- en: '![spwr ab02](assets/spwr_ab02.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure B-2\. IBM Cloud console displaying firewall rules
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As requested by your YAML file, you can see only a head; the worker nodes will
    be created to satisfy the execution requirements of submitted jobs. To verify
    that the cluster is running correctly, execute the [*localPython.py* script](https://oreil.ly/rl5SL).
  prefs: []
  type: TYPE_NORMAL
- en: Installing Ray on Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When it comes to the actual cluster’s installation on Kubernetes, Ray provides
    two basic mechanisms:'
  prefs: []
  type: TYPE_NORMAL
- en: Cluster launcher
  prefs: []
  type: TYPE_NORMAL
- en: Similar to installation using VMs, this makes it simple to deploy a Ray cluster
    on any cloud. It will provision a new instance or machine using the cloud provider’s
    SDK, execute shell commands to set up Ray with the provided options, and initialize
    the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Ray Kubernetes operator
  prefs: []
  type: TYPE_NORMAL
- en: This facilitates deploying Ray on an existing Kubernetes cluster. The operator
    defines a [custom resource](https://oreil.ly/RTWR9) called a `RayCluster`, which
    describes the desired state of the Ray cluster, and a [custom controller](https://oreil.ly/ADp7y),
    the Ray Operator, which processes RayCluster resources and manages the Ray cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When you install Ray on a Kubernetes cluster by using both the cluster launcher
    and operator, Ray uses Kubernetes capabilities to create a new Ray node in the
    form of Kubernetes pod. Although the Ray autoscaler works the same way, it effectively
    “steals” resources from the Kubernetes cluster. Therefore, your Kubernetes cluster
    has to either be large enough to support all of Ray’s resource requirements or
    provide its own autoscaling mechanism. In addition, because Ray’s nodes are in
    this case implemented as underlying Kubernetes pods, the Kubernetes resource manager
    can delete these pods at any time to obtain additional resources.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Ray on a kind Cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To demonstrate both approaches, let’s start by installing and accessing the
    Ray cluster on a [kind (Kubernetes in Docker) cluster](https://oreil.ly/qvuAi).
    This popular tool runs local Kubernetes clusters by using Docker container “nodes”
    and is often used for local development. To do this, you need to create a cluster
    first by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This creates a cluster with a default configuration. To modify the configuration,
    refer to the [configuration documentation](https://oreil.ly/Rvq54). Once the cluster
    is up and running, you can use either `ray up` or the Kubernetes operator to create
    a Ray cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Using ray up
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To create a Ray cluster by using `ray up`, you must specify the resource requirements
    in a YAML file, such as [*raycluster.yaml*](https://oreil.ly/YGOp5), which was
    adapted from the Ray Kubernetes autoscaler defaults in the [Ray GitHub repository](https://oreil.ly/m2mm2).
    This file contains all the information required to create the Ray cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: General information about the cluster name and autoscaling parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Information about the cluster provider (Kubernetes, in our case), including
    provider-specific information required for the creation of Ray cluster’s nodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Node-specific information (CPU/memory, etc). This also includes a list of node
    startup commands, including Python libraries required for the installation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With this file in place, a command to create a cluster looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the cluster creation completes, you can see that several pods are running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'As requested by our YAML file, you can see one head and two worker nodes. To
    verify that the cluster is running correctly, you can use the following [job](https://oreil.ly/swESN):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The execution results in something similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Once your job is up, you can additionally port-forward the `ray-ray-head` service
    by running the following:^([3](app02.html#idm45354756330752))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Then, connect to it from your local machine by using the [*localPython.py* testing
    script from the book’s example files](https://oreil.ly/RV8yx). Execution of this
    code produces the same results as shown previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, you can port-forward ray service to port 8265 to look at the
    Ray dashboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Once this is done, you can take a look at the Ray dashboard ([Figure B-3](#fig-appB-3)).
  prefs: []
  type: TYPE_NORMAL
- en: '![spwr ab03](assets/spwr_ab03.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure B-3\. Ray dashboard
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can uninstall the Ray cluster by using the following command:^([4](app02.html#idm45354756317264))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Using the Ray Kubernetes Operator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For deployment to the Kubernetes cluster, we can also use the Ray operator,
    which is a recommended approach. To simplify usage of the operator, Ray provides
    a [Helm chart](https://oreil.ly/4jjfI) available as part of the Ray GitHub repository.
    Here, instead of the Helm chart, we are using several YAML files to deploy Ray
    to make installation a bit simpler.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our deployment is split into three files: [*operatorcrd.yaml*](https://oreil.ly/wORyi),
    containing all the commands for CustomResourceDefinition (CRD) creation; [*operator.yaml*](https://oreil.ly/RyjD7),
    containing all the commands for operator creation; and [*rayoperatorcluster.yaml*](https://oreil.ly/Ibqbn),
    containing all the commands for cluster creation. It is assumed in these files
    that the operator is created in the namespace *ray*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To install the operator itself, we need to execute these two commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Once this is done, use the following command to ensure that the operator pod
    is running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Once the operator is up and running, you can start the cluster itself by using
    the following command:^([5](app02.html#idm45354756301680))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Here the content of *rayoperatorcluster.yaml* is similar to the content of *raycluster.yaml*
    but formatted slightly differently. Once the cluster is up and running, you can
    use the same validation code as described previously for `ray up`.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Ray on OpenShift
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenShift is a type of Kubernetes cluster, so theoretically the Kubernetes operator
    can be used to install Ray on an OpenShift cluster. Unfortunately, this installation
    is a little bit more involved.
  prefs: []
  type: TYPE_NORMAL
- en: If you have ever used OpenShift, you know that by default all of the pods in
    OpenShift run in [restrictive mode](https://oreil.ly/ZkcDY). This mode denies
    access to all host features and requires pods to be run with a unique identifier
    (UID) and Security-Enhanced Linux (SELinux) context that are allocated to the
    namespace.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, this does not quite work for the Ray operator, designed to run
    as user 1000\. To enable this, you need to introduce several changes to the files
    that you used for installing on the kind (and any other plain Kubernetes cluster):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the `ray-operator-serviceaccount` service account, which is used by the
    operator, to `anyuid` mode. This allows users to run with any nonroot UID:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Modify [*operator.yaml*](https://oreil.ly/eYIht) to ensure that the operator
    pod is running as user 1000.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, a [testing job](https://oreil.ly/R2r8x) has to be modified slightly
    to run as user 1000\. This requires the creation of a `ray-node-serviceaccount`
    service account used for running a job and adding this service account to `anyuid`
    mode, which allows users to run with any nonroot UID.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ray provides a wealth of deployment options. When using Ray to solve a specific
    problem, you will need to decide which option is is most suitable for your specific
    situation.
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](app02.html#idm45354756650320-marker)) In the interest of transparency:
    Boris currently works at IBM, and Holden used to work at IBM. Holden has also
    worked for Google, Microsoft, and Amazon.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](app02.html#idm45354756645408-marker)) See the [“Boto3 Docs 1.24.95” documentation](https://oreil.ly/5A6jE)
    for information on setting up Boto3 configuration.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](app02.html#idm45354756330752-marker)) Theoretically, you can also create
    an ingress to connect to the Ray cluster. Unfortunately, in the case of the NGINX
    ingress controller, it will not work. The issue here is that the Ray client is
    using unsecure gRPC, while the NGINX ingress controller supports only secure gRPC
    calls. When using the Ray cluster on a specific cloud, check whether an ingress
    supports unsecure gRPC before exposing Ray’s head service as an ingress.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](app02.html#idm45354756317264-marker)) This command deletes pods, and it
    leaves behind the service created as part of a cluster. You have to manually delete
    a service for a complete cleanup.
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](app02.html#idm45354756301680-marker)) Although documentation mentions
    a cluster-wide deploy operator, it works only for a namespace where the operator
    is deployed.
  prefs: []
  type: TYPE_NORMAL
