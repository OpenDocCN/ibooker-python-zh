- en: Chapter 4\. Dask DataFrame
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章 Dask DataFrame
- en: Pandas DataFrames, while popular, quickly run into memory constraints as data
    sizes grow, since they store the entirety of the data in memory. Pandas DataFrames
    have a robust API for all kinds of data manipulation and are frequently the starting
    point for many analytics and machine learning projects. While pandas itself does
    not have machine learning built in, data scientists often use it as part of data
    and feature preparation during the exploratory phase of new projects. As such,
    scaling pandas DataFrames to be able to handle large datasets is of vital importance
    to many data scientists. Most data scientists are already familiar with the pandas
    libraries, and Dask’s DataFrame implements much of the pandas API while adding
    the ability to scale.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Pandas DataFrame非常流行，但随着数据规模的增长，它们很快会遇到内存限制，因为它们将整个数据存储在内存中。Pandas DataFrame具有强大的API，用于各种数据操作，并且经常是许多分析和机器学习项目的起点。虽然Pandas本身没有内置机器学习功能，但数据科学家们经常在新项目的探索阶段的数据和特征准备中使用它。因此，将Pandas
    DataFrame扩展到能够处理大型数据集对许多数据科学家至关重要。大多数数据科学家已经熟悉Pandas库，而Dask的DataFrame实现了大部分Pandas
    API，并且增加了扩展能力。
- en: Dask is one of the first to implement a usable subset of the pandas APIs, but
    other projects such as Spark have added their approaches. This chapter assumes
    you have a good understanding of the pandas DataFrame APIs; if not, you should
    check out [*Python for Data Analysis*](https://learning.oreilly.com/library/view/python-for-data/9781098104023).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Dask是最早实现可用子集的Pandas API之一，但其他项目如Spark已经添加了它们自己的方法。本章假定您已经对Pandas DataFrame
    API有很好的理解；如果没有，您应该查看[*Python for Data Analysis*](https://learning.oreilly.com/library/view/python-for-data/9781098104023)。
- en: You can often use Dask DataFrames as a replacement for pandas DataFrames with
    minor changes, thanks to duck-typing. However, this approach can have performance
    drawbacks, and some functions are not present. These drawbacks come from the distributed
    parallel nature of Dask, which adds communication costs for certain types of operations.
    In this chapter, you will learn how to minimize these performance drawbacks and
    work around any missing functionality.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 由于鸭子类型，你经常可以在只做少量更改的情况下使用Dask DataFrame替代Pandas DataFrame。然而，这种方法可能会有性能缺陷，并且一些功能是不存在的。这些缺点来自于Dask的分布式并行性质，它为某些类型的操作增加了通信成本。在本章中，您将学习如何最小化这些性能缺陷，并解决任何缺失功能。
- en: Dask DataFrames require that your data and your computation are well suited
    to pandas DataFrames. Dask has bags for unstructured data, arrays for array-structured
    data, the Dask delayed interface for arbitrary functions, and actors for stateful
    operations. If even at a small scale you wouldn’t consider using pandas for your
    problem, Dask DataFrames are probably not the right solution.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Dask DataFrame要求您的数据和计算与Pandas DataFrame非常匹配。Dask有用于非结构化数据的bags，用于数组结构化数据的arrays，用于任意函数的Dask延迟接口，以及用于有状态操作的actors。如果即使在小规模下您都不考虑使用Pandas解决您的问题，那么Dask
    DataFrame可能不是正确的解决方案。
- en: How Dask DataFrames Are Built
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dask DataFrame的构建方式
- en: Dask DataFrames build on top of pandas DataFrames. Each partition is stored
    as a pandas DataFrame.^([1](ch04.xhtml#id479)) Using pandas DataFrames for the
    partitions simplifies the implementation of much of the APIs. This is especially
    true for row-based operations, where Dask passes the function call down to each
    pandas DataFrame.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Dask DataFrame是基于Pandas DataFrame构建的。每个分区都存储为一个Pandas DataFrame。^([1](ch04.xhtml#id479))
    使用Pandas DataFrame作为分区简化了许多API的实现。特别是对于基于行的操作，Dask会将函数调用传递给每个Pandas DataFrame。
- en: Most of the distributed components of Dask DataFrames use the three core building
    blocks `map_partitions`, `reduction`, and `rolling`. You mostly won’t need to
    call these functions directly; you will use higher-level APIs instead. But understanding
    these functions and how they work is important to understanding how Dask works.
    `shuffle` is a critical building block of distributed DataFrames for reorganizing
    your data. Unlike the other building blocks, you may use it directly more frequently,
    as Dask is unable to abstract away partitioning.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数分布式组件都是基于三个核心构建模块`map_partitions`、`reduction`和`rolling`来构建的。通常情况下，你不需要直接调用这些函数；而是使用更高级的API。但是理解这些函数以及它们的工作原理对于理解Dask的工作方式非常重要。`shuffle`是重新组织数据的分布式DataFrame的关键构建块。与其他构建模块不同的是，你可能更频繁地直接使用它，因为Dask无法隐藏分区。
- en: Loading and Writing
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载和写入
- en: Data analytics is only as valuable as the data it has access to, and our insights
    are helpful only if they result in action. Since not all of our data is in Dask,
    it’s essential to read and write data from the rest of the world. So far, the
    examples in this book have mainly used local collections, but you have many more
    options.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析只有在能够访问到数据时才有价值，我们的见解只有在产生行动时才有帮助。由于我们的数据并非全部都在 Dask 中，因此从世界其他地方读取和写入数据至关重要。到目前为止，本书中的示例主要使用了本地集合，但您有更多选择。
- en: Dask supports reading and writing many standard file formats and filesystems.
    These formats include CSV, HDF, fixed-width, Parquet, and ORC. Dask supports many
    of the standard distributed filesystems, from HDFS to S3, and reading from regular
    filesystems.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 支持读取和写入许多标准文件格式和文件系统。这些格式包括 CSV、HDF、定宽、Parquet 和 ORC。Dask 支持许多标准的分布式文件系统，从
    HDFS 到 S3，以及从常规文件系统读取。
- en: Most important for Dask, distributed filesystems allow multiple computers to
    read and write to the same set of files. Distributed filesystems often store data
    on multiple computers, which allows for storing more data than a single computer
    can hold. Often, but not always, distributed filesystems are also fault tolerant
    (which they achieve through replication). Distributed filesystems can have important
    performance differences from what you are used to working with, so it’s important
    to skim the user documentation for the filesystems you are using. Some things
    to look for are block sizes (you often don’t want to write files smaller than
    these, as the rest is wasted space), latency, and consistency guarantees.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Dask 最重要的是，分布式文件系统允许多台计算机读取和写入相同的文件集。分布式文件系统通常在多台计算机上存储数据，这允许存储比单台计算机更多的数据。通常情况下，分布式文件系统也具有容错性（通过复制来实现）。分布式文件系统可能与您习惯的工作方式有重要的性能差异，因此重要的是查看您正在使用的文件系统的用户文档。需要关注的一些内容包括块大小（通常不希望写入比这些更小的文件，因为其余部分是浪费空间）、延迟和一致性保证。
- en: Tip
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Reading from regular local files can be complicated in Dask, as the files need
    to exist on all workers. If a file exists only on the head node, consider copying
    it to a distributed filesystem like S3 or NFS, or load it locally and use Dask’s
    `client.scatter` function to distribute the data if it’s small enough. Sufficiently
    small files *may* be a sign that you don’t yet need Dask, unless the processing
    on them is complex or slow.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Dask 中从常规本地文件读取可能会很复杂，因为文件需要存在于所有工作节点上。如果文件仅存在于主节点上，请考虑将其复制到像 S3 或 NFS 这样的分布式文件系统，或者在本地加载并使用
    Dask 的 `client.scatter` 函数来分发数据（如果数据足够小）。足够小的文件*可能*表明你还不需要使用 Dask，除非对其进行处理很复杂或很慢。
- en: Formats
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 格式
- en: Dask’s DataFrame loading and writing functions start with `to_` or `read_` as
    the prefixes. Each format has its own configuration, but in general, the first
    positional argument is the location of the data to be read. The location can be
    a wildcard path of files (e.g., *s3://test-bucket/magic/**), a list of files,
    or a regular file location.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 的 DataFrame 加载和写入函数以 `to_` 或 `read_` 作为前缀。每种格式都有自己的配置，但通常第一个位置参数是要读取的数据的位置。位置可以是文件的通配符路径（例如
    *s3://test-bucket/magic/**）、文件列表或常规文件位置。
- en: Note
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Wildcard paths work only with filesystems that support directory listing. For
    example, they do not work on HTTP.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 通配符路径仅适用于支持目录列表的文件系统。例如，它们在 HTTP 上不起作用。
- en: When loading data, having the right number of partitions will speed up all of
    your operations. Sometimes it’s not possible to load the data with the right number
    of partitions, and in those cases you can repartition your data after the load.
    As discussed, more partitions allow for more parallelism but have a non-zero overhead.
    The different formats have slightly different ways to control this. HDF takes
    `chunksize`, indicating the number of rows per partition. Parquet also takes `split_row_groups`,
    which takes an integer of the desired logical partitioning out of the Parquet
    file, and Dask will split the whole set into those chunks, or less. If not given,
    the default behavior is for each partition to correspond to a Parquet file. The
    text-based formats (CSV, fixed-width, etc.) take a `blocksize` parameter with
    the same meaning as Parquet’s `chunksize` but a maximum value of 64 MB. You can
    verify this by loading a dataset and seeing the number of tasks and partitions
    increase with smaller target sizes, as in [Example 4-1](#ex_load_1kb).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载数据时，正确设置分区数量将加快所有操作的速度。有时无法以正确的分区数加载数据，在这种情况下，您可以在加载后重新分区数据。正如讨论的那样，更多的分区允许更多的并行处理，但也带来非零的开销。不同的格式有略微不同的控制方式。HDF
    使用 `chunksize`，表示每个分区的行数。Parquet 也使用 `split_row_groups`，它接受一个整数，表示期望从 Parquet
    文件中逻辑分区的划分，并且 Dask 将整个数据集分割成这些块，或更少。如果未指定，默认行为是每个分区对应一个 Parquet 文件。基于文本的格式（CSV、固定宽度等）使用
    `blocksize` 参数，其含义与 Parquet 的 `chunksize` 相同，但最大值为 64 MB。您可以通过加载数据集并查看任务和分区数量随着较小的目标大小增加来验证这一点，就像
    [示例 4-1](#ex_load_1kb) 中所示。
- en: Example 4-1\. Dask DataFrame loading CSV with 1 KB chunks
  id: totrans-19
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-1\. 使用 1 KB 块加载 CSV 的 Dask DataFrame
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Loading CSV and JSON files can be more complicated than Parquet, and other self-describing
    data types don’t have any schema information encoded. Dask DataFrames need to
    know the types of the different columns to serialize the data correctly. By default,
    Dask will automatically look at the first few records and guess the data types
    for each column. This process is known as schema inference, and it can be quite
    slow.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 加载 CSV 和 JSON 文件可能比 Parquet 更复杂，而其他自描述数据类型没有编码任何模式信息。Dask DataFrame 需要知道不同列的类型，以正确地序列化数据。默认情况下，Dask
    将自动查看前几条记录并猜测每列的数据类型。这个过程称为模式推断，但它可能相当慢。
- en: Unfortunately, schema inference does not always work. For example, if you try
    to load the UK’s gender pay gap disparity data from *https​://gender-pay-gap​.ser⁠vice.gov.uk/viewing/download-data/2021*,
    when you access the data, as in [Example 4-2](#ex_load_uk_gender_pay_gap_infered),
    you will get an error of “Mismatched dtypes found in `pd.read​_csv`/`pd.read_table`.”
    When Dask’s column type inference is incorrect, you can override it (per column)
    by specifying the `dtype` parameter, as shown in [Example 4-3](#ex_load_uk_gender_pay_gap).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，模式推断并不总是有效。例如，如果尝试从 *https​://gender-pay-gap​.ser⁠vice.gov.uk/viewing/download-data/2021*
    加载英国性别工资差距数据时，如同 [示例 4-2](#ex_load_uk_gender_pay_gap_infered) 中所示，将会出现 “在 `pd.read​_csv`/`pd.read_table`
    中找到的不匹配的数据类型” 的错误。当 Dask 的列类型推断错误时，您可以通过指定 `dtype` 参数（每列）来覆盖它，就像 [示例 4-3](#ex_load_uk_gender_pay_gap)
    中所示。
- en: Example 4-2\. Dask DataFrame loading CSV, depending entirely on inference
  id: totrans-23
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-2\. 使用完全依赖推断加载 CSV 的 Dask DataFrame
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Example 4-3\. Dask DataFrame loading CSV and specifying data type
  id: totrans-25
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-3\. 使用指定数据类型加载 CSV 的 Dask DataFrame
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In theory, you can have Dask sample more records by specifying more bytes with
    the `sample` parameter, but this does not currently fix the problem. The current
    sampling code does not strictly respect the number of bytes requested.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在理论上，通过使用 `sample` 参数并指定更多字节，可以让 Dask 采样更多记录，但目前这并不能解决问题。当前的采样代码并没有严格遵守请求的字节数量。
- en: Even when schema inference does not return an error, depending on it has a number
    of drawbacks. Schema inference involves sampling data, and its results are therefore
    both probabilistic and slow. When you can, you should use self-describing formats
    or otherwise avoid schema inference; your data loading will be faster and more
    reliable. Some common self-describing formats you may encounter include Parquet,
    Avro, and ORC.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 即使模式推断没有返回错误，完全依赖它也有许多缺点。模式推断涉及对数据的抽样，因此其结果既是概率性的又很慢。在可以的情况下，应使用自描述格式或避免模式推断；这样可以提高数据加载速度并增强可靠性。您可能会遇到的一些常见自描述格式包括
    Parquet、Avro 和 ORC。
- en: Reading and writing from/to new file formats is a lot of work, especially if
    there are no existing Python libraries. If there are existing libraries, you might
    find it easier to read the raw data into a bag and parse it with a `map` function,
    which we will explore further in the next chapter.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 读取和写入新文件格式是一项繁重的工作，特别是如果没有现成的Python库。如果有现成的库，您可能会发现将原始数据读入一个包并使用`map`函数解析它会更容易，我们将在下一章进一步探讨这一点。
- en: Tip
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Dask does not detect sorted data on load. Instead, if you have presorted data,
    add the `sorted=true` parameter when setting an index to take advantage of your
    already sorted data, a step you will learn about in the next section. If you specify
    this when the data is not sorted, however, you may get silent data corruption.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Dask在加载时不会检测排序数据。相反，如果您有预排序数据，在设置索引时添加`sorted=true`参数可以利用您已经排序的数据，这是您将在下一节中学习的步骤。但是，如果在数据未排序时指定此选项，则可能会导致数据静默损坏。
- en: You can also connect Dask to databases or microservices. Relational databases
    are a fantastic tool and are often quite performant at both simple reads and writes.
    Often relational databases support distributed deployment whereby the data is
    split up on multiple nodes, and this is mostly used with large datasets. Relational
    databases tend to be great at handling transactions at scale, but running analytic
    capabilities on the same node can encounter issues. Dask can be used to efficiently
    read and compute over SQL databases.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以将Dask连接到数据库或微服务。关系型数据库是一种很棒的工具，通常在简单读写方面表现出色。通常，关系型数据库支持分布式部署，其中数据分割在多个节点上，这在处理大型数据集时经常使用。关系型数据库通常非常擅长处理大规模的事务，但在同一节点上运行分析功能可能会遇到问题。Dask可用于有效地读取和计算SQL数据库中的数据。
- en: You can use Dask’s built-in support for loading SQL databases using SQLAlchemy.
    For Dask to split up the query on multiple machines, you need to give it an index
    key. Often SQL databases will have a primary key or numerical index key that you
    can use for this purpose (e.g., `read_sql_table("customers", index_col="customer_id")`).
    An example of this is shown in [Example 4-4](#ex_read_SQL_Dataframe).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用Dask的内置支持通过SQLAlchemy加载SQL数据库。为了让Dask在多台机器上拆分查询，您需要给它一个索引键。通常，SQL数据库会有一个主键或数字索引键，您可以用于此目的（例如，`read_sql_table("customers",
    index_col="customer_id")`）。示例在[示例 4-4](#ex_read_SQL_Dataframe)中展示了这一点。
- en: Example 4-4\. Reading from and writing to SQL with Dask DataFrame
  id: totrans-35
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-4\. 使用Dask DataFrame从SQL读取和写入数据
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: More advanced connections to databases or microservices are best made using
    the bag interface and writing your custom load code, which you will learn more
    about in the next chapter.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 更高级的与数据库或微服务的连接最好使用包接口并编写自定义加载代码，关于这一点您将在下一章中学到更多。
- en: Filesystems
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文件系统
- en: Loading data can be a substantial amount of work and a bottleneck, so Dask distributes
    this like most other tasks. If you are using Dask distributed, each worker must
    have access to the files to parallelize the loading. Rather than copying the file
    to each worker, network filesystems allow everyone to access the files. Dask’s
    file access layer uses the FSSPEC library (from the intake project) to access
    the different filesystems. Since FSSPEC supports a range of filesystems, it does
    not install the requirements for every supported filesystem. Use the code in [Example 4-5](#ex_supported_fs)
    to see which filesystems are supported and which ones need additional packages.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 加载数据可能是大量工作和瓶颈，因此Dask像大多数其他任务一样进行分布式处理。如果使用Dask分布式，每个工作节点必须能够访问文件以并行加载。与将文件复制到每个工作节点不同，网络文件系统允许每个人访问文件。Dask的文件访问层使用FSSPEC库（来自intake项目）来访问不同的文件系统。由于FSSPEC支持一系列文件系统，因此它不会为每个支持的文件系统安装要求。使用[示例 4-5](#ex_supported_fs)中的代码查看支持的文件系统及需要额外包的文件系统。
- en: Example 4-5\. Getting a list of FSSPEC-supported filesystems
  id: totrans-40
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-5\. 获取FSSPEC支持的文件系统列表
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Many filesystems require some kind of configuration, be it endpoint or credentials.
    Often new filesystems, like MinIO, offer S3-compatible APIs but overload the endpoint
    and require some extra configuration to function. With Dask you specify the configuration
    parameters to the read/write function with `storage​_options`. Everyone’s configuration
    here will likely be a bit different.^([2](ch04.xhtml#id522)) Dask will use your
    `storage_options` dict as the keyword arguments to the underlying FSSPEC implementation.
    For example, my `storage_options` for MinIO are shown in [Example 4-6](#minio_storage_options).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 许多文件系统都需要某种配置，无论是端点还是凭证。通常新的文件系统，比如 MinIO，提供与 S3 兼容的 API，但超载端点并需要额外的配置才能正常运行。使用
    Dask，您可以通过 `storage​_options` 参数来指定读写函数的配置参数。每个人的配置可能会有所不同。^([2](ch04.xhtml#id522))
    Dask 将使用您的 `storage_options` 字典作为底层 FSSPEC 实现的关键字参数。例如，我对 MinIO 的 `storage_options`
    如 [示例 4-6](#minio_storage_options) 所示。
- en: Example 4-6\. Configuring Dask to talk to MinIO
  id: totrans-43
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-6\. 配置 Dask 以连接到 MinIO
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Indexing
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 索引
- en: Indexing into a DataFrame is one of the powerful features of pandas, but it
    comes with some restrictions when moving into a distributed system like Dask.
    Since Dask does not track the size of each partition, positional indexing by row
    is not supported. You can use positional indexing for columns, as well as label
    indexing for columns or rows.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DataFrame 中进行索引是 pandas 的强大功能之一，但在进入像 Dask 这样的分布式系统时，会有一些限制。由于 Dask 不跟踪每个分区的大小，不支持按行进行位置索引。您可以对列使用位置索引，以及对列或行使用标签索引。
- en: Indexing is frequently used to filter the data to have only the components you
    need. We did this for the San Francisco COVID-19 data by looking at just the case
    rates for people of all vaccine statuses, as shown in [Example 4-7](#index_covid_data).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 索引经常用于过滤数据，仅保留您需要的组件。我们通过查看仅显示所有疫苗接种状态的人的案例率来处理旧金山 COVID-19 数据，如 [示例 4-7](#index_covid_data)
    所示。
- en: Example 4-7\. Dask DataFrame indexing
  id: totrans-48
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-7\. Dask DataFrame 索引
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: If you truly need positional indexing by row, you can implement your own by
    computing the size of each partition and using this to select the desired partition
    subsets. This is very inefficient, so Dask avoids implementing it directly; make
    an intentional choice before doing this.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您真的需要按行进行位置索引，请通过计算每个分区的大小并使用它来选择所需的分区子集来实现。这非常低效，因此 Dask 避免直接实现它；在执行此操作之前，请做出明智的选择。
- en: Shuffles
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 洗牌
- en: As mentioned in the previous chapter, shuffles are expensive. The primary causes
    of the expensive nature of shuffles are the serialization overhead in moving data
    between processes and the comparative slowness of networks relative to reading
    data from memory. These costs scale as the amount of data being shuffled increases,
    so Dask has techniques to reduce the amount of data being shuffled. These techniques
    depend on certain data properties or the operation being performed.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前一章所述，洗牌是昂贵的。导致洗牌昂贵的主要原因是在进程之间移动数据时的序列化开销，以及与从内存读取数据相比，网络的相对慢速。这些成本会随着被洗牌的数据量增加而增加，因此
    Dask 有一些技术来减少被洗牌的数据量。这些技术取决于特定的数据属性或正在执行的操作。
- en: Rolling Windows and map_overlap
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 滚动窗口和 map_overlap
- en: One situation that can trigger the need for a shuffle is a rolling window, where
    at the edges of a partition your function needs some records from its neighbors.
    Dask DataFrame has a special `map_overlap` function in which you can specify a
    *look-after* window (also called a *look-ahead* window) and *look-behind* window
    (also called a *look-back* window) of rows to transfer (either an integer or a
    time delta). The simplest example taking advantage of this is a rolling average,
    shown in [Example 4-8](#rolling_date_ex).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 触发洗牌的一种情况是滚动窗口，在分区的边缘，您的函数需要其邻居的一些记录。Dask DataFrame 具有特殊的 `map_overlap` 函数，您可以在其中指定一个*后视*窗口（也称为*向前*窗口）和一个*前视*窗口（也称为*向后*窗口）来传输行数（可以是整数或时间差）。利用此功能的最简单示例是滚动平均，如
    [示例 4-8](#rolling_date_ex) 所示。
- en: Example 4-8\. Dask DataFrame rolling average
  id: totrans-55
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-8\. Dask DataFrame 滚动平均
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Using `map_overlap` allows Dask to transfer only the data needed. For this implementation
    to work correctly, your minimum partition size needs to be larger than your largest
    window.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `map_overlap` 允许 Dask 仅传输所需的数据。为使此实现正常工作，您的最小分区大小必须大于最大窗口。
- en: Warning
  id: totrans-58
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Dask’s rolling windows will not cross multiple partitions. If your DataFrame
    is partitioned so that the look-after or look-back is greater than the length
    of the neighbor’s partition, the results will either fail or be incorrect. Dask
    validates this for time delta look-after, but no such checks are performed for
    look-backs or integer look-after.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 的滚动窗口不会跨多个分区。如果你的 DataFrame 被分区，以至于向后或向前查看大于相邻分区的长度，结果将失败或不正确。Dask 对时间增量向后查看进行验证，但对向前查看或整数向后查看不执行此类检查。
- en: An effective but expensive technique for working around the single-partition
    look-ahead/look-behind of Dask is to `repartition` your Dask DataFrames.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 解决 Dask 单分区向前/向后查看的有效但昂贵的技术是`repartition`你的 Dask DataFrames。
- en: Aggregations
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚合
- en: Aggregations are another special case that can reduce the amount of data that
    needs to be transferred over the network. Aggregations are functions that combine
    records. If you are coming from a map/reduce or Spark background, `reduceByKey`
    is the classic aggregation. Aggregations can either be “by key” or be global across
    an entire DataFrame.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合是另一种特殊情况，可以减少需要通过网络传输的数据量。聚合是将记录组合的函数。如果你来自 map/reduce 或 Spark 背景，`reduceByKey`是经典的聚合函数。聚合可以是“按键”或全局的跨整个
    DataFrame。
- en: To aggregate by key, you first need to call `groupby` with the column(s) representing
    the key, or the keying function to aggregate on. For example, calling `df.groupby("PostCode")`
    groups your DataFrame by postal code, or calling `df.groupby(["PostCode", "SicCodes"])`
    uses a combination of columns for grouping. Function-wise, many of the same pandas
    aggregates are available, but the performance of aggregates in Dask are very different
    from local pandas DataFrames.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 要按键聚合，首先需要使用表示键的列调用`groupby`，或用于聚合的键函数。例如，调用`df.groupby("PostCode")`按邮政编码对 DataFrame
    进行分组，或调用`df.groupby(["PostCode", "SicCodes"])`使用多列进行分组。在功能上，许多与 pandas 相同的聚合函数可用，但
    Dask 中的聚合性能与本地 pandas DataFrames 有很大不同。
- en: Tip
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If you’re aggregating by partition key, Dask can compute the aggregation without
    needing a shuffle.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果按分区键聚合，Dask 可以在不需要洗牌的情况下计算聚合结果。
- en: The first way to speed up your aggregations is to reduce the columns that you
    are aggregating on, since the fastest data to process is no data. Finally, when
    possible, doing multiple aggregations at the same time reduces the number of times
    the same data needs to be shuffled. Therefore, if you need to compute the average
    and the max, you should compute both at the same time (see [Example 4-9](#max_mean)).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 加快聚合的第一种方法是减少正在进行聚合的列，因为处理速度最快的数据是没有数据。最后，如果可能的话，同时进行多次聚合减少了需要洗牌同样数据的次数。因此，如果需要计算平均值和最大值，应同时计算两者（见[示例
    4-9](#max_mean)）。
- en: Example 4-9\. Dask DataFrame max and mean
  id: totrans-67
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-9\. Dask DataFrame 最大值和平均值
- en: '[PRE8]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: For distributed systems like Dask, if an aggregation can be partially evaluated
    and then merged, you can potentially combine some records pre-shuffle. Not all
    partial aggregations are created equal. What matters with partial aggregations
    is that the amount of data is reduced when merging values with the same key compared
    to the original multiple values.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像 Dask 这样的分布式系统，如果可以部分评估然后合并聚合结果，你可以在洗牌之前组合一些记录。并非所有部分聚合都是相同的。部分聚合的关键在于，与原始的多个值相比，在合并相同键的值时数据量有所减少。
- en: The most efficient aggregations take a sublinear amount of space regardless
    of the number of records. Some of these, such as sum, count, first, minimum, maximum,
    mean, and standard deviation, can take constant space. More complicated tasks,
    like quantiles and distinct counts, also have sublinear approximation options.
    These approximation options can be great, as exact answers can require linear
    growth in storage.^([3](ch04.xhtml#id539))
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 最有效的聚合需要亚线性数量的空间，不管记录的数量如何。其中一些，如 sum、count、first、minimum、maximum、mean 和 standard
    deviation，可以占用恒定空间。更复杂的任务，如分位数和不同计数，也有亚线性的近似选项。这些近似选项非常好用，因为精确答案可能需要存储的线性增长。^([3](ch04.xhtml#id539))
- en: Some aggregation functions are not sublinear in growth, but tend to or might
    not grow too quickly. Counting the distinct values is in this group, but if all
    your values are unique there is no space saving.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 有些聚合函数在增长上不是亚线性的，但往往或可能增长不是太快。计数不同值属于此类，但如果所有值都是唯一的，则没有节省空间。
- en: To take advantage of efficient aggregations, you need to use a built-in aggregation
    from Dask, or write your own using Dask’s aggregation class. Whenever you can,
    use a built-in. Built-ins not only require less effort but also are often faster.
    Not all of the pandas aggregates are directly supported in Dask, so sometimes
    your only choice is to write your own aggregate.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 要利用高效的聚合功能，您需要使用来自Dask的内置聚合，或者使用Dask的聚合类编写自己的聚合方法。在可以的情况下，使用内置聚合。内置聚合不仅需要更少的工作量，而且通常更快。并非所有的pandas聚合在Dask中都直接支持，因此有时你唯一的选择是编写自己的聚合。
- en: 'If you choose to write your own aggregate, you have three functions to define:
    `chunk` for handling each group-partition/chunk, `agg` to combine the results
    of `chunk` between partitions, and (optionally) `finalize` to take the result
    of `agg` and produce a final value.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果选择编写自己的聚合，需要定义三个函数：`chunk`用于处理每个组-分区/块，`agg`用于在分区之间组合`chunk`的结果，以及（可选的）`finalize`用于获取`agg`的结果并生成最终值。
- en: The fastest way to understand how to use partial aggregation is by looking at
    an example that uses all three functions. Using the weighted average in [Example 4-10](#weight_avg)
    can help you think of what is needed for each function. The first function needs
    to compute the weighted values and the weights. The `agg` function combines these
    by summing each side part of the tuple. Finally, the `finalize` function divides
    the total by the weights.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 理解如何使用部分聚合的最快方法是查看一个使用所有三个函数的示例。在[示例 4-10](#weight_avg)中使用加权平均值可以帮助你思考每个函数所需的内容。第一个函数需要计算加权值和权重。`agg`函数通过对元组的每一部分进行求和来结合这些值。最后，`finalize`函数通过权重将总和除以。
- en: Example 4-10\. Dask custom aggregate
  id: totrans-75
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-10\. Dask自定义聚合
- en: '[PRE9]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In some cases, such as with a pure summation, you don’t need to do any post-processing
    on `agg`’s output, so you can skip the `finalize` function.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，例如纯粹的求和，您不需要在`agg`的输出上进行任何后处理，因此可以跳过`finalize`函数。
- en: Not all aggregations must be by key; you can also compute aggregations across
    all rows. Dask’s custom aggregation interface, however, is exposed only with by-key
    operations.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有的聚合都必须按键进行；您还可以跨所有行计算聚合。然而，Dask的自定义聚合接口仅在按键操作时才暴露出来。
- en: Dask’s built-in full DataFrame aggregations use a lower-level interface called
    `apply_contact_apply` for partial aggregations. Rather than learn two different
    APIs for partial aggregations, we prefer to do a static `groupby` by providing
    a constant grouping function. This way, we only have to know one interface for
    aggregations. You can use this to find the aggregate COVID-19 numbers across the
    DataFrame, as shown in [Example 4-11](#agg_entire).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Dask的内置完整DataFrame聚合使用一个称为`apply_contact_apply`的低级接口进行部分聚合。与学习两种不同的部分聚合API相比，我们更喜欢通过提供一个常量分组函数来进行静态的`groupby`。这样，我们只需了解一个聚合的接口。您可以使用此方法在DataFrame中查找聚合COVID-19数字，如[示例 4-11](#agg_entire)所示。
- en: Example 4-11\. Aggregating across the entire DataFrame
  id: totrans-80
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-11\. 跨整个DataFrame进行聚合
- en: '[PRE10]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'When built-in aggregation exists, it will likely be better than anything we
    would write. Sometimes a partial aggregation is partially implemented, as in the
    case of Dask’s HyperLogLog: it is implemented only for full DataFrames. You can
    often translate simple aggregations using `apply_contact_apply` or `aca` by copying
    the `chunk` function, using the `combine` parameter for `agg`, and using the `aggregate`
    parameter for `finalize`. This is shown via porting Dask’s HyperLogLog implementation
    in [Example 4-12](#custom_agg_hyperloglog).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 当存在内置聚合时，它很可能比我们编写的任何内容都要好。有时，部分聚合是部分实现的，例如Dask的HyperLogLog：它仅适用于完整的DataFrames。您通常可以通过复制`chunk`函数，使用`agg`的`combine`参数以及`finalize`的`aggregate`参数来转换简单的聚合。这通过在[示例 4-12](#custom_agg_hyperloglog)中移植Dask的HyperLogLog实现来展示。
- en: Example 4-12\. Wrapping Dask’s HyperLogLog in `dd.Aggregation`
  id: totrans-83
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-12\. 使用`dd.Aggregation`包装Dask的HyperLogLog
- en: '[PRE11]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Slow/inefficient aggregations (or those very likely to cause an out-of-memory
    exception) use storage proportional to the records being aggregated. Examples
    from this slow group include making a list and naively computing exact quantiles.^([4](ch04.xhtml#id545))
    With these slow aggregates, using Dask’s aggregation class has no benefit over
    the `apply` API, which you may wish to use for simplicity. For example, if you
    just wanted a list of employer IDs by postal code, rather than having to write
    three functions you could use a one-liner like `df.groupby("PostCode")["EmployerId"].apply(lambda
    g: list(g))`. Dask implements the `apply` function as a full shuffle, which is
    covered in the next section.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '缓慢/低效的聚合操作（或者那些很可能导致内存不足异常的操作）使用与被聚合的记录数量成比例的存储空间。这些缓慢的操作包括制作列表和简单计算精确分位数。^([4](ch04.xhtml#id545))
    在这些缓慢的聚合操作中，使用 Dask 的聚合类与 `apply` API 没有任何优势，后者可能更简单。例如，如果您只想要一个按邮政编码分组的雇主 ID
    列表，而不必编写三个函数，可以使用像 `df.groupby("PostCode")["EmployerId"].apply(lambda g: list(g))`
    这样的一行代码。Dask 将 `apply` 函数实现为一个完全的洗牌，这在下一节中有详细介绍。'
- en: Warning
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Dask is unable to apply partial aggregations when you use the `apply` function.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 在使用 `apply` 函数时无法应用部分聚合。
- en: Full Shuffles and Partitioning
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 完全洗牌和分区
- en: If an operation seems to be slower inside of Dask than you would expect from
    working in local DataFrames, it might be because it requires a full shuffle. An
    example of this is sorting, which is inherently expensive in distributed systems
    because it most often requires a shuffle. Full shuffles are sometimes an unavoidable
    part of working in Dask. Counterintuitively, while full shuffles are themselves
    slow, you can use them to speed up future operations that are all happening on
    the same grouping key(s). As mentioned in the aggregation section, one of the
    ways a full shuffle is triggered is by using the `apply` method when partitioning
    is not aligned.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 Dask 内部的操作比在本地 DataFrame 中的预期要慢，可能是因为它需要进行完全洗牌。例如，排序就是一个例子，因为在分布式系统中排序通常需要进行洗牌，所以它本质上是昂贵的。在
    Dask 中，有时完全洗牌是无法避免的。与完全洗牌本身慢速的相反，您可以使用它们来加速将来在相同分组键上进行的操作。正如在聚合部分提到的那样，触发完全洗牌的一种方式是在不对齐分区的情况下使用
    `apply` 方法。
- en: Partitioning
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分区
- en: You will most commonly use full shuffles to repartition your data. It’s important
    to have the right partitioning when dealing with aggregations, rolling windows,
    or look-ups/indexing. As discussed in the rolling window section, Dask cannot
    do more than one partition’s worth of look-ahead or look-behind, so having the
    right partitioning is required to get the correct results. For most other operations,
    having incorrect partitioning will slow down your job.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在重新分区数据时，您最常使用完全洗牌。在处理聚合、滚动窗口或查找/索引时，拥有正确的分区非常重要。正如在滚动窗口部分讨论的那样，Dask 不能做超过一个分区的向前或向后查找，因此需要正确的分区才能获得正确的结果。对于大多数其他操作，错误的分区将减慢作业速度。
- en: 'Dask has three primary methods for controlling the partitioning of a DataFrame:
    `set_index`, `repartition`, and `shuffle` (see [Table 4-1](#table_ch04_1687446786141)).
    You use `set_index` when changing the partitioning to a new key/index. `repartition`
    keeps the same key/index but changes the splits. `repartition` and `set_index`
    take similar parameters, with `repartition` not taking an index key name. In general,
    if you are not changing the column used for the index, you should use `repartition`.
    `shuffle` is a bit different since it does not produce a known partitioning scheme
    that operations like `groupby` can take advantage of.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 有三种主要方法来控制 DataFrame 的分区：`set_index`、`repartition` 和 `shuffle`（参见[表 4-1](#table_ch04_1687446786141)）。当将分区更改为新的键/索引时，使用
    `set_index`。`repartition` 保持相同的键/索引，但更改了分割。`repartition` 和 `set_index` 使用类似的参数，`repartition`
    不需要索引键名称。一般来说，如果不更改用于索引的列，应该使用 `repartition`。`shuffle` 稍有不同，因为它不会产生类似于 `groupby`
    可以利用的已知分区方案。
- en: Table 4-1\. Functions to control partitioning
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4-1\. 控制分区的函数
- en: '| Method | Changes index key | Sets number of partitions | Results in a known
    partitioning scheme | Ideal use case |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 更改索引键 | 设置分区数 | 导致已知分区方案 | 理想使用情况 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| `set_index` | Yes | Yes | Yes | Changing the index key |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| `set_index` | 是 | 是 | 是 | 更改索引键 |'
- en: '| `repartition` | No | Yes | Yes | Increasing/decreasing number of partitions
    |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| `repartition` | 否 | 是 | 是 | 增加/减少分区数 |'
- en: '| `shuffle` | No | Yes | No | Skewed distribution of key^([a](ch04.xhtml#id556))
    |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| `shuffle` | 否 | 是 | 否 | 键的分布倾斜^([a](ch04.xhtml#id556)) |'
- en: '| ^([a](ch04.xhtml#id556-marker)) Hashes the key for distribution, which can
    help randomly distribute skewed data *if* the keys are unique (but clustered).
    |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| ^([a](ch04.xhtml#id556-marker)) 为分布哈希键，可以帮助随机分布倾斜数据 *如果* 键是唯一的（但是集中的）。 |'
- en: The first step in getting the right partitioning for your DataFrame is to decide
    whether you want an index. Indexes are useful when filtering data by an indexed
    value, indexing, grouping, and for almost any other by-key operation. One such
    by-key operation would be a `groupby`, in which the column being grouped on could
    be a good candidate for the key. If you use a rolling window over a column, that
    column must be the key, which makes choosing the key relatively easy. Once you’ve
    decided on an index, you can call `set_index` with the column name of the index
    (e.g., `set_index("PostCode")`). This will, under most circumstances, result in
    a shuffle, so it’s a good time to size your partitions.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了为 DataFrame 获取正确的分区，第一步是决定是否需要索引。索引在按索引值过滤数据、索引、分组以及几乎任何其他按键操作时都非常有用。其中一种按键操作是
    `groupby`，其中被分组的列可以是一个很好的键候选。如果您在列上使用滚动窗口，该列必须是键，这使得选择键相对容易。一旦确定了索引，您可以使用索引列名称调用
    `set_index`（例如，`set_index("PostCode")`）。这通常会导致 shuffle，因此现在是调整分区大小的好时机。
- en: Tip
  id: totrans-101
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If you’re unsure what the current key used for partitioning is, you can check
    the `index` property to see the partitioning key.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不确定当前用于分区的键是什么，可以检查 `index` 属性以查看分区键。
- en: 'Once you’ve chosen your key, the next question is how to size your partitions.
    The advice in [“Partitioning/Chunking Collections”](ch03.xhtml#basic_partitioning)
    generally applies here: shoot for enough partitions to keep each machine busy,
    but keep in mind the general sweet spot of 100 MB to 1 GB. Dask generally computes
    pretty even splits if you give it a target number of partitions.^([5](ch04.xhtml#id558))
    Thankfully, `set_index` will also take `npartitions`. To repartition the data
    by postal code, with 10 partitions, you would add `set_index("PostCode", npartitions=10)`;
    otherwise Dask will default to the number of input partitions.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 选择了键之后，下一个问题是如何设置分区大小。通常适用于这里的建议是 [“分区/块集合”](ch03.xhtml#basic_partitioning)：尝试保持足够的分区以使每台机器保持忙碌，但请记住大约在
    100 MB 到 1 GB 的一般甜点。如果给定目标分区数，Dask 通常会计算出相当均匀的分割。^([5](ch04.xhtml#id558)) 幸运的是，`set_index`
    也将接受 `npartitions`。要通过邮政编码重新分区数据，使用 10 个分区，您可以添加 `set_index("PostCode", npartitions=10)`；否则，Dask
    将默认使用输入分区数。
- en: If you plan to use rolling windows, you will likely need to ensure that you
    have the right size (in terms of key range) covered in each partition. To do this
    as part of `set_index`, you would need to compute your own divisions to ensure
    each partition has the right range of records present. Divisions are specified
    as a list starting from the minimal value of the first partition up to the maximum
    value of the last partition. Each value in between is a “cut” point between the
    pandas DataFrames that make up the Dask DataFrame. To make a DataFrame with partitions
    `[0, 100) [100, 200), [200, 300), [300, 500)`, you would write `df.set_index("NumEmployees",
    divisions=[0, 100, 200, 300, 500])`. Similarly, for the date range to support
    a rolling window of up to seven days from around the start of the COVID-19 pandemic
    to today, see [Example 4-13](#set_index_with_rolling_window).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您计划使用滚动窗口，您可能需要确保每个分区覆盖了正确大小的键范围。作为 `set_index` 的一部分，您需要计算自己的分区来确保每个分区具有正确范围的记录。分区被指定为列表，从第一个分区的最小值到最后一个分区的最大值。在构建由
    Pandas DataFrame 组成的 Dask DataFrame 的分区 `[0, 100) [100, 200), [200, 300), [300,
    500)`，您可以编写 `df.set_index("NumEmployees", divisions=[0, 100, 200, 300, 500])`。类似地，为了支持从
    COVID-19 疫情开始到今天最多七天的滚动窗口的日期范围，请参见 [Example 4-13](#set_index_with_rolling_window)。
- en: Example 4-13\. Dask DataFrame rolling window with `set_index`
  id: totrans-105
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-13\. 使用 `set_index` 的 Dask DataFrame 滚动窗口
- en: '[PRE12]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Warning
  id: totrans-107
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Dask, including for rolling time windows, assumes that your partition index
    is monotonically increasing.^([6](ch04.xhtml#id559))
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Dask，包括用于滚动时间窗口，假设您的分区索引是单调递增的。^([6](ch04.xhtml#id559))
- en: So far, you’ve had to specify the number of partitions, or the specific divisions,
    but you might be wondering if Dask can just figure that out itself. Thankfully,
    Dask’s repartition function has the ability to pick divisions for a given target
    size, as shown in [Example 4-14](#repartition_ex). However, doing this has a non-trivial
    cost, as Dask must evaluate the DataFrame as well as the repartition itself.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你必须指定分区的数量或具体的分割点，但你可能想知道Dask是否可以自己找出这些。幸运的是，Dask的repartition函数有能力为给定的目标大小选择分割点，就像在[Example 4-14](#repartition_ex)中展示的那样。然而，这样做会有一个不可忽视的成本，因为Dask必须评估DataFrame以及重新分区本身。
- en: Example 4-14\. Dask DataFrame automatic partitioning
  id: totrans-110
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 4-14\. Dask DataFrame 自动分区
- en: '[PRE13]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Warning
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Dask’s `set_index` has a similar `partition_size` parameter but, as of this
    writing, [works only to reduce the number of partitions](https://oreil.ly/3I9Gm).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 截至本文撰写时，Dask的`set_index`有一个类似的`partition_size`参数，但仅适用于减少分区的数量。
- en: As you saw at the start of this chapter, when writing a DataFrame, each partition
    is given its own file, but sometimes this can result in files that are too big
    or too small. Some tools can accept only one file as input, so you need to repartition
    everything into a single partition. At other times, the data storage system is
    optimized for a certain file size, like the HDFS default block size of 128 MB.
    The good news is that techniques such as `repartition` and `set_index` solve these
    problems for you.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在本章开头看到的，当写入一个DataFrame时，每个分区都有其自己的文件，但有时这会导致文件过大或过小。有些工具只能接受一个文件作为输入，因此你需要将所有内容重新分区为单个分区。其他时候，数据存储系统被优化为特定的文件大小，例如HDFS的默认块大小为128
    MB。好消息是，诸如`repartition`和`set_index`的技术已经为你解决了这些问题。
- en: Embarrassingly Parallel Operations
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 尴尬的并行操作
- en: Dask’s `map_partitions` function applies a function to each of the partitions
    underlying pandas DataFrames, and the result is also a pandas DataFrame. Functions
    implemented with `map_partitions` are embarrassingly parallel since they don’t
    require any inter-worker transfer of data.^([7](ch04.xhtml#id566)) Dask implements
    `map` with `map_partitions`, as well as many row-wise operations. If you want
    to use a row-wise operation that you find missing, you can implement it yourself,
    as shown in [Example 4-15](#filna_ex).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Dask的`map_partitions`函数将一个函数应用于底层pandas DataFrame的每个分区，结果也是一个pandas DataFrame。使用`map_partitions`实现的函数是尴尬的并行，因为它们不需要任何数据的跨worker传输。^([7](ch04.xhtml#id566))
    Dask实现了`map`与`map_partitions`，以及许多逐行操作。如果你想使用一个你找不到的逐行操作，你可以自己实现，就像在[Example 4-15](#filna_ex)中展示的那样。
- en: Example 4-15\. Dask DataFrame `fillna`
  id: totrans-117
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 4-15\. Dask DataFrame `fillna`
- en: '[PRE14]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: You aren’t limited to calling pandas built-ins. Provided that your function
    takes and returns a DataFrame, you can do pretty much anything you want inside
    `map​_parti⁠tions`.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 你并不局限于调用pandas内置函数。只要你的函数接受并返回一个DataFrame，你几乎可以在`map_partitions`中做任何你想做的事情。
- en: The full pandas API is too long to cover in this chapter, but if a function
    can operate on a row-by-row basis without any knowledge of the rows before or
    after, it may already be implemented in Dask DataFrames using `map_partitions`.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的pandas API在本章中太长无法涵盖，但如果一个函数可以在不知道前后行的情况下逐行操作，那么它可能已经在Dask DataFrame中使用`map_partitions`实现了。
- en: When using `map_partitions` on a DataFrame, you can change anything about each
    row, including the key that it is partitioned on. If you *are* changing the values
    in the partition key, you *must* either clear the partitioning information on
    the resulting DataFrame with `clear_divisions()` *or* specify the correct indexing
    with `set_index`, which you’ll learn more about in the next section.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 当在一个DataFrame上使用`map_partitions`时，你可以改变每行的任何内容，包括它分区的键。如果你改变了分区键中的值，你*必须*用`clear_divisions()`清除结果DataFrame上的分区信息，*或者*用`set_index`指定正确的索引，这个你将在下一节学到更多。
- en: Warning
  id: totrans-122
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Incorrect partitioning information can result in incorrect results, not just
    exceptions, as Dask may miss relevant data.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 不正确的分区信息可能导致不正确的结果，而不仅仅是异常，因为Dask可能会错过相关数据。
- en: Working with Multiple DataFrames
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理多个DataFrame
- en: Pandas and Dask have four common functions for combining DataFrames. At the
    root is the `concat` function, which allows you to join DataFrames on any axis.
    Concatenating DataFrames is generally slower in Dask since it involves inter-worker
    communication. The other three functions are `join`, `merge`, and `append`, all
    of which implement special cases for common situations on top of `concat` and
    have slightly different performance considerations. Having good divisions/partitioning,
    in terms of key selection and number of partitions, makes a huge difference when
    working on multiple DataFrames.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas和Dask有四个常用的用于组合DataFrame的函数。在根上是 `concat` 函数，它允许您在任何轴上连接DataFrames。由于涉及到跨worker的通信，Dask中的DataFrame连接通常较慢。另外三个函数是
    `join`、`merge` 和 `append`，它们都在 `concat` 的基础上针对常见情况实现了特殊处理，并具有略微不同的性能考虑。在处理多个DataFrame时，通过良好的分区和键选择，尤其是分区数量，可以显著提升性能。
- en: Dask’s `join` and `merge` functions take most of the standard pandas arguments
    along with an extra optional one, `npartitions`. `npartitions` specifies a target
    number of output partitions, but it is used only for hash joins (which you’ll
    learn about in [“Multi-DataFrame Internals”](#multidataframe_internals_ch04_1689595647992)).
    Both functions automatically repartition your input DataFrames if needed. This
    is great, as you might not know the partitioning, but since repartitioning can
    be slow, explicitly using the lower-level `concat` function when you don’t expect
    any partitioning changes to be needed can help catch performance problems early.
    Dask’s `join` can take more than two DataFrames at a time only when doing a *left*
    or *outer* join type.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Dask的 `join` 和 `merge` 函数接受大多数标准的pandas参数，还有一个额外的可选参数 `npartitions`。 `npartitions`
    指定了目标输出分区的数量，但仅在哈希连接中使用（您将在 [“多DataFrame内部”](#multidataframe_internals_ch04_1689595647992)
    中了解到）。这两个函数会根据需要自动重新分区您的输入DataFrames。这非常好，因为您可能不了解分区情况，但由于重新分区可能很慢，当您不希望进行任何分区更改时，明确使用较低级别的
    `concat` 函数可以帮助及早发现性能问题。Dask的 `join` 在进行*left*或*outer*连接类型时可以一次处理多个DataFrame。
- en: Tip
  id: totrans-127
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Dask has special logic to speed up multi-DataFrame joins, so in most cases,
    rather than doing `a.join(b).join(c).join(d)​.join(e)`, you will benefit from
    doing `a.join([b, c, d, e])`. However, if you are performing a left join with
    a small dataset, then the first syntax may be more efficient.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Dask具有特殊逻辑，可加速多个DataFrame的连接操作，因此在大多数情况下，您可以通过执行 `a.join([b, c, d, e])` 而不是
    `a.join(b).join(c).join(d).join(e)` 获得更好的性能。但是，如果您正在执行与小数据集的左连接，则第一种语法可能更有效。
- en: When you combine or `concat` DataFrames by row (similar to a SQL UNION), the
    performance depends on whether divisions of the DataFrames being combined are
    *well ordered*. We call the divisions of a series of DataFrames well ordered if
    all the divisions are known and the highest division of the previous DataFrame
    is below that of the lowest division of the next. If any input has an unknown
    division, Dask will produce an output without known partitioning. With all known
    partitions, Dask treats row-based concatenations as a metadata-only change and
    will not perform any shuffle. This requires that there is no overlap between the
    divisions. There is also an extra `interleave_partitions` parameter, which will
    change the join type for row-based combinations to one without the input partitioning
    restriction and result in a known partitioner. Dask DataFrames with known partitioners
    can support faster look-ups and operations by key.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 当您按行合并或 `concat` DataFrames（类似于SQL UNION）时，性能取决于被合并DataFrames的分区是否 *well ordered*。如果一系列DataFrame的分区是良好排序的，那么所有分区都是已知的，并且前一个DataFrame的最高分区低于下一个DataFrame的最低分区，则我们称这些DataFrame的分区是良好排序的。如果任何输入具有未知分区，Dask将产生一个没有已知分区的输出。对于所有已知分区，Dask将行合并视为仅元数据的更改，并且不会执行任何数据重排。这要求分区之间没有重叠。还有一个额外的
    `interleave_partitions` 参数，它将行合并的连接类型更改为无输入分区限制的连接类型，并导致已知的分区结果。具有已知分区的Dask DataFrame可以通过键支持更快的查找和操作。
- en: Dask’s column-based `concat` (similar to a SQL JOIN) also has restrictions around
    the divisions/partitions of the DataFrames it is combining. Dask’s version of
    `concat` supports only inner or full outer join, not left or right. Column-based
    joins require that all inputs have known partitioners and also result in a DataFrame
    with known partitioning. Having a known partitioner can be useful for subsequent
    joins.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Dask的基于列的`concat`（类似于SQL JOIN）在合并的DataFrame的分区/分片上也有限制。Dask的`concat`仅支持内连接或全外连接，不支持左连接或右连接。基于列的连接要求所有输入具有已知的分区器，并且结果是具有已知分区的DataFrame。拥有已知的分区器对后续的连接非常有用。
- en: Warning
  id: totrans-131
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Don’t use Dask’s `concat` when operating by row on a DataFrame with unknown
    divisions, as it will likely return incorrect results.^([8](ch04.xhtml#id579))
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理具有未知分区的DataFrame时，不要使用Dask的`concat`，因为它可能会返回不正确的结果。^([8](ch04.xhtml#id579))
- en: Multi-DataFrame Internals
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多DataFrame内部
- en: 'Dask uses four techniques—​hash, broadcast, partitioned, and `stack_partitions`—to
    combine DataFrames, and each has very different performance. These four functions
    do not map 1:1 with the join functions you choose from. Rather, Dask chooses the
    technique based on the indexes, divisions, and requested join type (e.g., outer/left/inner).
    The three column-based join techniques are hash joins, broadcast joins, and partitioned
    joins. When doing row-based combinations (e.g., `append`), Dask has a special
    technique called `stack_partitions` that is extra fast. It’s important that you
    understand the performance of each of these techniques and the conditions that
    will cause Dask to pick each approach:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Dask使用四种技术——哈希、广播、分区和`stack_partitions`——来组合DataFrame，每种技术的性能差异很大。这四个函数与您从中选择的连接函数并不一一对应。相反，Dask根据索引、分区和请求的连接类型（例如外部/左/内部）选择技术。三种基于列的连接技术是哈希连接、广播连接和分区连接。在进行基于行的组合（例如`append`）时，Dask具有一种称为`stack_partitions`的特殊技术，速度特别快。重要的是，您理解每种技术的性能以及Dask选择每种方法的条件：
- en: Hash joins
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 哈希连接
- en: The default that Dask uses when no other join technique is suitable. Hash joins
    shuffle the data for all the input DataFrames to partition on the target key.
    They use the hash values of keys, which results in a DataFrame that is not in
    any particular order. As such, the result of a hash join does not have any known
    divisions.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 当没有其他适合的连接技术时，Dask默认使用哈希连接。哈希连接会对所有输入的DataFrame进行数据分区，以目标键为基准。它使用键的哈希值，导致结果DataFrame没有特定的顺序。因此，哈希连接的结果没有任何已知的分区。
- en: Broadcast joins
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 广播连接
- en: Ideal for joining large DataFrames with small DataFrames. In a broadcast join,
    Dask takes the smaller DataFrame and distributes it to all the workers. This means
    that the smaller DataFrame must be able to fit in memory. To tell Dask that a
    DataFrame is a good candidate for broadcasting, you make sure it is all stored
    in one partition, such as by calling `repartition(npartitions=1)`.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 适用于将大型DataFrame与小型DataFrame连接。在广播连接中，Dask获取较小的DataFrame并将其分发到所有工作节点。这意味着较小的DataFrame必须能够适应内存。要告诉Dask一个DataFrame适合广播，请确保它全部存储在一个分区中，例如通过调用`repartition(npartitions=1)`。
- en: Partitioned joins
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 分区连接
- en: Occur when combining DataFrames along an index where the partitions/​divi⁠sions
    are known for all the DataFrames. Since the input partitions are known, Dask is
    able to align the partitions between the DataFrames, involving less data transfer,
    as each output partition has less than a full set of inputs.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在沿索引组合DataFrame时发生，其中所有DataFrame的分区/分片都是已知的。由于输入的分区是已知的，Dask能够在DataFrame之间对齐分区，涉及的数据传输更少，因为每个输出分区都包含不到完整输入集的数据。
- en: Since partitioned and broadcast joins are faster, doing some work to help Dask
    can be worth it. For example, concatenating several DataFrames with known and
    aligned partitions/divisions and one unaligned DataFrame will result in an expensive
    hash join. Instead, try to either set the index and partition on the remaining
    DataFrame or join the less expensive DataFrames first and then perform the expensive
    join after.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 由于分区连接和广播连接速度更快，为帮助Dask做一些工作是值得的。例如，将几个具有已知和对齐分区/分片的DataFrame连接到一个未对齐的DataFrame会导致昂贵的哈希连接。相反，尝试设置剩余DataFrame的索引和分区，或者先连接较便宜的DataFrame，然后再执行昂贵的连接。
- en: The fourth technique, `stack_partitions`, is different from the other options
    since it doesn’t involve any movement of data. Instead, the resulting DataFrame
    partitions list is a union of the upstream partitions from the input DataFrames.
    Dask uses `stack_partitions` for most row-based combinations except when all of
    the input DataFrame divisions are known, they are not well ordered, and you ask
    Dask to `interleave_partitions`. The `stack_partitions` technique is able to provide
    known partitioning in its output only when the input divisions are known and well
    ordered. If all of the divisions are known but not well ordered and you set `interleave​_parti⁠tions`,
    Dask will use a partitioned join instead. While this approach is comparatively
    inexpensive, it is not free, and it can result in an excessively large number
    of partitions, requiring you to repartition anyway.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 第四种技术 `stack_partitions` 与其他选项不同，因为它不涉及任何数据的移动。相反，生成的 DataFrame 分区列表是输入 DataFrames
    的上游分区的并集。Dask 在大多数基于行的组合中使用 `stack_partitions` 技术，除非所有输入 DataFrame 的分区都已知，它们没有被很好地排序，并且你要求
    Dask `interleave_partitions`。在输出中，`stack_partitions` 技术只有在输入分区已知且有序时才能提供已知分区。如果所有分区都已知但排序不好，并且你设置了
    `interleave​_parti⁠tions`，Dask 将使用分区连接。虽然这种方法相对廉价，但并非免费，而且可能导致分区数量过多，需要重新分区。
- en: Missing Functionality
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缺失功能
- en: Not all multi-DataFrame operations are implemented, like `compare`, which leads
    us into the next section about the limitations of Dask DataFrames.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有的多数据框操作都已实现，比如 `compare`，这将我们引入关于 Dask DataFrames 限制的下一节。
- en: What Does Not Work
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不起作用的功能
- en: Dask’s DataFrame implements most, but not all, of the pandas DataFrame API.
    Some of the pandas API is not implemented in Dask because of the development time
    involved. Other parts are not used to avoid exposing an API that would be unexpectedly
    slow.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 的 DataFrame 实现了大部分但并非全部的 pandas DataFrame API。由于开发时间的原因，Dask 中未实现部分 pandas
    API。其他部分则未使用，以避免暴露可能意外缓慢的 API。
- en: Sometimes the API is just missing small parts, as both pandas and Dask are under
    active development. An example is the `split` function from [Example 2-10](ch02.xhtml#wc_dataframe).
    In local pandas, instead of doing `split().explode()`, you could have called `split(expand=true)`.
    Some of these missing parts can be excellent places for you to get involved and
    [contribute to the Dask project](https://oreil.ly/Txd_R) if you are interested.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候 API 只是缺少一些小部分，因为 pandas 和 Dask 都在积极开发中。一个例子是来自 [Example 2-10](ch02.xhtml#wc_dataframe)
    的 `split` 函数。在本地 pandas 中，你可以调用 `split(expand=true)` 而不是 `split().explode()`。如果你有兴趣，这些缺失部分可以是你参与并
    [贡献到 Dask 项目](https://oreil.ly/Txd_R) 的绝佳机会。
- en: Some libraries do not parallelize as well as others. In these cases, a common
    approach is to try to filter or aggregate the data down enough that it can be
    represented locally and then apply the local libraries to the data. For example,
    with graphing, it’s common to pre-aggregate the counts or take a random sample
    and graph the result.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 有些库并不像其他那样有效地并行化。在这些情况下，一种常见的方法是尝试将数据筛选或聚合到足够可以在本地表示的程度，然后再将本地库应用到数据上。例如，在绘图时，通常会预先聚合计数或取随机样本并绘制结果。
- en: While much of the pandas DataFrame API will work, before you swap in Dask DataFrame,
    it’s important to make sure you have good test coverage to catch the situations
    where it does not.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然大部分 pandas DataFrame API 可以正常工作，但在你切换到 Dask DataFrame 之前，确保有充分的测试覆盖来捕捉它不适用的情况是非常重要的。
- en: What’s Slower
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 速度慢
- en: Usually, using Dask DataFrames will improve performance, but not always. Generally,
    smaller datasets will perform better in local pandas. As discussed, anything involving
    shuffles is generally slower in a distributed system than in a local one. Iterative
    algorithms can also produce large graphs of operations, which are slow to evaluate
    in Dask compared to traditional greedy evaluation.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，使用 Dask DataFrames 会提高性能，但并非总是如此。一般来说，较小的数据集在本地 pandas 中表现更好。正如讨论的那样，在涉及到洗牌的任何情况下，分布式系统中通常比本地系统慢。迭代算法也可能产生大量的操作图，这在
    Dask 中与传统的贪婪评估相比，评估速度较慢。
- en: Some problems are generally unsuitable for data-parallel computing. For example,
    writing out to a data store with a single lock that has more parallel writers
    will increase the lock contention and may make it slower than if a single thread
    was doing the writing. In these situations, you can sometimes repartition your
    data or write individual partitions to avoid lock contention.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 某些问题通常不适合数据并行计算。例如，写入带有单个锁的数据存储，其具有更多并行写入者，将增加锁竞争并可能比单个线程进行写入更慢。在这些情况下，有时可以重新分区数据或写入单个分区以避免锁竞争。
- en: Handling Recursive Algorithms
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理递归算法
- en: Dask’s lazy evaluation, powered by its lineage graph, is normally beneficial,
    allowing it to combine steps automatically. However, when the graph gets too large,
    Dask can struggle to manage it, which often shows up as a slow driver process
    or notebook, and sometimes as an out-of-memory exception. Thankfully, you can
    work around this by writing out your DataFrame and reading it back in. Generally,
    Parquet is the best format for doing this as it is space-efficient and self-describing,
    so no schema inference is required.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 的惰性评估，由其谱系图支持，通常是有益的，允许它自动组合步骤。然而，当图变得过大时，Dask 可能会难以管理，通常表现为驱动进程或笔记本运行缓慢，有时会出现内存不足的异常。幸运的是，您可以通过将
    DataFrame 写出并重新读取来解决这个问题。一般来说，Parquet 是这样做的最佳格式，因为它在空间上高效且自我描述，因此无需进行模式推断。
- en: Re-computed Data
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重新计算的数据
- en: Another challenge of lazy evaluation is if you want to reuse an element multiple
    times. For example, say you want to load a few DataFrames and then compute multiple
    pieces of information. You can ask Dask to keep a collection (including DataFrame,
    series, etc.) in memory by running `client.persist(collection)`. Not all re-computed
    data needs to be avoided; for example, if loading the DataFrames is fast enough,
    it might be fine not to persist them.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 惰性评估的另一个挑战是如果您想多次重用一个元素。例如，假设您想加载几个 DataFrame，然后计算多个信息片段。您可以要求 Dask 通过运行 `client.persist(collection)`
    将集合（包括 DataFrame、Series 等）保存在内存中。并非所有重新计算的数据都需要避免；例如，如果加载 DataFrame 很快，不持久化它们可能是可以接受的。
- en: Warning
  id: totrans-157
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Like other functions in Dask, `persist()` does not modify the DataFrame—​and
    if you call functions on it you will still have your data re-computed. This is
    notably different from Apache Spark.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Apache Spark 明显不同，像 Dask 的其他函数一样，`persist()` 不会修改 DataFrame — 如果您在其上调用函数，数据仍然会重新计算。
- en: How Other Functions Are Different
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他函数的不同之处
- en: 'For performance reasons, various parts of Dask DataFrames behave a little differently
    than local DataFrames:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 由于性能原因，Dask DataFrame 的各个部分行为可能与本地 DataFrame 稍有不同：
- en: '`reset_index`'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '`reset_index`'
- en: The index will start back over at zero on each partition.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 每个分区的索引将在零点重新开始。
- en: '`kurtosis`'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`kurtosis`'
- en: This function does not filter out NaNs and uses SciPy defaults.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数不会过滤掉 NaN，并使用 SciPy 的默认值。
- en: '`concat`'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '`concat`'
- en: Instead of coercing category types, each category type is expanded to the union
    of all the categories it is concatenated with.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 不同于强制转换类别类型，每个类别类型都会扩展到与其连接的所有类别的并集。
- en: '`sort_values`'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '`sort_values`'
- en: Dask supports only single-column sorts.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 仅支持单列排序。
- en: Joining multiple DataFrames
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 连接多个 DataFrame
- en: When joining more than two DataFrames at the same time, the join type must be
    either outer or left.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 当同时连接两个以上的 DataFrame 时，连接类型必须是 outer 或 left。
- en: When porting your code to use Dask DataFrames, you should be especially mindful
    anytime you use these functions, as they might not exactly work in the axis you
    intended. Work small first and test the correctness of the numbers, as issues
    can often be tricky to track down.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在将代码移植到使用 Dask DataFrame 时，您应特别注意任何时候使用这些函数，因为它们可能不会完全在您预期的轴上工作。首先进行小范围测试，并测试数字的正确性，因为问题通常很难追踪。
- en: When porting existing pandas code to Dask, consider using the local single-machine
    version to produce test datasets to compare the results with, to ensure that all
    changes are intentional.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在将现有的 pandas 代码移植到 Dask 时，请考虑使用本地单机版本生成测试数据集，以便与结果进行比较，以确保所有更改都是有意的。
- en: 'Data Science with Dask DataFrame: Putting It Together'
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Dask DataFrame 进行数据科学：将其放在一起
- en: Dask DataFrame has already proven to be a popular framework for big data uses,
    so we wanted to highlight a common use case and considerations. Here, we use a
    canonical data science challenge dataset, the New York City yellow taxicab, and
    walk through what a data engineer working with this dataset might consider. In
    the subsequent chapters covering ML workloads, we will be using many of the DataFrame
    tools to build on.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Dask DataFrame已经被证明是用于大数据的流行框架，因此我们希望强调一个常见的用例和考虑因素。在这里，我们使用一个经典的数据科学挑战数据集，即纽约市黄色出租车，并介绍一个数据工程师处理此数据集可能考虑的内容。在涵盖机器学习工作负载的后续章节中，我们将使用许多DataFrame工具来构建。
- en: Deciding to Use Dask
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决定使用Dask
- en: As discussed earlier, Dask excels in data-parallel tasks. A particularly good
    fit is a dataset that may already be available in columnar format, like Parquet.
    We also assess where the data lives, such as in S3 or in other remote storage
    options. Many data scientists and engineers would probably have a dataset that
    cannot be contained on a single machine or cannot be stored locally due to compliance
    constraints. Dask’s design lends itself well to these use cases.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面讨论的，Dask在数据并行任务中表现出色。一个特别适合的数据集是可能已经以列格式，如Parquet格式，可用的数据集。我们还评估数据存储在哪里，例如在S3或其他远程存储选项中。许多数据科学家和工程师可能会有一个不能在单台机器上容纳或由于合规性约束而无法在本地存储的数据集。Dask的设计非常适合这些用例。
- en: 'Our NYC taxi data fits all these criteria: the data is stored in S3 by the
    City of New York in Parquet format, and it is easily scalable up and down, as
    it is partitioned by dates. Additionally, we evaluate that the data is structured
    already, so we can use Dask DataFrame. Since Dask DataFrames and pandas DataFrames
    are similar, we can also use a lot of existing workflows for pandas. We can sample
    a few of these, do our exploratory data analysis in a smaller dev environment,
    and then scale it up to the full dataset, all with the same code. Note that for
    [Example 4-16](#ex_load_nyc_taxi), we use row groups to specify chunking behavior.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的NYC出租车数据符合所有这些标准：数据以Parquet格式由纽约市存储在S3中，并且它可以轻松地进行横向和纵向扩展，因为它按日期进行了分区。此外，我们评估数据已经结构化，因此我们可以使用Dask
    DataFrame。由于Dask DataFrames和pandas DataFrames相似，我们还可以使用许多现有的pandas工作流。我们可以对其中一些样本进行采样，在较小的开发环境中进行探索性数据分析，然后使用相同的代码扩展到完整数据集。请注意，在[示例 4-16](#ex_load_nyc_taxi)中，我们使用行组来指定分块行为。
- en: Example 4-16\. Dask DataFrame loading multiple Parquet files
  id: totrans-178
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-16\. 使用Dask DataFrame加载多个Parquet文件
- en: '[PRE15]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Exploratory Data Analysis with Dask
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Dask进行探索性数据分析
- en: The first step of data science often consists of exploratory data analysis (EDA),
    or understanding the dataset and plotting its shape. Here, we use Dask DataFrames
    to walk through the process and examine the common troubleshooting issues that
    arise from nuanced differences between pandas DataFrame and Dask DataFrame.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学的第一步通常包括探索性数据分析（EDA），或者了解数据集并绘制其形状。在这里，我们使用Dask DataFrames来走过这个过程，并检查由于pandas
    DataFrame和Dask DataFrame之间微妙差异而引起的常见故障排除问题。
- en: Loading Data
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载数据
- en: The first time you load the data into your dev environment, you might encounter
    block size issues or schema issues. While Dask tries to infer both, at times it
    cannot. Block size issues will often show up when you call `.compute()` on trivial
    code and see one worker hitting the memory ceiling. In that case, some manual
    work would be involved in determining the right chunk size. Schema issues would
    show up as an error or a warning as you read the data, or in subtle ways later
    on, such as mismatching float32 and float64\. If you know the schema already,
    it’s a good idea to enforce that by specifying dtypes at reading.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次将数据加载到您的开发环境中时，您可能会遇到块大小问题或模式问题。虽然Dask尝试推断两者，但有时会失败。块大小问题通常会在您对微不足道的代码调用`.compute()`时出现，看到一个工作线程达到内存限制。在这种情况下，需要进行一些手动工作来确定正确的块大小。模式问题将显示为读取数据时的错误或警告，或者稍后以微妙的方式显示，例如不匹配的float32和float64。如果您已经了解模式，建议在读取时通过指定dtype来强制执行。
- en: As you further explore a dataset, you might encounter data printed by default
    in a format that you don’t like, for example, scientific notation. The control
    for that is through pandas, not Dask itself. Dask implicitly calls pandas, so
    you want to explicitly set your preferred format using pandas.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在进一步探索数据集时，您可能会遇到默认以您不喜欢的格式打印的数据，例如科学计数法。这可以通过pandas而不是Dask本身来控制。Dask隐式调用pandas，因此您希望使用pandas显式设置您喜欢的格式。
- en: Summary statistics on the data work just like `.describe()` from pandas, along
    with specified percentiles or `.quantile()`. Remember to chain multiple computes
    together if you are running several of these, which will save compute time back
    and forth. Using Dask DataFrame `describe` is shown in [Example 4-17](#ex_describe_percentiles).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的汇总统计工作类似于 pandas 的`.describe()`，还可以指定百分位数或`.quantile()`。请注意，如果运行多个这样的计算，请链式调用它们，这样可以节省计算时间。在
    [示例 4-17](#ex_describe_percentiles) 中展示了如何使用 Dask DataFrame 的 `describe` 方法。
- en: Example 4-17\. Dask DataFrame describing percentiles with pretty formatting
  id: totrans-186
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-17\. 使用漂亮格式描述百分位数的 Dask DataFrame
- en: '[PRE16]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Plotting Data
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 绘制数据
- en: Plotting data is often an important step in getting to know your dataset. Plotting
    big data is a tricky subject. We as data engineers often get around that issue
    by first working with a smaller sampled dataset. For that, Dask would work alongside
    a Python plotting library such as matplotlib or seaborn, just like pandas. The
    advantage of Dask DataFrame is that we are now able to plot the entire dataset,
    if desired. We can use plotting frameworks along with Dask to plot the entire
    dataset. Here, Dask does the filtering, the aggregation on the distributed workers,
    and then collects down to one worker to give to a non-distributed library like
    matplotlib to render. Plotting a Dask DataFrame is shown in [Example 4-18](#ex_plot_distances).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制数据通常是了解数据集的重要步骤。绘制大数据是一个棘手的问题。作为数据工程师，我们经常通过首先使用较小的采样数据集来解决这个问题。为此，Dask 可以与
    Python 绘图库（如 matplotlib 或 seaborn）一起使用，就像 pandas 一样。Dask DataFrame 的优势在于，现在我们可以绘制整个数据集（如果需要的话）。我们可以使用绘图框架以及
    Dask 来绘制整个数据集。在这里，Dask 进行筛选、分布式工作节点的聚合，然后收集到一个非分布式库（如 matplotlib）来渲染的工作节点。Dask
    DataFrame 的绘图示例显示在 [示例 4-18](#ex_plot_distances) 中。
- en: Example 4-18\. Dask DataFrame plotting trip distance
  id: totrans-190
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-18\. Dask DataFrame 绘制行程距离
- en: '[PRE17]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Tip
  id: totrans-192
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Note that if you’re used to the NumPy logic, you will have to think of the Dask
    DataFrame layer when plotting. For example, NumPy users would be familiar with
    `df[col].values` syntax for defining plotting variables. The `.values` mean a
    different action in Dask; what we pass is `df[col]` instead.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果你习惯于 NumPy 的逻辑，绘制时需要考虑到 Dask DataFrame 层。例如，NumPy 用户会熟悉 `df[col].values`
    语法用于定义绘图变量。在 Dask 中，`.values` 执行的操作不同；我们传递的是 `df[col]`。
- en: Inspecting Data
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查数据
- en: Pandas DataFrame users would be familiar with `.loc()` and `.iloc()` for inspecting
    data at a particular row or column. This logic translates to Dask DataFrame, with
    important differences in `.iloc()` behaviors.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas DataFrame 用户熟悉`.loc()`和`.iloc()`用于检查特定行或列的数据。这种逻辑转换到 Dask DataFrame，但是`.iloc()`的行为有重要的区别。
- en: A sufficiently large Dask DataFrame will contain multiple pandas DataFrames.
    This changes the way we should think about numbering and addressing indices. For
    example, `.iloc()` (a way to access the positions by index) doesn’t work exactly
    the same for Dask, since each smaller DataFrame would have its own `.iloc()` value,
    and Dask does not track the size of each smaller DataFrame. In other words, a
    global index value is hard for Dask to figure out, since Dask will have to iteratively
    count through each DataFrame to get to an index. Users should check `.iloc()`
    on their DataFrame and ensure that indices return the correct values.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 充分大的 Dask DataFrame 将包含多个 pandas DataFrame。这改变了我们应该如何思考编号和索引的方式。例如，对于 Dask，像`.iloc()`（通过索引访问位置的方法）不会完全像
    pandas 那样工作，因为每个较小的 DataFrame 都有自己的`.iloc()`值，并且 Dask 不会跟踪每个较小 DataFrame 的大小。换句话说，全局索引值对于
    Dask 来说很难确定，因为 Dask 将不得不逐个计算每个 DataFrame 才能获得索引。用户应该检查他们的 DataFrame 上的`.iloc()`并确保索引返回正确的值。
- en: Tip
  id: totrans-197
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Be aware that calling methods like `.reset_index()` can reset indices in each
    of the smaller DataFrames, potentially returning multiple values when users call
    `.iloc()`.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，调用像`.reset_index()`这样的方法可能会重置每个较小的 DataFrame 中的索引，当用户调用`.iloc()`时可能返回多个值。
- en: Conclusion
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this chapter, you’ve learned how to understand what kinds of operations are
    slower than you might expect with Dask. You’ve also gained a number of techniques
    to deal with the performance differences between pandas DataFrames and Dask DataFrames.
    By understanding the situations in which Dask DataFrames performance may not meet
    your needs, you’ve also gained an understanding of what problems are not well
    suited to Dask. So that you can put this all together, you’ve also learned about
    Dask DataFrame IO options. From here you will go on to learn more about Dask’s
    other collections and then how to move beyond collections.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '在本章中，你已经了解到了如何理解 Dask 中哪些操作比你预期的更慢。你还学到了一些处理 pandas DataFrames 和 Dask DataFrames
    性能差异的技术。通过理解 Dask DataFrames 性能可能不符合需求的情况，你也了解到了哪些问题不适合使用 Dask。为了能够综合这些内容，你还了解了
    Dask DataFrame 的 IO 选项。从这里开始，你将继续学习有关 Dask 的其他集合，然后再进一步了解如何超越集合。 '
- en: In this chapter, you have learned what may cause your Dask DataFrames to behave
    differently or more slowly than you might expect. This same understanding of how
    Dask DataFrames are implemented can help you decide whether distributed DataFrames
    are well suited to your problem. You’ve also seen how to get datasets larger than
    a single machine can handle into and out of Dask’s DataFrames.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你已经了解到可能导致 Dask DataFrames 行为与预期不同或更慢的原因。对于 Dask DataFrames 实现方式的相同理解可以帮助你确定分布式
    DataFrames 是否适合你的问题。你还看到了如何将超过单台机器处理能力的数据集导入和导出 Dask 的 DataFrames。
- en: ^([1](ch04.xhtml#id479-marker)) See [“Partitioning/Chunking Collections”](ch03.xhtml#basic_partitioning)
    for a review of partitioning.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch04.xhtml#id479-marker)) 参见[“分区/分块集合”](ch03.xhtml#basic_partitioning)进行分区的复习。
- en: ^([2](ch04.xhtml#id522-marker)) [FSSPEC documentation](https://oreil.ly/ZfcRv)
    includes the specifics for configuring each of the backends.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch04.xhtml#id522-marker)) [FSSPEC 文档](https://oreil.ly/ZfcRv)包含配置每个后端的具体信息。
- en: ^([3](ch04.xhtml#id539-marker)) This can lead to out-of-memory exceptions while
    executing the aggregation. The linear growth in storage requires that (within
    a constant factor) all the data must be able to fit on a single process, which
    limits how effective Dask can be.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch04.xhtml#id539-marker)) 这可能导致在执行聚合时出现内存不足异常。存储空间的线性增长要求（在一个常数因子内）所有数据都必须能够适应单个进程，这限制了
    Dask 的有效性。
- en: ^([4](ch04.xhtml#id545-marker)) Alternate algorithms for exact quantiles depend
    on more shuffles to reduce the space overhead.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch04.xhtml#id545-marker)) 准确分位数的备选算法依赖更多洗牌操作来减少空间开销。
- en: ^([5](ch04.xhtml#id558-marker)) Key-skew can make this impossible for a known
    partitioner.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch04.xhtml#id558-marker)) 键偏斜可能会使已知分区器无法处理。
- en: ^([6](ch04.xhtml#id559-marker)) Strictly increasing with no repeated values
    (e.g., 1, 4, 7 is monotonically increasing, but 1, 4, 4, 7 is not).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch04.xhtml#id559-marker)) 严格递增且无重复值（例如，1、4、7 是单调递增的，但 1、4、4、7 不是）。
- en: ^([7](ch04.xhtml#id566-marker)) [Embarrassingly parallel problems](https://oreil.ly/30938)
    are ones in which the overhead of distributed computing and communication is low.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch04.xhtml#id566-marker)) [尴尬并行问题](https://oreil.ly/30938)是指分布式计算和通信的开销很低的问题。
- en: ^([8](ch04.xhtml#id579-marker)) Dask assumes the indices are aligned when there
    are no indices present.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ^   ^([8](ch04.xhtml#id579-marker)) 当不存在索引时，Dask 假定索引是对齐的。
