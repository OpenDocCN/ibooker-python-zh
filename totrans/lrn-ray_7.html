<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 8. Online Inference with Ray Serve" data-type="chapter" epub:type="chapter"><div class="chapter" id="chapter_08">
<h1><span class="label">Chapter 8. </span>Online Inference with Ray Serve</h1>
<p class="byline">Edward Oakes</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm44990018923552">
<h5>A Note for Early Release Readers</h5>
<p>With Early Release ebooks, you get books in their earliest form—the author’s raw and unedited content as they write—so you can take advantage of these technologies long before the official release of these titles.</p>
</div></aside>
<section data-pdf-bookmark="Online Inference" data-type="sect1"><div class="sect1" id="idm44990018921968">
<h1>Online Inference</h1>
<p>In previous chapters you’ve learned how to use Ray to process data, train machine learning (ML) models, and apply them in a batch inference setting.
However, many of the most exciting use cases for machine learning involve “online inference:” using ML models to enhance API endpoints that users interact with directly or indirectly.
Online inference is important in situations where latency matters: you can’t simply apply models to data behind the scenes and serve the results.
There are many real-world examples of use cases where online inference can provide a lot of value:</p>
<p><strong>Recommendation systems</strong>: Providing recommendations for products (e.g., online shopping) or content (e.g., social media) is a bread-and-butter use case for machine learning. While it’s possible to do this in an offline manner, recommendation systems often benefit from reacting to users’ preferences in real time. This requires performing online inference using recent behavior as a key feature.</p>
<p><strong>Chat bots</strong>: Online services often have real-time chat windows to provide support to customers from the comfort of their keyboard. Traditionally, these chat windows were staffed by customer support staff but there has been a recent trend to reduce labor costs and improve time-to-resolution by replacing them with ML-powered chat bots that can be online 24/7. These chat bots require a sophisticaed mix of multiple machine learning techniques and must be able to respond to customer input in real time.</p>
<p><strong>Estimating arrival times</strong>: Ride sharing, navigation, and food delivery services all rely on being able to provide an accurate estimation for arrival times (e.g., for your driver, yourself, or your dinner). Providing accurate estimates is very difficult because it requires accounting for real-world factors such as traffic patterns, weather, and accidents. Estimates are also often refreshed many times over the course of one trip.</p>
<p>These are just a few examples where applying machine learning in an online fashion can provide a lot of value in application domains that are traditionally very difficult (imagine hand-writing logic to estimate arrival time!).
The list of applications goes on: a number of nascent domains such as self-driving cars, robotics, video-processing pipelines are also being redefined by machine learning.
All of these applications share one key feature: latency is crucial.
In the case of online services, low latency is paramount to providing a good user experience and for applications interacting with the real world such as robotics or self-driving cars, higher latency can have even stronger implications in safety or correctness.</p>
<p>This chapter will provide a gentle introduction to Ray Serve, a Ray-native library that enables building online inference applications on top of Ray.
First, we will discuss the challenges of online inference that Ray Serve addresses.
Then, we’ll cover the architecture of Ray Serve and introduce its core set of functionality.
Finally, we will use Ray Serve to build an end-to-end online inference API consisting of multiple natural language processing models.</p>
</div></section>
<section data-pdf-bookmark="Challenges of online inference" data-type="sect1"><div class="sect1" id="idm44990018917360">
<h1>Challenges of online inference</h1>
<p>In the previous section we discussed that the main goal of online inference is to interact with machine learning models with low latency.
However, this has long been a key requirement for API backends and web servers, so a natural question is: what’s different about serving machine learning models?</p>
<section data-pdf-bookmark="1. ML models are compute intensive" data-type="sect2"><div class="sect2" id="idm44990018821728">
<h2>1. ML models are compute intensive</h2>
<p>Many of the challenges in online inference follow from one key characteristic: machine learning models are very compute intensive.
Compared to traditional web serving where requests are primarily handled by I/O intensive database queries or other API calls, most machine learning models boil down to performing many linear algebra computations, be it to provide a recommendation, estimate an arrival time, or detect an object in an image.
This is especially true for the recent trend of “deep learning,” where the number of weights and computations a single model performs is growing larger and larger over time.
Often, deep learning models can also benefit significantly from using specialized hardware such as GPUs or TPUs, which have special-purpose instructions optimized for machine learning computations and enable vectorized computations across multiple inputs in parallel.</p>
<p>Many online inference applications are required to be run 24/7 by nature.
When combined with the fact that machine learning models are compute intensive, operating online inference services can be very expensive, requiring allocating many CPUs and GPUs at all times.
The primary challenges of online inference boil down to serving models in a way that minimizes end-to-end latency while also reducing cost.
There are a few key properties that online inference systems provide in order to satisfy these requirements:
- Support for specialized hardware such as GPUs and TPUs.
- The ability to scale up and down the resources used for a model in response to request load.
- Support for request batching to take advantage of vectorized computations.</p>
</div></section>
<section data-pdf-bookmark="2. ML models aren’t useful in isolation" data-type="sect2"><div class="sect2" id="idm44990018819952">
<h2>2. ML models aren’t useful in isolation</h2>
<p>Often when machine learning is discussed in the academic or research setting, the focus is on an individual, isolated task such as object recognition or classification.
However, in the real world applications are not usually so clear cut and well-defined.
Instead, a combination of multiple ML models and business logic is required to solve a problem end-to-end.
For example, consider a product recommendation use case.
While there are a multitude of known ML techniques we could apply to the core problem of making a recommendation, there are also a lot of equally important challenges around the edges, many of which will be specific to each use case:
- Validating inputs and outputs to ensure the result returned to the user makes sense semantically. Often, we may have some manual rules such as avoiding returning the same recommendation to a user multiple times in succession.
- Fetching up-to-date information about the user and available products and converting it into features for the model (in some cases, this may be performed by an online feature store).
- Combining the results of multiple models using manual rules such as filtering to the top results or selecting the model with highest confidence.</p>
<p>Implementing an online inference API requires the ability to integrate all of these pieces together into one unified service.
Therefore, it’s important to have the flexibility to compose multiple models together along with custom business logic.
These pieces can’t really be viewed completely in isolation: the “glue” logic often needs to evolve alongside the models themselves.</p>
</div></section>
</div></section>
<section data-pdf-bookmark="An Introduction to Ray Serve" data-type="sect1"><div class="sect1" id="idm44990018816944">
<h1>An Introduction to Ray Serve</h1>
<p>Ray Serve is a scalable compute layer for serving machine learning models on top of Ray.
Serve is framework-agnostic, meaning that it isn’t tied to a specific machine learning library, but rather treats models as ordinary Python code.
Additionally, it allows you to flexibly combine normal Python business logic alongside machine learning models.
This makes it possible to build online inference services completely end-to-end: a Serve application could validate user input, query a database, perform inference scalably across multiple ML models, and combine, filter, and validate the output all in the process of handling a single inference request.
Indeed, combining the results of multiple machine learning models is one of the key strengths of Ray Serve, as you’ll see later in the chapter as we explore common multi-model serving patterns.</p>
<p>While being flexible in nature, Serve has purpose-built features for compute-heavy machine learning models, enabling dynamic scaling and resource allocation to ensure that request load can be handled efficiently across many CPUs and/or GPUs.
Here, Serve inherits a lot of benefits from being built on top of Ray: it’s scalable to hundreds of machines, offers flexible scheduling policies, and offers low-overhead communication across processes using Ray’s core APIs.</p>
<p>This section will incrementally introduce core funcionality from Ray Serve with a focus on how it helps to address the challenges of online inference as outlined above.
To follow along with the code samples in this section, you’ll need the following Python packages installed locally:</p>
<pre data-type="programlisting">pip install ray["serve"]==1.12.0 transformers requests</pre>
<p>Running the examples assumes that you have the code saved locally in a file named <code>app.py</code> in the current working directory.</p>
<section data-pdf-bookmark="Architectural overview" data-type="sect2"><div class="sect2" id="idm44990018813088">
<h2>Architectural overview</h2>
<p>Ray Serve is built on top of Ray, so it inherits a lot of benefits such as scalability, low overhead communication, an API well-suited to parallelism, and the ability to leverage shared memory via the object store.
The core primitive in Ray Serve is a <strong>deployment</strong>, which you can think of as a managed group of Ray actors that can be addressed together and will handle requests load-balanced across them.
Each actor in a deployment is called a <strong>replica</strong> in Ray Serve.
Often, a deployment will map one-to-one with a machine learning model, but deployments can contain arbitrary Python code so they might also house business logic.</p>
<p>Ray Serve enables exposing deployments over HTTP and defining the input parsing and output logic.
However, one of the most important features of Ray Serve is that deployments can also call into each other directly using a native Python API, which will translate to direct actor calls between the replicas.
This enables flexible, high performance composition of models and business logic; you’ll see this in action later in the section.</p>
<p>Under the hood, the deployments making up a Ray Serve application are managed by a centralized <strong>controller</strong> actor.
This is a detached actor that is managed by Ray and will be restarted upon failure.
The controller is in charge of creating and updating replica actors, broadcasting updates to other actors in the system, and performing health checking and failure recovery.
If a replica or an entire Ray node crashes for any reason, the controller will detect the failures and ensure that the actors are recovered and can continue serving traffic.</p>
</div></section>
<section data-pdf-bookmark="Defining a basic HTTP endpoint" data-type="sect2"><div class="sect2" id="idm44990018809472">
<h2>Defining a basic HTTP endpoint</h2>
<p>This section will introduce Ray Serve by defining a simple HTTP endpoint wrapping a single ML model.
The model we’ll deploy is a sentiment classifier: given a text input, it will predict if the output had a positive or negative sentiment.
We’ll be using a pretrained sentiment classifier from the <a href="https://huggingface.co/">Hugging Face</a> <code>transformers</code> library, which provides a simple Python API for pretrained models that will abstract away the details of the model and allow us to focus on the serving logic.
To deploy this model using Ray Serve, we need to define a Python class and turn it into a Serve <strong>deployment</strong> using the <code>@serve.deployment</code> decorator.
The decorator allows us to pass a number of useful options to configure the deployment; we will explore some of those options later in the chapter.</p>
<div data-type="example">
<h5><span class="label">Example 8-1. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">ray</code> <code class="kn">import</code> <code class="n">serve</code>

<code class="kn">from</code> <code class="nn">starlette.requests</code> <code class="kn">import</code> <code class="n">Request</code>
<code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">pipeline</code>


<code class="nd">@serve</code><code class="o">.</code><code class="n">deployment</code>
<code class="k">class</code> <code class="nc">SentimentAnalysis</code><code class="p">:</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">_classifier</code> <code class="o">=</code> <code class="n">pipeline</code><code class="p">(</code><code class="s2">"sentiment-analysis"</code><code class="p">)</code>

    <code class="k">def</code> <code class="fm">__call__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">request</code><code class="p">:</code> <code class="n">Request</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">str</code><code class="p">:</code>
        <code class="n">input_text</code> <code class="o">=</code> <code class="n">request</code><code class="o">.</code><code class="n">query_params</code><code class="p">[</code><code class="s2">"input_text"</code><code class="p">]</code>
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">_classifier</code><code class="p">(</code><code class="n">input_text</code><code class="p">)[</code><code class="mi">0</code><code class="p">][</code><code class="s2">"label"</code><code class="p">]</code></pre></div>
<p>There are a few important points to note here.
First, we instantiate our model in the constructor of the class.
This model may be very large, so downloading it and loading it into memory can be slow (10s of seconds or longer).
In Ray Serve, the code in the constructor will only be run once in each replica on startup and any properties can be cached for future use.
Second, we define the logic to handle a request in the <code>__call__</code> method.
This takes a <code>Starlette</code> HTTP request as input and can return any JSON-serializable output.
In this case, we’ll return a single string from the output of our model: <code>"POSITIVE"</code> or <code>"NEGATIVE"</code>.</p>
<p>Once a deployment is defined, we use the <code>.bind()</code> API to instantiate a copy of it.
This is where we can pass optional arguments to the constructor to configure the deployment (such as a remote path to download model weights from).
Note that this doesn’t actually run the deployment, but just packages it up with its arguments (this will be more important later when we combine multiple models together).</p>
<div data-type="example">
<h5><span class="label">Example 8-2. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="n">basic_deployment</code> <code class="o">=</code> <code class="n">SentimentAnalysis</code><code class="o">.</code><code class="n">bind</code><code class="p">()</code></pre></div>
<p>We can run the bound deployment using the <code>serve.run</code> Python API or corresponding <code>serve run</code> CLI command.
Assuming you save the above code in a file called <code>app.py</code>, you can run it locally with the following command:</p>
<pre data-type="programlisting">serve run app:basic_deployment</pre>
<p>This will instantiate a single replica of our deployment and host it behind a local HTTP server.
To test it, we can use the Python <code>requests</code> package:</p>
<div data-type="example">
<h5><span class="label">Example 8-3. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">requests</code>
<code class="nb">print</code><code class="p">(</code><code class="n">requests</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s2">"http://localhost:8000/"</code><code class="p">,</code> <code class="n">params</code><code class="o">=</code><code class="p">{</code><code class="s2">"input_text"</code><code class="p">:</code> <code class="s2">"Hello friend!"</code><code class="p">})</code><code class="o">.</code><code class="n">json</code><code class="p">())</code></pre></div>
<p>Testing the sentiment classifier on a sample input text of <code>"Hello friend!"</code>, it correctly classifies the text as positive!</p>
<p>This example is effectively the “hello world” of Ray Serve: we deployed a single model behind a basic HTTP endpoint.
Note, however, that we had to manually parse the input HTTP request and feed it into our model.
For this basic example it was just a single line of code, but real world applications often take a more complex schema as input and hand-writing HTTP logic can be tedious and error prone.
To enable writing more expressive HTTP APIs, Serve integrates with the <a href="https://fastapi.tiangolo.com/">FastAPI</a> Python framework.</p>
<p>A Serve deployment can wrap a <code>FastAPI</code> app, making use of its expressive APIs for parsing inputs and configuring HTTP behavior.
In the following example, we rely on <code>FastAPI</code> to handle parsing the <code>input_text</code> query parameter for us, allowing to remove the boilerplate parsing code.</p>
<div data-type="example">
<h5><span class="label">Example 8-4. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">fastapi</code> <code class="kn">import</code> <code class="n">FastAPI</code>

<code class="n">app</code> <code class="o">=</code> <code class="n">FastAPI</code><code class="p">()</code>


<code class="nd">@serve</code><code class="o">.</code><code class="n">deployment</code>
<code class="nd">@serve</code><code class="o">.</code><code class="n">ingress</code><code class="p">(</code><code class="n">app</code><code class="p">)</code>
<code class="k">class</code> <code class="nc">SentimentAnalysis</code><code class="p">:</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">_classifier</code> <code class="o">=</code> <code class="n">pipeline</code><code class="p">(</code><code class="s2">"sentiment-analysis"</code><code class="p">)</code>

    <code class="nd">@app</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s2">"/"</code><code class="p">)</code>
    <code class="k">def</code> <code class="nf">classify</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">input_text</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">str</code><code class="p">:</code>
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">_classifier</code><code class="p">(</code><code class="n">input_text</code><code class="p">)[</code><code class="mi">0</code><code class="p">][</code><code class="s2">"label"</code><code class="p">]</code>


<code class="n">fastapi_deployment</code> <code class="o">=</code> <code class="n">SentimentAnalysis</code><code class="o">.</code><code class="n">bind</code><code class="p">()</code></pre></div>
<p>The modified deployment should have exactly the same behavior on the example above (try it out using <code>serve run</code>!), but will gracefully handle invalid inputs.
These may look like small benefits for this simple example, but for more complex APIs this can make a world of difference.
We won’t delve deeper into the details of FastAPI here, but for more details on its features and syntax check out their excellent <a href="https://fastapi.tiangolo.com/">documentation</a>.</p>
</div></section>
<section data-pdf-bookmark="Scaling and resource allocation" data-type="sect2"><div class="sect2" id="idm44990018808848">
<h2>Scaling and resource allocation</h2>
<p>As mentioned above, machine learning models are often compute hungry.
Therefore, it’s important to be able to allocate the correct amount of resources to your ML application to handle request load while minimizing cost.
Ray Serve allows you to adjust the resources dedicated to a deployment in two ways: by tuning the number of <strong>replicas</strong> of the deployment and tuning the resources allocated to each replica.
By default, a deployment consists of a single replica that uses a single CPU but these parameters can be adjusted in the <code>@serve.deployment</code> decorator (or using the corresponding <code>deployment.options</code> API).</p>
<p>Let’s modify the <code>SentimentClassifier</code> example from above to scale out to multiple replicas and adjust the resource allocation so that each replica uses 2 CPUs instead of 1 (in practice, you would want to profile and understand your model in order to set this paramter correctly).
We’ll also add a print statement to log the process ID of the process handling each request to show that the requests are now load balanced across two replicas.</p>
<div data-type="example">
<h5><span class="label">Example 8-5. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="n">app</code> <code class="o">=</code> <code class="n">FastAPI</code><code class="p">()</code>


<code class="nd">@serve</code><code class="o">.</code><code class="n">deployment</code><code class="p">(</code><code class="n">num_replicas</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">ray_actor_options</code><code class="o">=</code><code class="p">{</code><code class="s2">"num_cpus"</code><code class="p">:</code> <code class="mi">2</code><code class="p">})</code>
<code class="nd">@serve</code><code class="o">.</code><code class="n">ingress</code><code class="p">(</code><code class="n">app</code><code class="p">)</code>
<code class="k">class</code> <code class="nc">SentimentAnalysis</code><code class="p">:</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">_classifier</code> <code class="o">=</code> <code class="n">pipeline</code><code class="p">(</code><code class="s2">"sentiment-analysis"</code><code class="p">)</code>

    <code class="nd">@app</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s2">"/"</code><code class="p">)</code>
    <code class="k">def</code> <code class="nf">classify</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">input_text</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">str</code><code class="p">:</code>
        <code class="kn">import</code> <code class="nn">os</code>
        <code class="nb">print</code><code class="p">(</code><code class="s2">"from process:"</code><code class="p">,</code> <code class="n">os</code><code class="o">.</code><code class="n">getpid</code><code class="p">())</code>
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">_classifier</code><code class="p">(</code><code class="n">input_text</code><code class="p">)[</code><code class="mi">0</code><code class="p">][</code><code class="s2">"label"</code><code class="p">]</code>


<code class="n">scaled_deployment</code> <code class="o">=</code> <code class="n">SentimentAnalysis</code><code class="o">.</code><code class="n">bind</code><code class="p">()</code></pre></div>
<p>Running this new version of our classifier with <code>serve run app:scaled_deployment</code> and querying it using <code>requests</code> as we did above, you should see that there are now two copies of the model handling requests!
We could easily scale up to tens or hundreds of replicas just by tweaking <code>num_replicas</code> in the same way: Ray enables scaling to hundreds of machines and thousands of processes in a single cluster.</p>
<p>In this example we scaled out to a static number of replicas with each replica consuming two full CPUs, but Serve also supports more expressive resource allocation policies:
- Enabling a deployment to use GPUs simpliy requires setting <code>num_gpus</code> instead of <code>num_cpus</code>. Serve supports the same resource types as Ray core, so deployments can also use TPUs or other custom resources.
- Resources can be <strong>fractional</strong>, allowing replicas to be efficiently bin-packed. For example, if a single replica doesn’t saturate a full GPU you can allocate <code>num_gpus=0.5</code> to it and multiplex with another model.
- For applications with varying request load, a deployment can be configured to dynamically autoscale the number of replicas based on the number of requests currently in flight.</p>
<p>For more details about resource allocation options, refer to the latest Ray Serve documentation.</p>
</div></section>
<section data-pdf-bookmark="Request batching" data-type="sect2"><div class="sect2" id="idm44990018371136">
<h2>Request batching</h2>
<p>Many machine learning models can be efficiently <strong>vectorized</strong>, meaning that multiple computations can be run in parallel more efficiently than running them sequentially.
This is especially beneficial when running models on GPUs which are purpose-built for efficiently performing many computations in parallel.
In the context of online inference this offers a path for optimization: serving multiple requests (possibly from different sources) in parallel can drastically improve the throughput of the system (and therefore save cost).</p>
<p>There are two high-level strategies to take advantage of request batching: <strong>client-side</strong> batching and <strong>server-side</strong> batching.
In client-side batching, the server accepts multiple inputs in a single request and clients include logic to send them in batches instead of one at a time.
This is mostly useful in situations where a single client is frequently sending many inference requests.
Server-side batching, in contrast, enables the server to batch multiple requests without requiring any modification on the client.
This can also be used to batch requests across multiple clients, which enables efficient batching even in situations with many clients that each sends relatively few requests.</p>
<p>Ray Serve offers a built-in utility for server-side batching, the <code>@serve.batch</code> decorator, that requires just a few code changes.
This batching support uses Python’s <code>asyncio</code> capabilities to enqueue multiple requests into a single function call.
The function should take in a list of inputs and return the corresponding list of outputs.</p>
<p>Once again, let’s revisit the sentiment classifier from earlier and this time modify it to perform server-side batching.
The underlying Hugging Face <code>pipeline</code> supports vectorized inference, all we need to do is pass a list of inputs and it will return the corresponding list of outputs.
We’ll split out the call to the classifier into a new method, <code>classify_batched</code>, that will take a list of input texts as input, perform inference across them, and return the outputs in a formatted list.
<code>classify_batched</code> will use the <code>@serve.batch</code> decorator to automatically perform batching.
The behavior can be configured using the <code>max_batch_size</code> and <code>batch_timeout_wait_s</code> parameters, here we’ll set the max batch size to 10 and wait for up to 100ms.</p>
<div data-type="example">
<h5><span class="label">Example 8-6. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">List</code>


<code class="n">app</code> <code class="o">=</code> <code class="n">FastAPI</code><code class="p">()</code>


<code class="nd">@serve</code><code class="o">.</code><code class="n">deployment</code>
<code class="nd">@serve</code><code class="o">.</code><code class="n">ingress</code><code class="p">(</code><code class="n">app</code><code class="p">)</code>
<code class="k">class</code> <code class="nc">SentimentAnalysis</code><code class="p">:</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">_classifier</code> <code class="o">=</code> <code class="n">pipeline</code><code class="p">(</code><code class="s2">"sentiment-analysis"</code><code class="p">)</code>

    <code class="nd">@serve</code><code class="o">.</code><code class="n">batch</code><code class="p">(</code><code class="n">max_batch_size</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">batch_wait_timeout_s</code><code class="o">=</code><code class="mf">0.1</code><code class="p">)</code>
    <code class="k">async</code> <code class="k">def</code> <code class="nf">classify_batched</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">batched_inputs</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="nb">str</code><code class="p">])</code> <code class="o">-&gt;</code> <code class="n">List</code><code class="p">[</code><code class="nb">str</code><code class="p">]:</code>
        <code class="nb">print</code><code class="p">(</code><code class="s2">"Got batch size:"</code><code class="p">,</code> <code class="nb">len</code><code class="p">(</code><code class="n">batched_inputs</code><code class="p">))</code>
        <code class="n">results</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">_classifier</code><code class="p">(</code><code class="n">batched_inputs</code><code class="p">)</code>
        <code class="k">return</code> <code class="p">[</code><code class="n">result</code><code class="p">[</code><code class="s2">"label"</code><code class="p">]</code> <code class="k">for</code> <code class="n">result</code> <code class="ow">in</code> <code class="n">results</code><code class="p">]</code>

    <code class="nd">@app</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s2">"/"</code><code class="p">)</code>
    <code class="k">async</code> <code class="k">def</code> <code class="nf">classify</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">input_text</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">str</code><code class="p">:</code>
        <code class="k">return</code> <code class="k">await</code> <code class="bp">self</code><code class="o">.</code><code class="n">classify_batched</code><code class="p">(</code><code class="n">input_text</code><code class="p">)</code>


<code class="n">batched_deployment</code> <code class="o">=</code> <code class="n">SentimentAnalysis</code><code class="o">.</code><code class="n">bind</code><code class="p">()</code></pre></div>
<p>Notice that both the <code>classify</code> and <code>classify_batched</code> methods now use Python’s <code>async</code> and <code>await</code> syntax, meaning that many of these calls can run concurrently in the same process.
To test this behavior, we’ll use the <code>serve.run</code> Python API to send requests using the Python-native handle to our deployment.</p>
<pre data-type="programlisting">from app import batched_deployment

# Get a handle to the deployment so we can send requests in parallel.
handle = serve.run(batched_deployment)
ray.get([handle.classify.remote("sample text") for _ in range(10)])</pre>
<p>The handle returned by <code>serve.run</code> can be used to send multiple requests in parallel: here, we send 10 requests in parallel and wait for them all to return.
Without batching, each request would be handled sequentially, but because we enabled batching we should see the requests handled all at once (evidenced by batch size printed in the <code>classify_batched</code> method).
Running on a CPU, this might be marginally faster than running sequentially, but running the same handler on a GPU we would observe a significant speedup for the batched version.</p>
</div></section>
<section data-pdf-bookmark="Multi-model inference graphs" data-type="sect2"><div class="sect2" id="idm44990018212896">
<h2>Multi-model inference graphs</h2>
<p>Up until now, we’ve been deploying and querying a single Serve deployment wrapping one ML model.
As described earlier, machine learning models are not often useful in isolation: many applications require multiple models to be composed together and for business logic to be intertwined with machine learning.
The real power of Ray Serve is in its ability to compose multiple models along with regular Python logic into a single application.
This is possible by instantiating many different deployments and passing reference between them.
Each of these deployments can use all of the features we’ve discussed up until this point: they can be independently scaled, perform request batching, and use flexible resource allocations.</p>
<p>This section provides illustrative examples of common multi-model serving patterns but doesn’t actually contain any ML models in order to focus on the core capabilities that Serve provides.
Later in the chapter we will explore an end-to-end multi-model inference graph containing ML models.</p>
<section data-pdf-bookmark="Core feature: binding multiple deployments" data-type="sect3"><div class="sect3" id="idm44990018210784">
<h3>Core feature: binding multiple deployments</h3>
<p>All types of multi-model inference graphs in Ray Serve center around the ability to pass a reference to one deployment into the constructor of another.
In order to do this, we use another feature of the <code>.bind()</code> API: a bound deployment can be passed to another call to <code>.bind()</code> and this will resolve to a “handle” to the deployment at runtime.
This enables deployments to be deployed and instantiated independently and then call each other at runtime.
Below is the most basic example of a multi-deployment Serve application.</p>
<div data-type="example">
<h5><span class="label">Example 8-7. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="nd">@serve</code><code class="o">.</code><code class="n">deployment</code>
<code class="k">class</code> <code class="nc">DownstreamModel</code><code class="p">:</code>
    <code class="k">def</code> <code class="fm">__call__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">inp</code><code class="p">:</code> <code class="nb">str</code><code class="p">):</code>
        <code class="k">return</code> <code class="s2">"Hi from downstream model!"</code>


<code class="nd">@serve</code><code class="o">.</code><code class="n">deployment</code>
<code class="k">class</code> <code class="nc">Driver</code><code class="p">:</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">downstream</code><code class="p">):</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">_d</code> <code class="o">=</code> <code class="n">downstream</code>

    <code class="k">async</code> <code class="k">def</code> <code class="fm">__call__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="o">*</code><code class="n">args</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">str</code><code class="p">:</code>
        <code class="k">return</code> <code class="k">await</code> <code class="bp">self</code><code class="o">.</code><code class="n">_d</code><code class="o">.</code><code class="n">remote</code><code class="p">()</code>


<code class="n">downstream</code> <code class="o">=</code> <code class="n">DownstreamModel</code><code class="o">.</code><code class="n">bind</code><code class="p">()</code>
<code class="n">driver</code> <code class="o">=</code> <code class="n">Driver</code><code class="o">.</code><code class="n">bind</code><code class="p">(</code><code class="n">downstream</code><code class="p">)</code></pre></div>
<p>In this example, the downstream model is passed into the “driver” deployment.
Then at runtime the driver deployment calls into the downstream model.
The driver could take any number of models passed in, and the downstream model could even take other downstream models of its own.</p>
</div></section>
<section data-pdf-bookmark="Pattern 1: Pipelining" data-type="sect3"><div class="sect3" id="idm44990018144912">
<h3>Pattern 1: Pipelining</h3>
<p>The first common multi-model pattern among machine learning applications is “pipelining:” calling multiple models in sequence, where the input of one model depends on the output of the previous.
Image processing, for example, often consists of a pipeline consistenting of multiple stages of transformations such as cropping, segmentation, and object recognition or optical character recognition (OCR).
Each of these models may have different properties with some of them being lightweight transformations that can run on a CPU and others being heavyweight deep learning models that run on a CPU.</p>
<p>Such pipelines can easily be expressed using Serve’s API.
Each stage of the pipeline is defined as an independent deployment and each deployment is passed into a top-level “pipeline driver.”
In the example below, we pass two deployments into a top-level driver and the driver calls them in sequence.
Note that there could be many requests to the driver happening concurrently, therefore it is possible to efficiently saturate all stages of the pipeline.</p>
<div data-type="example">
<h5><span class="label">Example 8-8. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="nd">@serve</code><code class="o">.</code><code class="n">deployment</code>
<code class="k">class</code> <code class="nc">DownstreamModel</code><code class="p">:</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">my_val</code><code class="p">:</code> <code class="nb">str</code><code class="p">):</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">_my_val</code> <code class="o">=</code> <code class="n">my_val</code>

    <code class="k">def</code> <code class="fm">__call__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">inp</code><code class="p">:</code> <code class="nb">str</code><code class="p">):</code>
        <code class="k">return</code> <code class="n">inp</code> <code class="o">+</code> <code class="s2">"|"</code> <code class="o">+</code> <code class="bp">self</code><code class="o">.</code><code class="n">_my_val</code>


<code class="nd">@serve</code><code class="o">.</code><code class="n">deployment</code>
<code class="k">class</code> <code class="nc">PipelineDriver</code><code class="p">:</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">model1</code><code class="p">,</code> <code class="n">model2</code><code class="p">):</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">_m1</code> <code class="o">=</code> <code class="n">model1</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">_m2</code> <code class="o">=</code> <code class="n">model2</code>

    <code class="k">async</code> <code class="k">def</code> <code class="fm">__call__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="o">*</code><code class="n">args</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">str</code><code class="p">:</code>
        <code class="n">intermediate</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">_m1</code><code class="o">.</code><code class="n">remote</code><code class="p">(</code><code class="s2">"input"</code><code class="p">)</code>
        <code class="n">final</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">_m2</code><code class="o">.</code><code class="n">remote</code><code class="p">(</code><code class="n">intermediate</code><code class="p">)</code>
        <code class="k">return</code> <code class="k">await</code> <code class="n">final</code>


<code class="n">m1</code> <code class="o">=</code> <code class="n">DownstreamModel</code><code class="o">.</code><code class="n">bind</code><code class="p">(</code><code class="s2">"val1"</code><code class="p">)</code>
<code class="n">m2</code> <code class="o">=</code> <code class="n">DownstreamModel</code><code class="o">.</code><code class="n">bind</code><code class="p">(</code><code class="s2">"val2"</code><code class="p">)</code>
<code class="n">pipeline_driver</code> <code class="o">=</code> <code class="n">PipelineDriver</code><code class="o">.</code><code class="n">bind</code><code class="p">(</code><code class="n">m1</code><code class="p">,</code> <code class="n">m2</code><code class="p">)</code></pre></div>
<p>To test this example, you can once again use the <code>serve run</code> API.
Sending a test request to the pipeline returns <code>"'input|val1|val2'"</code> as output: each downstream “model” appended its own value to construct the final result.
In practice, each of these deployments could be wrapping its own ML model and a single request may flow across many physical nodes in a cluster.</p>
</div></section>
<section data-pdf-bookmark="Pattern 2: Broadcasting" data-type="sect3"><div class="sect3" id="idm44990017970880">
<h3>Pattern 2: Broadcasting</h3>
<p>In addition to sequentially chaining models together, it’s often useful to perform inference on multiple models in parallel.
This could be to perform “ensembling,” or combining the results of multiple independent models into a single result, or in a situation where different models may perform better on different inputs.
Often the results of the models need to be combined in some way into a final result: either simply concatenated together or maybe a single result chosen from the lot.</p>
<p>This is expressed very similarly to the pipelining example: a number of downstream models are passed into a top-level “driver.”
In this case, it’s important that we call the models in parallel: waiting for the result of each before calling the next would dramatically increase the overall latency of the system.</p>
<div data-type="example">
<h5><span class="label">Example 8-9. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="nd">@serve</code><code class="o">.</code><code class="n">deployment</code>
<code class="k">class</code> <code class="nc">DownstreamModel</code><code class="p">:</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">my_val</code><code class="p">:</code> <code class="nb">str</code><code class="p">):</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">_my_val</code> <code class="o">=</code> <code class="n">my_val</code>

    <code class="k">def</code> <code class="fm">__call__</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">_my_val</code>


<code class="nd">@serve</code><code class="o">.</code><code class="n">deployment</code>
<code class="k">class</code> <code class="nc">BroadcastDriver</code><code class="p">:</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">model1</code><code class="p">,</code> <code class="n">model2</code><code class="p">):</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">_m1</code> <code class="o">=</code> <code class="n">model1</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">_m2</code> <code class="o">=</code> <code class="n">model2</code>

    <code class="k">async</code> <code class="k">def</code> <code class="fm">__call__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="o">*</code><code class="n">args</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">str</code><code class="p">:</code>
        <code class="n">output1</code><code class="p">,</code> <code class="n">output2</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">_m1</code><code class="o">.</code><code class="n">remote</code><code class="p">(),</code> <code class="bp">self</code><code class="o">.</code><code class="n">_m2</code><code class="o">.</code><code class="n">remote</code><code class="p">()</code>
        <code class="k">return</code> <code class="p">[</code><code class="k">await</code> <code class="n">output1</code><code class="p">,</code> <code class="k">await</code> <code class="n">output2</code><code class="p">]</code>


<code class="n">m1</code> <code class="o">=</code> <code class="n">DownstreamModel</code><code class="o">.</code><code class="n">bind</code><code class="p">(</code><code class="s2">"val1"</code><code class="p">)</code>
<code class="n">m2</code> <code class="o">=</code> <code class="n">DownstreamModel</code><code class="o">.</code><code class="n">bind</code><code class="p">(</code><code class="s2">"val2"</code><code class="p">)</code>
<code class="n">broadcast_driver</code> <code class="o">=</code> <code class="n">BroadcastDriver</code><code class="o">.</code><code class="n">bind</code><code class="p">(</code><code class="n">m1</code><code class="p">,</code> <code class="n">m2</code><code class="p">)</code></pre></div>
<p>Testing this endpoint after running it once again with <code>serve run</code> returns <code>'["val1", "val2"]'</code>, the combined output of the two models called in parallel.</p>
</div></section>
<section data-pdf-bookmark="Pattern 3: Conditional logic" data-type="sect3"><div class="sect3" id="idm44990017875264">
<h3>Pattern 3: Conditional logic</h3>
<p>Finally, while many machine learning applications fit roughly into one of the above patterns, often having static control flow can be very limiting.
Take, for instance, the example of building a service to extract license plate numbers from user-uploaded images.
In this case, we’ll likely need to build an image processing pipeline as discussed above, but we also don’t simply want to feed any image into the pipeline blindly.
If the user uploads something other than a car or with an image that is low quality, we likely want to short circuit, avoid calling into the heavy-weight and expensive pipeline, and provide a useful error message.
Similarly, in a product recommendation use case we may want to select a downstream model based on user input or the result of an intermediate model.
Each of these examples requires embedding custom logic alongside our ML models.</p>
<p>We can accomplish this trivially using Serve’s multi-model API because our computation graph is defined as ordinary Python logic rather than as a statically-defined graph.
For instance, in the example below, we use a simple random number generator (RNG) to decide which of two downstream models to call into.
In a real-world example, the RNG could be replaced with business logic, a database query, or the result of an intermediate model.</p>
<div data-type="example">
<h5><span class="label">Example 8-10. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="nd">@serve</code><code class="o">.</code><code class="n">deployment</code>
<code class="k">class</code> <code class="nc">DownstreamModel</code><code class="p">:</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">my_val</code><code class="p">:</code> <code class="nb">str</code><code class="p">):</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">_my_val</code> <code class="o">=</code> <code class="n">my_val</code>

    <code class="k">def</code> <code class="fm">__call__</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">_my_val</code>


<code class="nd">@serve</code><code class="o">.</code><code class="n">deployment</code>
<code class="k">class</code> <code class="nc">ConditionalDriver</code><code class="p">:</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">model1</code><code class="p">,</code> <code class="n">model2</code><code class="p">):</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">_m1</code> <code class="o">=</code> <code class="n">model1</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">_m2</code> <code class="o">=</code> <code class="n">model2</code>

    <code class="k">async</code> <code class="k">def</code> <code class="fm">__call__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="o">*</code><code class="n">args</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">str</code><code class="p">:</code>
        <code class="kn">import</code> <code class="nn">random</code>
        <code class="k">if</code> <code class="n">random</code><code class="o">.</code><code class="n">random</code><code class="p">()</code> <code class="o">&gt;</code> <code class="mf">0.5</code><code class="p">:</code>
            <code class="k">return</code> <code class="k">await</code> <code class="bp">self</code><code class="o">.</code><code class="n">_m1</code><code class="o">.</code><code class="n">remote</code><code class="p">()</code>
        <code class="k">else</code><code class="p">:</code>
            <code class="k">return</code> <code class="k">await</code> <code class="bp">self</code><code class="o">.</code><code class="n">_m2</code><code class="o">.</code><code class="n">remote</code><code class="p">()</code>


<code class="n">m1</code> <code class="o">=</code> <code class="n">DownstreamModel</code><code class="o">.</code><code class="n">bind</code><code class="p">(</code><code class="s2">"val1"</code><code class="p">)</code>
<code class="n">m2</code> <code class="o">=</code> <code class="n">DownstreamModel</code><code class="o">.</code><code class="n">bind</code><code class="p">(</code><code class="s2">"val2"</code><code class="p">)</code>
<code class="n">conditional_driver</code> <code class="o">=</code> <code class="n">ConditionalDriver</code><code class="o">.</code><code class="n">bind</code><code class="p">(</code><code class="n">m1</code><code class="p">,</code> <code class="n">m2</code><code class="p">)</code></pre></div>
<p>Each call to this endpoint returns either <code>"val1"</code> or <code>"val2"</code> with 50/50 probability.</p>
</div></section>
</div></section>
<section data-pdf-bookmark="Deploying on Kubernetes" data-type="sect2"><div class="sect2" id="idm44990017659424">
<h2>Deploying on Kubernetes</h2>
<p>TODO: the Kubernetes deployment story for Ray Serve is currently being reworked, will update this section once it’s finalized.</p>
</div></section>
</div></section>
<section data-pdf-bookmark="End-to-end Example: Building an NLP-powered API" data-type="sect1"><div class="sect1" id="idm44990018816352">
<h1>End-to-end Example: Building an NLP-powered API</h1>
<p>In this section, we’ll use Ray Serve to build an end-to-end natural language processing (NLP) pipeline hosted for online inference.
Our goal will be to provide a Wikipedia summarization endpoint that will leverage multiple NLP models and some custom logic to provide a succinct summary of the most relevant Wikipedia page for a given search term.</p>
<p>This task will bring together many of the concepts and features discussed above:
- We’ll be combining custom business logic along with multiple machine learning models.
- The inference graph will consist of all three multi-model patterns discussed above: pipelining, broadcasting, and conditional logic.
- Each model will be hosted as a separate Serve deployment, so they can be independently scaled and given their own resource allocation.
- One of the models will leverage vectorized computation via batching.
- The API will be defined using Ray Serve’s <code>FastAPI</code> for input parsing and defining our output schema.</p>
<p>Our online inference pipeline will be structured as follows:
- The user will provide a keyword search term.
- We’ll fetch the content for the most relevant Wikipedia article for the search term.
- A sentiment analysis model will be applied to the article. Anything with a “negative” sentiment will be rejected and we’ll return early.
- The article content will be broadcast to summarizer and named entity recognition models.
- We’ll return a composed result based on the summarizer and named entity recognition outputs.</p>
<p>This pipeline will be exposed over HTTP and return the results in a structured format.
By the end of this section, we’ll have the pipeline running end-to-end locally and ready to scale up on a cluster.
Let’s get started!</p>
<section data-pdf-bookmark="Step 0: Install dependencies" data-type="sect2"><div class="sect2" id="idm44990017522528">
<h2>Step 0: Install dependencies</h2>
<p>Before we dive into the code, you’ll need the following Python packages installed locally in order to follow along:.</p>
<pre data-type="programlisting">pip install ray["serve"]==1.12.0 transformers requests wikipedia</pre>
<p>Additionally, in this section we’ll assume that all of the code samples are available locally in a file called <code>app.py</code> so we can run the deployments using <code>serve run</code> from the same directory.</p>
</div></section>
<section data-pdf-bookmark="Step 1: Fetching content &amp; preprocessing" data-type="sect2"><div class="sect2" id="idm44990017519072">
<h2>Step 1: Fetching content &amp; preprocessing</h2>
<p>The first step is to fetch the most relevant page from Wikipedia given a user-provided search term.
For this, we will leverage the <code>wikipedia</code> package on PyPI to do the heavy lifting.
We’ll first search for the term, then select the top result and return its page content.
If no results are found, we’ll return <code>None</code> — this edge case will be handled later when we define the API.</p>
<div data-type="example">
<h5><span class="label">Example 8-11. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">Optional</code>

<code class="kn">import</code> <code class="nn">wikipedia</code>


<code class="k">def</code> <code class="nf">fetch_wikipedia_page</code><code class="p">(</code><code class="n">search_term</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Optional</code><code class="p">[</code><code class="nb">str</code><code class="p">]:</code>
    <code class="n">results</code> <code class="o">=</code> <code class="n">wikipedia</code><code class="o">.</code><code class="n">search</code><code class="p">(</code><code class="n">search_term</code><code class="p">)</code>
    <code class="c1"># If no results, return to caller.</code>
    <code class="k">if</code> <code class="nb">len</code><code class="p">(</code><code class="n">results</code><code class="p">)</code> <code class="o">==</code> <code class="mi">0</code><code class="p">:</code>
        <code class="k">return</code> <code class="kc">None</code>

    <code class="c1"># Get the page for the top result.</code>
    <code class="k">return</code> <code class="n">wikipedia</code><code class="o">.</code><code class="n">page</code><code class="p">(</code><code class="n">results</code><code class="p">[</code><code class="mi">0</code><code class="p">])</code><code class="o">.</code><code class="n">content</code></pre></div>
</div></section>
<section data-pdf-bookmark="Step 2: NLP models" data-type="sect2"><div class="sect2" id="idm44990017492656">
<h2>Step 2: NLP models</h2>
<p>Next, we need to define the ML models that will do the heavy lifting of our API.
As we did in the introduction section, we’ll be using the <a href="https://huggingface.co/">Hugging Face</a> <code>transformers</code> library as it provides convenient APIs to pretrained state-of-the-art ML models, so we can focus on the serving logic.</p>
<p>The first model we’ll use is a sentiment classifier, the same one we used in the examples above.
The deployment for this model will take advantage of vectorized computations using Serve’s batching API.</p>
<div data-type="example">
<h5><span class="label">Example 8-12. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">pipeline</code>


<code class="nd">@serve</code><code class="o">.</code><code class="n">deployment</code>
<code class="k">class</code> <code class="nc">SentimentAnalysis</code><code class="p">:</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">_classifier</code> <code class="o">=</code> <code class="n">pipeline</code><code class="p">(</code><code class="s2">"sentiment-analysis"</code><code class="p">)</code>

    <code class="nd">@serve</code><code class="o">.</code><code class="n">batch</code><code class="p">(</code><code class="n">max_batch_size</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">batch_wait_timeout_s</code><code class="o">=</code><code class="mf">0.1</code><code class="p">)</code>
    <code class="k">async</code> <code class="k">def</code> <code class="nf">is_positive_batched</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">inputs</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="nb">str</code><code class="p">])</code> <code class="o">-&gt;</code> <code class="n">List</code><code class="p">[</code><code class="nb">bool</code><code class="p">]:</code>
        <code class="n">results</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">_classifier</code><code class="p">(</code><code class="n">inputs</code><code class="p">,</code> <code class="n">truncation</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
        <code class="k">return</code> <code class="p">[</code><code class="n">result</code><code class="p">[</code><code class="s2">"label"</code><code class="p">]</code> <code class="o">==</code> <code class="s2">"POSITIVE"</code> <code class="k">for</code> <code class="n">result</code> <code class="ow">in</code> <code class="n">results</code><code class="p">]</code>

    <code class="k">async</code> <code class="k">def</code> <code class="fm">__call__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">input_text</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">bool</code><code class="p">:</code>
        <code class="k">return</code> <code class="k">await</code> <code class="bp">self</code><code class="o">.</code><code class="n">is_positive_batched</code><code class="p">(</code><code class="n">input_text</code><code class="p">)</code></pre></div>
<p>We’ll also use a text summarization model to provide a succinct summary for the selected article.
This model takes an optional “max_length” argument to cap the length of the summary.
Because we know this is the most computationally expensive of the models, we set <code>num_replicas=2</code> — that way, if we have many requests coming in at the same time, it can keep up with the throughput of the other models.
In practice, we may need more replicas to keep up with the input load, but we could only know that from profiling and monitoring.</p>
<div data-type="example">
<h5><span class="label">Example 8-13. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="nd">@serve</code><code class="o">.</code><code class="n">deployment</code><code class="p">(</code><code class="n">num_replicas</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>
<code class="k">class</code> <code class="nc">Summarizer</code><code class="p">:</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">max_length</code><code class="p">:</code> <code class="n">Optional</code><code class="p">[</code><code class="nb">int</code><code class="p">]</code> <code class="o">=</code> <code class="kc">None</code><code class="p">):</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">_summarizer</code> <code class="o">=</code> <code class="n">pipeline</code><code class="p">(</code><code class="s2">"summarization"</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">_max_length</code> <code class="o">=</code> <code class="n">max_length</code>

    <code class="k">def</code> <code class="fm">__call__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">input_text</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">str</code><code class="p">:</code>
        <code class="n">result</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">_summarizer</code><code class="p">(</code>
            <code class="n">input_text</code><code class="p">,</code> <code class="n">max_length</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">_max_length</code><code class="p">,</code> <code class="n">truncation</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">result</code><code class="p">[</code><code class="mi">0</code><code class="p">][</code><code class="s2">"summary_text"</code><code class="p">]</code></pre></div>
<p>The final model in our pipeline will be a named entity recognition model: this will attempt extract named entities from the text.
Each result will have a confidence score, so we can set a threshold to only accept results above a certain threshold.
We may also want to cap the total number of entities returned.
The request handler for this deployment calls the model, then uses some basic business logic to enforce the provided confidence threshold and limit on the number of entities.</p>
<div data-type="example">
<h5><span class="label">Example 8-14. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="nd">@serve</code><code class="o">.</code><code class="n">deployment</code>
<code class="k">class</code> <code class="nc">EntityRecognition</code><code class="p">:</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">threshold</code><code class="p">:</code> <code class="nb">float</code> <code class="o">=</code> <code class="mf">0.90</code><code class="p">,</code> <code class="n">max_entities</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">10</code><code class="p">):</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">_entity_recognition</code> <code class="o">=</code> <code class="n">pipeline</code><code class="p">(</code><code class="s2">"ner"</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">_threshold</code> <code class="o">=</code> <code class="n">threshold</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">_max_entities</code> <code class="o">=</code> <code class="n">max_entities</code>

    <code class="k">def</code> <code class="fm">__call__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">input_text</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">List</code><code class="p">[</code><code class="nb">str</code><code class="p">]:</code>
        <code class="n">final_results</code> <code class="o">=</code> <code class="p">[]</code>
        <code class="k">for</code> <code class="n">result</code> <code class="ow">in</code> <code class="bp">self</code><code class="o">.</code><code class="n">_entity_recognition</code><code class="p">(</code><code class="n">input_text</code><code class="p">):</code>
            <code class="k">if</code> <code class="n">result</code><code class="p">[</code><code class="s2">"score"</code><code class="p">]</code> <code class="o">&gt;</code> <code class="bp">self</code><code class="o">.</code><code class="n">_threshold</code><code class="p">:</code>
                <code class="n">final_results</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">result</code><code class="p">[</code><code class="s2">"word"</code><code class="p">])</code>
            <code class="k">if</code> <code class="nb">len</code><code class="p">(</code><code class="n">final_results</code><code class="p">)</code> <code class="o">==</code> <code class="bp">self</code><code class="o">.</code><code class="n">_max_entities</code><code class="p">:</code>
                <code class="k">break</code>

        <code class="k">return</code> <code class="n">final_results</code></pre></div>
</div></section>
<section data-pdf-bookmark="Step 3: HTTP handling and driver logic" data-type="sect2"><div class="sect2" id="idm44990017132720">
<h2>Step 3: HTTP handling and driver logic</h2>
<p>With the input preprocessing and ML models defined, we’re ready to define the HTTP API and driver logic.
First, we define the schema of the response that we’ll return from the API using <a href="https://pydantic-docs.helpmanual.io/">Pydantic</a>.
The response includes whether or not the request was successful and a status message in addition to our summary and named entities.
This will allow us to return a helpful response in error conditions such as when no result is found or the sentiment analysis comes back as negative.</p>
<div data-type="example">
<h5><span class="label">Example 8-15. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">pydantic</code> <code class="kn">import</code> <code class="n">BaseModel</code>


<code class="k">class</code> <code class="nc">Response</code><code class="p">(</code><code class="n">BaseModel</code><code class="p">):</code>
    <code class="n">success</code><code class="p">:</code> <code class="nb">bool</code>
    <code class="n">message</code><code class="p">:</code> <code class="nb">str</code> <code class="o">=</code> <code class="s2">""</code>
    <code class="n">summary</code><code class="p">:</code> <code class="nb">str</code> <code class="o">=</code> <code class="s2">""</code>
    <code class="n">named_entities</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="nb">str</code><code class="p">]</code> <code class="o">=</code> <code class="p">[]</code></pre></div>
<p>Next, we need to define the actual control flow logic that will run in the driver deployment.
The driver will not do any of the actual heavy lifting itself, but instead call into our three downstream model deployments and interpret their results.
It will also house the FastAPI app definition, parsing the input and returning the correct <code>Response</code> model based on the results of the pipeline.</p>
<div data-type="example">
<h5><span class="label">Example 8-16. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">fastapi</code> <code class="kn">import</code> <code class="n">FastAPI</code>

<code class="n">app</code> <code class="o">=</code> <code class="n">FastAPI</code><code class="p">()</code>


<code class="nd">@serve</code><code class="o">.</code><code class="n">deployment</code>
<code class="nd">@serve</code><code class="o">.</code><code class="n">ingress</code><code class="p">(</code><code class="n">app</code><code class="p">)</code>
<code class="k">class</code> <code class="nc">NLPPipelineDriver</code><code class="p">:</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">sentiment_analysis</code><code class="p">,</code> <code class="n">summarizer</code><code class="p">,</code> <code class="n">entity_recognition</code><code class="p">):</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">_sentiment_analysis</code> <code class="o">=</code> <code class="n">sentiment_analysis</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">_summarizer</code> <code class="o">=</code> <code class="n">summarizer</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">_entity_recognition</code> <code class="o">=</code> <code class="n">entity_recognition</code>

    <code class="nd">@app</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s2">"/"</code><code class="p">,</code> <code class="n">response_model</code><code class="o">=</code><code class="n">Response</code><code class="p">)</code>
    <code class="k">async</code> <code class="k">def</code> <code class="nf">summarize_article</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">search_term</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Response</code><code class="p">:</code>
        <code class="c1"># Fetch the top page content for the search term if found.</code>
        <code class="n">page_content</code> <code class="o">=</code> <code class="n">fetch_wikipedia_page</code><code class="p">(</code><code class="n">search_term</code><code class="p">)</code>
        <code class="k">if</code> <code class="n">page_content</code> <code class="ow">is</code> <code class="kc">None</code><code class="p">:</code>
            <code class="k">return</code> <code class="n">Response</code><code class="p">(</code><code class="n">success</code><code class="o">=</code><code class="kc">False</code><code class="p">,</code> <code class="n">message</code><code class="o">=</code><code class="s2">"No pages found."</code><code class="p">)</code>

        <code class="c1"># Conditionally continue based on the sentiment analysis.</code>
        <code class="n">is_positive</code> <code class="o">=</code> <code class="k">await</code> <code class="bp">self</code><code class="o">.</code><code class="n">_sentiment_analysis</code><code class="o">.</code><code class="n">remote</code><code class="p">(</code><code class="n">page_content</code><code class="p">)</code>
        <code class="k">if</code> <code class="ow">not</code> <code class="n">is_positive</code><code class="p">:</code>
            <code class="k">return</code> <code class="n">Response</code><code class="p">(</code><code class="n">success</code><code class="o">=</code><code class="kc">False</code><code class="p">,</code> <code class="n">message</code><code class="o">=</code><code class="s2">"Only positivitiy allowed!"</code><code class="p">)</code>

        <code class="c1"># Query the summarizer and named entity recognition models in parallel.</code>
        <code class="n">summary_result</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">_summarizer</code><code class="o">.</code><code class="n">remote</code><code class="p">(</code><code class="n">page_content</code><code class="p">)</code>
        <code class="n">entities_result</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">_entity_recognition</code><code class="o">.</code><code class="n">remote</code><code class="p">(</code><code class="n">page_content</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">Response</code><code class="p">(</code>
            <code class="n">success</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
            <code class="n">summary</code><code class="o">=</code><code class="k">await</code> <code class="n">summary_result</code><code class="p">,</code>
            <code class="n">named_entities</code><code class="o">=</code><code class="k">await</code> <code class="n">entities_result</code>
        <code class="p">)</code></pre></div>
<p>In the body of the main handler, we first fetch the page content using our <code>fetch_wikipedia_page</code> logic (if no result is found, an error is returned).
Then, we call into the sentiment analysis model.
If this returns negative, we terminate early and return an error to avoid calling the other expensive ML models..
Finally, we broadcast the article contents to both the summary and named entity recognition models in parallel.
The results of the two models are stitched together into the final response and we return success.
Remember that we may have many calls to this handler running concurrently: the calls to the downstream models don’t block the driver and it could coordinate calls to many replicas of the heavyweight models.</p>
</div></section>
<section data-pdf-bookmark="Step 4: Putting it all together" data-type="sect2"><div class="sect2" id="idm44990016944960">
<h2>Step 4: Putting it all together</h2>
<p>At this point, all of the core logic is defined.
All that’s left is to bind the graph of deployments together and run it!</p>
<div data-type="example">
<h5><span class="label">Example 8-17. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="n">sentiment_analysis</code> <code class="o">=</code> <code class="n">SentimentAnalysis</code><code class="o">.</code><code class="n">bind</code><code class="p">()</code>
<code class="n">summarizer</code> <code class="o">=</code> <code class="n">Summarizer</code><code class="o">.</code><code class="n">bind</code><code class="p">()</code>
<code class="n">entity_recognition</code> <code class="o">=</code> <code class="n">EntityRecognition</code><code class="o">.</code><code class="n">bind</code><code class="p">(</code><code class="n">threshold</code><code class="o">=</code><code class="mf">0.95</code><code class="p">,</code> <code class="n">max_entities</code><code class="o">=</code><code class="mi">5</code><code class="p">)</code>
<code class="n">nlp_pipeline_driver</code> <code class="o">=</code> <code class="n">NLPPipelineDriver</code><code class="o">.</code><code class="n">bind</code><code class="p">(</code>
    <code class="n">sentiment_analysis</code><code class="p">,</code> <code class="n">summarizer</code><code class="p">,</code> <code class="n">entity_recognition</code><code class="p">)</code>
<code class="c1"># end::final_driver[]</code></pre></div>
<p>First, we need to instantiate each of the deployments with any relevant input arguments.
For example, here we pass a threshold and limit for the entity recognition model.
The most important piece is we pass a reference to each of the three models into the driver so it can coordinate the computation.
Now that we’ve defined the full NLP pipeline, we can run it using <code>serve run</code>:</p>
<pre data-type="programlisting">serve run ch_08_model_serving:nlp_pipeline_driver</pre>
<p>This will deploy each of the four deployments locally and make the driver available at <code>http://localhost:8000/</code>.
We can query the pipeline using the <code>requests</code> to see it in action.
First, let’s try querying for an entry on Ray Serve.</p>
<div data-type="example">
<h5><span class="label">Example 8-18. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="nb">print</code><code class="p">(</code><code class="n">requests</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s2">"http://localhost:8000/"</code><code class="p">,</code> <code class="n">params</code><code class="o">=</code><code class="p">{</code><code class="s2">"search_term"</code><code class="p">:</code> <code class="s2">"rayserve"</code><code class="p">})</code><code class="o">.</code><code class="n">text</code><code class="p">)</code>
<code class="s1">'{"success":false,"message":"No pages found.","summary":"","named_entities":[]}'</code></pre></div>
<p>Unfortunately, this page doesn’t exist yet!
The first chunk of validation business logic kicks in and returns a “No pages found” message.
Let’s try finding looking for something more common:</p>
<div data-type="example">
<h5><span class="label">Example 8-19. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="nb">print</code><code class="p">(</code><code class="n">requests</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s2">"http://localhost:8000/"</code><code class="p">,</code> <code class="n">params</code><code class="o">=</code><code class="p">{</code><code class="s2">"search_term"</code><code class="p">:</code> <code class="s2">"war"</code><code class="p">})</code><code class="o">.</code><code class="n">text</code><code class="p">)</code>
<code class="s1">'{"success":false,"message":"Only positivitiy allowed!","summary":"","named_entities":[]}'</code></pre></div>
<p>Maybe we were just interested in learning about history, but this article was a bit too negative for our sentiment classifier.
Let’s try something more neutral this time — what about science?</p>
<div data-type="example">
<h5><span class="label">Example 8-20. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="nb">print</code><code class="p">(</code><code class="n">requests</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s2">"http://localhost:8000/"</code><code class="p">,</code> <code class="n">params</code><code class="o">=</code><code class="p">{</code><code class="s2">"search_term"</code><code class="p">:</code> <code class="s2">"physicist"</code><code class="p">})</code><code class="o">.</code><code class="n">text</code><code class="p">)</code>
<code class="s1">'{"success":true,"message":"","summary":" Physics is the natural science that studies matter, its fundamental constituents, its motion and behavior through space and time, and the related entities of energy and force . During the Scientific Revolution in the 17th century these natural sciences emerged as unique research endeavors in their own right . Physics intersects with many interdisciplinary areas of research, such as biophysics and quantum chemistry .","named_entities":["Scientific","Revolution","Ancient","Greek","Egyptians"]}'</code></pre></div>
<p>This example successfully ran through the full pipeline: the API responded with a cogent summary of the article and a list of relevant named entities.</p>
<p>To recap, in this section we were able to build an online natural language processing API using Ray Serve.
This inference graph consisted of multiple machine learning models in addition to custom business logic and dynamic control flow.
Each model can be independently scaled and have its own resource allocation, and we can exploit vectorized computations using server-side batching.
Now that we were able to test the API locally, the next step would be to deploy to production.
Ray Serve makes it easy to deploy on Kubernetes or other cloud provider offerings using the Ray cluster launcher, and we could easily scale up to handle many users by tweaking the resource allocations for our deployments.</p>
</div></section>
</div></section>
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm44990017525296">
<h1>Summary</h1>
</div></section>
</div></section></div>

<div id="sbo-rt-content"><section class="abouttheauthor" data-pdf-bookmark="About the Author" data-type="colophon" epub:type="colophon"><div class="colophon" id="idm44990016521152">
<h1>About the Author</h1>
<p><strong>Max Pumperla</strong> is a data science professor and software engineer located in Hamburg, Germany. He’s an active open source contributor, maintainer of several Python packages, author of machine learning books and speaker at international conferences. As head of product research at Pathmind Inc. he’s developing reinforcement learning solutions for industrial applications at scale using Ray. Pathmind works closely with the AnyScale team and is a power user of Ray’s RLlib, Tune and Serve libraries. Max has been a core developer of DL4J at Skymind, helped grow and extend the Keras ecosystem, and is a Hyperopt maintainer.</p>
<p><strong>Edward Oakes</strong></p>
<p><strong>Richard Liaw</strong></p>
</div></section></div></body></html>