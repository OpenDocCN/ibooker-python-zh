- en: Chapter 2\. How to Work with Dask DataFrames
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章。如何使用 Dask DataFrames
- en: In the previous chapter, we explained the architecture of Dask DataFrames and
    how they’re built on pandas DataFrames. We saw how pandas cannot scale to larger-than-memory
    datasets and how Dask overcomes this scaling limitation. Now it’s time to dive
    into the specific tactics we’ll need to master when working with Dask DataFrames.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们解释了 Dask DataFrames 的架构以及它们是如何构建在 pandas DataFrames 上的。我们看到 pandas 无法扩展到大于内存的数据集，以及
    Dask 如何克服这种扩展限制。现在是时候深入研究我们在处理 Dask DataFrames 时需要掌握的具体策略了。
- en: In this chapter, we will apply the lessons we’ve learned to process large tabular
    datasets with Dask. We will walk through hands-on code examples to read, inspect,
    process, and write large datasets using Dask DataFrames. By working through these
    examples we will learn the Dask DataFrame API in depth and build understanding
    that will enable us to implement best practices when using Dask in real-world
    projects. It’s a sort of ‘behind the scenes’ deep-dive into the end-to-end example
    Dask DataFrame we worked through in Chapter 2.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将应用我们学到的经验来处理使用 Dask 处理大型表格数据集。我们将通过实际的代码示例来读取、检查、处理和写入大型数据集，使用 Dask
    DataFrames。通过解决这些示例，我们将深入了解 Dask DataFrame API，并建立起在实际项目中使用 Dask 时实施最佳实践的理解。这是对我们在第二章中处理的端到端示例
    Dask DataFrame 的一种‘幕后’深入挖掘。
- en: An important lesson to carry with us from the previous chapter is that pandas
    only uses one core to process computations while Dask can speed up query execution
    time with parallel processing on multiple cores. Using Dask therefore means entering
    the world of *parallel computing*.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 从上一章中带给我们的一个重要教训是，pandas 只使用一个核心来处理计算，而 Dask 可以通过多个核心上的并行处理加快查询执行时间。使用 Dask
    因此意味着进入*并行计算*的世界。
- en: This means that even though much of the Dask DataFrame API syntax may appear
    familiar to pandas users, there are important underlying architectural differences
    to be aware of. The previous chapter explains those differences at a conceptual
    level. This chapter proceeds from there to dive into the hands-on application
    of the Dask API. It will explain how to leverage the power of parallel computing
    in practice.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，即使 Dask DataFrame API 的许多语法对于 pandas 用户来说可能看起来很熟悉，但需要注意重要的底层架构差异。前一章对这些差异进行了概念层面的解释。本章从那里开始，深入探讨了
    Dask API 的实际应用。它将解释如何实践利用并行计算的威力。
- en: 'As we proceed through this chapter, remember that:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们在本章的进行，记住：
- en: Dask DataFrames consist of **multiple partitions**, each of which is a pandas
    DataFrame.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dask DataFrames由**多个分区**组成，每个分区都是一个 pandas DataFrame。
- en: These partitions are **processed in parallel** by multiple cores at once.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些分区由多个核心**并行处理**。
- en: Dask uses **lazy evaluation** to optimize large-scale computations for parallel
    computation.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dask 使用**惰性求值**来优化大规模计算以进行并行计算。
- en: These fundamental concepts will be important to understand how to optimally
    leverage the Dask DataFrame API functions.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这些基本概念将是理解如何最优地利用 Dask DataFrame API 函数的重要基础。
- en: Reading Data into a Dask DataFrame
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将数据读入 Dask DataFrame
- en: 'Imagine that you’re working for a company that processes large-scale time series
    datasets for its customers. You are working on the Data Analysis team and have
    just received a copy of the file `0000.csv` from your colleague Azeem with the
    request to analyze patterns in the data. This sample file contains 1 week worth
    of data and is only 90 MB large, which means you can use pandas to analyze this
    subset just fine:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，您正在为一家为其客户处理大规模时间序列数据集的公司工作。您正在 Data Analysis 团队工作，刚刚从同事 Azeem 那里收到了文件
    `0000.csv` 的副本，并要求分析其中的数据模式。这个示例文件包含 1 周的数据，仅90 MB 大，这意味着您可以使用 pandas 来分析这个子集：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Once the file has been loaded into a DataFrame, you can use pandas to analyze
    the data it contains. For example, use the head() method to see the first few
    rows of data in the DataFrame:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦文件加载到 DataFrame 中，您就可以使用 pandas 分析其中包含的数据。例如，使用 head() 方法查看 DataFrame 中的前几行数据：
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Or calculate the number of records with positive values for a certain column:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 或计算某一列中具有正值记录的数量：
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now that you’ve confirmed you can run analyses on the data, Azeem asks you to
    crunch the data for the first 6 months of the year. He has downloaded the data
    from the server as a single CSV which is about 2.4GB in size. You may still be
    able to process this with pandas, but it’s pushing your machine to its limits
    and your analyses are running slow. This is a problem because Azeem has a deadline
    coming up and needs the results fast.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在确认可以对数据进行分析后，Azeem 要求你处理年初的数据。他已经从服务器下载了一个单个约 2.4GB 大小的 CSV 文件。你可能仍然可以用 pandas
    处理它，但这会推动你的机器到极限，分析速度很慢。这是个问题，因为 Azeem 的截止日期临近，他需要快速得到结果。
- en: Fortunately, Dask can chop this single large 2.4GB file into smaller chunks
    and process these chunks in parallel. While pandas would have to process all the
    data on a single core, Dask can spread the computation out over all the cores
    in your machine. This will be much faster.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Dask 可以将这个单个大约 2.4GB 的文件切分成小块，并并行处理这些块。而 pandas 必须在单个核心上处理所有数据，Dask 可以将计算分布到机器的所有核心上。这样会快得多。
- en: Read a single file into a Dask DataFrame
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将单个文件读入 Dask DataFrame
- en: 'You can read a single CSV file like Azeem’s 2.4GB `6M.csv` into a Dask DataFrame
    using the following syntax:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用以下语法将 Azeem 的 2.4GB 的单个 CSV 文件 `6M.csv` 读入 Dask DataFrame 中：
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: '[NOTE] Note: we use `ddf` to refer to **D**ask **D**ata**F**rames and the conventional
    `df` to refer to pandas DataFrames.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[注意] 注意：我们使用 `ddf` 表示 **D**ask **D**ata**F**rames，使用传统的 `df` 表示 pandas DataFrames。'
- en: Dask will read the data in this large single CSV file into a Dask DataFrame.
    A Dask DataFrame is always subdivided into appropriately sized ‘chunks’ called
    **partitions**. By cutting up the data into these conveniently smaller chunks,
    Dask DataFrame can distribute computations over multiple cores. This is what allows
    Dask to scale to much larger datasets than pandas. See the Working with Partitioned
    Data section for more details.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 将这个大型单个 CSV 文件的数据读入 Dask DataFrame。Dask DataFrame 总是被切分成适当大小的称为 **partitions**
    的‘块’。通过将数据切分为这些方便的小块，Dask DataFrame 可以在多个核心上分布计算。这就是使 Dask 能够处理比 pandas 更大数据集的原因。详见处理分区数据一节。
- en: '![Fig 4 1\. Dask reads a CSV file into a partitioned DataFrame.](Images/how_to_work_with_dask_dataframes_343327_01.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图 4-1\. Dask 将 CSV 文件读入分区 DataFrame。](Images/how_to_work_with_dask_dataframes_343327_01.png)'
- en: Figure 2-1\. Dask reads a CSV file into a partitioned DataFrame.
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-1\. Dask 将 CSV 文件读入分区 DataFrame。
- en: Once your 2.4GB CSV file is loaded in, you can use Dask DataFrame to analyze
    the data it contains. This will look and feel much the same as it did with pandas,
    except you are no longer limited by the memory capacities of your machine and
    are able to run analyses on millions rather than thousands of rows. And perhaps
    most importantly, you are able to do so *before* Azeem’s important deadline.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你加载了这个 2.4GB 的 CSV 文件，你可以使用 Dask DataFrame 分析其中包含的数据。这在外观和感觉上与 pandas 差不多，不过你不再受到机器内存容量的限制，可以在数百万行而不是数千行上运行分析。而且最重要的是，你可以在
    Azeem 的重要截止日期之前进行分析。
- en: Read multiple files into a Dask DataFrame
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将多个文件读入 Dask DataFrame
- en: After seeing you crunch the `6M.csv` file with the data for Q1 and Q2 successfully,
    Azeem is starting to see the power of Dask. He’s now asked you to take a look
    at ***all the data*** he has available. That’s twenty years’ worth of time series
    data, totalling almost 60GB, scattered across 1,095 separate CSV files. This would
    have been an impossible task with pandas, but Dask has opened up a whole new world
    of possibilities for you here.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在成功处理了包含 Q1 和 Q2 数据的 `6M.csv` 文件后，Azeem 开始看到 Dask 的强大之处。他现在要求你查看他所有可用的数据。这是二十年的时间序列数据，总计近
    60GB，分布在 1,095 个单独的 CSV 文件中。这在 pandas 中是不可能完成的任务，但 Dask 在这里为你打开了全新的可能性。
- en: 'You ask Azeem to point you to the directory that contains the rest of the data,
    which looks like this:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你要求 Azeem 指出包含其余数据的目录，看起来是这样的：
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Dask DataFrame let’s you use * as a glob string to read all of these separate
    CSV files into a Dask DataFrame at once:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Dask DataFrame 允许你使用 * 作为 glob 字符串一次性读取所有这些单独的 CSV 文件到 Dask DataFrame 中：
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![Fig 4 2\. Dask reads multiple CSV files into a partitioned DataFrame.](Images/how_to_work_with_dask_dataframes_343327_02.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图 4-2\. Dask 将多个 CSV 文件读入分区 DataFrame。](Images/how_to_work_with_dask_dataframes_343327_02.png)'
- en: Figure 2-2\. Dask reads multiple CSV files into a partitioned DataFrame.
  id: totrans-35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-2\. Dask 将多个 CSV 文件读入分区 DataFrame。
- en: Just like we saw above with the single CSV file, Dask will read the data across
    all of these separate CSV files into a single Dask DataFrame. The DataFrame will
    be divided into appropriately sized ‘chunks’ called **partitions**. Let’s look
    more closely at how to work with Dask partitions.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在单个CSV文件中看到的那样，Dask将跨所有这些单独的CSV文件读取数据到一个单独的Dask DataFrame中。DataFrame将被划分为适当大小的‘块’，称为**分区**。让我们更仔细地看看如何处理Dask分区。
- en: Working with partitioned data
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用分区数据工作
- en: Dask intentionally and automatically splits up the data so operations can be
    performed on partitions of the data in parallel. This is done regardless of the
    original file format. It’s important to understand how you can influence the partitioning
    of your data in order to choose a partitioning strategy that will deliver optimal
    performance for your dataset.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Dask故意并自动地将数据分割成分区，以便可以并行地在数据的分区上执行操作。这是无论原始文件格式如何都会进行的操作。了解如何影响数据分区以选择适合您数据集的分区策略至关重要，以提供最佳性能。
- en: You can run `ddf.partitions` to see how many partitions the data is divided
    amongst.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`ddf.partitions`可以查看数据分成了多少个分区。
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Setting Partition Size
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置分区大小
- en: By default, Dask will split the data into a certain number of partitions by
    calculating the optimal `blocksize`, which is based on the available memory and
    number of cores on the machine. This means the number of partitions for the same
    dataset may vary when working on different machines. Dask will ensure that the
    partitions are small enough to be processed quickly but not so small as to create
    unnecessary overhead. The maximum `blocksize` that Dask will calculate by default
    is 64 MB.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Dask将根据可用内存和机器上的核数计算出最佳的`blocksize`来将数据分成一定数量的分区。这意味着在不同的机器上处理同一数据集时，分区的数量可能会有所不同。Dask将确保分区足够小以便快速处理，但又不至于过小以致造成不必要的开销。Dask默认计算的最大`blocksize`为64
    MB。
- en: Dask splits up the data from the CSV files into one partition per file.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Dask将CSV文件的数据分割成每个文件一个分区。
- en: '![Fig 4 2\. Dask reads a single large CSV file into multiple partitions.](Images/how_to_work_with_dask_dataframes_343327_03.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图 4-2\. Dask将一个大的CSV文件读取为多个分区。](Images/how_to_work_with_dask_dataframes_343327_03.png)'
- en: Figure 2-3\. Dask reads a single large CSV file into multiple partitions.
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-3\. Dask将一个大的CSV文件读取为多个分区。
- en: You can manually set the number of partitions that the DataFrame contains using
    the `blocksize` parameter. You can tweak the partition size for optimal performance.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`blocksize`参数手动设置DataFrame包含的分区数量。您可以调整分区大小以获得最佳性能。
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This dataset will be read into a Dask DataFrame with 2190 partitions when the
    blocksize is set to 16 MB. The number of partitions goes up when the blocksize
    decreases.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当块大小设置为16 MB时，这个数据集将被读入一个包含2190个分区的Dask DataFrame中。当块大小减小时，分区数量会增加。
- en: '![Fig 4 3\. The number of partitions increases as the partitions become smaller.](Images/how_to_work_with_dask_dataframes_343327_04.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图 4-3\. 随着分区变小，分区数量增加。](Images/how_to_work_with_dask_dataframes_343327_04.png)'
- en: Figure 2-4\. The number of partitions increases as the partitions become smaller.
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-4\. 随着分区变小，分区数量增加。
- en: Note
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: '[TIP] As a rule of thumb, we recommend working with partition sizes of 100MB
    or less. This will ensure that the partitions are small enough to avoid bottlenecks
    but not so small that they start to incur unnecessary overhead.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[提示] 作为一个经验法则，我们建议使用每个分区不超过100MB的大小。这样可以确保分区足够小，避免瓶颈，但又不至于过小导致不必要的开销。'
- en: Inspecting Data Types
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查数据类型
- en: After successfully reading the 1,095 CSV files into a single Dask DataFrame,
    you’re now curious to know if the data for the single `0000.csv` file that was
    analyzed with pandas contains the same data types as the other 1,094 files in
    the folder. Remember that `df` is a pandas DataFrame containing the 93MB sample
    data and `ddf` is a Dask DataFrame containing 58 GB of data.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 成功将1,095个CSV文件读取到一个单独的Dask DataFrame后，您现在想知道使用pandas分析的单个`0000.csv`文件中的数据类型是否与文件夹中其他1,094个文件的数据类型相同。请记住，`df`是一个包含93MB样本数据的pandas
    DataFrame，`ddf`是一个包含58GB数据的Dask DataFrame。
- en: 'You can inspect the data types with pandas using the `dtypes` method:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`dtypes`方法在pandas中检查数据类型：
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: pandas scans all the data to get the data types. This is fine for a 93MB file
    that easily fits into local memory. However, when working with Dask DataFrames
    we want to avoid reading all the data into memory. Loading all the CSV files into
    memory to get the data types will cause a `MemoryError`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 扫描所有数据以获取数据类型。对于一个容易适应本地内存的 93MB 文件来说，这是可以接受的。然而，在处理 Dask DataFrames
    时，我们希望避免将所有数据读入内存。加载所有 CSV 文件到内存中以获取数据类型将导致 `MemoryError`。
- en: Instead, Dask *infers* the column data types (provided they are not explicitly
    set by the user). It does so by reading the first *n* rows of the DataFrame into
    memory and using the contents to infer the data types for the rest of the rows.
    *n* here is determined by the value of either the `sample` or `sample_rows` arguments
    to `read_csv``.` `sample` defines the number of bytes to read, `sample_rows` defines
    the number of rows to read.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，Dask *推断* 列数据类型（如果用户没有明确设置的话）。它通过将 DataFrame 的前 *n* 行读入内存，并使用内容推断其余行的数据类型。这里的
    *n* 由 `sample` 或 `sample_rows` 参数的值确定，`sample` 定义要读取的字节数，`sample_rows` 定义要读取的行数。
- en: 'Let’s inspect the data types of our Dask DataFrame:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下我们的 Dask DataFrame 的数据类型：
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: '[CAUTION] You should be aware that inferring data types based on a sample of
    the rows is error-prone. Dask may incorrectly infer data types based on a sample
    of the rows which will cause downstream computations to error out.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[警告] 你应该意识到基于少量行样本推断数据类型是容易出错的。Dask 可能会根据少量行的样本错误地推断数据类型，这将导致下游计算出错。'
- en: For example, the first 1000 rows of a column `id` may contain only integers
    and the 1001th row a string. If Dask is reading only the first 1000 rows, the
    dtype for `id` will be inferred as `int64`. This will lead to errors downstream
    when trying to run operations meant for integers on the column, since it contains
    at least one string. The correct dtype for this column should be `object`.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个列 `id` 的前 1000 行可能只包含整数，而第 1001 行可能是字符串。如果 Dask 只读取了前 1000 行，那么 `id` 的
    dtype 将被推断为 `int64`。当试图对列上的整数运行操作时，这将导致下游错误，因为它包含至少一个字符串。这个列的正确 dtype 应该是 `object`。
- en: You can avoid data type inference by explicitly specifying dtypes when reading
    CSV files.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 当读取 CSV 文件时，你可以通过明确指定 dtypes 来避免数据类型推断。
- en: 'Let’s manually set the name column to be a string, which is more efficient
    than object type columns:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们手动将 name 列设置为字符串，这比对象类型列更有效率：
- en: '[PRE10]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Dask will infer the data types for the columns that you don’t manually specify.
    If you specify the data types for all the columns, then Dask won’t do any data
    type inference.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 将推断那些你没有手动指定数据类型的列的数据类型。如果你为所有列指定了数据类型，那么 Dask 将不会进行任何数据类型推断。
- en: Reading Remote Data from the Cloud
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从云中读取远程数据
- en: After showcasing the power of Dask to your Team Lead using the entire CSV dataset,
    you’ve now been tasked with running analyses on all the data in production. Now
    it turns out that Azeem had actually downloaded the CSV files from the company’s
    S3 bucket. That won’t fly when running these crucial analyses in production and
    also happens to be against the company’s data management best practices. You’ve
    been tasked with redoing the analysis without downloading the data locally.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在向团队领导展示了 Dask 的强大之后，你现在被要求在生产环境中对所有数据进行分析。现在问题是，Azeem 实际上已经从公司的 S3 存储桶中下载了
    CSV 文件。这在生产环境中运行这些关键分析时是不可接受的，而且也违反了公司的数据管理最佳实践。你被要求重新进行分析，而不是将数据下载到本地。
- en: Dask readers make it easy to read data that’s stored in remote object data stores,
    like AWS S3.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 读取器可以轻松读取存储在远程对象数据存储中的数据，比如 AWS S3。
- en: 'Here’s how to read a CSV file that’s stored in a public S3 bucket to your local
    machine:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何将存储在公共 S3 存储桶中的 CSV 文件读取到本地机器上的方法：
- en: '[PRE11]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: '[TIP] Dask DataFrame readers expose a `storage_options` keyword that allows
    you to pass additional configurations to the remote data store, such as credentials
    for reading from private buckets or whether to use a SSL-encrypted connection.
    This can be helpful when working with private buckets or when your IT department
    has particular security policies in place.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[提示] Dask DataFrame 读取器暴露了一个 `storage_options` 关键字，允许你将额外的配置传递给远程数据存储，比如从私有存储桶读取时的凭据或者是否使用
    SSL 加密连接。当与私有存储桶一起工作或者你的 IT 部门有特定的安全政策时，这将非常有用。'
- en: Because of Dask’s lazy evaluation, running the command above will complete quickly.
    But for subsequent operations, remember that the data will actually need to be
    downloaded from the remote cloud storage to your local machine where the computations
    are running. This is called ‘moving data to compute’ and is generally inefficient.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Dask 的惰性评估，运行上述命令将很快完成。但对于后续的操作，请记住数据实际上需要从远程云存储下载到你的本地机器上进行计算。这称为“将数据移动到计算”，通常效率不高。
- en: Because of this, it’s usually best to run computations on remote data using
    cloud computing.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通常最好在云计算环境中运行远程数据上的计算。
- en: Here’s how to spin up a Dask cluster with Coiled and run these computations
    in the cloud. You will need a Coiled account to spin up these resources. If you’ve
    purchased this book, you get XXX free Coiled credits. Go to LINK to sign up for
    your personal Coiled account.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是如何使用 Coiled 在云端启动 Dask 集群并运行这些计算任务。你需要一个 Coiled 账户来启动这些资源。如果你已购买本书，将获得 XXX
    免费的 Coiled 信用额度。请访问链接 [LINK](https://www.coiled.io) 注册你的个人 Coiled 账户。
- en: 'We’ll import Dask and Coiled:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将导入 Dask 和 Coiled：
- en: '[PRE12]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'And then launch a Dask cluster with 5 workers:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 然后启动一个包含 5 个 worker 的 Dask 集群：
- en: '[PRE13]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Finally, we’ll connect our local Dask client to the remote cluster:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将把我们的本地 Dask 客户端连接到远程集群：
- en: '[PRE14]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: All subsequent Dask computations will now be executed in the cloud, instead
    of on your local machine.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 所有后续的 Dask 计算现在将在云端执行，而不是在你的本地机器上。
- en: Processing Data with Dask DataFrames
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Dask DataFrames 处理数据
- en: In the previous section we saw how to load data from various sources and file
    formats into a Dask DataFrame and highlighted considerations regarding partitioning
    and data type inference. You’ve now got your CSV data loaded into your Dask DataFrame
    and are ready to dig into the data.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们看到了如何从各种来源和文件格式加载数据到 Dask DataFrame，并强调了分区和数据类型推断的考虑。现在你已经将 CSV 数据加载到你的
    Dask DataFrame 中，并准备好深入研究这些数据了。
- en: Dask is intentionally designed to seamlessly scale existing popular PyData libraries
    like NumPy, scikit-learn and pandas. This means that processing data with Dask
    DataFrames will look and feel much like it would in pandas.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 是有意设计成可以无缝扩展现有流行的 PyData 库，如 NumPy、scikit-learn 和 pandas。这意味着使用 Dask DataFrames
    处理数据将看起来和感觉上与在 pandas 中一样。
- en: 'For example, the code below demonstrates how you can use Dask DataFrames to
    filter rows, compute reductions and perform groupby aggregations:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，下面的代码演示了如何使用 Dask DataFrames 来过滤行、计算减少和执行分组聚合：
- en: '[PRE15]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: However, there are a few cases where Dask operations require additional considerations,
    mainly because we are now operating in a parallel computing environment. The following
    section digs into those particular cases.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，有一些情况需要额外考虑 Dask 操作，主要是因为我们现在在并行计算环境中操作。接下来的部分深入探讨了这些特定情况。
- en: Converting to Parquet files
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换为 Parquet 文件
- en: 'Azeem heard that Parquet files are faster and more efficient to query than
    CSV files. He’s not sure about the performance benefits offered quite yet, but
    figures out how to make the conversion relatively easily with the help of a coworker:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Azeem 听说 Parquet 文件比 CSV 文件查询更快更有效。他还不确定提供的性能优势，但在同事的帮助下，他相对容易地完成了转换：
- en: '[PRE16]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: See the Benefits of Parquet section for more details.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请参阅 Parquet 部分的优势。
- en: Let’s take a look at the various ways Azeem can query and manipulate the data
    in the 20-years Parquet dataset.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 Azeem 如何查询和操作这个 20 年的 Parquet 数据集的各种方式。
- en: Materializing results in memory with compute
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在内存中实现计算结果
- en: You can convert a Dask DataFrame to a pandas DataFrame with `compute()`. When
    the dataset is small, it’s fine to convert to a pandas DataFrame. In general,
    you don’t want to convert Dask DataFrames to pandas DataFrames because then you
    lose all the parallelism and lazy execution benefits of Dask.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 `compute()` 将 Dask DataFrame 转换为 pandas DataFrame。当数据集较小时，将其转换为 pandas
    DataFrame 是可以接受的。总的来说，你不应该将 Dask DataFrames 转换为 pandas DataFrames，因为这样会失去 Dask
    的所有并行性和惰性执行的优势。
- en: We saw earlier in the Chapter that Dask DataFrames are composed of a collection
    of underlying pandas DataFrames (partitions). Calling `compute()` concatenates
    all the Dask DataFrame partitions into a single Pandas DataFrame.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的前面部分，我们看到 Dask DataFrames 由一组基础的 pandas DataFrames（分区）组成。调用 `compute()`
    将所有 Dask DataFrame 分区连接成单个 Pandas DataFrame。
- en: When the Dask DataFrame contains data that’s split across multiple nodes in
    a cluster, then `compute()` may run slowly. It can also cause out of memory errors
    if the data isn’t small enough to fit in the memory of a single machine.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 当 Dask DataFrame 包含分布在集群中多个节点上的数据时，`compute()` 可能运行缓慢。如果数据不能小到足以适应单台机器的内存，它还可能导致内存错误。
- en: Dask was created to solve the memory issues of using pandas on a single machine.
    When you run `compute()`, you’ll face all the normal memory limitations of pandas.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 的创建目的是解决在单台机器上使用 pandas 时的内存问题。当你运行 `compute()` 时，你将面临 pandas 的所有正常内存限制。
- en: Let’s look at some examples and see when it’s best to use compute() in your
    analyses.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一些例子，看看在分析中何时最好使用 compute()。
- en: Small DataFrame example
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小型 DataFrame 示例
- en: '`compute()` converts a Dask DataFrame to a pandas DataFrame. Let’s demonstrate
    with a small example.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`compute()` 将 Dask DataFrame 转换为 pandas DataFrame。我们通过一个小例子来演示。'
- en: 'Create a two column Dask DataFrame and then convert it to a pandas DataFrame:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个包含两列的 Dask DataFrame，然后将其转换为 pandas DataFrame：
- en: '[PRE17]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Verify that `compute()` returns a pandas DataFrame:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 验证 `compute()` 返回一个 pandas DataFrame：
- en: '[PRE18]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Dask DataFrames are composed of many underlying pandas DataFrames, each of which
    is called a partition. It’s no problem calling `compute()` when the data is small
    enough to get collected in a single pandas DataFrame, but this will break down
    whenever the data is too big to fit in the memory of a single machine.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Dask DataFrames 由许多底层的 pandas DataFrames 组成，每个称为一个分区。当数据足够小以适应单个 pandas DataFrame
    的内存时，调用 `compute()` 是没有问题的，但是一旦数据过大以至于不能在单台机器的内存中容纳，这种方法就会失败。
- en: Let’s run `compute()` on a large DataFrame in a cloud cluster environment and
    take a look at the error message.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在云集群环境中对一个大 DataFrame 运行 `compute()` 并查看错误信息。
- en: Large DataFrame example
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大型 DataFrame 示例
- en: 'Create a 5 node Dask cluster and read in a 662 million row dataset into a Dask
    DataFrame:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个包含 662 百万行数据的 Dask DataFrame，并将其读入一个 5 节点的 Dask 集群中：
- en: '[PRE19]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Perform a filtering operation on the Dask DataFrame and then collect the result
    into a single pandas DataFrame with `compute()`:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 对 Dask DataFrame 进行过滤操作，然后使用 `compute()` 将结果收集到一个单独的 pandas DataFrame 中：
- en: '[PRE20]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This works because `res` only has 1,103 rows of data. It’s easy to collect such
    a small dataset into a single pandas DataFrame.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这个操作之所以有效，是因为 `res` 只有 1,103 行数据。这样小规模的数据集可以轻松地收集到一个单独的 pandas DataFrame 中。
- en: '`ddf.compute()` will error out if we try to collect the entire 663 million
    row dataset into a pandas DataFrame.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们试图将整个 663 百万行数据集收集到一个 pandas DataFrame 中，`ddf.compute()` 将会报错。
- en: Azeem’s dataset has 58 GB of data, which is too large for a single machine.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Azeem 的数据集有 58 GB，这对于单台机器来说太大了。
- en: Compute intuition
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Compute 的直觉
- en: Here’s a diagram that visually demonstrates how compute() works, to give you
    some additional intuition.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个图示直观地展示了 compute() 的工作原理，帮助你更好地理解。
- en: Suppose you have a 3-node cluster with 4 partitions. You run a compute() operation
    to collect all of the data in a single Pandas DataFrame.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个包含 4 个分区的 3 节点集群。你运行一个 compute() 操作将所有数据收集到一个单独的 Pandas DataFrame 中。
- en: '![Fig 4 4\. Calling compute   may create memory errors.](Images/how_to_work_with_dask_dataframes_343327_05.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图 4 4\. 调用 compute 可能会导致内存错误。](Images/how_to_work_with_dask_dataframes_343327_05.png)'
- en: Figure 2-5\. Calling compute() may create memory errors.
  id: totrans-122
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-5\. 调用 compute() 可能会导致内存错误。
- en: This diagram clearly illustrates why the compute() operation can cause out of
    memory exceptions. Data that fits when it’s split across multiple machines in
    a cluster won’t necessarily fit in a single pandas DataFrame on only one machine.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图示清楚地说明了为什么 compute() 操作可能会导致内存异常。数据在集群中分布时可以拆分到多台机器上，但在单台机器的单个 pandas DataFrame
    中可能无法容纳。
- en: Minimizing compute() calls
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 减少 compute() 调用次数
- en: compute() can be an expensive operation, so you want to minimize compute() calls
    whenever possible.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: compute() 可能是一个昂贵的操作，因此尽可能减少 compute() 的调用次数是明智的。
- en: The following code snippet runs compute() twice and takes 63 seconds to run
    on a 5 node Dask cluster.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码片段运行了两次 compute()，在一个包含 5 个节点的 Dask 集群上运行时间为 63 秒。
- en: '[PRE21]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We can refactor this code to only run compute() once and it’ll run in 33 seconds.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以重构这段代码，只运行一次 compute()，运行时间为 33 秒。
- en: '[PRE22]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Fewer compute() calls will always run faster than more compute() invocations
    because Dask can optimize computations with shared tasks.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 比起多次调用 compute()，少调用 compute() 总是更快的，因为 Dask 可以优化共享任务的计算。
- en: When to call compute()
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 何时调用 compute()
- en: You can call compute() if you’ve performed a large filtering operation or another
    operation that decreases the size of the overall dataset. If your data comfortably
    fits in memory, you don’t need to use Dask. Just stick with pandas if your data
    is small enough.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您执行了大型过滤操作或其他减小整体数据集大小的操作，则可以调用`compute()`。如果您的数据适合内存，那么您不需要使用 Dask。如果您的数据足够小，请坚持使用
    pandas。
- en: You will also call compute() when you’d like to force Dask to execute the computations
    and return a result. Dask executes computations lazily by default. It’ll avoid
    running expensive computations until you run a method like compute() that forces
    computations to be executed.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 当您希望强制 Dask 执行计算并返回结果时，您还将调用`compute()`。Dask 默认惰性执行计算。它将避免运行昂贵的计算，直到您运行像`compute()`这样的方法，强制执行计算。
- en: Materializing results in memory with persist()
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用`persist()`将结果在内存中实现。
- en: Instead of calling compute(), you can store Dask DataFrames in memory with persist().
    This will store the contents of the Dask DataFrame in cluster memory which will
    make downstream queries that depend on the persisted data faster. This is great
    when you perform some expensive computations and want to save the results in memory
    so they’re not rerun multiple times.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`persist()`将 Dask 数据框架存储在内存中，而不是调用`compute()`。这将把 Dask 数据框架的内容存储在集群内存中，从而使依赖于持久化数据的下游查询更快。当您执行一些昂贵的计算并希望将结果保存在内存中以避免多次重新运行时，这将非常有用。
- en: Note
  id: totrans-136
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: '[CAUTION] Be careful with using persist() when working locally. Calling persist()
    will load the results of a computation into memory, since you are not operating
    with a cluster to give you additional resources, this may mean that persist()
    creates memory errors. Using persist() is most beneficial when working with a
    remote cluster.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[注意] 在本地工作时要小心使用`persist()`。调用`persist()`将计算结果加载到内存中，因为您没有操作集群以提供额外的资源，这可能意味着`persist()`会导致内存错误。当使用远程集群时，使用`persist()`效果最佳。'
- en: Note
  id: totrans-138
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: '[NOTE] Note that persist() commands do not block. This means that you can continue
    running other commands immediately afterwards and the persist() computation will
    run in the background.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[注意] 请注意，`persist()`命令不会阻塞。这意味着您可以立即继续运行其他命令，`persist()`计算将在后台运行。'
- en: Many Dask users erroneously assume that Dask DataFrames are persisted in memory
    by default, which isn’t true. Dask runs computations in memory. It doesn’t store
    data in memory unless you explicitly call `persist()`.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 许多 Dask 用户错误地认为 Dask 数据框架默认存储在内存中，这是不正确的。Dask 在内存中运行计算。除非显式调用`persist()`，否则它不会将数据存储在内存中。
- en: Let’s start with examples of persist() on small DataFrames and then move to
    examples on larger DataFrames so you can see some realistic performance benchmarks.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从对小型数据框架上的`persist()`示例开始，然后转向对较大数据框架的示例，这样您就可以看到一些真实的性能基准。
- en: Simple example
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 简单的例子
- en: 'Let’s create a small Dask DataFrame to demonstrate how persist() works:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个小型 Dask 数据框架来演示`persist()`的工作原理：
- en: '[PRE23]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The persisted_ddf is saved in memory when `persist()` is called. Subsequent
    queries that run off of `persisted_ddf` will execute more quickly than if you
    hadn’t called persist().
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 当调用`persist()`时，`persisted_ddf`保存在内存中。后续基于`persisted_ddf`运行的查询将比没有调用`persist()`时执行速度更快。
- en: Let’s run `persist()` on a larger DataFrame to see some real computation runtimes.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在较大的数据框架上运行`persist()`，以查看一些真实的计算运行时间。
- en: Example on big dataset
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大数据集的示例
- en: Let’s run some queries on a dataset that’s not persisted to get some baseline
    query runtimes. Then let’s persist the dataset, run the same queries, and quantify
    the performance gains from persisting the dataset.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对一个尚未持久化的数据集运行一些查询，以获取一些基准查询运行时间。然后让我们持久化数据集，运行相同的查询，并量化持久化数据集带来的性能提升。
- en: These queries are run on Azeem’s 20 year Parquet timeseries dataset.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这些查询在 Azeem 的 20 年 Parquet 时间序列数据集上运行。
- en: Here’s the code that creates a computation cluster, reads in a DataFrame, and
    then creates a filtered DataFrame.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这是创建计算集群、读入数据框架，然后创建过滤数据框架的代码。
- en: '[PRE24]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Let’s time a couple of analytical queries.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计时一些分析查询。
- en: '[PRE25]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Let’s persist the dataset to cluster memory and then run the same queries to
    see how long they take to execute.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将数据集持久化到集群内存中，然后运行相同的查询，看看它们执行需要多长时间。
- en: '[PRE26]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The queries took over a minute to run before the data was persisted, but only
    takes 2 seconds to run on the persisted dataset.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据持久化之前，这些查询执行需要超过一分钟，但在持久化数据集上只需2秒即可完成。
- en: Of course it takes some time to persist the data. Let’s look at why this particular
    example gave us great results when persisting.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，持久化数据需要一些时间。让我们看看为什么这个特定的示例在持久化时给了我们很好的结果。
- en: Note
  id: totrans-158
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: '[NOTE] Great opportunity to persist'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[注意] 持久化的绝佳机会'
- en: Persisting helps sometimes and causes problems other times. Let’s look at high
    level patterns when it’ll usually help and when it’ll usually cause problems.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 持久化有时会有所帮助，但有时会引起问题。让我们看一看通常何时会有帮助，何时通常会引起问题的高级模式。
- en: 'Our example uses the following pattern:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的示例使用以下模式：
- en: Start with a large dataset
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从一个大型数据集开始
- en: Filter it down to a much smaller datasets (that’s much smaller than the memory
    of the cluster)
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将其过滤为一个比集群内存还小得多的数据集
- en: Run analytical queries on the filtered dataset
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在经过筛选的数据集上运行分析查询
- en: You can expect good results from persist() with this set of circumstances.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以期待在这种情况下从 persist() 获得良好的结果。
- en: Here’s a different pattern that won’t usually give such a good result.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个不太可能产生如此好结果的不同模式。
- en: Read in a large dataset that’s bigger than memory
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 读入一个大于内存的大型数据集
- en: Persist the entire dataset
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持久化整个数据集
- en: Run a single analytical operation
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行单个分析操作
- en: In this case, the cost of running the persist operation will be greater than
    the benefits of having a single query run a little bit faster. Persisting doesn’t
    always help.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，运行持久化操作的成本将大于使单个查询运行速度稍快的好处。持久化并不总是有帮助的。
- en: Writing to disk vs persisting in memory
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 写入磁盘 vs 持久化在内存中
- en: We can also “persist” results by writing to disk rather than saving the data
    in memory.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过将结果写入磁盘而不是保存数据在内存中来“持久化”结果。
- en: Let’s persist the filtered dataset in S3 and run the analytical queries to quantify
    time savings.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将经过筛选的数据集持久化到 S3，并运行分析查询以量化节省的时间。
- en: '[PRE27]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The filtered dataset that was written to disk can be queried with subsecond
    response times.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 写入磁盘的经过筛选的数据集可以在亚秒级的响应时间内进行查询。
- en: Writing temporary files to disk isn’t always ideal because then you have stale
    files sitting around that need to get cleaned up later.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 将临时文件写入磁盘并不总是理想的，因为这样您就会有过时的文件挂在那里，需要稍后清理。
- en: Repartitioning and persisting
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重新分区和持久化
- en: We can also repartition before persisting, which will make our analytical queries
    in this example run even faster.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以在持久化之前重新分区，这将使我们在这个示例中运行的分析查询速度更快。
- en: '[PRE28]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The filtered dataset is tiny and doesn’t need a lot of partitions. That’s why
    repartitioning drops query times from around 2 seconds to 0.3 seconds in this
    example.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 经过筛选的数据集很小，不需要很多分区。这就是为什么在这个示例中，重新分区将查询时间从大约 2 秒降低到 0.3 秒的原因。
- en: Persist summary
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 持久化摘要
- en: Persist is a powerful optimization technique to have in your Dask toolkit.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 持久化是您 Dask 工具包中强大的优化技术。
- en: It’s especially useful when you’ve performed some expensive operations that
    reduce the dataset size and subsequent operations benefit from having the computations
    stored.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 当您执行了一些昂贵的操作以减少数据集大小，并且随后的操作从存储的计算中受益时，这将非常有用。
- en: Some new Dask programmers can misuse persist() and slow down analyses by persisting
    too often or trying to persist massive datasets. It helps sometimes, but it can
    cause analyses to run slower when used incorrectly.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 一些新的 Dask 程序员可能会误用 persist()，通过过于频繁地持久化或尝试持久化大量数据集来减慢分析速度。有时它确实有所帮助，但如果使用不正确，则可能导致分析运行速度变慢。
- en: 'Persisting will generally speed up analyses when one or more items in this
    set of facts are true:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 当以下一项或多项事实为真时，持久化通常会加快分析速度：
- en: You’ve performed expensive computations that have reduced the dataset size
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您执行了减少数据集大小的昂贵计算
- en: The reduced dataset comfortably fits in memory
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少的数据集可以舒适地适应内存
- en: You want to run multiple queries on the reduced dataset
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您希望在减少的数据集上运行多个查询
- en: Repartitioning Dask DataFrames
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重新分区 Dask DataFrames
- en: This section explains how to redistribute data among partitions in a Dask DataFrame
    with repartitioning. Analyses run slower when data is unevenly distributed across
    partitions, and repartitioning can smooth out the data and provide significant
    performance boost.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 本节解释了如何使用重新分区在 Dask DataFrame 中重新分配数据。当数据在分区之间分布不均匀时，分析速度会变慢，而重新分区可以平滑数据并提供显著的性能提升。
- en: Dask DataFrames consist of partitions, each of which is a pandas DataFrame.
    Dask performance will suffer if there are lots of partitions that are too small
    or some partitions that are too big. Repartitioning a Dask DataFrame solves the
    issue of “partition imbalance”.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: Dask DataFrames 包含多个分区，每个分区都是一个 pandas DataFrame。如果有很多太小的分区或者有一些太大的分区，Dask 的性能会受到影响。重新分区
    Dask DataFrame 可以解决“分区不平衡”的问题。
- en: Let’s start with some simple examples to get you familiar with the repartition()
    syntax.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一些简单的示例开始，让你熟悉 repartition() 的语法。
- en: Simple examples
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 简单的示例
- en: 'Let’s create a Dask DataFrame with six rows of data organized in three partitions:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个包含六行数据的 Dask DataFrame，分为三个分区：
- en: '[PRE29]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Print the content of each DataFrame partition:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 打印每个 DataFrame 分区的内容：
- en: '[PRE30]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Repartition the DataFrame into two partitions:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 将 DataFrame 重新分区为两个分区：
- en: '[PRE31]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: repartition(2) causes Dask to combine partition 1 and partition 2 into a single
    partition. Dask’s repartition algorithm is smart to coalesce existing partitions
    and avoid full data shuffles.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: repartition(2) 导致 Dask 将分区 1 和分区 2 合并为一个单独的分区。Dask 的重新分区算法能够智能地合并现有分区并避免完全的数据洗牌。
- en: 'You can also increase the number of partitions with repartition. Repartition
    the DataFrame into 5 partitions:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以通过重新分区来增加分区的数量。将 DataFrame 重新分区为 5 个分区：
- en: '[PRE32]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'In practice, it’s easier to repartition by specifying a target size for each
    partition (e.g. 100 MB per partition). You want Dask to do the hard work of figuring
    out the optimal number of partitions for your dataset. Here’s the syntax for repartitioning
    into 100MB partitions:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，通过指定每个分区的目标大小（例如每个分区 100MB）来重新分区更为简单。你希望 Dask 完成确定数据集最佳分区数量的繁重工作。以下是重新分区为
    100MB 分区的语法：
- en: '[PRE33]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: When to repartition
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 何时重新分区
- en: Of course, repartitioning isn’t free and takes time. The cost of performing
    a full data shuffle can outweigh the benefits of subsequent query performance.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，重新分区并非免费，需要时间。执行完整数据洗牌的成本可能会超过后续查询性能的好处。
- en: You shouldn’t always repartition whenever a dataset is imbalanced. Repartitioning
    should be approached on a case-by-case basis and only performed when the benefits
    outweigh the costs.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据集不平衡时，并非总是应该进行重新分区。重新分区应该根据具体情况逐案评估，并且只有在利大于弊的情况下才进行。
- en: Common causes of partition imbalance
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分区不平衡的常见原因
- en: Filtering is a common cause of DataFrame partition imbalance.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 筛选是导致 DataFrame 分区不平衡的常见原因。
- en: 'Suppose you have a DataFrame with a first_name column and the following data:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个包含 first_name 列和以下数据的 DataFrame：
- en: 'Partition 0: Everyone has a first_name “Allie”'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第 0 分区：每个人的名字都是“Allie”
- en: 'Partition 1: Everyone has first_name “Matt”'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第 1 分区：每个人的名字都是“Matt”
- en: 'Partition 2: Everyone has first_name “Sam”'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第 2 分区：每个人的名字都是“Sam”
- en: If you filter for all the rows with first_name equal to “Allie”, then Partition
    1 and Partition 2 will be empty. Empty partitions cause inefficient Dask execution.
    It’s often wise to repartition after filtering.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你筛选出所有名字为“Allie”的行，那么第 1 分区和第 2 分区将会是空的。空的分区会导致 Dask 执行效率低下。通常在筛选后重新分区是明智的选择。
- en: Filtering Dask DataFrames
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 筛选 Dask DataFrame
- en: This section explains how to filter Dask DataFrames based on the DataFrame index
    and on column values using loc().
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 本节说明了如何根据 DataFrame 的索引和列值使用 loc() 进行筛选 Dask DataFrame。
- en: Filtering Dask DataFrames can cause data to be unbalanced across partitions
    which isn’t desirable from a performance perspective. This section illustrates
    how filtering can cause the “empty partition problem” and how to eliminate empty
    partitions with the repartitioning techniques we learned in the previous section.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 筛选 Dask DataFrame 可能会导致数据在分区之间不平衡，从性能的角度来看这是不可取的。本节说明了筛选可能引起的“空分区问题”，以及如何通过前面学习的重新分区技术消除空分区。
- en: Index filtering
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 索引筛选
- en: Dask DataFrames consist of multiple partitions, each of which is a pandas DataFrame.
    Each pandas DataFrame has an index. Dask allows you to filter multiple pandas
    DataFrames on their index in parallel, which is quite fast.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: Dask DataFrames 包含多个分区，每个分区都是一个 pandas DataFrame。每个 pandas DataFrame 都有一个索引。Dask
    允许你并行地根据它们的索引筛选多个 pandas DataFrame，速度非常快。
- en: 'Let’s create a Dask DataFrame with 6 rows of data organized in two partitions:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个包含 6 行数据的 Dask DataFrame，分为两个分区：
- en: '[PRE34]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Let’s visualize the data in each partition:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们可视化每个分区中的数据：
- en: '[PRE35]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Dask automatically added an integer index column to our data.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 自动为我们的数据添加了一个整数索引列。
- en: 'Grab rows 2 and 5 from the DataFrame:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 从 DataFrame 中获取第 2 和第 5 行：
- en: '[PRE36]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Grab rows 3, 4, and 5 from the DataFrame:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 从 DataFrame 中获取第 3、4 和第 5 行：
- en: '[PRE37]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Let’s learn more about how Dask tracks information about divisions in sorted
    DataFrames to perform `loc` filtering efficiently.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更多地了解Dask如何跟踪排序的DataFrame中关于分区的信息，以便高效执行`loc`过滤。
- en: Divisions refresher
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分区刷新器
- en: We’ve already discussed Dask DataFrame divisions in the architecture chapter,
    but we’re going to do a full refresher here because they’re so critical when working
    with the Dask DataFrames index.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在架构章节中讨论了Dask DataFrame的分区，但我们在这里要进行全面的复习，因为在处理Dask DataFrame索引时它们非常关键。
- en: Dask is aware of the starting and ending index value for each partition in the
    DataFrame and stores this division’s metadata to perform quick filtering.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: Dask知道DataFrame每个分区的起始和结束索引值，并存储此分区的元数据以便快速过滤。
- en: You can verify that Dask is aware of the divisions for this particular DataFrame
    by running `ddf.known_divisions` and seeing it returns `True`. Dask isn’t always
    aware of the DataFrame divisions.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行`ddf.known_divisions`并看到它返回`True`，您可以验证Dask知道这个特定DataFrame的分区。Dask并非总是知道DataFrame的分区。
- en: 'Print all the divisions of the DataFrame:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 打印DataFrame的所有分区：
- en: '[PRE38]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Take a look at the values in each partition of the DataFrame to better understand
    this divisions output.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 查看DataFrame每个分区中的值，以更好地理解这些分区的输出。
- en: '[PRE39]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The first division is from 0-3, and the second division is from 3-5\. This means
    the first division contains rows 0 to 2, and the last division contains rows 3
    to 5.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个分区是从0到3，第二个分区是从3到5。这意味着第一个分区包含行0到2，最后一个分区包含行3到5。
- en: Dask’s division awareness in this example lets it know exactly what partitions
    it needs to fetch from when filtering.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，Dask的分区意识让它精确地知道在过滤时需要从哪些分区获取数据。
- en: Column value filtering
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于列值的过滤
- en: You won’t always be able to filter based on index values. Sometimes you need
    to filter based on actual column values.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 您不总是能够基于索引值进行过滤。有时需要基于实际列值进行过滤。
- en: 'Fetch all rows in the DataFrame where `nums` is even:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 获取DataFrame中所有`nums`为偶数的行：
- en: '[PRE40]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Find all rows where `nums` is even, and `letters` contains either `b` or `f`:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 查找所有`nums`为偶数，并且`letters`包含`b`或`f`的行：
- en: '[PRE41]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Dask makes it easy to apply multiple logic conditions when filtering.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在过滤时，Dask使应用多个逻辑条件变得容易。
- en: Empty partition problem
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 空分区问题
- en: Let’s read Azeem’s 662 million rows Parquet dataset into a Dask DataFrame and
    perform a filtering operation to illustrate the empty partition problem.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们读取Azeem的662百万行Parquet数据集到一个Dask DataFrame，并执行过滤操作以说明空分区问题。
- en: 'Read in the data and create the Dask DataFrame:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 读取数据并创建Dask DataFrame：
- en: '[PRE42]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '`ddf.npartitions` shows that the DataFrame has 1,095 partitions:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '`ddf.npartitions`显示DataFrame有1,095个分区：'
- en: '[PRE43]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Here’s how to filter the DataFrame to only include rows with an id greater
    than 1150:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何过滤DataFrame以仅包含id大于1150的行：
- en: '[PRE44]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Run `len(res)` to see that the DataFrame only has 1,103 rows after this filtering
    operation:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`len(res)`以查看在此过滤操作后DataFrame只有1,103行：
- en: '[PRE45]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: This was a big filter, and only a small fraction of the original 662 million
    rows remain.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个大的过滤操作，仅剩下原始662百万行的一小部分。
- en: 'We can run `res.npartitions` to see that the DataFrame still has 1,095 partitions.
    The filtering operation didn’t change the number of partitions:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以运行`res.npartitions`来查看DataFrame仍然有1,095个分区。过滤操作没有改变分区数目：
- en: '[PRE46]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Run `res.map_partitions(len).compute()` to visually inspect how many rows of
    data are in each partition:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`res.map_partitions(len).compute()`来直观地检查每个分区中有多少行数据：
- en: '[PRE47]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: A lot of the partitions are empty and others only have a few rows of data.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 许多分区是空的，其他分区只有少量数据行。
- en: 'Dask often works best with partition sizes of at least 100MB. Let’s repartition
    our data to two partitions and persist it in memory:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，Dask最适合至少100MB的分区大小。让我们将数据重新分区为两个分区并将其持久化到内存中：
- en: '[PRE48]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Subsequent operations on res2 will be really fast because the data is stored
    in memory. `len(res)` takes 57 seconds whereas `len(res2)` only takes 0.3 seconds.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 对`res2`的后续操作将非常快，因为数据存储在内存中。`len(res)`花费了57秒，而`len(res2)`仅花费了0.3秒。
- en: The filtered dataset is so small in this example that you could even convert
    it to a pandas DataFrame with `res3 = res.compute()`. It only takes 0.000011 seconds
    to execute `len(res3)`.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，过滤后的数据集非常小，您甚至可以将其转换为pandas DataFrame，使用`res3 = res.compute()`仅需0.000011秒执行`len(res3)`。
- en: Note
  id: totrans-267
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: '[TIP] You don’t have to filter datasets at the computation engine level. You
    can also filter at the database level and only send a fraction of the data to
    the computation engine.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '[提示] 您无需在计算引擎级别过滤数据集。您也可以在数据库级别进行过滤，并只将部分数据发送到计算引擎。'
- en: Query pushdown
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 查询下推
- en: Query pushdown is when you perform data operations before sending the data to
    the Dask cluster. Part of the work is “pushed down” to the database level.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 查询下推是指在将数据发送到 Dask 集群之前执行数据操作。工作的一部分被“下推”到数据库级别。
- en: 'Here’s the high level process for filtering with a Dask cluster:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是使用 Dask 集群进行过滤的高级流程：
- en: Read all the data from disk into the cluster
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有数据从磁盘读入集群
- en: Perform the filtering operation
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行过滤操作
- en: Repartition the filtered DataFrame
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新分区经过过滤的 DataFrame
- en: Possibly write the result to disk (ETL style workflow) or persist in memory
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能将结果写入磁盘（ETL 类型工作流）或持久化在内存中。
- en: Organizations often need to optimize data storage and leverage query pushdown
    in a manner that’s optimized for their query patterns and latency needs.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 组织通常需要优化数据存储，并以针对其查询模式和延迟需求优化的方式利用查询下推。
- en: Best practices
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最佳实践
- en: Dask makes it easy to filter DataFrames, but you need to be cognizant of the
    implications of big filters.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 可以轻松过滤 DataFrame，但您需要意识到大过滤操作的影响。
- en: After filtering a lot of data, you should consider repartitioning and persisting
    the data in memory.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在过滤大量数据后，您应考虑重新分区并将数据持久化在内存中。
- en: You should also consider filtering at the database level and bypassing cluster
    filtering altogether. Lots of Dask analyses run slower than they should because
    a large filtering operation was performed, and the analyst is running operations
    on a DataFrame with tons of empty partitions.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 您还应考虑在数据库级别进行过滤，并完全绕过集群过滤。许多 Dask 分析的运行速度比预期慢，因为执行了大量的过滤操作，并且分析师在处理具有大量空分区的
    DataFrame 上运行操作。
- en: Setting the Index
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置索引
- en: Indexes are used by normal pandas DataFrames, Dask DataFrames, and many databases
    in general. Indexes let you efficiently find rows that have a certain value, without
    having to scan each row.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 索引被普通的 pandas DataFrame、Dask DataFrame 和许多一般数据库使用。索引让您可以高效地找到具有特定值的行，而无需扫描每一行。
- en: In plain pandas, it means that after a `set_index("col")`, `df.loc["foo"]` is
    faster than `df`[`df.col` `== "foo"`] was before. The loc() uses an efficient
    data structure that only has to check a couple rows to figure out where “foo”
    is. Whereas `df[df.col == "foo"]` has to scan every single row to see which ones
    match.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在普通的 pandas 中，这意味着在`set_index("col")`之后，`df.loc["foo"]`比之前的`df[df.col == "foo"]`更快。loc()
    使用了一个高效的数据结构，只需检查几行即可找出“foo”在哪里。而`df[df.col == "foo"]`则必须扫描每一行以查看哪些匹配。
- en: The thing is, computers are very, very fast at scanning memory, so when you’re
    running pandas computations on one machine, index optimizations aren’t as important.
    But scanning memory across many distributed machines is not fast. So index optimizations
    that you don’t notice much with pandas makes an enormous difference with Dask.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，计算机在扫描内存时非常快，因此在单台机器上运行 pandas 计算时，索引优化并不重要。但在多台分布式计算机上扫描内存却不快。因此，pandas
    中您可能察觉不到的索引优化在 Dask 中会产生巨大差异。
- en: Dask DataFrames Divisions
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Dask DataFrame 分区
- en: Remember that a Dask DataFrame is composed of many pandas DataFrames, potentially
    living on different machines, each one of which we call a partition. Each of these
    partitions is a pandas DataFrame that has its own index.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 要记住，Dask DataFrame 由许多 pandas DataFrame 组成，这些 DataFrame 可能存在于不同的机器上，每个称为一个分区。这些分区是具有自己索引的
    pandas DataFrame。
- en: '![Fig 4 5\. Figure title needed.](Images/how_to_work_with_dask_dataframes_343327_06.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![图 4 5\. 需要图标题。](Images/how_to_work_with_dask_dataframes_343327_06.png)'
- en: Figure 2-6\. Figure title needed.
  id: totrans-288
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-6\. 需要图标题。
- en: Dask DataFrame has its own version of an index for the distributed DataFrame
    as a whole, called `divisions`. The `divisions` are like an index for the indexes—it
    tracks the index bounds for each partition, so you can easily tell which partition
    contains a given value (just like pandas’s index tracks which row will contain
    a given value).
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: Dask DataFrame 对于分布式 DataFrame 有自己的版本索引，称为`divisions`。`divisions` 类似于索引的索引，它跟踪每个分区的索引边界，因此您可以轻松地确定哪个分区包含给定值（就像
    pandas 的索引跟踪哪行将包含给定值一样）。
- en: '![Fig 4 6\. Figure title needed.](Images/how_to_work_with_dask_dataframes_343327_07.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![图 4 6\. 需要图标题。](Images/how_to_work_with_dask_dataframes_343327_07.png)'
- en: Figure 2-7\. Figure title needed.
  id: totrans-291
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-7\. 需要图标题。
- en: When there are millions of rows spread across hundreds of machines, it’s too
    much to track every single row. We just want to get in the right ballpark—which
    machine will hold this value?—and then tell that machine to go find the row itself.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 当数百台机器上分布着数百万行时，跟踪每一行就太多了。我们只想大概知道——哪台机器将持有这个值？——然后告诉那台机器去找到这行数据。
- en: So `divisions` is just a simple list giving the lower and upper bounds of values
    that each partition contains. Using this, Dask does a quick binary search locally
    to figure out which partition contains a given value.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 因此 `divisions` 只是一个简单的列表，给出了每个分区包含的值的下限和上限。利用这一点，Dask 在本地进行快速二分搜索，以确定包含给定值的分区。
- en: Just like with a pandas index, having known divisions lets us change a search
    that would scan every row (`df``[df.col == "foo"]`) to one that quickly goes straight
    to the right place (`df.loc``["foo"]`).
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 就像使用 pandas 索引一样，有了已知的分区，我们可以将一个扫描每一行的搜索（`df``[df.col == "foo"]`）改为一个快速定位到正确位置的搜索（`df.loc``["foo"]`）。
- en: How to Set the Index
  id: totrans-295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何设置索引
- en: 'You can set the index of a Dask DataFrame using the `set_index()` method:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 `set_index()` 方法设置 Dask DataFrame 的索引：
- en: '[PRE49]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Notice that `ddf.set_index("id")` does not specify divisions, so Dask needs
    to go figure them out. To do this, it has to load and compute all of `ddf` immediately
    to look at the distribution of its values. Then, when you later call `.compute``()`,
    it’ll load and compute `ddf` a second time. This is slow in general, and particularly
    bad if the DataFrame already has lots of operations applied to it—all those operations
    also have to run twice.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 `ddf.set_index("id")` 没有指定分区，因此 Dask 需要去找出它们。为此，它必须立即加载和计算整个 `ddf` 来查看其值的分布。然后，当你稍后调用
    `.compute``()` 时，它将再次加载和计算 `ddf`。总体而言，这是慢的，特别是如果 DataFrame 已经应用了许多操作——所有这些操作也必须运行两次。
- en: Instead, when divisions are passed, Dask doesn’t need to compute the whole DataFrame
    to figure them out, which is obviously a lot faster. To pick good divisions, you
    must use your knowledge of the dataset. What range of values is there for the
    column? What sort of general distribution do they follow—a normal bell curve,
    a continuous distribution? Are there known outliers?
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，当传递分区时，Dask 不需要计算整个 DataFrame 来找出它们，这显然快得多。要选择良好的分区，你必须利用对数据集的了解。列的值范围是什么？它们遵循什么样的一般分布——正态钟形曲线，连续分布？是否存在已知的异常值？
- en: 'Another strategy is to let Dask compute the divisions once, then copy-paste
    them to reuse later:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种策略是让 Dask 首次计算分区，然后复制粘贴它们以便稍后重用：
- en: '[PRE50]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: This is especially helpful if you’ll be rerunning a script or notebook on the
    same (or similar) data many times. However, you shouldn’t set divisions if the
    data you’re processing is very unpredictable. In that case, it’s better to spend
    the extra time and let Dask re-compute good divisions each time.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将在相同（或类似）的数据上多次运行脚本或笔记本，这将特别有帮助。然而，如果你处理的数据非常不可预测，那么最好花费额外时间，让 Dask 每次重新计算良好的分区。
- en: When to Set the Index
  id: totrans-303
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 何时设置索引
- en: Since `set_index()` is an expensive operation, you should only run the computation
    when it’ll help subsequent computations run faster.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 `set_index()` 是一个昂贵的操作，你应该仅在它能帮助后续计算更快时才运行它。
- en: 'Here are the main operations that’ll run faster when an index is set:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是当设置索引后将运行得更快的主要操作：
- en: Filtering on the index
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在索引上进行过滤
- en: Joining on the index
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按索引连接
- en: Groupby on the index
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按索引分组
- en: Sophisticated custom code in map_partitions (advanced use case)
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: map_partitions 中的复杂自定义代码（高级用例）
- en: By all means, set indexes whenever it’ll make your analysis faster. Just don’t
    run these expensive computations unnecessarily.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，只要能加速分析，就应该设置索引。只是不要无谓地运行这些昂贵的计算。
- en: 'For example, you shouldn’t always `set_index()` before you `.``loc`. If you
    just need to pull a value out once, it’s not worth the cost of a whole shuffle.
    But if you need to pull lots of values out, then it is. Same with a merge: if
    you’re just merging a DataFrame to another, don’t `set_index()` first (the merge
    will do this internally anyway). But if you’re merging the same DataFrame multiple
    times, then the `set_index()` is worth it.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你不应该总是在 `.``loc` 前使用 `set_index()`。如果你只需要提取一个值，那么完全没必要进行整个洗牌的代价。但如果你需要多次提取值，那么就有意义了。与
    merge 类似：如果只是将一个 DataFrame 与另一个合并，那么不需要先 `set_index()`（合并会在内部进行这个操作）。但如果多次合并同一个
    DataFrame，那么 `set_index()` 是值得的。
- en: As a rule of thumb, you should `set_index()` if you’ll do a merge, `groupby(df.index)`,
    or `.loc` on the re-indexed DataFrame more than once. You may also want to re-index
    your data before writing it to storage in a partitioned format like Parquet. That
    way, when you read the data later, it’s already partitioned the way you want,
    and you don’t have to re-index it every time.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个经验法则，如果你将在重新索引的 DataFrame 上执行 merge、groupby(df.index) 或 .loc 操作超过一次，你应该使用
    `set_index()`。你可能还希望在将数据写入像 Parquet 这样的分区格式存储之前重新索引数据。这样，当你稍后读取数据时，它已经按照你想要的方式进行了分区，并且你不必每次都重新索引它。
- en: Joining Dask DataFrames
  id: totrans-313
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 连接 Dask DataFrames
- en: In this section we’ll look at how to merge Dask DataFrames and discuss important
    considerations when making large joins. We’ll learn how to join a large Dask DataFrame
    to a small pandas DataFrame, how to join two large Dask DataFrames and how to
    structure our joins for optimal performance.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论如何合并 Dask DataFrames 并讨论在进行大型连接时的重要考虑因素。我们将学习如何将一个大型 Dask DataFrame
    与一个小型 pandas DataFrame 进行连接，如何连接两个大型 Dask DataFrames 以及如何为了最佳性能结构化我们的连接。
- en: Join a Dask DataFrame to a pandas DataFrame
  id: totrans-315
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将 Dask DataFrame 连接到 pandas DataFrame
- en: We can join a Dask DataFrame to a small pandas DataFrame by using the
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用以下语法将 Dask DataFrame 连接到一个小型 pandas DataFrame
- en: '`dask.dataframe.merge()`method, similar to the pandas api. Below we execute
    a left join on our Dask DataFrame `ddf` with a small pandas DataFrame `df` that
    contains a boolean value for every name in the dataset:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '`dask.dataframe.merge()` 方法，类似于 pandas API。以下我们执行一个左连接，将我们的 Dask DataFrame
    `ddf` 与一个包含数据集中每个名称布尔值的小型 pandas DataFrame `df` 进行连接：'
- en: '[PRE51]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '![Fig 4 7\. Figure title needed.](Images/how_to_work_with_dask_dataframes_343327_08.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![Fig 4 7\. 需要图标题。](Images/how_to_work_with_dask_dataframes_343327_08.png)'
- en: Figure 2-8\. Figure title needed.
  id: totrans-320
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-8\. 需要图标题。
- en: If you’re working with a small Dask DataFrame instead of a pandas DataFrame,
    you have two options. You can convert it into a pandas DataFrame using compute().
    This will load the DataFrame into memory. Alternatively, if you can’t or don’t
    want to load it into your single machine memory, you can turn the small Dask DataFrame
    into a single partition by using the repartition() method instead. These two operations
    are programmatically equivalent which means there’s no meaningful difference in
    performance between them. See the Compute and Repartitioning sections respectively
    for more details.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是小型 Dask DataFrame 而不是 pandas DataFrame，则有两个选项。你可以使用 compute() 方法将其转换为
    pandas DataFrame。这将加载 DataFrame 到内存中。或者，如果你不能或不想将其加载到单个机器内存中，则可以使用 repartition()
    方法将小型 Dask DataFrame 转换为单个分区。这两个操作在程序上是等效的，这意味着它们在性能上没有显著差异。分别查看 Compute 和 Repartitioning
    部分以获取更多详细信息。
- en: '![Fig 4 8\. Figure title needed.](Images/how_to_work_with_dask_dataframes_343327_09.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![Fig 4 8\. 需要图标题。](Images/how_to_work_with_dask_dataframes_343327_09.png)'
- en: Figure 2-9\. Figure title needed.
  id: totrans-323
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-9\. 需要图标题。
- en: Joining two Dask DataFrames
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 连接两个 Dask DataFrames
- en: 'To join two large Dask DataFrames, you can use the exact same syntax. In this
    case we are specifying the `left_index` and `right_index` keywords to tell Dask
    to use the indices of the two DataFrames as the columns to join on. This will
    join the data based on the timestamp column:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 要连接两个大型 Dask DataFrames，你可以使用完全相同的语法。在这种情况下，我们正在指定 `left_index` 和 `right_index`
    关键字，告诉 Dask 使用两个 DataFrame 的索引作为要连接的列。这将根据时间戳列连接数据：
- en: '[PRE52]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '![Fig 4 9\. Figure title needed.](Images/how_to_work_with_dask_dataframes_343327_10.png)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![Fig 4 9\. 需要图标题。](Images/how_to_work_with_dask_dataframes_343327_10.png)'
- en: Figure 2-10\. Figure title needed.
  id: totrans-328
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-10\. 需要图标题。
- en: However, merging two large Dask DataFrames requires careful consideration of
    your data structure and the final result you’re interested in. Joins are expensive
    operations, especially in a distributed computing context. Understanding both
    your data and your desired end result can help you set up your computations efficiently
    to optimize performance. The most important consideration is whether and how to
    set your DataFrame’s index before executing the join.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，合并两个大型 Dask DataFrames 需要仔细考虑你的数据结构和你感兴趣的最终结果。连接是昂贵的操作，特别是在分布式计算环境中。了解你的数据和你期望的最终结果可以帮助你有效地设置计算以优化性能。最重要的考虑因素是在执行连接之前是否以及如何设置你的
    DataFrame 索引。
- en: Considerations when joining Dask DataFrames
  id: totrans-330
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 连接 Dask DataFrames 时需要考虑的因素
- en: 'Joining two DataFrames can be either very expensive or very cheap depending
    on the situation. It is cheap in the following cases:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 连接两个 DataFrame 可能非常昂贵，也可能非常便宜，这取决于情况。在以下情况下，连接是便宜的：
- en: Joining a Dask DataFrame with a Pandas DataFrame
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 Dask DataFrame 与 Pandas DataFrame 进行连接
- en: Joining a Dask DataFrame with another Dask DataFrame of a single partition
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将一个 Dask DataFrame 与另一个单分区的 Dask DataFrame 进行连接
- en: Joining Dask DataFrames along their indexes
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 沿着它们的索引连接 Dask DataFrames
- en: As explained earlier in the book, Dask DataFrames are divided into partitions,
    where each single partition is a pandas DataFrame. Dask can track how the data
    is partitioned (i.e. where one partition starts and the next begins) using a DataFrame’s
    divisions. If a Dask DataFrame’s divisions are known, then Dask knows the minimum
    value of every partition’s index and the maximum value of the last partition’s
    index. This enables Dask to take efficient shortcuts when looking up specific
    values. Instead of searching the entire dataset, it can find out which partition
    the value is in by looking at the divisions and then limit its search to only
    that specific partition. This is called a sorted join.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本书前面所解释的那样，Dask DataFrames 被分为分区，其中每个单个分区是一个 pandas DataFrame。Dask 可以使用 DataFrame
    的 divisions 跟踪数据如何分区（即一个分区从哪里开始，下一个从哪里开始）。如果知道 Dask DataFrame 的 divisions，则 Dask
    知道每个分区索引的最小值和最后一个分区索引的最大值。这使得 Dask 在查找特定值时可以采取高效的快捷方式。它不必搜索整个数据集，而是可以通过查看 divisions
    找出值所在的分区，并限制其搜索范围仅限于该特定分区。这称为排序连接。
- en: If divisions are not known, then Dask will need to move all of your data around
    so that rows with matching values in the joining columns end up in the same partition.
    This is called an unsorted join and it’s an extremely memory-intensive process,
    especially if your machine runs out of memory and Dask will have to read and write
    data to disk instead. This is a situation you want to avoid.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不知道分区，那么 Dask 将需要移动所有数据，使得连接列中具有匹配值的行最终在同一分区。这称为未排序连接，这是一个极耗内存的过程，特别是如果机器内存不足，Dask
    将不得不读写数据到磁盘。这是一个要避免的情况。
- en: If you are planning to run repeated joins against a large Dask DataFrame, it’s
    best to sort the Dask DataFrame using the `set_index()` method first to improve
    performance. See Section 2.5 above for more on the `set_index()`method and `divisions`.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 如果计划对大型 Dask DataFrame 进行重复连接，最好先使用 `set_index()` 方法对 Dask DataFrame 进行排序以提高性能。详见上述第
    2.5 节关于 `set_index()` 方法和 `divisions` 的内容。
- en: It’s good practice to write sorted DataFrames to the Apache Parquet file format
    in order to preserve the index. See the Working with Parquet Section for more
    on the benefits of working with this data format.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 最好将排序后的 DataFrame 写入 Apache Parquet 文件格式，以保留索引。有关使用此数据格式的好处，请参见与 Parquet 一起工作的部分。
- en: Mapping Custom Functions
  id: totrans-339
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 映射自定义函数
- en: This section provides a quick refresher of how we can run custom functions on
    pandas DataFrames and then demonstrates how we can parallelize these operations
    on Dask DataFrames. Dask makes it easy to apply custom functions on each of the
    underlying pandas DataFrames it contains.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 本节快速刷新了如何在 pandas DataFrame 上运行自定义函数，然后演示了如何在 Dask DataFrame 上并行化这些操作。Dask 可以轻松地在其包含的每个基础
    pandas DataFrame 上应用自定义函数。
- en: pandas apply refresher
  id: totrans-341
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: pandas 的 apply 刷新器
- en: pandas apply lets you apply a function to each row in a pandas DataFrame. Let’s
    create a function that’ll look at all the columns, find the max value, find the
    min value, and compute the difference for each row in the pandas DataFrame.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 的 apply 允许您将函数应用于 pandas DataFrame 中的每一行。让我们创建一个函数，该函数将查看所有列，找到最大值，找到最小值，并计算
    pandas DataFrame 中每一行的差异。
- en: 'Start by creating a pandas DataFrame with three columns and three rows of data:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 首先创建一个具有三列和三行数据的 pandas DataFrame：
- en: '[PRE53]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Here are the contents of the DataFrame:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是 DataFrame 的内容：
- en: '[PRE54]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Define a `minmax` function that will take the difference between the max value
    and the min value and then use the pandas `apply()` method to run this function
    on each row in the DataFrame:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个 `minmax` 函数，该函数将取最大值和最小值之差，然后使用 pandas 的 `apply()` 方法在 DataFrame 的每一行上运行此函数：
- en: '[PRE55]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Here’s the result:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 这是结果：
- en: '[PRE56]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: The pandas apply() function returns the result as a Series object.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 的 apply() 函数将结果作为 Series 对象返回。
- en: '[PRE57]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Let’s look at how to parallelize this function with Dask.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用 Dask 并行化这个函数。
- en: Parallelizing pandas apply with map_partitions
  id: totrans-354
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 `map_partitions` 并行化 pandas 的 apply
- en: 'Convert the pandas DataFrame to a Dask DataFrame with two partitions:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 将 pandas DataFrame 转换为具有两个分区的 Dask DataFrame：
- en: '[PRE58]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Create a minmax2 function that wraps the original minmax function and use map_partitions()
    to run it on each partition in a Dask DataFrame.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个 minmax2 函数，它包装了原始的 minmax 函数，并使用 map_partitions() 在 Dask DataFrame 的每个分区上运行它。
- en: '[PRE59]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Map the function across all partitions in the Dask DataFrame:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Dask DataFrame 的所有分区上映射函数：
- en: '[PRE60]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: map_partitions() runs the pandas apply() operation on all the partitions in
    parallel, so map_partitions() is a great way to parallelize a pandas apply() operation
    and make it run faster.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '`map_partitions()`在所有分区上并行运行pandas的`apply()`操作，因此`map_partitions()`是并行化pandas
    `apply()`操作并使其运行更快的好方法。'
- en: Note
  id: totrans-362
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: '[CAUTION] In the toy examples, the Dask version may not run faster than the
    pandas version because the data is so small and the Dask scheduler incurs minimal
    overhead. Remember that Dask is meant for large datasets where the benefits of
    parallel computing (processing speed) outweigh the costs (overhead of)'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '[注意] 在玩具示例中，Dask版本可能不会比pandas版本运行得更快，因为数据量太小，而且Dask调度器的开销很小。请记住，Dask是为大数据集设计的，其中并行计算（处理速度）的优势超过了成本（开销）。'
- en: Calculating memory usage of a DataFrame with map_partitions
  id: totrans-364
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用`map_partitions`计算DataFrame的内存使用量
- en: 'Let’s look at two approaches for calculating the memory for each partition
    of a 662 million row dataset. You don’t need to actually use this approach to
    compute the memory usage of a DataFrame because Dask has a built-in memory_usage()
    method that’s more convenient. Nonetheless, this example is a good way to demonstrate
    the power of the map_partitions() method:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何计算662百万行数据集每个分区的内存。你不需要实际使用这种方法来计算DataFrame的内存使用量，因为Dask有一个更方便的内置`memory_usage()`方法。尽管如此，这个示例是演示`map_partitions()`方法强大之处的好方法：
- en: '[PRE61]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: This computation takes 124 seconds on a 5-node cluster.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 这个计算在一个5节点集群上需要124秒。
- en: Dask has a `sizeof()` function that estimates the size of each partition and
    runs faster.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: Dask有一个`sizeof()`函数，可以估算每个分区的大小并运行得更快。
- en: '[PRE62]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: This takes 92 seconds to run, which is 21% faster than `memory_usage()` on the
    same dataset.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 这需要92秒才能运行，比相同数据集上的`memory_usage()`快21%。
- en: The `sizeof()` results are an approximation, but they’re pretty close as you
    can see.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '`sizeof()`的结果是一个近似值，但它们非常接近，正如你所见。'
- en: groupby aggregations
  id: totrans-372
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分组聚合
- en: This section explains how to perform groupby() aggregations with Dask DataFrames.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了如何在Dask DataFrames中执行`groupby()`聚合操作。
- en: You’ll learn how to perform groupby() operations with one and many columns.
    You’ll also learn how to compute aggregations like sum, mean, and count.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 你将学习如何使用一个或多个列执行`groupby()`操作。你还将学习如何计算诸如总和、平均值和计数等聚合操作。
- en: After you learn the basic syntax, we’ll discuss the best practices when performing
    groupby() operations.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习基本语法之后，我们将讨论执行`groupby()`操作时的最佳实践。
- en: Dask DataFrame groupby sum
  id: totrans-376
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Dask DataFrame的分组求和
- en: Let’s read a sample dataset from S3 into a Dask DataFrame to perform some sample
    groupby computations. We will use Coiled to launch a Dask computation cluster
    with 5 nodes.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从S3中读取一个示例数据集到一个Dask DataFrame中，以执行一些示例的groupby计算。我们将使用Coiled启动一个包含5个节点的Dask计算集群。
- en: '[PRE63]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Let’s groupby the values in the `id` column and then sum the values in the x
    column.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按`id`列中的值进行分组，然后对x列中的值求和。
- en: '[PRE64]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: You can also use an alternative syntax and get the same result.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用另一种语法，并得到相同的结果。
- en: '[PRE65]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: agg takes a more complex code path in Dask, so you should generally stick with
    the simple syntax unless you need to perform multiple aggregations.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 在Dask中，`agg`采用了更复杂的代码路径，因此除非需要执行多个聚合操作，否则通常应坚持使用简单的语法。
- en: Dask DataFrame groupby for a single column is pretty straightforward. Let’s
    look at how to groupby with multiple columns.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单列的Dask DataFrame groupby非常简单。让我们看看如何按多列进行分组。
- en: Dask DataFrame groupby multiple columns
  id: totrans-385
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Dask DataFrame多列分组
- en: 'Here’s how to group by the `id` and `name` columns and then sum the values
    in x:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 这是如何按`id`和`name`列进行分组，然后对x列中的值求和：
- en: '[PRE66]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: You can pass a list to the Dask groupby method to group by multiple columns.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将一个列表传递给Dask的groupby方法，以按多列进行分组。
- en: Now let’s look at how to perform multiple aggregations after grouping.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何在分组之后执行多个聚合操作。
- en: Dask groupby multiple aggregations
  id: totrans-390
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Dask多列分组
- en: 'Here’s how to group by `id` and compute the sum of x and the mean of `y`:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是如何按`id`分组并计算x的总和和`y`的平均值：
- en: '[PRE67]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '![Fig 4 10\. Figure title needed.](Images/how_to_work_with_dask_dataframes_343327_11.png)'
  id: totrans-393
  prefs: []
  type: TYPE_IMG
  zh: '![图4 10\. 需要图标题。](Images/how_to_work_with_dask_dataframes_343327_11.png)'
- en: Figure 2-11\. Figure title needed.
  id: totrans-394
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-11\. 需要图标题。
- en: You can pass a dictionary to the `agg` method to perform different types of
    aggregations.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将一个字典传递给`agg`方法，以执行不同类型的聚合。
- en: Let’s turn our attention to how Dask implements groupby computations. Specifically,
    let’s look at how Dask changes the number of partitions in the DataFrame when
    a groupby operation is performed. This is important because you need to manually
    set the number of partitions properly when the aggregated DataFrame is large.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们关注 Dask 如何实现 groupby 计算。具体来说，让我们看看当执行 groupby 操作时，Dask 如何改变 DataFrame 的分区数量。这很重要，因为当聚合的
    DataFrame 很大时，您需要手动设置正确的分区数量。
- en: How Dask groupby impacts npartitions
  id: totrans-397
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Dask groupby 如何影响 npartitions
- en: Dask doesn’t know the contents of your DataFrame ahead of time. So it can’t
    know how many groups the groupby operation will produce. By default, it assumes
    you’ll have relatively few groups, so the number of rows is reduced so significantly
    that the result will fit comfortably in a single partition.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 事先不知道 DataFrame 的内容。因此，它不知道 groupby 操作将产生多少组。默认情况下，它假设您将拥有相对较少的组，因此行数显著减少，结果可以轻松地适合单个分区。
- en: However, when your data has many groups, you’ll need to tell Dask to split the
    results into multiple partitions in order to not overwhelm one unlucky worker.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，当您的数据具有许多组时，您需要告诉 Dask 将结果分割成多个分区，以避免压倒一个不幸的工作节点。
- en: Dask DataFrame groupby will return a DataFrame with a single partition by default.
    Let’s look at a DataFrame, confirm it has multiple partitions, run a groupby operation,
    and then observe how the resulting DataFrame only has a single partition.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Dask DataFrame groupby 将返回一个具有单个分区的 DataFrame。让我们看一个 DataFrame，确认它有多个分区，运行一个
    groupby 操作，然后观察结果 DataFrame 只有一个分区。
- en: 'The DataFrame that we’ve been querying has 1,095 partitions:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在查询的 DataFrame 有 1,095 个分区：
- en: '[PRE68]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Now let’s run a groupby operation on the DataFrame and see how many partitions
    are in the result.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们对 DataFrame 运行一个 groupby 操作，看看结果有多少分区。
- en: '[PRE69]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Dask will output groupby results to a single partition Dask DataFrame by default.
    A single partition DataFrame is all that’s needed in most cases. `groupby` operations
    usually reduce the number of rows in a DataFrame significantly so they can be
    held in a single partition DataFrame.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Dask 将 groupby 结果输出到单个分区的 Dask DataFrame。在大多数情况下，单个分区的 DataFrame 就足够了。`groupby`
    操作通常会显著减少 DataFrame 中的行数，因此它们可以保存在单个分区的 DataFrame 中。
- en: 'You can set the `split_out` argument to return a DataFrame with multiple partitions
    if the result of the groupby operation is too large for a single partition Dask
    DataFrame:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 groupby 操作的结果太大，以至于单个分区的 Dask DataFrame 无法容纳，您可以设置 `split_out` 参数以返回具有多个分区的
    DataFrame：
- en: '[PRE70]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: In this example, `split_out` was set to two, so the groupby operation results
    in a DataFrame with two partitions. **The onus is on you to properly set the**
    **`split_out`** **size when the resulting** **DataFrame** **is large.**
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，`split_out` 设置为两个，因此 groupby 操作的结果是一个具有两个分区的 DataFrame。**当生成的** **DataFrame**
    **很大时，您有责任正确设置** **`split_out`** **大小。**
- en: Performance considerations
  id: totrans-409
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 性能考虑
- en: Dask DataFrames are divided into many partitions, each of which is a pandas
    DataFrame. Dask performs groupby operations by running groupby on each of the
    individual pandas DataFrames and then aggregating all the results. The Dask DataFrame
    parallel execution of groupby on multiple subsets of the data makes it more scalable
    than pandas and often quicker too.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: Dask DataFrames 被分成许多分区，每个分区都是一个 pandas DataFrame。Dask 通过在每个单独的 pandas DataFrame
    上运行 groupby，然后聚合所有结果来执行 groupby 操作。Dask DataFrame 在多个数据子集上并行执行 groupby，使其比 pandas
    更具可扩展性，通常也更快。
- en: Memory usage
  id: totrans-411
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存使用情况
- en: This section shows you how to compute the memory usage of a Dask DataFrame and
    how to develop a partitioning strategy based on the distribution of your data.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 这一节展示了如何计算 Dask DataFrame 的内存使用情况，并根据数据的分布制定分区策略。
- en: Dask DataFrames distribute data in partitions, so computations can be run in
    parallel. Each partition in a Dask DataFrame is a pandas DataFrame. This section
    explains how to measure the amount of data in each Dask partition. Intelligently
    distributing the data across partitions is important for performance.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: Dask DataFrames 将数据分布在分区中，因此可以并行运行计算。Dask DataFrame 中的每个分区都是一个 pandas DataFrame。本节解释了如何测量每个
    Dask 分区中的数据量。智能地分布数据在分区中对性能至关重要。
- en: There aren’t hard-and-fast rules on optimal partition sizes. It depends on the
    computing power of the nodes in your cluster and the analysis you’re running.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 没有关于最佳分区大小的硬性规则。这取决于集群中节点的计算能力和您正在运行的分析。
- en: A general rule of thumb is to target 100 MB of data per memory partition in
    a cluster. This section shows you how to measure the distribution of data in your
    cluster so you know when and if you need to repartition.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 一个经验法则是在集群中的每个内存分区目标为 100 MB 的数据。本节展示了如何测量集群中数据的分布，以便知道何时以及是否需要重新分区。
- en: 'Here’s what you’ll learn in this section:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是本节中你将学到的内容：
- en: Calculation memory usage of a small Dask DataFrame
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算小型 Dask DataFrame 的内存使用情况
- en: Memory usage of a large Dask DataFrame
  id: totrans-418
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 大型 Dask DataFrame 的内存使用情况
- en: Filtering can cause partition imbalances
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 过滤可能会导致分区不平衡
- en: Assessing when a Dask DataFrame’s memory usage is unevenly distributed
  id: totrans-420
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估 Dask DataFrame 的内存使用是否不均匀分布
- en: Fixing imbalances with repartitioning
  id: totrans-421
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用重新分区修复不均衡
- en: Other ways to compute memory usage by partition
  id: totrans-422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 其他计算分区内存使用的方法
- en: Memory usage of small Dask DataFrames
  id: totrans-423
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小型 Dask DataFrame 的内存使用情况
- en: 'Create a small Dask DataFrame with two partitions:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个包含两个分区的小型 Dask DataFrame：
- en: '[PRE71]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Print the data in each of the partitions:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 打印每个分区中的数据：
- en: '[PRE72]'
  id: totrans-427
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Use the pandas `memory_usage` method to print the bytes of memory used in each
    column of the first partition:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 pandas 的 `memory_usage` 方法来打印第一个分区每列使用的内存字节数：
- en: '[PRE73]'
  id: totrans-429
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Print the total memory used by each partition in the DataFrame with the Dask
    `memory_usage_per_partition` method:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Dask 的 `memory_usage_per_partition` 方法打印 DataFrame 中每个分区使用的总内存：
- en: '[PRE74]'
  id: totrans-431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Both of these partitions are tiny because the entire DataFrame only contains
    six rows of data.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个分区都很小，因为整个 DataFrame 只包含六行数据。
- en: 'If deep is set to False then the memory usage of the object columns is not
    counted:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 deep 设置为 False，则不计算对象列的内存使用量：
- en: '[PRE75]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Calculating the memory usage of object columns is slow, so you can set `deep`
    to `False` and make the computation run faster. We care about how much memory
    all the columns are using, so our examples use `deep=True`.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 计算对象列的内存使用是很慢的，所以你可以将 `deep` 设置为 `False`，使计算更快速运行。我们关心所有列使用了多少内存，所以我们的示例使用了
    `deep=True`。
- en: Memory usage of large Dask DataFrame
  id: totrans-436
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大型 Dask DataFrame 的内存使用情况
- en: Let’s calculate the memory for each partition of the dataset.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算数据集每个分区的内存。
- en: '[PRE76]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: The DataFrame has 1,095 partitions and each partition has 57 MB of data.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame 有 1,095 个分区，每个分区有 57 MB 的数据。
- en: The data is evenly balanced across each partition in the DataFrame. There aren’t
    lots of tiny, empty, or huge partitions. You probably don’t need to repartition
    this DataFrame because all the memory partitions are reasonably sized and the
    data is evenly distributed.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame 中每个分区之间的数据是均匀分布的。没有大量的小型、空的或者巨大的分区。你可能不需要重新分区这个 DataFrame，因为所有的内存分区都是合理大小且数据分布均匀。
- en: Tips on managing memory
  id: totrans-441
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于管理内存的提示
- en: Let’s see how Azeem can reduce the memory usage of the large Parquet DataFrame,
    so his analysis doesn’t consume as many resources and runs more efficiently.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 Azeem 如何减少大型 Parquet DataFrame 的内存使用，以便他的分析不消耗太多资源并且运行更有效。
- en: Azeem can limit the amount of memory his analysis takes by sending less data
    to the cluster or by using data types that are more memory efficient.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: Azeem 可以通过发送更少的数据到集群或使用更节省内存的数据类型来限制其分析占用的内存量。
- en: 'Let’s take a look at how much memory Azeem’s Parquet dataset uses in memory:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 Azeem 的 Parquet 数据集在内存中使用了多少内存：
- en: '[PRE77]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'The dataset takes 58 GB in memory. Let’s see how much memory it takes, by column:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集在内存中占用 58 GB。让我们看看每列占用多少内存：
- en: '[PRE78]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'The name column is by far the most memory-greedy, requiring more memory than
    all the other columns combined. Let’s take a look at the data types for this DataFrame
    to see why `name` is taking so much memory:'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 名称列远比其他所有列加起来都需要更多的内存。让我们查看此 DataFrame 的数据类型，看看为什么 `name` 列占用如此之多内存：
- en: '[PRE79]'
  id: totrans-449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '`name` is an object column, which isn’t surprising because object columns are
    notoriously memory hungry. Let’s change the `name` column to a different type
    and see if that helps.'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: '`name` 是一个对象列，这并不奇怪，因为对象列以消耗大量内存而著称。让我们将 `name` 列更改为其他类型，看看是否有所帮助。'
- en: String types
  id: totrans-451
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 字符串类型
- en: 'Let’s change the name column to be a string and see how that impacts memory
    usage of the DataFrame:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将名字列改为字符串，并看看这如何影响 DataFrame 的内存使用：
- en: '[PRE80]'
  id: totrans-453
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: The name column only takes 5.76 GiB in memory now. It used to take 38.45 GiB.
    That’s a significant 6.7x memory reduction. Avoid object columns whenever possible
    as they are very memory inefficient.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 现在 `name` 列只占用了 5.76 GiB 的内存。之前它占用了 38.45 GiB。这是显著的 6.7 倍内存减少。尽量避免使用对象列，因为它们非常浪费内存。
- en: Smaller numeric types
  id: totrans-455
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更小的数值类型
- en: You can also use smaller numeric types for columns that don’t require 64 bits.
    Let’s look at the values in the id column and see if it really needs to be typed
    as an int64.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以对不需要 64 位的列使用更小的数值类型。让我们查看 id 列中的值，并看看它是否真的需要被类型化为 int64。
- en: 'Here’s how to compute the min and max values in the id column:'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 这是如何计算 id 列的最小值和最大值：
- en: '[PRE81]'
  id: totrans-458
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'We don’t need int64s for holding such small values. Let’s switch to int16 and
    quantify memory savings:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 对于保存这样小的值，我们不需要 int64。让我们转换为 int16 并量化内存节省：
- en: '[PRE82]'
  id: totrans-460
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: The id column used to take 4.93 GiB and now only takes 1.23 GiB. That’s 4 times
    smaller, which isn’t surprising because 16 bit numbers are four times smaller
    than 64 bit numbers.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: id 列以前占用了 4.93 GiB，现在只占用了 1.23 GiB。这是缩小了 4 倍，这并不奇怪，因为16位数字比64位数字小四倍。
- en: We’ve seen how column types can reduce the memory requirements of a DataFrame.
    Now let’s look at how loading less data to the cluster also can reduce memory
    requirements.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到列类型如何减少 DataFrame 的内存需求。现在让我们看看如何将更少的数据加载到集群中也可以减少内存需求。
- en: Column pruning
  id: totrans-463
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 列修剪
- en: Suppose you need to run a query that only requires the x column and won’t use
    the data in the other columns.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你需要运行一个只需要 x 列并且不会使用其他列数据的查询。
- en: 'You don’t need to read all the data into the cluster, you can just read the
    x column with column pruning:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 您不需要将所有数据读入集群，您可以只读取带有列修剪的 x 列：
- en: '[PRE83]'
  id: totrans-466
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'This DataFrame only contains the x column and the index:'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 此 DataFrame 仅包含 x 列和索引：
- en: '[PRE84]'
  id: totrans-468
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: Only storing a fraction of the columns obviously makes the overall memory footprint
    much lower.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 仅存储部分列显然会大大降低整体内存占用。
- en: Predicate pushdown filters
  id: totrans-470
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 谓词推送过滤器
- en: Parquet files also let you skip entire row groups for some queries which also
    limits the amount of data that’s sent to the computation cluster.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 文件还可以让您跳过某些查询的整个行组，这也限制了发送到计算集群的数据量。
- en: 'Here’s how to read the data and only include row groups that contain at least
    one value with an id greater than 1170:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 这是如何读取数据，并且只包括至少包含一个 id 大于 1170 的值的行组的方法：
- en: '[PRE85]'
  id: totrans-473
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'There isn’t much data with an id greater than 1170 for this dataset, so this
    predicate pushdown filter greatly reduces the size of the data in memory:'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此数据集，id 大于 1170 的数据并不多，因此这个谓词推送过滤器极大地减少了内存中数据的大小。
- en: '[PRE86]'
  id: totrans-475
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: The more row groups that are excluded by the predicate pushdown filters, the
    smaller the DataFrame in memory.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 谓词推送过滤器排除的行组越多，内存中的 DataFrame 就越小。
- en: Converting to number columns with to_numeric
  id: totrans-477
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 to_numeric 转换为数字列
- en: This section explains how to convert Dask DataFrame object columns to floating
    point columns with `to_numeric()` and why it’s more flexible than `astype()` in
    certain situations. This design pattern is especially useful when you’re working
    with data in text based file formats like CSV. You’ll often have to read in numeric
    columns stored in CSV files as object columns because of messy data and then convert
    the numeric columns to floating point values to null out the bad data. You can’t
    perform numerical operations on columns that are typed as objects. You can use
    the tactics outlined in this section for data munging in an extract, transform,
    & load (ETL) pipeline.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 本节说明了如何使用 `to_numeric()` 将 Dask DataFrame 对象列转换为浮点列，以及在某些情况下为什么它比 `astype()`
    更灵活。当您使用文本文件格式如 CSV 中的数据时，这种设计模式尤其有用。您通常会将存储在 CSV 文件中的数值列作为对象列读入，因为数据混乱，然后将数值列转换为浮点值以清空坏数据。您不能对类型为对象的列执行数值操作。您可以在数据抓取、转换和加载（ETL）管道中使用本节中概述的策略。
- en: Cleaning data is often the first step of a data project. Luckily Dask has great
    helper methods like `to_numeric` that make it easy to clean the data and properly
    type the columns in your DataFrames.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 清理数据通常是数据项目的第一步。幸运的是，Dask 提供了像 `to_numeric` 这样的很好的辅助方法，使得清理数据和正确类型化列变得容易。
- en: Converting object column with to_numeric
  id: totrans-480
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 转换带有 to_numeric 的对象列
- en: 'Let’s look at a simple example with a DataFrame that contains an invalid string
    value in a column that should only contain numbers. Let’s start by creating a
    DataFrame with `nums` and `letters` columns:'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个简单的例子，DataFrame 中包含一个应该只包含数字的列中的无效字符串值。让我们首先创建一个带有 `nums` 和 `letters`
    列的 DataFrame：
- en: '[PRE87]'
  id: totrans-482
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Now let’s print the contents of the DataFrame so it’s easy to visualize:'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们打印 DataFrame 的内容，这样就可以轻松可视化：
- en: '[PRE88]'
  id: totrans-484
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: Notice that row 4 in the nums column has the value “hi”. That’s a string value
    that Python cannot convert into a numerical value.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 nums 列中的第 4 行具有值“hi”。这是 Python 无法将其转换为数值的字符串值。
- en: 'Let’s look at the data types of the columns and see that Dask is treating both
    `nums` and `letters` as object type columns:'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看列的数据类型，并看到 Dask 正将 `nums` 和 `letters` 都视为对象类型的列：
- en: '[PRE89]'
  id: totrans-487
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Let’s convert the `nums` column to be a number column with `to_numeric`:'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 `to_numeric` 将 `nums` 列转换为数值列：
- en: '[PRE90]'
  id: totrans-489
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: Dask has conveniently nulled out the “hi” value in row 4 to be NaN. Nulling
    out values that cannot be easily converted to numerical values is often what you’ll
    want. Alternatively, you can set `errors=“raise”` to raise an error when a value
    can’t be cast to numeric dtype.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 已经方便地将第 4 行中的“hi”值设为 NaN。将无法轻松转换为数值的值设为 NaN 是你经常需要的。或者，你可以设置 `errors=“raise”`
    来在无法将值转换为数值类型时引发错误。
- en: Limitations of astype
  id: totrans-491
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '`astype` 的限制'
- en: Many beginning Dask users tend to use the `astype` method to convert object
    columns to numeric columns. This has important limitations.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 许多初学者倾向于使用 `astype` 方法将对象列转换为数值列。这有重要的限制。
- en: 'Let’s create another DataFrame to see when `astype` can be used to convert
    from object columns to numeric columns and when it falls short:'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建另一个 DataFrame，看看在什么情况下 `astype` 可以将对象列转换为数值列，以及什么情况下它不起作用：
- en: '[PRE91]'
  id: totrans-494
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'You can run `ddf2.dtypes` to see that both `n1` and `n2` are object columns:'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以运行 `ddf2.dtypes` 来查看 `n1` 和 `n2` 都是对象列：
- en: '[PRE92]'
  id: totrans-496
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '`n2` is an object type column because it contains string and float values.'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: '`n2` 是一个对象类型的列，因为它包含字符串和浮点值。'
- en: 'Let’s convert `n2` to be a `float64` column using `astype`:'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 `astype` 将 `n2` 转换为 `float64` 列：
- en: '[PRE93]'
  id: totrans-499
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '`astype` can convert `n2` to be a float column without issue.'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: '`astype` 可以将 `n2` 转换为一个浮点数列，没有问题。'
- en: 'Now let’s try to convert `n1` to be a float column with `astype`:'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试使用 `astype` 将 `n1` 转换为浮点列：
- en: '[PRE94]'
  id: totrans-502
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'This errors out with the following error message:'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 这会导致以下错误消息：
- en: '[PRE95]'
  id: totrans-504
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '`astype` raises errors when columns contain string values that cannot be converted
    to numbers. It doesn’t coerce string values to `NaN`.'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 当列包含无法转换为数字的字符串值时，`astype` 会引发错误。它不会将字符串值强制转换为 `NaN`。
- en: '`to_numeric` also has the same default behavior and this code will error out
    as well:'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: '`to_numeric` 也具有相同的默认行为，这段代码也会出错：'
- en: '[PRE96]'
  id: totrans-507
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'You need to set errors="coerce” to successfully invoke `to_numeric`:'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要设置 `errors="coerce"` 才能成功调用 `to_numeric`：
- en: '[PRE97]'
  id: totrans-509
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: Best practices for numeric columns
  id: totrans-510
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数值列的最佳实践
- en: Dask makes it easy to convert object columns into number columns with `to_numeric`.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 可以轻松地使用 `to_numeric` 将对象列转换为数值列。
- en: '`to_numeric` is customizable with different error behavior when values cannot
    be converted to numbers. You can coerce these values to `NaN`, raise an error,
    or ignore these values. Choose the behavior that works best for your application.'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: '`to_numeric` 可以根据不同的错误行为定制化，当值无法转换为数字时。你可以将这些值强制转换为 `NaN`，引发错误或忽略这些值。选择最适合你的应用程序的行为。'
- en: It’s good practice to make sure all your numeric columns are properly typed
    before performing your analysis, so you don’t get weird downstream bugs.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行分析之前，确保所有数值列都正确地进行了类型转换是一个好习惯，这样你就不会遇到奇怪的下游错误。
- en: Vertically union Dask DataFrames
  id: totrans-514
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 垂直合并 Dask DataFrame
- en: This section teaches you how to union Dask DataFrames vertically with `concat`
    and the important related technical details. Vertical concatenation combines DataFrames
    like the SQL UNION operator combines tables which is common when joining datasets
    for reporting and machine learning. It’s useful whenever you have two tables with
    identical schemas that you’d like to combine into a single DataFrame.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 本节教你如何使用 `concat` 垂直合并 Dask DataFrames 及其相关重要技术细节。垂直合并将 DataFrames 合并，类似于 SQL
    中 UNION 运算符合并表格，这在连接用于报告和机器学习的数据集时很常见。当你有两个具有相同模式的表格，并想将它们合并为一个 DataFrame 时，这非常有用。
- en: The tactics outlined in this section will help you combine two DataFrame with
    the same, or similar, schemas into a single DataFrame. It’s a useful design pattern
    to have in your toolkit.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中提到的策略将帮助你将两个具有相同或相似模式的 DataFrame 合并为一个单一的 DataFrame。这是你工具箱中有用的设计模式。
- en: 'Here’s how this section on vertical concatenations is organized:'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是关于垂直合并的章节的组织方式：
- en: Concatenating DataFrames with identical schemas / dtypes
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 合并具有相同模式/数据类型的 DataFrame
- en: Interleaving partitions to maintain divisions integrity
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交错分区以保持分区完整性
- en: Concatenating DataFrames with different schemas
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 合并具有不同模式的 DataFrame
- en: Concatenating large DataFrames
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 合并大型 DataFrame
- en: Concatenate DataFrames with identical schemas
  id: totrans-522
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 合并具有相同模式的 DataFrame
- en: 'Create two Dask DataFrames with identical schemas:'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 创建两个具有相同模式的 Dask DataFrame：
- en: '[PRE98]'
  id: totrans-524
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'Now concatenate both the DataFrames into a single DataFrame:'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 现在将两个 DataFrame 合并为一个单一的 DataFrame：
- en: '[PRE99]'
  id: totrans-526
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'Print the contents of `ddf3` to verify it contains all the rows from `ddf1`
    and `ddf2`:'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 打印`ddf3`的内容以验证它包含了`ddf1`和`ddf2`的所有行：
- en: '[PRE100]'
  id: totrans-528
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: '`ddf1` has two partitions and `ddf2` has one partition. `ddf1` and `ddf2` are
    combined to `ddf3`, which has three total partitions:'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: '`ddf1`有两个分区，`ddf2`有一个分区。将`ddf1`和`ddf2`合并成`ddf3`，总共有三个分区：'
- en: '[PRE101]'
  id: totrans-530
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: Dask can use information on divisions to speed up certain queries. The creation
    of `ddf3` above wiped out information about DataFrame divisions. Let’s see how
    we can interleave partitions when concatenating DataFrames to avoid losing divisions
    data.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: Dask可以使用分区信息加快某些查询。上面创建的`ddf3`清除了有关数据框分区的信息。让我们看看如何在连接数据框时交错分区以避免丢失分区数据。
- en: Interleaving partitions
  id: totrans-532
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 交错分区
- en: Let’s revisit our example with a focus on DataFrame divisions to illustrate
    how `concat` wipes out the DataFrame divisions by default.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新审视我们的示例，重点关注数据框分区，以说明`concat`默认情况下如何清除数据框的分区。
- en: 'Recreate the `ddf1` DataFrame and look at its divisions:'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 重新创建`ddf1`数据框并查看其分区：
- en: '[PRE102]'
  id: totrans-535
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'Here’s how to interpret this divisions output:'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何解释这些分区输出的方式：
- en: The first partition has index values between 0 and 2
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个分区的索引值介于0到2之间
- en: The second partitions has index values between 3 and 5
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个分区的索引值介于3到5之间
- en: 'Let’s print every partition of the DataFrame to visualize the actual data and
    reason about the division’s values:'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打印数据框的每个分区，以可视化实际数据并推理分区的值：
- en: '[PRE103]'
  id: totrans-540
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'Let’s recreate `ddf2` and view its divisions too:'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新创建`ddf2`并查看其分区：
- en: '[PRE104]'
  id: totrans-542
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: '`ddf2` has a single partition with index values between zero and one:'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: '`ddf2`有一个单独的分区，索引值介于零和一之间：'
- en: '[PRE105]'
  id: totrans-544
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'Let’s concatenate the DataFrames and see what happens with the divisions:'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们连接这些数据框，并查看分区的情况：
- en: '[PRE106]'
  id: totrans-546
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: Dask has lost all information about divisions for `ddf3` and won’t be able to
    use divisions related optimizations for subsequent computations.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: Dask对`ddf3`的分区信息已全部丢失，并且将无法对后续计算使用分区相关的优化。
- en: 'You can set `interleave_partitions` to `True` when concatenating DataFrames
    to avoid losing information about divisions:'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在连接数据框时将`interleave_partitions`设置为`True`，以避免丢失关于分区的信息：
- en: '[PRE107]'
  id: totrans-549
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'Take a look at how the data is distributed across partitions in `ddf3_interleave`:'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 查看`ddf3_interleave`中数据在分区中的分布方式：
- en: '[PRE108]'
  id: totrans-551
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: Dask can optimize certain computations when divisions exist. Set `interleave_partitions`
    to `True` if you’d like to take advantage of these optimizations after concatenating
    DataFrames.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 当存在分区时，Dask可以优化某些计算。如果希望在连接数据框后利用这些优化，请将`interleave_partitions`设置为`True`。
- en: Concatenating DataFrames with different schemas
  id: totrans-553
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 连接具有不同模式的数据框
- en: You can also concatenate DataFrames with different schemas. Let’s create two
    DataFrames with different schemas, concatenate them, and see how Dask behaves.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以连接具有不同模式的数据框。让我们创建两个具有不同模式的数据框，将它们连接起来，并查看Dask的行为。
- en: 'Start by creating the two DataFrames:'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 首先创建两个数据框：
- en: '[PRE109]'
  id: totrans-556
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'Concatenate the DataFrames and print the result:'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 连接数据框并打印结果：
- en: '[PRE110]'
  id: totrans-558
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: Dask fills in the missing values with `NaN` to make the concatenation possible.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: Dask使用`NaN`填充缺失的值以便进行连接。
- en: Concatenating large DataFrames
  id: totrans-560
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 连接大型数据框
- en: Lets create a Dask cluster and concatenate the 20-year DataFrame with a DataFrame
    that contains 311 million row of timeseries data from January 1, 1990 till December
    31, 1999\. This section demonstrates that `concat` can scale to multi-node workflows.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个Dask集群，并将包含1990年1月1日至1999年12月31日的311百万行时间序列数据的数据框与包含20年数据的数据框进行连接。本节展示了`concat`可以扩展到多节点工作流程。
- en: 'Create a 5-node Coiled cluster and read in a Parquet dataset into a DataFrame:'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个5节点的Coiled集群，并将Parquet数据集读入数据框：
- en: '[PRE111]'
  id: totrans-563
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: 'Run `ddf1990s.head()` to visually inspect the contents of the DataFrame:'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`ddf1990s.head()`以视觉检查数据框的内容：
- en: '[PRE112]'
  id: totrans-565
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: '![Fig 4 11\. Figure title needed.](Images/how_to_work_with_dask_dataframes_343327_12.png)'
  id: totrans-566
  prefs: []
  type: TYPE_IMG
  zh: '![Fig 4 11\. 需要图标题。](Images/how_to_work_with_dask_dataframes_343327_12.png)'
- en: Figure 2-12\. Figure title needed.
  id: totrans-567
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-12\. 需要图标题。
- en: 'Let’s run some analytical queries on `ddf1990s` to better understand the data
    it contains:'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对`ddf1990s`运行一些分析查询，以更好地理解其中包含的数据：
- en: '[PRE113]'
  id: totrans-569
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: 'Now let’s look at our original Parquet dataset with data from January 1, 2000
    till December 31, 2020:'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看一下我们的原始Parquet数据集，其中包含从2000年1月1日至2020年12月31日的数据：
- en: '[PRE114]'
  id: totrans-571
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: 'Concatenate the two DataFrames and inspect the contents of the resulting DataFrame:'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 连接这两个数据框并检查结果数据框的内容：
- en: '[PRE115]'
  id: totrans-573
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: These DataFrames were concatenated without `interleave_partitions=True` and
    the divisions metadata was not lost like we saw earlier.
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 这些DataFrames是在没有`interleave_partitions=True`的情况下连接的，并且分区的元数据没有像我们之前看到的那样丢失。
- en: The DataFrames in this example don’t have any overlapping divisions, so you
    don’t need to set `interleave_partitions=True`.
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，DataFrames没有重叠的分区，因此不需要设置`interleave_partitions=True`。
- en: Writing Data with Dask DataFrames
  id: totrans-576
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Dask DataFrames写入数据
- en: You’ve completed all your analyses and want to store the results sitting in
    your Dask DataFrames.You can write the contents of Dask DataFrames to CSV files,
    Parquet files, HDF files, SQL tables and convert them into other Dask collections
    like Dask Bags, Dask Arrays and Dask Delayed. In this chapter we will cover the
    first 2 options (CSV and Parquet). See the Dask documentation for more information
    on the other options.
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 您已完成所有分析工作，并希望存储在您的Dask DataFrames中的结果。您可以将Dask DataFrames的内容写入CSV文件、Parquet文件、HDF文件、SQL表，并将它们转换为其他Dask集合，如Dask
    Bags、Dask Arrays和Dask Delayed。在本章中，我们将介绍前两个选项（CSV和Parquet）。有关其他选项的更多信息，请参阅Dask文档。
- en: Note
  id: totrans-578
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: '[NOTE] Depending on the analysis you are running, your results may at this
    point be small enough to fit into local memory. For example if you’ve run groupby
    aggregations on your dataset. In this case, you should stop using Dask and switch
    back to normal pandas by calling `results.compute()`.'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: '[注意] 根据您正在运行的分析，此时您的结果可能足够小以适应本地内存。例如，如果您在数据集上运行了groupby聚合。在这种情况下，您应该停止使用Dask，并通过调用`results.compute()`切换回普通的pandas。'
- en: 'There are a lot of different keyword arguments to the `to_csv` and `to_parquet`
    functions, more than we’re going to cover here. However there are a few essential
    kwargs you should know about and we’re going to cover them below: options for
    file compression, writing to multiple vs single files, and partitioning on specific
    columns.'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: '`to_csv`和`to_parquet`函数有许多不同的关键字参数，超出我们将在此处讨论的范围。但是，有几个必要的关键字参数您应该知道，我们将在下面讨论它们：文件压缩选项、写入多个文件与单个文件以及在特定列上分区。'
- en: Let’s start with the basics.
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从基础知识开始。
- en: 'You can write a Dask DataFrame out to a CSV or Parquet file using the `to_csv`
    and `to_parquet` method, respectively:'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`to_csv`和`to_parquet`方法将Dask DataFrame写入CSV或Parquet文件，分别如下：
- en: '[PRE116]'
  id: totrans-583
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: Dask DataFrames can be written to a variety of different sources and file formats.
    We’ll talk about working with different file formats in more detail in Chapter
    7.
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将Dask DataFrames写入各种不同的源和文件格式。我们将在第7章中更详细地讨论使用不同文件格式的工作。
- en: File Compression
  id: totrans-585
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文件压缩
- en: 'Both the to_csv and to_parquet writers have multiple options for file compression.
    The to_csv writer allows the following compression options: gzip, bz2 and xz.'
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: '`to_csv`和`to_parquet`写入器都具有多个文件压缩选项。`to_csv`写入器允许以下压缩选项：gzip、bz2和xz。'
- en: 'The to_parquer writer defaults to ‘snappy’. It also accepts other Parquet compression
    options like gzip, and blosc. You can also pass a dictionary to this keyword argument
    to map columns to compressors, for example: `{"name": "gzip", "values": "snappy"}`.
    We recommend using the default “snappy” compressor.'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: '`to_parquet`写入器默认为‘snappy’。它还接受其他Parquet压缩选项，如gzip和blosc。您还可以将字典传递给此关键字参数，以将列映射到压缩器，例如：`{"name":
    "gzip", "values": "snappy"}`。我们建议使用默认的“snappy”压缩器。'
- en: 'to_csv: single_file'
  id: totrans-588
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: to_csv：single_file
- en: Dask DataFrames consist of multiple partitions. By default, writing a Dask DataFrame
    to CSV will write each partition to a separate CSV file.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: Dask DataFrames由多个分区组成。默认情况下，将Dask DataFrame写入CSV将每个分区写入单独的CSV文件。
- en: 'Let’s illustrate. We’ll start by creating a partitioned Dask DataFrame:'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来说明一下。我们将从创建一个分区化的Dask DataFrame开始：
- en: '[PRE117]'
  id: totrans-591
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: 'Let’s inspect the contents of the first partition:'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查第一个分区的内容：
- en: '[PRE118]'
  id: totrans-593
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: 'Now let’s write the whole Dask DataFrame out to CSV with default settings:'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用默认设置将整个Dask DataFrame写入CSV：
- en: '[PRE119]'
  id: totrans-595
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: 'Now switch to a terminal, `cd` into the data.csv folder and `ls` the contents:'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 现在切换到终端，`cd`进入data.csv文件夹，然后`ls`查看内容：
- en: '[PRE120]'
  id: totrans-597
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: 'We clearly see two partial CSV files here. To confirm, let’s load in just the
    0.part file and inspect the contents:'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 我们清楚地看到这里有两个部分的CSV文件。为了确认，让我们只加载0.part文件并检查内容：
- en: '[PRE121]'
  id: totrans-599
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: 'As expected, the content matches that of the first partition of our original
    Dask DataFrame ddf **except** for the fact that Dask seems to have duplicated
    the Index column. This happens because, just like in pandas, to_csv writes the
    index as a separate column by default. You can change this setting by setting
    the index keyword to False, just like you would in pandas:'
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，内容与我们原始的 Dask DataFrame ddf 的第一个分区匹配，**但** Dask 似乎复制了索引列。这是因为与 pandas
    类似，`to_csv` 默认将索引作为单独的列写入。您可以通过将 index 关键字设置为 False 来更改此设置，就像在 pandas 中一样：
- en: '[PRE122]'
  id: totrans-601
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: Note
  id: totrans-602
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: '[NOTE] The dask.dataframe.to_csv writer, just like the dask.dataframe.read_csv
    reader, accepts many of the same keyword arguments as their pandas equivalents.'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: '[注意] `dask.dataframe.to_csv` 的写入器和 `dask.dataframe.read_csv` 的读取器接受许多与 pandas
    等效的关键字参数。'
- en: 'Fortunately, Dask makes it easy for you to read multiple CSV files located
    in a single directory into a Dask DataFrame, using the * character as a glob string:'
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Dask 让您可以轻松地将位于单个目录中的多个 CSV 文件读入 Dask DataFrame，使用 `*` 字符作为通配符字符串：
- en: '[PRE123]'
  id: totrans-605
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: Having your data scattered over multiple CSV files may seem impractical at first,
    especially if you’re used to working with CSV files in pandas. But remember that
    you’re likely using Dask because either your dataset is too large to fit into
    memory or you want to enjoy the benefits of parallelism – or both! Having the
    CSV file split up into smaller parts means Dask can process the file in parallel
    and maximize performance.
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 将你的数据分散在多个 CSV 文件中一开始可能看起来不实际，特别是如果你习惯于在 pandas 中使用 CSV 文件。但请记住，你可能正在使用 Dask
    是因为你的数据集太大而无法装入内存，或者你想要享受并行处理的好处 — 或者两者兼而有之！将 CSV 文件分割成较小的部分意味着 Dask 可以并行处理文件并最大化性能。
- en: If, however, you want to write your data out to a single CSV file, you can change
    the default setting by setting `single_file` to `True``:`
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果您想将数据写入单个 CSV 文件，您可以通过将 `single_file` 设置为 `True` 来更改默认设置：
- en: '[PRE124]'
  id: totrans-608
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: Note
  id: totrans-609
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: '[CAUTION] The to_csv writer clobbers existing files in the event of a name
    conflict. Be careful whenever you’re writing to a folder with existing data, especially
    if you’re using the default file names.'
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: '[警告] 当文件名冲突时，`to_csv` 写入器会覆盖现有文件。无论何时写入包含现有数据的文件夹时，请特别小心，特别是如果使用默认文件名。'
- en: 'to_parquet: engine'
  id: totrans-611
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '`to_parquet: engine`'
- en: The engine keyword can be used to choose which Parquet library to use for writing
    Dask DataFrames to Parquet. We strongly recommend using the pyarrow engine, which
    will be the default starting from
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: '`engine` 关键字可以用于选择将 Dask DataFrame 写入 Parquet 时要使用的 Parquet 库。我们强烈建议使用 `pyarrow`
    引擎，从而成为默认设置。'
- en: 'to_parquet: partition_on'
  id: totrans-613
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '`to_parquet: partition_on`'
- en: Dask DataFrame’s to_parquet writer allows you to partition the resulting Parquet
    files according to the values of a particular column.
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: Dask DataFrame 的 `to_parquet` 写入器允许您根据特定列的值对生成的 Parquet 文件进行分区。
- en: 'Let’s illustrate with an example. We’ll Create a Dask DataFrame with letter
    and number columns and write it out to disk, partitioning on the letter column:'
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举个例子。我们将创建一个具有字母和数字列的 Dask DataFrame，并将其写入磁盘，以字母列进行分区：
- en: '[PRE125]'
  id: totrans-616
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: 'Here are the files that are written to disk:'
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是写入磁盘的文件：
- en: '[PRE126]'
  id: totrans-618
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: Organizing the data in this directory structure lets you easily skip files for
    certain read operations. For example, if you only want the data where the letter
    equals a, then you can look in the tmp/partition/1/letter=a directory and skip
    the other Parquet files.
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据组织在这种目录结构中可以让您在某些读取操作中轻松跳过文件。例如，如果您只想要字母等于 a 的数据，那么您可以查看 `tmp/partition/1/letter=a`
    目录并跳过其他 Parquet 文件。
- en: The letter column is referred to as the partition key in this example.
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，字母列被称为分区键。
- en: 'Here’s how to read the data in the letter=a disk partition:'
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是如何读取字母为 a 的磁盘分区中的数据：
- en: '[PRE127]'
  id: totrans-622
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: Dask is smart enough to apply the partition filtering based on the filters argument.
    Dask only reads the data from output/partition_on/letters=a into the DataFrame
    and skips all the files in other partitions.
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 足够智能，可以根据 filters 参数应用分区过滤。Dask 只会从 `output/partition_on/letters=a` 中读取数据到
    DataFrame 中，并跳过其他分区中的所有文件。
- en: Note
  id: totrans-624
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: '[CAUTION] Disk partitioning improves performance for queries that filter on
    the partition key. It usually hurts query performance for queries that don’t filter
    on the partition key.'
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: '[警告] 磁盘分区对于过滤分区键的查询可以提高性能。但通常会降低不过滤分区键的查询性能。'
- en: 'Example query that runs faster on disk partitioned lake:'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 在磁盘分区的湖中运行更快的示例查询：
- en: '[PRE128]'
  id: totrans-627
  prefs: []
  type: TYPE_PRE
  zh: '[PRE128]'
- en: 'Example query that runs slower on disk partitioned lake:'
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 在磁盘分区的湖中运行较慢的示例查询：
- en: '[PRE129]'
  id: totrans-629
  prefs: []
  type: TYPE_PRE
  zh: '[PRE129]'
- en: The performance drag from querying a partitioned lake without filtering on the
    partition key depends on the underlying filesystem. Unix-like filesystems are
    good at performing file listing operations on nested directories. So you might
    not notice much of a performance drag on your local machine.
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有过滤分区键的情况下查询分区湖可能会导致性能下降，这取决于底层文件系统。类Unix文件系统在执行嵌套目录的文件列表操作时表现良好。因此，在本地机器上可能不会注意到性能下降。
- en: Cloud-based object stores, like AWS S3, are not Unix-like filesystems. They
    store data as key value pairs and are slow when listing nested files with a wildcard
    character (a.k.a. globbing).
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: 基于云的对象存储，如AWS S3，并非类Unix文件系统。它们将数据存储为键值对，在使用通配符字符（也称为globbing）列出嵌套文件时速度较慢。
- en: You need to carefully consider your organization’s query patterns when evaluating
    the costs/benefits of disk partitioning. If you are always filtering on the partition
    key, then a partitioned lake is typically best. Disk partitioning might not be
    worth it if you only filter on the partitioned key sometimes.
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估磁盘分区的成本/效益时，您需要仔细考虑组织的查询模式。如果您总是在分区键上进行过滤，则分区湖通常是最佳选择。如果您只是偶尔在分区键上进行过滤，则可能不值得进行磁盘分区。
- en: Other Keywords
  id: totrans-633
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他关键字
- en: You are now familiar with the most important keyword arguments to the to_csv
    and to_parquer writers. These tools will help you write your Dask DataFrames to
    the two most popular tabular file formats effectively and with maximum performance.
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在熟悉了to_csv和to_parquet写入器的最重要的关键字参数。这些工具将帮助您有效地将您的Dask DataFrames写入到两种最流行的表格文件格式，并实现最大性能。
- en: See the Dask documentation for more information on the other keyword arguments.
    See Chapter 7 for more details on working with other file formats.
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 有关其他关键字参数的更多信息，请参阅Dask文档。有关使用其他文件格式的详细信息，请参阅第7章。
- en: Summary
  id: totrans-636
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter concludes the Dask DataFrame section of this book. To recap, in
    Chapter 2 we worked through a real-world end-to-end example to illustrate how
    Dask DataFrames help you process massive amounts of tabular data to gain valuable
    insights. Chapter 3 took a step back to explain the architecture of Dask DataFrame
    and how it transcends the scalability limitations of pandas. Finally, this chapter
    has given you in-depth tools and a collection of best practices for processing
    tabular data with Dask. You could think of this final chapter as the definitive
    guide for working with Dask DataFrames, presented to you by the team that has
    spent years building, maintaining and optimizing Dask. Taken together, these three
    chapters contain everything you need to take the Dask DataFrame car off the beaten
    track and forge your own trails forward.
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束了本书的Dask DataFrame部分。回顾一下，在第2章中，我们通过一个真实的端到端示例来说明了Dask DataFrames如何帮助您处理大量表格数据以获得有价值的见解。第3章退后一步解释了Dask
    DataFrame的架构以及如何超越pandas的可扩展性限制。最后，本章为您提供了深入的工具和一系列处理Dask表格数据的最佳实践。您可以将这最后一章视为Dask
    DataFrames的权威指南，由多年来构建、维护和优化Dask的团队呈现给您。总之，这三章内容涵盖了您在使用Dask DataFrame时所需的一切，使您能够在未开发的领域中尝试并开辟自己的道路。
- en: 'In this chapter, you learned:'
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您学到了：
- en: How to read data into Dask DataFrames from various sources and file formats
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何从各种来源和文件格式读取数据到Dask DataFrames
- en: How to execute common data-processing operations with Dask DataFrames, including
    groupby aggregations, joins, converting data types, and mapping custom Python
    functions over your data.
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用Dask DataFrames执行常见的数据处理操作，包括分组聚合、连接、数据类型转换以及在数据上映射自定义Python函数。
- en: How to optimize your data structure for maximum performance by setting the index,
    repartitioning, and managing memory usage.
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何通过设置索引、重新分区和管理内存使用来优化数据结构，以实现最大性能。
- en: How to write your processed data out to CSV and Parquet.
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何将处理过的数据写入CSV和Parquet文件中。
- en: The next two chapters will cover working with array data using Dask Array. Chapter
    5 will work through a real-world end-to-end example, and Chapter 6 will combine
    an explanation of the Dask Array architecture with our definitive collection of
    best practices.
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的两章将介绍如何使用Dask Array处理数组数据。第5章将通过一个真实的端到端示例进行讲解，而第6章将结合对Dask Array架构的解释和我们权威的最佳实践集合。
