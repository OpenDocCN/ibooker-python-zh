- en: Chapter 15\. Data Engineering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 15 章。数据工程
- en: Data science may be the sexiest job of the 21st century, but the field is evolving
    rapidly into different job titles. Data scientist has been too crude a description
    for a whole series of tasks. As of 2020, two jobs that can pay the same or more
    are data engineer and machine learning engineer.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学可能是21世纪最性感的工作，但这个领域正在迅速演变成不同的职位头衔。数据科学家已经过于粗糙地描述了一系列任务。截至2020年，两个可以支付相同或更高工资的工作是数据工程师和机器学习工程师。
- en: Even more surprising is the vast number of data engineer roles needed to support
    a traditional data scientist. It is somewhere between three to five data engineers
    to one data scientist.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 更令人惊讶的是，支持传统数据科学家所需的大量数据工程师角色。大约需要三到五名数据工程师才能支持一个数据科学家。
- en: What is happening? Let’s look at it from another angle. Let’s pretend we are
    writing headlines for a newspaper and want to say something eye-catching. We could
    say, “CEO is the sexiest job for the rich.” There are few CEOs, just like there
    are few NBA stars, just like there are few professional actors who are making
    a living. For every CEO, how many people are working to make that CEO successful?
    This last statement is content-free and meaningless, like “water is wet.”
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 发生了什么？让我们从另一个角度来看待。假设我们正在为一家报纸写头条新闻，想要说一些吸引眼球的事情。我们可以说，“CEO 是富人最性感的工作。”CEO 很少，就像
    NBA 明星很少，像是靠演艺为生的职业演员很少一样。每个 CEO 背后，有多少人在努力使他们成功？这个陈述与内容空洞、毫无意义，就像“水是湿的”一样。
- en: This statement isn’t to say that you can’t make a living as a data scientist;
    it is more a critique of the logistics behind the statement. There is a huge demand
    for skills in data, and they range from DevOps to machine learning to communication.
    The term data scientist is nebulous. Is it a job or a behavior? In a way, it is
    a lot like the word DevOps. Is DevOps a job, or is it a behavior?
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这个陈述并不是说你不能以数据科学家的身份谋生；这更多是对这种说法背后逻辑的批评。在数据技能方面存在巨大的需求，从DevOps到机器学习再到沟通，都有涉及。数据科学家这个术语是模糊的。它是一种工作还是一种行为？在某种程度上，它很像DevOps这个词。DevOps是一种工作，还是一种行为？
- en: In looking at job posting data and salary data, it appears the job market is
    saying there is an apparent demand for actual roles in data engineering and machine
    learning engineering. This is because those roles perform identifiable tasks.
    A data engineer task could be creating a pipeline in the cloud that collects both
    batch and streaming data and then creates APIs to access that data and schedule
    those jobs. This job is not a squishy task. It works, or it doesn’t.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看职位发布数据和薪资数据时，似乎市场正在表明对数据工程和机器学习工程实际角色存在显著需求。这是因为这些角色执行可识别的任务。数据工程师的任务可能是在云中创建收集批处理和流处理数据的管道，然后创建API以访问该数据并安排这些作业。这项工作不是模糊的任务。它要么有效，要么无效。
- en: Likewise, a machine learning engineer builds machine learning models and deploys
    them in a way that they are maintainable. This job is also not squishy. An engineer
    can do data engineering or machine learning engineering though, and still exhibit
    behaviors attributed to data science and DevOps. Today is an exciting time to
    be involved in data, as there are some considerable opportunities to build complex
    and robust data pipelines that feed into other complex and powerful prediction
    systems. There is an expression that says, “you can never be too rich or too thin.”
    Likewise, with data, you can never have too much DevOps or data science skills.
    Let’s dive into some DevOps-flavored ideas for data engineering.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，机器学习工程师建立机器学习模型并以可维护的方式部署它们。这个工作也不含糊。一个工程师可以做数据工程或机器学习工程，但仍然表现出与数据科学和DevOps相关的行为。现在是参与数据领域的激动人心时刻，因为有一些重要的机会可以建立复杂而强大的数据管道，这些管道供给其他复杂而强大的预测系统。有一句话说，“你永远不能太富有或太瘦。”同样，对于数据来说，你永远不能拥有太多的DevOps或数据科学技能。让我们深入一些DevOps风格的数据工程思想。
- en: Small Data
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 小数据
- en: Toolkits are an exciting concept. If you call a plumber to your house, they
    arrive with tools that help them be more effective than you could be at a task.
    If you hire a carpenter to build something at your house, they also have a unique
    set of tools that help them perform a task in a fraction of the time you could.
    Tools are essential to professionals, and DevOps is no exception.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 工具包是一个令人兴奋的概念。如果你叫水管工来你家，他们会带着工具，帮助他们比你更有效地完成任务。如果你雇用木工在你家建造东西，他们也会有一套独特的工具，帮助他们在比你更短的时间内完成任务。工具对专业人士来说至关重要，DevOps
    也不例外。
- en: In this section, the tools of data engineering outline themselves. These tools
    include small data tasks like reading and writing files, using `pickle,` using
    `JSON,` and writing and reading `YAML` files. Being able to master these formats
    is critical to be the type of automator who can tackle any task and turn it into
    a script. Tools for Big Data tasks are also covered later in the chapter. It discusses
    distinctly different tools than the tools used for small data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，数据工程的工具概述了它们自己。这些工具包括读写文件、使用`pickle`、使用`JSON`、写入和读取`YAML`文件等小数据任务。掌握这些格式对于成为能够处理任何任务并将其转化为脚本的自动化人员是至关重要的。后面还涵盖了大数据任务的工具。它讨论了与小数据使用不同的工具明显不同的工具。
- en: What is Big Data and what is small data then? One easy way to figure the distinction
    out is the laptop test. Does it work on your laptop? If it doesn’t, then it is
    Big Data. A good example is Pandas. Pandas require between 5 to 10 times the amount
    of RAM as the dataset. If you have a 2-GB file and you are using Pandas, most
    likely your laptop won’t work.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是大数据，什么是小数据？一个简单的区分方法是笔记本电脑测试。它在你的笔记本上能运行吗？如果不能，那么它就是大数据。一个很好的例子是 Pandas。Pandas
    需要的 RAM 量是数据集的 5 到 10 倍。如果你有一个 2 GB 的文件，并且你正在使用 Pandas，很可能你的笔记本无法运行。
- en: Dealing with Small Data Files
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理小数据文件
- en: If there was a single defining trait of Python, it would be a relentless pursuit
    of efficiency in the language. A typical Python programmer wants to write just
    enough code to get a task done but wants to stop at the point where the code becomes
    unreadable or terse. Also, a typical Python programmer will not want to write
    boilerplate code. This environment has led to a continuous evolution of useful
    patterns.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 Python 有一个单一的定义特性，那将是语言中效率的不懈追求。一个典型的 Python 程序员希望编写足够的代码来完成任务，但在代码变得难以理解或简洁时停止。此外，一个典型的
    Python 程序员也不愿意编写样板代码。这种环境促使有用模式的持续演变。
- en: One example of an active pattern is using the `with` statement to read and write
    files. The `with` statement handles the boring boilerplate parts of closing a
    file handle after the work has completed. The `with` statement is also used in
    other parts of the Python language to make tedious tasks less annoying.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`with`语句来读写文件的一个活跃模式的例子。`with`语句处理了烦人的样板部分，即在工作完成后关闭文件句柄。`with`语句还在 Python
    语言的其他部分中使用，使烦琐的任务不那么令人讨厌。
- en: Write a File
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 写一个文件
- en: 'This example shows that writing a file using the `with` statement automatically
    closes the file handle upon execution of a code block. This syntax prevents bugs
    that can occur quickly when the handle is accidentally not closed:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子展示了使用`with`语句写入文件时，执行代码块后会自动关闭文件句柄。这种语法可以防止因为意外未关闭句柄而导致的 bug：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The output of the file reads like this:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 文件的输出如下所示：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Read a File
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读一个文件
- en: 'The `with` context also is the recommended way to read a file. Notice that
    using `readlines()` uses line breaks to return a lazily evaluated iterator:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '`with`上下文也是推荐的读取文件的方式。注意，使用`readlines()`方法使用换行符返回一个惰性评估的迭代器：'
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In practice, this means that you can handle large log files by using generator
    expressions and not worry about consuming all of the memory on your machine.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，这意味着你可以通过使用生成器表达式处理大型日志文件，而不必担心消耗机器上的所有内存。
- en: Generator Pipeline to Read and Process Lines
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成器管道用于读取和处理行
- en: 'This code is a generator function that opens a file and returns a generator:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码是一个生成器函数，打开一个文件并返回一个生成器：
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, this generator is used to create a pipeline to perform operations line
    by line. In this example, the line converts to a lowercase string. Many other
    actions could be chained together here, and it would be very efficient because
    it is only using the memory necessary to process a line at a time:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，这个生成器被用来创建一个管道，逐行执行操作。在这个例子中，将行转换为小写字符串。这里可以链式连接许多其他操作，而且非常高效，因为只使用了处理一行数据所需的内存：
- en: '[PRE5]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This is the output of the pipeline:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这是管道的输出：
- en: '[PRE6]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In practice, this means that files that are effectively infinite because they
    are so large could still be processed if the code works in a way that exits when
    it finds a condition. For example, perhaps you need to find a customer ID across
    terabytes of log data. A generator pipeline could look for this customer ID and
    then exit the processing at the first occurrence. In the world of Big Data, this
    is no longer a theoretical problem.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，这意味着那些因为太大而实际上是无限的文件，如果代码能够在找到条件时退出，则仍然可以处理。例如，也许你需要在数千兆字节的日志数据中找到一个客户
    ID。生成器管道可以寻找这个客户 ID，然后在第一次出现时退出处理。在大数据的世界中，这不再是一个理论性的问题。
- en: Using YAML
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 YAML
- en: YAML is becoming an emerging standard for DevOps-related configuration files.
    It is a human-readable data serialization format that is a superset of JSON. It
    stands for “YAML Ain’t Markup Language.” You often see YAML in build systems such
    as [AWS CodePipeline](https://oreil.ly/WZnIl), [CircleCI](https://oreil.ly/0r8cK),
    or PaaS offerings such as [Google App Engine](https://oreil.ly/ny_TD).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: YAML 正在成为与 DevOps 相关的配置文件的新兴标准。它是一种人类可读的数据序列化格式，是 JSON 的超集。它代表“YAML 不是一种标记语言。”
    你经常会在像 [AWS CodePipeline](https://oreil.ly/WZnIl)，[CircleCI](https://oreil.ly/0r8cK)
    这样的构建系统，或者像 [Google App Engine](https://oreil.ly/ny_TD) 这样的 PaaS 提供中看到 YAML。
- en: 'There is a reason YAML is so often used. There is a need for a configuration
    language that allows rapid iteration when interacting with highly automated systems.
    Both a nonprogrammer and a programmer can intuitively figure out how to edit these
    files. Here is an example:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: YAML 如此经常被使用是有原因的。需要一种配置语言，允许在与高度自动化的系统交互时进行快速迭代。无论是非程序员还是程序员都可以直观地了解如何编辑这些文件。以下是一个例子：
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output written to disk looks like this:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 写入磁盘的输出如下所示：
- en: '[PRE8]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The takeway is that it makes it trivial to serialize a Python data structure
    into a format that is easy to edit and iterate on. Reading this file back is just
    two lines of code.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 结论是，它使得将 Python 数据结构序列化为易于编辑和迭代的格式变得非常简单。将此文件读回的代码仅需两行。
- en: '[PRE9]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output then can be pretty printed:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然后可以对输出进行漂亮的打印：
- en: '[PRE10]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Big Data
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大数据
- en: Data has been growing at a rate faster than the growth of computer processing
    power. To make things even more interesting, Moore’s Law, which states that speed
    and capability of computers can be expected to double every two years, effectively
    ceased to apply around 2015, according to Dr. David Patterson at UC Berkeley.
    CPU speed is only increasing around 3% per a year now.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的增长速度比计算机处理能力的增长速度更快。更有趣的是，摩尔定律认为计算机的速度和能力每两年可以翻一番，但根据加州大学伯克利分校的 David Patterson
    博士的说法，这种增长在 2015 年左右就停止适用了。CPU 速度现在每年只增长约 3%。
- en: New methods of dealing with Big Data are necessary. Some of the new methods,
    include using ASICs like GPUs, tensor processing units (TPUs), and as well as
    AI and data platforms provided by cloud vendors. On the chip level, this means
    that a GPU could be the ideal target for a complex IT process instead of a CPU.
    Often this GPU is paired together with a system that can deliver a distributed
    storage mechanism that allows both distributed computing and distributed disk
    I/O. An excellent example of this is Apache Spark, Amazon SageMaker, or the Google
    AI Platform. All of them can utilize ASICs (GPU, TPU, and more), plus distributed
    storage along with a management system. Another example that is more low level
    is Amazon Spot Instance deep learning AMIs with Amazon Elastic File System (EFS)
    mount points.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 处理大数据的新方法是必需的。一些新方法包括使用像 GPU、张量处理单元（TPU）等 ASICs，以及云供应商提供的 AI 和数据平台。在芯片级别上，这意味着
    GPU 可能是复杂 IT 过程的理想目标，而不是 CPU。通常，这个 GPU 与能够提供分布式存储机制的系统配对，该机制允许分布式计算和分布式磁盘 I/O。一个很好的例子是
    Apache Spark，Amazon SageMaker 或 Google AI 平台。它们都可以利用 ASICs（GPU、TPU 等），以及分布式存储和管理系统。另一个更低级别的例子是
    Amazon Spot 实例深度学习 AMIs，配有 Amazon 弹性文件系统（EFS）挂载点。
- en: For a DevOps professional, this means a few things. First, it means that special
    care when delivering software to these systems makes sense. For example, does
    the target platform have the correct GPU drivers? Are you deploying via containers?
    Is this system going to use distributed GPU processing? Is the data mostly batch,
    or is it streaming? Thinking about these questions up front can go a long way
    to ensuring the correct architecture.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个DevOps专业人员，这意味着几件事情。首先，这意味着在将软件交付到这些系统时需要特别注意。例如，目标平台是否有正确的GPU驱动程序？你是通过容器部署吗？这个系统是否将使用分布式GPU处理？数据主要是批处理还是流处理？提前考虑这些问题可以确保选择正确的架构。
- en: One problem with buzzwords like AI, Big Data, cloud, or data scientist is that
    they mean different things to different people. Take data scientist for example.
    In one company it could mean someone who generates business intelligence dashboards
    for the sales team, and in another company, it could mean someone who is developing
    self-driving car software. Big Data has a similar context issue; it can mean many
    different things depending on whom you meet. Here is one definition to consider.
    Do you need different software packages to handle data on your laptop than in
    production?
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 像AI、大数据、云或数据科学家这样的流行词存在一个问题，即它们对不同的人有不同的含义。以数据科学家为例。在一家公司，它可能意味着为销售团队生成业务智能仪表板的人，而在另一家公司，它可能意味着正在开发自动驾驶汽车软件的人。大数据也存在类似的语境问题；它可以根据你遇到的人的不同而有许多不同的含义。这里有一个考虑的定义。你是否需要不同的软件包来处理你笔记本电脑上的数据和生产环境中的数据？
- en: An excellent example of a “small data” tool is the Pandas package. According
    to the author of the Pandas package, it can take between 5 and 10 times the amount
    of RAM as the size of the file used. In practice, if your laptop has 16 GB of
    RAM and you open a 2-GB CSV file, it is now a Big Data problem because your laptop
    may not have enough RAM, 20 GB, to work with the file. Instead, you may need to
    rethink how to handle the problem. Perhaps you can open a sample of the data,
    or truncate the data to get around the problem initially.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很好的“小数据”工具的典范是Pandas包。根据Pandas包的作者，它可能需要比使用的文件大小多5到10倍的RAM。实际上，如果你的笔记本电脑有16
    GB的RAM，并且打开了一个2 GB的CSV文件，那么现在就变成了一个大数据问题，因为你的笔记本电脑可能没有足够的RAM（20 GB）来处理这个文件。相反，你可能需要重新思考如何处理这个问题。也许你可以打开数据的样本，或者截断数据以首先解决问题。
- en: 'Here is an example of this exact problem and a workaround. Let’s say you are
    supporting data scientists that keep running into Pandas out-of-memory errors
    because they are using files too large for Pandas. One such example is the Open
    Food Facts [dataset from Kaggle](https://oreil.ly/w-tmA). When uncompressed, the
    dataset is over 1 GB. This problem fits precisely into the sweet spot of where
    Pandas could struggle to process it. One thing you can do is use the Unix `shuf`
    command to create a shuffled sample:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个确切问题及其解决方法的例子。假设你正在支持数据科学家，他们因为使用了对于Pandas来说太大的文件而经常遇到内存不足的错误。其中一个例子是来自Kaggle的Open
    Food Facts[数据集](https://oreil.ly/w-tmA)。解压后，数据集超过1 GB。这个问题正好符合Pandas可能难以处理的情况。你可以做的一件事是使用Unix的`shuf`命令创建一个打乱的样本：
- en: '[PRE11]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In a little under two seconds, the file can be cut down to a manageable size.
    This approach is preferable to simply using heads or tails, because the samples
    are randomly selected. This problem is significant for a data science workflow.
    Also, you can inspect the lines of the file to see what you are dealing with first:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 不到两秒钟，文件就可以被削减到一个可以处理的大小。这种方法比简单地使用头或尾部更可取，因为样本是随机选择的。这对数据科学工作流程非常重要。此外，你可以检查文件的行以先了解你要处理的内容：
- en: '[PRE12]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The source file is about 350,000 lines, so grabbing 100,000 shuffled lines
    is approximately a third of the data. This task can be confirmed by looking at
    the transformed file. It shows 272 MB, around one-third of the size of the original
    1-GB file:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 源文件大约有350,000行，因此获取100,000个打乱的行大约占据了数据的三分之一。这个任务可以通过查看转换后的文件来确认。它显示272 MB，大约是原始1
    GB文件大小的三分之一：
- en: '[PRE13]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This size is more much manageable by Pandas, and this process could then be
    turned into an automation workflow that creates randomized sample files for Big
    Data sources. This type of process is just one of many particular workflows that
    Big Data demands.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这种大小对Pandas来说更容易管理，并且这个过程可以转化为一个自动化工作流程，为大数据源创建随机样本文件。这种类型的过程只是大数据要求的许多特定工作流程之一。
- en: Another definition of Big Data is by McKinsey, who defined Big Data in 2011
    as “datasets whose size is beyond the ability of typical database software tools
    to capture, store, manage, and analyze.” This definition is reasonable as well,
    with the slight modification that it isn’t just database software tools; it is
    any tool that touches data. When a tool that works well on a laptop, such as Pandas,
    Python, MySQL, deep learning/machine learning, Bash, and more, fails to perform
    conventionally due to the size or velocity (rate of change) of the data, it is
    now a Big Data problem. Big Data problems require specialized tools, and the next
    section dives into this requirement.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个关于大数据的定义来自麦肯锡，他们在2011年将大数据定义为“数据集，其大小超出典型数据库软件工具捕捉、存储、管理和分析的能力”。这个定义也是合理的，稍作修改，它不仅仅是数据库软件工具，而是任何接触数据的工具。当适用于笔记本电脑的工具（如Pandas、Python、MySQL、深度学习/机器学习、Bash等）因数据的规模或速度（变化率）而无法传统方式运行时，它现在是一个大数据问题。大数据问题需要专门的工具，下一节将深入探讨这个需求。
- en: Big Data Tools, Components, and Platforms
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大数据工具、组件和平台
- en: Another way to discuss Big Data is to break it down into tools and platforms.
    [Figure 15-1](#Figure-15-1) shows a typical Big Data architecture life cycle.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种讨论大数据的方式是将其分解为工具和平台。[图 15-1](#Figure-15-1) 显示了典型的大数据架构生命周期。
- en: '![pydo 1501](assets/pydo_1501.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![pydo 1501](assets/pydo_1501.png)'
- en: Figure 15-1\. Big Data architecture
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 15-1\. 大数据架构
- en: Let’s discuss a few key components.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论几个关键组件。
- en: Data Sources
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据来源
- en: Some of the familiar sources of Big Data include social networks and digital
    transactions. As people have migrated more of their conversations and their business
    transactions online, it has led to an explosion of data. Additionally, mobile
    technology such as tablets, phones, and laptops that record audio and video, exponentially
    create sources of data.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一些熟悉的大数据来源包括社交网络和数字交易。随着人们将更多的对话和业务交易迁移到在线平台，数据爆炸性增长。此外，诸如平板电脑、手机和笔记本电脑等移动技术记录音频和视频，进一步创造了数据源。
- en: Other data sources include the Internet of Things (IoT), which includes sensors,
    lightweight chips, and devices. All of this leads to an unstoppable proliferation
    of data that needs to be stored somewhere. The tools involved in Data Sources
    could range from IoT client/server systems such as AWS IoT Greengrass, to object
    storage systems such as Amazon S3 or Google Cloud Storage.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 其他数据来源包括物联网（IoT），包括传感器、轻量级芯片和设备。所有这些导致数据不可阻挡地增加，需要在某处进行存储。涉及数据来源的工具可能从物联网客户端/服务器系统（如AWS
    IoT Greengrass）、到对象存储系统（如Amazon S3或Google Cloud Storage）等广泛应用。
- en: Filesystems
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文件系统
- en: Filesystems have played a huge role in computing. Their implementation, though,
    is continuously evolving. In dealing with Big Data, one issue is having enough
    disk I/O, to handle distributed operations.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 文件系统在计算中发挥了重要作用。它们的实现不断演变。在处理大数据时，一个问题是有足够的磁盘I/O来处理分布式操作。
- en: One modern tool that deals with this is the Hadoop Distributed File System (HDFS).
    It works by clustering many servers together, allowing aggregated CPU, disk I/O,
    and storage. In practice, this makes HDFS a fundamental technology for dealing
    with Big Data. It can migrate large volumes of data or filesystems for distributed
    computing jobs. It is also the backbone of Spark, which can do both stream- and
    batch-based machine learning.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 处理这一问题的现代工具之一是Hadoop分布式文件系统（HDFS）。它通过将许多服务器集群在一起来工作，允许聚合的CPU、磁盘I/O和存储。实际上，这使得HDFS成为处理大数据的基础技术。它可以迁移大量数据或文件系统用于分布式计算作业。它还是Spark的支柱，可以进行流式和批量机器学习。
- en: Other types of filesystems include object storage filesystems such as Amazon
    S3 filesystem and Google Cloud Platform storage. They allow for huge files to
    be stored in a distributed and highly available manner, or more precisely, 99.999999999%
    reliability. There are Python APIs and command-line tools available that communicate
    with these filesystems, enabling easy automation. These cloud APIs are covered
    in more detail in [Chapter 10](ch10.html#infra-as-code).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 其他类型的文件系统包括对象存储文件系统，如Amazon S3文件系统和Google Cloud平台存储。它们允许将大文件以分布式和高可用的方式存储，或者更精确地说是99.999999999%的可靠性。有Python
    API和命令行工具可用于与这些文件系统通信，实现简单的自动化。这些云API将在[第10章](ch10.html#infra-as-code)中详细介绍。
- en: Finally, another type of filesystem to be aware of is a traditional network
    filesystem, or NFS, made available as a managed cloud service. An excellent example
    of this is Amazon Elastic File System (Amazon EFS). For a DevOps professional,
    a highly available and elastic NFS filesystem can be an incredibly versatile tool,
    especially coupled with containers technology. [Figure 15-2](#Figure-15-2) shows
    an example of mounting EFS in a container.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，另一种需要注意的文件系统是传统的网络文件系统，或者NFS，作为托管云服务提供。Amazon Elastic File System（Amazon
    EFS）是这方面的一个很好的例子。对于DevOps专业人员来说，一个高可用和弹性的NFS文件系统可以是一个非常多才多艺的工具，特别是与容器技术结合使用。[图15-2](#Figure-15-2)展示了在容器中挂载EFS的一个示例。
- en: '![pydo 1502](assets/pydo_1502.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![pydo 1502](assets/pydo_1502.png)'
- en: Figure 15-2\. Mounting EFS in a container
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-2\. 将EFS挂载在容器中
- en: One powerful automation workflow is to programmatically create Docker containers
    through a build system such as AWS CodePipeline or Google Cloud Build. Those containers
    then get registered in the cloud container registry, for example, Amazon ECR.
    Next, a container management system such as Kubernetes spawns containers that
    mount the NFS. This allows both the power of immutable container images that spawn
    quickly, and access to centralized source code libraries and data. This type of
    workflow could be ideal for an organization looking to optimize machine learning
    operations.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一个强大的自动化工作流程是通过构建系统（例如AWS CodePipeline或Google Cloud Build）通过编程方式创建Docker容器。然后，这些容器被注册到云容器注册表，例如Amazon
    ECR。接下来，一个容器管理系统，比如Kubernetes，会生成挂载NFS的容器。这样一来，既可以享受到产生迅速的不可变容器镜像的强大功能，又可以访问到集中的源代码库和数据。这种类型的工作流程对于希望优化机器学习操作的组织来说可能是理想的。
- en: Data Storage
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据存储
- en: Ultimately the data needs to live somewhere, and this creates some exciting
    opportunities and challenges. One emerging trend is to utilize the concept of
    a data lake. Why do you care about a data lake? A data lake allows for data processing
    in the same location as storage. As a result, many data lakes need to have infinite
    storage and provide infinite computing (i.e., be on the cloud). Amazon S3 is often
    a common choice for a data lake.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，数据需要存放在某个地方，这带来了一些令人兴奋的机会和挑战。一个新兴的趋势是利用数据湖的概念。你为什么关心数据湖？数据湖允许在存储的同一位置进行数据处理。因此，许多数据湖需要具有无限存储和提供无限计算（即在云上）。Amazon
    S3通常是数据湖的常见选择。
- en: The data lake constructed in this manner can also be utilized by a machine learning
    pipeline that may depend on the training data living in the lake, as well as the
    trained models. The trained models could then always be A/B tested to ensure the
    latest model is improving the production prediction (inference) system, as shown
    in [Figure 15-3](#Figure-15-3).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式构建的数据湖也可以被机器学习流水线利用，该流水线可能依赖于存储在湖中的训练数据，以及训练模型。然后，可以始终对训练模型进行A/B测试，以确保最新模型正在改善生产预测（推断）系统，如[图15-3](#Figure-15-3)所示。
- en: '![pydo 1503](assets/pydo_1503.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![pydo 1503](assets/pydo_1503.png)'
- en: Figure 15-3\. Data lake
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-3\. 数据湖
- en: Other forms of storage will be very familiar to traditional software developers.
    These storage systems include relational databases, key/value databases, search
    engines like Elasticsearch, and graph databases. In a Big Data architecture, each
    type of storage system may play a more specific role. In a small-scale system,
    a relational database may be a jack of all trades, but in a Big Data architecture,
    there is less room for tolerance of a mismatch of the storage system.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 其他形式的存储对于传统软件开发人员来说可能非常熟悉。这些存储系统包括关系型数据库、键/值数据库、像Elasticsearch这样的搜索引擎以及图数据库。在大数据架构中，每种类型的存储系统可能会发挥更具体的作用。在小规模系统中，关系型数据库可能是一个万能工具，但在大数据架构中，对于存储系统的不匹配容忍度较小。
- en: An excellent example of mismatch in storage choice is using a relational database
    as a search engine by enabling full-text search capabilities instead of using
    a specialized solution, like Elasticsearch. Elasticsearch is designed to create
    a scalable search solution, while a relational database is designed to provide
    referential integrity and transactions. The CTO of Amazon, Werner Vogel, makes
    this point very well by stating that a “one size database doesn’t fit anyone.”
    This problem is illustrated in [Figure 15-4](#Figure-15-4), which shows that each
    type of database has a specific purpose.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在存储选择中出现的一个极好的不匹配例子是使用关系数据库作为搜索引擎，通过启用全文搜索功能，而不是使用专门的解决方案，比如Elasticsearch。Elasticsearch旨在创建可扩展的搜索解决方案，而关系数据库旨在提供引用完整性和事务。亚马逊的CTO
    Werner Vogel非常明确地指出“一个数据库规模并不适合所有人”。这个问题在[图15-4](#Figure-15-4)中有所说明，该图显示每种类型的数据库都有特定的用途。
- en: '![pydo 1504](assets/pydo_1504.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![pydo 1504](assets/pydo_1504.png)'
- en: Figure 15-4\. Amazon databases
  id: totrans-81
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-4\. Amazon数据库
- en: Picking the correct storage solutions, including which combination of databases
    to use, is a crucial skill for any type of data architect to ensure that a system
    works at optimal efficiency. In thinking through the design of a system that is
    fully automated and efficient, maintenance should be a consideration. If a particular
    technology choice is being abused, such as using a relational database for a highly
    available messaging queue, then maintenance costs could explode, which in turn
    creates more automation work. So another component to consider is how much automation
    work it takes to maintain a solution.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 选择正确的存储解决方案，包括使用哪种组合的数据库，对于任何类型的数据架构师来说都是一项关键技能，以确保系统以最佳效率运行。在考虑设计一个完全自动化和高效的系统时，应该考虑维护成本。如果滥用特定的技术选择，比如使用关系数据库作为高可用消息队列，那么维护成本可能会激增，从而带来更多的自动化工作。因此，另一个需要考虑的组成部分是维护解决方案所需的自动化工作量。
- en: Real-Time Streaming Ingestion
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实时流式传输摄取
- en: Real-time streaming data is a particularly tricky type of data to deal with.
    The stream itself increases the complexity of dealing with the data, and it is
    possible the stream needs to route to yet another part of the system that intends
    to process the data in a streaming fashion. One example of a cloud-based streaming
    ingestion solution is Amazon Kinesis Data Firehose. See [Figure 15-5](#Figure-15-5).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 实时流式数据是一种特别棘手的数据类型。流本身增加了处理数据的复杂性，可能需要将流路由到系统的另一部分，该部分意图以流式处理数据。一个云端流式摄取解决方案的示例是Amazon
    Kinesis Data Firehose。见[图15-5](#Figure-15-5)。
- en: '![pydo 1505](assets/pydo_1505.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![pydo 1505](assets/pydo_1505.png)'
- en: Figure 15-5\. Kinesis log files
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-5\. Kinesis日志文件
- en: 'Here is an example of code that would do that. Notice that Python’s `asyncio`
    module allows for highly concurrent network operations that are single threaded.
    Nodes could emit this in a job farm, and it could be metrics or error logs:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个执行此操作的代码示例。请注意，Python的`asyncio`模块允许高度并发的单线程网络操作。节点可以在作业农场中发出这些操作，这可能是指标或错误日志：
- en: '[PRE14]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Kinesis Data Firehose works by accepting capture data and routing it continuously
    to any number of destinations: Amazon S3, Amazon Redshift, Amazon Elasticsearch
    Service, or some third-party service like Splunk. An open source alternative to
    using a managed service like Kinesis is to use Apache Kafka. Apache Kafka has
    similar principles in that it works as a pub/sub architecture.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Kinesis Data Firehose通过接受捕获的数据并将其持续路由到多个目的地来工作：Amazon S3、Amazon Redshift、Amazon
    Elasticsearch服务，或者像Splunk这样的第三方服务。使用类似Kinesis这样的托管服务的一个开源替代方案是使用Apache Kafka。Apache
    Kafka具有类似的原则，它作为发布/订阅架构工作。
- en: 'Case Study: Building a Homegrown Data Pipeline'
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究：构建自制数据管道
- en: In the early days of Noah’s work as CTO and General Manager at a startup in
    the early 2000s, one problem that cropped up was how to build the company’s first
    machine learning pipeline and data pipeline. A rough sketch of what that looked
    like is in the following diagram of a Jenkins data pipeline ([Figure 15-6](#Figure-15-6)).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在诺亚（Noah）早期担任CTO兼总经理的初创企业的2000年初期，出现了一个问题，即如何构建公司的第一个机器学习管道和数据管道。下图显示了Jenkins数据管道的草图（[图15-6](#Figure-15-6)）。
- en: '![pydo 1506](assets/pydo_1506.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![pydo 1506](assets/pydo_1506.png)'
- en: Figure 15-6\. Jenkins data pipeline
  id: totrans-93
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-6\. Jenkins数据管道
- en: The inputs to the data pipeline are any data source needed for business analytics
    or machine learning predictions. These sources included a relational database,
    Google Analytics, and social media metrics, to a name a few. The collection jobs
    ran every hour and generated CSV files that were available internally by Apache
    web service. This solution was a compelling and straightforward process.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 数据管道的输入是任何需要用于业务分析或机器学习预测的数据源。这些来源包括关系数据库、Google Analytics 和社交媒体指标等等。收集作业每小时运行一次，并生成
    CSV 文件，这些文件可以通过 Apache web 服务在内部使用。这个解决方案是一个引人注目且简单的过程。
- en: The jobs themselves were Jenkins jobs that were Python scripts that ran. If
    something needed to be changed, it was fairly straightforward to change a Python
    script for a particular job. An added benefit to this system was that it was straightforward
    to debug. If a job had a failure, the jobs showed up as failed, and it was straightforward
    to look at the output of the job and see what happened.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这些作业本身是 Jenkins 作业，是运行的 Python 脚本。如果需要更改某些内容，更改特定作业的 Python 脚本相当简单。这个系统的另一个好处是它很容易调试。如果作业失败了，作业就会显示为失败，查看作业的输出并查看发生了什么是很简单的。
- en: The final stage of the pipeline then created machine learning predictions and
    an analytics dashboard that served out dashboards via an `R`-based Shiny application.
    The simplicity of the approach is the most influential factor in this type of
    architecture, and as a bonus it leverages existing DevOps skills.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 管道的最终阶段然后创建了机器学习预测和分析仪表板，通过基于 `R` 的 Shiny 应用程序提供仪表板服务。这种方法的简单性是这种架构的最有影响力的因素，而且作为一个额外的奖励，它利用了现有的
    DevOps 技能。
- en: Serverless Data Engineering
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无服务器数据工程
- en: Another emerging pattern is serverless data engineering. [Figure 15-7](#Figure-15-7)
    is a high-level architectural diagram of what a serverless data pipeline is.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个新兴的模式是无服务器数据工程。[图 15-7](#Figure-15-7) 是一个无服务器数据管道的高层架构图。
- en: '![pydo 1507](assets/pydo_1507.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![pydo 1507](assets/pydo_1507.png)'
- en: Figure 15-7\. Serverless data pipeline
  id: totrans-100
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 15-7\. 无服务器数据管道
- en: Next, let’s look at what a timed lambda does.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看定时 lambda 做了什么。
- en: Using AWS Lambda with CloudWatch Events
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 AWS Lambda 与 CloudWatch 事件
- en: You can create a CloudWatch timer to call the lambda using the AWS Lambda console
    and to set up a trigger, as shown in [Figure 15-8](#Figure-15-8).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 AWS Lambda 控制台上创建一个 CloudWatch 计时器来调用 lambda，并设置触发器，如[图 15-8](#Figure-15-8)所示。
- en: '![pydo 1508](assets/pydo_1508.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![pydo 1508](assets/pydo_1508.png)'
- en: Figure 15-8\. CloudWatch Lambda timer
  id: totrans-105
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 15-8\. CloudWatch Lambda 计时器
- en: Using Amazon CloudWatch Logging with AWS Lambda
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Amazon CloudWatch Logging 与 AWS Lambda
- en: Using CloudWatch logging is an essential step for Lambda development. [Figure 15-9](#Figure-15-9)
    is an example of a CloudWatch event log.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 CloudWatch 日志记录是 Lambda 开发的一个重要步骤。[图 15-9](#Figure-15-9) 是 CloudWatch 事件日志的一个示例。
- en: '![pydo 1509](assets/pydo_1509.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![pydo 1509](assets/pydo_1509.png)'
- en: Figure 15-9\. CloudWatch event log
  id: totrans-109
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 15-9\. CloudWatch 事件日志
- en: Using AWS Lambda to Populate Amazon Simple Queue Service
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 AWS Lambda 填充 Amazon Simple Queue Service
- en: 'Next, you want to do the following locally in AWS Cloud9:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您希望在 AWS Cloud9 中本地执行以下操作：
- en: Create a new Lambda with Serverless Wizard.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Serverless Wizard 创建一个新的 Lambda。
- en: '`cd` into lambda and install packages one level up.'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`cd` 进入 lambda 并在上一级安装包。'
- en: '[PRE15]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Next, you can test local and deploy this code:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你可以在本地测试并部署这段代码：
- en: '[PRE16]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: import json
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: import json
- en: import boto3
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: import boto3
- en: import botocore
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: import botocore
- en: '#import pandas as pd'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '#import pandas as pd'
- en: import pandas as pd
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: import pandas as pd
- en: import wikipedia
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: import wikipedia
- en: import boto3
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: import boto3
- en: from io import StringIO
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: from io import StringIO
- en: '#SETUP LOGGING'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '#设置日志记录'
- en: import logging
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: import logging
- en: from pythonjsonlogger import jsonlogger
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: from pythonjsonlogger import jsonlogger
- en: LOG = logging.getLogger()
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: LOG = logging.getLogger()
- en: LOG.setLevel(logging.DEBUG)
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: LOG.setLevel(logging.DEBUG)
- en: logHandler = logging.StreamHandler()
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: logHandler = logging.StreamHandler()
- en: formatter = jsonlogger.JsonFormatter()
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: formatter = jsonlogger.JsonFormatter()
- en: logHandler.setFormatter(formatter)
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: logHandler.setFormatter(formatter)
- en: LOG.addHandler(logHandler)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: LOG.addHandler(logHandler)
- en: '#S3 BUCKET'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '#S3 存储桶'
- en: REGION = "us-east-1"
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: REGION = "us-east-1"
- en: SQS Utils###
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SQS 工具###
- en: 'def sqs_queue_resource(queue_name):'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 'def sqs_queue_resource(queue_name):'
- en: '"""Returns an SQS queue resource connection'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '"""返回一个 SQS 队列资源连接'
- en: 'Usage example:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 使用示例：
- en: 'In [2]: queue = sqs_queue_resource("dev-job-24910")'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [2] 中：queue = sqs_queue_resource("dev-job-24910")
- en: 'In [4]: queue.attributes'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [4] 中：queue.attributes
- en: 'Out[4]:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 'Out[4]:'
- en: '{''ApproximateNumberOfMessages'': ''0'','
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '{''ApproximateNumberOfMessages'': ''0'','
- en: '''ApproximateNumberOfMessagesDelayed'': ''0'','
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '''ApproximateNumberOfMessagesDelayed'': ''0'','
- en: '''ApproximateNumberOfMessagesNotVisible'': ''0'','
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '''ApproximateNumberOfMessagesNotVisible'': ''0'','
- en: '''CreatedTimestamp'': ''1476240132'','
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '''CreatedTimestamp'': ''1476240132'','
- en: '''DelaySeconds'': ''0'','
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '''DelaySeconds'': ''0'','
- en: '''LastModifiedTimestamp'': ''1476240132'','
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '''LastModifiedTimestamp'': ''1476240132'','
- en: '''MaximumMessageSize'': ''262144'','
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '''MaximumMessageSize'': ''262144'','
- en: '''MessageRetentionPeriod'': ''345600'','
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '''MessageRetentionPeriod'': ''345600'','
- en: '''QueueArn'': ''arn:aws:sqs:us-west-2:414930948375:dev-job-24910'','
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '''QueueArn'': ''arn:aws:sqs:us-west-2:414930948375:dev-job-24910'','
- en: '''ReceiveMessageWaitTimeSeconds'': ''0'','
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '''ReceiveMessageWaitTimeSeconds'': ''0'','
- en: '''VisibilityTimeout'': ''120''}'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '''VisibilityTimeout'': ''120''}'
- en: '"""'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '"""'
- en: sqs_resource = boto3.resource('sqs', region_name=REGION)
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: sqs_resource = boto3.resource('sqs', region_name=REGION)
- en: log_sqs_resource_msg =\
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: log_sqs_resource_msg =\
- en: '"Creating SQS resource conn with qname: [%s] in region: [%s]" %\'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '"Creating SQS resource conn with qname: [%s] in region: [%s]" %\'
- en: (queue_name, REGION)
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: (queue_name, REGION)
- en: LOG.info(log_sqs_resource_msg)
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LOG.info(log_sqs_resource_msg)
- en: queue = sqs_resource.get_queue_by_name(QueueName=queue_name)
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: queue = sqs_resource.get_queue_by_name(QueueName=queue_name)
- en: return queue
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: return queue
- en: 'def sqs_connection():'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 'def sqs_connection():'
- en: '"""Creates an SQS Connection which defaults to global var REGION"""'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '"""Creates an SQS Connection which defaults to global var REGION"""'
- en: sqs_client = boto3.client("sqs", region_name=REGION)
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: sqs_client = boto3.client("sqs", region_name=REGION)
- en: 'log_sqs_client_msg = "Creating SQS connection in Region: [%s]" % REGION'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'log_sqs_client_msg = "Creating SQS connection in Region: [%s]" % REGION'
- en: LOG.info(log_sqs_client_msg)
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LOG.info(log_sqs_client_msg)
- en: return sqs_client
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: return sqs_client
- en: 'def sqs_approximate_count(queue_name):'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 'def sqs_approximate_count(queue_name):'
- en: '"""Return an approximate count of messages left in queue"""'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '"""Return an approximate count of messages left in queue"""'
- en: queue = sqs_queue_resource(queue_name)
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: queue = sqs_queue_resource(queue_name)
- en: attr = queue.attributes
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: attr = queue.attributes
- en: num_message = int(attr['ApproximateNumberOfMessages'])
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: num_message = int(attr['ApproximateNumberOfMessages'])
- en: num_message_not_visible = int(attr['ApproximateNumberOfMessagesNotVisible'])
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: num_message_not_visible = int(attr['ApproximateNumberOfMessagesNotVisible'])
- en: queue_value = sum([num_message, num_message_not_visible])
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: queue_value = sum([num_message, num_message_not_visible])
- en: sum_msg = """'ApproximateNumberOfMessages' and\
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: sum_msg = """'ApproximateNumberOfMessages' and\
- en: '''ApproximateNumberOfMessagesNotVisible'' =\'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '''ApproximateNumberOfMessagesNotVisible'' =\'
- en: '*** [%s] *** for QUEUE NAME: [%s]""" %\'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '*** [%s] *** for QUEUE NAME: [%s]""" %\'
- en: (queue_value, queue_name)
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: (queue_value, queue_name)
- en: LOG.info(sum_msg)
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LOG.info(sum_msg)
- en: return queue_value
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: return queue_value
- en: 'def delete_sqs_msg(queue_name, receipt_handle):'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 'def delete_sqs_msg(queue_name, receipt_handle):'
- en: sqs_client = sqs_connection()
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: sqs_client = sqs_connection()
- en: 'try:'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'try:'
- en: queue_url = sqs_client.get_queue_url(QueueName=queue_name)["QueueUrl"]
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: queue_url = sqs_client.get_queue_url(QueueName=queue_name)["QueueUrl"]
- en: delete_log_msg = "Deleting msg with ReceiptHandle %s" % receipt_handle
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: delete_log_msg = "Deleting msg with ReceiptHandle %s" % receipt_handle
- en: LOG.info(delete_log_msg)
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: LOG.info(delete_log_msg)
- en: response = sqs_client.delete_message(QueueUrl=queue_url,
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: response = sqs_client.delete_message(QueueUrl=queue_url,
- en: ReceiptHandle=receipt_handle)
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: ReceiptHandle=receipt_handle)
- en: 'except botocore.exceptions.ClientError as error:'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'except botocore.exceptions.ClientError as error:'
- en: exception_msg =\
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: exception_msg =\
- en: '"FAILURE TO DELETE SQS MSG: Queue Name [%s] with error: [%s]" %\'
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '"FAILURE TO DELETE SQS MSG: Queue Name [%s] with error: [%s]" %\'
- en: (queue_name, error)
  id: totrans-193
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: (queue_name, error)
- en: LOG.exception(exception_msg)
  id: totrans-194
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: LOG.exception(exception_msg)
- en: return None
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: return None
- en: 'delete_log_msg_resp = "Response from delete from queue: %s" % response'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'delete_log_msg_resp = "Response from delete from queue: %s" % response'
- en: LOG.info(delete_log_msg_resp)
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LOG.info(delete_log_msg_resp)
- en: return response
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: return response
- en: 'def names_to_wikipedia(names):'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 'def names_to_wikipedia(names):'
- en: wikipedia_snippit = []
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: wikipedia_snippit = []
- en: 'for name in names:'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'for name in names:'
- en: wikipedia_snippit.append(wikipedia.summary(name, sentences=1))
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: wikipedia_snippit.append(wikipedia.summary(name, sentences=1))
- en: df = pd.DataFrame(
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: df = pd.DataFrame(
- en: '{'
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '{'
- en: '''names'':names,'
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '''names'':names,'
- en: '''wikipedia_snippit'': wikipedia_snippit'
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '''wikipedia_snippit'': wikipedia_snippit'
- en: '}'
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '}'
- en: )
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: )
- en: return df
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: return df
- en: 'def create_sentiment(row):'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 'def create_sentiment(row):'
- en: '"""Uses AWS Comprehend to Create Sentiments on a DataFrame"""'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '"""Uses AWS Comprehend to Create Sentiments on a DataFrame"""'
- en: LOG.info(f"Processing {row}")
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LOG.info(f"Processing {row}")
- en: comprehend = boto3.client(service_name='comprehend')
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: comprehend = boto3.client(service_name='comprehend')
- en: payload = comprehend.detect_sentiment(Text=row, LanguageCode='en')
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: payload = comprehend.detect_sentiment(Text=row, LanguageCode='en')
- en: 'LOG.debug(f"Found Sentiment: {payload}")'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'LOG.debug(f"Found Sentiment: {payload}")'
- en: sentiment = payload['Sentiment']
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: sentiment = payload['Sentiment']
- en: return sentiment
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: return sentiment
- en: 'def apply_sentiment(df, column="wikipedia_snippit"):'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 'def apply_sentiment(df, column="wikipedia_snippit"):'
- en: '"""Uses Pandas Apply to Create Sentiment Analysis"""'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '"""Uses Pandas Apply to Create Sentiment Analysis"""'
- en: df['Sentiment'] = df[column].apply(create_sentiment)
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: df['Sentiment'] = df[column].apply(create_sentiment)
- en: return df
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: return df
- en: 'S3 ###'
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'S3 ###'
- en: 'def write_s3(df, bucket):'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 'def write_s3(df, bucket):'
- en: '"""Write S3 Bucket"""'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '"""Write S3 Bucket"""'
- en: csv_buffer = StringIO()
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: csv_buffer = StringIO()
- en: df.to_csv(csv_buffer)
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: df.to_csv(csv_buffer)
- en: s3_resource = boto3.resource('s3')
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: s3_resource = boto3.resource('s3')
- en: res = s3_resource.Object(bucket, 'fang_sentiment.csv').\
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: res = s3_resource.Object(bucket, 'fang_sentiment.csv').\
- en: put(Body=csv_buffer.getvalue())
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: put(Body=csv_buffer.getvalue())
- en: 'LOG.info(f"result of write to bucket: {bucket} with:\n {res}")'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'LOG.info(f"result of write to bucket: {bucket} with:\n {res}")'
- en: 'def lambda_handler(event, context):'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 'def lambda_handler(event, context):'
- en: '"""Entry Point for Lambda"""'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '"""Lambda 的入口点"""'
- en: LOG.info(f"SURVEYJOB LAMBDA, event {event}, context {context}")
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LOG.info(f"SURVEYJOB LAMBDA，事件 {event}，上下文 {context}")
- en: 'receipt_handle  = event[''Records''][0][''receiptHandle''] #sqs message'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'receipt_handle  = event[''Records''][0][''receiptHandle''] # sqs 消息'
- en: '#''eventSourceARN'': ''arn:aws:sqs:us-east-1:561744971673:producer'''
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '#''eventSourceARN'': ''arn:aws:sqs:us-east-1:561744971673:producer'''
- en: event_source_arn = event['Records'][0]['eventSourceARN']
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: event_source_arn = event['Records'][0]['eventSourceARN']
- en: 'names = [] #Captured from Queue'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'names = [] # 从队列中捕获'
- en: Process Queue
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理队列
- en: 'for record in event[''Records'']:'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'for record in event[''Records'']:'
- en: body = json.loads(record['body'])
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: body = json.loads(record['body'])
- en: company_name = body['name']
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: company_name = body['name']
- en: '#Capture for processing'
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '# 用于处理的捕获'
- en: names.append(company_name)
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: names.append(company_name)
- en: 'extra_logging = {"body": body, "company_name":company_name}'
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'extra_logging = {"body": body, "company_name":company_name}'
- en: 'LOG.info(f"SQS CONSUMER LAMBDA, splitting arn: {event_source_arn}",'
  id: totrans-245
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'LOG.info(f"SQS 消费者 LAMBDA，分割 arn: {event_source_arn}",'
- en: extra=extra_logging)
  id: totrans-246
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: extra=extra_logging)
- en: qname = event_source_arn.split(":")[-1]
  id: totrans-247
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: qname = event_source_arn.split(":")[-1]
- en: extra_logging["queue"] = qname
  id: totrans-248
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: extra_logging["queue"] = qname
- en: LOG.info(f"Attempting Delete SQS {receipt_handle} {qname}",
  id: totrans-249
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: LOG.info(f"尝试删除 SQS {receipt_handle} {qname}",
- en: extra=extra_logging)
  id: totrans-250
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: extra=extra_logging)
- en: res = delete_sqs_msg(queue_name=qname, receipt_handle=receipt_handle)
  id: totrans-251
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: res = delete_sqs_msg(queue_name=qname, receipt_handle=receipt_handle)
- en: LOG.info(f"Deleted SQS receipt_handle {receipt_handle} with res {res}",
  id: totrans-252
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: LOG.info(f"删除 SQS receipt_handle {receipt_handle} 的结果为 {res}",
- en: extra=extra_logging)
  id: totrans-253
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: extra=extra_logging)
- en: Make Pandas dataframe with wikipedia snippts
  id: totrans-254
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Pandas 构建带有维基百科片段的数据框架
- en: 'LOG.info(f"Creating dataframe with values: {names}")'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LOG.info(f"使用以下值创建数据框架：{names}")
- en: df = names_to_wikipedia(names)
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: df = names_to_wikipedia(names)
- en: Perform Sentiment Analysis
  id: totrans-257
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行情感分析
- en: df = apply_sentiment(df)
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: df = apply_sentiment(df)
- en: 'LOG.info(f"Sentiment from FANG companies: {df.to_dict()}")'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LOG.info(f"FANG 公司情感分析结果：{df.to_dict()}")
- en: Write result to S3
  id: totrans-260
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将结果写入 S3
- en: write_s3(df=df, bucket="fangsentiment")
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: write_s3(df=df, bucket="fangsentiment")
- en: '[PRE18]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: noah:/tmp $ aws s3 cp --recursive s3://fangsentiment/ .
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: noah:/tmp $ aws s3 cp --recursive s3://fangsentiment/ .
- en: 'download: s3://fangsentiment/netflix_sentiment.csv to ./netflix_sentiment.csv'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 'download: s3://fangsentiment/netflix_sentiment.csv to ./netflix_sentiment.csv'
- en: 'download: s3://fangsentiment/google_sentiment.csv to ./google_sentiment.csv'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 'download: s3://fangsentiment/google_sentiment.csv to ./google_sentiment.csv'
- en: 'download: s3://fangsentiment/facebook_sentiment.csv to ./facebook_sentiment.csv'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 'download: s3://fangsentiment/facebook_sentiment.csv to ./facebook_sentiment.csv'
- en: '```'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '```'
- en: OK, so what did we accomplish? [Figure 15-16](#Figure-15-16) shows our serverless
    AI data engineering pipeline.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，我们完成了什么？[图 15-16](#Figure-15-16) 展示了我们的无服务器 AI 数据工程流水线。
- en: '![pydo 1516](assets/pydo_1516.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![pydo 1516](assets/pydo_1516.png)'
- en: Figure 15-16\. Serverless AI data engineering pipeline
  id: totrans-270
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 15-16\. 无服务器 AI 数据工程流水线
- en: Conclusion
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Data engineering is an evolving job title, and it benefits greatly from strong
    DevOps skills. The DevOps best practices of Microservices, Continous Delivery,
    Infrastructure as Code, and Monitoring and Logging play a tremendous role in this
    category. Often by leveraging cloud-native technologies, it makes hard problems
    possible, and simple problems effortless.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程是一个不断发展的职业头衔，它极大地受益于强大的 DevOps 技能。微服务、持续交付、基础设施即代码和监控日志记录等 DevOps 最佳实践在这一类别中发挥了重要作用。通过利用云原生技术，使得复杂问题成为可能，简单问题变得无需费力。
- en: Here are some next steps to continue on a journey of mastery with data engineering.
    Learn serverless technology. It doesn’t matter what the cloud is, learn it! This
    environment is the future, and data engineering, in particular, is well suited
    to capitalize on this trend.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些继续掌握数据工程技能旅程的下一步。学习无服务器技术。无论云环境是什么，都要学习！这种环境是未来，特别是数据工程非常适合抓住这一趋势。
- en: Exercises
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: Explain what Big Data is and what its key characteristics are.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释 Big Data 是什么及其主要特征。
- en: Use small data tools in Python to solve common problems.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Python 中的小数据工具解决常见问题。
- en: Explain what a data lake is and how it is used.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释数据湖是什么及其用途。
- en: Explain the appropriate use cases for different types of purpose-built databases.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释不同类型用途专用数据库的适当使用案例。
- en: Build a serverless data engineering pipeline in Python.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Python 中构建无服务器数据工程流水线。
- en: Case Study Question
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究问题
- en: Using the same architecture shown in this chapter, build an end-to-end serverless
    data engineering pipeline that scrapes a website using Scrapy, Beautiful Soup,
    or a similar library, and sends image files to Amazon Rekognition to be analyzed.
    Store the results of the Rekognition API call in Amazon DynamoDB. Run this job
    on a timer once a day.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用本章展示的相同架构，构建一个端到端的无服务器数据工程管道，使用Scrapy、Beautiful Soup或类似的库来爬取网站，并将图像文件发送到Amazon
    Rekognition进行分析。将Rekognition API调用的结果存储在Amazon DynamoDB中。每天定时运行此作业。
