<html><head></head><body><div id="sbo-rt-content" class="calibre2"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 12. Lessons from the Field" class="calibre3"><div class="preface" id="chapter-lessons-from-the-field">
<h1 class="calibre23"><span class="publishername">Chapter 12. </span>Lessons from the Field</h1>

<aside data-type="sidebar" epub:type="sidebar" class="calibre40"><div class="sidebar" id="idm46122396930456">
<h5 class="calibre41">Questions You’ll Be Able to Answer After This Chapter</h5>
<ul class="printings">
<li class="calibre21">
<p class="calibre42">How do successful start-ups handle large volumes of data and machine learning?</p>
</li>
<li class="calibre21">
<p class="calibre42">What monitoring and deployment technologies keep systems stable?</p>
</li>
<li class="calibre21">
<p class="calibre42">What lessons have successful CTOs learned about their technologies and teams?</p>
</li>
<li class="calibre21">
<p class="calibre42">How widely can PyPy be deployed?</p>
</li>
</ul>
</div></aside>

<p class="author1"><a data-type="indexterm" data-primary="lessons for start-ups and CTOs" id="lsc_ch" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>In this chapter we have collected stories from successful companies that use Python in high-data-volume and speed-critical situations. The stories are written by key people in each organization who have many years of experience; they share not just their technology choices but also some of their hard-won wisdom. We have four great new stories for you from other experts in our domain. We’ve also kept the “Lessons from the Field” from the first edition of this book; their titles are marked “(2014).”</p>






<section data-type="sect1" class="calibre3" data-pdf-bookmark="Streamlining Feature Engineering Pipelines with  Feature-engine"><div class="preface" id="idm46122396923400">
<h1 class="calibre25">Streamlining Feature Engineering Pipelines with 
<span class="publishername">Feature-engine</span></h1>

<p class="byline1">Soledad Galli (trainindata.com)</p>
<aside data-type="sidebar" epub:type="sidebar" class="calibre40"><div class="sidebar" id="idm46122396920488">
<h5 class="calibre41"/>
<p class="calibre73"><a data-type="indexterm" data-primary="feature engineering pipelines" id="fep_ab" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="Feature-engine, feature engineering pipelines with" id="fe_fep" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="Galli, Soledad" id="gs_ab" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="pipelining" id="pip_ab" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="Train in Data" id="td_ab" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Soledad Galli is the lead data scientist and founder of Train in Data. She has experience in
finance and insurance, received a Data Leadership Award in 2018, and was selected as one of
LinkedIn’s “Top Voices” in data science and analytics in 2019. Soledad is passionate about
sharing knowledge and helping others succeed in data 
<span class="publishername">science</span>.</p>
</div></aside>

<p class="author1">Train in Data is an education project led by experienced data scientists and AI software
engineers. We help professionals improve coding and data science skills and adopt machine
learning best practices. We create advanced online courses on machine learning and AI
software engineering and open source libraries, like <a href="https://feature-engine.readthedocs.io" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6">Feature-engine</a>, to smooth the delivery of machine learning solutions.</p>








<section data-type="sect2" data-pdf-bookmark="Feature Engineering for Machine Learning" class="calibre3"><div class="preface" id="idm46122396911656">
<h2 class="calibre43">Feature Engineering for Machine Learning</h2>

<p class="author1"><a data-type="indexterm" data-primary="machine learning" id="idm46122396910424" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Machine learning models take in a bunch of input variables and output a prediction. In finance and insurance, we build models to predict, for example, the likelihood of a loan being repaid, the probability of an application being fraudulent, and whether a car should be repaired or replaced after an accident. The data we collect and store or recall from third-party APIs is almost never suitable to train machine learning models or return predictions. Instead, we transform variables extensively before feeding them to machine learning algorithms. We refer to the collection of variable transformations as <em class="hyperlink">feature engineering</em>.</p>

<p class="author1">Feature engineering includes procedures to impute missing data, encode categorical variables, transform or discretize numerical variables, put features in the same scale, combine features into new variables, extract information from dates, aggregate transactional data, and derive features from time series, text, or even images. There are  many techniques for each of these feature engineering steps, and your choice will depend on the characteristics of the variables and the algorithms you intend to use. Thus, when feature engineers build and consume machine learning in organizations, we do not speak of machine learning models but of machine learning pipelines, where a big part of the pipeline is devoted to feature engineering and data 
<span class="publishername">transformation</span>.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="The Hard Task of Deploying Feature Engineering Pipelines" class="calibre3"><div class="preface" id="idm46122396906424">
<h2 class="calibre43">The Hard Task of Deploying Feature Engineering Pipelines</h2>

<p class="author1"><a data-type="indexterm" data-primary="deploying feature engineering pipelines" id="idm46122396904984" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Many feature engineering transformations learn parameters from data. I have seen organizations utilize config files with hardcoded parameters. These files limit versatility and are hard to maintain (every time you retrain your model, you need to rewrite the config file with the new parameters). To create highly performant feature engineering pipelines, it’s better  to develop algorithms that automatically learn and store these parameters and can also be saved and loaded, ideally as one object.</p>

<p class="author1">At Train in Data, we develop machine learning pipelines in the research environment and deploy them in the production environment. These pipelines should be reproducible. Reproducibility is the ability to duplicate a machine learning model exactly, such that, given the same data as input, both models return the same output. Utilizing the same code in the research and production environment smooths the deployment of machine learning pipelines by minimizing the amount of code to be rewritten, maximizing reproducibility.</p>

<p class="author1">Feature engineering transformations need to be tested. Unit tests for each feature engineering procedure ensure that the algorithm returns the desired outcome. Extensive code refactoring in production to add unit and integration tests is extremely time-consuming and provides new opportunities to introduce bugs, or find bugs introduced during the research phase due to the lack of testing. To minimize code refactoring in production, it is better to introduce unit testing as we develop the engineering algorithms in the research phase.</p>

<p class="author1">The same feature engineering transformations are used across projects. To avoid different code implementations of the same technique, which often occurs in teams with many data scientists, and to enhance team performance, speed up model development, and smooth model operationalization, we want to reuse code that was previously built and tested. The best way to do that is to create in-house packages. Creating packages may seem time-consuming, since it involves building tests and documentation. But it is more efficient in the long run, because it allows us to enhance code and add new features incrementally, while reusing code and functionality that has already been developed and tested. Package development can be tracked through versioning and even shared with the community as open source, raising the profile of the developers and the organization.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Leveraging the Power of Open Source Python Libraries" class="calibre3"><div class="preface" id="idm46122396900296">
<h2 class="calibre43">Leveraging the Power of Open Source Python Libraries</h2>

<p class="author1"><a data-type="indexterm" data-primary="open source Python libraries" data-secondary="leveraging power of" id="idm46122396899176" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>It’s important to use established open source projects or thoroughly developed in-house libraries. This is more efficient for several reasons:</p>

<ul class="printings">
<li class="calibre21">
<p class="calibre27">Well-developed projects tend to be thoroughly documented, so it is clear what each piece of code intends to achieve.</p>
</li>
<li class="calibre21">
<p class="calibre27">Well-established open source packages are tested to prevent the introduction of bugs, ensure that the transformation achieves the intended outcome, and maximize reproducibility.</p>
</li>
<li class="calibre21">
<p class="calibre27">Well-established projects have been widely adopted and approved by the community, giving you peace of mind that the code is of quality.</p>
</li>
<li class="calibre21">
<p class="calibre27">You can use the same package in the research and production environment, minimizing code refactoring during deployment.</p>
</li>
<li class="calibre21">
<p class="calibre27">Packages are clearly versioned, so you can deploy the version you used when developing your pipeline to ensure reproducibility, while newer versions continue to add functionality.</p>
</li>
<li class="calibre21">
<p class="calibre27">Open source packages can be shared, so different organizations can build tools together.</p>
</li>
<li class="calibre21">
<p class="calibre27">While open source packages are maintained by a group of experienced developers, the community can also contribute, providing new ideas and features that raise the quality of the package and code.</p>
</li>
<li class="calibre21">
<p class="calibre27">Using a well-established open source library removes the task of coding from our hands, improving team performance, reproducibility, and collaboration, while reducing model research and deployment timelines.</p>
</li>
</ul>

<p class="author1">Open source Python libraries like<a data-type="indexterm" data-primary="scikit-learn" data-secondary="about" id="idm46122396888504" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="Category encoders" id="idm46122396887528" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="Featuretools" id="idm46122396886856" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/> <a href="https://oreil.ly/j-4ob" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6">scikit-learn</a>, <a href="https://oreil.ly/DtSL7" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6">Category encoders</a>, and <a href="https://oreil.ly/DOB7V" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6">Featuretools</a> provide feature engineering functionality. To expand on existing functionality and smooth the creation and deployment of machine learning pipelines, I created the open source Python package <a href="https://oreil.ly/CZrSB" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6">Feature-engine</a>, which provides an exhaustive battery of feature engineering procedures and supports the implementation of different transformations to distinct feature spaces.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Feature-engine Smooths Building and Deployment of Feature Engineering Pipelines" class="calibre3"><div class="preface" id="idm46122396882728">
<h2 class="calibre43">Feature-engine Smooths Building and Deployment of Feature Engineering Pipelines</h2>

<p class="author1">Feature engineering algorithms need to  learn parameters from data automatically, return a data format that facilitates use in research and production environments, and include an exhaustive battery of transformations to encourage adoption across projects. Feature-engine was conceived and designed to fulfill all these requirements. Feature-engine transformers—that is, the classes that implement a feature engineering transformation—learn and store parameters from data. Feature-engine transformers return Pandas DataFrames, which are suitable for data analysis and visualization during the research phase. Feature-engine supports the creation and storage of an entire end-to-end engineering pipeline in a single object, making deployment easier. And to facilitate adoption across projects, it includes an exhaustive list of feature transformations.</p>

<p class="author1">Feature-engine includes many procedures to impute missing data, encode categorical variables, transform and discretize numerical variables, and remove or censor outliers. Each transformer can learn, or alternatively have specified, the group of variables it should modify. Thus, the transformer can receive the entire dataframe, yet it will alter only the selected variable group, removing the need of additional transformers or manual work to slice the dataframe and then join it back together.</p>

<p class="author1">Feature-engine transformers use the <a data-type="indexterm" data-primary="fit/transform methods" id="idm46122396879176" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>fit/transform methods from scikit-learn and expand its functionality to include additional engineering techniques. Fit/transform functionality makes Feature-engine transformers usable within the scikit-learn pipeline. Thus with Feature-engine we can store an entire machine learning pipeline into a single object that can be saved and retrieved or placed in memory for live scoring.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Helping with the Adoption of a New Open Source Package" class="calibre3"><div class="preface" id="idm46122396877688">
<h2 class="calibre43">Helping with the Adoption of a New Open Source Package</h2>

<p class="author1"><a data-type="indexterm" data-primary="open source Python libraries" data-secondary="adopting new" id="idm46122396876312" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>No matter how good an open source package is, if no one knows it exists or if the community can’t easily understand how to use it, it will not succeed. Making a successful open source package entails making code that is performant, well tested, well documented, and useful—and then letting the community know that it exists, encouraging its adoption by users who can suggest new features, and attracting a developer community to add more functionality, improve the documentation, and enhance code quality to raise its performance. For a package developer, this means that we need to factor in time to develop code and to design and execute a sharing strategy. Here are some strategies that have worked for me and for other package developers.</p>

<p class="author1">We can leverage the power of well-established open source functionality to facilitate adoption by the community. Scikit-learn is the reference library for machine learning in Python. Thus, adopting scikit-learn fit/transform functionality in a new package facilitates an easy and fast adoption by the community. The learning curve to use the package is shorter as users are already familiar with this implementation. Some packages that leverage the use of fit/transform are <a data-type="indexterm" data-primary="Keras" id="idm46122396873992" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a href="https://keras.io" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6">Keras</a>, Category encoders (perhaps the most renowned), and of course Feature-engine.</p>

<p class="author1">Users want to know how the package can be used and shared, so include a license stating these conditions in the code repository. Users also need instructions and examples of the code functionality. Docstrings in code files with information about functionality and examples of its use is a good start, but it is not enough. Widespread packages include additional documentation (which can be generated with ReStructuredText files) with descriptions of code functionality, examples of its use and the outputs returned, installation guidelines, the channels where the package is available (PyPI, conda), how to get started, changelogs, and more. Good documentation should empower users to use the library without reading the source code. The 
<span class="publishername">documentation</span> of the machine learning visualization library<a data-type="indexterm" data-primary="Yellowbrick" id="idm46122396759864" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/> <a href="https://oreil.ly/j96lT" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6">Yellowbrick</a> is a good example. I have adopted this practice for Feature-engine as well.</p>

<p class="author1">How can we increase package visibility? How do we reach potential users? Teaching an online course can help you reach people, especially if it’s on a prestigious online learning platform. In addition, publishing documentation in <a href="https://readthedocs.org" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6">Read the Docs</a>, creating YouTube tutorials, and presenting the package at meetups and meetings all increase visibility. Presenting the package functionality while answering relevant questions in well-established user networks like <a data-type="indexterm" data-primary="Stack Overflow" id="idm46122396757016" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="Stack Exchange" id="idm46122396756408" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="Quora" id="idm46122396755800" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Stack Overflow, Stack Exchange, and Quora can also help. The developers of Featuretools and 
<span class="publishername">Yellowbrick</span> have leveraged the power of these networks. Creating a dedicated Stack Overflow issues list lets users ask questions and shows the package is being actively maintained.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Developing, Maintaining, and Encouraging Contribution to Open Source Libraries" class="calibre3"><div class="preface" id="idm46122396754008">
<h2 class="calibre43">Developing, Maintaining, and Encouraging Contribution to Open Source Libraries</h2>

<p class="author1"><a data-type="indexterm" data-primary="open source Python libraries" data-secondary="developing, maintaining, and encouraging contribution to" id="idm46122396752760" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>For a package to be successful and relevant, it needs an active developer community. A developer community is composed of one or, ideally, a group of dedicated developers or maintainers who will watch for overall functionality, documentation, and direction. An active community allows and welcomes additional ad hoc contributors.</p>

<p class="author1">One thing to consider when developing a package is <a data-type="indexterm" data-primary="code maintainability" id="idm46122396751048" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>code maintainability. The simpler and shorter the code, the easier it is to maintain, which makes it more likely to attract contributors and maintainers. To simplify development and maintainability, Feature-engine leverages the power of scikit-learn base transformers. Scikit-learn provides an API with a bunch of base classes that developers can build upon to create new transformers. In addition, scikit-learn’s API provides many tests to ensure compatibility between packages and also that the transformer delivers the intended functionality. By using these, Feature-engine developers and maintainers focus only on feature engineering functionality, while the maintenance of base code is taken care of by the bigger scikit-learn community. This, of course, has a trade-off. If scikit-learn changes its base functionality, we need to update our library to ensure it is compatible with the latest version. Other open source packages that use scikit-learn API are Yellowbrick and Category encoders.</p>

<p class="author1">To encourage developers to collaborate, <a data-type="indexterm" data-primary="NumFOCUS" id="idm46122396748680" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a href="https://numfocus.org" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6">NumFOCUS</a> recommends creating a code of conduct and encouraging inclusion and diversity. The project needs to be open, which generally means the code should be publicly hosted, with guidelines to orient new contributors about project development and discussion forums open to public participation, like a mailing list or a Slack channel. While some open source Python libraries have their own codes of conduct, others, like<a data-type="indexterm" data-primary="Yellowbrick" id="idm46122396747016" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/> Yellowbrick and Feature-engine, follow the <a data-type="indexterm" data-primary="Python Community Code of Conduct" id="idm46122396746280" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a href="https://oreil.ly/8k4Tc" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6">Python Community Code of Conduct</a>. Many open source projects, including Feature-engine, are publicly hosted on GitHub. Contributing guidelines 
<span class="publishername">list ways</span> new contributors can help—for example, by fixing bugs, adding new 
<span class="publishername">functionality</span>, or enhancing the documentation. Contributing guidelines also inform new developers of the contributing cycle, how to fork the repository, how to work on the contributing branch, how the code review cycle works, and how to make a Pull Request.</p>

<p class="author1">Collaboration can raise the quality and performance of the library by enhancing code quality or functionality, adding new features, and improving documentation. Contributions can be as simple as reporting typos in the documentation, reporting code that doesn’t return the intended outcome, or requesting new features. Collaborating on open source libraries can also help raise the collaborator’s profile while exposing them to new engineering and coding practices, improving their skills.</p>

<p class="author1">Many developers and data scientists believe that they need to be top-notch developers to contribute to open source projects. I used to believe that myself, and it discouraged me from making contributions or requesting features—even though, as a user, I had a clear idea of what features were available and which were missing. This is far from true. Any user can contribute to the library.  And package maintainers love 
<span class="publishername">contributions</span>.</p>

<p class="author1">Useful contributions to Feature-engine have included simple things like adding a line to the <em class="hyperlink">.gitignore</em>, sending a message through LinkedIn to bring typos in the docs to my attention, making a PR to correct typos themselves, highlighting warning issues raised by newer versions of scikit-learn, requesting new functionality, or expanding the battery of unit tests.</p>

<p class="author1">If you want to contribute but have no experience, it is useful to go through the issues found on the package repository. Issues are lists with the modifications to the code that have priority. They are tagged with labels like “code enhancement,” “new functionality,” “bug fix,” or “docs.” To start, it is good to go after issues tagged as “good first issue” or “good for new contributors”; those tend to be smaller code changes and will allow you to get familiar with the contribution cycle. Then you can jump into more complex code modifications. Just by solving an easy issue, you will learn a lot about software development, Git, and code review cycles.</p>

<p class="author1">Feature-engine is currently a small package with straightforward code implementations. It is easy to navigate and has few dependencies, so it is a good starting point for contributing to open source. If you want to get started, do get in touch. I will be delighted to hear from you. Good luck!<a data-type="indexterm" data-primary="" data-startref="fep_ab" id="idm46122396737528" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="" data-startref="fe_fep" id="idm46122396736552" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="" data-startref="gs_ab" id="idm46122396735608" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="" data-startref="pip_ab" id="idm46122396734664" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="" data-startref="td_ab" id="idm46122396733720" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/></p>
</div></section>





</div></section>













<section data-type="sect1" class="calibre3" data-pdf-bookmark="Highly Performant Data Science Teams"><div class="preface" id="idm46122396922808">
<h1 class="calibre25">Highly Performant Data Science Teams</h1>

<p class="byline1">Linda Uruchurtu (Fiit)</p>
<aside data-type="sidebar" epub:type="sidebar" class="calibre40"><div class="sidebar" id="idm46122396730696">
<h5 class="calibre41"/>
<p class="calibre73"><a data-type="indexterm" data-primary="data science" id="ds_about" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="Fiit" id="fiit_ab" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="Uruchurtu, Linda" id="ul_ab" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Linda Uruchurtu is a senior data scientist and software engineer, currently working at Fiit.  Since 2013, she has been helping small-to-medium start-ups build products using data. She has experience in analytics, statistics, machine learning, and product building in a variety of industries, including transportation, retail, health, and fitness.</p>

<p class="calibre73">Linda holds a PhD in theoretical physics from Cambridge University. She has been both a speaker and a reviewer at PyData London, and has served as chair of the PyData London review committee since 2017.</p>
</div></aside>

<p class="author1">Data science teams are different from other technical teams, because the scope of what they do varies according to where they sit and the type of problems they tackle. However, whether  the team is responsible for answering “why” and “how” questions or simply delivering fully operational ML services, in order to  deliver successfully, they need to keep the stakeholders happy.</p>

<p class="author1">This can be challenging. Most data science projects have a degree of uncertainty attached to them, and since there are different types of stakeholders,  “happy” can mean different things. Some stakeholders might be concerned only with final deliverables, whereas others might care about side effects or common interfaces. Additionally, some might not be technical or have a limited understanding of the specifics of the project. Here I  will share some lessons I have learned that make a difference in the way projects are carried out and delivered.</p>








<section data-type="sect2" data-pdf-bookmark="How Long Will It Take?" class="calibre3"><div class="preface" id="idm46122396723592">
<h2 class="calibre43">How Long Will It Take?</h2>

<p class="author1">This is possibly the most common question data science team leads are asked. Picture the following: management asks the project manager (PM), or whoever is responsible for delivery, to solve a given problem. The PM goes to the team, presents them with this information, and asks them to plan a solution. Cue this question, from the PM or from other stakeholders: How long will it take?</p>

<p class="author1">First, the team should ask questions to better define the scope of their solutions. These might include the following:</p>

<ul class="printings">
<li class="calibre21">
<p class="calibre27">Why is this a problem?</p>
</li>
<li class="calibre21">
<p class="calibre27">What is the impact of solving this problem?</p>
</li>
<li class="calibre21">
<p class="calibre27">What is the definition of done?</p>
</li>
<li class="calibre21">
<p class="calibre27">What is the minimal version of a solution that satisfies said definition?</p>
</li>
<li class="calibre21">
<p class="calibre27">Is there a way to validate a solution early on?</p>
</li>
</ul>

<p class="author1">Notice that “How long will it take?” isn’t on this list.</p>

<p class="author1">The strategy should be twofold. First, get a time-boxed period to ask these questions and propose one or more solutions. Once a solution is agreed upon, the PM should explain to stakeholders that the team can provide a timeline once the work for said solution is planned.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Discovery and Planning" class="calibre3"><div class="preface" id="idm46122396714792">
<h2 class="calibre43">Discovery and Planning</h2>

<p class="author1">The team has a fixed amount of time to come up with solutions. What’s next? They need to generate hypotheses, followed by exploratory work and quick prototyping, to keep or discard potential solutions successively.</p>

<p class="author1">Depending on the solution chosen, other teams may become stakeholders. Development teams could have requirements from APIs they hold, or they could become consumers of a service; product, operations, customer service, and other  teams might use visualizations and reports. The PM’s team should discuss their ideas with these teams.</p>

<p class="author1">Once this process has taken place, the team is usually in a good position to determine how much uncertainty and/or risk adheres to each option. The PM can now assess which option is preferred.</p>

<p class="author1">Once an option is chosen, the PM can define a timeline for milestones and deliverables. Useful points to raise here are as follows:</p>

<ul class="printings">
<li class="calibre21">
<p class="calibre27">Can the deliverables be reasonably reviewed and tested?</p>
</li>
<li class="calibre21">
<p class="calibre27">If work depends on other teams, can work be scheduled so no delay is 
<span class="publishername">introduced</span>?</p>
</li>
<li class="calibre21">
<p class="calibre27">Can the team provide value from intermediate milestones?</p>
</li>
<li class="calibre21">
<p class="calibre27">Is there a way to reduce risk from parts of the project that have a significant amount of uncertainty?</p>
</li>
</ul>

<p class="author1">Tasks derived from the plan can then be sized and time-boxed to provide a time estimate. It is a good idea to allow for extra time: some people like to double or triple the time they think they’ll need!</p>

<p class="author1">Some tasks are frequently underestimated and simplified, including data collection and dataset building, testing, and validation. Getting good data for model building can often be more complex and expensive than it initially seems. One option may be to start with small datasets for prototyping and postpone further collection until after proof of concept. Testing, too, is fundamental, both for correctness and for reproducibility. Are inputs as expected? Are processing pipelines introducing errors? Are outputs correct? Unit testing and integration tests should be part of every effort. Finally, validation is important, particularly in the real world. Be sure to factor in realistic estimates for all of these tasks.</p>

<p class="author1">Once you’ve done that,  the team has not only an answer to the “time” question, but also a plan with milestones that everyone can use to understand what work is being carried out.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Managing Expectations and Delivery" class="calibre3"><div class="preface" id="idm46122396703400">
<h2 class="calibre43">Managing Expectations and Delivery</h2>

<p class="author1">Lots of issues can affect the time required before a delivery is achieved. Keep an eye on the following points to make sure you manage the team’s expectations:</p>
<dl class="calibre28">
<dt class="calibre29"><a data-type="indexterm" data-primary="scope creep" id="idm46122396700792" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="delivery, managing" id="idm46122396700056" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="expectations, managing" id="idm46122396699384" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Scope creep</dt>
<dd class="calibre30">
<p class="calibre31">Scope creep is the subtle shifting of the scope of work, so that more work is expected than was initially planned. Pairing and reviews can help mitigate this.</p>
</dd>
<dt class="calibre29"><a data-type="indexterm" data-primary="nontechnical tasks, underestimating" id="idm46122396697256" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Underestimating nontechnical tasks</dt>
<dd class="calibre30">
<p class="calibre31">Discussions, user research, documentation, and many other tasks can easily be underestimated by those who don’t know them well.</p>
</dd>
<dt class="calibre29"><a data-type="indexterm" data-primary="availability" id="idm46122396695192" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Availability</dt>
<dd class="calibre30">
<p class="calibre31">Team members’ scheduling and availability can also introduce delays.</p>
</dd>
<dt class="calibre29"><a data-type="indexterm" data-primary="data" data-secondary="issues with quality of" id="idm46122396693128" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Issues with data quality</dt>
<dd class="calibre30">
<p class="calibre31">From making sure working datasets are good to go to discovering sources of bias, data quality can introduce complications or even invalidate pieces of work.</p>
</dd>
<dt class="calibre29"><a data-type="indexterm" data-primary="options, alternative" id="idm46122396690728" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Alternative options</dt>
<dd class="calibre30">
<p class="calibre31">When unexpected difficulties arise, it might make sense to consider other approaches. However, sunk costs might prevent the team from wanting to raise this,  which could delay the work and risk creating the impression that the team doesn’t know what they are doing.</p>
</dd>
<dt class="calibre29"><a data-type="indexterm" data-primary="testing" id="idm46122396688456" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Lack of testing</dt>
<dd class="calibre30">
<p class="calibre31">Sudden changes in data inputs or bugs in data pipelines can invalidate assumptions. Having good test coverage from the start will improve team velocity and pay dividends at the end.</p>
</dd>
<dt class="calibre29">Difficulty testing or validating</dt>
<dd class="calibre30">
<p class="calibre31">If not enough time is allowed for testing and validating hypotheses, the schedule can be delayed. Changes in assumptions can also lead to alterations in the testing plan.</p>
</dd>
</dl>

<p class="author1">Use weekly refinement and planning sessions to spot issues and discuss whether tasks need to be added or removed. This will give the PM enough information to update the final stakeholders.  Prioritization should also happen with the same cadence. If opportunities arise to do some tasks earlier than expected, these should be pushed forward.</p>

<p class="author1">Intermediate deliverables, particularly if they provide value outside the scope of the project, continuously justify the work. That’s good for the team, in terms of focus and morale, as well as stakeholders, who will have a sense of progress. The continuous process of redrawing the game plan and reviewing and adjusting iterations will ensure the team has a clear sense of direction and freedom to work, while providing enough information and value to keep stakeholders keen on continuing to support the project.</p>

<p class="author1">To become highly performant while tackling a new project, your data science team’s  main focus has to be on making data uncertainty and business-need uncertainty less risky by delivering lightweight <a data-type="indexterm" data-primary="minimum viable product (MVP)" id="idm46122396682904" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="MVP (minimum viable product)" id="idm46122396682184" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>minimum viable product (MVP) solutions (think scripts and Python notebooks). The initial conceived MVP might actually turn out to be leaner than or different from the first concept, given findings down the line or changes in business needs. Only after validation should you proceed with a production-ready version.</p>

<p class="author1">The discovery and planning process is critical, and so is thinking in terms of iterations.   Keep in mind that the discovery phase is always ongoing and that external events will always affect the plan.<a data-type="indexterm" data-primary="" data-startref="ds_about" id="idm46122396680424" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="" data-startref="fiit_ab" id="idm46122396679448" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="" data-startref="ul_ab" id="idm46122396678504" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/></p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Numba" class="calibre3"><div class="preface" id="lesson-from-field-numba">
<h1 class="calibre25">Numba</h1>

<p class="byline1">Valentin Haenel (<a href="http://haenel.co" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"><em class="hyperlink">http://haenel.co</em></a>)</p>
<aside data-type="sidebar" epub:type="sidebar" class="calibre40"><div class="sidebar" id="idm46122396674200">
<h5 class="calibre41"/>
<p class="calibre73"><a data-type="indexterm" data-primary="Haenel, Valentin" id="hv_abt" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="Numba" id="numba_ab" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Valentin Haenel is a longtime “Python for Data” user and developer who still remembers hearing <a data-type="indexterm" data-primary="Oliphant, Travis" id="idm46122396671000" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Travis Oliphant’s keynote about NumPy at the first EuroSciPy conference in 2008. He then proceeded to use Python for simple modeling of spiking neurons and to evaluate data from perception experiments while pursuing his master’s degree in computational neuroscience. Since then he has contributed to more than 80 open source projects. He now works for Anaconda as an open source developer on the Numba project.</p>

<p class="calibre73"><em class="hyperlink">The author would like to thank three of the Numba core developers, <a data-type="indexterm" data-primary="Archibald, Stuart" id="idm46122396669224" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="Siu Kwan Lam" id="idm46122396668440" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="Seibert, Stan" id="idm46122396667768" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Stuart Archibald, Siu Kwan Lam, and Stan Seibert, for productive discussions, advice, and feedback about this text</em>.</p>
</div></aside>

<p class="author1"><em class="hyperlink">Numba</em> is an open source, JIT function compiler for numerically focused Python. Initially created at <a data-type="indexterm" data-primary="Continuum Analytics" id="idm46122396665800" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="Anaconda Inc." id="idm46122396665096" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Continuum Analytics (now Anaconda Inc) in 2012, it has since grown into a mature open source project on GitHub with a large and diverse group of contributors. Its primary use case is the acceleration of numerical and/or scientific Python code. The main entry point is a <a data-type="indexterm" data-primary="@jit decorator" data-primary-sortas="jit" id="idm46122396664008" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="@jit decorator" id="idm46122396663064" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>decorator—the <code class="calibre26">@jit</code> decorator—which is used to annotate the specific functions, ideally the bottlenecks of the application, that should be compiled. Numba will compile these functions just-in-time, which simply means that the function will be compiled at the first, or initial, execution. All subsequent executions with the same argument types will then use the compiled variant of the function, which should be faster than the original.</p>

<p class="author1">Numba not only can compile Python, but also is NumPy-aware and can handle code that uses NumPy. Under the hood, Numba relies on the well-known<a data-type="indexterm" data-primary="LLVM project" id="idm46122396660840" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/> <a href="https://llvm.org" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6">LLVM project</a>, a collection of modular and reusable compiler and toolchain technologies. Last, Numba isn’t a fully fledged Python compiler. It can compile only a subset of both Python and NumPy—albeit a large enough subset to make it useful in a wide range of applications. For more information, please consult the <a href="https://numba.pydata.org" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6">documentation</a>.</p>








<section data-type="sect2" data-pdf-bookmark="A Simple Example" class="calibre3"><div class="preface" id="idm46122396658168">
<h2 class="calibre43">A Simple Example</h2>

<p class="author1">As a simple example, let’s use Numba to accelerate a Python implementation of an ancient algorithm for finding all prime numbers up to a given maximum<a data-type="indexterm" data-primary="Sieve of Eratosthenes" id="idm46122396656296" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/> (<em class="hyperlink">N</em>): the sieve of Eratosthenes. It works as follows:</p>

<ul class="printings">
<li class="calibre21">
<p class="calibre27">First, initialize a Boolean array of length <em class="hyperlink">N</em> to all true values.</p>
</li>
<li class="calibre21">
<p class="calibre27">Then starting with the first prime, 2, cross off (set the position in the Boolean list corresponding to that number to false) all multiples of the number up to <em class="hyperlink">N</em>.</p>
</li>
<li class="calibre21">
<p class="calibre27">Proceed to the next number that has not yet been crossed off, in this case 3, and again cross off all multiples of it.</p>
</li>
<li class="calibre21">
<p class="calibre27">Continue to proceed through the numbers and cross off their multiples until you reach <em class="hyperlink">N</em>.</p>
</li>
<li class="calibre21">
<p class="calibre27">When you reach <em class="hyperlink">N</em>, all the numbers that have not been crossed off are the set of prime numbers up to <em class="hyperlink">N</em>.</p>
</li>
</ul>

<p class="author1">A reasonably efficient Python implementation might look something like this:</p>

<pre data-type="programlisting" data-code-language="python" class="calibre50"><code class="kn">import</code> <code class="nn">numpy</code> <code class="kn">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">numba</code> <code class="kn">import</code> <code class="n">jit</code>

<code class="nd">@jit</code><code class="p">(</code><code class="n">nopython</code><code class="o">=</code><code class="nb">True</code><code class="p">)</code>  <code class="c"># simply add the jit decorator</code>
<code class="kn">def</code> <code class="nf">primes</code><code class="p">(</code><code class="n">N</code><code class="o">=</code><code class="mi">100000</code><code class="p">):</code>
    <code class="n">numbers</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">ones</code><code class="p">(</code><code class="n">N</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="n">np</code><code class="o">.</code><code class="n">uint8</code><code class="p">)</code>  <code class="c"># initialize the boolean array</code>
    <code class="kn">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="n">N</code><code class="p">):</code>
        <code class="kn">if</code> <code class="n">numbers</code><code class="p">[</code><code class="n">i</code><code class="p">]</code> <code class="o">==</code> <code class="mi">0</code><code class="p">:</code>  <code class="c"># has previously been crossed off</code>
            <code class="kn">continue</code>
        <code class="kn">else</code><code class="p">:</code>  <code class="c"># it is a prime, cross off all multiples</code>
            <code class="n">x</code> <code class="o">=</code> <code class="n">i</code> <code class="o">+</code> <code class="n">i</code>
            <code class="kn">while</code> <code class="n">x</code> <code class="o">&lt;</code> <code class="n">N</code><code class="p">:</code>
                <code class="n">numbers</code><code class="p">[</code><code class="n">x</code><code class="p">]</code> <code class="o">=</code> <code class="mi">0</code>
                <code class="n">x</code> <code class="o">+=</code> <code class="n">i</code>
    <code class="c"># return all primes, as indicated by all boolean positions that are one</code>
    <code class="kn">return</code> <code class="n">np</code><code class="o">.</code><code class="n">nonzero</code><code class="p">(</code><code class="n">numbers</code><code class="p">)[</code><code class="mi">0</code><code class="p">][</code><code class="mi">2</code><code class="p">:]</code></pre>

<p class="author1">After placing this in a file called <em class="hyperlink">sieve.py</em>, you can use the <code class="calibre26">%timeit</code> magic to micro-benchmark the code:</p>

<pre data-type="programlisting" data-code-language="python" class="calibre50"><code class="n">In</code> <code class="p">[</code><code class="mi">1</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sieve</code> <code class="kn">import</code> <code class="n">primes</code>

<code class="n">In</code> <code class="p">[</code><code class="mi">2</code><code class="p">]:</code> <code class="n">primes</code><code class="p">()</code>  <code class="c"># run it once to make sure it is compiled</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">2</code><code class="p">]:</code> <code class="n">array</code><code class="p">([</code>    <code class="mi">2</code><code class="p">,</code>     <code class="mi">3</code><code class="p">,</code>     <code class="mi">5</code><code class="p">,</code> <code class="o">...</code><code class="p">,</code> <code class="mi">99971</code><code class="p">,</code> <code class="mi">99989</code><code class="p">,</code> <code class="mi">99991</code><code class="p">])</code>

<code class="n">In</code> <code class="p">[</code><code class="mi">3</code><code class="p">]:</code> <code class="o">%</code><code class="n">timeit</code> <code class="n">primes</code><code class="o">.</code><code class="n">py_func</code><code class="p">()</code>  <code class="c"># 'py_func' contains</code>
                                  <code class="c"># the original Python implementation</code>
<code class="mi">145</code> <code class="n">ms</code> <code class="err">±</code> <code class="mi">1.86</code> <code class="n">ms</code> <code class="n">per</code> <code class="n">loop</code> <code class="p">(</code><code class="n">mean</code> <code class="err">±</code> <code class="n">std</code><code class="o">.</code> <code class="n">dev</code><code class="o">.</code> <code class="n">of</code> <code class="mi">7</code> <code class="n">runs</code><code class="p">,</code> <code class="mi">10</code> <code class="n">loops</code> <code class="n">each</code><code class="p">)</code>

<code class="n">In</code> <code class="p">[</code><code class="mi">4</code><code class="p">]:</code> <code class="o">%</code><code class="n">timeit</code> <code class="n">primes</code><code class="p">()</code>  <code class="c"># this benchmarks the Numba compiled version</code>
<code class="mi">340</code> <code class="err">µ</code><code class="n">s</code> <code class="err">±</code> <code class="mi">3.98</code> <code class="err">µ</code><code class="n">s</code> <code class="n">per</code> <code class="n">loop</code> <code class="p">(</code><code class="n">mean</code> <code class="err">±</code> <code class="n">std</code><code class="o">.</code> <code class="n">dev</code><code class="o">.</code> <code class="n">of</code> <code class="mi">7</code> <code class="n">runs</code><code class="p">,</code> <code class="mi">1000</code> <code class="n">loops</code> <code class="n">each</code><code class="p">)</code></pre>

<p class="author1">That is a speedup of roughly four hundredfold; your mileage may vary. Nonetheless, there are a few things of interest here:</p>

<ul class="printings">
<li class="calibre21">
<p class="calibre27">Compilation happened at the function level.</p>
</li>
<li class="calibre21">
<p class="calibre27">Simply adding the decorator <code class="calibre26">@jit</code> was enough to instruct Numba to compile the function. No further modifications to the function source code, such as type annotations of the variables, were needed.</p>
</li>
<li class="calibre21">
<p class="calibre27">Numba is NumPy-aware, so all the NumPy calls in this implementation were supported and could be compiled successfully.</p>
</li>
<li class="calibre21">
<p class="calibre27">The original, pure Python function is available as the <code class="calibre26">py_func</code> attribute of the compiled function.</p>
</li>
</ul>

<p class="author1">There is a faster but less educational version of this algorithm, the implementation of which is left to the interested reader.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Best Practices and Recommendations" class="calibre3"><div class="preface" id="idm46122396657512">
<h2 class="calibre43">Best Practices and Recommendations</h2>

<p class="author1">One of the most important recommendations for Numba is to use nopython mode whenever possible. To activate this mode, simply use the <code class="calibre26">nopython=True</code> option with the <code class="calibre26">@jit</code> decorator, as shown in the prime number example. Alternatively, you can use the <code class="calibre26">@njit</code> decorator alias, which is accessible by doing <code class="calibre26">from numba import njit</code>. In nopython mode, Numba attempts a large number of optimizations and can significantly improve performance.  However, this mode is very strict;  for compilation to succeed, Numba needs to be able to infer the types of all the variables within your function.</p>

<p class="author1">You can also  use object mode by doing<a data-type="indexterm" data-primary="@jit decorator" data-primary-sortas="jit" id="idm46122396393976" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="@jit decorator" id="idm46122396393000" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/> <code class="calibre26">@jit(forceobj=True)</code>. In this mode, Numba becomes very permissive about what it can and cannot compile, which limits it to  performing a minimal set of optimizations. This  is likely to have a significant negative effect on performance. To take advantage of  Numba’s full potential, you really should use nopython mode.</p>

<p class="author1">If you can’t decide whether you want to use object mode or not, there is the option to use an object-mode block. This can come in handy when only a small part of your code needs to execute in object mode: for example, if you have a long-running loop and would like to use string formatting to print the current progress of your program. For example:</p>

<pre data-type="programlisting" data-code-language="python" class="calibre50"><code class="kn">from</code> <code class="nn">numba</code> <code class="kn">import</code> <code class="n">njit</code><code class="p">,</code> <code class="n">objmode</code>

<code class="nd">@njit</code><code class="p">()</code>
<code class="kn">def</code> <code class="nf">foo</code><code class="p">():</code>
    <code class="kn">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">1000000</code><code class="p">):</code>
        <code class="c"># do compute</code>
        <code class="kn">if</code> <code class="n">i</code> <code class="o">%</code> <code class="mi">100000</code> <code class="o">==</code> <code class="mi">0</code><code class="p">:</code>
            <code class="kn">with</code> <code class="n">objmode</code><code class="p">:</code>  <code class="c"># to escape to object-mode</code>
                           <code class="c"># using 'format' is permissible here</code>
                <code class="kn">print</code><code class="p">(</code><code class="s">"epoch: {}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">i</code><code class="p">))</code>

<code class="n">foo</code><code class="p">()</code></pre>

<p class="author1">Pay attention to the types of the variables that you use. Numba works very well with both <a data-type="indexterm" data-primary="numpy" data-secondary="arrays in" id="idm46122396389016" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>NumPy arrays and NumPy views on other datatypes. Therefore, if you can, use NumPy arrays as  your preferred data structure. Tuples, strings, enums, and simple scalar types such as int, float, and Boolean are also reasonably well supported. Globals are fine for constants, but  pass the rest of your data as arguments to your function. Python lists and dictionaries are unfortunately not very well supported.  This largely stems from the fact that they can be type heterogeneous: a specific Python list may contain differently typed items; for example, integers, floats and strings. This poses a problem for Numba, because it needs the container to hold only items of a single type in order to compile it. However, these two data structures are probably one of the most used features of the Python language and are even one of the first things programmers learn about.</p>

<p class="author1">To remedy this shortcoming, Numba supports the so-called typed containers: <code class="calibre26">typed-list</code> and <code class="calibre26">typed-dict</code>.  These are homogeneously typed variants of the Python list and dict. This means that they may contain only items of a single type: for example, a <code class="calibre26">typed-list</code> of only integer values. Beyond this limitation, they behave much like their Python counterparts and support a largely identical API. Additionally, they may be used within regular Python code or within Numba compiled functions, and can be passed into and returned from Numba compiled functions. These are available from the <code class="calibre26">numba.typed</code> submodule. Here is a  simple example of a <code class="calibre26">typed-list</code>:</p>

<pre data-type="programlisting" data-code-language="python" class="calibre50"><code class="kn">from</code> <code class="nn">numba</code> <code class="kn">import</code> <code class="n">njit</code>
<code class="kn">from</code> <code class="nn">numba.typed</code> <code class="kn">import</code> <code class="n">List</code>

<code class="nd">@njit</code>
<code class="kn">def</code> <code class="nf">foo</code><code class="p">(</code><code class="n">x</code><code class="p">):</code>
    <code class="sd">""" Copy x, append 11 to the result. """</code>
    <code class="n">result</code> <code class="o">=</code> <code class="n">x</code><code class="o">.</code><code class="n">copy</code><code class="p">()</code>
    <code class="n">result</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="mi">11</code><code class="p">)</code>
    <code class="kn">return</code> <code class="n">result</code>

<code class="n">a</code> <code class="o">=</code> <code class="n">List</code><code class="p">()</code> <code class="c"># Create a new typed-list</code>
<code class="kn">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">5</code><code class="p">,</code> <code class="mi">7</code><code class="p">):</code>
    <code class="c"># Add the content to the typed-list,</code>
    <code class="c"># the type is inferred from the first item added.</code>
    <code class="n">a</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">i</code><code class="p">)</code>
<code class="n">b</code> <code class="o">=</code> <code class="n">foo</code><code class="p">(</code><code class="n">a</code><code class="p">)</code> <code class="c"># make the call, append 11; this list will go to eleven</code></pre>

<p class="author1">While Python does have limitations, you can rethink them and understand which ones can be safely disregarded when using Numba. Two specific examples come to mind: calling functions and <code class="calibre26">for</code> loops. Numba enables a technique called <em class="hyperlink">inlining</em> in the underlying LLVM library to optimize away the overhead of calling functions.  This means that during compilation, any function calls that are amenable to inlining are replaced with a block of code that is the equivalent of the function being called.  As a result, there is practically no performance impact from breaking up a large function into a few or many small ones in order to aid readability and comprehensibility.</p>

<p class="author1">One of the main criticisms of Python is that its <a data-type="indexterm" data-primary="for loops" id="idm46122396187240" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><code class="calibre26">for</code> loops are slow. Many people recommend using alternative constructs instead when attempting to improve the performance of a Python program: list comprehensions or even NumPy arrays. Numba does not suffer from this limitation, and using <code class="calibre26">for</code> loops in Numba compiled functions is fine.  Observe:</p>

<pre data-type="programlisting" data-code-language="python" class="calibre50"><code class="kn">from</code> <code class="nn">numba</code> <code class="kn">import</code> <code class="n">njit</code>

<code class="nd">@njit</code>
<code class="kn">def</code> <code class="nf">numpy_func</code><code class="p">(</code><code class="n">a</code><code class="p">):</code>
    <code class="c"># uses Numba's implementation of NumPy's sum, will also be fast in</code>
    <code class="c"># Python</code>
    <code class="kn">return</code> <code class="n">a</code><code class="o">.</code><code class="n">sum</code><code class="p">()</code>

<code class="nd">@njit</code>
<code class="kn">def</code> <code class="nf">for_loop</code><code class="p">(</code><code class="n">a</code><code class="p">):</code>
    <code class="c"># uses a simple for-loop over the array</code>
    <code class="n">acc</code> <code class="o">=</code> <code class="mi">0</code>
    <code class="kn">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="n">a</code><code class="p">:</code>
        <code class="n">acc</code> <code class="o">+=</code> <code class="n">i</code>
    <code class="kn">return</code> <code class="n">acc</code></pre>

<p class="author1">We can now benchmark the preceding code:</p>

<pre data-type="programlisting" data-code-language="python" class="calibre50"><code class="n">In</code> <code class="p">[</code><code class="mi">1</code><code class="p">]:</code> <code class="o">...</code> <code class="c"># import the above functions</code>

<code class="n">In</code> <code class="p">[</code><code class="mi">2</code><code class="p">]:</code> <code class="kn">import</code> <code class="nn">numpy</code> <code class="kn">as</code> <code class="nn">np</code>

<code class="n">In</code> <code class="p">[</code><code class="mi">3</code><code class="p">]:</code> <code class="n">a</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">arange</code><code class="p">(</code><code class="mi">1000000</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="n">np</code><code class="o">.</code><code class="n">int64</code><code class="p">)</code>

<code class="n">In</code> <code class="p">[</code><code class="mi">4</code><code class="p">]:</code> <code class="n">numpy_func</code><code class="p">(</code><code class="n">a</code><code class="p">)</code>  <code class="c"># sanity check and compile</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">4</code><code class="p">]:</code> <code class="mi">499999500000</code>

<code class="n">In</code> <code class="p">[</code><code class="mi">5</code><code class="p">]:</code> <code class="n">for_loop</code><code class="p">(</code><code class="n">a</code><code class="p">)</code>  <code class="c"># sanity check and compile</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">5</code><code class="p">]:</code> <code class="mi">499999500000</code>

<code class="n">In</code> <code class="p">[</code><code class="mi">6</code><code class="p">]:</code> <code class="o">%</code><code class="n">timeit</code> <code class="n">numpy_func</code><code class="p">(</code><code class="n">a</code><code class="p">)</code>  <code class="c"># Compiled version of the NumPy func</code>
<code class="mi">174</code> <code class="err">µ</code><code class="n">s</code> <code class="err">±</code> <code class="mi">3.05</code> <code class="err">µ</code><code class="n">s</code> <code class="n">per</code> <code class="n">loop</code> <code class="p">(</code><code class="n">mean</code> <code class="err">±</code> <code class="n">std</code><code class="o">.</code> <code class="n">dev</code><code class="o">.</code> <code class="n">of</code> <code class="mi">7</code> <code class="n">runs</code><code class="p">,</code> <code class="mi">10000</code> <code class="n">loops</code> <code class="n">each</code><code class="p">)</code>

<code class="n">In</code> <code class="p">[</code><code class="mi">7</code><code class="p">]:</code> <code class="o">%</code><code class="n">timeit</code> <code class="n">for_loop</code><code class="p">(</code><code class="n">a</code><code class="p">)</code>    <code class="c"># Compiled version of the for-loop</code>
<code class="mi">186</code> <code class="err">µ</code><code class="n">s</code> <code class="err">±</code> <code class="mi">7.59</code> <code class="err">µ</code><code class="n">s</code> <code class="n">per</code> <code class="n">loop</code> <code class="p">(</code><code class="n">mean</code> <code class="err">±</code> <code class="n">std</code><code class="o">.</code> <code class="n">dev</code><code class="o">.</code> <code class="n">of</code> <code class="mi">7</code> <code class="n">runs</code><code class="p">,</code> <code class="mi">1000</code> <code class="n">loops</code> <code class="n">each</code><code class="p">)</code>

<code class="n">In</code> <code class="p">[</code><code class="mi">8</code><code class="p">]:</code> <code class="o">%</code><code class="n">timeit</code> <code class="n">numpy_func</code><code class="o">.</code><code class="n">py_func</code><code class="p">(</code><code class="n">a</code><code class="p">)</code>  <code class="c"># Pure NumPy func</code>
<code class="mi">336</code> <code class="err">µ</code><code class="n">s</code> <code class="err">±</code> <code class="mi">6.72</code> <code class="err">µ</code><code class="n">s</code> <code class="n">per</code> <code class="n">loop</code> <code class="p">(</code><code class="n">mean</code> <code class="err">±</code> <code class="n">std</code><code class="o">.</code> <code class="n">dev</code><code class="o">.</code> <code class="n">of</code> <code class="mi">7</code> <code class="n">runs</code><code class="p">,</code> <code class="mi">1000</code> <code class="n">loops</code> <code class="n">each</code><code class="p">)</code>

<code class="n">In</code> <code class="p">[</code><code class="mi">9</code><code class="p">]:</code> <code class="o">%</code><code class="n">timeit</code> <code class="n">for_loop</code><code class="o">.</code><code class="n">py_func</code><code class="p">(</code><code class="n">a</code><code class="p">)</code>    <code class="c"># Pure Python for-loop</code>
<code class="mi">156</code> <code class="n">ms</code> <code class="err">±</code> <code class="mi">3.07</code> <code class="n">ms</code> <code class="n">per</code> <code class="n">loop</code> <code class="p">(</code><code class="n">mean</code> <code class="err">±</code> <code class="n">std</code><code class="o">.</code> <code class="n">dev</code><code class="o">.</code> <code class="n">of</code> <code class="mi">7</code> <code class="n">runs</code><code class="p">,</code> <code class="mi">10</code> <code class="n">loops</code> <code class="n">each</code><code class="p">)</code></pre>

<p class="author1">As you can see, both Numba compiled variants have very similar performance characteristics, whereas the pure Python <code class="calibre26">for</code> loop implementation is significantly (800 times) slower than its compiled counterpart.</p>

<p class="author1">If you are now thinking about rewriting your NumPy array expressions as <code class="calibre26">for</code> loops, don’t! As shown in the preceding example, Numba is perfectly happy with NumPy arrays and their associated functions. In fact, Numba has an additional ace up its sleeve: an optimization known as <em class="hyperlink">loop fusion</em>. Numba performs this technique predominantly on array expression operations. For example:</p>

<pre data-type="programlisting" data-code-language="python" class="calibre50"><code class="kn">from</code> <code class="nn">numba</code> <code class="kn">import</code> <code class="n">njit</code>

<code class="nd">@njit</code>
<code class="kn">def</code> <code class="nf">loop_fused</code><code class="p">(</code><code class="n">a</code><code class="p">,</code> <code class="n">b</code><code class="p">):</code>
    <code class="kn">return</code> <code class="n">a</code> <code class="o">*</code> <code class="n">b</code> <code class="o">-</code> <code class="mi">4.1</code> <code class="o">*</code> <code class="n">a</code> <code class="o">&gt;</code> <code class="mi">2.5</code> <code class="o">*</code> <code class="n">b</code>

<code class="n">In</code> <code class="p">[</code><code class="mi">1</code><code class="p">]:</code> <code class="o">...</code> <code class="c"># import the example</code>

<code class="n">In</code> <code class="p">[</code><code class="mi">2</code><code class="p">]:</code> <code class="kn">import</code> <code class="nn">numpy</code> <code class="kn">as</code> <code class="nn">np</code>

<code class="n">In</code> <code class="p">[</code><code class="mi">3</code><code class="p">]:</code> <code class="n">a</code><code class="p">,</code> <code class="n">b</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">arange</code><code class="p">(</code><code class="mi">1e6</code><code class="p">),</code> <code class="n">np</code><code class="o">.</code><code class="n">arange</code><code class="p">(</code><code class="mi">1e6</code><code class="p">)</code>

<code class="n">In</code> <code class="p">[</code><code class="mi">4</code><code class="p">]:</code> <code class="n">loop_fused</code><code class="p">(</code><code class="n">a</code><code class="p">,</code> <code class="n">b</code><code class="p">)</code>  <code class="c"># compile the function</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">4</code><code class="p">]:</code> <code class="n">array</code><code class="p">([</code><code class="nb">False</code><code class="p">,</code> <code class="nb">False</code><code class="p">,</code> <code class="nb">False</code><code class="p">,</code> <code class="o">...</code><code class="p">,</code>  <code class="nb">True</code><code class="p">,</code>  <code class="nb">True</code><code class="p">,</code>  <code class="nb">True</code><code class="p">])</code>


<code class="n">In</code> <code class="p">[</code><code class="mi">5</code><code class="p">]:</code> <code class="o">%</code><code class="n">timeit</code> <code class="n">loop_fused</code><code class="p">(</code><code class="n">a</code><code class="p">,</code> <code class="n">b</code><code class="p">)</code>
<code class="mi">643</code> <code class="err">µ</code><code class="n">s</code> <code class="err">±</code> <code class="mi">18</code> <code class="err">µ</code><code class="n">s</code> <code class="n">per</code> <code class="n">loop</code> <code class="p">(</code><code class="n">mean</code> <code class="err">±</code> <code class="n">std</code><code class="o">.</code> <code class="n">dev</code><code class="o">.</code> <code class="n">of</code> <code class="mi">7</code> <code class="n">runs</code><code class="p">,</code> <code class="mi">1000</code> <code class="n">loops</code> <code class="n">each</code><code class="p">)</code>

<code class="n">In</code> <code class="p">[</code><code class="mi">6</code><code class="p">]:</code> <code class="o">%</code><code class="n">timeit</code> <code class="n">loop_fused</code><code class="o">.</code><code class="n">py_func</code><code class="p">(</code><code class="n">a</code><code class="p">,</code> <code class="n">b</code><code class="p">)</code>
<code class="mi">5.2</code> <code class="n">ms</code> <code class="err">±</code> <code class="mi">205</code> <code class="err">µ</code><code class="n">s</code> <code class="n">per</code> <code class="n">loop</code> <code class="p">(</code><code class="n">mean</code> <code class="err">±</code> <code class="n">std</code><code class="o">.</code> <code class="n">dev</code><code class="o">.</code> <code class="n">of</code> <code class="mi">7</code> <code class="n">runs</code><code class="p">,</code> <code class="mi">100</code> <code class="n">loops</code> <code class="n">each</code><code class="p">)</code></pre>

<p class="author1">As you can see, the Numba compiled version is eight times faster than the pure NumPy one. What is going on? Without Numba, the array expression will lead to several <code class="calibre26">for</code> loops and several so-called temporaries in memory.  Loosely speaking, for each arithmetic operation in the expression, a <code class="calibre26">for</code> loop over arrays must execute, and the result of each must be stored in a temporary array in memory.  What loop fusion does is fuse the various loops over arithmetic operations together into a single loop, thereby reducing both the total number of memory lookups and the overall memory required to compute the result. In fact, the loop-fused variant may well look something like this:</p>

<pre data-type="programlisting" data-code-language="python" class="calibre50"><code class="kn">import</code> <code class="nn">numpy</code> <code class="kn">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">numba</code> <code class="kn">import</code> <code class="n">njit</code>

<code class="nd">@njit</code>
<code class="kn">def</code> <code class="nf">manual_loop_fused</code><code class="p">(</code><code class="n">a</code><code class="p">,</code> <code class="n">b</code><code class="p">):</code>
    <code class="n">N</code> <code class="o">=</code> <code class="nb">len</code><code class="p">(</code><code class="n">a</code><code class="p">)</code>
    <code class="n">result</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">empty</code><code class="p">(</code><code class="n">N</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="n">np</code><code class="o">.</code><code class="n">bool_</code><code class="p">)</code>
    <code class="kn">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">N</code><code class="p">):</code>
        <code class="n">a_i</code><code class="p">,</code> <code class="n">b_i</code> <code class="o">=</code> <code class="n">a</code><code class="p">[</code><code class="n">i</code><code class="p">],</code> <code class="n">b</code><code class="p">[</code><code class="n">i</code><code class="p">]</code>
        <code class="n">result</code><code class="p">[</code><code class="n">i</code><code class="p">]</code> <code class="o">=</code> <code class="n">a_i</code> <code class="o">*</code> <code class="n">b_i</code> <code class="o">-</code> <code class="mi">4.1</code> <code class="o">*</code> <code class="n">a_i</code> <code class="o">&gt;</code> <code class="mi">2.5</code> <code class="o">*</code> <code class="n">b_i</code>
    <code class="kn">return</code> <code class="n">result</code></pre>

<p class="author1">Running this will show  performance characteristics similar to those of the loop-fusion example:</p>

<pre data-type="programlisting" data-code-language="python" class="calibre50"><code class="n">In</code> <code class="p">[</code><code class="mi">1</code><code class="p">]:</code> <code class="o">%</code><code class="n">timeit</code> <code class="n">manual_loop_fused</code><code class="p">(</code><code class="n">a</code><code class="p">,</code> <code class="n">b</code><code class="p">)</code>
<code class="mi">636</code> <code class="err">µ</code><code class="n">s</code> <code class="err">±</code> <code class="mi">49.1</code> <code class="err">µ</code><code class="n">s</code> <code class="n">per</code> <code class="n">loop</code> <code class="p">(</code><code class="n">mean</code> <code class="err">±</code> <code class="n">std</code><code class="o">.</code> <code class="n">dev</code><code class="o">.</code> <code class="n">of</code> <code class="mi">7</code> <code class="n">runs</code><code class="p">,</code> <code class="mi">1000</code> <code class="n">loops</code> <code class="n">each</code><code class="p">)</code></pre>

<p class="author1">Finally, I recommend targeting serial execution initially, but keep parallel execution in mind. Don’t assume from the outset that only a parallel version will lead to your targeted performance characteristics. Instead, focus on developing a clean serial implementation first. Parallelism makes everything harder to reason about and can be a source of difficulty when debugging problems. If you are satisfied with your results and would still like to investigate parallelizing your code, Numba does come with a <code class="calibre26">parallel=True</code> option for the <a data-type="indexterm" data-primary="@jit decorator" data-primary-sortas="jit" id="idm46122395366264" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="@jit decorator" id="idm46122395365384" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><code class="calibre26">@jit</code> decorator and a corresponding parallel range, the <code class="calibre26">prange</code> construct, to make creating parallel loops easier.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Getting Help" class="calibre3"><div class="preface" id="idm46122395548952">
<h2 class="calibre43">Getting Help</h2>

<p class="author1">As of early 2020, the two main recommended communication channels for Numba are the<a data-type="indexterm" data-primary="GitHub" id="idm46122395363336" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="Gitter chat" id="idm46122395362632" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/> <a href="https://oreil.ly/hXGfE" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6">GitHub issue tracker</a> and the <a href="https://oreil.ly/8YGl1" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6">Gitter chat</a>; this is where the action happens. There are also a mailing list and a Twitter account, but these are fairly low-traffic and mostly used to announce new releases and other important project news.<a data-type="indexterm" data-primary="" data-startref="hv_abt" id="idm46122395360216" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="" data-startref="numba_ab" id="idm46122395359240" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/></p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Optimizing Versus Thinking" class="calibre3"><div class="preface" id="idm46122395358040">
<h1 class="calibre25">Optimizing Versus Thinking</h1>

<p class="byline1">Vincent D. Warmerdam, Senior Person at GoDataDriven (<a href="http://koaning.io" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"><em class="hyperlink">http://koaning.io</em></a>)</p>

<p class="author1"><a data-type="indexterm" data-primary="GoDataDriven" id="gdd_ab" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="optimizing, thinking versus" id="ot_ab" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="thinking, optimizing versus" id="to_ab" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="Warmerdam, Vincent D." id="wvd_ab" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>This is a story of a team that was solving the wrong problem. We were optimizing efficiency while ignoring effectiveness. My hope is that this story is a cautionary tale for others. This story actually happened, but I’ve changed parts and kept the details vague in the interest of keeping things incognito.</p>

<p class="author1">I was consulting for a client with a common logistics problem: they wanted to predict the number of trucks that would arrive at their warehouses. There was a good business case for this. If we knew the number of vehicles, we would know how large the workforce had to be  to handle the workload for that day.</p>

<p class="author1">The planning department had been trying to tackle this problem for years (using Excel). They were skeptical that an algorithm could improve things. Our job was to explore if machine learning could help out here.</p>

<p class="author1">From the start, it was apparent it was a difficult  time-series problem:</p>

<ul class="printings">
<li class="calibre21">
<p class="calibre27">There were many (seriously, many!) holidays that we needed to keep in mind since the warehouses operated internationally. The effect of the holiday might depend on the day of the week,  since the warehouses did not open during weekends. Certain holidays meant demand would go up, while other holidays meant that the warehouses were closed (which would sometimes cause a three-day weekend).</p>
</li>
<li class="calibre21">
<p class="calibre27">Seasonal shifts were not unheard of.</p>
</li>
<li class="calibre21">
<p class="calibre27">Suppliers would frequently enter and leave the market.</p>
</li>
<li class="calibre21">
<p class="calibre27">The seasonal patterns were always changing because the market was continuously evolving.</p>
</li>
<li class="calibre21">
<p class="calibre27">There were many warehouses, and although they were in separate buildings, there was a reason to believe the number of trucks arriving at the different warehouses were correlated.</p>
</li>
</ul>

<p class="author1">The diagram in <a data-type="xref" href="#FIG-lesson-vincent1" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6">Figure 12-1</a> shows the process for the algorithm that would calculate the seasonal effect as well as the long-term trend. As long as there weren’t any holidays, our method would work. The planning department warned us about this; the holidays were the hard part. After spending a lot of time collecting relevant features, we ended up building a system that mainly focused on trying to deal with the 
<span class="publishername">holidays</span>.</p>

<figure class="calibre46"><div id="FIG-lesson-vincent1" class="figure">
<img src="Images/hpp2_1201.png" alt="hpp2 1201" class="calibre125"/>
<h6 class="calibre47"><span class="publishername">Figure 12-1. </span>Seasonal effects and long-term trend</h6>
</div></figure>

<p class="author1">So we iterated, did more feature engineering, and designed the algorithm. It got to the point where we needed to calculate a time-series model per warehouse, which would be post-processed with a heuristic model per holiday per day of the week. A holiday just before the weekend would cause a different shift than a holiday just after the weekend. As you might imagine, this calculation can get quite expensive when you also want to apply a grid search, as shown in <a data-type="xref" href="#FIG-lesson-vincent2" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6">Figure 12-2</a>.</p>

<figure class="calibre46"><div id="FIG-lesson-vincent2" class="figure">
<img src="Images/hpp2_1202.png" alt="hpp2 1202" class="calibre126"/>
<h6 class="calibre47"><span class="publishername">Figure 12-2. </span>Many variations cost a lot of compute time</h6>
</div></figure>

<p class="author1">There were many effects that we had to estimate accurately, including the decay of the past measurements, how smooth the seasonal effect is, the regularization parameters, and how to tackle the correlation between different warehouses.</p>

<p class="author1">What didn’t help was that we needed to predict months ahead. Another hard thing was the cost function: it was discrete. The planning department did not care about (or even appreciate ) mean squared error; they cared only about the number of days where the prediction error would exceed 100 trucks.</p>

<p class="author1">You can imagine that, in addition to statistical concerns, the model presented performance concerns. To keep this at bay, we restricted ourselves to simpler machine learning models. We gained a lot of iteration speed by doing this, which allowed us to focus on feature engineering. A few weeks went by before we had a version we could demo. We had still made a model that performed well enough, except for the 
<span class="publishername">holidays</span>.</p>

<p class="author1">The model went into a proof-of-concept phase; it performed reasonably well but not significantly better than the current planning team’s method. The model was useful since it allowed the planning department to reflect if the model disagreed, but no one was comfortable having the model automate the planning.</p>

<p class="author1">Then it happened. It was my final week with the client, just before a colleague would be taking over. I was hanging out at the coffee corner, talking with an analyst about a different project for which I needed some data from him. We started reviewing the available tables in the database. Eventually, he told me about a “carts” table (shown in <a data-type="xref" href="#FIG-lesson-vincent3" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6">Figure 12-3</a>).</p>

<p class="author1">Me: “A carts table? What’s in there?”
Analyst: “Oh, it contains all the orders for the carts.”
Me: “Suppliers buy them from the warehouse?”
Analyst: “No, actually, they rent them. They usually rent them three to five days in advance before they return them filled with goods for the warehouse.”
Me: “All of your suppliers work this way?”
Analyst: “Pretty much.”</p>

<figure class="calibre46"><div id="FIG-lesson-vincent3" class="figure">
<img src="Images/hpp2_1203.png" alt="hpp2 1203" class="calibre127"/>
<h6 class="calibre47"><span class="publishername">Figure 12-3. </span>Carts contain the leading information that’s critical to this challenge!</h6>
</div></figure>

<p class="author1">I spotted the most significant performance issue of them all: we were solving the wrong problem. This wasn’t a machine learning problem; it was a SQL problem. The number of rented carts was a robust proxy for how many trucks the company would send. They wouldn’t need a machine learning model. We could just project the number of carts being rented a few days ahead, divide by the number of carts that fit into a truck, and get a sensible approximation of what to expect. If we had realized this earlier, we would not have needed to optimize the code for a gigantic grid search because there would have been no need.</p>

<p class="author1">It is rather straightforward to translate a business case into an analytical problem that doesn’t reflect reality. Anything that you can do to prevent this will yield the most significant performance benefit you can imagine.<a data-type="indexterm" data-primary="" data-startref="gdd_ab" id="idm46122395332488" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="" data-startref="ot_ab" id="idm46122395331512" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="" data-startref="to_ab" id="idm46122395330568" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="" data-startref="wvd_ab" id="idm46122395329624" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/></p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Adaptive Lab’s Social Media Analytics (2014)" class="calibre3"><div class="preface" id="lessons-from-field-ben">
<h1 class="calibre25">Adaptive Lab’s Social Media Analytics (2014)</h1>

<p class="byline1">Ben Jackson (<a class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6" href="http://www.adaptivelab.com/">adaptivelab.com</a>)</p>

<p class="author1"><a data-type="indexterm" data-primary="Adaptive Lab" id="al_ab" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="Jackson, Ben" id="jb_ab" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="SoMA (Social Media Analytics)" id="soma_ab" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Adaptive Lab is a product development and innovation company based in London’s Tech City area, Shoreditch.  We apply our lean, user-centric method of product design and delivery collaboratively with a wide range of companies, from start-ups to large corporates.</p>

<p class="author1">YouGov is a global market research company whose stated ambition is to supply a live stream of continuous, accurate data and insight into what people are thinking and doing all over the world—and that’s just what we managed to provide for them.  Adaptive Lab designed a way to listen passively to real discussions happening in social media and gain insight into users’ feelings on a customizable range of topics.  We built a scalable system capable of capturing a large volume of streaming information, processing it, storing it indefinitely, and presenting it through a powerful, filterable interface in real time.  The system was built using Python.</p>








<section data-type="sect2" data-pdf-bookmark="Python at Adaptive Lab" class="calibre3"><div class="preface" id="idm46122395321112">
<h2 class="calibre43">Python at Adaptive Lab</h2>

<p class="author1"><a data-type="indexterm" data-primary="Python" data-secondary="at Adaptive Lab" id="idm46122395319912" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Python is one of our core technologies.  We use it in performance-critical applications and whenever we work with clients that have in-house Python skills, so that the work we produce for them can be taken on in-house.</p>

<p class="author1">Python is ideal for small, self-contained, long-running daemons, and it’s just as great with flexible, feature-rich web frameworks like <a data-type="indexterm" data-primary="Django" id="idm46122395318024" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="Pyramid" id="idm46122395317320" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Django and Pyramid.  The Python community is thriving, which means that there’s a huge library of open source tools out there that allow us to build quickly and with confidence, leaving us to focus on the new and innovative stuff, solving problems for users.</p>

<p class="author1">Across all of our projects, we at Adaptive Lab reuse several tools that are built in Python but that can be used in a language-agnostic way.  For example, we use SaltStack for server provisioning and Mozilla’s Circus for managing long-running processes.  The benefit to us when a tool is open source and written in a language we’re familiar with is that if we find any problems, we can solve them ourselves and get those solutions taken up, which benefits the community.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="SoMA’s Design" class="calibre3"><div class="preface" id="idm46122395315112">
<h2 class="calibre43">SoMA’s Design</h2>

<p class="author1">Our Social Media Analytics (SoMA) tool needed to cope with a high throughput of social media data and the storage and retrieval in real time of a large amount of information.  After researching various data stores and search engines, we settled on<a data-type="indexterm" data-primary="Elasticsearch" id="idm46122395313528" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/> Elasticsearch as our real-time document store.  As its name suggests, it’s highly scalable, but it is also easy to use and is capable of providing statistical responses as well as search—ideal for our application.  Elasticsearch itself is built in Java, but like any well-architected component of a modern system, it has a good API and is well catered to with a Python library and tutorials.</p>

<p class="author1">The system we designed uses queues with <a data-type="indexterm" data-primary="Celery" id="idm46122395311832" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Celery held in <a data-type="indexterm" data-primary="Redis" id="idm46122395311000" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Redis to quickly hand a large stream of data to any number of servers for independent processing and indexing.  Each component of the whole complex system was designed to be small, individually simple, and able to work in isolation.  Each focused on one task, like analyzing a conversation for sentiment or preparing a document for indexing into Elasticsearch.  Several of these were configured to run as daemons using <a data-type="indexterm" data-primary="Circus" id="idm46122395309736" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Mozilla’s Circus, which keeps all the processes up and running and allows them to be scaled up or down on individual servers.</p>

<p class="author1"><a data-type="indexterm" data-primary="SaltStack" id="idm46122395308536" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>SaltStack is used to define and provision the complex cluster and handles the setup of all of the libraries, languages, databases, and document stores.  We also make use of <a data-type="indexterm" data-primary="Fabric" id="idm46122395307512" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Fabric, a Python tool for running arbitrary tasks on the command line.  Defining servers in code has many benefits: complete parity with the production environment; version control of the configuration; having everything in one place. It also serves as documentation on the setup and dependencies required by a cluster.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Our Development Methodology" class="calibre3"><div class="preface" id="idm46122395306120">
<h2 class="calibre43">Our Development Methodology</h2>

<p class="author1">We aim to make it as easy as possible for a newcomer to a project to be able to quickly get into adding code and deploying confidently.  We use <a data-type="indexterm" data-primary="Vagrant" id="idm46122395304696" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Vagrant to build the <span class="publishername">complexities</span> of a system locally, inside a virtual machine that has complete parity with the production environment.  A simple <code class="calibre26">vagrant up</code> is all a newcomer needs to get set up with all the dependencies required for their work.</p>

<p class="author1">We work in an agile way, planning together, discussing architecture decisions, and determining a consensus on task estimates.  For SoMA, we made the decision to include at least a few tasks considered as corrections for “technical debt” in each sprint.  Also included were tasks for documenting the system (we eventually established a wiki to house all the knowledge for this ever-expanding project).  Team members review each other’s code after each task, to sanity check, offer feedback, and understand the new code that is about to get added to the system.</p>

<p class="author1">A good test suite helped bolster confidence that any changes weren’t going to cause existing features to fail.  Integration tests are vital in a system like SoMA, composed of many moving parts.  A staging environment offers a way to test the performance of new code; on SoMA in particular, it was only through testing against the kind of large datasets seen in production that problems could occur and be dealt with, so it was often necessary to reproduce that amount of data in a separate environment.<a data-type="indexterm" data-primary="Amazon EC2" id="idm46122395300488" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="EC2 (Elastic Compute Cloud)" id="idm46122395299784" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/> Amazon’s Elastic Compute Cloud (EC2) gave us the flexibility to do this.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Maintaining SoMA" class="calibre3"><div class="preface" id="idm46122395298664">
<h2 class="calibre43">Maintaining SoMA</h2>

<p class="author1">The SoMA system runs continuously, and the amount of information it consumes grows every day.  We have to account for peaks in the data stream, network issues, and problems in any of the third-party service providers it relies on.  So, to make things easy on ourselves, SoMA is designed to fix itself whenever it can.  Thanks to Circus, processes that crash out will come back to life and resume their tasks from where they left off.  A task will queue up until a process can consume it, and there’s enough breathing room there to stack up tasks while the system recovers.</p>

<p class="author1">We use <a data-type="indexterm" data-primary="Server Density" id="idm46122395296360" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Server Density to monitor the many SoMA servers.  It’s very simple to set up, but quite powerful.  A nominated engineer can receive a push message via phone as soon as a problem is likely to occur, in order to react in time to ensure it doesn’t become a problem.  With Server Density, it’s also very easy to write custom plug-ins in Python, making it possible, for example, to set up instant alerts on aspects of 
<span class="publishername">Elasticsearch’s</span> behavior.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Advice for Fellow Engineers" class="calibre3"><div class="preface" id="idm46122395294056">
<h2 class="calibre43">Advice for Fellow Engineers</h2>

<p class="author1">Above all, you and your team need to be confident and comfortable that what is about to be deployed into a live environment is going to work flawlessly.  To get to that point, you have to work backward, spending time on all of the components of the system that will give you that sense of comfort. Make deployment simple and foolproof; use a staging environment to test the performance with real-world data; ensure you have a good, solid test suite with high coverage; implement a process for incorporating new code into the system; and make sure technical debt gets addressed sooner rather than later.  The more you shore up your technical infrastructure and improve your processes, the happier and more successful at engineering the right solutions your team will be.</p>

<p class="author1">If a solid foundation of code and ecosystem are not in place but the business is pressuring you to get things live, it’s only going to lead to problem software.  It’s going to be your responsibility to push back and stake out time for incremental improvements to the code, and the tests and operations involved in getting things out the door.<a data-type="indexterm" data-primary="" data-startref="al_ab" id="idm46122395291160" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="" data-startref="jb_ab" id="idm46122395290184" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="" data-startref="soma_ab" id="idm46122395289240" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/></p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Making Deep Learning Fly with RadimRehurek.com (2014)" class="calibre3"><div class="preface" id="lessons-from-field-radim">
<h1 class="calibre25">Making Deep Learning Fly with RadimRehurek.com (2014)</h1>

<p class="byline1">Radim Řehůřek (<a href="http://www.radimrehurek.com" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"><em class="hyperlink">radimrehurek.com</em></a>)</p>

<p class="author1"><a data-type="indexterm" data-primary="deep learning" id="dl_ab" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="RadimRehurek.com" id="rr_ab" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>When Ian asked me to write my “lessons from the field” on Python and optimizations for this book, I immediately thought, “Tell them how you made a Python port faster than Google’s C original!” It’s an inspiring story of making a machine learning algorithm, Google’s poster child for deep learning, 12,000× faster than a naive Python implementation. Anyone can write bad code and then trumpet about large speedups. But the optimized Python port also runs, somewhat astonishingly, almost four times faster than the original code written by Google’s team! That is, four times faster than opaque, tightly profiled, and optimized C.</p>

<p class="author1">But before drawing “machine-level” optimization lessons, some general advice about “human-level” optimizations.</p>








<section data-type="sect2" data-pdf-bookmark="The Sweet Spot" class="calibre3"><div class="preface" id="idm46122395281608">
<h2 class="calibre43">The Sweet Spot</h2>

<p class="author1">I run a small consulting business laser-focused on machine learning, where my colleagues and I help companies make sense of the tumultuous world of data analysis, in order to make money or save costs (or both). We help clients design and build wondrous systems for data processing, especially text data.</p>

<p class="author1">The clients range from large multinationals to nascent start-ups, and while each project is different and requires a different tech stack, plugging into the client’s existing data flows and pipelines, Python is a clear favorite. Not to preach to the choir, but Python’s no-nonsense development philosophy, its malleability, and the rich library ecosystem make it an ideal choice.</p>

<p class="author1">First, a few thoughts “from the field” on what works:</p>
<dl class="calibre28">
<dt class="calibre29">Communication, communication, communication</dt>
<dd class="calibre30">
<p class="calibre31">This one’s obvious, but worth repeating. Understand the client’s problem on a higher (business) level before deciding on an approach. Sit down and talk through what they think they need (based on their partial knowledge of what’s possible and/or what they Googled up before contacting you), until it becomes clear what they really need, free of cruft and preconceptions. Agree on ways to validate the solution beforehand. I like to visualize this process as a long, winding road to be built: get the starting line right (problem definition, available data sources) and the finish line right (evaluation, solution priorities), and the path in between falls into place.</p>
</dd>
<dt class="calibre29">Be on the lookout for promising technologies</dt>
<dd class="calibre30">
<p class="calibre31">An emergent technology that is reasonably well understood and robust, is gaining traction, yet is still relatively obscure in the industry can bring huge value to the client (or yourself). As an example, a few years ago, <a data-type="indexterm" data-primary="Elasticsearch" id="idm46122395274360" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Elasticsearch was a little-known and somewhat raw open source project. But I evaluated its approach as solid (built on top of Apache Lucene, offering replication, cluster sharding, etc.) and recommended its use to a client. We consequently built a search system with Elasticsearch at its core, saving the client significant amounts of money in licensing, development, and maintenance <span class="publishername">compared</span> to the considered alternatives (large commercial databases). Even more importantly, using a new, flexible, powerful technology gave the product a massive competitive advantage. Nowadays, Elasticsearch has entered the enterprise market and conveys no competitive advantage at all—everyone knows it and uses it. Getting the timing right is what I call hitting the “sweet spot,” maximizing the value/cost ratio.</p>
</dd>
<dt class="calibre29">KISS (Keep It Simple, Stupid!)</dt>
<dd class="calibre30">
<p class="calibre31">This is another no-brainer. The best code is code you don’t have to write and maintain. Start simple, and improve and iterate where necessary. I prefer tools that follow the Unix philosophy of “do one thing, and do it well.” Grand programming frameworks can be tempting, with everything imaginable under one roof and fitting neatly together. But invariably, sooner or later, you need something the grand framework didn’t imagine, and then even <span class="publishername">modifications</span> that seem simple (conceptually) cascade into a nightmare (programmatically). Grand projects and their all-encompassing APIs tend to collapse under their own weight. Use modular, focused tools, with APIs in between that are as small and uncomplicated as possible. Prefer text formats that are open to simple visual inspection, unless performance dictates otherwise.</p>
</dd>
<dt class="calibre29">Use manual sanity checks in data pipelines</dt>
<dd class="calibre30">
<p class="calibre31">When optimizing data processing systems, it’s easy to stay in the “binary mindset” mode, using tight pipelines, efficient binary data formats, and compressed I/O. As the data passes through the system unseen, and unchecked (except for perhaps its type), it remains invisible until something outright blows up. Then debugging commences. I advocate sprinkling a few simple log messages throughout the code, showing what the data looks like at various internal points of processing, as good practice—nothing fancy, just an analogy to the Unix <code class="calibre26">head</code> command, picking and visualizing a few data points. Not only does this help during the aforementioned debugging, but seeing the data in a human-readable format leads to “aha!” moments surprisingly often, even when all seems to be going well. Strange tokenization! They promised input would always be encoded in latin1! How did a document in this language get in there? Image files leaked into a pipeline that expects and parses text files! These are often insights that go way beyond those offered by automatic type checking or a fixed unit test, hinting at issues beyond component boundaries. Real-world data is messy. Catch early even things that wouldn’t necessarily lead to exceptions or glaring errors. Err on the side of too much verbosity.</p>
</dd>
<dt class="calibre29">Navigate fads carefully</dt>
<dd class="calibre30">
<p class="calibre31">Just because a client keeps hearing about X and says they must have X too doesn’t mean they really need it. It might be a marketing problem rather than a technology one, so take care to discern the two and deliver accordingly. X changes over time as hype waves come and go; a recent value would be X = big data.</p>
</dd>
</dl>

<p class="author1">All right, enough business talk—here’s how I got <em class="hyperlink">word2vec</em> in Python to run faster <span class="publishername">than C</span>.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Lessons in Optimizing" class="calibre3"><div class="preface" id="idm46122395262808">
<h2 class="calibre43">Lessons in Optimizing</h2>

<p class="author1"><a href="https://oreil.ly/SclZ0" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"><em class="hyperlink">word2vec</em></a> <a data-type="indexterm" data-primary="word2vec" id="idm46122395260584" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>is a deep learning algorithm that allows detection of similar words and phrases. With interesting applications in text analytics and search engine optimization (SEO), and with <a data-type="indexterm" data-primary="Google" id="idm46122395259560" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Google’s lustrous brand name attached to it, start-ups and businesses flocked to take advantage of this new tool.</p>

<p class="author1">Unfortunately, the only available code was that produced by Google itself, an open source Linux command-line tool written in C. This was a well-optimized but rather hard-to-use implementation. The primary reason I decided to port <em class="hyperlink">word2vec</em> to Python was so I could extend <em class="hyperlink">word2vec</em> to other platforms, making it easier to integrate and extend for clients.</p>

<p class="author1">The details are not relevant here, but <em class="hyperlink">word2vec</em> requires a training phase with a lot of input data to produce a useful similarity model. For example, the folks at Google ran <em class="hyperlink">word2vec</em> on their GoogleNews dataset, training on approximately 100 billion words. Datasets of this scale obviously don’t fit in RAM, so a memory-efficient approach must be taken.</p>

<p class="author1">I’ve authored a machine learning library<a data-type="indexterm" data-primary="Gensim library" id="gen_ab" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>, <a href="https://oreil.ly/6SYgs" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"><code class="calibre26">gensim</code></a>, that targets exactly that sort of memory-optimization problem: datasets that are no longer trivial (“trivial” being anything that fits fully into RAM), yet not large enough to warrant petabyte-scale clusters of MapReduce computers. This “terabyte” problem range fits a surprisingly large portion of real-world cases, <em class="hyperlink">word2vec</em> included.</p>

<p class="author1">Details are described <a href="http://bit.ly/RR_blog" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6">on my blog</a>, but here are a few optimization takeaways:</p>
<dl class="calibre28">
<dt class="calibre29">Stream your data, watch your memory</dt>
<dd class="calibre30">
<p class="calibre31">Let your input be accessed and processed one data point at a time, for a small, constant memory footprint. The streamed data points (sentences, in the case of <em class="hyperlink">word2vec</em>) may be grouped into larger batches internally for performance (such as processing 100 sentences at a time), but a high-level, streamed API proved a powerful and flexible abstraction. The Python <span class="publishername">language</span> supports this pattern very naturally and elegantly, with its built-in generators—a truly beautiful problem–tech match. Avoid committing to algorithms and tools that load everything into RAM, unless you know your data will always remain small, or you don’t mind reimplementing a production version yourself later.</p>
</dd>
<dt class="calibre29">Take advantage of Python’s rich ecosystem</dt>
<dd class="calibre30">
<p class="calibre31">I started with a readable, clean port of <em class="hyperlink">word2vec</em> in <code class="calibre26">numpy</code>. <code class="calibre26">numpy</code> is covered in depth in  <a data-type="xref" href="ch06_split_000.xhtml#matrix_computation" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6">Chapter 6</a> of this book, but as a short reminder, it is an amazing library, a cornerstone of Python’s scientific community and the de facto standard for number crunching in Python. Tapping into <code class="calibre26">numpy</code>’s powerful array interfaces, memory access patterns, and wrapped BLAS routines for ultrafast common vector operations leads to concise, clean, and fast code—code that is hundreds of times faster than naive Python code. Normally I’d call it a day at this point, but “hundreds of times faster” was still 20× slower than Google’s optimized C version, so I pressed on.</p>
</dd>
<dt class="calibre29">Profile and compile hotspots</dt>
<dd class="calibre30">
<p class="calibre31"><em class="hyperlink">word2vec</em> is a typical high performance computing app, in that a few lines of code in one inner loop account for 90% of the entire training runtime. Here I rewrote a single core routine (approximately 20 lines of code) in C, using an external Python library, <a data-type="indexterm" data-primary="Cython" data-secondary="about" id="idm46122395240904" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Cython, as the glue. While it’s technically brilliant, I don’t consider Cython a particularly convenient tool conceptually—it’s basically like learning another language, a nonintuitive mix between Python,<a data-type="indexterm" data-primary="numpy" data-secondary="about" id="idm46122395239576" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/> <code class="calibre26">numpy</code>, and C, with its own caveats and idiosyncrasies. But until Python’s JIT technologies mature, Cython is probably our best bet. With a Cython-compiled hotspot, performance of the Python <em class="hyperlink">word2vec</em> port is now on par with the original C code. An additional advantage of having started with a clean <code class="calibre26">numpy</code> version is that we get free tests for correctness, by comparing against the slower but correct version.</p>
</dd>
<dt class="calibre29">Know your BLAS</dt>
<dd class="calibre30">
<p class="calibre31">A neat feature of <code class="calibre26">numpy</code> is that it internally wraps<a data-type="indexterm" data-primary="BLAS (Basic Linear Algebra Subprograms)" id="idm46122395234984" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="Basic Linear Algebra Subprograms (BLAS)" id="idm46122395234280" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/> Basic Linear Algebra Subprograms (BLAS), where available. These are sets of low-level routines, optimized directly by processor vendors (Intel, AMD, etc.) in assembly, Fortran, or C, and designed to squeeze out maximum performance from a particular processor architecture. For example, calling an axpy BLAS routine computes <code class="calibre26">vector_y += scalar * vector_x</code> way faster than what a generic compiler would produce for an equivalent explicit <code class="calibre26">for</code> loop. Expressing <em class="hyperlink">word2vec</em> training as BLAS operations resulted in another 4× speedup, topping the performance of C <em class="hyperlink">word2vec</em>. Victory! To be fair, the C code could link to BLAS as well, so this is not some inherent advantage of Python per se. <code class="calibre26">numpy</code> just makes things like this stand out and makes them easy to take advantage of.</p>
</dd>
<dt class="calibre29">Parallelization and multiple cores</dt>
<dd class="calibre30">
<p class="calibre31"><code class="calibre26">gensim</code> contains distributed cluster implementations of a few algorithms. For <em class="hyperlink">word2vec</em>, I opted for multithreading on a single machine, because of the fine-grained nature of its training algorithm. Using threads also allows us to avoid the fork-without-exec POSIX issues that Python’s multiprocessing brings, especially in combination with certain BLAS libraries. Because our core routine is already in Cython, we can afford to release Python’s<a data-type="indexterm" data-primary="GIL (global interpreter lock)" id="idm46122395228376" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="global interpreter lock (GIL)" id="idm46122395227704" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/> GIL (global interpreter lock; see <a data-type="xref" href="ch07.xhtml#compiling-cython-omp" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6">“Parallelizing the Solution with OpenMP on One Machine”</a>), which normally renders multithreading useless for CPU-intensive tasks. Speedup: another 3×, on a machine with four cores.</p>
</dd>
<dt class="calibre29">Static memory allocations</dt>
<dd class="calibre30">
<p class="calibre31"><a data-type="indexterm" data-primary="static memory" id="idm46122395224776" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>At this point, we’re processing tens of thousands of sentences per second. Training is so fast that even little things like creating a new <code class="calibre26">numpy</code> array (calling <code class="calibre26">malloc</code> for each streamed sentence) slow us down. Solution: preallocate a static “work” memory and pass it around, in good old Fortran fashion. Brings tears to my eyes. The lesson here is to keep as much bookkeeping and app logic in the clean Python code as possible, and to keep the optimized hotspot lean and mean.</p>
</dd>
<dt class="calibre29">Problem-specific optimizations</dt>
<dd class="calibre30">
<p class="calibre31">The original C implementation contained specific micro-optimizations, such as aligning arrays onto specific memory boundaries or precomputing certain functions into memory lookup tables. A nostalgic blast from the past, but with today’s complex CPU instruction pipelines, memory cache hierarchies, and coprocessors, such optimizations are no longer a clear winner. Careful profiling suggested a few percent improvement, which may not be worth the extra code complexity. Takeaway: use annotation and profiling tools to highlight poorly optimized spots. Use your domain knowledge to introduce algorithmic approximations that trade accuracy for performance (or vice versa). But never take it on faith; profile, preferably using real production data.<a data-type="indexterm" data-primary="" data-startref="gen_ab" id="idm46122395220520" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/></p>
</dd>
</dl>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Conclusion" class="calibre3"><div class="preface" id="idm46122395250536">
<h2 class="calibre43">Conclusion</h2>

<p class="author1">Optimize where appropriate. In my experience, there’s never enough communication to fully ascertain the problem scope, priorities, and connection to the client’s business goals—a.k.a. the “human-level” optimizations. Make sure you deliver on a problem that matters, rather than getting lost in “geek stuff” for the sake of it. And when you do roll up your sleeves, make it worth it!<a data-type="indexterm" data-primary="" data-startref="dl_ab" id="idm46122395217736" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="" data-startref="rr_ab" id="idm46122395216760" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/></p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Large-Scale Productionized Machine Learning  at Lyst.com (2014)" class="calibre3"><div class="preface" id="lessons-from-field-sebastjan">
<h1 class="calibre25">Large-Scale Productionized Machine Learning 
<span class="publishername">at Lyst.com (2014)</span></h1>

<p class="byline1">Sebastjan Trepca (<a class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6" href="http://www.lyst.com">lyst.com</a>)</p>
<aside data-type="sidebar" epub:type="sidebar" class="calibre40"><div class="sidebar" id="idm46122395211736">
<h5 class="calibre41"/>
<p class="calibre73"><a data-type="indexterm" data-primary="Lyst.com" id="lc_abt" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="machine learning" id="ml_abt" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Lyst.com is a fashion recommendation engine based in London; it has over 2 million monthly users who learn about new fashion through Lyst’s scraping, cleaning, and modeling processes. Founded in 2010, it has raised $20 million of investment.</p>

<p class="calibre73"><a data-type="indexterm" data-primary="Trepca, Sebastjan" id="idm46122395208104" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Sebastjan Trepca was the technical founder and is the CTO; he created the site using Django, and Python has helped the team to quickly test new ideas.</p>
</div></aside>

<p class="author1"><a data-type="indexterm" data-primary="Python" data-secondary="at Lyst" id="idm46122395206728" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="Django" id="idm46122395205752" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Python and Django have been at the heart of Lyst since the site’s creation. As internal projects have grown, some of the Python components have been replaced with other tools and languages to fit the maturing needs of the system.</p>








<section data-type="sect2" data-pdf-bookmark="Cluster Design" class="calibre3"><div class="preface" id="idm46122395204584">
<h2 class="calibre43">Cluster Design</h2>

<p class="author1"><a data-type="indexterm" data-primary="cluster design" id="idm46122395203416" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>The cluster runs on <a data-type="indexterm" data-primary="Amazon EC2" id="idm46122395202584" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="EC2 (Elastic Compute Cloud)" id="idm46122395201880" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Amazon EC2. In total there are approximately 100 machines, including the more recent C3 instances, which have good CPU performance.</p>

<p class="author1"><a data-type="indexterm" data-primary="Redis" id="idm46122395200664" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Redis is used for queuing with <a data-type="indexterm" data-primary="PyRes" id="idm46122395199832" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>PyRes and storing metadata. The dominant data format is <a data-type="indexterm" data-primary="JSON" id="idm46122395199032" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>JSON, for ease of human comprehension.<a data-type="indexterm" data-primary="supervisord" id="idm46122395198104" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/> <code class="calibre26">supervisord</code> keeps the processes alive.</p>

<p class="author1"><a data-type="indexterm" data-primary="Elasticsearch" id="idm46122395196552" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="PyES" id="idm46122395195848" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Elasticsearch and PyES are used to index all products. The Elasticsearch cluster stores 60 million documents across seven machines. <a data-type="indexterm" data-primary="Solr" id="idm46122395194904" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Solr was investigated but discounted because of its lack of real-time updating features.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Code Evolution in a Fast-Moving Start-Up" class="calibre3"><div class="preface" id="idm46122395193736">
<h2 class="calibre43">Code Evolution in a Fast-Moving Start-Up</h2>

<p class="author1">It is better to write code that can be implemented quickly so that a business idea can be tested than to spend a long time attempting to write “perfect code” in the first pass. If code is useful, it can be refactored; if the idea behind the code is poor, it is cheap to delete it and remove a feature. This can lead to a complicated code base with many objects being passed around, but this is acceptable as long as the team makes time to refactor code that is useful to the business.</p>

<p class="author1"><a data-type="indexterm" data-primary="Docstrings" id="idm46122395191608" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Docstrings are used heavily in Lyst—an external Sphinx documentation system was tried but dropped in favor of just reading the code. A wiki is used to document processes and larger systems. We also started creating very small services instead of chucking everything into one code base.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Building the Recommendation Engine" class="calibre3"><div class="preface" id="idm46122395190216">
<h2 class="calibre43">Building the Recommendation Engine</h2>

<p class="author1">At first the recommendation engine was coded in Python, using <code class="calibre26">numpy</code> and <code class="calibre26">scipy</code> for computations. Subsequently, performance-critical parts of the recommender were sped up using <a data-type="indexterm" data-primary="Cython" data-secondary="about" id="idm46122395187624" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Cython. The core matrix factorization operations were written entirely in Cython, yielding an order of magnitude improvement in speed. This was mostly due to the ability to write performant loops over <a data-type="indexterm" data-primary="numpy" data-secondary="about" id="idm46122395186296" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><code class="calibre26">numpy</code> arrays in Python, something that is extremely slow in pure Python and performed poorly when vectorized because it necessitated memory copies of <code class="calibre26">numpy</code> arrays. The culprit was <code class="calibre26">numpy</code>’s fancy indexing, which always makes a data copy of the array being sliced: if no data copy is necessary or intended, Cython loops will be far faster.</p>

<p class="author1">Over time, the online components of the system (responsible for computing recommendations at request time) were integrated into our search component, Elasticsearch. In the process, they were translated into <a data-type="indexterm" data-primary="Java" id="idm46122395183320" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Java to allow full integration with Elasticsearch. The main reason behind this was not performance, but the utility of integrating the recommender with the full power of a search engine, allowing us to apply business rules to served recommendations more easily. The Java component itself is extremely simple and implements primarily efficient sparse vector inner products. The more complex offline component remains written in Python, using standard components of the Python scientific stack (mostly Python and Cython).</p>

<p class="author1">In our experience, Python is useful as more than a prototyping language: the availability of tools such as <code class="calibre26">numpy</code>, Cython, and <code class="calibre26">weave</code> (and more recently Numba) allowed us to achieve very good performance in the performance-critical parts of the code while maintaining Python’s clarity and expressiveness where low-level optimization would be counterproductive.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Reporting and Monitoring" class="calibre3"><div class="preface" id="idm46122395180056">
<h2 class="calibre43">Reporting and Monitoring</h2>

<p class="author1"><a data-type="indexterm" data-primary="Graphite" id="idm46122395178712" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Graphite is used for reporting. Currently, performance regressions can be seen by eye after a deployment. This makes it easy to drill into detailed event reports or to zoom out and see a high-level report of the site’s behavior, adding and removing events as necessary.</p>

<p class="author1">Internally, a larger infrastructure for performance testing is being designed. It will include representative data and use cases to properly test new builds of the site.</p>

<p class="author1">A staging site will also be used to let a small fraction of real visitors see the latest version of the deployment—if a bug or performance regression is seen, it will have affected only a minority of visitors, and this version can quickly be retired. This will make the deployment of bugs significantly less costly and problematic.</p>

<p class="author1"><a data-type="indexterm" data-primary="Sentry" id="idm46122395175960" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Sentry is used to log and diagnose Python stack traces.</p>

<p class="author1"><a data-type="indexterm" data-primary="Jenkins" id="idm46122395174808" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Jenkins is used for <a data-type="indexterm" data-primary="CI (Continuous Integration)" id="idm46122395173976" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="Continuous Integration (CI)" id="idm46122395173208" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>continuous integration (CI) with an in-memory database configuration. This enables parallelized testing so that check-ins quickly reveal any bugs to the developer.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Some Advice" class="calibre3"><div class="preface" id="idm46122395172088">
<h2 class="calibre43">Some Advice</h2>

<p class="author1">It’s really important to have good tools to track the effectiveness of what you’re building, and to be super-practical at the beginning. Start-ups change constantly, and engineering evolves: you start with a super-exploratory phase, building prototypes all the time and deleting code until you hit the gold mine, and then you start to go deeper, improving code, performance, etc. Until then, it’s all about quick iterations and good monitoring/analytics. I guess this is pretty standard advice that has been repeated over and over, but I think many don’t really get how important it is.</p>

<p class="author1">I don’t think technologies matter that much nowadays, so use whatever works for you. I’d think twice before moving to hosted environments like <a data-type="indexterm" data-primary="App Engine" id="idm46122395169640" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>App Engine or <a data-type="indexterm" data-primary="Heroku" id="idm46122395168808" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Heroku, though.<a data-type="indexterm" data-primary="" data-startref="lc_abt" id="idm46122395167976" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="" data-startref="ml_abt" id="idm46122395167000" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/></p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Large-Scale Social Media Analysis at Smesh (2014)" class="calibre3"><div class="preface" id="lessons-from-field-alex">
<h1 class="calibre25">Large-Scale Social Media Analysis at Smesh (2014)</h1>

<p class="byline1">Alex Kelly (<a href="http://www.sme.sh" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"><em class="hyperlink">sme.sh</em></a>)</p>

<p class="author1"><a data-type="indexterm" data-primary="Kelly, Alex" id="ka_ab" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="Smesh" id="sme_ab" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="social media analysis" id="sma_ab" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>At Smesh, we produce software that ingests data from a wide variety of APIs across the web; filters, processes, and aggregates it; and then uses that data to build bespoke apps for a variety of clients. For example, we provide the tech that powers the tweet filtering and streaming in Beamly’s second-screen TV app, run a brand and campaign monitoring platform for mobile network EE, and run a bunch of Adwords data analysis projects for Google.</p>

<p class="author1">To do that, we run a variety of streaming and polling services, frequently polling Twitter, Facebook, YouTube, and a host of other services for content and processing several million tweets daily.</p>








<section data-type="sect2" data-pdf-bookmark="Python’s Role at Smesh" class="calibre3"><div class="preface" id="idm46122395158568">
<h2 class="calibre43">Python’s Role at Smesh</h2>

<p class="author1"><a data-type="indexterm" data-primary="Python" data-secondary="at Smesh" id="idm46122395157400" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>We use Python extensively—the majority of our platform and services are built with it. The wide variety of libraries, tools, and frameworks available allows us to use it across the board for most of what we do.</p>

<p class="author1">That variety gives us the ability to (hopefully) pick the right tool for the job. For example, we’ve created apps using <a data-type="indexterm" data-primary="Django" id="idm46122395155320" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="Flask" id="idm46122395154616" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="Pyramid" id="idm46122395153944" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Django, Flask, and Pyramid. Each has its own benefits, and we can pick the one that’s right for the task at hand. We use <a data-type="indexterm" data-primary="Celery" id="idm46122395153000" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Celery for tasks; <a data-type="indexterm" data-primary="Boto" id="idm46122395152200" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Boto for interacting with <a data-type="indexterm" data-primary="AWS (Amazon Web Services)" id="idm46122395151368" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="Amazon Web Services (AWS)" id="idm46122395150680" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>AWS; and <a data-type="indexterm" data-primary="PyMongo" id="idm46122395149864" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>PyMongo, <a data-type="indexterm" data-primary="MongoEngine" id="idm46122395149032" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="redis-py" id="idm46122395148328" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="Psycopg" id="idm46122395147656" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>MongoEngine, redis-py, Psycopg, etc. for all our data needs. The list goes on and on.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="The Platform" class="calibre3"><div class="preface" id="idm46122395146728">
<h2 class="calibre43">The Platform</h2>

<p class="author1">Our main platform consists of a central Python module that provides hooks for data input, filtering, aggregations and processing, and a variety of other core functions. Project-specific code imports functionality from that core and then implements more specific data processing and view logic, as each application requires.</p>

<p class="author1">This has worked well for us up to now, and allows us to build fairly complex applications that ingest and process data from a wide variety of sources without much duplication of effort. However, it isn’t without its drawbacks—each app is dependent on a common core module, making the process of updating the code in that module and keeping all the apps that use it up-to-date a major task.</p>

<p class="author1">We’re currently working on a project to redesign that core software and move toward more of a <a data-type="indexterm" data-primary="service-oriented architecture (SoA)" id="idm46122395143768" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="SoA (service-oriented architecture)" id="idm46122395143048" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>service-oriented architecture (SoA) approach. It seems that finding the right time to make that sort of architectural change is one of the challenges that faces most software teams as a platform grows. There is overhead in building components as individual services, and often the deep domain-specific knowledge required to build each service is acquired only through an initial iteration of development, where that architectural overhead is a hindrance to solving the real problem at hand. Hopefully, we’ve chosen a sensible time to revisit our architectural choices to move things forward. Time will tell.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="High Performance Real-Time String Matching" class="calibre3"><div class="preface" id="idm46122395141352">
<h2 class="calibre43">High Performance Real-Time String Matching</h2>

<p class="author1"><a data-type="indexterm" data-primary="string matching, high performance real-time" id="sm_abt" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>We consume lots of data from the <a data-type="indexterm" data-primary="Twitter Streaming API" id="idm46122395138776" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Twitter Streaming API. As we stream in tweets, we match the input strings against a set of keywords so that we know which of the terms we’re tracking that each tweet is related to. That’s not such a problem with a low rate of input, or a small set of keywords, but doing that matching for hundreds of tweets per second, against hundreds or thousands of possible keywords, starts to get tricky.</p>

<p class="author1">To make things even trickier, we’re not interested in simply whether the keyword string exists in the tweet, but in more complex pattern matching against word boundaries, start and end of line, and optionally the use of # and @ characters to prefix the string. The most effective way to encapsulate that matching knowledge is using regular expressions. However, running thousands of regex patterns across hundreds of tweets per second is computationally intensive. Previously, we had to run many worker nodes across a cluster of machines to perform the matching reliably in real time.</p>

<p class="author1">Knowing this was a major performance bottleneck in the system, we tried a variety of things to improve the performance of our matching system: simplifying the regexes, running enough processes to ensure we were utilizing all the cores on our servers, ensuring all our regex patterns are compiled and cached properly, running the matching tasks under PyPy instead of CPython, etc. Each of these resulted in a small increase in performance, but it was clear this approach was going to shave only a fraction of our processing time. We were looking for an order-of-magnitude speedup, not a fractional improvement.</p>

<p class="author1">It was obvious that rather than trying to increase the performance of each match, we needed to reduce the problem space before the pattern matching takes place. So we needed to reduce either the number of tweets to process, or the number of regex patterns we needed to match the tweets against. Dropping the incoming tweets wasn’t an option—that’s the data we’re interested in. So we set about finding a way to reduce the number of patterns we need to compare an incoming tweet to in order to perform the matching.</p>

<p class="author1">We started looking at various trie structures for allowing us to do pattern matching between sets of strings more efficiently, and came across the Aho-Corasick string-matching algorithm. It turned out to be ideal for our use case. The dictionary from which the trie is built needs to be static—you can’t add new members to the trie after the automaton has been finalized—but for us this isn’t a problem, as the set of keywords is static for the duration of a session streaming from Twitter. When we change the terms we’re tracking we must disconnect from and reconnect to the API, so we can rebuild the Aho-Corasick trie at the same time.</p>

<p class="author1">Processing an input against the strings using <a data-type="indexterm" data-primary="Aho-Corasick trie" id="idm46122395133064" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Aho-Corasick finds all possible matches simultaneously, stepping through the input string a character at a time and finding matching nodes at the next level down in the trie (or not, as the case may be). So we can very quickly find which of our keyword terms may exist in the tweet. We still don’t know for sure, as the pure string-in-string matching of Aho-Corasick doesn’t allow us to apply any of the more complex logic that is encapsulated in the regex patterns, but we can use the Aho-Corasick matching as a prefilter. Keywords that don’t exist in the string can’t match, so we know we have to try only a small subset of all our regex patterns, based on the keywords that do appear in the text. Rather than evaluating hundreds or thousands of regex patterns against every input, we rule out the majority and need to process only a handful for each tweet.</p>

<p class="author1">By reducing the number of patterns that we attempt to match against each incoming tweet to just a small handful, we’ve managed to achieve the speedup we were looking for. Depending on the complexity of the trie and the average length of the input tweets, our keyword matching system now performs somewhere between 10–100× faster than the original naive implementation.</p>

<p class="author1">If you’re doing a lot of regex processing, or other pattern matching, I highly recommend having a dig around the different variations of prefix and suffix tries that might help you to find a blazingly fast solution to your problem.<a data-type="indexterm" data-primary="" data-startref="sm_abt" id="idm46122395129912" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Reporting, Monitoring, Debugging, and Deployment" class="calibre3"><div class="preface" id="idm46122395128808">
<h2 class="calibre43">Reporting, Monitoring, Debugging, and Deployment</h2>

<p class="author1">We maintain a bunch of different systems running our Python software and the rest of the infrastructure that powers it all. Keeping it all up and running without interruption can be tricky. Here are a few lessons we’ve learned along the way.</p>

<p class="author1">It’s really powerful to be able to see both in real time and historically what’s going on inside your systems, whether that be in your own software or the infrastructure it runs on. We use <a data-type="indexterm" data-primary="Graphite" id="idm46122395126232" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Graphite with <code class="calibre26">collectd</code> and <code class="calibre26">statsd</code> to allow us to draw pretty graphs of what’s going on. That gives us a way to spot trends, and to retrospectively analyze problems to find the root cause. We haven’t got around to implementing it yet, but Etsy’s <a data-type="indexterm" data-primary="Skyline" id="idm46122395124296" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Skyline also looks brilliant as a way to spot the unexpected when you have more metrics than you can keep track of. Another useful tool is <a data-type="indexterm" data-primary="Sentry" id="idm46122395123304" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Sentry, a great system for event logging and keeping track of exceptions being raised across a cluster of machines.</p>

<p class="author1">Deployment can be painful, no matter what you’re using to do it. We’ve been users of <a data-type="indexterm" data-primary="Puppet" id="idm46122395122008" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="Ansible" id="idm46122395121304" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="Salt" id="idm46122395120632" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Puppet, Ansible, and Salt. They all have pros and cons, but none of them will make a complex deployment problem magically go away.</p>

<p class="author1">To maintain high availability for some of our systems, we run multiple geographically distributed clusters of infrastructure, running one system live and others as hot spares, with switchover being done by updates to DNS with low <a data-type="indexterm" data-primary="Time-to-Live (TTL) values" id="idm46122395119064" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="TTL (Time-to-Live) values" id="idm46122395118424" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Time-to-Live (TTL) values. Obviously that’s not always straightforward, especially when you have tight constraints on data consistency. Thankfully, we’re not affected by that too badly, making the <span class="publishername">approach</span> relatively straightforward. It also provides us with a fairly safe deployment strategy, updating one of our spare clusters and performing testing before promoting that cluster to live and updating the others.</p>

<p class="author1">Along with everyone else, we’re really excited by the prospect of what can be done with <a data-type="indexterm" data-primary="Docker" data-secondary="about" id="idm46122395115832" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a href="http://www.docker.com" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6">Docker</a>. Also along with pretty much everyone else, we’re still just at the stage of playing around with it to figure out how to make it part of our deployment processes. However, having the ability to rapidly deploy our software in a lightweight and reproducible fashion, with all its binary dependencies and system libraries included, seems to be just around the corner.</p>

<p class="author1">At a server level, there’s a whole bunch of routine stuff that just makes life easier. <a data-type="indexterm" data-primary="Monit" id="idm46122395113064" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Monit is great for keeping an eye on things for you. <a data-type="indexterm" data-primary="Upstart" id="idm46122395112168" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="supervisord" id="idm46122395111496" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Upstart and <code class="calibre26">supervisord</code> make running services less painful. <a data-type="indexterm" data-primary="Munin" id="idm46122395110280" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Munin is useful for some quick and easy system-level graphing if you’re not using a full Graphite/<code class="calibre26">collectd</code> setup. And <a data-type="indexterm" data-primary="Corosync/Pacemaker" id="idm46122395108952" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Corosync/Pacemaker can be a good solution for running services across a cluster of nodes (for example, when you have a bunch of services that you need to run somewhere, but not everywhere).</p>

<p class="author1">I’ve tried not to just list buzzwords here, but to point you toward software we’re using every day, which is really making a difference in how effectively we can deploy and run our systems. If you’ve heard of them all already, I’m sure you must have a whole bunch of other useful tips to share, so please drop me a line with some pointers. If not, go check them out—hopefully, some of them will be as useful to you as they are to us.<a data-type="indexterm" data-primary="" data-startref="ka_ab" id="idm46122395107160" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="" data-startref="sme_ab" id="idm46122395106184" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="" data-startref="sma_ab" id="idm46122395105240" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/></p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="PyPy for Successful Web and Data Processing Systems (2014)" class="calibre3"><div class="preface" id="lessons-from-field-marko">
<h1 class="calibre25">PyPy for Successful Web and Data Processing Systems (2014)</h1>

<p class="byline1">Marko Tasic (<a href="https://github.com/mtasic85" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"><em class="hyperlink">https://github.com/mtasic85</em></a>)</p>

<p class="author1"><a data-type="indexterm" data-primary="data" data-secondary="PyPy for processing" id="d_pypy" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="processing systems, PyPy for web and data" id="ps_pypy" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="PyPy" data-secondary="for web and data processing systems" id="pypy_dps" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="web processing systems, PyPy for" id="wps_pypy" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="Tasic, Marko" id="tm_ab" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Since I had a great experience early on with PyPy, I chose to use it everywhere where it was applicable. I have used it from small toy projects where speed was essential to medium-sized projects. The first project where I used it was a protocol implementation; the protocols we implemented were <a data-type="indexterm" data-primary="Modbus" id="idm46122395094744" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="DNP3" id="idm46122395094072" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Modbus and DNP3. Later, I used it for a compression algorithm implementation, and everyone was amazed by its speed. The first version I used in production was PyPy 1.2 with JIT out of the box, if I recall correctly. By version 1.4, we were sure it was the future of all our projects, because many bugs got fixed and the speed just increased more and more. We were surprised how simple cases were made 2–3× faster just by upgrading PyPy up to the next version.</p>

<p class="author1">I will explain two separate but deeply related projects that share 90% of the same code here, but to keep the explanation simple to follow, I will refer to both of them as “the project.”</p>

<p class="author1">The project was to create a system that collects newspapers, magazines, and blogs, applies <a data-type="indexterm" data-primary="OCR (optical character recognition)" id="idm46122395091672" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="optical character recognition (OCR)" id="idm46122395090952" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>optical character recognition (OCR) if necessary, classifies them, translates, applies sentiment analyzing, analyzes the document structure, and indexes them for later search. Users can search for keywords in any of the available languages and retrieve information about indexed documents. Search is cross-language, so users can write in English and get results in French. Additionally, users will receive articles and keywords highlighted from the document’s page with information about the space occupied and price of publication. A more advanced use case would be report generation, where users can see a tabular view of results with detailed information on spending by any particular company on advertising in monitored newspapers, magazines, and blogs. As well as advertising, it can also “guess” if an article is paid or objective and determine its tone.</p>








<section data-type="sect2" data-pdf-bookmark="Prerequisites" class="calibre3"><div class="preface" id="idm46122395089128">
<h2 class="calibre43">Prerequisites</h2>

<p class="author1">Obviously, PyPy was our favorite Python implementation. For the database, we used <a data-type="indexterm" data-primary="Cassandra" id="idm46122395087832" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="Elasticsearch" id="idm46122395086776" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Cassandra and Elasticsearch. Cache servers used <a data-type="indexterm" data-primary="Redis" id="idm46122395085976" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Redis. We used <a data-type="indexterm" data-primary="Celery" id="idm46122395085176" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Celery as a distributed task queue (workers), and for its broker, we used <a data-type="indexterm" data-primary="RabbitMQ" id="idm46122395084344" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>RabbitMQ. Results were kept in a Redis backend. Later on, Celery used Redis more exclusively for both brokers and backend. The OCR engine used is <a data-type="indexterm" data-primary="Tesseract" id="idm46122395083384" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Tesseract. The language translation engine and server used is <a data-type="indexterm" data-primary="Moses" id="idm46122395082584" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Moses. We used <a data-type="indexterm" data-primary="Scrapy" id="idm46122395081704" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Scrapy for crawling websites. For distributed locking in the whole system, we use a <a data-type="indexterm" data-primary="Zookeeper server" id="idm46122395080872" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>ZooKeeper server, but initially Redis was used for that. The web application is based on the excellent <a data-type="indexterm" data-primary="Flask" id="idm46122395079864" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Flask web framework and many of its extensions, such as Flask-Login, Flask-Principal, etc. The Flask application was hosted by <a data-type="indexterm" data-primary="Gunicorn" id="idm46122395078920" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="tornado" id="idm46122395078248" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="asynchronous programming" data-secondary="tornado" id="idm46122395077576" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Gunicorn and Tornado on every web server, and <a data-type="indexterm" data-primary="nginx" id="idm46122395076424" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>nginx was used as a reverse proxy server for the web servers. The rest of the code was written by us and is pure Python that runs on top of PyPy.</p>

<p class="author1">The whole project is hosted on an in-house <a data-type="indexterm" data-primary="OpenStack" id="idm46122395075240" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>OpenStack private cloud and executes between 100 and 1,000 instances of<a data-type="indexterm" data-primary="ArchLinux" id="idm46122395074408" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/> ArchLinux, depending on requirements, which can change dynamically on the fly. The whole system consumes up to 200 TB of storage every 6–12 months, depending on the mentioned requirements. All processing is done by our Python code, except OCR and translation.</p>
</div></section>













<section data-type="sect2" class="calibre3" data-pdf-bookmark="The Database"><div class="preface" id="idm46122395073000">
<h2 class="calibre43">The Database</h2>

<p class="author1">We developed a Python package that unifies model classes for Cassandra, Elasticsearch, and Redis. It is a simple object relational mapper (ORM) that maps everything to a dict or list of dicts, in the case where many records are retrieved from the database.</p>

<p class="author1">Since Cassandra 1.2 did not support complex queries on indices, we supported them with join-like queries. However, we allowed complex queries over small datasets (up to 4 GB) because much of that had to be processed while held in memory. PyPy ran in cases where CPython could not even load data into memory, thanks to its strategies applied to homogeneous lists to make them more compact in the memory. Another benefit of PyPy is that its JIT compilation kicked in loops where data manipulation or analysis happened. We wrote code in such a way that the types would stay static inside loops because that’s where JIT-compiled code is especially good.</p>

<p class="author1">Elasticsearch was used for indexing and fast searching of documents. It is very flexible when it comes to query complexity, so we did not have any major issues with it. One of the issues we had was related to updating documents; it is not designed for rapidly changing documents, so we had to migrate that part to Cassandra. Another limitation was related to facets and memory required on the database instance, but that was solved by having more smaller queries and then manually manipulating data in Celery workers. No major issues surfaced between PyPy and the <a data-type="indexterm" data-primary="PyES" id="idm46122395068888" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>PyES library used for interaction with Elasticsearch server pools.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="The Web Application" class="calibre3"><div class="preface" id="idm46122395067928">
<h2 class="calibre43">The Web Application</h2>

<p class="author1">As we’ve mentioned, we used the Flask framework with its third-party extensions. Initially, we started everything in <a data-type="indexterm" data-primary="Django" id="idm46122395066456" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Django, but we switched to Flask because of rapid changes in requirements. This does not mean that Flask is better than Django; it was just easier for us to follow code in Flask than in Django, since its project layout is very flexible. Gunicorn was used as a <a data-type="indexterm" data-primary="WSGI (Web Server Gateway Interface)" id="idm46122395065352" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="Web Server Gateway Interface (WSGI)" id="idm46122395064664" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Web Server Gateway Interface (WSGI) HTTP server, and its I/O loop was executed by Tornado. This allowed us to have up to one hundred concurrent connections per web server. This was lower than expected because many user queries can take a long time—a lot of analyzing happens in user requests, and data is returned in user interactions.</p>

<p class="author1">Initially, the web application depended on the <a data-type="indexterm" data-primary="Python Imaging Library (PIL)" id="idm46122395063048" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="PIL (Python Imaging Library)" id="idm46122395062328" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Python Imaging Library (PIL) for article and word highlighting. We had issues with the PIL library and <a data-type="indexterm" data-primary="PyPy" data-secondary="about" id="idm46122395061400" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>PyPy because at that time many memory leaks were associated with PIL. Then we switched to<a data-type="indexterm" data-primary="Pillow" id="idm46122395060216" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/> Pillow, which was more frequently maintained. In the end, we wrote a library that interacted with <a data-type="indexterm" data-primary="GraphicsMagick" id="idm46122395059304" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>GraphicsMagick via a subprocess module.</p>

<p class="author1">PyPy runs well, and the results are comparable with CPython. This is because usually web applications are I/O-bound. However, with the development of STM in PyPy, we hope to have scalable event handling on a multicore instance level soon.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="OCR and Translation" class="calibre3"><div class="preface" id="idm46122395057608">
<h2 class="calibre43">OCR and Translation</h2>

<p class="author1"><a data-type="indexterm" data-primary="OCR (optical character recognition)" id="idm46122395056408" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="optical character recognition (OCR)" id="idm46122395055640" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="translation" id="idm46122395054952" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>We wrote pure Python libraries for Tesseract and Moses because we had problems with CPython API-dependent extensions. PyPy has good support for the CPython API using <a data-type="indexterm" data-primary="CPyExt" id="idm46122395053976" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>CPyExt, but we wanted to be more in control of what happens under the hood. As a result, we made a PyPy-compatible solution with slightly faster code than on CPython. The reason it was not faster is that most of the processing happened in the C/C++ code of both <a data-type="indexterm" data-primary="Tesseract" id="idm46122395052904" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="Moses" id="idm46122395052232" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Tesseract and Moses. We could only speed up output processing and building Python structure of documents. There were no major issues at this stage with PyPy compatibility.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Task Distribution and Workers" class="calibre3"><div class="preface" id="idm46122395050984">
<h2 class="calibre43">Task Distribution and Workers</h2>

<p class="author1"><a data-type="indexterm" data-primary="task distribution" id="idm46122395049848" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="Celery" id="idm46122395049144" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Celery gave us the power to run many tasks in the background. Typical tasks are OCR, translation, analysis, etc. The whole thing could be done using <a data-type="indexterm" data-primary="Hadoop" id="idm46122395048184" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="MapReduce" id="idm46122395047512" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Hadoop for MapReduce, but we chose Celery because we knew that the project requirements might change often.</p>

<p class="author1">We had about 20 workers, and each worker had between 10 and 20 functions. Almost all functions had loops, or many nested loops. We cared that types stayed static, so the JIT compiler could do its job. The end results were a 2–5× speedup over CPython. The reason we did not get better speedups was because our loops were relatively small, between 20,000 and 100,000 iterations. In some cases where we had to do analysis on the word level, we had over 1 million iterations, and that’s where we got over a 10× speedup.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Conclusion" class="calibre3"><div class="preface" id="idm46122395045400">
<h2 class="calibre43">Conclusion</h2>

<p class="author1">PyPy is an excellent choice for every pure Python project that depends on speed of execution of readable and maintainable large source code. We found PyPy also to be very stable. All our programs were long-running with static and/or homogeneous types inside data structures, so JIT could do its job. When we tested the whole system on CPython, the results did not surprise us: we had roughly a 2× speedup with PyPy over CPython. In the eyes of our clients, this meant 2× better performance for the same price. In addition to all the good stuff that <a data-type="indexterm" data-primary="PyPy" data-secondary="about" id="idm46122395043288" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>PyPy has brought to us so far, we hope that its<a data-type="indexterm" data-primary="software transactional memory (STM)" id="idm46122395042120" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="STM (software transactional memory)" id="idm46122395041432" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/> software transactional memory (STM) implementation will bring to us scalable parallel execution for Python code.<a data-type="indexterm" data-primary="" data-startref="d_pypy" id="idm46122395040488" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="" data-startref="ps_pypy" id="idm46122395039544" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="" data-startref="pypy_dps" id="idm46122395038600" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="" data-startref="wps_pypy" id="idm46122395037656" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="" data-startref="tm_ab" id="idm46122395036712" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/></p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Task Queues at Lanyrd.com (2014)" class="calibre3"><div class="preface" id="lessons-from-field-andrew">
<h1 class="calibre25">Task Queues at Lanyrd.com (2014)</h1>

<p class="byline1">Andrew Godwin</p>

<p class="author1"><a data-type="indexterm" data-primary="Godwin, Andrew" id="ga_ab" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="Lanyrd.com" id="l_ab" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="task queues" id="tq_ab" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Lanyrd is a website for social discovery of conferences—our users sign in, and we use their friend graphs from social networks, as well as other indicators like their industry of work or their geographic location, to suggest relevant conferences.</p>

<p class="author1">The main work of the site is in distilling this raw data down into something we can show to the users—essentially, a ranked list of conferences. We have to do this offline, because we refresh the list of recommended conferences every couple of days and because we’re hitting external APIs that are often slow. We also use the Celery task queue for other things that take a long time, like fetching thumbnails for links people provide and sending email. There are usually well over 100,000 tasks in the queue each day, and sometimes many more.</p>








<section data-type="sect2" data-pdf-bookmark="Python’s Role at Lanyrd" class="calibre3"><div class="preface" id="idm46122395029160">
<h2 class="calibre43">Python’s Role at Lanyrd</h2>

<p class="author1"><a data-type="indexterm" data-primary="Python" data-secondary="at Lanyrd" id="idm46122395027816" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Lanyrd was built with Python and <a data-type="indexterm" data-primary="Django" id="idm46122395026664" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Django from day one, and virtually every part of it is written in Python—the website itself, the offline processing, our statistical and analysis tools, our mobile backend servers, and the deployment system. It’s a very versatile and mature language and one that’s incredibly easy to write things in quickly, mostly thanks to the large amount of libraries available and the language’s easily readable and concise syntax, which means it’s easy to update and refactor as well as easy to write initially.</p>

<p class="author1">The <a data-type="indexterm" data-primary="Celery" id="idm46122395024952" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/>Celery task queue was already a mature project when we evolved the need for a task queue (very early on), and the rest of Lanyrd was already in Python, so it was a natural fit. As we grew, there was a need to change the queue that backed it (which ended up being Redis), but it’s generally scaled very well.</p>

<p class="author1">As a start-up, we had to ship some known technical debt in order to make some headway—this is something you just have to do, and as long as you know what your issues are and when they might surface, it’s not necessarily a bad thing. Python’s flexibility in this regard is fantastic; it generally encourages loose coupling of components, which means it’s often easy to ship something with a “good enough” implementation and then easily refactor a better one in later.</p>

<p class="author1">Anything critical, such as payment code, had full unit test coverage, but for other parts of the site and task queue flow (especially display-related code) things were often moving too fast to make unit tests worthwhile (they would be too fragile). Instead, we adopted a very agile approach and had a two-minute deploy time and excellent error tracking; if a bug made it into live, we could often fix it and deploy within five minutes.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Making the Task Queue Performant" class="calibre3"><div class="preface" id="idm46122395021896">
<h2 class="calibre43">Making the Task Queue Performant</h2>

<p class="author1">The main issue with a task queue is throughput. If it gets backlogged, the website keeps working but starts getting mysteriously outdated—lists don’t update, page content is wrong, and emails don’t get sent for hours.</p>

<p class="author1">Fortunately, though, task queues also encourage a very scalable design; as long as your central messaging server (in our case, Redis) can handle the messaging overhead of the job requests and responses, for the actual processing you can spin up any number of worker daemons to handle the load.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Reporting, Monitoring, Debugging, and Deployment" class="calibre3"><div class="preface" id="idm46122395019384">
<h2 class="calibre43">Reporting, Monitoring, Debugging, and Deployment</h2>

<p class="author1">We had monitoring that kept track of our queue length, and if it started becoming long we would just deploy another server with more worker daemons. Celery makes this very easy to do. Our deployment system had hooks where we could increase the number of worker threads on a box (if our CPU utilization wasn’t optimal) and could easily turn a fresh server into a Celery worker within 30 minutes. It’s not like website response times going through the floor—if your task queues suddenly get a load spike, you have some time to implement a fix and usually it’ll smooth over itself, if you’ve left enough spare capacity.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Advice to a Fellow Developer" class="calibre3"><div class="preface" id="idm46122395017128">
<h2 class="calibre43">Advice to a Fellow Developer</h2>

<p class="author1">My main advice is to shove as much as you can into a task queue (or a similar loosely coupled architecture) as soon as possible. It takes some initial engineering effort, but as you grow, operations that used to take half a second can grow to half a minute, and you’ll be glad they’re not blocking your main rendering thread. Once you’ve got there, make sure you keep a close eye on your average queue latency (how long it takes a job to go from submission to completion), and make sure there’s some spare capacity for when your load increases.</p>

<p class="author1">Finally, be aware that having multiple task queues for different priorities of tasks makes sense. Sending email isn’t very high priority; people are used to emails taking minutes to arrive. However, if you’re rendering a thumbnail in the background and showing a spinner while you do it, you want that job to be high priority, as otherwise you’re making the user experience worse. You don’t want your 100,000-person mailshot to delay all thumbnailing on your site for the next 20 minutes!<a data-type="indexterm" data-primary="" data-startref="lsc_ch" id="idm46122395014280" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="" data-startref="ga_ab" id="idm46122395013304" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="" data-startref="l_ab" id="idm46122395012360" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/><a data-type="indexterm" data-primary="" data-startref="tq_ab" id="idm46122395011416" class="pcalibre2 pcalibre pcalibre3 pcalibre1 calibre6"/></p>
</div></section>





</div></section>







</div></section></div>



  </body></html>