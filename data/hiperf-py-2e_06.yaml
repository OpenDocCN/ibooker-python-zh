- en: Chapter 6\. Matrix and Vector Computation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章 矩阵与向量计算
- en: Regardless of what problem you are trying to solve on a computer, you will encounter
    vector computation at some point. Vector calculations are integral to how a computer
    works and how it tries to speed up runtimes of programs down at the silicon level—the
    only thing the computer knows how to do is operate on numbers, and knowing how
    to do several of those calculations at once will speed up your program.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您在计算机上尝试解决什么问题，都会在某个时候遇到向量计算。向量计算是计算机如何工作以及如何在硅级别上加快程序运行时速度的核心内容——计算机唯一能做的就是对数字进行操作，而同时进行多个这样的计算会加快程序的运行。
- en: In this chapter we try to unwrap some of the complexities of this problem by
    focusing on a relatively simple mathematical problem, solving the diffusion equation,
    and understanding what is happening at the CPU level. By understanding how different
    Python code affects the CPU and how to effectively probe these things, we can
    learn how to understand other problems as well.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们试图通过专注于一个相对简单的数学问题——解扩散方程来揭示这个问题的一些复杂性，并理解发生在CPU级别的情况。通过理解不同的Python代码如何影响CPU以及如何有效地探测这些内容，我们可以学会如何理解其他问题。
- en: We will start by introducing the problem and coming up with a quick solution
    using pure Python. After identifying some memory issues and trying to fix them
    using pure Python, we will introduce `numpy` and identify how and why it speeds
    up our code. Then we will start doing some algorithmic changes and specialize
    our code to solve the problem at hand. By removing some of the generality of the
    libraries we are using, we will yet again be able to gain more speed. Next, we
    introduce some extra modules that will help facilitate this sort of process out
    in the field, and also explore a cautionary tale about optimizing before profiling.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先介绍问题并使用纯Python提出一个快速解决方案。在识别出一些内存问题并尝试使用纯Python修复它们之后，我们将介绍`numpy`并识别它如何以及为什么加快我们的代码。然后，我们将开始进行一些算法变更，并专门优化我们的代码以解决手头的问题。通过去除我们正在使用的库的一些通用性，我们将再次能够获得更多的速度优势。接下来，我们引入一些额外的模块，将有助于在实地中促进这种过程，并探讨在优化之前进行性能分析的警示故事。
- en: Finally, we’ll take a look at the Pandas library, which builds upon `numpy`
    by taking columns of homogeneous data and storing them in a table of heterogeneous
    types. Pandas has grown beyond using pure `numpy` types and now can mix its own
    missing-data-aware types with `numpy` datatypes. While Pandas is incredibly popular
    with scientific developers and data scientists, there’s a lot of misinformation
    about ways to make it run quickly; we address some of these issues and give you
    tips for writing performant and supportable analysis code.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将介绍Pandas库，该库基于`numpy`，通过将同类数据的列存储在异构类型的表中来构建。Pandas已经超越了使用纯`numpy`类型，并且现在可以将其自己的缺失数据感知类型与`numpy`数据类型混合使用。虽然Pandas在科学开发人员和数据科学家中非常流行，但有关如何使其运行更快的误解很多；我们解决了其中一些问题，并为编写高性能和可支持的分析代码提供了建议。
- en: Introduction to the Problem
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题介绍
- en: To explore matrix and vector computation in this chapter, we will repeatedly
    use the example of diffusion in fluids. Diffusion is one of the mechanisms that
    moves fluids and tries to make them uniformly mixed.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 为了探索本章中的矩阵和向量计算，我们将反复使用液体扩散的示例。扩散是将流体移动并尝试使其均匀混合的机制之一。
- en: Note
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: This section is meant to give a deeper understanding of the equations we will
    be solving throughout the chapter. It is not strictly necessary that you understand
    this section in order to approach the rest of the chapter. If you wish to skip
    this section, make sure to at least look at the algorithm in Examples [6-1](ch06_split_000.xhtml#matrix_algo1)
    and [6-2](ch06_split_000.xhtml#matrix_algo2) to understand the code we will be
    optimizing.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本节旨在深入理解本章将要解决的方程式。您不一定需要严格理解本节即可继续学习本章的其余内容。如果您希望跳过本节，请至少查看示例[6-1](ch06_split_000.xhtml#matrix_algo1)和[6-2](ch06_split_000.xhtml#matrix_algo2)中的算法以了解我们将要优化的代码。
- en: On the other hand, if you read this section and want even more explanation,
    read Chapter 17 of [*Numerical Recipes*](https://oreil.ly/sSz8s), 3rd Edition,
    by William Press et al. (Cambridge University Press).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果您阅读了本节并希望获得更多解释，请阅读William Press等人编著的《*Numerical Recipes*》第3版的第17章（剑桥大学出版社）。
- en: In this section we will explore the mathematics behind the diffusion equation.
    This may seem complicated, but don’t worry! We will quickly simplify this to make
    it more understandable. Also, it is important to note that while having a basic
    understanding of the final equation we are solving will be useful while reading
    this chapter, it is not completely necessary; the subsequent chapters will focus
    mainly on various formulations of the code, not the equation. However, having
    an understanding of the equations will help you gain intuition about ways of optimizing
    your code. This is true in general—understanding the motivation behind your code
    and the intricacies of the algorithm will give you deeper insight about possible
    methods of optimization.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨扩散方程背后的数学原理。这可能看起来很复杂，但不要担心！我们将快速简化它，以使其更易理解。此外，需要注意的是，虽然在阅读本章时对我们要解决的最终方程有基本的理解会有所帮助，但这并非完全必要；后续章节主要将重点放在代码的各种形式化上，而不是方程。然而，理解方程将有助于您对优化代码的方法产生直观的认识。这在一般情况下是正确的——理解代码背后的动机和算法的复杂性将为您提供有关优化方法的更深入的洞察。
- en: 'One simple example of diffusion is dye in water: if you put several drops of
    dye into water at room temperature, the dye will slowly move out until it fully
    mixes with the water. Since we are not stirring the water, nor is it warm enough
    to create convection currents, diffusion will be the main process mixing the two
    liquids. When solving these equations numerically, we pick what we want the initial
    condition to look like and are able to evolve the initial condition forward in
    time to see what it will look like at a later time (see [Figure 6-2](ch06_split_000.xhtml#matrix_diffusion_image)).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散的一个简单例子是水中的染料：如果你把几滴染料放到室温水中，染料将慢慢扩散直到完全与水混合。由于我们没有搅拌水，也没有足够的温度来产生对流电流，扩散将是混合两种液体的主要过程。在数值上解这些方程时，我们选择初始条件的外观，并能够将初始条件向前演化到以后的时间以查看其外观（见[图6-2](ch06_split_000.xhtml#matrix_diffusion_image)）。
- en: 'All this being said, the most important thing to know about diffusion for our
    purposes is its formulation. Stated as a partial differential equation in one
    dimension (1D), the diffusion equation is written as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，对于我们的目的来说，关于扩散最重要的事情是它的公式化。作为一维（1D）偏微分方程陈述，扩散方程的形式如下：
- en: <math display="block"><mrow><mfrac><mi>∂</mi> <mrow><mi>∂</mi><mi>t</mi></mrow></mfrac>
    <mi>u</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>t</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mi>D</mi> <mo>·</mo> <mfrac><msup><mi>∂</mi> <mn>2</mn></msup> <mrow><mi>∂</mi><msup><mi>x</mi>
    <mn>2</mn></msup></mrow></mfrac> <mi>u</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo>
    <mi>t</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mfrac><mi>∂</mi> <mrow><mi>∂</mi><mi>t</mi></mrow></mfrac>
    <mi>u</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>t</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mi>D</mi> <mo>·</mo> <mfrac><msup><mi>∂</mi> <mn>2</mn></msup> <mrow><mi>∂</mi><msup><mi>x</mi>
    <mn>2</mn></msup></mrow></mfrac> <mi>u</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo>
    <mi>t</mi> <mo>)</mo></mrow></mrow></math>
- en: In this formulation, `u` is the vector representing the quantities we are diffusing.
    For example, we could have a vector with values of `0` where there is only water,
    and of `1` where there is only dye (and values in between where there is mixing).
    In general, this will be a 2D or 3D matrix representing an actual area or volume
    of fluid. In this way, we could have `u` be a 3D matrix representing the fluid
    in a glass, and instead of simply doing the second derivative along the `x` direction,
    we’d have to take it over all axes. In addition, `D` is a physical value that
    represents properties of the fluid we are simulating. A large value of `D` represents
    a fluid that can diffuse very easily. For simplicity, we will set `D = 1` for
    our code but still include it in the calculations.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式化中，`u`是表示我们正在扩散的量的向量。例如，我们可以有一个向量，其中在只有水的地方值为`0`，只有染料的地方值为`1`（以及在混合处的值）。一般来说，这将是一个表示实际区域或流体体积的二维或三维矩阵。这样，我们可以将`u`作为一个表示玻璃杯中流体的三维矩阵，并且不仅仅沿着`x`方向进行二阶导数计算，而是在所有轴上进行。此外，`D`是表示我们正在模拟的流体的属性的物理值。较大的`D`值表示一种可以很容易扩散的流体。为简单起见，我们将在我们的代码中将`D
    = 1`，但仍然将其包含在计算中。
- en: Note
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The diffusion equation is also called the *heat equation*. In this case, `u`
    represents the temperature of a region, and `D` describes how well the material
    conducts heat. Solving the equation tells us how the heat is being transferred.
    So instead of solving for how a couple of drops of dye diffuse through water,
    we might be solving for how the heat generated by a CPU diffuses into a heat sink.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散方程也称为*热传导方程*。在这种情况下，`u`代表了一个区域的温度，而`D`描述了材料导热的好坏程度。解方程告诉我们热量是如何传递的。因此，我们可能不是在解释几滴染料在水中扩散的过程，而是在解释CPU产生的热量如何传递到散热器中。
- en: What we will do is take the diffusion equation, which is continuous in space
    and time, and approximate it using discrete volumes and discrete times. We will
    do so using Euler’s method. *Euler’s method* simply takes the derivative and writes
    it as a difference, such that
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要做的是取扩散方程，该方程在空间和时间上是连续的，并使用离散的体积和离散的时间进行近似。我们将使用欧拉方法来实现这一点。*欧拉方法*简单地取导数，并将其写成一个差值，如下所示：
- en: <math display="block"><mrow><mfrac><mi>∂</mi> <mrow><mi>∂</mi><mi>t</mi></mrow></mfrac>
    <mi>u</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>t</mi> <mo>)</mo></mrow>
    <mo>≈</mo> <mfrac><mrow><mi>u</mi><mo>(</mo><mi>x</mi><mo>,</mo><mi>t</mi><mo>+</mo><mi>d</mi><mi>t</mi><mo>)</mo><mo>–</mo><mi>u</mi><mo>(</mo><mi>x</mi><mo>,</mo><mi>t</mi><mo>)</mo></mrow>
    <mrow><mi>d</mi><mi>t</mi></mrow></mfrac></mrow></math>
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mfrac><mi>∂</mi> <mrow><mi>∂</mi><mi>t</mi></mrow></mfrac>
    <mi>u</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>t</mi> <mo>)</mo></mrow>
    <mo>≈</mo> <mfrac><mrow><mi>u</mi><mo>(</mo><mi>x</mi><mo>,</mo><mi>t</mi><mo>+</mo><mi>d</mi><mi>t</mi><mo>)</mo><mo>–</mo><mi>u</mi><mo>(</mo><mi>x</mi><mo>,</mo><mi>t</mi><mo>)</mo></mrow>
    <mrow><mi>d</mi><mi>t</mi></mrow></mfrac></mrow></math>
- en: 'where `dt` is now a fixed number. This fixed number represents the time step,
    or the resolution in time for which we wish to solve this equation. It can be
    thought of as the frame rate of the movie we are trying to make. As the frame
    rate goes up (or `dt` goes down), we get a clearer picture of what happens. In
    fact, as `dt` approaches zero, Euler’s approximation becomes exact (note, however,
    that this exactness can be achieved only theoretically, since there is only finite
    precision on a computer and numerical errors will quickly dominate any results).
    We can thus rewrite this equation to figure out what `u(x, t + dt)` is, given
    `u(x,t)`. What this means for us is that we can start with some initial state
    (`u(x,0)`, representing the glass of water just as we add a drop of dye into it)
    and churn through the mechanisms we’ve outlined to “evolve” that initial state
    and see what it will look like at future times (`u(x,dt)`). This type of problem
    is called an *initial value problem* or *Cauchy problem*. Doing a similar trick
    for the derivative in `x` using the finite differences approximation, we arrive
    at the final equation:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 其中`dt`现在是一个固定的数字。这个固定的数字代表我们希望解决这个方程的时间步长，或者说时间上的分辨率。可以将其视为我们试图制作的电影的帧率。随着帧率的提高（或`dt`的减小），我们能够得到更清晰的图片。事实上，随着`dt`接近零，欧拉逼近变得精确（然而，请注意，这种精确性只能在理论上实现，因为计算机上只有有限的精度，数值误差将很快主导任何结果）。因此，我们可以重新编写这个方程，以便找出`u(x,
    t + dt)`是多少，给定`u(x,t)`。这对我们意味着我们可以从某个初始状态开始（`u(x,0)`，表示我们向其中加入一滴染料的水杯）并通过我们已经概述的机制来“演化”该初始状态，并查看在未来时刻（`u(x,dt)`）它将会是什么样子。这种问题类型称为*初值问题*或*柯西问题*。对`x`的导数使用有限差分逼近进行类似的技巧，我们得到最终方程：
- en: <math display="block"><mrow><mi>u</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo>
    <mi>t</mi> <mo>+</mo> <mi>d</mi> <mi>t</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>u</mi>
    <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>t</mi> <mo>)</mo></mrow> <mo>+</mo>
    <mi>d</mi> <mi>t</mi> <mo>*</mo> <mi>D</mi> <mo>*</mo> <mfrac><mrow><mi>u</mi><mo>(</mo><mi>x</mi><mo>+</mo><mi>d</mi><mi>x</mi><mo>,</mo><mi>t</mi><mo>)</mo><mo>+</mo><mi>u</mi><mo>(</mo><mi>x</mi><mo>–</mo><mi>d</mi><mi>x</mi><mo>,</mo><mi>t</mi><mo>)</mo><mo>–</mo><mn>2</mn><mo>·</mo><mi>u</mi><mo>(</mo><mi>x</mi><mo>,</mo><mi>t</mi><mo>)</mo></mrow>
    <mrow><mi>d</mi><msup><mi>x</mi> <mn>2</mn></msup></mrow></mfrac></mrow></math>
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>u</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo>
    <mi>t</mi> <mo>+</mo> <mi>d</mi> <mi>t</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>u</mi>
    <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>t</mi> <mo>)</mo></mrow> <mo>+</mo>
    <mi>d</mi> <mi>t</mi> <mo>*</mo> <mi>D</mi> <mo>*</mo> <mfrac><mrow><mi>u</mi><mo>(</mo><mi>x</mi><mo>+</mo><mi>d</mi><mi>x</mi><mo>,</mo><mi>t</mi><mo>)</mo><mo>+</mo><mi>u</mi><mo>(</mo><mi>x</mi><mo>–</mo><mi>d</mi><mi>x</mi><mo>,</mo><mi>t</mi><mo>)</mo><mo>–</mo><mn>2</mn><mo>·</mo><mi>u</mi><mo>(</mo><mi>x</mi><mo>,</mo><mi>t</mi><mo>)</mo></mrow>
    <mrow><mi>d</mi><msup><mi>x</mi> <mn>2</mn></msup></mrow></mfrac></mrow></math>
- en: Here, similar to how `dt` represents the frame rate, `dx` represents the resolution
    of the images—the smaller `dx` is, the smaller a region every cell in our matrix
    represents. For simplicity, we will set `D = 1` and `dx = 1`. These two values
    become very important when doing proper physical simulations; however, since we
    are solving the diffusion equation for illustrative purposes, they are not important
    to us.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，类似于`dt`表示帧率，`dx`表示图像的分辨率——`dx`越小，矩阵中每个单元格表示的区域就越小。为简单起见，我们将设置`D = 1`和`dx
    = 1`。这两个值在进行正确的物理模拟时变得非常重要；然而，由于我们解决扩散方程是为了说明的目的，它们对我们来说并不重要。
- en: Using this equation, we can solve almost any diffusion problem. However, there
    are some considerations regarding this equation. First, we said before that the
    spatial index in `u` (i.e., the `x` parameter) will be represented as the indices
    into a matrix. What happens when we try to find the value at `x – dx` when `x`
    is at the beginning of the matrix? This problem is called the *boundary condition*.
    You can have fixed boundary conditions that say “any value out of the bounds of
    my matrix will be set to *0*” (or any other value). Alternatively, you can have
    periodic boundary conditions that say that the values will wrap around. (That
    is, if one of the dimensions of your matrix has length `N`, the value in that
    dimension at index `-1` is the same as at `N – 1`, and the value at `N` is the
    same as at index `0`. In other words, if you are trying to access the value at
    index `i`, you will get the value at index `(i%N)`.)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个方程，我们可以解决几乎任何扩散问题。然而，对于这个方程有一些考虑。首先，我们之前说过，在`u`中的空间索引（即`x`参数）将被表示为矩阵的索引。当我们尝试找到`x
    - dx`处的值时，当`x`位于矩阵的开头时会发生什么？这个问题称为*边界条件*。您可以有固定的边界条件，即“超出我的矩阵范围的任何值都将被设置为*0*”（或任何其他值）。或者，您可以有周期性的边界条件，即值会循环。也就是说，如果矩阵的一个维度长度为`N`，那么该维度上索引为`-1`的值与`N
    - 1`处的值相同，索引为`N`处的值与索引为`0`处的值相同。（换句话说，如果您试图访问索引为`i`处的值，您将得到索引为`(i%N)`处的值。）
- en: 'Another consideration is how we are going to store the multiple time components
    of `u`. We could have one matrix for every time value we do our calculation for.
    At minimum, it seems that we will need two matrices: one for the current state
    of the fluid and one for the next state of the fluid. As we’ll see, there are
    very drastic performance considerations for this particular question.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个考虑因素是我们将如何存储`u`的多个时间分量。我们可以为每个计算时间点都有一个矩阵。至少，看起来我们需要两个矩阵：一个用于流体的当前状态，另一个用于流体的下一个状态。正如我们将看到的，对于这个特定问题，性能考虑非常重要。
- en: So what does it look like to solve this problem in practice? [Example 6-1](ch06_split_000.xhtml#matrix_algo1)
    contains some pseudocode to illustrate the way we can use our equation to solve
    the problem.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 那么在实践中解决这个问题是什么样子呢？[示例 6-1](ch06_split_000.xhtml#matrix_algo1)包含了一些伪代码，说明我们如何使用我们的方程来解决问题。
- en: Example 6-1\. Pseudocode for 1D diffusion
  id: totrans-25
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-1\. 一维扩散的伪代码
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This code will take an initial condition of the dye in water and tell us what
    the system looks like at every 0.0001-second interval in the future. The results
    can be seen in [Figure 6-1](ch06_split_000.xhtml#matrix_diffusion_1d_image), where
    we evolve our very concentrated drop of dye (represented by the top-hat function)
    into the future. We can see how, far into the future, the dye becomes well mixed,
    to the point where everywhere has a similar concentration of the dye.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码将以水中染料的初始条件作为输入，并告诉我们在未来每 0.0001 秒间隔下系统的样子。结果可以在[图 6-1](ch06_split_000.xhtml#matrix_diffusion_1d_image)中看到，我们将我们非常浓缩的染料滴（由顶帽函数表示）演化到未来。我们可以看到，远处的染料变得混合均匀，到达了染料的各处浓度相似的状态。
- en: '![Example of 1D diffusion](Images/hpp2_0601.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![一维扩散示例](Images/hpp2_0601.png)'
- en: Figure 6-1\. Example of 1D diffusion
  id: totrans-29
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-1\. 一维扩散示例
- en: 'For the purposes of this chapter, we will be solving the 2D version of the
    preceding equation. All this means is that instead of operating over a vector
    (or in other words, a matrix with one index), we will be operating over a 2D matrix.
    The only change to the equation (and thus to the subsequent code) is that we must
    now also take the second derivative in the `y` direction. This simply means that
    the original equation we were working with becomes the following:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章的目的，我们将解决前述方程的二维版本。这意味着我们将不再在一个向量上操作（或者换句话说，一个带有一个索引的矩阵），而是在一个二维矩阵上操作。方程（以及随后的代码）的唯一变化是我们现在还必须在`y`方向上进行第二次导数。这简单地意味着我们之前处理的原始方程变成了以下形式：
- en: <math display="block"><mrow><mfrac><mi>∂</mi> <mrow><mi>∂</mi><mi>t</mi></mrow></mfrac>
    <mi>u</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>,</mo> <mi>t</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mi>D</mi> <mo>·</mo> <mfenced separators="" open="("
    close=")"><mfrac><msup><mi>∂</mi> <mn>2</mn></msup> <mrow><mi>∂</mi><msup><mi>x</mi>
    <mn>2</mn></msup></mrow></mfrac> <mi>u</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo>
    <mi>y</mi> <mo>,</mo> <mi>t</mi> <mo>)</mo></mrow> <mo>+</mo> <mfrac><msup><mi>∂</mi>
    <mn>2</mn></msup> <mrow><mi>∂</mi><msup><mi>y</mi> <mn>2</mn></msup></mrow></mfrac>
    <mi>u</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>,</mo> <mi>t</mi>
    <mo>)</mo></mrow></mfenced></mrow></math>
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mfrac><mi>∂</mi> <mrow><mi>∂</mi><mi>t</mi></mrow></mfrac>
    <mi>u</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>,</mo> <mi>t</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mi>D</mi> <mo>·</mo> <mfenced separators="" open="("
    close=")"><mfrac><msup><mi>∂</mi> <mn>2</mn></msup> <mrow><mi>∂</mi><msup><mi>x</mi>
    <mn>2</mn></msup></mrow></mfrac> <mi>u</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo>
    <mi>y</mi> <mo>,</mo> <mi>t</mi> <mo>)</mo></mrow> <mo>+</mo> <mfrac><msup><mi>∂</mi>
    <mn>2</mn></msup> <mrow><mi>∂</mi><msup><mi>y</mi> <mn>2</mn></msup></mrow></mfrac>
    <mi>u</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>,</mo> <mi>t</mi>
    <mo>)</mo></mrow></mfenced></mrow></math>
- en: This numerical diffusion equation in 2D translates to the pseudocode in [Example 6-2](ch06_split_000.xhtml#matrix_algo2),
    using the same methods we used before.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这个二维数值扩散方程可以通过[示例 6-2](ch06_split_000.xhtml#matrix_algo2)中的伪代码转化为实际操作，使用我们之前使用的相同方法。
- en: Example 6-2\. Algorithm for calculating 2D diffusion
  id: totrans-33
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-2\. 计算二维扩散的算法
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We can now put all of this together and write the full Python 2D diffusion code
    that we will use as the basis for our benchmarks for the rest of this chapter.
    While the code looks more complicated, the results are similar to that of the
    1D diffusion (as can be seen in [Figure 6-2](ch06_split_000.xhtml#matrix_diffusion_image)).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将所有这些内容结合起来，编写完整的 Python 二维扩散代码，这将作为本章其余部分基准的基础。尽管代码看起来更复杂，但结果与一维扩散的结果相似（如[图 6-2](ch06_split_000.xhtml#matrix_diffusion_image)中所示）。
- en: If you’d like to do some additional reading on the topics in this section, check
    out the [Wikipedia page on the diffusion equation](http://bit.ly/diffusion_eq)
    and [Chapter 7](http://bit.ly/Gurevich) of *Numerical Methods for Complex Systems*
    by S. V. Gurevich.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想在本节中的相关主题上进行额外阅读，请查看[扩散方程的维基百科页面](http://bit.ly/diffusion_eq)和《复杂系统的数值方法》的第
    7 章[S. V. Gurevich](http://bit.ly/Gurevich)。
- en: '![Example of 2D diffusion for two sets of initial conditions](Images/hpp2_0602.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![两组初始条件下的二维扩散示例](Images/hpp2_0602.png)'
- en: Figure 6-2\. Example of 2D diffusion for two sets of initial conditions
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-2\. 两组初始条件下的二维扩散示例
- en: Aren’t Python Lists Good Enough?
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python 列表足够好吗？
- en: Let’s take our pseudocode from [Example 6-1](ch06_split_000.xhtml#matrix_algo1)
    and formalize it so we can better analyze its runtime performance. The first step
    is to write out the evolution function that takes in the matrix and returns its
    evolved state. This is shown in [Example 6-3](ch06_split_000.xhtml#matrix_pure_python).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从[示例 6-1](ch06_split_000.xhtml#matrix_algo1)中拿出我们的伪代码，并将其形式化，以便更好地分析其运行时性能。第一步是编写接受矩阵并返回其演化状态的演化函数。这在[示例 6-3](ch06_split_000.xhtml#matrix_pure_python)中展示出来。
- en: Example 6-3\. Pure Python 2D diffusion
  id: totrans-41
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-3\. 纯 Python 2D 扩散
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note
  id: totrans-43
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Instead of preallocating the `new_grid` list, we could have built it up in the
    `for` loop by using `append`s. While this would have been noticeably faster than
    what we have written, the conclusions we draw are still applicable. We chose this
    method because it is more illustrative.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 与预先分配 `new_grid` 列表不同，我们可以通过在 `for` 循环中使用 `append` 逐步构建它。虽然这比我们编写的方法明显更快，但我们得出的结论仍然适用。我们选择这种方法是因为它更具说明性。
- en: The global variable `grid_shape` designates how big a region we will simulate;
    and, as explained in [“Introduction to the Problem”](ch06_split_000.xhtml#matrix_intro),
    we are using periodic boundary conditions (which is why we use modulo for the
    indices). To actually use this code, we must initialize a grid and call `evolve`
    on it. The code in [Example 6-4](ch06_split_000.xhtml#matrix_pure_python_run)
    is a very generic initialization procedure that will be reused throughout the
    chapter (its performance characteristics will not be analyzed since it must run
    only once, as opposed to the `evolve` function, which is called repeatedly).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 全局变量 `grid_shape` 表示我们将模拟的区域大小；正如 [“问题介绍”](ch06_split_000.xhtml#matrix_intro)
    中所述，我们使用周期边界条件（这就是为什么在索引中使用模运算）。要使用此代码，我们必须初始化一个网格并对其调用 `evolve`。[示例 6-4](ch06_split_000.xhtml#matrix_pure_python_run)
    中的代码是一个非常通用的初始化过程，在本章中将被多次重复使用（由于它只需运行一次，所以不会分析其性能特征，而与需要重复调用的 `evolve` 函数相对）。
- en: Example 6-4\. Pure Python 2D diffusion initialization
  id: totrans-46
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-4\. 纯 Python 2D 扩散初始化
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[![1](Images/1.png)](ch06_split_000.xhtml#co_matrix_and_vector_computation_CO1-1)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](ch06_split_000.xhtml#co_matrix_and_vector_computation_CO1-1)'
- en: The initial conditions used here are the same as in the square example in [Figure 6-2](ch06_split_000.xhtml#matrix_diffusion_image).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 此处使用的初始条件与 [图 6-2](ch06_split_000.xhtml#matrix_diffusion_image) 中的正方形示例相同。
- en: The values for `dt` and grid elements have been chosen to be sufficiently small
    that the algorithm is stable. See [*Numerical Recipes*](https://oreil.ly/O8Seo)
    for a more in-depth treatment of this algorithm’s convergence characteristics.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`dt` 和网格元素的值选择足够小，使算法稳定。详见[*Numerical Recipes*](https://oreil.ly/O8Seo)以深入了解算法的收敛特性。'
- en: Problems with Allocating Too Much
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分配过多的问题
- en: By using `line_profiler` on the pure Python evolution function, we can start
    to unravel what is contributing to a possibly slow runtime. Looking at the profiler
    output in [Example 6-5](ch06_split_000.xhtml#matrix_pure_python_lineprof), we
    see that most of the time in the function is spent doing the derivative calculation
    and updating the grid.^([1](ch06_split_001.xhtml#idm46122422174440)) This is what
    we want, since this is a purely CPU-bound problem—any time not spent on solving
    the CPU-bound problem is an obvious place for optimization.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在纯 Python 进化函数上使用 `line_profiler`，我们可以开始分析可能导致运行时间较慢的原因。查看[示例 6-5](ch06_split_000.xhtml#matrix_pure_python_lineprof)中的分析输出，我们发现函数中大部分时间用于导数计算和网格更新。^([1](ch06_split_001.xhtml#idm46122422174440))
    这正是我们想要的，因为这是一个纯 CPU 限制的问题 —— 任何未用于解决此问题的时间都是显而易见的优化对象。
- en: Example 6-5\. Pure Python 2D diffusion profiling
  id: totrans-53
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-5\. 纯 Python 2D 扩散分析
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[![1](Images/1.png)](ch06_split_000.xhtml#co_matrix_and_vector_computation_CO2-1)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](ch06_split_000.xhtml#co_matrix_and_vector_computation_CO2-1)'
- en: This statement takes such a long time per hit because `grid_shape` must be retrieved
    from the local namespace (see [“Dictionaries and Namespaces”](ch04.xhtml#dict_namespace)
    for more information).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 每次击中此语句所需的时间很长，因为必须从本地命名空间检索 `grid_shape`（有关更多信息，请参见 [“字典和命名空间”](ch04.xhtml#dict_namespace)）。
- en: '[![2](Images/2.png)](ch06_split_000.xhtml#co_matrix_and_vector_computation_CO2-2)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](ch06_split_000.xhtml#co_matrix_and_vector_computation_CO2-2)'
- en: This line has 320,500 hits associated with it, because the grid we operated
    over had `xmax = 640` and the we ran the function 500 times. The calculation is
    `(640 + 1) * 500`, where the extra one evaluation is from the termination of the
    loop.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 此行与 320,500 次击中相关联，因为我们操作的网格具有 `xmax = 640`，并且我们运行该函数 500 次。计算公式为 `(640 + 1)
    * 500`，其中额外的一次评估来自循环的终止。
- en: '[![3](Images/3.png)](ch06_split_000.xhtml#co_matrix_and_vector_computation_CO2-3)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](ch06_split_000.xhtml#co_matrix_and_vector_computation_CO2-3)'
- en: This line has 500 hits associated with it, which informs us that the function
    was profiled over 500 runs.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 此行与 500 次击中相关联，这告诉我们该函数已在 500 次运行中进行了分析。
- en: It’s interesting to see the big difference in the `Per Hit` and `% Time` fields
    for line 15, where we allocate the new grid. This difference occurs because, while
    the line itself is quite slow (the `Per Hit` field shows that each run takes 0.0495
    seconds, much slower than all other lines), it isn’t called as often as other
    lines inside the loop. If we were to reduce the size of the grid and do more iterations
    (i.e., reduce the number of iterations of the loop but increase the number of
    times we call the function), we would see the `% Time` of this line increase and
    quickly dominate the runtime.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 看到第 15 行的 `Per Hit` 和 `% Time` 字段的巨大差异真是有趣，这是我们为新网格分配内存的地方。这种差异的原因在于，虽然这行代码本身运行相当慢（`Per
    Hit` 字段显示每次运行需要 0.0495 秒，比循环内的所有其他行都慢），但它并不像其他循环内的行那样频繁调用。如果我们减少网格的大小并增加迭代次数（即减少循环的迭代次数，但增加调用函数的次数），我们会看到这行代码的
    `% Time` 增加，并迅速主导运行时间。
- en: 'This is a waste, because the properties of `new_grid` do not change—no matter
    what values we send to `evolve`, the `new_grid` list will always be the same shape
    and size and contain the same values. A simple optimization would be to allocate
    this list once and simply reuse it. This way, we need to run this code only once,
    no matter the size of the grid or the number of iterations. This sort of optimization
    is similar to moving repetitive code outside a fast loop:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种浪费，因为 `new_grid` 的属性不会改变 —— 无论我们发送什么值到 `evolve`，`new_grid` 列表的形状和大小以及包含的值始终是相同的。一个简单的优化方法是只分配这个列表一次，然后简单地重用它。这样，我们只需要运行这段代码一次，不管网格的大小或迭代次数如何。这种优化类似于将重复的代码移出快速循环外：
- en: '[PRE5]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[![1](Images/1.png)](ch06_split_000.xhtml#co_matrix_and_vector_computation_CO3-1)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](ch06_split_000.xhtml#co_matrix_and_vector_computation_CO3-1)'
- en: The value of `sin(num_iterations)` doesn’t change throughout the loop, so there
    is no use recalculating it every time.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`sin(num_iterations)` 的值在整个循环过程中都没有改变，因此每次重新计算它都没有意义。'
- en: 'We can do a similar transformation to our diffusion code, as illustrated in
    [Example 6-6](ch06_split_000.xhtml#matrix_pure_python_memory). In this case, we
    would want to instantiate `new_grid` in [Example 6-4](ch06_split_000.xhtml#matrix_pure_python_run)
    and send it in to our `evolve` function. That function will do the same as it
    did before: read the `grid` list and write to the `new_grid` list. Then we can
    simply swap `new_grid` with `grid` and continue again.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对我们的扩散代码做类似的转换，如 [示例 6-6](ch06_split_000.xhtml#matrix_pure_python_memory)
    所示。在这种情况下，我们希望在 [示例 6-4](ch06_split_000.xhtml#matrix_pure_python_run) 中实例化 `new_grid`
    并将其发送到我们的 `evolve` 函数中。该函数将像以前一样读取 `grid` 列表并写入 `new_grid` 列表。然后我们可以简单地交换 `new_grid`
    和 `grid`，然后再继续。
- en: Example 6-6\. Pure Python 2D diffusion after reducing memory allocations
  id: totrans-67
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-6\. 减少内存分配后的纯 Python 2D 扩散
- en: '[PRE6]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can see from the line profile of the modified version of the code in [Example 6-7](ch06_split_000.xhtml#matrix_pure_python_mem_lineprof)
    that this small change has given us a 31.25% speedup.^([2](ch06_split_001.xhtml#idm46122422123864))
    This leads us to a conclusion similar to the one made during our discussion of
    `append` operations on lists (see [“Lists as Dynamic Arrays”](ch03.xhtml#list_as_dynamic_arrays)):
    memory allocations are not cheap. Every time we request memory to store a variable
    or a list, Python must take its time to talk to the operating system in order
    to allocate the new space, and then we must iterate over the newly allocated space
    to initialize it to some value.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从修改后的代码行剖析中看到，在 [示例 6-7](ch06_split_000.xhtml#matrix_pure_python_mem_lineprof)
    的版本中，这个小改动使我们的速度提高了 31.25%。这使我们得出一个类似于在我们讨论列表的 `append` 操作时得出的结论（参见 [“列表作为动态数组”](ch03.xhtml#list_as_dynamic_arrays)）：内存分配并不便宜。每次我们请求内存来存储变量或列表时，Python
    必须花费一些时间与操作系统通信，以便分配新的空间，然后我们必须迭代新分配的空间来初始化它的一些值。
- en: Whenever possible, reusing space that has already been allocated will give performance
    speedups. However, be careful when implementing these changes. While the speedups
    can be substantial, as always you should profile to make sure you are achieving
    the results you want and are not simply polluting your code base.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在可能的情况下，重用已分配的空间将提高性能。但在实现这些更改时要小心。虽然速度提升可能很大，但您应该始终进行剖析，以确保实现了您想要的结果，并且没有简单地污染了您的代码库。
- en: Example 6-7\. Line profiling Python diffusion after reducing allocations
  id: totrans-71
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-7\. 减少分配后 Python 扩散的行剖析
- en: '[PRE7]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Memory Fragmentation
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内存碎片化
- en: 'The Python code we wrote in [Example 6-6](ch06_split_000.xhtml#matrix_pure_python_memory)
    still has a problem that is at the heart of using Python for these sorts of vectorized
    operations: Python doesn’t natively support vectorization. There are two reasons
    for this: Python lists store pointers to the actual data, and Python bytecode
    is not optimized for vectorization, so `for` loops cannot predict when using vectorization
    would be beneficial.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[示例 6-6](ch06_split_000.xhtml#matrix_pure_python_memory)中编写的Python代码仍然存在一个问题，这是使用Python进行这些向量化操作的核心问题：Python不原生支持向量化。这有两个原因：Python列表存储指向实际数据的指针，并且Python字节码不针对向量化进行优化，因此`for`循环无法预测何时使用向量化会有益处。
- en: The fact that Python lists store *pointers* means that, instead of actually
    holding the data we care about, lists store locations where that data can be found.
    For most uses, this is good because it allows us to store whatever type of data
    we like inside a list. However, when it comes to vector and matrix operations,
    this is a source of a lot of performance degradation.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Python列表存储*指针*的事实意味着，与其实际持有我们关心的数据，列表存储了可以找到该数据的位置。对于大多数用途来说，这是好的，因为它允许我们在列表中存储任何类型的数据。然而，当涉及到向量和矩阵操作时，这是性能下降的一个源头。
- en: The degradation occurs because every time we want to fetch an element from the
    `grid` matrix, we must do multiple lookups. For example, doing `grid[5][2]` requires
    us to first do a list lookup for index `5` on the list `grid`. This will return
    a pointer to where the data at that location is stored. Then we need to do another
    list lookup on this returned object, for the element at index `2`. Once we have
    this reference, we have the location where the actual data is stored.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这种性能下降是因为每次我们想要从`grid`矩阵中获取一个元素时，我们必须进行多次查找。例如，执行`grid[5][2]`需要我们首先在列表`grid`上进行索引`5`的查找。这将返回存储在该位置的数据的指针。然后我们需要在这个返回的对象上再次进行列表查找，获取索引`2`处的元素。一旦我们有了这个引用，我们就知道了存储实际数据的位置。
- en: Tip
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Rather than creating a grid as a list-of-lists (`grid[x][y]`), how would you
    create a grid indexed by a tuple (`grid[(x, y)]`)? How would this affect the performance
    of the code?
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 与其创建一个列表的网格（`grid[x][y]`），你如何创建一个由元组索引的网格（`grid[(x, y)]`）？这会如何影响代码的性能？
- en: 'The overhead for one such lookup is not big and can be, in most cases, disregarded.
    However, if the data we wanted was located in one contiguous block in memory,
    we could move *all* of the data in one operation instead of needing two operations
    for each element. This is one of the major points with data fragmentation: when
    your data is fragmented, you must move each piece over individually instead of
    moving the entire block over. This means you are invoking more memory transfer
    overhead, and you are forcing the CPU to wait while data is being transferred.
    We will see with `perf` just how important this is when looking at the `cache-misses`.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种查找的开销并不大，大多数情况下可以忽略不计。然而，如果我们想要的数据位于内存中的一个连续块中，我们可以一次性移动*所有*数据，而不是每个元素需要两次操作。这是数据碎片化的一个主要问题之一：当数据碎片化时，你必须逐个移动每个片段，而不是移动整个块。这意味着你会引起更多的内存传输开销，并迫使CPU等待数据传输完成。我们将通过`perf`看到，这在查看`cache-misses`时是多么重要。
- en: This problem of getting the right data to the CPU when it is needed is related
    to the *von Neumann bottleneck*. This refers to the limited bandwidth that exists
    between the memory and the CPU as a result of the tiered memory architecture that
    modern computers use. If we could move data infinitely fast, we would not need
    any cache, since the CPU could retrieve any data it needed instantly. This would
    be a state in which the bottleneck is nonexistent.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 当CPU需要时将正确的数据传递给CPU这个问题与*冯·诺伊曼瓶颈*有关。这指的是现代计算机使用的分层存储器架构导致的内存和CPU之间存在的有限带宽。如果我们能够无限快地移动数据，我们就不需要任何缓存，因为CPU可以立即检索所需的任何数据。这将是一个不存在瓶颈的状态。
- en: Since we can’t move data infinitely fast, we must prefetch data from RAM and
    store it in smaller but faster CPU caches so that, hopefully, when the CPU needs
    a piece of data, it will be in a location that can be read from quickly. While
    this is a severely idealized way of looking at the architecture, we can still
    see some of the problems with it—how do we know what data will be needed in the
    future? The CPU does a good job with mechanisms called *branch prediction* and
    *pipelining*, which try to predict the next instruction and load the relevant
    portions of memory into the cache while still working on the current instruction.
    However, the best way to minimize the effects of the bottleneck is to be smart
    about how we allocate our memory and how we do our calculations over our data.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们无法无限快地移动数据，我们必须预取来自 RAM 的数据，并将其存储在更小但更快的 CPU 缓存中，这样，希望当 CPU 需要某个数据时，它会位于可以快速读取的位置。虽然这是一种极为理想化的架构视角，我们仍然可以看到其中的一些问题
    —— 我们如何知道未来会需要哪些数据？CPU 通过称为*分支预测*和*流水线*的机制来有效地处理这些问题，这些机制尝试预测下一条指令，并在当前指令的同时将相关的内存部分加载到缓存中。然而，减少瓶颈影响的最佳方法是聪明地分配内存和计算数据。
- en: Probing how well memory is being moved to the CPU can be quite hard; however,
    in Linux the `perf` tool can be used to get amazing amounts of insight into how
    the CPU is dealing with the program being run.^([3](ch06_split_001.xhtml#idm46122420832696))
    For example, we can run `perf` on the pure Python code from [Example 6-6](ch06_split_000.xhtml#matrix_pure_python_memory)
    and see just how efficiently the CPU is running our code.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 探测内存移动到 CPU 的效果可能非常困难；然而，在 Linux 中，`perf` 工具可以用来深入了解 CPU 处理正在运行的程序的方式。^([3](ch06_split_001.xhtml#idm46122420832696))
    例如，我们可以在纯 Python 代码的[例子 6-6](ch06_split_000.xhtml#matrix_pure_python_memory)上运行
    `perf`，看看 CPU 如何高效地运行我们的代码。
- en: The results are shown in [Example 6-8](ch06_split_000.xhtml#matrix_perf_python_memory).
    Note that the output in this example and the following `perf` examples has been
    truncated to fit within the margins of the page. The removed data included variances
    for each measurement, indicating how much the values changed throughout multiple
    benchmarks. This is useful for seeing how much a measured value is dependent on
    the actual performance characteristics of the program versus other system properties,
    such as other running programs using system resources.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示在[例子 6-8](ch06_split_000.xhtml#matrix_perf_python_memory)中。请注意，此示例和后续 `perf`
    示例中的输出已被截断以适应页面的边界。移除的数据包括每个测量的差异，显示了值在多次基准测试中变化的程度。这对于查看测量值在程序的实际性能特性与其他系统属性（例如，使用系统资源的其他运行中程序）之间的依赖性有用。
- en: 'Example 6-8\. Performance counters for pure Python 2D diffusion with reduced
    memory allocations (grid size: 640 × 640, 500 iterations)'
  id: totrans-84
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 例子 6-8\. 性能计数器用于纯 Python 2D 扩散，减少内存分配（网格大小：640 × 640，500 次迭代）
- en: '[PRE8]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Understanding perf
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解性能
- en: Let’s take a second to understand the various performance metrics that `perf`
    is giving us and their connection to our code. The `task-clock` metric tells us
    how many clock cycles our task took. This is different from the total runtime,
    because if our program took one second to run but used two CPUs, then the task-clock
    would be `2000` (`task-clock` is generally in milliseconds). Conveniently, `perf`
    does the calculation for us and tells us, next to this metric, how many CPUs were
    utilized (where it says “XXXX CPUs utilized”). This number wouldn’t be exactly
    `2` even when two CPUs are being used, though, because the process sometimes relied
    on other subsystems to do instructions for it (for example, when memory was allocated).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花一点时间理解 `perf` 给我们的各种性能指标及其与我们的代码的关系。`task-clock` 指标告诉我们任务花费了多少时钟周期。这与总运行时间不同，因为如果我们的程序运行了一秒钟，但使用了两个
    CPU，那么 `task-clock` 将是 `2000`（`task-clock` 通常以毫秒为单位）。方便的是，`perf` 为我们进行了计算，并告诉我们在这个指标旁边，使用了多少个
    CPU（它说“XXXX CPUs utilized”）。即使在使用两个 CPU 时，这个数字也不会完全是 `2`，因为进程有时依赖其他子系统来执行指令（例如，当分配内存时）。
- en: On the other hand, `instructions` tells us how many CPU instructions our code
    issued, and `cycles` tells us how many CPU cycles it took to run all of these
    instructions. The difference between these two numbers gives us an indication
    of how well our code is vectorizing and pipelining. With pipelining, the CPU is
    able to run the current operation while fetching and preparing the next one.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，`instructions` 告诉我们代码发出了多少 CPU 指令，而 `cycles` 则告诉我们运行所有这些指令所需的 CPU 周期数。这两个数字的差异显示了我们的代码向量化和流水线化的效果如何。通过流水线化，CPU
    能够在执行当前操作的同时获取和准备下一个操作。
- en: '`cs` (representing “context switches”) and `CPU-migrations` tell us about how
    the program is halted in order to wait for a kernel operation to finish (such
    as I/O), to let other applications run, or to move execution to another CPU core.
    When a `context-switch` happens, the program’s execution is halted and another
    program is allowed to run instead. This is a *very* time-intensive task and is
    something we would like to minimize as much as possible, but we don’t have too
    much control over when this happens. The kernel delegates when programs are allowed
    to be switched out; however, we can do things to disincentivize the kernel from
    moving *our* program. In general, the kernel suspends a program when it is doing
    I/O (such as reading from memory, disk, or the network). As you’ll see in later
    chapters, we can use asynchronous routines to make sure that our program still
    uses the CPU even when waiting for I/O, which will let us keep running without
    being context-switched. In addition, we could set the `nice` value of our program
    to give our program priority and stop the kernel from context-switching it.^([4](ch06_split_001.xhtml#idm46122421125416))
    Similarly, `CPU-migrations` happen when the program is halted and resumed on a
    different CPU than the one it was on before, in order to have all CPUs have the
    same level of utilization. This can be seen as an especially bad context switch,
    as not only is our program being temporarily halted, but we also lose whatever
    data we had in the L1 cache (recall that each CPU has its own L1 cache).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`cs`（代表“上下文切换”）和 `CPU-migrations` 告诉我们程序如何在等待内核操作完成（例如 I/O）、让其他应用程序运行或将执行移动到另一个
    CPU 核心时被暂停。当发生 `context-switch` 时，程序的执行被暂停，另一个程序被允许运行。这是一个*非常*耗时的任务，我们希望尽可能减少这种情况的发生，但是我们不能完全控制它何时发生。内核决定何时允许程序被切换出去；然而，我们可以做些事情来防止内核切换*我们*的程序。一般来说，内核在程序进行
    I/O 操作（如从内存、磁盘或网络读取数据）时暂停程序。正如你将在后面的章节中看到的那样，我们可以使用异步程序来确保我们的程序即使在等待 I/O 时也能继续使用
    CPU，这样我们可以在不被上下文切换的情况下继续运行。此外，我们可以设置程序的 `nice` 值，以提高程序的优先级并阻止内核对其进行上下文切换。^([4](ch06_split_001.xhtml#idm46122421125416))
    类似地，`CPU-migrations` 发生在程序被暂停并在不同的 CPU 上恢复执行时，目的是让所有 CPU 具有相同的利用率水平。这可以看作是一个特别糟糕的上下文切换，因为不仅我们的程序暂时停止了，而且我们还丢失了在
    L1 缓存中的任何数据（回想每个 CPU 都有自己的 L1 缓存）。'
- en: 'A `page-fault` (or just `fault`) is part of the modern Unix memory allocation
    scheme. When memory is allocated, the kernel doesn’t do much except give the program
    a reference to memory. Later, however, when the memory is first used, the operating
    system throws a minor page fault interrupt, which pauses the program that is being
    run and properly allocates the memory. This is called a *lazy allocation system*.
    While this method is an optimization over previous memory allocation systems,
    minor page faults are quite an expensive operation since most of the operations
    are done outside the scope of the program you are running. There is also a major
    page fault, which happens when the program requests data from a device (disk,
    network, etc.) that hasn’t been read yet. These are even more expensive operations:
    not only do they interrupt your program, but they also involve reading from whichever
    device the data lives on. This sort of page fault does not generally affect CPU-bound
    work; however, it will be a source of pain for any program that does disk or network
    reads/writes.^([5](ch06_split_001.xhtml#idm46122421117832))'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`page-fault`（或简称`fault`）是现代Unix内存分配方案的一部分。当分配内存时，内核并不做太多工作，只是给程序一个内存的引用。然而，当内存首次被使用时，操作系统会抛出一个轻微的页面错误中断，暂停正在运行的程序，并适当地分配内存。这被称为*惰性分配系统*。虽然这种方法优于先前的内存分配系统，但是轻微的页面错误是非常昂贵的操作，因为大部分操作都是在你运行的程序的范围之外完成的。还有一个主要页面错误，当程序请求尚未读取的设备数据（磁盘、网络等）时发生。这些操作更加昂贵：它们不仅会中断您的程序，还会涉及从存放数据的任何设备中读取。这种页面错误通常不会影响CPU密集型工作；然而，它将成为任何进行磁盘或网络读/写的程序的痛点。^([5](ch06_split_001.xhtml#idm46122421117832))'
- en: Once we have data in memory and we reference it, the data makes its way through
    the various tiers of memory (L1/L2/L3 memory—see [“Communications Layers”](ch01_split_000.xhtml#understanding_pp_communication)
    for a discussion of this). Whenever we reference data that is in our cache, the
    `cache-references` metric increases. If we do not already have this data in the
    cache and need to fetch it from RAM, this counts as a `cache-miss`. We won’t get
    a cache miss if we are reading data we have read recently (that data will still
    be in the cache) or data that is located *near* data we have recently read (data
    is sent from RAM into the cache in chunks). Cache misses can be a source of slowdowns
    when it comes to CPU-bound work, since we need to wait to fetch the data from
    RAM *and* we interrupt the flow of our execution pipeline (more on this in a second).
    As a result, reading through an array in order will give many `cache-references`
    but not many `cache-misses` since if we read element `i`, element `i + 1` will
    already be in cache. If, however, we read randomly through an array or otherwise
    don’t lay out our data in memory well, every read will require access to data
    that couldn’t possibly already be in cache. Later in this chapter we will discuss
    how to reduce this effect by optimizing the layout of data in memory.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们将数据加载到内存并引用它，数据将通过各个存储层（L1/L2/L3存储器—参见[“通信层”](ch01_split_000.xhtml#understanding_pp_communication)进行讨论）传输。每当我们引用已存在于缓存中的数据时，`cache-references`指标就会增加。如果我们在缓存中没有这些数据并且需要从RAM中获取，这被称为`cache-miss`。如果我们读取的是最近读取过的数据（该数据仍然存在于缓存中）或者是靠近最近读取过的数据的数据（数据会以块的形式从RAM中发送到缓存），我们就不会遇到缓存失效。缓存失效可能是CPU密集型工作中减慢速度的原因之一，因为我们需要等待从RAM获取数据，并且会中断执行流水线的流动（稍后详细讨论）。因此，顺序遍历数组会产生许多`cache-references`但不会有太多的`cache-misses`，因为如果我们读取第`i`个元素，第`i
    + 1`个元素已经在缓存中。然而，如果我们随机读取数组或者内存中的数据布局不佳，每次读取都需要访问不可能已经存在于缓存中的数据。本章后面将讨论如何通过优化内存中数据的布局来减少这种影响。
- en: A `branch` is a time in the code where the execution flow changes. Think of
    an `if...then` statement—depending on the result of the conditional, we will be
    executing either one section of code or another. This is essentially a branch
    in the execution of the code—the next instruction in the program could be one
    of two things. To optimize this, especially with regard to the pipeline, the CPU
    tries to guess which direction the branch will take and preload the relevant instructions.
    When this prediction is incorrect, we will get a `branch-miss`. Branch misses
    can be quite confusing and can result in many strange effects (for example, some
    loops will run substantially faster on sorted lists than on unsorted lists simply
    because there will be fewer branch misses).^([6](ch06_split_001.xhtml#idm46122421106008))
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`branch`是代码中执行流程发生变化的时间。想象一个`if...then`语句-根据条件的结果，我们将执行代码的一个部分或另一个部分。这本质上是代码执行中的一个分支-程序中的下一条指令可能是两种情况之一。为了优化这一点，特别是关于流水线，CPU尝试猜测分支将采取的方向并预加载相关指令。当这种预测不正确时，我们将得到一个`branch-miss`。分支未命中可能会非常令人困惑，并且可能会导致许多奇怪的效果（例如，一些循环在排序列表上运行的速度会比在未排序列表上快得多，仅仅是因为分支未命中较少）。'
- en: There are a lot more metrics that `perf` can keep track of, many of which are
    very specific to the CPU you are running the code on. You can run `perf list`
    to get the list of currently supported metrics on your system. For example, in
    the previous edition of this book, we ran on a machine that also supported `stalled-cycles-frontend`
    and `stalled-cycles-backend`, which tell us how many cycles our program was waiting
    for the frontend or backend of the pipeline to be filled. This can happen because
    of a cache miss, a mispredicted branch, or a resource conflict. The frontend of
    the pipeline is responsible for fetching the next instruction from memory and
    decoding it into a valid operation, while the backend is responsible for actually
    running the operation. These sorts of metrics can help tune the performance of
    a code to the optimizations and architecture choices of a particular CPU; however,
    unless you are always running on the same chip-set, it may be excessive to worry
    too much about them.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`perf`可以跟踪的指标还有很多，其中许多非常特定于你运行代码的CPU。您可以运行`perf list`以获取系统当前支持的指标列表。例如，在本书的上一版中，我们在一台机器上运行，该机器还支持`stalled-cycles-frontend`和`stalled-cycles-backend`，它们告诉我们我们的程序等待前端或后端管道填充的周期数。这可能是由于缓存未命中、错误的分支预测或资源冲突而发生的。管道的前端负责从内存中获取下一条指令并将其解码为有效操作，而后端负责实际运行操作。这些指标可以帮助调整代码的性能，以适应特定CPU的优化和架构选择；但是，除非您总是在相同的芯片组上运行，否则过度担心它们可能过多。'
- en: Tip
  id: totrans-94
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: If you would like a more thorough explanation of what is going on at the CPU
    level with the various performance metrics, check out Gurpur M. Prabhu’s fantastic
    [“Computer Architecture Tutorial.”](http://bit.ly/ca_tutorial) It deals with the
    problems at a very low level, which will give you a good understanding of what
    is going on under the hood when you run your code.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想要更深入地了解各种性能指标在CPU级别上的情况，请查看Gurpur M. Prabhu的出色的[“计算机体系结构教程。”](http://bit.ly/ca_tutorial)
    它处理非常低级别的问题，这将让您对在运行代码时底层发生的情况有很好的理解。
- en: Making Decisions with perf’s Output
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用perf的输出进行决策
- en: With all this in mind, the performance metrics in [Example 6-8](ch06_split_000.xhtml#matrix_perf_python_memory)
    are telling us that while running our code, the CPU had to reference the L1/L2
    cache 656,345,027 times. Of those references, 349,562,390 (or 53.3%) were requests
    for data that wasn’t in memory at the time and had to be retrieved. In addition,
    we can see that in each CPU cycle we are able to perform an average of 2.91 instructions,
    which tells us the total speed boost from pipelining, out-of-order execution,
    and hyperthreading (or any other CPU feature that lets you run more than one instruction
    per clock cycle).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到所有这些因素，在[示例 6-8](ch06_split_000.xhtml#matrix_perf_python_memory)中的性能指标告诉我们，当运行我们的代码时，CPU必须引用L1/L2缓存656,345,027次。在这些引用中，349,562,390（或53.3％）是在那时不在内存中的数据的请求，必须检索。此外，我们还可以看到在每个CPU周期中，我们平均能执行2.91条指令，这告诉我们通过流水线化、乱序执行和超线程（或任何其他允许您在每个时钟周期运行多个指令的CPU功能）获得的总速度提升。
- en: Fragmentation increases the number of memory transfers to the CPU. Additionally,
    since you don’t have multiple pieces of data ready in the CPU cache when a calculation
    is requested, you cannot vectorize the calculations. As explained in [“Communications
    Layers”](ch01_split_000.xhtml#understanding_pp_communication), vectorization of
    computations (or having the CPU do multiple computations at a time) can occur
    only if we can fill the CPU cache with all the relevant data. Since the bus can
    only move contiguous chunks of memory, this is possible only if the grid data
    is stored sequentially in RAM. Since a list stores pointers to data instead of
    the actual data, the actual values in the grid are scattered throughout memory
    and cannot be copied all at once.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 碎片化会增加到CPU的内存传输次数。另外，由于在请求计算时CPU缓存中没有准备好多个数据片段，因此无法对计算进行向量化。如[“通信层”](ch01_split_000.xhtml#understanding_pp_communication)中所解释的，计算向量化（或让CPU同时进行多个计算）只能在CPU缓存中填充所有相关数据时才能发生。由于总线只能移动连续的内存块，这只有在网格数据在RAM中顺序存储时才可能。由于列表存储数据的指针而不是实际数据，网格中的实际值分散在内存中，无法一次性复制。
- en: We can alleviate this problem by using the `array` module instead of lists.
    These objects store data sequentially in memory, so that a slice of the `array`
    actually represents a continuous range in memory. However, this doesn’t completely
    fix the problem—now we have data that is stored sequentially in memory, but Python
    still does not know how to vectorize our loops. What we would like is for any
    loop that does arithmetic on our array one element at a time to work on chunks
    of data, but as mentioned previously, there is no such bytecode optimization in
    Python (partly because of the extremely dynamic nature of the language).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用`array`模块而不是列表来缓解这个问题。这些对象在内存中顺序存储数据，因此`array`的切片实际上表示内存中的连续范围。然而，这并不能完全解决问题——现在我们的数据在内存中顺序存储了，但Python仍然不知道如何对我们的循环进行向量化。我们希望的是，任何对我们数组逐个元素进行算术运算的循环都能处理数据块，但正如前面提到的，Python中没有这样的字节码优化（部分原因是语言极其动态的特性）。
- en: Note
  id: totrans-100
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Why doesn’t having the data we want stored sequentially in memory automatically
    give us vectorization? If we look at the raw machine code that the CPU is running,
    vectorized operations (such as multiplying two arrays) use a different part of
    the CPU and different instructions than nonvectorized operations. For Python to
    use these special instructions, we must have a module that was created to use
    them. We will soon see how `numpy` gives us access to these specialized instructions.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们希望存储在内存中的数据自动给我们向量化？如果我们看一下CPU正在运行的原始机器代码，向量化操作（如两个数组相乘）使用CPU的不同部分和不同指令。要使Python使用这些特殊指令，我们必须使用一个专门用于使用它们的模块。我们很快将看到`numpy`如何让我们访问这些专门的指令。
- en: Furthermore, because of implementation details, using the `array` type when
    creating lists of data that must be iterated on is actually *slower* than simply
    creating a `list`. This is because the `array` object stores a very low-level
    representation of the numbers it stores, and this must be converted into a Python-compatible
    version before being returned to the user. This extra overhead happens every time
    you index an `array` type. That implementation decision has made the `array` object
    less suitable for math and more suitable for storing fixed-type data more efficiently
    in memory.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于实现细节的原因，在创建必须迭代的数据列表时，使用`array`类型实际上比简单创建`list`更*慢*。这是因为`array`对象存储其所存储数字的非常低级别的表示形式，这在返回给用户之前必须转换为Python兼容版本。每次索引`array`类型都会产生额外的开销。这个实现决策使得`array`对象在数学上不太适用，而更适合在内存中更有效地存储固定类型数据。
- en: Enter numpy
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进入numpy
- en: 'To deal with the fragmentation we found using `perf`, we must find a package
    that can efficiently vectorize operations. Luckily, `numpy` has all of the features
    we need—it stores data in contiguous chunks of memory and supports vectorized
    operations on its data. As a result, any arithmetic we do on `numpy` arrays happens
    in chunks without us having to explicitly loop over each element.^([7](ch06_split_001.xhtml#idm46122421072568))
    Not only is it much easier to do matrix arithmetic this way, but it is also faster.
    Let’s look at an example:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决我们使用`perf`找到的碎片化问题，我们必须找到一个能够高效矢量化操作的软件包。幸运的是，`numpy`具有我们需要的所有功能——它将数据存储在内存的连续块中，并支持对其数据进行矢量化操作。因此，我们在`numpy`数组上进行的任何算术运算都是按块进行的，而无需我们显式地循环遍历每个元素。[^7]
- en: '[PRE9]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[![1](Images/1.png)](ch06_split_000.xhtml#co_matrix_and_vector_computation_CO4-1)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](ch06_split_000.xhtml#co_matrix_and_vector_computation_CO4-1)'
- en: This creates two implied loops over `vector`, one to do the multiplication and
    one to do the sum. These loops are similar to the loops from `norm_square_list_comprehension`,
    but they are executed using `numpy`’s optimized numerical code.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这创建了对`vector`的两个隐含循环，一个用于执行乘法，另一个用于执行求和。这些循环类似于`norm_square_list_comprehension`中的循环，但是它们使用了`numpy`优化的数值代码来执行。
- en: '[![2](Images/2.png)](ch06_split_000.xhtml#co_matrix_and_vector_computation_CO4-2)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](ch06_split_000.xhtml#co_matrix_and_vector_computation_CO4-2)'
- en: This is the preferred way of doing vector norms in `numpy` by using the vectorized
    `numpy.dot` operation. The less efficient `norm_square_numpy` code is provided
    for illustration.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用矢量化的`numpy.dot`操作来执行向量范数的首选方法。提供了效率较低的`norm_square_numpy`代码供参考。
- en: 'The simpler `numpy` code runs 89× faster than `norm_square_list` and 83.65×
    faster than the “optimized” Python list comprehension. The difference in speed
    between the pure Python looping method and the list comprehension method shows
    the benefit of doing more calculation behind the scenes rather than explicitly
    in your Python code. By performing calculations using Python’s already-built machinery,
    we get the speed of the native C code that Python is built on. This is also partly
    the same reasoning behind why we have such a drastic speedup in the `numpy` code:
    instead of using the generalized list structure, we have a finely tuned and specially
    built object for dealing with arrays of numbers.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 更简单的`numpy`代码比`norm_square_list`快89倍，比“优化”的Python列表推导式快83.65倍。纯Python循环方法和列表推导式方法之间的速度差异显示了在Python代码中隐含更多计算胜过显式执行计算的好处。通过使用Python已经构建好的机制进行计算，我们获得了Python基于的本机C代码的速度。这也部分解释了为什么我们在`numpy`代码中有如此
    drasti 的速度提升：我们没有使用通用列表结构，而是使用了一个精心调整和专门构建的用于处理数字数组的对象。
- en: In addition to more lightweight and specialized machinery, the `numpy` object
    gives us memory locality and vectorized operations, which are incredibly important
    when dealing with numerical computations. The CPU is exceedingly fast, and most
    of the time simply getting it the data it needs faster is the best way to optimize
    code quickly. Running each function using the `perf` tool we looked at earlier
    shows that the `array` and pure Python functions take about 10^(12) instructions,
    while the `numpy` version takes about 10⁹ instructions. In addition, the `array`
    and pure Python versions have around 53% cache misses, while `numpy` has around
    20%.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 除了更轻量级和专业化的机器之外，`numpy`对象还为我们提供了内存局部性和矢量化操作，在处理数值计算时非常重要。CPU的速度非常快，而且通常只需更快地将数据提供给它即可快速优化代码。使用我们之前查看过的`perf`工具运行每个函数表明，`array`和纯Python函数需要约10^(12)条指令，而`numpy`版本需要约10⁹条指令。此外，`array`和纯Python版本的缓存缺失率约为53%，而`numpy`的缺失率约为20%。
- en: 'In our `norm_square_numpy` code, when doing `vector * vector`, there is an
    *implied* loop that `numpy` will take care of. The implied loop is the same loop
    we have explicitly written out in the other examples: loop over all items in `vector`,
    multiplying each item by itself. However, since we tell `numpy` to do this instead
    of explicitly writing it out in Python code, `numpy` can take advantage of all
    the optimizations it wants. In the background, `numpy` has very optimized C code
    that has been made specifically to take advantage of any vectorization the CPU
    has enabled. In addition, `numpy` arrays are represented sequentially in memory
    as low-level numerical types, which gives them the same space requirements as
    `array` objects (from the `array` module).'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的`norm_square_numpy`代码中，当执行`vector * vector`时，`numpy`会隐含地处理一个循环。这个隐含的循环与我们在其他示例中显式编写的循环相同：遍历`vector`中的所有项，将每个项乘以自身。但是，由于我们告诉`numpy`来做这件事，而不是在Python代码中显式写出来，`numpy`可以利用CPU启用的所有向量化优化。此外，`numpy`数组以低级数值类型在内存中顺序表示，这使它们具有与`array`模块中的`array`对象相同的空间要求。
- en: As an added bonus, we can reformulate the problem as a dot product, which `numpy`
    supports. This gives us a single operation to calculate the value we want, as
    opposed to first taking the product of the two vectors and then summing them.
    As you can see in [Figure 6-3](ch06_split_000.xhtml#matrix_norm_squared_figure),
    this operation, `norm_numpy_dot`, outperforms all the others by quite a substantial
    margin—this is thanks to the specialization of the function, and because we don’t
    need to store the intermediate value of `vector * vector` as we did in `norm_numpy`.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 作为额外的奖励，我们可以将问题重新表述为一个点积，`numpy`支持这种操作。这给了我们一个单一的操作来计算我们想要的值，而不是首先计算两个向量的乘积，然后对它们求和。正如您在[图 6-3](ch06_split_000.xhtml#matrix_norm_squared_figure)中看到的那样，这个操作`norm_numpy_dot`在性能上远远优于其他所有操作——这要归功于函数的专门化，以及因为我们不需要像在`norm_numpy`中那样存储`vector
    * vector`的中间值。
- en: '![Runtimes for the various norm-square routines with vectors of different lenghts](Images/hpp2_0603.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![不同长度向量的各种范数平方例程的运行时间](Images/hpp2_0603.png)'
- en: Figure 6-3\. Runtimes for the various norm squared routines with vectors of
    different length
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-3\. 不同长度向量的各种范数平方例程的运行时间
- en: Applying numpy to the Diffusion Problem
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将numpy应用于扩散问题
- en: 'Using what we’ve learned about `numpy`, we can easily adapt our pure Python
    code to be vectorized. The only new functionality we must introduce is `numpy`’s
    `roll` function. This function does the same thing as our modulo-index trick,
    but it does so for an entire `numpy` array. In essence, it vectorizes this reindexing:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们对`numpy`的了解，我们可以轻松地使我们的纯Python代码向量化。我们必须引入的唯一新功能是`numpy`的`roll`函数。这个函数做的事情与我们的模数索引技巧相同，但它是针对整个`numpy`数组的。从本质上讲，它向量化了这种重新索引：
- en: '[PRE10]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `roll` function creates a new `numpy` array, which can be thought of as
    both good and bad. The downside is that we are taking time to allocate new space,
    which then needs to be filled with the appropriate data. On the other hand, once
    we have created this new rolled array, we will be able to vectorize operations
    on it quite quickly without suffering from cache misses from the CPU cache. This
    can substantially affect the speed of the actual calculation we must do on the
    grid. Later in this chapter we will rewrite this so that we get the same benefit
    without having to constantly allocate more memory.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`roll`函数创建一个新的`numpy`数组，这既有利也有弊。不利之处在于，我们需要花时间来分配新空间，然后填充适当的数据。另一方面，一旦我们创建了这个新的滚动数组，我们就能够快速对其进行向量化操作，而不会受到来自CPU缓存的缓存未命中的影响。这可以极大地影响我们必须在网格上执行的实际计算的速度。本章后面我们将对此进行改写，以便在不断分配更多内存的情况下获得相同的好处。'
- en: With this additional function we can rewrite the Python diffusion code from
    [Example 6-6](ch06_split_000.xhtml#matrix_pure_python_memory) using simpler, and
    vectorized, `numpy` arrays. [Example 6-9](ch06_split_000.xhtml#matrix_numpy_naive)
    shows our initial `numpy` diffusion code.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 借助这个额外的函数，我们可以使用更简单、向量化的`numpy`数组重写[示例 6-6](ch06_split_000.xhtml#matrix_pure_python_memory)中的Python扩散代码。[示例 6-9](ch06_split_000.xhtml#matrix_numpy_naive)展示了我们最初的`numpy`扩散代码。
- en: Example 6-9\. Initial `numpy` diffusion
  id: totrans-121
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-9\. 初始的`numpy`扩散
- en: '[PRE11]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Immediately we see that this code is much shorter. This is sometimes a good
    indication of performance: we are doing a lot of the heavy lifting outside the
    Python interpreter, and hopefully inside a module specially built for performance
    and for solving a particular problem (however, this should always be tested!).
    One of the assumptions here is that `numpy` is using better memory management
    to give the CPU the data it needs more quickly. However, since whether this happens
    or not relies on the actual implementation of `numpy`, let’s profile our code
    to see whether our hypothesis is correct. [Example 6-10](ch06_split_000.xhtml#matrix_perf_numpy)
    shows the results.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 立即可以看到这段代码要简短得多。这有时是性能良好的一个很好的指标：我们在 Python 解释器外部完成了大部分重活，希望在专门为性能和解决特定问题而构建的模块内完成（但是，这始终需要测试！）。这里的一个假设是
    `numpy` 使用更好的内存管理来更快地为 CPU 提供所需的数据。然而，由于是否发生这种情况取决于 `numpy` 的实际实现，让我们分析我们的代码以查看我们的假设是否正确。[示例
    6-10](ch06_split_000.xhtml#matrix_perf_numpy) 展示了结果。
- en: 'Example 6-10\. Performance counters for `numpy` 2D diffusion (grid size: 640
    × 640, 500 iterations)'
  id: totrans-124
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-10\. `numpy` 二维扩散的性能计数器（网格大小：640 × 640，500 次迭代）
- en: '[PRE12]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This shows that the simple change to `numpy` has given us a 63.3× speedup over
    the pure Python implementation with reduced memory allocations ([Example 6-8](ch06_split_000.xhtml#matrix_perf_python_memory)).
    How was this achieved?
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明对 `numpy` 的简单更改使我们的纯 Python 实现在减少内存分配的情况下加速了 63.3 倍（[示例 6-8](ch06_split_000.xhtml#matrix_perf_python_memory)）。这是如何实现的呢？
- en: First of all, we can thank the vectorization that `numpy` gives. Although the
    `numpy` version seems to be running fewer instructions per cycle, each instruction
    does much more work. That is to say, one vectorized instruction can multiply four
    (or more) numbers in an array together instead of requiring four independent multiplication
    instructions. Overall, this results in fewer total instructions being necessary
    to solve the same problem.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以归功于 `numpy` 提供的矢量化功能。虽然 `numpy` 版本似乎每个周期运行的指令更少，但每个指令的工作量更大。换句话说，一个矢量化的指令可以将数组中的四个（或更多）数字相乘，而不是需要四个独立的乘法指令。总体而言，这导致解决同一问题所需的总指令数量更少。
- en: Several other factors contribute to the `numpy` version requiring a lower absolute
    number of instructions to solve the diffusion problem. One of them has to do with
    the full Python API being available when running the pure Python version, but
    not necessarily for the `numpy` version—for example, the pure Python grids can
    be appended to in pure Python but not in `numpy`. Even though we aren’t explicitly
    using this (or other) functionality, there is overhead in providing a system where
    it *could* be available. Since `numpy` can make the assumption that the data being
    stored is always going to be numbers, everything regarding the arrays can be optimized
    for operations over numbers. We will continue on the track of removing necessary
    functionality in favor of performance when we talk about Cython (see [“Cython”](ch07.xhtml#compiling-cython)),
    where it is even possible to remove list bounds checking to speed up list lookups.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 几个其他因素导致 `numpy` 版本需要较少的绝对指令数来解决扩散问题。其中一个因素与在纯 Python 版本中运行时可用的完整 Python API
    有关，但不一定适用于 `numpy` 版本，例如，纯 Python 网格可以在纯 Python 中追加，但不能在 `numpy` 中。即使我们没有显式使用这个（或其他）功能，提供可以使用的系统仍然存在开销。由于
    `numpy` 可以假设存储的数据始终是数字，因此所有关于数组的操作都可以进行优化。在我们讨论 Cython（参见 [“Cython”](ch07.xhtml#compiling-cython)）时，我们将继续删除必要的功能以换取性能，甚至可以删除列表边界检查以加快列表查找速度。
- en: 'Normally, the number of instructions doesn’t necessarily correlate with performance—the
    program with fewer instructions may not issue them efficiently, or they may be
    slow instructions. However, we see that in addition to reducing the number of
    instructions, the `numpy` version has also reduced a large inefficiency: cache
    misses (20.8% cache misses instead of 53.3%). As explained in [“Memory Fragmentation”](ch06_split_000.xhtml#matrix_memory_fragmentation),
    cache misses slow down computations because the CPU must wait for data to be retrieved
    from slower memory instead of having the data immediately available in its cache.
    In fact, memory fragmentation is such a dominant factor in performance that if
    we disable vectorization in `numpy` but keep everything else the same,^([8](ch06_split_001.xhtml#idm46122420993032))
    we still see a sizable speed increase compared to the pure Python version ([Example 6-11](ch06_split_000.xhtml#matrix_perf_numpy_novec)).'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，指令的数量并不一定与性能相关——指令较少的程序可能没有有效地发出它们，或者它们可能是缓慢的指令。然而，我们看到，除了减少指令的数量外，`numpy`版本还减少了一项重大的低效率：缓存未命中（20.8%的缓存未命中，而不是53.3%）。如[“内存碎片化”](ch06_split_000.xhtml#matrix_memory_fragmentation)中所解释的那样，缓存未命中会减慢计算，因为CPU必须等待数据从较慢的内存中检索，而不是在其缓存中立即可用数据。事实上，内存碎片化在性能中占主导地位，以至于如果我们在`numpy`中禁用矢量化但保持其他一切不变，^([8](ch06_split_001.xhtml#idm46122420993032))与纯Python版本（[例子
    6-11](ch06_split_000.xhtml#matrix_perf_numpy_novec)）相比，我们仍然看到了相当大的速度提升。
- en: 'Example 6-11\. Performance counters for `numpy` 2D diffusion without vectorization
    (grid size: 640 × 640, 500 iterations)'
  id: totrans-130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 例子 6-11\. `numpy` 2D扩散的性能计数器，没有矢量化（网格大小：640 × 640，500次迭代）
- en: '[PRE13]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This shows us that the dominant factor in our 63.3× speedup when introducing
    `numpy` is not the vectorized instruction set but rather the memory locality and
    reduced memory fragmentation. In fact, we can see from the preceding experiment
    that vectorization accounts for only about 13% of that 63.3× speedup.^([9](ch06_split_001.xhtml#idm46122420742936))
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明，当引入`numpy`时我们速度提升了63.3倍的主要因素不是矢量化指令集，而是内存局部性和减少的内存碎片化。事实上，我们可以从前面的实验中看到，矢量化仅占63.3倍速度提升的约13%。^([9](ch06_split_001.xhtml#idm46122420742936))
- en: This realization that memory issues are the dominant factor in slowing down
    our code doesn’t come as too much of a shock. Computers are very well designed
    to do exactly the calculations we are requesting them to do with this problem—multiplying
    and adding numbers together. The bottleneck is in getting those numbers to the
    CPU fast enough to see it do the calculations as fast as it can.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这一认识到内存问题是减慢我们代码速度的主要因素并不令人过于震惊。计算机非常精确地设计来执行我们请求的计算，如乘法和加法。瓶颈在于快速将这些数字传递给CPU，以便它能够尽可能快地执行计算。
- en: Memory Allocations and In-Place Operations
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存分配和原地操作
- en: To optimize the memory-dominated effects, let’s try using the same method we
    used in [Example 6-6](ch06_split_000.xhtml#matrix_pure_python_memory) to reduce
    the number of allocations we make in our `numpy` code. Allocations are quite a
    bit worse than the cache misses we discussed previously. Instead of simply having
    to find the right data in RAM when it is not found in the cache, an allocation
    also must make a request to the operating system for an available chunk of data
    and then reserve it. The request to the operating system generates quite a lot
    more overhead than simply filling a cache—while filling a cache miss is a hardware
    routine that is optimized on the motherboard, allocating memory requires talking
    to another process, the kernel, in order to complete.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 为了优化内存主导的影响，让我们尝试使用与我们在[例子 6-6](ch06_split_000.xhtml#matrix_pure_python_memory)中使用的相同方法来减少我们在`numpy`代码中的分配次数。分配远不及我们之前讨论的缓存未命中糟糕。不仅仅是在RAM中找到正确的数据而不是在缓存中找到时，分配还必须向操作系统请求一个可用的数据块然后进行保留。与简单地填充缓存不同，向操作系统发出请求会产生相当多的开销——尽管填充缓存未命中是在主板上优化的硬件例行程序，但分配内存需要与另一个进程——内核进行通信才能完成。
- en: To remove the allocations in [Example 6-9](ch06_split_000.xhtml#matrix_numpy_naive),
    we will preallocate some scratch space at the beginning of the code and then use
    only in-place operations. In-place operations (such as `+=`, `*=`, etc.) reuse
    one of the inputs as their output. This means that we don’t need to allocate space
    to store the result of the calculation.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 为了消除[示例 6-9](ch06_split_000.xhtml#matrix_numpy_naive)中的分配，我们将在代码开头预分配一些临时空间，然后仅使用原位操作。原位操作（如`+=`、`*=`等）重复使用其中一个输入作为输出。这意味着我们不需要分配空间来存储计算结果。
- en: To show this explicitly, we will look at how the `id` of a `numpy` array changes
    as we perform operations on it ([Example 6-12](ch06_split_000.xhtml#matrix_numpy_inplace_example1)).
    The `id` is a good way of tracking this for `numpy` arrays, since the `id` indicates
    which section of memory is being referenced. If two `numpy` arrays have the same
    `id`, they are referencing the same section of memory.^([10](ch06_split_001.xhtml#idm46122420722136))
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 为了明确显示这一点，我们将看看当对其执行操作时`numpy`数组的`id`如何变化（[示例 6-12](ch06_split_000.xhtml#matrix_numpy_inplace_example1)）。由于`id`指示了引用的内存段，因此`id`是跟踪`numpy`数组的良好方法。如果两个`numpy`数组具有相同的`id`，则它们引用相同的内存段。^([10](ch06_split_001.xhtml#idm46122420722136))
- en: Example 6-12\. In-place operations reducing memory allocations
  id: totrans-138
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-12\. 原位操作减少内存分配
- en: '[PRE14]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[![1](Images/1.png)](ch06_split_000.xhtml#CO9-1a), [![2](Images/2.png)](ch06_split_000.xhtml#CO9-2a)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](ch06_split_000.xhtml#CO9-1a), [![2](Images/2.png)](ch06_split_000.xhtml#CO9-2a)'
- en: These two `id`s are the same, since we are doing an in-place operation. This
    means that the memory address of `array1` does not change; we are simply changing
    the data contained within it.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个`id`是相同的，因为我们在执行原位操作。这意味着`array1`的内存地址并未改变；我们只是改变了其中包含的数据。
- en: '[![3](Images/3.png)](ch06_split_000.xhtml#CO9-3a)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](ch06_split_000.xhtml#CO9-3a)'
- en: Here, the memory address has changed. When doing `array1 + array2`, a new memory
    address is allocated and filled with the result of the computation. This does
    have benefits, however, for when the original data needs to be preserved (i.e.,
    `array3 = array1 + array2` allows you to keep using `array1` and `array2`, while
    in-place operations destroy some of the original data).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，内存地址已经改变。执行`array1 + array2`时，会分配一个新的内存地址，并将计算结果填充进去。然而，这样做有其好处，即在需要保留原始数据时（即`array3
    = array1 + array2`允许您继续使用`array1`和`array2`），而原位操作会破坏部分原始数据。
- en: Furthermore, we can see an expected slowdown from the non-in-place operation.
    In [Example 6-13](ch06_split_000.xhtml#matrix_numpy_inplace_benchmark), we see
    that using in-place operations gives us a 27% speedup for an array of 100 × 100
    elements. This margin will become larger as the arrays grow, since the memory
    allocations become more strenuous. However, it is important to note that this
    effect happens only when the array sizes are bigger than the CPU cache! When the
    arrays are smaller and the two inputs and the output can all fit into cache, the
    out-of-place operation is faster because it can benefit from vectorization.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以看到非原位操作的预期减速。在[示例 6-13](ch06_split_000.xhtml#matrix_numpy_inplace_benchmark)中，我们看到对于100
    × 100元素的数组，使用原位操作可获得27%的加速。随着数组的增长，这个差距将会更大，因为内存分配变得更加紧张。然而，重要的是要注意，这种效果仅在数组大小大于CPU缓存时发生！当数组较小且两个输入和输出都可以适应缓存时，离开原位操作更快，因为它可以从矢量化中获益。
- en: Example 6-13\. Runtime differences between in-place and out-of-place operations
  id: totrans-145
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-13\. 原位和离开原位操作的运行时差异
- en: '[PRE15]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[![1](Images/1.png)](ch06_split_000.xhtml#co_matrix_and_vector_computation_CO5-1)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](ch06_split_000.xhtml#co_matrix_and_vector_computation_CO5-1)'
- en: These arrays are too big to fit into the CPU cache, and the in-place operation
    is faster because of fewer allocations and cache misses.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些数组过大无法适应CPU缓存，原位操作速度更快，因为分配较少且缓存未命中较少。
- en: '[![2](Images/2.png)](ch06_split_000.xhtml#co_matrix_and_vector_computation_CO5-4)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](ch06_split_000.xhtml#co_matrix_and_vector_computation_CO5-4)'
- en: These arrays easily fit into cache, and we see the out-of-place operations as
    faster.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数组很容易适应缓存，我们看到离开原位操作更快。
- en: '[![3](Images/3.png)](ch06_split_000.xhtml#co_matrix_and_vector_computation_CO5-2)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](ch06_split_000.xhtml#co_matrix_and_vector_computation_CO5-2)'
- en: Note that we use `%%timeit` instead of `%timeit`, which allows us to specify
    code to set up the experiment that doesn’t get timed.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用`%%timeit`而不是`%timeit`，这使我们能够指定用于设置实验的代码，而不会计时。
- en: The downside is that while rewriting our code from [Example 6-9](ch06_split_000.xhtml#matrix_numpy_naive)
    to use in-place operations is not very complicated, it does make the resulting
    code a bit harder to read. In [Example 6-14](ch06_split_000.xhtml#matrix_numpy_memory1),
    we can see the results of this refactoring. We instantiate `grid` and `next_grid`
    vectors, and we constantly swap them with each other. `grid` is the current information
    we know about the system, and after running `evolve`, `next_grid` contains the
    updated information.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点是，虽然将我们的代码从 [示例 6-9](ch06_split_000.xhtml#matrix_numpy_naive) 重写为使用原地操作并不是很复杂，但这确实使得结果代码有些难以阅读。在
    [示例 6-14](ch06_split_000.xhtml#matrix_numpy_memory1) 中，我们可以看到这种重构的结果。我们实例化了 `grid`
    和 `next_grid` 向量，并且不断地将它们彼此交换。`grid` 是我们对系统当前信息的了解，而在运行 `evolve` 后，`next_grid`
    包含了更新后的信息。
- en: Example 6-14\. Making most `numpy` operations in-place
  id: totrans-154
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-14\. 使大多数 `numpy` 操作原地执行
- en: '[PRE16]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[![1](Images/1.png)](ch06_split_000.xhtml#co_matrix_and_vector_computation_CO6-1)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](ch06_split_000.xhtml#co_matrix_and_vector_computation_CO6-1)'
- en: Since the output of `evolve` gets stored in the output vector `next_grid`, we
    must swap these two variables so that, for the next iteration of the loop, `grid`
    has the most up-to-date information. This swap operation is quite cheap because
    only the references to the data are changed, not the data itself.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 `evolve` 的输出存储在输出向量 `next_grid` 中，我们必须交换这两个变量，以便在循环的下一次迭代中，`grid` 具有最新的信息。这种交换操作非常便宜，因为只改变了对数据的引用，而不是数据本身。
- en: Warning
  id: totrans-158
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: It is important to remember that since we want each operation to be in-place,
    whenever we do a vector operation, we must put it on its own line. This can make
    something as simple as `A = A * B + C` become quite convoluted. Since Python has
    a heavy emphasis on readability, we should make sure that the changes we have
    made give us sufficient speedups to be justified.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的是，由于我们希望每个操作都是原地执行，每当进行向量操作时，我们必须将其放在自己的一行上。这可能使得像 `A = A * B + C` 这样简单的操作变得非常复杂。由于
    Python 强调可读性，我们应确保我们所做的更改能够提供足够的速度优势来证明其合理性。
- en: Comparing the performance metrics from Examples [6-15](ch06_split_001.xhtml#matrix_numpy_memory1_perf)
    and [6-10](ch06_split_000.xhtml#matrix_perf_numpy), we see that removing the spurious
    allocations sped up our code by 30.9%. This speedup comes partly from a reduction
    in the number of cache misses but mostly from a reduction in minor faults. This
    comes from reducing the number of memory allocations necessary in the code by
    reusing already allocated space.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 从示例 [6-15](ch06_split_001.xhtml#matrix_numpy_memory1_perf) 和 [6-10](ch06_split_000.xhtml#matrix_perf_numpy)
    中比较性能指标，我们看到消除了不必要的分配后，代码的运行速度提高了30.9%。这种加速部分来自缓存未命中数量的减少，但主要来自于小故障的减少。这是通过减少代码中需要的内存分配次数来实现的，通过重复使用已分配的空间。
- en: Minor faults are caused when a program accesses newly allocated space in memory.
    Since memory addresses are lazily allocated by the kernel, when you first access
    newly allocated data, the kernel pauses your execution while it makes sure that
    the required space exists and creates references to it for the program to use.
    This added machinery is quite expensive to run and can slow a program down substantially.
    On top of those additional operations that need to be run, we also lose any state
    that we had in cache and any possibility of doing instruction pipelining. In essence,
    we have to drop everything we’re doing, including all associated optimizations,
    in order to go out and allocate some memory.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 程序访问内存中新分配的空间时会导致小故障。由于内存地址由内核延迟分配，当你首次访问新分配的数据时，内核会暂停你的执行，确保所需空间存在并为程序创建引用。这额外的机制运行起来非常昂贵，并且可能大幅减慢程序运行速度。除了需要运行的额外操作外，我们还会失去缓存中的任何状态以及进行指令流水线处理的可能性。本质上，我们不得不放弃正在进行的所有事情，包括所有相关的优化，以便去分配一些内存。
- en: 'Example 6-15\. Performance metrics for `numpy` with in-place memory operations
    (grid size: 640 × 640, 500 iterations)'
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-15\. 使用原地内存操作的 `numpy` 性能指标（网格大小：640 × 640，500 次迭代）
- en: '[PRE17]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Selective Optimizations: Finding What Needs to Be Fixed'
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择性优化：找出需要修复的问题
- en: 'Looking at the code from [Example 6-14](ch06_split_000.xhtml#matrix_numpy_memory1),
    it seems like we have addressed most of the issues at hand: we have reduced the
    CPU burden by using `numpy`, and we have reduced the number of allocations necessary
    to solve the problem. However, there is always more investigation to be done.
    If we do a line profile on that code ([Example 6-16](ch06_split_001.xhtml#matrix_numpy_memory_lineprof)),
    we see that the majority of our work is done within the `laplacian` function.
    In fact, 84% of the time that `evolve` takes to run is spent in `laplacian`.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 查看来自 [示例 6-14](ch06_split_000.xhtml#matrix_numpy_memory1) 的代码，我们似乎已经解决了大部分问题：通过使用
    `numpy` 减少了 CPU 负担，并减少了解决问题所需的分配次数。然而，还有更多的调查工作需要完成。如果我们对该代码进行行剖析（[示例 6-16](ch06_split_001.xhtml#matrix_numpy_memory_lineprof)），我们会发现大部分工作都是在
    `laplacian` 函数内完成的。事实上，`evolve` 运行所花费的时间中有 84% 是在 `laplacian` 函数内消耗的。
- en: Example 6-16\. Line profiling shows that `laplacian` is taking too much time
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-16\. 行剖析显示 `laplacian` 占用了太多时间。
- en: '[PRE18]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: There could be many reasons `laplacian` is so slow. However, there are two main
    high-level issues to consider. First, it looks like the calls to `np.roll` are
    allocating new vectors (we can verify this by looking at the documentation for
    the function). This means that even though we removed seven memory allocations
    in our previous refactoring, four outstanding allocations remain. Furthermore,
    `np.roll` is a very generalized function that has a lot of code to deal with special
    cases. Since we know exactly what we want to do (move just the first column of
    data to be the last in every dimension), we can rewrite this function to eliminate
    most of the spurious code. We could even merge the `np.roll` logic with the add
    operation that happens with the rolled data to make a very specialized `roll_add`
    function that does exactly what we want with the fewest number of allocations
    and the least extra logic.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '`laplacian` 如此缓慢可能有许多原因。然而，有两个主要的高级问题需要考虑。首先，看起来对 `np.roll` 的调用正在分配新向量（我们可以通过查看函数的文档来验证这一点）。这意味着尽管我们在之前的重构中删除了七次内存分配，但仍然有四次未解决的分配问题。此外，`np.roll`
    是一个非常通用的函数，其中有很多代码处理特殊情况。由于我们确切知道想要做什么（仅将数据的第一列移动到每个维度中的最后），我们可以重写此函数以消除大部分的多余代码。我们甚至可以将
    `np.roll` 的逻辑与滚动数据时发生的添加操作合并，以创建一个非常专门的 `roll_add` 函数，以最少的分配次数和最少的额外逻辑来完成我们想要的操作。'
- en: '[Example 6-17](ch06_split_001.xhtml#matrix_numpy_memory2) shows what this refactoring
    would look like. All we need to do is create our new `roll_add` function and have
    `laplacian` use it. Since `numpy` supports fancy indexing, implementing such a
    function is just a matter of not jumbling up the indices. However, as stated earlier,
    while this code may be more performant, it is much less readable.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 6-17](ch06_split_001.xhtml#matrix_numpy_memory2) 展示了此重构的样子。我们只需创建新的 `roll_add`
    函数并让 `laplacian` 使用它即可。由于 `numpy` 支持花式索引，实现这样一个函数只是不搞乱索引的问题。然而，正如前面所述，虽然这段代码可能性能更好，但可读性大大降低。'
- en: Warning
  id: totrans-170
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Notice the extra work that has gone into having an informative docstring for
    the function, in addition to full tests. When you are taking a route similar to
    this one, it is important to maintain the readability of the code, and these steps
    go a long way toward making sure that your code is always doing what it was intended
    to do and that future programmers can modify your code and know what things do
    and when things are not working.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，为函数编写了信息丰富的文档字符串，并进行了完整的测试工作。当你走类似这样的路线时，保持代码的可读性是很重要的，这些步骤对确保你的代码始终按照预期工作，并让未来的程序员能够修改你的代码并了解事物的作用和不工作时都有很大帮助。
- en: Example 6-17\. Creating our own `roll` function
  id: totrans-172
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-17\. 创建我们自己的 `roll` 函数
- en: '[PRE19]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: If we look at the performance counters in [Example 6-18](ch06_split_001.xhtml#matrix_numpy_memory2_perf)
    for this rewrite, we see that while it is 22% faster than [Example 6-14](ch06_split_000.xhtml#matrix_numpy_memory1),
    most of the counters are about the same. The major difference again is `cache-misses`,
    which is down 7×. This change also seems to have affected the throughput of instructions
    to the CPU, increasing the instructions per cycle from 0.85 to 0.99 (a 14% gain).
    Similarly, the faults went down 12.85%. This seems to be a result of first doing
    the rolls in place as well as reducing the amount of `numpy` machinery that needs
    to be in-place to do all of our desired computation. Instead of first rolling
    our array, doing other computation required by `numpy` in order to do bounds checking
    and error handling, and then adding the result, we are doing it all in one shot
    and not requiring the computer to refill the cache every time. This theme of trimming
    out unnecessary machinery in both `numpy` and Python in general will continue
    in [“Cython”](ch07.xhtml#compiling-cython).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看这个重写的性能计数器在 [示例 6-18](ch06_split_001.xhtml#matrix_numpy_memory2_perf)
    中，我们可以看到，虽然比 [示例 6-14](ch06_split_000.xhtml#matrix_numpy_memory1) 快 22%，但大多数计数器几乎都相同。主要的区别再次在于
    `cache-misses`，下降了 7 倍。这一变化似乎还影响了将指令传输到 CPU 的吞吐量，将每周期的指令数从 0.85 增加到 0.99（增加了 14%）。同样，故障减少了
    12.85%。这似乎是首先在原地进行滚动以及减少 `numpy` 机制所需的结果，以执行所有所需的计算。不再需要计算机在每次操作时重新填充缓存。这种消除 `numpy`
    和一般 Python 中不必要机制的主题将继续在 [“Cython”](ch07.xhtml#compiling-cython) 中。
- en: 'Example 6-18\. Performance metrics for `numpy` with in-place memory operations
    and custom `laplacian` function (grid size: 640 × 640, 500 iterations)'
  id: totrans-175
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-18\. `numpy` 使用原地内存操作和自定义 `laplacian` 函数的性能指标（网格大小：640 × 640，500 次迭代）
- en: '[PRE20]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'numexpr: Making In-Place Operations Faster and Easier'
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: numexpr：使原地操作更快速更便捷
- en: One downfall of `numpy`’s optimization of vector operations is that it occurs
    on only one operation at a time. That is to say, when we are doing the operation
    `A * B + C` with `numpy` vectors, first the entire `A * B` operation completes,
    and the data is stored in a temporary vector; then this new vector is added with
    `C`. The in-place version of the diffusion code in [Example 6-14](ch06_split_000.xhtml#matrix_numpy_memory1)
    shows this quite explicitly.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '`numpy` 对向量操作的优化的一个缺点是，它仅一次处理一个操作。也就是说，当我们使用 `numpy` 向量执行操作 `A * B + C` 时，首先完成整个
    `A * B` 操作，然后将数据存储在临时向量中；然后将这个新向量与 `C` 相加。[示例 6-14](ch06_split_000.xhtml#matrix_numpy_memory1)中的扩散代码的原地版本明确展示了这一点。'
- en: However, many modules can help with this. `numexpr` is a module that can take
    an entire vector expression and compile it into very efficient code that is optimized
    to minimize cache misses and temporary space used. In addition, the expressions
    can utilize multiple CPU cores (see [Chapter 9](ch09_split_000.xhtml#multiprocessing)
    for more information) and take advantage of the specialized instructions your
    CPU may support in order to get even greater speedups. It even supports OpenMP,
    which parallels out operations across multiple cores on your machine.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，许多模块都可以帮助实现这一点。 `numexpr` 是一个可以将整个向量表达式编译成非常高效的代码的模块，优化以减少缓存未命中和临时空间的使用。此外，这些表达式可以利用多个
    CPU 核心（详见[第 9 章](ch09_split_000.xhtml#multiprocessing)了解更多信息），并利用您的 CPU 可能支持的专用指令，以获得更大的加速效果。它甚至支持
    OpenMP，可以在您的机器上并行执行操作。
- en: 'It is very easy to change code to use `numexpr`: all that’s required is to
    rewrite the expressions as strings with references to local variables. The expressions
    are compiled behind the scenes (and cached so that calls to the same expression
    don’t incur the same cost of compilation) and run using optimized code. [Example 6-19](ch06_split_001.xhtml#matrix_numpy_numexpr)
    shows the simplicity of changing the `evolve` function to use `numexpr`. In this
    case, we chose to use the `out` parameter of the `evaluate` function so that `numexpr`
    doesn’t allocate a new vector to which to return the result of the calculation.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 改用 `numexpr` 很容易：所需的只是将表达式重写为带有本地变量引用的字符串。这些表达式在后台编译（并缓存，以便对相同表达式的调用不会再次付出编译成本），并使用优化代码运行。[示例
    6-19](ch06_split_001.xhtml#matrix_numpy_numexpr)展示了将 `evolve` 函数改用 `numexpr` 的简易程度。在这种情况下，我们选择使用
    `evaluate` 函数的 `out` 参数，以便 `numexpr` 不会分配新的向量来存储计算结果。
- en: Example 6-19\. Using `numexpr` to further optimize large matrix operations
  id: totrans-181
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-19\. 使用`numexpr`进一步优化大矩阵操作
- en: '[PRE21]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: An important feature of `numexpr` is its consideration of CPU caches. It specifically
    moves data around so that the various CPU caches have the correct data in order
    to minimize cache misses. When we run `perf` on the updated code ([Example 6-20](ch06_split_001.xhtml#matrix_numpy_numexpr_perf)),
    we see a speedup. However, if we look at the performance on a smaller, 256 × 256
    grid, we see a decrease in speed (see [Table 6-2](ch06_split_001.xhtml#matrix_table_speedup_perf)).
    Why is this?
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '`numexpr`的一个重要特性是考虑CPU缓存。它特别移动数据，以便各种CPU缓存具有正确的数据，以最小化缓存未命中。当我们在更新后的代码上运行`perf`（[示例 6-20](ch06_split_001.xhtml#matrix_numpy_numexpr_perf)）时，我们看到了加速。然而，如果我们看一下256
    × 256的较小网格的性能，则会看到速度下降（参见[表 6-2](ch06_split_001.xhtml#matrix_table_speedup_perf)）。这是为什么呢？'
- en: 'Example 6-20\. Performance metrics for `numpy` with in-place memory operations,
    custom `laplacian` function, and `numexpr` (grid size: 640 × 640, 500 iterations)'
  id: totrans-184
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-20\. 用于`numpy`的性能指标与原地内存操作，自定义`laplacian`函数和`numexpr`（网格大小：640 × 640，500次迭代）
- en: '[PRE22]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Much of the extra machinery we are bringing into our program with `numexpr`
    deals with cache considerations. When our grid size is small and all the data
    we need for our calculations fits in the cache, this extra machinery simply adds
    more instructions that don’t help performance. In addition, compiling the vector
    operation that we encoded as a string adds a large overhead. When the total runtime
    of the program is small, this overhead can be quite noticeable. However, as we
    increase the grid size, we should expect to see `numexpr` utilize our cache better
    than native `numpy` does. In addition, `numexpr` utilizes multiple cores to do
    its calculation and tries to saturate each of the cores’ caches. When the size
    of the grid is small, the extra overhead of managing the multiple cores overwhelms
    any possible increase in speed.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在程序中引入`numexpr`的大部分额外机制涉及缓存考虑。当我们的网格大小较小且所有计算所需的数据都适合缓存时，这些额外机制只会增加更多不利于性能的指令。此外，编译我们以字符串编码的向量操作会增加很大的开销。当程序的总运行时间较短时，这种开销可能非常明显。然而，随着网格大小的增加，我们应该期望看到`numexpr`比原生`numpy`更好地利用我们的缓存。此外，`numexpr`利用多个核心进行计算，并尝试饱和每个核心的缓存。当网格大小较小时，管理多个核心的额外开销会超过任何可能的速度增加。
- en: The particular computer we are running the code on has a 8,192 KB cache (Intel
    Core i7-7820HQ). Since we are operating on two arrays, one for input and one for
    output, we can easily do the calculation for the size of the grid that will fill
    up our cache. The number of grid elements we can store in total is 8,192 KB /
    64 bit = 1,024,000\. Since we have two grids, this number is split between two
    objects (so each one can be at most 1,024,000 / 2 = 512,000 elements). Finally,
    taking the square root of this number gives us the size of the grid that uses
    that many grid elements. All in all, this means that approximately two 2D arrays
    of size 715 × 715 would fill up the cache ( <math alttext="StartRoot 8192 upper
    K upper B slash 64 b i t slash 2 EndRoot equals 715.5"><mrow><msqrt><mrow><mn>8192</mn>
    <mi>K</mi> <mi>B</mi> <mo>/</mo> <mn>64</mn> <mi>b</mi> <mi>i</mi> <mi>t</mi>
    <mo>/</mo> <mn>2</mn></mrow></msqrt> <mo>=</mo> <mn>715</mn> <mo>.</mo> <mn>5</mn></mrow></math>
    ). In practice, however, we do not get to fill up the cache ourselves (other programs
    will fill up parts of the cache), so realistically we can probably fit two 640
    × 640 arrays. Looking at Tables [6-1](ch06_split_001.xhtml#matrix_table_runtime_perf)
    and [6-2](ch06_split_001.xhtml#matrix_table_speedup_perf), we see that when the
    grid size jumps from 512 × 512 to 1,024 × 1,024, the `numexpr` code starts to
    outperform pure `numpy`.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们运行代码的特定计算机具有8,192 KB的缓存（Intel Core i7-7820HQ）。因为我们正在操作两个数组，一个用于输入，一个用于输出，所以我们可以轻松地计算填充我们缓存的网格大小。我们总共可以存储的网格元素数量为8,192
    KB / 64位 = 1,024,000。由于我们有两个网格，所以这个数字分为两个对象（因此每个对象最多可以有1,024,000 / 2 = 512,000个元素）。最后，对这个数字取平方根会给出使用这么多网格元素的网格大小。总的来说，这意味着大约两个大小为715
    × 715的二维数组会填满缓存（ <math alttext="StartRoot 8192 upper K upper B slash 64 b i t
    slash 2 EndRoot equals 715.5"><mrow><msqrt><mrow><mn>8192</mn> <mi>K</mi> <mi>B</mi>
    <mo>/</mo> <mn>64</mn> <mi>b</mi> <mi>i</mi> <mi>t</mi> <mo>/</mo> <mn>2</mn></mrow></msqrt>
    <mo>=</mo> <mn>715</mn> <mo>.</mo> <mn>5</mn></mrow></math>）。然而，在实际操作中，我们并不能自己填满缓存（其他程序将填满部分缓存），所以实际上我们可能可以容纳两个640
    × 640的数组。查看表 [6-1](ch06_split_001.xhtml#matrix_table_runtime_perf) 和 [6-2](ch06_split_001.xhtml#matrix_table_speedup_perf)，我们可以看到，当网格大小从512
    × 512跳至1,024 × 1,024时，`numexpr`代码开始优于纯`numpy`。
- en: 'A Cautionary Tale: Verify “Optimizations” (scipy)'
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个警示故事：验证“优化”（scipy）
- en: 'An important thing to take away from this chapter is the approach we took to
    every optimization: profile the code to get a sense of what is going on, come
    up with a possible solution to fix slow parts, and then profile to make sure the
    fix actually worked. Although this sounds straightforward, things can get complicated
    quickly, as we saw in how the performance of `numexpr` depended greatly on the
    size of the grid we were considering.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 从本章中需要记住的重要一点是我们对每一个优化采取的方法：对代码进行分析以了解正在发生的情况，提出可能的解决方案来修复慢速部分，然后再次分析以确保修复确实有效。尽管这听起来很简单，但情况可能很快变得复杂，正如我们在考虑网格大小时看到的
    `numexpr` 性能如何依赖于。
- en: Of course, our proposed solutions don’t always work as expected. While writing
    the code for this chapter, we saw that the `laplacian` function was the slowest
    routine and hypothesized that the `scipy` routine would be considerably faster.
    This thinking came from the fact that Laplacians are a common operation in image
    analysis and probably have a very optimized library to speed up the calls. `scipy`
    has an image submodule, so we must be in luck!
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们提出的解决方案并不总是按预期工作。在编写本章代码时，我们发现 `laplacian` 函数是最慢的例程，并假设 `scipy` 的例程会快得多。这种想法来自于
    Laplacian 在图像分析中是常见操作，可能有非常优化的库来加速调用。`scipy` 有一个图像子模块，所以我们一定会有所收获！
- en: The implementation was quite simple ([Example 6-21](ch06_split_001.xhtml#matrix_numpy_scipy))
    and required little thought about the intricacies of implementing the periodic
    boundary conditions (or “wrap” condition, as `scipy` calls it).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 实现非常简单（[示例 6-21](ch06_split_001.xhtml#matrix_numpy_scipy)），并且几乎不需要思考如何实现周期性边界条件（或者如
    `scipy` 所称的“wrap”条件）。
- en: Example 6-21\. Using `scipy`’s `laplace` filter
  id: totrans-192
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-21\. 使用 `scipy` 的 `laplace` 过滤器
- en: '[PRE23]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Ease of implementation is quite important and definitely won this method some
    points before we considered performance. However, once we benchmarked the `scipy`
    code ([Example 6-22](ch06_split_001.xhtml#matrix_numpy_scipy_perf)), we had a
    revelation: this method offers no substantial speedup compared to the code it
    is based on ([Example 6-14](ch06_split_000.xhtml#matrix_numpy_memory1)). In fact,
    as we increase the grid size, this method starts performing worse (see [Figure 6-4](ch06_split_001.xhtml#matrix_figure_all_perf)
    at the end of the chapter).'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 实施的简易性非常重要，在我们考虑性能之前，这种方法确实赢得了一些好评。然而，一旦我们对 `scipy` 代码进行了基准测试（[示例 6-22](ch06_split_001.xhtml#matrix_numpy_scipy_perf)），我们有了一个领悟：与其基于的代码相比，这种方法并没有提供实质性的加速效果（参见[示例 6-14](ch06_split_000.xhtml#matrix_numpy_memory1)）。事实上，随着网格大小的增加，这种方法的性能开始变差（请查看本章末尾的[图 6-4](ch06_split_001.xhtml#matrix_figure_all_perf)）。
- en: 'Example 6-22\. Performance metrics for diffusion with `scipy`’s `laplace` function
    (grid size: 640 × 640, 500 iterations)'
  id: totrans-195
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-22\. 使用 `scipy` 的 `laplace` 函数进行扩散性能指标测试（网格大小：640 × 640，500 次迭代）
- en: '[PRE24]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Comparing the performance metrics of the `scipy` version of the code with those
    of our custom `laplacian` function ([Example 6-18](ch06_split_001.xhtml#matrix_numpy_memory2_perf)),
    we can start to get some indication as to why we aren’t getting the speedup we
    were expecting from this rewrite.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 比较 `scipy` 版本代码与我们自定义的 `laplacian` 函数的性能指标（[示例 6-18](ch06_split_001.xhtml#matrix_numpy_memory2_perf)），我们可以开始得出一些线索，了解为什么从这次重写中没有得到预期的加速效果。
- en: The metric that stands out the most is `instructions`. This shows us that the
    `scipy` code is requesting that the CPU do more than double the amount of work
    as our custom `laplacian` code. Even though these instructions are more numerically
    optimized (as we can see with the higher `insn per cycle` count, which says how
    many instructions the CPU can do in one clock cycle), the extra optimization doesn’t
    win out over the sheer number of added instructions.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 最引人注目的指标是 `instructions`。这表明 `scipy` 代码要求 CPU 执行的工作量是我们自定义 `laplacian` 代码的两倍以上。尽管这些指令在数值上更优化（正如我们从更高的
    `insn per cycle` 计数中可以看出来，这个数值表示 CPU 在一个时钟周期内可以执行多少指令），额外的优化并没有因为增加的指令数量而胜出。
- en: 'This could be in part because the `scipy` code is written very generally so
    that it can process all sorts of inputs with different boundary conditions (which
    requires extra code and thus more instructions). We can see this, in fact, by
    the high number of `branches` that the `scipy` code requires. When code has many
    branches, it means that we run commands based on a condition (like having code
    inside an `if` statement). The problem is that we don’t know whether we can evaluate
    an expression until we check the conditional, so vectorizing or pipelining isn’t
    possible. The machinery of branch prediction helps with this, but it isn’t perfect.
    This speaks more to the point of the speed of specialized code: if you don’t need
    to constantly check what you need to do and instead know the specific problem
    at hand, you can solve it much more efficiently.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这在一定程度上可能是因为 `scipy` 代码编写得非常普遍，因此它可以处理各种带有不同边界条件的输入（这需要额外的代码和更多指令）。实际上，我们可以通过
    `scipy` 代码所需的高分支数看到这一点。当代码有许多分支时，意味着我们基于条件运行命令（例如在 `if` 语句中有代码）。问题在于，在检查条件之前，我们不知道是否可以评估表达式，因此无法进行向量化或流水线处理。分支预测机制可以帮助解决这个问题，但并不完美。这更多地说明了专门代码速度的重要性：如果你不需要不断检查需要执行的操作，而是知道手头的具体问题，那么你可以更有效地解决它。
- en: Lessons from Matrix Optimizations
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 矩阵优化的教训
- en: 'Looking back on our optimizations, we seem to have taken two main routes: reducing
    the time taken to get data to the CPU and reducing the amount of work that the
    CPU had to do. Tables [6-1](ch06_split_001.xhtml#matrix_table_runtime_perf) and
    [6-2](ch06_split_001.xhtml#matrix_table_speedup_perf) compare of the results achieved
    by our various optimization efforts, for various dataset sizes, in relation to
    the original pure Python implementation.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾我们的优化过程，我们似乎走了两条主要路线：减少将数据传输到 CPU 的时间和减少 CPU 需要执行的工作量。表格 [6-1](ch06_split_001.xhtml#matrix_table_runtime_perf)
    和 [6-2](ch06_split_001.xhtml#matrix_table_speedup_perf) 比较了我们在各种数据集大小下，相对于原始纯
    Python 实现，通过各种优化努力达到的结果。
- en: '[Figure 6-4](ch06_split_001.xhtml#matrix_figure_all_perf) shows graphically
    how all these methods compared to one another. We can see three bands of performance
    that correspond to these two methods: the band along the bottom shows the small
    improvement made in relation to our pure Python implementation by our first effort
    at reducing memory allocations; the middle band shows what happened when we used
    `numpy` and further reduced allocations; and the upper band illustrates the results
    achieved by reducing the work done by our process.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6-4 显示了所有这些方法相互比较的图形化展示。我们可以看到三个性能带，分别对应这两种方法：底部的带显示了通过我们的第一个内存分配减少尝试对我们的纯
    Python 实现所做的小改进；中间带显示了当我们使用 `numpy` 进一步减少分配时发生的情况；而上带则说明了通过减少我们的处理过程所做工作来实现的结果。
- en: Table 6-1\. Total runtime of all schemes for various grid sizes and 500 iterations
    of the `evolve` function
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 6-1\. 各种网格大小和 `evolve` 函数的 500 次迭代的所有方案的总运行时间
- en: '| Method | 256 x 256 | 512 x 512 | 1024 x 1024 | 2048 x 2048 | 4096 x 4096
    |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 256 x 256 | 512 x 512 | 1024 x 1024 | 2048 x 2048 | 4096 x 4096 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| [Python](ch06_split_000.xhtml#matrix_pure_python_run) | 2.40s | 10.43s |
    41.75s | 168.82s | 679.16s |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| [Python](ch06_split_000.xhtml#matrix_pure_python_run) | 2.40s | 10.43s |
    41.75s | 168.82s | 679.16s |'
- en: '| [Python + memory](ch06_split_000.xhtml#matrix_pure_python_memory) | 2.26s
    | 9.76s | 38.85s | 157.25s | 632.76s |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| [Python + 内存](ch06_split_000.xhtml#matrix_pure_python_memory) | 2.26s | 9.76s
    | 38.85s | 157.25s | 632.76s |'
- en: '| [numpy](ch06_split_000.xhtml#matrix_numpy_naive) | 0.01s | 0.09s | 0.69s
    | 3.77s | 14.83s |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| [numpy](ch06_split_000.xhtml#matrix_numpy_naive) | 0.01s | 0.09s | 0.69s
    | 3.77s | 14.83s |'
- en: '| [numpy + memory](ch06_split_000.xhtml#matrix_numpy_memory1) | 0.01s | 0.07s
    | 0.60s | 3.80s | 14.97s |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| [numpy + 内存](ch06_split_000.xhtml#matrix_numpy_memory1) | 0.01s | 0.07s |
    0.60s | 3.80s | 14.97s |'
- en: '| [numpy + memory + laplacian](ch06_split_001.xhtml#matrix_numpy_memory2) |
    0.01s | 0.05s | 0.48s | 1.86s | 7.50s |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| [numpy + 内存 + 拉普拉斯](ch06_split_001.xhtml#matrix_numpy_memory2) | 0.01s |
    0.05s | 0.48s | 1.86s | 7.50s |'
- en: '| [numpy + memory + laplacian + numexpr](ch06_split_001.xhtml#matrix_numpy_numexpr)
    | 0.02s | 0.06s | 0.41s | 1.60s | 6.45s |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| [numpy + 内存 + 拉普拉斯 + numexpr](ch06_split_001.xhtml#matrix_numpy_numexpr)
    | 0.02s | 0.06s | 0.41s | 1.60s | 6.45s |'
- en: '| [numpy + memory + scipy](ch06_split_001.xhtml#matrix_numpy_scipy) | 0.05s
    | 0.25s | 1.15s | 6.83s | 91.43s |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| [numpy + 内存 + scipy](ch06_split_001.xhtml#matrix_numpy_scipy) | 0.05s | 0.25s
    | 1.15s | 6.83s | 91.43s |'
- en: Table 6-2\. Speedup compared to naive Python ([Example 6-3](ch06_split_000.xhtml#matrix_pure_python))
    for all schemes and various grid sizes over 500 iterations of the `evolve` function
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6-2\. 对纯 Python（[示例 6-3](ch06_split_000.xhtml#matrix_pure_python)）的加速比，所有方案和各种网格大小在
    `evolve` 函数的 500 次迭代中进行比较
- en: '| Method | 256 x 256 | 512 x 512 | 1024 x 1024 | 2048 x 2048 | 4096 x 4096
    |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 256 x 256 | 512 x 512 | 1024 x 1024 | 2048 x 2048 | 4096 x 4096 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| [Python](ch06_split_000.xhtml#matrix_pure_python_run) | 1.00x | 1.00x | 1.00x
    | 1.00x | 1.00x |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| [Python](ch06_split_000.xhtml#matrix_pure_python_run) | 1.00x | 1.00x | 1.00x
    | 1.00x | 1.00x |'
- en: '| [Python + memory](ch06_split_000.xhtml#matrix_pure_python_memory) | 1.06x
    | 1.07x | 1.07x | 1.07x | 1.07x |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| [Python + memory](ch06_split_000.xhtml#matrix_pure_python_memory) | 1.06x
    | 1.07x | 1.07x | 1.07x | 1.07x |'
- en: '| [numpy](ch06_split_000.xhtml#matrix_numpy_naive) | 170.59x | 116.16x | 60.49x
    | 44.80x | 45.80x |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| [numpy](ch06_split_000.xhtml#matrix_numpy_naive) | 170.59x | 116.16x | 60.49x
    | 44.80x | 45.80x |'
- en: '| [numpy + memory](ch06_split_000.xhtml#matrix_numpy_memory1) | 185.97x | 140.10x
    | 69.67x | 44.43x | 45.36x |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| [numpy + memory](ch06_split_000.xhtml#matrix_numpy_memory1) | 185.97x | 140.10x
    | 69.67x | 44.43x | 45.36x |'
- en: '| [numpy + memory + laplacian](ch06_split_001.xhtml#matrix_numpy_memory2) |
    203.66x | 208.15x | 86.41x | 90.91x | 90.53x |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| [numpy + memory + laplacian](ch06_split_001.xhtml#matrix_numpy_memory2) |
    203.66x | 208.15x | 86.41x | 90.91x | 90.53x |'
- en: '| [numpy + memory + laplacian + numexpr](ch06_split_001.xhtml#matrix_numpy_numexpr)
    | 97.41x | 167.49x | 102.38x | 105.69x | 105.25x |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| [numpy + memory + laplacian + numexpr](ch06_split_001.xhtml#matrix_numpy_numexpr)
    | 97.41x | 167.49x | 102.38x | 105.69x | 105.25x |'
- en: '| [numpy + memory + scipy](ch06_split_001.xhtml#matrix_numpy_scipy) | 52.27x
    | 42.00x | 36.44x | 24.70x | 7.43x |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| [numpy + memory + scipy](ch06_split_001.xhtml#matrix_numpy_scipy) | 52.27x
    | 42.00x | 36.44x | 24.70x | 7.43x |'
- en: One important lesson to take away from this is that you should always take care
    of any administrative things the code must do during initialization. This may
    include allocating memory, or reading configuration from a file, or even precomputing
    values that will be needed throughout the lifetime of the program. This is important
    for two reasons. First, you are reducing the total number of times these tasks
    must be done by doing them once up front, and you know that you will be able to
    use those resources without too much penalty in the future. Second, you are not
    disrupting the flow of the program; this allows it to pipeline more efficiently
    and keep the caches filled with more pertinent data.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 从中可以得到一个重要教训，那就是你应该始终处理代码在初始化期间必须完成的任何管理事务。这可能包括分配内存，或从文件中读取配置，甚至预先计算将在程序生命周期中需要的值。这有两个重要原因。首先，通过一次性地执行这些任务，你减少了这些任务必须完成的总次数，并且你知道未来将能够在不太多的惩罚的情况下使用这些资源。其次，你不会打断程序的流程；这使其能够更有效地进行流水线处理，并保持缓存中充满更相关的数据。
- en: You also learned more about the importance of data locality and how important
    simply getting data to the CPU is. CPU caches can be quite complicated, and often
    it is best to allow the various mechanisms designed to optimize them take care
    of the issue. However, understanding what is happening and doing all that is possible
    to optimize the way memory is handled can make all the difference. For example,
    by understanding how caches work, we are able to understand that the decrease
    in performance that leads to a saturated speedup no matter the grid size in [Figure 6-4](ch06_split_001.xhtml#matrix_figure_all_perf)
    can probably be attributed to the L3 cache being filled up by our grid. When this
    happens, we stop benefiting from the tiered memory approach to solving the von
    Neumann bottleneck.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 你还学到了更多关于数据局部性的重要性，以及将数据简单传输到 CPU 的重要性。CPU 缓存可能非常复杂，通常最好让设计用于优化它们的各种机制来处理这个问题。然而，理解正在发生的事情，并尽可能优化内存处理方式，可以产生重大影响。例如，通过理解缓存的工作原理，我们能够理解，无论网格大小如何，在
    [图 6-4](ch06_split_001.xhtml#matrix_figure_all_perf) 中导致性能下降的饱和加速度可能是由于我们的网格填满了
    L3 缓存。当这种情况发生时，我们停止从层次化内存方法中受益，以解决冯·诺依曼瓶颈。
- en: '![Summary of speedups from the methods attempted in this chapter](Images/hpp2_0604.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![本章尝试方法的速度提升总结](Images/hpp2_0604.png)'
- en: Figure 6-4\. Summary of speedups from the methods attempted in this chapter
  id: totrans-226
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-4\. 本章尝试方法的速度提升总结
- en: Another important lesson concerns the use of external libraries. Python is fantastic
    for its ease of use and readability, which allow you to write and debug code fast.
    However, tuning performance down to the external libraries is essential. These
    external libraries can be extremely fast, because they can be written in lower-level
    languages—but since they interface with Python, you can also still write code
    that uses them quickly.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的教训涉及使用外部库。Python 以其易用性和可读性而闻名，使您能够快速编写和调试代码。然而，调整性能以适应外部库是至关重要的。这些外部库可以非常快，因为它们可以用较低级别语言编写，但由于它们与
    Python 接口，您仍然可以快速编写使用它们的代码。
- en: Finally, we learned the importance of benchmarking everything and forming hypotheses
    about performance before running the experiment. By forming a hypothesis before
    running the benchmark, we are able to form conditions to tell us whether our optimization
    actually worked. Was this change able to speed up runtime? Did it reduce the number
    of allocations? Is the number of cache misses lower? Optimization can be an art
    at times because of the vast complexity of computer systems, and having a quantitative
    probe into what is actually happening can help enormously.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们学到了在运行实验之前对所有内容进行基准测试并形成性能假设的重要性。通过在运行基准测试之前形成假设，我们能够制定条件来告诉我们优化是否真正起作用。这个改变能够加快运行时间吗？它减少了分配的数量吗？缓存未命中的数量是否更少？优化有时候可以成为一种艺术，因为计算系统的复杂性非常庞大，通过定量探索实际发生的事情可以极大地帮助。
- en: One last point about optimization is that a lot of care must be taken to make
    sure that the optimizations you make generalize to different computers (the assumptions
    you make and the results of benchmarks you do may be dependent on the architecture
    of the computer you are running on, how the modules you are using were compiled,
    etc.). In addition, when making these optimizations, it is incredibly important
    to consider other developers and how the changes will affect the readability of
    your code. For example, we realized that the solution we implemented in [Example 6-17](ch06_split_001.xhtml#matrix_numpy_memory2)
    was potentially vague, so care was taken to make sure that the code was fully
    documented and tested to help not only us but also other people on the team.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一点关于优化的内容是必须非常小心确保您所做的优化可以推广到不同的计算机（您所做的假设和基准测试的结果可能取决于您正在运行的计算机的体系结构，以及您使用的模块是如何编译的等等）。此外，在进行这些优化时，考虑其他开发人员以及更改将如何影响代码的可读性非常重要。例如，我们意识到我们在
    [示例 6-17](ch06_split_001.xhtml#matrix_numpy_memory2) 中实施的解决方案可能存在模糊性，因此需要确保代码被充分记录和测试，以帮助我们团队内部以及其他人。
- en: Sometimes, however, your numerical algorithms also require quite a lot of data
    wrangling and manipulation that aren’t just clear-cut mathematical operations.
    In these cases, Pandas is a very popular solution, and it has its own performance
    characteristics. We’ll now do a deep dive into Pandas and understand how to better
    use it to write performant numerical code.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有时候，您的数值算法也需要相当多的数据整理和操作，而不仅仅是明确的数学操作。在这些情况下，Pandas 是一个非常流行的解决方案，它有其自身的性能特征。我们现在将深入研究
    Pandas 并了解如何更好地使用它来编写高性能的数值代码。
- en: Pandas
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pandas
- en: '*Pandas* is the de facto data manipulation tool in the scientific Python ecosystem
    for tabular data. It enables easy manipulation with Excel-like tables of heterogeneous
    datatypes, known as DataFrames, and has strong support for time-series operations.
    Both the public interface and the internal machinery have evolved a lot since
    2008, and there’s a lot of conflicting information in public forums on “fast ways
    to solve problems.” In this section, we’ll fix some misconceptions about common
    use cases of Pandas.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '*Pandas* 是科学 Python 生态系统中处理表格数据的事实标准工具。它能轻松处理类似 Excel 的异构数据类型表格，称为 DataFrames，并且对时间序列操作有强大支持。自
    2008 年以来，公共接口和内部机制都有了很大的发展，公共论坛上关于“快速解决问题的方法”存在很多争议信息。在本节中，我们将纠正关于 Pandas 常见用例的一些误解。'
- en: We’ll review the internal model for Pandas, find out how to apply a function
    efficiently across a DataFrame, see why concatenating to a DataFrame repeatedly
    is a poor way to build up a result, and look at faster ways of handling strings.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将审查 Pandas 的内部模型，找出如何在 DataFrame 上高效应用函数，看看为什么重复连接到 DataFrame 是构建结果的一个不良方式，并探讨处理字符串的更快方法。
- en: Pandas’s Internal Model
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pandas 内部模型
- en: Pandas uses an in-memory, 2D, table-like data structure—if you have in mind
    an Excel sheet, you have a good initial mental model. Originally, Pandas focused
    on NumPy’s `dtype` objects such as signed and unsigned numbers for each column.
    As the library evolved, it expanded beyond NumPy types and can now handle both
    Python strings and extension types (including nullable `Int64` objects—note the
    capital “I”—and IP addresses).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas 使用内存中的二维表格数据结构 —— 如果你想象一个 Excel 表格，那么你已经有了一个很好的初始心理模型。最初，Pandas 主要专注于
    NumPy 的 `dtype` 对象，如每列的有符号和无符号数字。随着库的发展，它扩展到超出 NumPy 类型，现在可以处理 Python 字符串和扩展类型（包括可空的
    `Int64` 对象 —— 注意大写的 “I” —— 和 IP 地址）。
- en: Operations on a DataFrame apply to all cells in a column (or all cells in a
    row if the `axis=1` parameter is used), all operations are executed eagerly, and
    there’s no support for query planning. Operations on columns often generate temporary
    intermediate arrays, which consume RAM. The general advice is to expect a temporary
    memory usage envelope of up to three to five times your current usage when you’re
    manipulating your DataFrames. Typically, Pandas works well for datasets under
    10 GB in size, assuming you have sufficient RAM for temporary results.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame 上的操作适用于列中的所有单元格（或者如果使用 `axis=1` 参数，则适用于行中的所有单元格），所有操作都是急切执行的，并且不支持查询计划。对列的操作通常会生成临时中间数组，这些数组会消耗
    RAM。一般建议是，当您操作您的 DataFrame 时，预期的临时内存使用量应为当前使用量的三到五倍。通常情况下，假设您有足够的 RAM 用于临时结果，Pandas
    对小于 10 GB 大小的数据集效果很好。
- en: Operations can be single-threaded and may be limited by Python’s global interpreter
    lock (GIL). Increasingly, improved internal implementations are allowing the GIL
    to be disabled automatically, enabling parallelized operations. We’ll explore
    an approach to parallelization with Dask in [“Parallel Pandas with Dask”](ch10.xhtml#clustering-dask).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 操作可以是单线程的，可能受制于 Python 的全局解释器锁（GIL）。随着内部实现的改进，越来越多的情况下，GIL 可以自动禁用，从而实现并行操作。我们将探讨使用
    Dask 进行并行化的方法，在[《使用 Dask 进行并行 Pandas 操作》](ch10.xhtml#clustering-dask)中。
- en: Behind the scenes, columns of the same `dtype` are grouped together by a `BlockManager`.
    This piece of hidden machinery works to make row-wise operations on columns of
    the same datatype faster. It is one of the many hidden technical details that
    make the Pandas code base complex but make the high-level user-facing operations
    faster.^([11](ch06_split_001.xhtml#idm46122419857272))
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 幕后，相同 `dtype` 的列被 `BlockManager` 分组在一起。这个隐藏的机制部件旨在加快对相同数据类型列的行操作。这是使 Pandas
    代码库复杂但使高级用户界面操作更快的许多隐藏技术细节之一。^([11](ch06_split_001.xhtml#idm46122419857272))
- en: Performing operations on a subset of data from a single common block typically
    generates a view, while taking a slice of rows that cross blocks of different
    `dtypes` can cause a copy, which may be slower. One consequence is that while
    numeric columns directly reference their NumPy data, string columns reference
    a list of Python strings, and these individual strings are scattered in memory—this
    can lead to unexpected speed differences for numeric and string operations.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在单个公共块的数据子集上执行操作通常会生成一个视图，而在跨不同 `dtype` 块的行切片可能会导致复制，这可能会较慢。一个后果是，虽然数字列直接引用它们的
    NumPy 数据，但字符串列引用一个 Python 字符串列表，并且这些单独的字符串在内存中分散 —— 这可能会导致数字和字符串操作的速度差异出乎意料。
- en: Behind the scenes, Pandas uses a mix of NumPy datatypes and its own extension
    datatypes. Examples from NumPy include `int8` (1 byte), `int64` (8 bytes—and note
    the lowercase “i”), `float16` (2 bytes), `float64` (8 bytes), and `bool` (1 byte).
    Additional types provided by Pandas include `categorical` and `datetimetz`. Externally,
    they appear to work similarly, but behind the scenes in the Pandas code base they
    cause a lot of type-specific Pandas code and duplication.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 幕后，Pandas 使用了 NumPy 数据类型和其自身的扩展数据类型的混合。来自 NumPy 的示例包括 `int8`（1 字节）、`int64`（8
    字节 — 注意小写的 “i”）、`float16`（2 字节）、`float64`（8 字节）和 `bool`（1 字节）。Pandas 提供的附加类型包括
    `categorical` 和 `datetimetz`。外观上，它们看起来工作方式类似，但在 Pandas 代码库的幕后，它们会引起大量特定于类型的 Pandas
    代码和重复。
- en: Note
  id: totrans-241
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Whilst Pandas originally used only `numpy` datatypes, it has evolved its own
    set of additional Pandas datatypes that understand missing data (NaN) behavior
    with three-valued logic. You must distinguish the `numpy` `int64`, which is not
    NaN-aware, from the Pandas `Int64`, which uses two columns of data behind the
    scenes for the integers and for the NaN bit mask. Note that the `numpy` `float64`
    is naturally NaN-aware.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Pandas最初只使用`numpy`数据类型，但它已经发展出了自己的一套额外的Pandas数据类型，可以理解缺失数据（NaN）的行为，具有三值逻辑。你必须区分`numpy`的`int64`（不支持NaN）和Pandas的`Int64`，后者在幕后使用两列数据来处理整数和NaN的掩码位。需要注意的是，`numpy`的`float64`天生支持NaN。
- en: One side effect of using NumPy’s datatypes has been that, while a `float` has
    a NaN (missing value) state, the same is not true for `int` and `bool` objects.
    If you introduce a NaN value into an `int` or `bool` Series in Pandas, your Series
    will be promoted to a `float`. Promoting `int` types to a `float` may reduce the
    numeric accuracy that can be represented in the same bits, and the smallest `float`
    is `float16`, which takes twice as many bytes as a `bool`.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 使用NumPy数据类型的一个副作用是，虽然`float`具有NaN（缺失值）状态，但对于`int`和`bool`对象并非如此。如果在Pandas的`int`或`bool`系列中引入NaN值，该系列将被提升为`float`。将`int`类型提升为`float`可能会降低可以在相同位数表示的数值精度，最小的`float`是`float16`，它的字节数是`bool`的两倍。
- en: The nullable `Int64` (note the capitalized “I”) was introduced in version 0.24
    as an extension type in Pandas. Internally, it uses a NumPy `int64` and a second
    Boolean array as a NaN-mask. Equivalents exist for `Int32` and `Int8`. As of Pandas
    version 1.0, there is also an equivalent nullable Boolean (with `dtype` `boolean`
    as opposed to the `numpy` `bool`, which isn’t NaN-aware). A `StringDType` has
    been introduced that may in the future offer higher performance and less memory
    usage than the standard Python `str`, which is stored in a column of `object`
    `dtype`.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 可空的`Int64`（注意大写的“I”）是Pandas 0.24版本中作为扩展类型引入的。在内部，它使用NumPy的`int64`和第二个布尔数组作为NaN掩码。对于`Int32`和`Int8`也有等价的类型。截至Pandas
    1.0版本，还引入了等效的可空布尔值（使用`dtype`为`boolean`，而不是`numpy`的`bool`，它不支持NaN）。引入了`StringDType`，可能在未来提供比标准Python
    `str`更高的性能和更少的内存使用，后者存储在`object` `dtype`列中。
- en: Applying a Function to Many Rows of Data
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对许多数据行应用函数
- en: It is very common to apply functions to rows of data in Pandas. There’s a selection
    of approaches, and the idiomatic Python approaches using loops are generally the
    slowest. We’ll work through an example based on a real-world challenge, showing
    different ways of solving this problem and ending with a reflection on the trade-off
    between speed and maintainability.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在Pandas中，对数据行应用函数是非常常见的。有多种方法可供选择，使用循环的Python惯用方法通常是最慢的。我们将通过一个基于真实挑战的示例来展示解决此问题的不同方法，并最终思考速度与可维护性之间的权衡。
- en: '*Ordinary Least Squares* (OLS) is a bread-and-butter method in data science
    for fitting a line to data. It solves for the slope and intercept in the `m *
    x + c` equation, given some data. This can be incredibly useful when trying to
    understand the trend of the data: is it generally increasing, or is it decreasing?'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '*普通最小二乘法*（OLS）是数据科学中拟合数据线性的基本方法。它解决了`m * x + c`方程中的斜率和截距，根据给定的数据。当试图理解数据趋势时，这非常有用：它通常是增加还是减少？'
- en: 'An example of its use from our work is a research project for a telecommunications
    company where we want to analyze a set of potential user-behavior signals (e.g.,
    marketing campaigns, demographics, and geographic behavior). The company has the
    number of hours a person spends on their cell phone every day, and its question
    is: is this person increasing or decreasing their usage, and how does this change
    over time?'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们工作中使用的一个示例是为一家电信公司的研究项目，我们希望分析一组潜在用户行为信号（例如市场营销活动、人口统计学和地理行为）。公司记录了每个人每天在手机上花费的小时数，并且问题是：这个人的使用是增加还是减少，以及这种变化随时间的推移如何？
- en: One way to approach this problem is to take the company’s large dataset of millions
    of users over years of data and break it into smaller windows of data (each window,
    for example, representing 14 days out of the years of data). For each window,
    we model the users’ use through OLS and record whether they are increasing or
    decreasing their usage.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 要解决这个问题的一种方法是将公司多年来数百万用户的大数据集分成较小的数据窗口（例如，每个窗口代表多年数据中的14天）。对于每个窗口，我们通过OLS建模用户的使用情况，并记录他们的使用是否增加或减少。
- en: In the end, we have a sequence for each user showing whether, for a given 14-day
    period, their use was generally increasing or decreasing. However, to get there,
    we have to run OLS a massive number of times!
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们为每个用户列出了一个序列，显示出在给定的14天期间内，他们的使用情况通常是增加还是减少。然而，要达到这一点，我们必须大量运行OLS！
- en: For one million users and two years of data, we might have 730 windows,^([12](ch06_split_001.xhtml#idm46122419819656))
    and thus 730,000,000 calls to OLS! To solve this problem practically, our OLS
    implementation should be fairly well tuned.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一百万用户和两年的数据，我们可能有730个窗口，^([12](ch06_split_001.xhtml#idm46122419819656)) 因此总共有730,000,000次OLS调用！为了实际解决这个问题，我们的OLS实现应该是相当精调的。
- en: In order to understand the performance of various OLS implementations, we will
    generate some smaller but representative synthetic data to give us good indications
    of what to expect on the larger dataset. We’ll generate data for 100,000 rows,
    each representing a synthetic user, and each containing 14 columns representing
    “hours used per day” for 14 days, as a continuous variable.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解各种OLS实现的性能，我们将生成一些较小但代表性的合成数据，以便给我们提供对更大数据集预期的良好指示。我们将为100,000行生成数据，每行代表一个合成用户，每行包含14列，“每天使用小时数”，作为连续变量。
- en: We’ll draw from a Poisson distribution (with `lambda==60` as minutes) and divide
    by 60 to give us simulated hours of usage as continuous values. The true nature
    of the random data doesn’t matter for this experiment; it is convenient to use
    a distribution that has a minimum value of 0 as this represents the real-world
    minimum. You can see a sample in [Example 6-23](ch06_split_001.xhtml#pandas_ols_data_snippet).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从泊松分布（`lambda==60`，单位为分钟）中抽取，并除以60得到模拟的使用小时数作为连续值。对于这个实验来说，随机数据的真实性并不重要；使用具有最小值为0的分布是方便的，因为这代表了真实世界的最小值。您可以在[示例 6-23](ch06_split_001.xhtml#pandas_ols_data_snippet)中看到一个样本。
- en: Example 6-23\. A snippet of our data
  id: totrans-254
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-23。我们数据的片段
- en: '[PRE25]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In [Figure 6-5](ch06_split_001.xhtml#pandas_random_hours_mobile_phone_usage_3_people),
    we see three rows of 14 days of synthetic data.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 6-5](ch06_split_001.xhtml#pandas_random_hours_mobile_phone_usage_3_people)中，我们看到了三行14天的合成数据。
- en: '![Pandas memory grouped into dtype-specific blocks](Images/hpp2_0605.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![Pandas内存分组为特定数据类型的块](Images/hpp2_0605.png)'
- en: Figure 6-5\. Synthetic data for the first three simulated users showing 14 days
    of cell phone usage
  id: totrans-258
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-5。前三个模拟用户的合成数据，显示了14天的手机使用情况
- en: A bonus of generating 100,000 rows of data is that some rows will, by random
    variation alone, exhibit “increasing counts,” and some will exhibit “decreasing
    counts.” Note that there is no signal behind this in our synthetic data since
    the points are drawn independently; simply because we generate many rows of data,
    we’re going to see a variance in the ultimate slopes of the lines we calculate.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 生成100,000行数据的奖励是，仅仅因为随机变化，某些行将表现出“计数增加”，而某些行将表现出“计数减少”。请注意，我们的合成数据中没有这背后的信号，因为这些点是独立绘制的；仅仅因为我们生成了许多数据行，我们将看到我们计算的线的最终斜率存在差异。
- en: This is convenient, as we can identify the “most growing” and “most declining”
    lines and draw them as a validation that we’re identifying the sort of signal
    we hope to export on the real-world problem. [Figure 6-6](ch06_split_001.xhtml#pandas_random_hours_mobile_min_max_slopes)
    shows two of our random traces with maximal and minimal slopes (`m`).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这很方便，因为我们可以识别出“增长最多”和“下降最多”的线，并将它们绘制出来，以验证我们是否正在识别出希望在真实世界问题中导出的信号。[图 6-6](ch06_split_001.xhtml#pandas_random_hours_mobile_min_max_slopes)展示了我们两条随机轨迹的最大和最小斜率（`m`）。
- en: '![Pandas memory grouped into dtype-specific blocks](Images/hpp2_0606.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![Pandas内存分组为特定数据类型的块](Images/hpp2_0606.png)'
- en: Figure 6-6\. The “most increasing” and “most decreasing” usage in our randomly
    generated dataset
  id: totrans-262
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-6。我们随机生成的数据集中“增长最多”和“下降最多”的使用情况
- en: We’ll start with scikit-learn’s `LinearRegression` estimator to calculate each
    `m`. While this method is correct, we’ll see in the following section that it
    incurs a surprising overhead against another approach.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从scikit-learn的`LinearRegression`估算器开始计算每个`m`。虽然这种方法是正确的，但我们将在接下来的部分看到，与另一种方法相比，它会产生意外的开销。
- en: Which OLS implementation should we use?
  id: totrans-264
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们应该使用哪种OLS实现？
- en: '[Example 6-24](ch06_split_001.xhtml#pandas_ols_functions) shows three implementations
    that we’d like to try. We’ll evaluate the scikit-learn implementation against
    the direct linear algebra implementation using NumPy. Both methods ultimately
    perform the same job and calculate the slopes (`m`) and intercept (`c`) of the
    target data from each Pandas row given an increasing `x` range (with values [0,
    1, …, 13]).'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 6-24](ch06_split_001.xhtml#pandas_ols_functions)展示了我们想尝试的三种实现。我们将评估scikit-learn实现与直接使用NumPy的线性代数实现。这两种方法最终执行相同的工作，并计算每个Pandas行的目标数据的斜率(`m`)和截距(`c`)，给定一个增加的`x`范围（值为[0,
    1, ..., 13]）。'
- en: scikit-learn will be a default choice for many machine learning practitioners,
    while a linear algebra solution may be preferred by those coming from other disciplines.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多机器学习从业者来说，scikit-learn将是默认选择，而线性代数解决方案可能会受到来自其他学科背景人员的青睐。
- en: Example 6-24\. Solving Ordinary Least Squares with NumPy and scikit-learn
  id: totrans-267
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-24. 使用NumPy和scikit-learn解决普通最小二乘法
- en: '[PRE26]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Surprisingly, if we call `ols_sklearn` 10,000 times with the `timeit` module,
    we find that it takes at least 0.483 microseconds to execute, while `ols_lstsq`
    on the same data takes 0.182 microseconds. The popular scikit-learn solution takes
    more than twice as long as the terse NumPy variant!
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，如果我们使用`timeit`模块对`ols_sklearn`进行10000次调用，在相同的数据上，执行时间至少为0.483微秒，而在相同数据上，`ols_lstsq`只需0.182微秒。流行的scikit-learn解决方案花费的时间超过了简洁的NumPy变体的两倍！
- en: Building on the profiling from [“Using line_profiler for Line-by-Line Measurements”](ch02.xhtml#profiling-line-profiler),
    we can use the object interface (rather than the command line or Jupyter magic
    interfaces) to learn *why* the scikit-learn implementation is slower. In [Example 6-25](ch06_split_001.xhtml#pandas_profiling_sklearn_ols),
    we tell LineProfiler to profile `est.fit` (that’s the scikit-learn `fit` method
    on our `LinearRegression` estimator) and then call `run` with arguments based
    on the DataFrame we used before.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 基于来自[“使用`line_profiler`进行逐行测量”](ch02.xhtml#profiling-line-profiler)的分析，我们可以使用对象接口（而不是命令行或Jupyter魔术界面）来了解scikit-learn实现较慢的原因。在[示例
    6-25](ch06_split_001.xhtml#pandas_profiling_sklearn_ols)中，我们告诉`LineProfiler`分析`est.fit`（这是我们的`LinearRegression`估计器上的scikit-learn
    `fit`方法），然后根据之前使用的DataFrame调用`run`方法。
- en: We see a couple of surprises. The very last line of `fit` calls the same `linalg.lstsq`
    that we’ve called in `ols_lstsq`—so what else is going on to cause our slowdown?
    `LineProfiler` reveals that scikit-learn is calling two other expensive methods,
    namely `check_X_y` and `_preprocess_data`.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到了一些意外情况。`fit`的最后一行调用了与我们在`ols_lstsq`中调用的相同的`linalg.lstsq`，那么是什么导致我们的速度变慢？`LineProfiler`显示scikit-learn在调用另外两个昂贵的方法，即`check_X_y`和`_preprocess_data`。
- en: Both of these are designed to help us avoid making mistakes—indeed, your author
    Ian has been saved repeatedly from passing in inappropriate data such as a wrongly
    shaped array or one containing NaNs to scikit-learn estimators. A consequence
    of this checking is that it takes more time—more safety makes things run slower!
    We’re trading developer time (and sanity) against execution time.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法都旨在帮助我们避免犯错——确实，您的作者Ian已多次因将不合适的数据（如形状错误的数组或包含NaN的数组）传递给scikit-learn估计器而受到拯救。这种检查的结果是需要更多时间——更多的安全性导致运行速度变慢！我们在开发者时间（和理智）与执行时间之间进行权衡。
- en: Example 6-25\. Digging into scikit-learn’s `LinearRegression.fit` call
  id: totrans-273
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-25. 探索scikit-learn的`LinearRegression.fit`调用
- en: '[PRE27]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Behind the scenes, these two methods are performing various checks, including
    these:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，这两种方法都在执行各种检查，包括以下内容：
- en: Checking for appropriate sparse NumPy arrays (even though we’re using dense
    arrays in this example)
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查适当的稀疏NumPy数组（尽管在此示例中我们使用的是密集数组）
- en: Offsetting the input array to a mean of 0 to improve numerical stability on
    wider data ranges than we’re using
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将输入数组的偏移量设置为0的平均值，以改善比我们使用的更宽的数据范围的数值稳定性
- en: Checking that we’re providing a 2D X array
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查我们提供了一个2D的X数组
- en: Checking that we’re not providing NaN or Inf values
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查我们不提供NaN或Inf值
- en: Checking that we’re providing non-empty arrays of data
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查我们提供的数据数组不为空
- en: Generally, we prefer to have all of these checks enabled—they’re here to help
    us avoid painful debugging sessions, which kill developer productivity. If we
    know that our data is of the correct form for our chosen algorithm, these checks
    will add a penalty. It is up to you to decide when the safety of these methods
    is hurting your overall productivity.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们更倾向于启用所有这些检查——它们帮助我们避免痛苦的调试过程，这会降低开发者的生产力。如果我们知道我们的数据对于所选算法是正确的形式，这些检查将会增加负担。你需要决定，当这些方法的安全性影响整体生产力时。
- en: As a general rule—stay with the safer implementations (scikit-learn, in this
    case) *unless* you’re confident that your data is in the right form and you’re
    optimizing for performance. We’re after increased performance, so we’ll continue
    with the `ols_lstsq` approach.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一般规则——在这种情况下，留在更安全的实现方式（例如 scikit-learn），*除非*你确信你的数据是正确的形式，并且你正在优化性能。我们追求更高的性能，因此我们将继续使用
    `ols_lstsq` 方法。
- en: Applying lstsq to our rows of data
  id: totrans-283
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将 lstsq 应用到我们的数据行
- en: We’ll start with an approach that many Python developers who come from other
    programming languages may try. This is *not* idiomatic Python, nor is it common
    or even efficient for Pandas. It does have the advantage of being very easy to
    understand. In [Example 6-26](ch06_split_001.xhtml#pandas_calc_ols_iloc), we’ll
    iterate over the index of the DataFrame from row 0 to row 99,999; on each iteration
    we’ll use `iloc` to retrieve a row, and then we’ll calculate OLS on that row.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从许多来自其他编程语言的 Python 开发人员常用的方法开始。这 *不是* Python 的习惯用法，也不是 Pandas 中常见或高效的方法。它的优点是非常易于理解。在
    [示例 6-26](ch06_split_001.xhtml#pandas_calc_ols_iloc) 中，我们将迭代 DataFrame 的索引从行 0
    到行 99,999；在每次迭代中，我们将使用 `iloc` 检索一行，然后计算该行的 OLS。
- en: The calculation is common to each of the following methods—what’s different
    is how we iterate over the rows. This method takes 18.6 seconds; it is by far
    the slowest approach (by a factor of 3) among the options we’re evaluating.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 计算对于以下每种方法都是常见的——不同的是我们如何遍历行。这种方法需要 18.6 秒；它是我们评估选项中最慢的方法（慢了 3 倍）。
- en: Behind the scenes, each dereference is expensive—`iloc` does a lot of work to
    get to the row using a fresh `row_idx`, which is then converted into a new Series
    object, which is returned and assigned to `row`.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，每次解引用都是昂贵的——`iloc` 通过使用新的 `row_idx` 很多工作来获取行，然后将其转换为新的 Series 对象，返回并赋值给
    `row`。
- en: Example 6-26\. Our worst implementation—counting and fetching rows one at a
    time with `iloc`
  id: totrans-287
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-26\. 我们最糟糕的实现方式——逐行计数和抓取，使用 `iloc`
- en: '[PRE28]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Next, we’ll take a more idiomatic Python approach: in [Example 6-27](ch06_split_001.xhtml#pandas_cals_ols_iterrows),
    we iterate over the rows using `iterrows`, which looks similar to how we might
    iterate over a Python iterable (e.g., a `list` or `set`) with a `for` loop. This
    method looks sensible and is a little faster—it takes 12.4 seconds.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将采用更符合 Python 习惯的方法：在 [示例 6-27](ch06_split_001.xhtml#pandas_cals_ols_iterrows)
    中，我们使用 `iterrows` 迭代行，这看起来类似于我们如何使用 `for` 循环迭代 Python 可迭代对象（例如 `list` 或 `set`）。这种方法看起来合理，而且稍快——花费
    12.4 秒。
- en: This is more efficient, as we don’t have to do so many lookups—`iterrows` can
    walk along the rows without doing lots of sequential lookups. `row` is still created
    as a fresh Series on each iteration of the loop.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 这更高效，因为我们不必进行如此多的查找——`iterrows` 可以沿着行行走，而不进行大量的顺序查找。`row` 仍然在每次循环迭代中作为新的 Series
    创建。
- en: Example 6-27\. `iterrows` for more efficient and “Python-like” row operations
  id: totrans-291
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-27\. `iterrows` 用于更高效和“Python式”的行操作
- en: '[PRE29]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[Example 6-28](ch06_split_001.xhtml#pandas_calc_ols_apply) skips a lot of the
    Pandas machinery, so a lot of overhead is avoided. `apply` passes the function
    `ols_lstsq` a new row of data directly (again, a fresh Series is constructed behind
    the scenes for each row) without creating Python intermediate references. This
    costs 6.8 seconds—this is a significant improvement, and the code is more compact
    and readable!'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 6-28](ch06_split_001.xhtml#pandas_calc_ols_apply) 跳过了大量的 Pandas 机制，因此避免了大量的开销。`apply`
    直接将函数 `ols_lstsq` 传递给新的数据行（再次，在幕后为每行构造了新的 Series），而不创建 Python 中间引用。这需要 6.8 秒——这是一个显著的改进，并且代码更简洁易读！'
- en: Example 6-28\. `apply` for idiomatic Pandas function application
  id: totrans-294
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-28\. `apply` 用于习惯性的 Pandas 函数应用
- en: '[PRE30]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Our final variant in [Example 6-29](ch06_split_001.xhtml#pandas_calc_ols_apply_raw)
    uses the same `apply` call with an additional `raw=True` argument. Using `raw=True`
    stops the creation of an intermediate Series object. As we don’t have a Series
    object, we have to use our third OLS function, `ols_lstsq_raw`; this variant has
    direct access to the underlying NumPy array.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 [示例 6-29](ch06_split_001.xhtml#pandas_calc_ols_apply_raw) 中的最终变体使用了相同的 `apply`
    调用，加上额外的 `raw=True` 参数。使用 `raw=True` 可以阻止中间 Series 对象的创建。由于我们没有 Series 对象，我们必须使用我们的第三个
    OLS 函数 `ols_lstsq_raw`；这个变体直接访问底层的 NumPy 数组。
- en: By avoiding the creation and dereferencing of an intermediate Series object,
    we shave our execution time a little more, down to 5.3 seconds.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 避免创建和解除引用中间的 Series 对象，我们将执行时间再次减少，降至 5.3 秒。
- en: Example 6-29\. Avoiding intermediate Series creation using `raw=True`
  id: totrans-298
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-29\. 使用 `raw=True` 避免中间 Series 对象的创建
- en: '[PRE31]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The use of `raw=True` gives us the option to compile with Numba ([“Numba to
    Compile NumPy for Pandas”](ch07.xhtml#compiling-numba-for-pandas)) or with Cython
    as it removes the complication of compiling Pandas layers that currently aren’t
    supported.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `raw=True` 选项使我们有选择地使用 Numba（[“Numba 为 Pandas 编译 NumPy”](ch07.xhtml#compiling-numba-for-pandas)）或
    Cython 进行编译，因为它消除了编译 Pandas 层的复杂性，这些层目前不受支持。
- en: We’ll summarize the execution times in [Table 6-3](ch06_split_001.xhtml#pandas_table_iteration_costs)
    for 100,000 rows of data on a single window of 14 columns of simulated data. New
    Pandas users often use `iloc` and `iterrows` (or the similar `itertuples`) when
    `apply` would be preferred.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在 [表 6-3](ch06_split_001.xhtml#pandas_table_iteration_costs) 中总结对于单个窗口的 14
    列模拟数据的 100,000 行数据的执行时间。新的 Pandas 用户经常在更喜欢使用 `apply` 时使用 `iloc` 和 `iterrows`（或类似的
    `itertuples`）。
- en: By performing our analysis and considering our potential need to perform OLS
    on 1,000,000 rows by up to 730 windows of data, we can see that a first naive
    approach combining `iloc` with `ols_sklearn` might cost 10 (our larger dataset
    factor) * 730 * 18 seconds * 2 (our slowdown factor against `ols_lstsq`) == 73
    hours.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 通过进行分析并考虑我们可能需要对 1,000,000 行数据进行 OLS 分析，最多达到 730 个数据窗口，我们可以看到，第一次朴素的方法结合 `iloc`
    和 `ols_sklearn` 可能会花费 10（我们更大的数据集系数）* 730 * 18 秒 * 2（我们相对于 `ols_lstsq` 的减速系数）==
    73 小时。
- en: If we used `ols_lstsq_raw` and our fastest approach, the same calculations might
    take 10 * 730 * 5.3 seconds == 10 hours. This is a significant saving for a task
    that represents what might be a suite of similar operations. We’ll see even faster
    solutions if we compile and run on multiple cores.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用 `ols_lstsq_raw` 和我们最快的方法，相同的计算可能需要 10 * 730 * 5.3 秒 == 10 小时。对于一个可能代表一套类似操作的任务来说，这是一个显著的节省。如果我们编译并在多个核心上运行，我们将看到更快的解决方案。
- en: Table 6-3\. Cost for using `lstsq` with various Pandas row-wise approaches
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6-3\. 使用不同的 Pandas 逐行方法与 `lstsq` 的成本
- en: '| Method | Time in seconds |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 时间（秒） |'
- en: '| --- | --- |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| iloc | 18.6 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| iloc | 18.6 |'
- en: '| iterrows | 12.4 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| iterrows | 12.4 |'
- en: '| apply | 6.8 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 应用 | 6.8 |'
- en: '| apply raw=True | 5.3 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 应用 raw=True | 5.3 |'
- en: Earlier we discovered that the scikit-learn approach adds significant overhead
    to our execution time by covering our data with a safety net of checks. We can
    remove this safety net but with a potential cost on developer debugging time.
    Your authors strongly urge you to consider adding unit-tests to your code that
    would verify that a well-known and well-debugged method is used to test any optimized
    method you settle on. If you added a unit test to compare the scikit-learn `LinearRegression`
    approach against `ols_lstsq`, you’d be giving yourself and other colleagues a
    future hint about why you developed a less obvious solution to what appeared to
    be a standard problem.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 早些时候，我们发现 scikit-learn 方法会在执行时间上增加显着的开销，因为它通过一系列检查来覆盖我们的数据。我们可以去掉这个安全网，但这可能会增加开发人员调试时间的成本。你的作者强烈建议你考虑向你的代码添加单元测试，以验证是否使用了一个众所周知且调试良好的方法来测试你选择的任何优化方法。如果你添加了一个单元测试来比较
    scikit-learn 的 `LinearRegression` 方法和 `ols_lstsq`，你将为自己和其他同事提供一个关于为什么对一个看似标准的问题开发一个不太明显的解决方案的未来提示。
- en: Having experimented, you may also conclude that the heavily tested scikit-learn
    approach is more than fast enough for your application and that you’re more comfortable
    using a library that is well known by other developers. This could be a very sane
    conclusion.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行了实验后，你可能还会得出一个结论，即经过充分测试的 scikit-learn 方法对于你的应用来说已经足够快了，而且你更乐意使用其他开发人员熟悉的库。这可能是一个非常明智的结论。
- en: Later in [“Parallel Pandas with Dask”](ch10.xhtml#clustering-dask), we’ll look
    at running Pandas operations across multiple cores by dividing data into groups
    of rows using Dask and Swifter. In [“Numba to Compile NumPy for Pandas”](ch07.xhtml#compiling-numba-for-pandas),
    we look at compiling the `raw=True` variant of `apply` to achieve an order of
    magnitude speedup. Compilation and parallelization can be combined for a really
    significant final speedup, dropping our expected runtime from around 10 hours
    to just 30 minutes.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在[“使用Dask进行并行Pandas”](ch10.xhtml#clustering-dask)的稍后部分，我们将通过使用Dask和Swifter将Pandas操作跨多个核心进行分组的数据。在[“用于Pandas的Numba编译”](ch07.xhtml#compiling-numba-for-pandas)中，我们研究了编译`apply`的`raw=True`变体，以实现数量级的加速。编译和并行化可以结合起来，实现显著的最终加速，将我们预期的运行时间从大约10小时降低到只有30分钟。
- en: Building DataFrames and Series from Partial Results Rather than Concatenating
  id: totrans-314
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从部分结果构建数据框架和系列，而不是简单地连接它们。
- en: You may have wondered in [Example 6-26](ch06_split_001.xhtml#pandas_calc_ols_iloc)
    why we built up a list of partial results that we then turned into a Series, rather
    than incrementally building up the Series as we went. Our earlier approach required
    building up a list (which has a memory overhead) and then building a *second*
    structure for the Series, giving us two objects in memory. This brings us to another
    common mistake when using Pandas and NumPy.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例 6-26](ch06_split_001.xhtml#pandas_calc_ols_iloc)中，您可能会想知道为什么我们要建立一个部分结果列表，然后将其转换为系列，而不是逐步建立系列。我们先前的方法需要建立一个列表（具有内存开销），然后为系列建立*第二*结构，使我们在内存中有两个对象。这将我们带到使用Pandas和NumPy时的另一个常见错误。
- en: As a general rule, you should avoid repeated calls to `concat` in Pandas (and
    to the equivalent `concatenate` in NumPy). In [Example 6-30](ch06_split_001.xhtml#pandas_calc_ols_many_concatenates),
    we see a similar solution to the preceding one but without the intermediate `ms`
    list. This solution takes 56 seconds, as opposed to the solution using a list
    at 18.6 seconds!
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，您应该避免在Pandas中重复调用`concat`（以及在NumPy中的等效`concatenate`）。在[示例 6-30](ch06_split_001.xhtml#pandas_calc_ols_many_concatenates)中，我们看到与前述解决方案类似的解决方案，但没有中间的`ms`列表。此解决方案花费了56秒，而使用列表的解决方案则为18.6秒！
- en: Example 6-30\. Concatenating each result incurs a significant overhead—avoid
    this!
  id: totrans-317
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-30\. 每次连接都会带来显著的开销——要避免这种情况！
- en: '[PRE32]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Each concatenation creates an entirely *new* Series object in a new section
    of memory that is one row longer than the previous item. In addition, we have
    to make a temporary Series object for each new `m` on each iteration. We strongly
    recommend building up lists of intermediate results and then constructing a Series
    or DataFrame from this list, rather than concatenating to an existing object.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 每次连接都会创建一个全新的*新*系列对象，存储在内存的新部分，比前一个项目多一行。此外，我们必须为每个迭代的新`m`创建一个临时系列对象。我们强烈建议先建立中间结果列表，然后从此列表构建系列或数据框架，而不是连接到现有对象。
- en: There’s More Than One (and Possibly a Faster) Way to Do a Job
  id: totrans-320
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有多种（可能还有更快的）完成工作的方法。
- en: Because of the evolution of Pandas, there are typically a couple of approaches
    to solving the same task, some of which incur more overhead than others. Let’s
    take the OLS DataFrame and convert one column into a string; we’ll then time some
    string operations. With string-based columns containing names, product identifiers,
    or codes, it is common to have to preprocess the data to turn it into something
    we can analyze.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Pandas的演变，通常有几种解决同一任务的方法，其中一些方法比其他方法更耗费资源。让我们拿OLS数据框架并将一个列转换为字符串；然后我们将计时一些字符串操作。对于包含名称、产品标识符或代码的基于字符串的列，通常需要预处理数据，将其转换为可以分析的内容。
- en: Let’s say that we need to find the location, if it exists, of the number 9 in
    the digits from one of the columns. While this operation serves no real purpose,
    it is very similar to checking for the presence of a code-bearing symbol in an
    identifier’s sequence or checking for an honorific in a name. Typically for these
    operations, we’d use `strip` to remove extraneous whitespace, `lower` and `replace`
    to normalize the string, and `find` to locate something of interest.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们需要找到某列中数字9的位置（如果存在）。尽管此操作没有真正的目的，但它与检查标识符序列中是否存在带代码的符号或检查名称中的尊称非常相似。通常情况下，对于这些操作，我们将使用`strip`去除多余的空格，`lower`和`replace`来规范化字符串，并使用`find`来定位感兴趣的内容。
- en: In [Example 6-31](ch06_split_001.xhtml#pandas_calc_ols_str_operations), we first
    build a new Series named `0_as_str`, which is the zeroth Series of random numbers
    converted into a printable string form. We’ll then run two variants of string
    manipulation code—both will remove the leading digit and decimal point and then
    use Python’s `find` to locate the first `9` if it exists, returning –1 otherwise.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [示例 6-31](ch06_split_001.xhtml#pandas_calc_ols_str_operations) 中，我们首先构建了一个名为
    `0_as_str` 的新 Series，它是将第零个随机数序列转换为可打印的字符串形式。然后我们将运行两种字符串操作的变体——两者都将移除开头的数字和小数点，然后使用
    Python 的 `find` 方法来查找第一个 `9`，如果不存在则返回 -1。
- en: Example 6-31\. `str` Series operations versus `apply` for string processing
  id: totrans-324
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-31\. `str` Series 操作与字符串处理中的 `apply` 方法的比较
- en: '[PRE33]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The one-line approach uses Pandas’s `str` operations to access Python’s string
    methods for a Series. For `split`, we expand the returned result into two columns
    (the first column contains the leading digit, and the second contains everything
    after the decimal place), and we select column 1\. We then apply `find` to locate
    the digit 9\. The second approach uses `apply` and the function `find_9`, which
    reads like a regular Python string-processing function.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 单行方法使用 Pandas 的 `str` 操作来访问 Python 的字符串方法，应用于 Series。对于 `split` 操作，我们将返回的结果扩展为两列（第一列包含前导数字，第二列包含小数点后的所有内容），然后选择第二列。然后我们应用
    `find` 来定位数字 9。第二种方法使用 `apply` 和函数 `find_9`，其读起来像是常规的 Python 字符串处理函数。
- en: We can use `%timeit` to check the runtime—this shows us that there’s a 3.5×
    speed difference between the two methods, even though they both produce the same
    result! In the former one-line case, Pandas has to make several new intermediate
    Series objects, which adds overhead; in the `find_9` case, all of the string-processing
    work occurs one line at a time without creating new intermediate Pandas objects.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `%timeit` 来检查运行时间——这显示出两种方法之间存在 3.5 倍的速度差异，尽管它们都产生相同的结果！在前一种单行情况下，Pandas
    必须创建几个新的中间 Series 对象，这增加了开销；在 `find_9` 情况下，所有字符串处理工作都是逐行进行，而不创建新的中间 Pandas 对象。
- en: Further benefits of the `apply` approach are that we could parallelize this
    operation (see [“Parallel Pandas with Dask”](ch10.xhtml#clustering-dask) for an
    example with Dask and Swifter), and we can write a unit test that succinctly confirms
    the operations performed by `find_9`, which will aid in readability and maintenance.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '`apply` 方法的进一步好处是我们可以并行化此操作（请参阅 [“使用 Dask 和 Swifter 进行并行 Pandas”](ch10.xhtml#clustering-dask)
    作为示例），并且我们可以编写一个单元测试来简洁地确认 `find_9` 执行的操作，这将有助于可读性和维护性。'
- en: Advice for Effective Pandas Development
  id: totrans-329
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有效使用 Pandas 的建议
- en: Install the optional dependencies `numexpr` and `bottleneck` for additional
    performance improvements. These don’t get installed by default, and you won’t
    be told if they’re missing. `bottleneck` is rarely used in the code base; `numexpr`,
    however, will give significant speedups in some situations when you use `exec`.
    You can test for the presence of both in your environment with `import bottleneck`
    and `import numexpr`.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 安装可选的依赖项 `numexpr` 和 `bottleneck` 可以提高性能。这些不会默认安装，并且如果缺少它们，你将得不到提示。`bottleneck`
    在代码库中很少使用；然而，`numexpr` 在使用 `exec` 时，在某些情况下会显著加快速度。你可以通过在环境中导入 `import bottleneck`
    和 `import numexpr` 来测试它们的存在。
- en: Don’t write your code too tersely; remember to make your code easy to read and
    debug to help your future self. While the “method chaining” style is supported,
    your authors would caution against chaining too many rows of Pandas operations
    in sequence. It typically becomes difficult to figure out which line has problems
    when debugging, and then you have to split up the lines—you’re better off chaining
    only a couple of operations together at most to simplify your maintenance.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 不要将代码写得太简洁；记住要使代码易于阅读和调试，以帮助你的未来自己。虽然“方法链”风格得到支持，但我们建议不要在序列中链接太多行的 Pandas 操作。通常在调试时很难找出哪一行有问题，然后你不得不拆分这些行——最好是最多只链接几个操作来简化维护。
- en: 'Avoid doing more work than necessary: it is preferable to filter your data
    before calculating on the remaining rows rather than filtering after calculating,
    if possible. For high performance in general, we want to ask the machine to do
    as little computation as possible; if you can filter out or mask away portions
    of your data, you’re probably winning. If you’re consuming from a SQL source and
    later joining or filtering in Pandas, you might want to try to filter first at
    the SQL level, to avoid pulling more data than necessary into Pandas. You may
    *not* want to do this at first if you’re investigating data quality, as having
    a simplified view on the variety of datatypes you have might be more beneficial.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 避免做比必要更多的工作：最好是在计算剩余行之前过滤您的数据，而不是在计算后过滤。一般来说，为了高性能，我们希望机器尽可能少地进行计算；如果您能够过滤或遮蔽掉数据的部分，那么您可能会胜出。如果您从
    SQL 源获取数据，然后在 Pandas 中进行连接或过滤，您可能希望首先在 SQL 层面进行过滤，以避免将不必要的数据拉入 Pandas。如果您正在调查数据质量，则可能不希望在最初就这样做，因为对您拥有的各种数据类型进行简化视图可能更有益。
- en: Check the schema of your DataFrames as they evolve; with a tool like `bulwark`,
    you guarantee at runtime that your schema is being met, and you can visually confirm
    when you’re reviewing code that your expectations are being met. Keep renaming
    your columns as you generate new results so that your DataFrame’s contents make
    sense to you; sometimes `groupby` and other operations give you silly default
    names, which can later be confusing. Drop columns that you no longer need with
    `.drop()` to reduce bloat and memory usage.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 DataFrames 的演变，请检查它们的模式；使用像`bulwark`这样的工具，可以在运行时保证模式的符合，并在检查代码时视觉确认您的预期得到满足。在生成新结果时继续重命名您的列，以便您的
    DataFrame 内容对您有意义；有时`groupby`和其他操作会给出愚蠢的默认名称，这可能会在以后造成困惑。使用`.drop()`删除不再需要的列以减少臃肿和内存使用。
- en: For large Series containing strings with low cardinality (“yes” and “no,” for
    example, or “type_a,” “type_b,” and “type_c”), try converting the Series to a
    Category `dtype` with `df['series_of_strings'].astype('category')`; you may find
    that operations like `value_counts` and `groupby` run faster, and the Series is
    likely to consume less RAM.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 对于包含低基数字符串的大 Series（例如“yes”和“no”，或“type_a”、“type_b”和“type_c”），尝试将 Series 转换为分类`dtype`，使用`df['series_of_strings'].astype('category')`；您可能会发现像`value_counts`和`groupby`这样的操作运行得更快，而且
    Series 可能会消耗更少的 RAM。
- en: Similarly, you may want to convert 8-byte `float64` and `int64` columns to smaller
    datatypes—perhaps the 2-byte `float16` or 1-byte `int8` if you need a smaller
    range to further save RAM.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，你可能想要将 8 字节的`float64`和`int64`列转换为更小的数据类型——也许是 2 字节的`float16`或者 1 字节的`int8`，如果你需要更小的范围以进一步节省
    RAM。
- en: As you evolve DataFrames and generate new copies, remember that you can use
    the `del` keyword to delete earlier references and clear them from memory, if
    they’re large and wasting space. You can also use the Pandas `drop` method to
    delete unused columns.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在演变 DataFrames 并生成新副本时，请记住可以使用`del`关键字删除早期的引用并从内存中清除，如果它们很大且浪费空间。您也可以使用 Pandas
    的`drop`方法删除未使用的列。
- en: If you’re manipulating large DataFrames while you prepare your data for processing,
    it may make sense to do these operations once in a function or a separate script
    and then persist the prepared version to disk by using `to_pickle`. You can subsequently
    work on the prepared DataFrame without having to process it each time.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在准备数据进行处理时操作大型 DataFrames，可能最好是在函数或单独的脚本中执行这些操作一次，然后使用`to_pickle`将准备好的版本持久化到磁盘上。之后您可以在准备好的
    DataFrame 上进行后续工作，而无需每次都处理它。
- en: Avoid the `inplace=True` operator—in-place operations are scheduled to be removed
    from the library over time.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 避免使用`inplace=True`操作符——逐步删除库中的原位操作。
- en: Finally, always add unit tests to any processing code, as it will quickly become
    more complex and harder to debug. Developing your tests up front guarantees that
    your code meets your expectations and helps you to avoid silly mistakes creeping
    in later that cost developer time to debug.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请始终为任何处理代码添加单元测试，因为代码很快就会变得更复杂且更难调试。在开始开发测试时就可以保证您的代码符合预期，并帮助您避免稍后出现的愚蠢错误，这些错误会消耗开发人员的调试时间。
- en: Existing tools for making Pandas go faster include [Modin](https://pypi.org/project/modin)
    and the GPU-focused [cuDF](https://pypi.org/project/cudf). Modin and cuDF take
    different approaches to parallelizing common data operations on a Pandas DataFrame–like
    object.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 加速Pandas的现有工具包括[Modin](https://pypi.org/project/modin)和专注于GPU的[cuDF](https://pypi.org/project/cudf)。Modin和cuDF采用不同的方法来并行化在类似于Pandas
    DataFrame的对象上的常见数据操作。
- en: We’d like to also give an honorable mention to the new [Vaex library](https://github.com/vaexio/vaex).
    Vaex is designed to work on very large datasets that exceed RAM by using lazy
    evaluation while retaining a similar interface to that of Pandas. In addition,
    Vaex offers a slew of built-in visualization functions. One design goal is to
    use as many CPUs as possible, offering parallelism for free where possible.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也想对新的[Vaex库](https://github.com/vaexio/vaex)表示敬意。Vaex专为处理超出RAM范围的大型数据集而设计，通过惰性评估保留了与Pandas类似的接口。此外，Vaex提供了大量内置的可视化函数。一个设计目标是尽可能多地利用CPU，尽可能提供并行性。
- en: Vaex specializes in both larger datasets and string-heavy operations; the authors
    have rewritten many of the string operations to avoid the standard Python functions
    and instead use faster Vaex implementations in `C++`. Note that Vaex is not guaranteed
    to work in the same way as Pandas, so it is possible that you’ll find edge cases
    with different behavior—as ever, back your code with unit tests to gain confidence
    if you’re trying both Pandas and Vaex to process the same data.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: Vaex专注于处理更大的数据集和字符串密集型操作；作者已经重写了许多字符串操作，避免使用标准的Python函数，而是使用更快的Vaex的`C++`实现。请注意，Vaex不能保证与Pandas完全相同的工作方式，因此可能会在不同行为的边缘情况下找到问题——无论如何，如果您尝试使用Pandas和Vaex处理相同的数据，请通过单元测试来确保代码的可靠性。
- en: Wrap-Up
  id: totrans-343
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In the next chapter, we will talk about how to create your own external modules
    that can be finely tuned to solve specific problems with much greater efficiencies.
    This allows us to follow the rapid prototyping method of making our programs—first
    solve the problem with slow code, then identify the elements that are slow, and
    finally, find ways to make those elements faster. By profiling often and trying
    to optimize only the sections of code we *know* are slow, we can save ourselves
    time while still making our programs run as fast as possible.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论如何创建自己的外部模块，以便更好地解决特定问题并提高效率。这使我们能够遵循快速原型制作程序的方法——首先用慢速代码解决问题，然后识别慢的元素，最后找到使这些元素更快的方法。通过频繁进行性能分析并仅优化*已知*慢的代码部分，我们可以节省时间，同时使程序尽可能快地运行。
- en: ^([1](ch06_split_000.xhtml#idm46122422174440-marker)) This is the code from
    [Example 6-3](ch06_split_000.xhtml#matrix_pure_python), truncated to fit within
    the page margins. Recall that `kernprof` requires functions to be decorated with
    `@profile` in order to be profiled (see [“Using line_profiler for Line-by-Line
    Measurements”](ch02.xhtml#profiling-line-profiler)).
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch06_split_000.xhtml#idm46122422174440-marker)) 这是来自[示例 6-3](ch06_split_000.xhtml#matrix_pure_python)的代码，经过截断以适应页面边距。请记住，`kernprof`要求函数在进行性能分析时必须使用`@profile`进行装饰（参见[“使用line_profiler进行逐行测量”](ch02.xhtml#profiling-line-profiler)）。
- en: ^([2](ch06_split_000.xhtml#idm46122422123864-marker)) The code profiled in [Example 6-7](ch06_split_000.xhtml#matrix_pure_python_mem_lineprof)
    is the code from [Example 6-6](ch06_split_000.xhtml#matrix_pure_python_memory);
    it has been truncated to fit within the page margins.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch06_split_000.xhtml#idm46122422123864-marker)) 在[示例 6-7](ch06_split_000.xhtml#matrix_pure_python_mem_lineprof)中分析的代码是来自[示例 6-6](ch06_split_000.xhtml#matrix_pure_python_memory)的代码，经过截断以适应页面边距。
- en: ^([3](ch06_split_000.xhtml#idm46122420832696-marker)) On macOS you can get similar
    metrics by using Google’s [`gperftools`](https://oreil.ly/MCCVv) and the provided
    `Instruments` app. For Windows, we are told Visual Studio Profiler works well;
    however, we don’t have experience with it.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch06_split_000.xhtml#idm46122420832696-marker)) 在macOS上，您可以使用Google的[`gperftools`](https://oreil.ly/MCCVv)和提供的`Instruments`应用程序获得类似的度量。对于Windows，我们听说Visual
    Studio Profiler工作效果很好；但是我们没有使用经验。
- en: ^([4](ch06_split_000.xhtml#idm46122421125416-marker)) This can be done by running
    the Python process through the `nice` utility (`nice -n –20 python` `program.py`).
    A nice value of -20 will make sure it yields execution as little as possible.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch06_split_000.xhtml#idm46122421125416-marker)) 可以通过使用`nice`实用程序来运行Python进程来实现（`nice
    -n –20 python` `program.py`）。将nice值设为-20将确保尽可能少地执行代码。
- en: ^([5](ch06_split_000.xhtml#idm46122421117832-marker)) A good survey of the various
    faults can be found at [*https://oreil.ly/12Beq*](https://oreil.ly/12Beq).
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch06_split_000.xhtml#idm46122421117832-marker)) 可以在[*https://oreil.ly/12Beq*](https://oreil.ly/12Beq)找到对各种故障的良好调查。
- en: ^([6](ch06_split_000.xhtml#idm46122421106008-marker)) This effect is beautifully
    explained in this [Stack Overflow response](https://stackoverflow.com/a/11227902).
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch06_split_000.xhtml#idm46122421106008-marker)) 这种效果在这个[Stack Overflow回答](https://stackoverflow.com/a/11227902)中有很好的解释。
- en: ^([7](ch06_split_000.xhtml#idm46122421072568-marker)) For an in-depth look at
    `numpy` over a variety of problems, check out [*From Python to Numpy*](https://oreil.ly/KHdg_)
    by Nicolas P. Rougier.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch06_split_000.xhtml#idm46122421072568-marker)) 要深入了解`numpy`在各种问题上的应用，请查看Nicolas
    P. Rougier的[*From Python to Numpy*](https://oreil.ly/KHdg_)。
- en: '^([8](ch06_split_000.xhtml#idm46122420993032-marker)) We do this by compiling
    `numpy` with the `-fno-tree-vectorize` flag. For this experiment, we built `numpy`
    1.17.3 with the following command: `$ OPT=''-fno-tree-vectorize'' FOPT=''-fno-tree-vectorize''
    BLAS=None LAPACK=None ATLAS=None python setup.py build`.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch06_split_000.xhtml#idm46122420993032-marker)) 我们通过在`numpy`中使用`-fno-tree-vectorize`标志来实现这一点。对于这个实验，我们使用以下命令构建了`numpy`
    1.17.3：`$ OPT='-fno-tree-vectorize' FOPT='-fno-tree-vectorize' BLAS=None LAPACK=None
    ATLAS=None python setup.py build`。
- en: ^([9](ch06_split_000.xhtml#idm46122420742936-marker)) This is contingent on
    what CPU is being used.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch06_split_000.xhtml#idm46122420742936-marker)) 这取决于使用的CPU。
- en: ^([10](ch06_split_000.xhtml#idm46122420722136-marker)) This is not strictly
    true, since two `numpy` arrays can reference the same section of memory but use
    different striding information to represent the same data in different ways. These
    two `numpy` arrays will have different `id`s. There are many subtleties to the
    `id` structure of `numpy` arrays that are outside the scope of this discussion.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch06_split_000.xhtml#idm46122420722136-marker)) 这并不严格正确，因为两个`numpy`数组可以引用内存的同一部分，但使用不同的步幅信息以不同的方式表示相同的数据。这两个`numpy`数组将具有不同的`id`。`numpy`数组的`id`结构有许多微妙之处，超出了本讨论的范围。
- en: '^([11](ch06_split_001.xhtml#idm46122419857272-marker)) See the DataQuest blog
    post [“Tutorial: Using Pandas with Large Data Sets in Python”](https://oreil.ly/frZrr)
    for more details.'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch06_split_001.xhtml#idm46122419857272-marker)) 查看DataQuest的博文[“教程：在Python中使用Pandas处理大数据集”](https://oreil.ly/frZrr)获取更多详细信息。
- en: ^([12](ch06_split_001.xhtml#idm46122419819656-marker)) If we’re going to use
    a sliding window, it might be possible to apply rolling window optimized functions
    such as `RollingOLS` from `statsmodels`.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch06_split_001.xhtml#idm46122419819656-marker)) 如果我们要使用滑动窗口，可能可以应用像`statsmodels`中的`RollingOLS`这样优化过的滚动窗口函数。
