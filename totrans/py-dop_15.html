<html><head></head><body><section data-pdf-bookmark="Chapter 15. Data Engineering" data-type="chapter" epub:type="chapter"><div class="chapter" id="DataEngineering">&#13;
<h1><span class="label">Chapter 15. </span>Data Engineering</h1>&#13;
&#13;
&#13;
<p><a data-primary="data engineering" data-type="indexterm" id="ix_ch15-asciidoc0"/>Data science may be the sexiest job of the 21st century, but the field is evolving rapidly into different job titles. Data scientist has been too crude a description for a whole series of tasks.  As of 2020, two jobs that can pay the same or more are data engineer and machine learning engineer.</p>&#13;
&#13;
<p>Even more surprising is the vast number of data engineer roles needed to support a traditional data scientist.  It is somewhere between three to five data engineers to one data scientist.</p>&#13;
&#13;
<p>What is happening?  Let’s look at it from another angle. Let’s pretend we are writing headlines for a newspaper and want to say something eye-catching.  We could say, “CEO is the sexiest job for the rich.”  There are few CEOs, just like there are few NBA stars, just like there are few professional actors who are making a living.  For every CEO, how many people are working to make that CEO successful? This last statement is content-free and meaningless, like “water is wet.”</p>&#13;
&#13;
<p>This statement isn’t to say that you can’t make a living as a data scientist; it is more a critique of the logistics behind the statement.  There is a huge demand for skills in data, and they range from DevOps to machine learning to communication.  The term data scientist is nebulous.  Is it a job or a behavior?  In a way, it is a lot like the word DevOps.  Is DevOps a job, or is it a behavior?</p>&#13;
&#13;
<p>In looking at job posting data and salary data, it appears the job market is saying there is an apparent demand for actual roles in data engineering and machine learning engineering.  This is because those roles perform identifiable tasks.  A data engineer task could be creating a pipeline in the cloud that collects both batch and streaming data and then creates APIs to access that data and schedule those jobs.  This job is not a squishy task.  It works, or it doesn’t.</p>&#13;
&#13;
<p>Likewise, a machine learning engineer builds machine learning models and deploys them in a way that they are maintainable.  This job is also not squishy.  An engineer can do data engineering or machine learning engineering though, and still exhibit behaviors attributed to data science and DevOps. Today is an exciting time to be involved in data, as there are some considerable opportunities to build complex and robust data pipelines that feed into other complex and powerful prediction systems.  There is an expression that says, “you can never be too rich or too thin.”  Likewise, with data, you can never have too much DevOps or data science skills.  Let’s dive into some DevOps-flavored ideas for data engineering.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Small Data" data-type="sect1"><div class="sect1" id="idm46691313971544">&#13;
<h1>Small Data</h1>&#13;
&#13;
<p><a data-primary="data engineering" data-secondary="small data" data-type="indexterm" id="idm46691313970296"/><a data-primary="small data" data-type="indexterm" id="idm46691313969448"/>Toolkits are an exciting concept.  If you call a plumber to your house, they arrive with tools that help them be more effective than you could be at a task.  If you hire a carpenter to build something at your house, they also have a unique set of tools that help them perform a task in a fraction of the time you could.  Tools are essential to professionals, and DevOps is no exception.</p>&#13;
&#13;
<p>In this section, the tools of data engineering outline themselves.  These tools include small data tasks like reading and writing files, using <code>pickle,</code> using <code>JSON,</code> and writing and reading <code>YAML</code> files.  Being able to master these formats is critical to be the type of automator who can tackle any task and turn it into a script.  Tools for Big Data tasks are also covered later in the chapter.  It discusses distinctly different tools than the tools used for small data.</p>&#13;
&#13;
<p>What is Big Data and what is small data then?  One easy way to figure the distinction out is the laptop test.  Does it work on your laptop?  If it doesn’t, then it is Big Data.  A good example is Pandas.  Pandas require between 5 to 10 times the amount of RAM as the dataset.  If you have a 2-GB file and you are using Pandas, most likely your laptop won’t work.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Dealing with Small Data Files" data-type="sect2"><div class="sect2" id="idm46691313965240">&#13;
<h2>Dealing with Small Data Files</h2>&#13;
&#13;
<p>If there was a single defining trait of Python, it would be a relentless pursuit of efficiency in the language.  A typical Python programmer wants to write just enough code to get a task done but wants to stop at the point where the code becomes unreadable or terse.  Also, a typical Python programmer will not want to write boilerplate code.  This environment has led to a continuous evolution of useful patterns.</p>&#13;
&#13;
<p>One example of an active pattern is using the <a data-primary="with statement" data-type="indexterm" id="idm46691313963032"/><code>with</code> statement to read and write files.  The <code>with</code> statement handles the boring boilerplate parts of closing a file handle after the work has completed.  The <code>with</code> statement is also used in other parts of the Python language to make tedious tasks less annoying.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Write a File" data-type="sect1"><div class="sect1" id="idm46691313960552">&#13;
<h1>Write a File</h1>&#13;
&#13;
<p><a data-primary="data engineering" data-secondary="writing a file" data-type="indexterm" id="idm46691313959176"/>This example shows that writing a file using the <code>with</code> statement automatically closes the file handle upon execution of a code block.  This syntax prevents bugs that can occur quickly when the handle is accidentally not closed:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="s2">"containers.txt"</code><code class="p">,</code> <code class="s2">"w"</code><code class="p">)</code> <code class="k">as</code> <code class="n">file_to_write</code><code class="p">:</code>&#13;
  <code class="n">file_to_write</code><code class="o">.</code><code class="n">write</code><code class="p">(</code><code class="s2">"Pod/n"</code><code class="p">)</code>&#13;
  <code class="n">file_to_write</code><code class="o">.</code><code class="n">write</code><code class="p">(</code><code class="s2">"Service/n"</code><code class="p">)</code>&#13;
  <code class="n">file_to_write</code><code class="o">.</code><code class="n">write</code><code class="p">(</code><code class="s2">"Volume/n"</code><code class="p">)</code>&#13;
  <code class="n">file_to_write</code><code class="o">.</code><code class="n">write</code><code class="p">(</code><code class="s2">"Namespace/n"</code><code class="p">)</code></pre>&#13;
&#13;
<p>The output of the file reads like this:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">cat containers.txt&#13;
&#13;
Pod&#13;
Service&#13;
Volume&#13;
Namespace</pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Read a File" data-type="sect1"><div class="sect1" id="idm46691313880216">&#13;
<h1>Read a File</h1>&#13;
&#13;
<p><a data-primary="data engineering" data-secondary="reading a file" data-type="indexterm" id="idm46691313920568"/><a data-primary="reading a file" data-type="indexterm" id="idm46691313919592"/>The <code>with</code> context also is the recommended way to read a file.  Notice that using <a data-primary="readlines() method" data-type="indexterm" id="idm46691313918376"/><code>readlines()</code> uses line breaks to return a lazily evaluated iterator:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="s2">"containers.txt"</code><code class="p">)</code> <code class="k">as</code> <code class="n">file_to_read</code><code class="p">:</code>&#13;
  <code class="n">lines</code> <code class="o">=</code> <code class="n">file_to_read</code><code class="o">.</code><code class="n">readlines</code><code class="p">()</code>&#13;
  <code class="k">print</code><code class="p">(</code><code class="n">lines</code><code class="p">)</code></pre>&#13;
&#13;
<p>The output:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="p">[</code><code class="s1">'Pod</code><code class="se">\n</code><code class="s1">'</code><code class="p">,</code> <code class="s1">'Service</code><code class="se">\n</code><code class="s1">'</code><code class="p">,</code> <code class="s1">'Volume</code><code class="se">\n</code><code class="s1">'</code><code class="p">,</code> <code class="s1">'Namespace</code><code class="se">\n</code><code class="s1">'</code><code class="p">]</code></pre>&#13;
&#13;
<p>In practice, this means that you can handle large log files by using generator expressions and not worry about consuming all of the memory on your machine.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Generator Pipeline to Read and Process Lines" data-type="sect1"><div class="sect1" id="idm46691313839560">&#13;
<h1>Generator Pipeline to Read and Process Lines</h1>&#13;
&#13;
<p><a data-primary="data engineering" data-secondary="generator pipeline to read and process lines" data-type="indexterm" id="idm46691313838280"/><a data-primary="generator pipeline, read and process lines with" data-type="indexterm" id="idm46691313837272"/><a data-primary="read and process lines, generator pipeline to" data-type="indexterm" id="idm46691313836568"/>This code is a generator function that opens a file and returns a generator:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">process_file_lazily</code><code class="p">():</code>&#13;
  <code class="sd">"""Uses generator to lazily process file"""</code>&#13;
&#13;
  <code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="s2">"containers.txt"</code><code class="p">)</code> <code class="k">as</code> <code class="n">file_to_read</code><code class="p">:</code>&#13;
    <code class="k">for</code> <code class="n">line</code> <code class="ow">in</code> <code class="n">file_to_read</code><code class="o">.</code><code class="n">readlines</code><code class="p">():</code>&#13;
      <code class="k">yield</code> <code class="n">line</code></pre>&#13;
&#13;
<p>Next, this generator is used to create a pipeline to perform operations line by line.  In this example, the line converts to a lowercase string.  Many other actions could be chained together here, and it would be very efficient because it is only using the memory necessary to process a line at a time:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Create generator object</code>&#13;
<code class="n">pipeline</code> <code class="o">=</code> <code class="n">process_file_lazily</code><code class="p">()</code>&#13;
<code class="c1"># convert to lowercase</code>&#13;
<code class="n">lowercase</code> <code class="o">=</code> <code class="p">(</code><code class="n">line</code><code class="o">.</code><code class="n">lower</code><code class="p">()</code> <code class="k">for</code> <code class="n">line</code> <code class="ow">in</code> <code class="n">pipeline</code><code class="p">)</code>&#13;
<code class="c1"># print first processed line</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="nb">next</code><code class="p">(</code><code class="n">lowercase</code><code class="p">))</code></pre>&#13;
&#13;
<p>This is the output of the pipeline:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">pod</code></pre>&#13;
&#13;
<p>In practice, this means that files that are effectively infinite because they are so large could still be processed if the code works in a way that exits when it finds a condition. For example, perhaps you need to find a customer ID across terabytes of log data. A generator pipeline could look for this customer ID and then exit the processing at the first occurrence.  In the world of Big Data, this is no longer a theoretical problem.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Using YAML" data-type="sect1"><div class="sect1" id="idm46691313711304">&#13;
<h1>Using YAML</h1>&#13;
&#13;
<p><a data-primary="data engineering" data-secondary="YAML" data-type="indexterm" id="idm46691313709832"/><a data-primary="YAML" data-type="indexterm" id="idm46691313691736"/>YAML is becoming an emerging standard for DevOps-related configuration files. It is a human-readable data serialization format that is a superset of JSON. It stands for “YAML Ain’t Markup Language.” You often see YAML in build systems such as <a href="https://oreil.ly/WZnIl">AWS CodePipeline</a>, <a href="https://oreil.ly/0r8cK">CircleCI</a>, or PaaS offerings such as <a href="https://oreil.ly/ny_TD">Google App Engine</a>.</p>&#13;
&#13;
<p>There is a reason YAML is so often used. There is a need for a configuration language that allows rapid iteration when interacting with highly automated systems. Both a nonprogrammer and a programmer can intuitively figure out how to edit these files. Here is an example:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">yaml</code>&#13;
&#13;
<code class="n">kubernetes_components</code> <code class="o">=</code> <code class="p">{</code>&#13;
    <code class="s2">"Pod"</code><code class="p">:</code> <code class="s2">"Basic building block of Kubernetes."</code><code class="p">,</code>&#13;
    <code class="s2">"Service"</code><code class="p">:</code> <code class="s2">"An abstraction for dealing with Pods."</code><code class="p">,</code>&#13;
    <code class="s2">"Volume"</code><code class="p">:</code> <code class="s2">"A directory accessible to containers in a Pod."</code><code class="p">,</code>&#13;
    <code class="s2">"Namespaces"</code><code class="p">:</code> <code class="s2">"A way to divide cluster resources between users."</code>&#13;
<code class="p">}</code>&#13;
&#13;
<code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="s2">"kubernetes_info.yaml"</code><code class="p">,</code> <code class="s2">"w"</code><code class="p">)</code> <code class="k">as</code> <code class="n">yaml_to_write</code><code class="p">:</code>&#13;
  <code class="n">yaml</code><code class="o">.</code><code class="n">safe_dump</code><code class="p">(</code><code class="n">kubernetes_components</code><code class="p">,</code> <code class="n">yaml_to_write</code><code class="p">,</code> <code class="n">default_flow_style</code><code class="o">=</code><code class="bp">False</code><code class="p">)</code></pre>&#13;
&#13;
<p>The output written to disk looks like this:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">cat kubernetes_info.yaml&#13;
&#13;
Namespaces: A way to divide cluster resources between users.&#13;
Pod: Basic building block of Kubernetes.&#13;
Service: An abstraction <code class="k">for</code> dealing with Pods.&#13;
Volume: A directory accessible to containers in a Pod.</pre>&#13;
&#13;
<p>The takeway is that it makes it trivial to serialize a Python data structure into a format that is easy to edit and iterate on.  Reading this file back is just two lines of code.</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">yaml</code>&#13;
&#13;
<code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="s2">"kubernetes_info.yaml"</code><code class="p">,</code> <code class="s2">"rb"</code><code class="p">)</code> <code class="k">as</code> <code class="n">yaml_to_read</code><code class="p">:</code>&#13;
  <code class="n">result</code> <code class="o">=</code> <code class="n">yaml</code><code class="o">.</code><code class="n">safe_load</code><code class="p">(</code><code class="n">yaml_to_read</code><code class="p">)</code></pre>&#13;
&#13;
<p>The output then can be pretty printed:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">pprint</code>&#13;
<code class="n">pp</code> <code class="o">=</code> <code class="n">pprint</code><code class="o">.</code><code class="n">PrettyPrinter</code><code class="p">(</code><code class="n">indent</code><code class="o">=</code><code class="mi">4</code><code class="p">)</code>&#13;
<code class="n">pp</code><code class="o">.</code><code class="n">pprint</code><code class="p">(</code><code class="n">result</code><code class="p">)</code>&#13;
<code class="p">{</code>   <code class="s1">'Namespaces'</code><code class="p">:</code> <code class="s1">'A way to divide cluster resources between users.'</code><code class="p">,</code>&#13;
    <code class="s1">'Pod'</code><code class="p">:</code> <code class="s1">'Basic building block of Kubernetes.'</code><code class="p">,</code>&#13;
    <code class="s1">'Service'</code><code class="p">:</code> <code class="s1">'An abstraction for dealing with Pods.'</code><code class="p">,</code>&#13;
    <code class="s1">'Volume'</code><code class="p">:</code> <code class="s1">'A directory accessible to containers in a Pod.'</code><code class="p">}</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Big Data" data-type="sect1"><div class="sect1" id="idm46691313710712">&#13;
<h1>Big Data</h1>&#13;
&#13;
<p><a data-primary="big data" data-type="indexterm" id="ix_ch15-asciidoc1"/><a data-primary="data engineering" data-secondary="big data" data-type="indexterm" id="ix_ch15-asciidoc2"/>Data<a data-primary="big data" data-secondary="defining" data-type="indexterm" id="ix_ch15-asciidoc3"/> has been growing at a rate faster than the growth of computer processing power. To make things even more interesting, Moore’s Law, which states that speed and capability of computers can be expected to double every two years, effectively ceased to apply around 2015, according to Dr. David Patterson at UC Berkeley. CPU speed is only increasing around 3% per a year now.</p>&#13;
&#13;
<p>New methods of dealing with Big Data are necessary.  Some of the new methods, include using ASICs like GPUs, tensor processing units (TPUs), and as well as AI and data platforms provided by cloud vendors.  On the chip level, this means that a GPU could be the ideal target for a complex IT process instead of a CPU.  Often this GPU is paired together with a system that can deliver a distributed storage mechanism that allows both distributed computing and distributed disk I/O. An excellent example of this is Apache Spark, Amazon SageMaker, or the Google AI Platform.  All of them can utilize ASICs (GPU, TPU, and more), plus distributed storage along with a management system.  Another example that is more low level is Amazon Spot Instance deep learning AMIs with Amazon Elastic File System (EFS) mount points.</p>&#13;
&#13;
<p>For a DevOps professional, this means a few things.  First, it means that special care when delivering software to these systems makes sense.  For example, does the target platform have the correct GPU drivers?  Are you deploying via containers?  Is this system going to use distributed GPU processing?  Is the data mostly batch, or is it streaming?  Thinking about these questions up front can go a long way to ensuring the correct architecture.</p>&#13;
&#13;
<p>One problem with buzzwords like AI, Big Data, cloud, or data scientist is that they mean different things to different people.  Take data scientist for example. In one company it could mean someone who generates business intelligence dashboards for the sales team, and in another company, it could mean someone who is developing self-driving car software.  Big Data has a similar context issue; it can mean many different things depending on whom you meet.  Here is one definition to consider. Do you need different software packages to handle data on your laptop than in <span class="keep-together">production?</span></p>&#13;
&#13;
<p>An excellent example of a “small data” tool is the <a data-primary="Pandas package" data-secondary="small data tools" data-type="indexterm" id="idm46691313474872"/>Pandas package.  According to the author of the Pandas package, it can take between 5 and 10 times the amount of RAM as the size of the file used.  In practice, if your laptop has 16 GB of RAM and you open a 2-GB CSV file, it is now a Big Data problem because your laptop may not have enough RAM, 20 GB, to work with the file.  Instead, you may need to rethink how to handle the problem.  Perhaps you can open a sample of the data, or truncate the data to get around the problem initially.</p>&#13;
&#13;
<p>Here is an example of this exact problem and a workaround.  Let’s say you are supporting data scientists that keep running into Pandas out-of-memory errors because they are using files too large for Pandas. One such example is the Open Food Facts <a href="https://oreil.ly/w-tmA">dataset from Kaggle</a>.  When uncompressed, the dataset is over 1 GB.  This problem fits precisely into the sweet spot of where Pandas could struggle to process it.  One thing you can do is use the Unix <code>shuf</code> command to create a shuffled sample:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting"><code class="nb">time </code>shuf -n <code class="m">100000</code> en.openfoodfacts.org.products.tsv<code class="se">\</code>&#13;
    &gt; 10k.sample.en.openfoodfacts.org.products.tsv&#13;
    1.89s user 0.80s system 97% cpu 2.748 total</pre>&#13;
&#13;
<p>In a little under two seconds, the file can be cut down to a manageable size. This approach is preferable to simply using heads or tails, because the samples are randomly selected.  This problem is significant for a data science workflow. Also, you can inspect the lines of the file to see what you are dealing with first:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">wc -l en.openfoodfacts.org.products.tsv&#13;
  <code class="m">356002</code> en.openfoodfacts.org.products.tsv</pre>&#13;
&#13;
<p>The source file is about 350,000 lines, so grabbing 100,000 shuffled lines is approximately a third of the data.  This task can be confirmed by looking at the transformed file. It shows 272 MB, around one-third of the size of the original 1-GB file:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">du -sh 10k.sample.en.openfoodfacts.org.products.tsv&#13;
272M    10k.sample.en.openfoodfacts.org.products.tsv</pre>&#13;
&#13;
<p>This size is more much manageable by Pandas, and this process could then be turned into an automation workflow that creates randomized sample files for Big Data sources.  This type of process is just one of many particular workflows that Big Data demands.</p>&#13;
&#13;
<p>Another definition of Big Data is by McKinsey, who defined Big Data in 2011 as “datasets whose size is beyond the ability of typical database software tools to capture, store, manage, and analyze.”  This definition is reasonable as well, with the slight modification that it isn’t just database software tools; it is any tool that touches data.  When a tool that works well on a laptop, such as  Pandas, Python, MySQL, deep learning/machine learning, Bash, and more, fails to perform conventionally due to the size or velocity (rate of change) of the data, it is now a Big Data problem.  Big Data problems require specialized tools, and the next section dives into this requirement.<a data-startref="ix_ch15-asciidoc3" data-type="indexterm" id="idm46691313455384"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Big Data Tools, Components, and Platforms" data-type="sect1"><div class="sect1" id="idm46691313458280">&#13;
<h1>Big Data Tools, Components, and Platforms</h1>&#13;
&#13;
<p><a data-primary="big data" data-secondary="tools, components, and platforms" data-type="indexterm" id="ix_ch15-asciidoc4"/>Another way to discuss Big Data is to break it down into tools and platforms. <a data-type="xref" href="#Figure-15-1">Figure 15-1</a> shows a typical Big Data architecture life cycle.</p>&#13;
&#13;
<figure><div class="figure" id="Figure-15-1">&#13;
<img alt="pydo 1501" src="assets/pydo_1501.png"/>&#13;
<h6><span class="label">Figure 15-1. </span>Big Data architecture</h6>&#13;
</div></figure>&#13;
&#13;
<p>Let’s discuss a few key components.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Data Sources" data-type="sect2"><div class="sect2" id="idm46691313395832">&#13;
<h2>Data Sources</h2>&#13;
&#13;
<p><a data-primary="big data" data-secondary="sources" data-type="indexterm" id="idm46691313394632"/>Some of the familiar sources of Big Data include social networks and digital transactions.  As people have migrated more of their conversations and their business transactions online, it has led to an explosion of data.  Additionally, mobile technology such as tablets, phones, and laptops that record audio and video, exponentially create sources of data.</p>&#13;
&#13;
<p>Other data sources include the Internet of Things (IoT), which includes sensors, lightweight chips, and devices.  All of this leads to an unstoppable proliferation of data that needs to be stored somewhere.  The tools involved in Data Sources could range from IoT client/server systems such as AWS IoT Greengrass, to object storage systems such as Amazon S3 or Google Cloud Storage.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Filesystems" data-type="sect2"><div class="sect2" id="idm46691313392120">&#13;
<h2>Filesystems</h2>&#13;
&#13;
<p><a data-primary="big data" data-secondary="file systems" data-type="indexterm" id="idm46691313374312"/>Filesystems have played a huge role in computing.  Their implementation, though, is continuously evolving. In dealing with Big Data, one issue is having enough disk I/O, to handle distributed operations.</p>&#13;
&#13;
<p>One modern tool that deals with this is the Hadoop Distributed File System (HDFS).  It works by clustering many servers together, allowing aggregated CPU, disk I/O, and storage.  In practice, this makes HDFS a fundamental technology for dealing with Big Data. It can migrate large volumes of data or filesystems for distributed computing jobs.  It is also the backbone of Spark, which can do both stream- and batch-based machine learning.</p>&#13;
&#13;
<p>Other types of filesystems include object storage filesystems such as Amazon S3 filesystem and Google Cloud Platform storage.  They allow for huge files to be stored in a distributed and highly available manner, or more precisely, 99.999999999% reliability.  There are Python APIs and command-line tools available that communicate with these filesystems, enabling easy automation.  These cloud APIs are covered in more detail in <a data-type="xref" href="ch10.html#infra-as-code">Chapter 10</a>.</p>&#13;
&#13;
<p>Finally, another type of filesystem to be aware of is a traditional network filesystem, or NFS, made available as a managed cloud service. An excellent example of this is Amazon Elastic File System (Amazon EFS).  For a DevOps professional, a highly available and elastic NFS filesystem can be an incredibly versatile tool, especially coupled with containers technology. <a data-type="xref" href="#Figure-15-2">Figure 15-2</a> shows an example of mounting EFS in a container.</p>&#13;
&#13;
<figure><div class="figure" id="Figure-15-2">&#13;
<img alt="pydo 1502" src="assets/pydo_1502.png"/>&#13;
<h6><span class="label">Figure 15-2. </span>Mounting EFS in a container</h6>&#13;
</div></figure>&#13;
&#13;
<p>One powerful automation workflow is to programmatically create Docker containers through a build system such as AWS CodePipeline or Google Cloud Build.  Those containers then get registered in the cloud container registry, for example, Amazon ECR.  Next, a container management system such as Kubernetes spawns containers that mount the NFS.  This allows both the power of immutable container images that spawn quickly, and access to centralized source code libraries and data.  This type of workflow could be ideal for an organization looking to optimize machine learning operations.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Data Storage" data-type="sect2"><div class="sect2" id="idm46691313371736">&#13;
<h2>Data Storage</h2>&#13;
&#13;
<p><a data-primary="big data" data-secondary="data storage" data-type="indexterm" id="idm46691313445176"/><a data-primary="big data" data-secondary="storage" data-type="indexterm" id="idm46691313444200"/><a data-primary="storage, big data" data-type="indexterm" id="idm46691313354536"/>Ultimately the data needs to live somewhere, and this creates some exciting opportunities and challenges.  One emerging trend is to utilize the concept of a data lake.  Why do you care about a data lake?  A <a data-primary="Data Lake" data-type="indexterm" id="idm46691313353576"/>data lake allows for data processing in the same location as storage.  As a result, many data lakes need to have infinite storage and provide infinite computing (i.e., be on the cloud).  Amazon S3 is often a common choice for a data lake.</p>&#13;
&#13;
<p>The data lake constructed in this manner can also be utilized by a machine learning pipeline that may depend on the training data living in the lake, as well as the trained models.  The trained models could then always be A/B tested to ensure the latest model is improving the production prediction (inference) system, as shown in <a data-type="xref" href="#Figure-15-3">Figure 15-3</a>.</p>&#13;
&#13;
<figure><div class="figure" id="Figure-15-3">&#13;
<img alt="pydo 1503" src="assets/pydo_1503.png"/>&#13;
<h6><span class="label">Figure 15-3. </span>Data lake</h6>&#13;
</div></figure>&#13;
&#13;
<p>Other forms of storage will be very familiar to traditional software developers.  These storage systems include relational databases, key/value databases, search engines like Elasticsearch, and graph databases.  In a Big Data architecture, each type of storage system may play a more specific role.  In a small-scale system, a relational database may be a jack of all trades, but in a Big Data architecture, there is less room for tolerance of a mismatch of the storage system.</p>&#13;
&#13;
<p>An excellent example of mismatch in storage choice is using a relational database as a search engine by enabling full-text search capabilities instead of using a specialized solution, like Elasticsearch.  Elasticsearch is designed to create a scalable search solution, while a relational database is designed to provide referential integrity and transactions.  The CTO of Amazon, Werner Vogel, makes this point very well by stating that a “one size database doesn’t fit anyone.”  This problem is illustrated in <a data-type="xref" href="#Figure-15-4">Figure 15-4</a>, which shows that each type of database has a specific purpose.</p>&#13;
&#13;
<figure><div class="figure" id="Figure-15-4">&#13;
<img alt="pydo 1504" src="assets/pydo_1504.png"/>&#13;
<h6><span class="label">Figure 15-4. </span>Amazon databases</h6>&#13;
</div></figure>&#13;
&#13;
<p>Picking the correct storage solutions, including which combination of databases to use, is a crucial skill for any type of data architect to ensure that a system works at optimal efficiency. In thinking through the design of a system that is fully automated and efficient, maintenance should be a consideration. If a particular technology choice is being abused, such as using a relational database for a highly available messaging queue, then maintenance costs could explode, which in turn creates more automation work.  So another component to consider is how much automation work it takes to maintain a solution<a data-startref="ix_ch15-asciidoc4" data-type="indexterm" id="idm46691313343416"/>.<a data-startref="ix_ch15-asciidoc2" data-type="indexterm" id="idm46691313342584"/><a data-startref="ix_ch15-asciidoc1" data-type="indexterm" id="idm46691313341880"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Real-Time Streaming Ingestion" data-type="sect1"><div class="sect1" id="idm46691313341080">&#13;
<h1>Real-Time Streaming Ingestion</h1>&#13;
&#13;
<p><a data-primary="data engineering" data-secondary="real-time streaming ingestion" data-type="indexterm" id="idm46691313339800"/><a data-primary="real-time streaming ingestion" data-type="indexterm" id="idm46691313338856"/>Real-time streaming data is a particularly tricky type of data to deal with.  The stream itself increases the complexity of dealing with the data, and it is possible the stream needs to route to yet another part of the system that intends to process the data in a streaming fashion.  One example of a cloud-based streaming ingestion solution is Amazon Kinesis Data Firehose. See <a data-type="xref" href="#Figure-15-5">Figure 15-5</a>.</p>&#13;
&#13;
<figure><div class="figure" id="Figure-15-5">&#13;
<img alt="pydo 1505" src="assets/pydo_1505.png"/>&#13;
<h6><span class="label">Figure 15-5. </span>Kinesis log files</h6>&#13;
</div></figure>&#13;
&#13;
<p>Here is an example of code that would do that.  Notice that Python’s <code>asyncio</code> module allows for highly concurrent network operations that are single threaded.  Nodes could emit this in a job farm, and it could be metrics or error logs:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">asyncio</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">send_async_firehose_events</code><code class="p">(</code><code class="n">count</code><code class="o">=</code><code class="mi">100</code><code class="p">):</code>&#13;
    <code class="sd">"""Async sends events to firehose"""</code>&#13;
&#13;
    <code class="n">start</code> <code class="o">=</code> <code class="n">time</code><code class="o">.</code><code class="n">time</code><code class="p">()</code>&#13;
    <code class="n">client</code> <code class="o">=</code> <code class="n">firehose_client</code><code class="p">()</code>&#13;
    <code class="n">extra_msg</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"aws_service"</code><code class="p">:</code> <code class="s2">"firehose"</code><code class="p">}</code>&#13;
    <code class="n">loop</code> <code class="o">=</code> <code class="n">asyncio</code><code class="o">.</code><code class="n">get_event_loop</code><code class="p">()</code>&#13;
    <code class="n">tasks</code> <code class="o">=</code> <code class="p">[]</code>&#13;
    <code class="n">LOG</code><code class="o">.</code><code class="n">info</code><code class="p">(</code><code class="n">f</code><code class="s2">"sending aysnc events TOTAL {count}"</code><code class="p">,</code><code class="n">extra</code><code class="o">=</code><code class="n">extra_msg</code><code class="p">)</code>&#13;
    <code class="n">num</code> <code class="o">=</code> <code class="mi">0</code>&#13;
    <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">count</code><code class="p">):</code>&#13;
        <code class="n">tasks</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">asyncio</code><code class="o">.</code><code class="n">ensure_future</code><code class="p">(</code><code class="n">put_record</code><code class="p">(</code><code class="n">gen_uuid_events</code><code class="p">(),</code>&#13;
                                                      <code class="n">client</code><code class="p">)))</code>&#13;
        <code class="n">LOG</code><code class="o">.</code><code class="n">info</code><code class="p">(</code><code class="n">f</code><code class="s2">"sending aysnc events: COUNT {num}/{count}"</code><code class="p">)</code>&#13;
        <code class="n">num</code> <code class="o">+=</code><code class="mi">1</code>&#13;
    <code class="n">loop</code><code class="o">.</code><code class="n">run_until_complete</code><code class="p">(</code><code class="n">asyncio</code><code class="o">.</code><code class="n">wait</code><code class="p">(</code><code class="n">tasks</code><code class="p">))</code>&#13;
    <code class="n">loop</code><code class="o">.</code><code class="n">close</code><code class="p">()</code>&#13;
    <code class="n">end</code> <code class="o">=</code> <code class="n">time</code><code class="o">.</code><code class="n">time</code><code class="p">()</code>&#13;
    <code class="n">LOG</code><code class="o">.</code><code class="n">info</code><code class="p">(</code><code class="s2">"Total time: {}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">end</code> <code class="o">-</code> <code class="n">start</code><code class="p">))</code></pre>&#13;
&#13;
<p>Kinesis Data Firehose works by accepting capture data and routing it continuously to any number of destinations:  Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, or some third-party service like Splunk.  An open source alternative to using a managed service like Kinesis is to use Apache Kafka.  Apache Kafka has similar principles in that it works as a pub/sub architecture.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Case Study:  Building a Homegrown Data Pipeline" data-type="sect1"><div class="sect1" id="idm46691313330616">&#13;
<h1>Case Study:  Building a Homegrown Data Pipeline</h1>&#13;
&#13;
<p><a data-primary="data engineering" data-secondary="case study" data-type="indexterm" id="idm46691313220104"/>In the early days of Noah’s work as CTO and General Manager at a startup in the early 2000s, one problem that cropped up was how to build the company’s first machine learning pipeline and data pipeline.  A rough sketch of what that looked like is in the following diagram of a Jenkins data pipeline (<a data-type="xref" href="#Figure-15-6">Figure 15-6</a>).</p>&#13;
&#13;
<figure><div class="figure" id="Figure-15-6">&#13;
<img alt="pydo 1506" src="assets/pydo_1506.png"/>&#13;
<h6><span class="label">Figure 15-6. </span>Jenkins data pipeline</h6>&#13;
</div></figure>&#13;
&#13;
<p>The inputs to the data pipeline are any data source needed for business analytics or machine learning predictions.  These sources included a relational database, Google Analytics, and social media metrics, to a name a few.  The collection jobs ran every hour and generated CSV files that were available internally by Apache web service.  This solution was a compelling and straightforward process.</p>&#13;
&#13;
<p>The jobs themselves were Jenkins jobs that were Python scripts that ran.  If something needed to be changed, it was fairly straightforward to change a Python script for a particular job.  An added benefit to this system was that it was straightforward to debug.  If a job had a failure, the jobs showed up as failed, and it was straightforward to look at the output of the job and see what happened.</p>&#13;
&#13;
<p>The final stage of the pipeline then created machine learning predictions and an analytics dashboard that served out dashboards via an <code>R</code>-based Shiny application.  The simplicity of the approach is the most influential factor in this type of architecture, and as a bonus it leverages existing DevOps skills.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Serverless Data Engineering" data-type="sect1"><div class="sect1" id="idm46691313212952">&#13;
<h1>Serverless Data Engineering</h1>&#13;
&#13;
<p><a data-primary="data engineering" data-secondary="serverless" data-type="indexterm" id="ix_ch15-asciidoc5"/><a data-primary="serverless data engineering" data-type="indexterm" id="ix_ch15-asciidoc6"/>Another emerging pattern is serverless data engineering.  <a data-type="xref" href="#Figure-15-7">Figure 15-7</a> is a high-level architectural diagram of what a serverless data pipeline is.</p>&#13;
&#13;
<figure><div class="figure" id="Figure-15-7">&#13;
<img alt="pydo 1507" src="assets/pydo_1507.png"/>&#13;
<h6><span class="label">Figure 15-7. </span>Serverless data pipeline</h6>&#13;
</div></figure>&#13;
&#13;
<p>Next, let’s look at what a timed lambda does.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="pagebreak-before less_space" data-pdf-bookmark="Using AWS Lambda with CloudWatch Events" data-type="sect2"><div class="sect2" id="idm46691313205944">&#13;
<h2>Using AWS Lambda with CloudWatch Events</h2>&#13;
&#13;
<p><a data-primary="AWS Lambda" data-secondary="CloudWatch events" data-type="indexterm" id="idm46691313204488"/><a data-primary="CloudWatch events" data-type="indexterm" id="idm46691313203512"/><a data-primary="data engineering" data-secondary="using AWS Lambda with CloudWatch events" data-type="indexterm" id="idm46691313202840"/><a data-primary="serverless data engineering" data-secondary="using AWS Lambda with CloudWatch events" data-type="indexterm" id="idm46691313201832"/>You can create a CloudWatch timer to call the lambda using the AWS Lambda console and to set up a trigger, as shown in <a data-type="xref" href="#Figure-15-8">Figure 15-8</a>.</p>&#13;
&#13;
<figure><div class="figure" id="Figure-15-8">&#13;
<img alt="pydo 1508" src="assets/pydo_1508.png"/>&#13;
<h6><span class="label">Figure 15-8. </span>CloudWatch Lambda timer</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Using Amazon CloudWatch Logging with AWS Lambda" data-type="sect2"><div class="sect2" id="idm46691313197576">&#13;
<h2>Using Amazon CloudWatch Logging with AWS Lambda</h2>&#13;
&#13;
<p><a data-primary="AWS (Amazon Web Services)" data-secondary="CloudWatch logging" data-type="indexterm" id="idm46691313196312"/><a data-primary="AWS Lambda" data-secondary="AWS CloudWatch logging with" data-type="indexterm" id="idm46691313195320"/><a data-primary="data engineering" data-secondary="using AWS CloudWatch logging with AWS Lambda" data-type="indexterm" id="idm46691313194360"/><a data-primary="serverless data engineering" data-secondary="using AWS CloudWatch logging with AWS Lambda" data-type="indexterm" id="idm46691313193320"/>Using CloudWatch logging is an essential step for Lambda development. <a data-type="xref" href="#Figure-15-9">Figure 15-9</a> is an example of a CloudWatch event log.</p>&#13;
&#13;
<figure><div class="figure" id="Figure-15-9">&#13;
<img alt="pydo 1509" src="assets/pydo_1509.png"/>&#13;
<h6><span class="label">Figure 15-9. </span>CloudWatch event log</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Using AWS Lambda to Populate Amazon Simple Queue Service" data-type="sect2"><div class="sect2" id="idm46691313189208">&#13;
<h2>Using AWS Lambda to Populate Amazon Simple Queue Service</h2>&#13;
&#13;
<p><a data-primary="AWS Lambda" data-secondary="populating AWS SQS with" data-type="indexterm" id="ix_ch15-asciidoc7"/><a data-primary="AWS SQS (Simple Queuing Service)" data-secondary="AWS Lambda populating" data-type="indexterm" id="ix_ch15-asciidoc8"/><a data-primary="data engineering" data-secondary="using AWS Lambda to populate AWS SQS" data-type="indexterm" id="ix_ch15-asciidoc9"/><a data-primary="serverless data engineering" data-secondary="AWS Lambda populating AWS SQS" data-type="indexterm" id="ix_ch15-asciidoc10"/><a data-primary="SQS (Simple Queuing Service)" data-secondary="AWS Lambda populating" data-type="indexterm" id="ix_ch15-asciidoc11"/>Next, you want to do the following locally in AWS Cloud9:</p>&#13;
<ol>&#13;
<li>&#13;
<p>Create a new Lambda with Serverless Wizard.</p>&#13;
</li>&#13;
<li>&#13;
<p><code>cd</code> into lambda and install packages one level up.</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">pip3 install boto3 --target ../&#13;
pip3 install python-json-logger --target ../</pre>&#13;
&#13;
<p>Next, you can test local and deploy this code:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="sd">'''</code>&#13;
<code class="sd">Dynamo to SQS</code>&#13;
<code class="sd">'''</code>&#13;
&#13;
<code class="kn">import</code> <code class="nn">boto3</code>&#13;
<code class="kn">import</code> <code class="nn">json</code>&#13;
<code class="kn">import</code> <code class="nn">sys</code>&#13;
<code class="kn">import</code> <code class="nn">os</code>&#13;
&#13;
<code class="n">DYNAMODB</code> <code class="o">=</code> <code class="n">boto3</code><code class="o">.</code><code class="n">resource</code><code class="p">(</code><code class="s1">'dynamodb'</code><code class="p">)</code>&#13;
<code class="n">TABLE</code> <code class="o">=</code> <code class="s2">"fang"</code>&#13;
<code class="n">QUEUE</code> <code class="o">=</code> <code class="s2">"producer"</code>&#13;
<code class="n">SQS</code> <code class="o">=</code> <code class="n">boto3</code><code class="o">.</code><code class="n">client</code><code class="p">(</code><code class="s2">"sqs"</code><code class="p">)</code>&#13;
&#13;
<code class="c1">#SETUP LOGGING</code>&#13;
<code class="kn">import</code> <code class="nn">logging</code>&#13;
<code class="kn">from</code> <code class="nn">pythonjsonlogger</code> <code class="kn">import</code> <code class="n">jsonlogger</code>&#13;
&#13;
<code class="n">LOG</code> <code class="o">=</code> <code class="n">logging</code><code class="o">.</code><code class="n">getLogger</code><code class="p">()</code>&#13;
<code class="n">LOG</code><code class="o">.</code><code class="n">setLevel</code><code class="p">(</code><code class="n">logging</code><code class="o">.</code><code class="n">INFO</code><code class="p">)</code>&#13;
<code class="n">logHandler</code> <code class="o">=</code> <code class="n">logging</code><code class="o">.</code><code class="n">StreamHandler</code><code class="p">()</code>&#13;
<code class="n">formatter</code> <code class="o">=</code> <code class="n">jsonlogger</code><code class="o">.</code><code class="n">JsonFormatter</code><code class="p">()</code>&#13;
<code class="n">logHandler</code><code class="o">.</code><code class="n">setFormatter</code><code class="p">(</code><code class="n">formatter</code><code class="p">)</code>&#13;
<code class="n">LOG</code><code class="o">.</code><code class="n">addHandler</code><code class="p">(</code><code class="n">logHandler</code><code class="p">)</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">scan_table</code><code class="p">(</code><code class="n">table</code><code class="p">):</code>&#13;
    <code class="sd">'''Scans table and return results'''</code>&#13;
&#13;
    <code class="n">LOG</code><code class="o">.</code><code class="n">info</code><code class="p">(</code><code class="n">f</code><code class="s2">"Scanning Table {table}"</code><code class="p">)</code>&#13;
    <code class="n">producer_table</code> <code class="o">=</code> <code class="n">DYNAMODB</code><code class="o">.</code><code class="n">Table</code><code class="p">(</code><code class="n">table</code><code class="p">)</code>&#13;
    <code class="n">response</code> <code class="o">=</code> <code class="n">producer_table</code><code class="o">.</code><code class="n">scan</code><code class="p">()</code>&#13;
    <code class="n">items</code> <code class="o">=</code> <code class="n">response</code><code class="p">[</code><code class="s1">'Items'</code><code class="p">]</code>&#13;
    <code class="n">LOG</code><code class="o">.</code><code class="n">info</code><code class="p">(</code><code class="n">f</code><code class="s2">"Found {len(items)} Items"</code><code class="p">)</code>&#13;
    <code class="k">return</code> <code class="n">items</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">send_sqs_msg</code><code class="p">(</code><code class="n">msg</code><code class="p">,</code> <code class="n">queue_name</code><code class="p">,</code> <code class="n">delay</code><code class="o">=</code><code class="mi">0</code><code class="p">):</code>&#13;
    <code class="sd">'''Send SQS Message</code>&#13;
&#13;
<code class="sd">    Expects an SQS queue_name and msg in a dictionary format.</code>&#13;
<code class="sd">    Returns a response dictionary.</code>&#13;
<code class="sd">    '''</code>&#13;
&#13;
    <code class="n">queue_url</code> <code class="o">=</code> <code class="n">SQS</code><code class="o">.</code><code class="n">get_queue_url</code><code class="p">(</code><code class="n">QueueName</code><code class="o">=</code><code class="n">queue_name</code><code class="p">)[</code><code class="s2">"QueueUrl"</code><code class="p">]</code>&#13;
    <code class="n">queue_send_log_msg</code> <code class="o">=</code> <code class="s2">"Send message to queue url: </code><code class="si">%s</code><code class="s2">, with body: </code><code class="si">%s</code><code class="s2">"</code> <code class="o">%</code>\&#13;
        <code class="p">(</code><code class="n">queue_url</code><code class="p">,</code> <code class="n">msg</code><code class="p">)</code>&#13;
    <code class="n">LOG</code><code class="o">.</code><code class="n">info</code><code class="p">(</code><code class="n">queue_send_log_msg</code><code class="p">)</code>&#13;
    <code class="n">json_msg</code> <code class="o">=</code> <code class="n">json</code><code class="o">.</code><code class="n">dumps</code><code class="p">(</code><code class="n">msg</code><code class="p">)</code>&#13;
    <code class="n">response</code> <code class="o">=</code> <code class="n">SQS</code><code class="o">.</code><code class="n">send_message</code><code class="p">(</code>&#13;
        <code class="n">QueueUrl</code><code class="o">=</code><code class="n">queue_url</code><code class="p">,</code>&#13;
        <code class="n">MessageBody</code><code class="o">=</code><code class="n">json_msg</code><code class="p">,</code>&#13;
        <code class="n">DelaySeconds</code><code class="o">=</code><code class="n">delay</code><code class="p">)</code>&#13;
    <code class="n">queue_send_log_msg_resp</code> <code class="o">=</code> <code class="s2">"Message Response: </code><code class="si">%s</code><code class="s2"> for queue url: </code><code class="si">%s</code><code class="s2">"</code> <code class="o">%</code>\&#13;
        <code class="p">(</code><code class="n">response</code><code class="p">,</code> <code class="n">queue_url</code><code class="p">)</code>&#13;
    <code class="n">LOG</code><code class="o">.</code><code class="n">info</code><code class="p">(</code><code class="n">queue_send_log_msg_resp</code><code class="p">)</code>&#13;
    <code class="k">return</code> <code class="n">response</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">send_emissions</code><code class="p">(</code><code class="n">table</code><code class="p">,</code> <code class="n">queue_name</code><code class="p">):</code>&#13;
    <code class="sd">'''Send Emissions'''</code>&#13;
&#13;
    <code class="n">items</code> <code class="o">=</code> <code class="n">scan_table</code><code class="p">(</code><code class="n">table</code><code class="o">=</code><code class="n">table</code><code class="p">)</code>&#13;
    <code class="k">for</code> <code class="n">item</code> <code class="ow">in</code> <code class="n">items</code><code class="p">:</code>&#13;
        <code class="n">LOG</code><code class="o">.</code><code class="n">info</code><code class="p">(</code><code class="n">f</code><code class="s2">"Sending item {item} to queue: {queue_name}"</code><code class="p">)</code>&#13;
        <code class="n">response</code> <code class="o">=</code> <code class="n">send_sqs_msg</code><code class="p">(</code><code class="n">item</code><code class="p">,</code> <code class="n">queue_name</code><code class="o">=</code><code class="n">queue_name</code><code class="p">)</code>&#13;
        <code class="n">LOG</code><code class="o">.</code><code class="n">debug</code><code class="p">(</code><code class="n">response</code><code class="p">)</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">lambda_handler</code><code class="p">(</code><code class="n">event</code><code class="p">,</code> <code class="n">context</code><code class="p">):</code>&#13;
    <code class="sd">'''</code>&#13;
<code class="sd">    Lambda entrypoint</code>&#13;
<code class="sd">    '''</code>&#13;
&#13;
    <code class="n">extra_logging</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"table"</code><code class="p">:</code> <code class="n">TABLE</code><code class="p">,</code> <code class="s2">"queue"</code><code class="p">:</code> <code class="n">QUEUE</code><code class="p">}</code>&#13;
    <code class="n">LOG</code><code class="o">.</code><code class="n">info</code><code class="p">(</code><code class="n">f</code><code class="s2">"event {event}, context {context}"</code><code class="p">,</code> <code class="n">extra</code><code class="o">=</code><code class="n">extra_logging</code><code class="p">)</code>&#13;
    <code class="n">send_emissions</code><code class="p">(</code><code class="n">table</code><code class="o">=</code><code class="n">TABLE</code><code class="p">,</code> <code class="n">queue_name</code><code class="o">=</code><code class="n">QUEUE</code><code class="p">)</code>&#13;
&#13;
<code class="sb">``</code><code class="err">`</code></pre>&#13;
&#13;
<p>This code does the following:</p>&#13;
<ol>&#13;
<li>&#13;
<p>Grabs company names from Amazon DynamoDB.</p>&#13;
</li>&#13;
<li>&#13;
<p>Puts the names into Amazon SQS.</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
&#13;
<p class="pagebreak-before">To test it, you can do a local test in Cloud9 (<a data-type="xref" href="#Figure-15-10">Figure 15-10</a>).</p>&#13;
&#13;
<figure><div class="figure" id="Figure-15-10">&#13;
<img alt="pydo 1510" src="assets/pydo_1510.png"/>&#13;
<h6><span class="label">Figure 15-10. </span>Local test in Cloud9</h6>&#13;
</div></figure>&#13;
&#13;
<p class="pagebreak-before">Next you can verify messages in SQS, as shown in <a data-type="xref" href="#Figure-15-11">Figure 15-11</a>.</p>&#13;
&#13;
<figure><div class="figure" id="Figure-15-11">&#13;
<img alt="pydo 1511" src="assets/pydo_1511.png"/>&#13;
<h6><span class="label">Figure 15-11. </span>SQS verification</h6>&#13;
</div></figure>&#13;
&#13;
<p class="pagebreak-before">Don’t forget to set the correct IAM role!  You need to assign the lambda an IAM role that can write messages to SQS, as shown in <a data-type="xref" href="#Figure-15-12">Figure 15-12</a>.<a data-startref="ix_ch15-asciidoc11" data-type="indexterm" id="idm46691312730024"/><a data-startref="ix_ch15-asciidoc10" data-type="indexterm" id="idm46691312729320"/><a data-startref="ix_ch15-asciidoc9" data-type="indexterm" id="idm46691312728648"/><a data-startref="ix_ch15-asciidoc8" data-type="indexterm" id="idm46691312727976"/><a data-startref="ix_ch15-asciidoc7" data-type="indexterm" id="idm46691312727304"/></p>&#13;
&#13;
<figure><div class="figure" id="Figure-15-12">&#13;
<img alt="pydo 1512" src="assets/pydo_1512.png"/>&#13;
<h6><span class="label">Figure 15-12. </span>Permission error</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="pagebreak-before less_space" data-pdf-bookmark="Wiring Up CloudWatch Event Trigger" data-type="sect2"><div class="sect2" id="idm46691313188552">&#13;
<h2>Wiring Up CloudWatch Event Trigger</h2>&#13;
&#13;
<p><a data-primary="CloudWatch event trigger" data-type="indexterm" id="idm46691312723032"/><a data-primary="data engineering" data-secondary="wiring up CloudWatch event trigger" data-type="indexterm" id="idm46691312722264"/><a data-primary="serverless data engineering" data-secondary="wiring up CloudWatch event trigger" data-type="indexterm" id="idm46691312721304"/>The final step to enable CloudWatch trigger does the following: enable timed execution of producer, and verify that messages flow into SQS, as shown in <a data-type="xref" href="#Figure-15-13">Figure 15-13</a>.</p>&#13;
&#13;
<figure><div class="figure" id="Figure-15-13">&#13;
<img alt="pydo 1513" src="assets/pydo_1513.png"/>&#13;
<h6><span class="label">Figure 15-13. </span>Configure timer</h6>&#13;
</div></figure>&#13;
&#13;
<p>You can now see messages in the SQS queue (<a data-type="xref" href="#Figure-15-14">Figure 15-14</a>).</p>&#13;
&#13;
<figure><div class="figure" id="Figure-15-14">&#13;
<img alt="pydo 1514" src="assets/pydo_1514.png"/>&#13;
<h6><span class="label">Figure 15-14. </span>SQS queue</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Creating Event-Driven Lambdas" data-type="sect2"><div class="sect2" id="idm46691312713880">&#13;
<h2>Creating Event-Driven Lambdas</h2>&#13;
&#13;
<p><a data-primary="data engineering" data-secondary="creating event-driven lambdas" data-type="indexterm" id="idm46691312712792"/><a data-primary="event-driven lambda" data-type="indexterm" id="idm46691312711752"/><a data-primary="serverless data engineering" data-secondary="creating event-driven lambdas" data-type="indexterm" id="idm46691312711080"/>With the producer lambda out of the way, next up is to create an event-driven lambda that fires asynchronously upon every message in SQS (the consumer).  The Lambda function can now fire in response to every SQS message (<a data-type="xref" href="#Figure-15-15">Figure 15-15</a>).</p>&#13;
&#13;
<figure><div class="figure" id="Figure-15-15">&#13;
<img alt="pydo 1515" src="assets/pydo_1515.png"/>&#13;
<h6><span class="label">Figure 15-15. </span>Fire on SQS event</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Reading Amazon SQS Events from AWS Lambda" data-type="sect2"><div class="sect2" id="idm46691312706712">&#13;
<h2>Reading Amazon SQS Events from AWS Lambda</h2>&#13;
&#13;
<p><a data-primary="AWS Lambda" data-secondary="reading AWS SQS events from" data-type="indexterm" id="ix_ch15-asciidoc12"/><a data-primary="AWS SQS (Simple Queuing Service)" data-secondary="reading events from AWS Lambda" data-type="indexterm" id="ix_ch15-asciidoc13"/><a data-primary="data engineering" data-secondary="reading AWS SQS events from AWS Lambda" data-type="indexterm" id="ix_ch15-asciidoc14"/><a data-primary="serverless data engineering" data-secondary="reading AWS SQS events from AWS Lambda" data-type="indexterm" id="ix_ch15-asciidoc15"/><a data-primary="SQS (Simple Queuing Service)" data-secondary="reading events from AWS Lambda" data-type="indexterm" id="ix_ch15-asciidoc16"/>The only task left is to write the code to consume the messages from SQS, process them using our API, and then write the results to S3:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">json</code>&#13;
&#13;
<code class="kn">import</code> <code class="nn">boto3</code>&#13;
<code class="kn">import</code> <code class="nn">botocore</code>&#13;
<code class="c1">#import pandas as pd</code>&#13;
<code class="kn">import</code> <code class="nn">pandas</code> <code class="kn">as</code> <code class="nn">pd</code>&#13;
<code class="kn">import</code> <code class="nn">wikipedia</code>&#13;
<code class="kn">import</code> <code class="nn">boto3</code>&#13;
<code class="kn">from</code> <code class="nn">io</code> <code class="kn">import</code> <code class="n">StringIO</code>&#13;
&#13;
<code class="c1">#SETUP LOGGING</code>&#13;
<code class="kn">import</code> <code class="nn">logging</code>&#13;
<code class="kn">from</code> <code class="nn">pythonjsonlogger</code> <code class="kn">import</code> <code class="n">jsonlogger</code>&#13;
&#13;
<code class="n">LOG</code> <code class="o">=</code> <code class="n">logging</code><code class="o">.</code><code class="n">getLogger</code><code class="p">()</code>&#13;
<code class="n">LOG</code><code class="o">.</code><code class="n">setLevel</code><code class="p">(</code><code class="n">logging</code><code class="o">.</code><code class="n">DEBUG</code><code class="p">)</code>&#13;
<code class="n">logHandler</code> <code class="o">=</code> <code class="n">logging</code><code class="o">.</code><code class="n">StreamHandler</code><code class="p">()</code>&#13;
<code class="n">formatter</code> <code class="o">=</code> <code class="n">jsonlogger</code><code class="o">.</code><code class="n">JsonFormatter</code><code class="p">()</code>&#13;
<code class="n">logHandler</code><code class="o">.</code><code class="n">setFormatter</code><code class="p">(</code><code class="n">formatter</code><code class="p">)</code>&#13;
<code class="n">LOG</code><code class="o">.</code><code class="n">addHandler</code><code class="p">(</code><code class="n">logHandler</code><code class="p">)</code>&#13;
&#13;
<code class="c1">#S3 BUCKET</code>&#13;
<code class="n">REGION</code> <code class="o">=</code> <code class="s2">"us-east-1"</code>&#13;
&#13;
<code class="c1">### SQS Utils###</code>&#13;
<code class="k">def</code> <code class="nf">sqs_queue_resource</code><code class="p">(</code><code class="n">queue_name</code><code class="p">):</code>&#13;
    <code class="sd">"""Returns an SQS queue resource connection</code>&#13;
&#13;
<code class="sd">    Usage example:</code>&#13;
<code class="sd">    In [2]: queue = sqs_queue_resource("dev-job-24910")</code>&#13;
<code class="sd">    In [4]: queue.attributes</code>&#13;
<code class="sd">    Out[4]:</code>&#13;
<code class="sd">    {'ApproximateNumberOfMessages': '0',</code>&#13;
<code class="sd">     'ApproximateNumberOfMessagesDelayed': '0',</code>&#13;
<code class="sd">     'ApproximateNumberOfMessagesNotVisible': '0',</code>&#13;
<code class="sd">     'CreatedTimestamp': '1476240132',</code>&#13;
<code class="sd">     'DelaySeconds': '0',</code>&#13;
<code class="sd">     'LastModifiedTimestamp': '1476240132',</code>&#13;
<code class="sd">     'MaximumMessageSize': '262144',</code>&#13;
<code class="sd">     'MessageRetentionPeriod': '345600',</code>&#13;
<code class="sd">     'QueueArn': 'arn:aws:sqs:us-west-2:414930948375:dev-job-24910',</code>&#13;
<code class="sd">     'ReceiveMessageWaitTimeSeconds': '0',</code>&#13;
<code class="sd">     'VisibilityTimeout': '120'}</code>&#13;
&#13;
<code class="sd">    """</code>&#13;
&#13;
    <code class="n">sqs_resource</code> <code class="o">=</code> <code class="n">boto3</code><code class="o">.</code><code class="n">resource</code><code class="p">(</code><code class="s1">'sqs'</code><code class="p">,</code> <code class="n">region_name</code><code class="o">=</code><code class="n">REGION</code><code class="p">)</code>&#13;
    <code class="n">log_sqs_resource_msg</code> <code class="o">=</code>\&#13;
      <code class="s2">"Creating SQS resource conn with qname: [</code><code class="si">%s</code><code class="s2">] in region: [</code><code class="si">%s</code><code class="s2">]"</code> <code class="o">%</code>\&#13;
    <code class="p">(</code><code class="n">queue_name</code><code class="p">,</code> <code class="n">REGION</code><code class="p">)</code>&#13;
    <code class="n">LOG</code><code class="o">.</code><code class="n">info</code><code class="p">(</code><code class="n">log_sqs_resource_msg</code><code class="p">)</code>&#13;
    <code class="n">queue</code> <code class="o">=</code> <code class="n">sqs_resource</code><code class="o">.</code><code class="n">get_queue_by_name</code><code class="p">(</code><code class="n">QueueName</code><code class="o">=</code><code class="n">queue_name</code><code class="p">)</code>&#13;
    <code class="k">return</code> <code class="n">queue</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">sqs_connection</code><code class="p">():</code>&#13;
    <code class="sd">"""Creates an SQS Connection which defaults to global var REGION"""</code>&#13;
&#13;
    <code class="n">sqs_client</code> <code class="o">=</code> <code class="n">boto3</code><code class="o">.</code><code class="n">client</code><code class="p">(</code><code class="s2">"sqs"</code><code class="p">,</code> <code class="n">region_name</code><code class="o">=</code><code class="n">REGION</code><code class="p">)</code>&#13;
    <code class="n">log_sqs_client_msg</code> <code class="o">=</code> <code class="s2">"Creating SQS connection in Region: [</code><code class="si">%s</code><code class="s2">]"</code> <code class="o">%</code> <code class="n">REGION</code>&#13;
    <code class="n">LOG</code><code class="o">.</code><code class="n">info</code><code class="p">(</code><code class="n">log_sqs_client_msg</code><code class="p">)</code>&#13;
    <code class="k">return</code> <code class="n">sqs_client</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">sqs_approximate_count</code><code class="p">(</code><code class="n">queue_name</code><code class="p">):</code>&#13;
    <code class="sd">"""Return an approximate count of messages left in queue"""</code>&#13;
&#13;
    <code class="n">queue</code> <code class="o">=</code> <code class="n">sqs_queue_resource</code><code class="p">(</code><code class="n">queue_name</code><code class="p">)</code>&#13;
    <code class="n">attr</code> <code class="o">=</code> <code class="n">queue</code><code class="o">.</code><code class="n">attributes</code>&#13;
    <code class="n">num_message</code> <code class="o">=</code> <code class="nb">int</code><code class="p">(</code><code class="n">attr</code><code class="p">[</code><code class="s1">'ApproximateNumberOfMessages'</code><code class="p">])</code>&#13;
    <code class="n">num_message_not_visible</code> <code class="o">=</code> <code class="nb">int</code><code class="p">(</code><code class="n">attr</code><code class="p">[</code><code class="s1">'ApproximateNumberOfMessagesNotVisible'</code><code class="p">])</code>&#13;
    <code class="n">queue_value</code> <code class="o">=</code> <code class="nb">sum</code><code class="p">([</code><code class="n">num_message</code><code class="p">,</code> <code class="n">num_message_not_visible</code><code class="p">])</code>&#13;
    <code class="n">sum_msg</code> <code class="o">=</code> <code class="s2">"""'ApproximateNumberOfMessages' and</code><code class="se">\</code>&#13;
<code class="s2">    'ApproximateNumberOfMessagesNotVisible' =</code><code class="se">\</code>&#13;
<code class="s2">      *** [</code><code class="si">%s</code><code class="s2">] *** for QUEUE NAME: [</code><code class="si">%s</code><code class="s2">]"""</code> <code class="o">%</code>\&#13;
        <code class="p">(</code><code class="n">queue_value</code><code class="p">,</code> <code class="n">queue_name</code><code class="p">)</code>&#13;
    <code class="n">LOG</code><code class="o">.</code><code class="n">info</code><code class="p">(</code><code class="n">sum_msg</code><code class="p">)</code>&#13;
    <code class="k">return</code> <code class="n">queue_value</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">delete_sqs_msg</code><code class="p">(</code><code class="n">queue_name</code><code class="p">,</code> <code class="n">receipt_handle</code><code class="p">):</code>&#13;
&#13;
    <code class="n">sqs_client</code> <code class="o">=</code> <code class="n">sqs_connection</code><code class="p">()</code>&#13;
    <code class="k">try</code><code class="p">:</code>&#13;
        <code class="n">queue_url</code> <code class="o">=</code> <code class="n">sqs_client</code><code class="o">.</code><code class="n">get_queue_url</code><code class="p">(</code><code class="n">QueueName</code><code class="o">=</code><code class="n">queue_name</code><code class="p">)[</code><code class="s2">"QueueUrl"</code><code class="p">]</code>&#13;
        <code class="n">delete_log_msg</code> <code class="o">=</code> <code class="s2">"Deleting msg with ReceiptHandle </code><code class="si">%s</code><code class="s2">"</code> <code class="o">%</code> <code class="n">receipt_handle</code>&#13;
        <code class="n">LOG</code><code class="o">.</code><code class="n">info</code><code class="p">(</code><code class="n">delete_log_msg</code><code class="p">)</code>&#13;
        <code class="n">response</code> <code class="o">=</code> <code class="n">sqs_client</code><code class="o">.</code><code class="n">delete_message</code><code class="p">(</code><code class="n">QueueUrl</code><code class="o">=</code><code class="n">queue_url</code><code class="p">,</code>&#13;
          <code class="n">ReceiptHandle</code><code class="o">=</code><code class="n">receipt_handle</code><code class="p">)</code>&#13;
    <code class="k">except</code> <code class="n">botocore</code><code class="o">.</code><code class="n">exceptions</code><code class="o">.</code><code class="n">ClientError</code> <code class="k">as</code> <code class="n">error</code><code class="p">:</code>&#13;
        <code class="n">exception_msg</code> <code class="o">=</code>\&#13;
          <code class="s2">"FAILURE TO DELETE SQS MSG: Queue Name [</code><code class="si">%s</code><code class="s2">] with error: [</code><code class="si">%s</code><code class="s2">]"</code> <code class="o">%</code>\&#13;
            <code class="p">(</code><code class="n">queue_name</code><code class="p">,</code> <code class="n">error</code><code class="p">)</code>&#13;
        <code class="n">LOG</code><code class="o">.</code><code class="n">exception</code><code class="p">(</code><code class="n">exception_msg</code><code class="p">)</code>&#13;
        <code class="k">return</code> <code class="bp">None</code>&#13;
&#13;
    <code class="n">delete_log_msg_resp</code> <code class="o">=</code> <code class="s2">"Response from delete from queue: </code><code class="si">%s</code><code class="s2">"</code> <code class="o">%</code> <code class="n">response</code>&#13;
    <code class="n">LOG</code><code class="o">.</code><code class="n">info</code><code class="p">(</code><code class="n">delete_log_msg_resp</code><code class="p">)</code>&#13;
    <code class="k">return</code> <code class="n">response</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">names_to_wikipedia</code><code class="p">(</code><code class="n">names</code><code class="p">):</code>&#13;
&#13;
    <code class="n">wikipedia_snippit</code> <code class="o">=</code> <code class="p">[]</code>&#13;
    <code class="k">for</code> <code class="n">name</code> <code class="ow">in</code> <code class="n">names</code><code class="p">:</code>&#13;
        <code class="n">wikipedia_snippit</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">wikipedia</code><code class="o">.</code><code class="n">summary</code><code class="p">(</code><code class="n">name</code><code class="p">,</code> <code class="n">sentences</code><code class="o">=</code><code class="mi">1</code><code class="p">))</code>&#13;
    <code class="n">df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code>&#13;
        <code class="p">{</code>&#13;
            <code class="s1">'names'</code><code class="p">:</code><code class="n">names</code><code class="p">,</code>&#13;
            <code class="s1">'wikipedia_snippit'</code><code class="p">:</code> <code class="n">wikipedia_snippit</code>&#13;
        <code class="p">}</code>&#13;
    <code class="p">)</code>&#13;
    <code class="k">return</code> <code class="n">df</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">create_sentiment</code><code class="p">(</code><code class="n">row</code><code class="p">):</code>&#13;
    <code class="sd">"""Uses AWS Comprehend to Create Sentiments on a DataFrame"""</code>&#13;
&#13;
    <code class="n">LOG</code><code class="o">.</code><code class="n">info</code><code class="p">(</code><code class="n">f</code><code class="s2">"Processing {row}"</code><code class="p">)</code>&#13;
    <code class="n">comprehend</code> <code class="o">=</code> <code class="n">boto3</code><code class="o">.</code><code class="n">client</code><code class="p">(</code><code class="n">service_name</code><code class="o">=</code><code class="s1">'comprehend'</code><code class="p">)</code>&#13;
    <code class="n">payload</code> <code class="o">=</code> <code class="n">comprehend</code><code class="o">.</code><code class="n">detect_sentiment</code><code class="p">(</code><code class="n">Text</code><code class="o">=</code><code class="n">row</code><code class="p">,</code> <code class="n">LanguageCode</code><code class="o">=</code><code class="s1">'en'</code><code class="p">)</code>&#13;
    <code class="n">LOG</code><code class="o">.</code><code class="n">debug</code><code class="p">(</code><code class="n">f</code><code class="s2">"Found Sentiment: {payload}"</code><code class="p">)</code>&#13;
    <code class="n">sentiment</code> <code class="o">=</code> <code class="n">payload</code><code class="p">[</code><code class="s1">'Sentiment'</code><code class="p">]</code>&#13;
    <code class="k">return</code> <code class="n">sentiment</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">apply_sentiment</code><code class="p">(</code><code class="n">df</code><code class="p">,</code> <code class="n">column</code><code class="o">=</code><code class="s2">"wikipedia_snippit"</code><code class="p">):</code>&#13;
    <code class="sd">"""Uses Pandas Apply to Create Sentiment Analysis"""</code>&#13;
&#13;
    <code class="n">df</code><code class="p">[</code><code class="s1">'Sentiment'</code><code class="p">]</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="n">column</code><code class="p">]</code><code class="o">.</code><code class="n">apply</code><code class="p">(</code><code class="n">create_sentiment</code><code class="p">)</code>&#13;
    <code class="k">return</code> <code class="n">df</code>&#13;
&#13;
<code class="c1">### S3 ###</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">write_s3</code><code class="p">(</code><code class="n">df</code><code class="p">,</code> <code class="n">bucket</code><code class="p">):</code>&#13;
    <code class="sd">"""Write S3 Bucket"""</code>&#13;
&#13;
    <code class="n">csv_buffer</code> <code class="o">=</code> <code class="n">StringIO</code><code class="p">()</code>&#13;
    <code class="n">df</code><code class="o">.</code><code class="n">to_csv</code><code class="p">(</code><code class="n">csv_buffer</code><code class="p">)</code>&#13;
    <code class="n">s3_resource</code> <code class="o">=</code> <code class="n">boto3</code><code class="o">.</code><code class="n">resource</code><code class="p">(</code><code class="s1">'s3'</code><code class="p">)</code>&#13;
    <code class="n">res</code> <code class="o">=</code> <code class="n">s3_resource</code><code class="o">.</code><code class="n">Object</code><code class="p">(</code><code class="n">bucket</code><code class="p">,</code> <code class="s1">'fang_sentiment.csv'</code><code class="p">)</code><code class="o">.</code>\&#13;
        <code class="n">put</code><code class="p">(</code><code class="n">Body</code><code class="o">=</code><code class="n">csv_buffer</code><code class="o">.</code><code class="n">getvalue</code><code class="p">())</code>&#13;
    <code class="n">LOG</code><code class="o">.</code><code class="n">info</code><code class="p">(</code><code class="n">f</code><code class="s2">"result of write to bucket: {bucket} with:</code><code class="se">\n</code><code class="s2"> {res}"</code><code class="p">)</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">lambda_handler</code><code class="p">(</code><code class="n">event</code><code class="p">,</code> <code class="n">context</code><code class="p">):</code>&#13;
    <code class="sd">"""Entry Point for Lambda"""</code>&#13;
&#13;
    <code class="n">LOG</code><code class="o">.</code><code class="n">info</code><code class="p">(</code><code class="n">f</code><code class="s2">"SURVEYJOB LAMBDA, event {event}, context {context}"</code><code class="p">)</code>&#13;
    <code class="n">receipt_handle</code>  <code class="o">=</code> <code class="n">event</code><code class="p">[</code><code class="s1">'Records'</code><code class="p">][</code><code class="mi">0</code><code class="p">][</code><code class="s1">'receiptHandle'</code><code class="p">]</code> <code class="c1">#sqs message</code>&#13;
    <code class="c1">#'eventSourceARN': 'arn:aws:sqs:us-east-1:561744971673:producer'</code>&#13;
    <code class="n">event_source_arn</code> <code class="o">=</code> <code class="n">event</code><code class="p">[</code><code class="s1">'Records'</code><code class="p">][</code><code class="mi">0</code><code class="p">][</code><code class="s1">'eventSourceARN'</code><code class="p">]</code>&#13;
&#13;
    <code class="n">names</code> <code class="o">=</code> <code class="p">[]</code> <code class="c1">#Captured from Queue</code>&#13;
&#13;
    <code class="c1"># Process Queue</code>&#13;
    <code class="k">for</code> <code class="n">record</code> <code class="ow">in</code> <code class="n">event</code><code class="p">[</code><code class="s1">'Records'</code><code class="p">]:</code>&#13;
        <code class="n">body</code> <code class="o">=</code> <code class="n">json</code><code class="o">.</code><code class="n">loads</code><code class="p">(</code><code class="n">record</code><code class="p">[</code><code class="s1">'body'</code><code class="p">])</code>&#13;
        <code class="n">company_name</code> <code class="o">=</code> <code class="n">body</code><code class="p">[</code><code class="s1">'name'</code><code class="p">]</code>&#13;
&#13;
        <code class="c1">#Capture for processing</code>&#13;
        <code class="n">names</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">company_name</code><code class="p">)</code>&#13;
&#13;
        <code class="n">extra_logging</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"body"</code><code class="p">:</code> <code class="n">body</code><code class="p">,</code> <code class="s2">"company_name"</code><code class="p">:</code><code class="n">company_name</code><code class="p">}</code>&#13;
        <code class="n">LOG</code><code class="o">.</code><code class="n">info</code><code class="p">(</code><code class="n">f</code><code class="s2">"SQS CONSUMER LAMBDA, splitting arn: {event_source_arn}"</code><code class="p">,</code>&#13;
          <code class="n">extra</code><code class="o">=</code><code class="n">extra_logging</code><code class="p">)</code>&#13;
        <code class="n">qname</code> <code class="o">=</code> <code class="n">event_source_arn</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="s2">":"</code><code class="p">)[</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code>&#13;
        <code class="n">extra_logging</code><code class="p">[</code><code class="s2">"queue"</code><code class="p">]</code> <code class="o">=</code> <code class="n">qname</code>&#13;
        <code class="n">LOG</code><code class="o">.</code><code class="n">info</code><code class="p">(</code><code class="n">f</code><code class="s2">"Attempting Delete SQS {receipt_handle} {qname}"</code><code class="p">,</code>&#13;
          <code class="n">extra</code><code class="o">=</code><code class="n">extra_logging</code><code class="p">)</code>&#13;
        <code class="n">res</code> <code class="o">=</code> <code class="n">delete_sqs_msg</code><code class="p">(</code><code class="n">queue_name</code><code class="o">=</code><code class="n">qname</code><code class="p">,</code> <code class="n">receipt_handle</code><code class="o">=</code><code class="n">receipt_handle</code><code class="p">)</code>&#13;
        <code class="n">LOG</code><code class="o">.</code><code class="n">info</code><code class="p">(</code><code class="n">f</code><code class="s2">"Deleted SQS receipt_handle {receipt_handle} with res {res}"</code><code class="p">,</code>&#13;
          <code class="n">extra</code><code class="o">=</code><code class="n">extra_logging</code><code class="p">)</code>&#13;
&#13;
    <code class="c1"># Make Pandas dataframe with wikipedia snippts</code>&#13;
    <code class="n">LOG</code><code class="o">.</code><code class="n">info</code><code class="p">(</code><code class="n">f</code><code class="s2">"Creating dataframe with values: {names}"</code><code class="p">)</code>&#13;
    <code class="n">df</code> <code class="o">=</code> <code class="n">names_to_wikipedia</code><code class="p">(</code><code class="n">names</code><code class="p">)</code>&#13;
&#13;
    <code class="c1"># Perform Sentiment Analysis</code>&#13;
    <code class="n">df</code> <code class="o">=</code> <code class="n">apply_sentiment</code><code class="p">(</code><code class="n">df</code><code class="p">)</code>&#13;
    <code class="n">LOG</code><code class="o">.</code><code class="n">info</code><code class="p">(</code><code class="n">f</code><code class="s2">"Sentiment from FANG companies: {df.to_dict()}"</code><code class="p">)</code>&#13;
&#13;
    <code class="c1"># Write result to S3</code>&#13;
    <code class="n">write_s3</code><code class="p">(</code><code class="n">df</code><code class="o">=</code><code class="n">df</code><code class="p">,</code> <code class="n">bucket</code><code class="o">=</code><code class="s2">"fangsentiment"</code><code class="p">)</code></pre>&#13;
&#13;
<p>You can see that one easy way to download the files is to use the AWS CLI:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">noah:/tmp <code class="nv">$ </code>aws s3 cp --recursive s3://fangsentiment/ .&#13;
download: s3://fangsentiment/netflix_sentiment.csv to ./netflix_sentiment.csv&#13;
download: s3://fangsentiment/google_sentiment.csv to ./google_sentiment.csv&#13;
download: s3://fangsentiment/facebook_sentiment.csv to ./facebook_sentiment.csv</pre>&#13;
&#13;
<p>OK, so what did we accomplish? <a data-type="xref" href="#Figure-15-16">Figure 15-16</a> shows our serverless AI data engineering pipeline.</p>&#13;
&#13;
<figure><div class="figure" id="Figure-15-16">&#13;
<img alt="pydo 1516" src="assets/pydo_1516.png"/>&#13;
<h6><span class="label">Figure 15-16. </span>Serverless AI data engineering pipeline</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Conclusion" data-type="sect1"><div class="sect1" id="idm46691313212328">&#13;
<h1>Conclusion</h1>&#13;
&#13;
<p>Data engineering is an evolving job title, and it benefits greatly from strong DevOps skills.  The DevOps best practices of Microservices, Continous Delivery, Infrastructure as Code, and Monitoring and Logging play a tremendous role in this category.  Often by leveraging cloud-native technologies, it makes hard problems possible, and simple problems effortless.</p>&#13;
&#13;
<p class="pagebreak-before">Here are some next steps to continue on a journey of mastery with data engineering.  Learn serverless technology.  It doesn’t matter what the cloud is, learn it!  This environment is the future, and data engineering, in particular, is well suited to capitalize on this trend<a data-startref="ix_ch15-asciidoc16" data-type="indexterm" id="idm46691312410136"/><a data-startref="ix_ch15-asciidoc15" data-type="indexterm" id="idm46691312409432"/><a data-startref="ix_ch15-asciidoc14" data-type="indexterm" id="idm46691311988568"/><a data-startref="ix_ch15-asciidoc13" data-type="indexterm" id="idm46691311987896"/><a data-startref="ix_ch15-asciidoc12" data-type="indexterm" id="idm46691311987224"/>.<a data-startref="ix_ch15-asciidoc6" data-type="indexterm" id="idm46691311986424"/><a data-startref="ix_ch15-asciidoc5" data-type="indexterm" id="idm46691311985720"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Exercises" data-type="sect1"><div class="sect1" id="idm46691311984792">&#13;
<h1>Exercises</h1>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Explain what Big Data is and what its key characteristics are.</p>&#13;
</li>&#13;
<li>&#13;
<p>Use small data tools in Python to solve common problems.</p>&#13;
</li>&#13;
<li>&#13;
<p>Explain what a data lake is and how it is used.</p>&#13;
</li>&#13;
<li>&#13;
<p>Explain the appropriate use cases for different types of purpose-built databases.</p>&#13;
</li>&#13;
<li>&#13;
<p>Build a serverless data engineering pipeline in Python.</p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Case Study Question" data-type="sect1"><div class="sect1" id="idm46691311978728">&#13;
<h1>Case Study Question</h1>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Using the same architecture shown in this chapter, build an end-to-end serverless data engineering pipeline that scrapes a website using Scrapy, Beautiful Soup, or a similar library, and sends image files to Amazon Rekognition to be analyzed.  Store the results of the Rekognition API call in Amazon DynamoDB.  Run this job on a timer once a day.<a data-startref="ix_ch15-asciidoc0" data-type="indexterm" id="idm46691311976392"/></p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>