- en: Chapter 8\. Online Inference with Ray Serve
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Edward Oakes
  prefs: []
  type: TYPE_NORMAL
- en: Online Inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In previous chapters you’ve learned how to use Ray to process data, train machine
    learning (ML) models, and apply them in a batch inference setting. However, many
    of the most exciting use cases for machine learning involve “online inference:”
    using ML models to enhance API endpoints that users interact with directly or
    indirectly. Online inference is important in situations where latency matters:
    you can’t simply apply models to data behind the scenes and serve the results.
    There are many real-world examples of use cases where online inference can provide
    a lot of value:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Recommendation systems**: Providing recommendations for products (e.g., online
    shopping) or content (e.g., social media) is a bread-and-butter use case for machine
    learning. While it’s possible to do this in an offline manner, recommendation
    systems often benefit from reacting to users’ preferences in real time. This requires
    performing online inference using recent behavior as a key feature.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Chat bots**: Online services often have real-time chat windows to provide
    support to customers from the comfort of their keyboard. Traditionally, these
    chat windows were staffed by customer support staff but there has been a recent
    trend to reduce labor costs and improve time-to-resolution by replacing them with
    ML-powered chat bots that can be online 24/7\. These chat bots require a sophisticaed
    mix of multiple machine learning techniques and must be able to respond to customer
    input in real time.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Estimating arrival times**: Ride sharing, navigation, and food delivery services
    all rely on being able to provide an accurate estimation for arrival times (e.g.,
    for your driver, yourself, or your dinner). Providing accurate estimates is very
    difficult because it requires accounting for real-world factors such as traffic
    patterns, weather, and accidents. Estimates are also often refreshed many times
    over the course of one trip.'
  prefs: []
  type: TYPE_NORMAL
- en: 'These are just a few examples where applying machine learning in an online
    fashion can provide a lot of value in application domains that are traditionally
    very difficult (imagine hand-writing logic to estimate arrival time!). The list
    of applications goes on: a number of nascent domains such as self-driving cars,
    robotics, video-processing pipelines are also being redefined by machine learning.
    All of these applications share one key feature: latency is crucial. In the case
    of online services, low latency is paramount to providing a good user experience
    and for applications interacting with the real world such as robotics or self-driving
    cars, higher latency can have even stronger implications in safety or correctness.'
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will provide a gentle introduction to Ray Serve, a Ray-native library
    that enables building online inference applications on top of Ray. First, we will
    discuss the challenges of online inference that Ray Serve addresses. Then, we’ll
    cover the architecture of Ray Serve and introduce its core set of functionality.
    Finally, we will use Ray Serve to build an end-to-end online inference API consisting
    of multiple natural language processing models.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges of online inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section we discussed that the main goal of online inference
    is to interact with machine learning models with low latency. However, this has
    long been a key requirement for API backends and web servers, so a natural question
    is: what’s different about serving machine learning models?'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. ML models are compute intensive
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Many of the challenges in online inference follow from one key characteristic:
    machine learning models are very compute intensive. Compared to traditional web
    serving where requests are primarily handled by I/O intensive database queries
    or other API calls, most machine learning models boil down to performing many
    linear algebra computations, be it to provide a recommendation, estimate an arrival
    time, or detect an object in an image. This is especially true for the recent
    trend of “deep learning,” where the number of weights and computations a single
    model performs is growing larger and larger over time. Often, deep learning models
    can also benefit significantly from using specialized hardware such as GPUs or
    TPUs, which have special-purpose instructions optimized for machine learning computations
    and enable vectorized computations across multiple inputs in parallel.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Many online inference applications are required to be run 24/7 by nature. When
    combined with the fact that machine learning models are compute intensive, operating
    online inference services can be very expensive, requiring allocating many CPUs
    and GPUs at all times. The primary challenges of online inference boil down to
    serving models in a way that minimizes end-to-end latency while also reducing
    cost. There are a few key properties that online inference systems provide in
    order to satisfy these requirements: - Support for specialized hardware such as
    GPUs and TPUs. - The ability to scale up and down the resources used for a model
    in response to request load. - Support for request batching to take advantage
    of vectorized computations.'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. ML models aren’t useful in isolation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Often when machine learning is discussed in the academic or research setting,
    the focus is on an individual, isolated task such as object recognition or classification.
    However, in the real world applications are not usually so clear cut and well-defined.
    Instead, a combination of multiple ML models and business logic is required to
    solve a problem end-to-end. For example, consider a product recommendation use
    case. While there are a multitude of known ML techniques we could apply to the
    core problem of making a recommendation, there are also a lot of equally important
    challenges around the edges, many of which will be specific to each use case:
    - Validating inputs and outputs to ensure the result returned to the user makes
    sense semantically. Often, we may have some manual rules such as avoiding returning
    the same recommendation to a user multiple times in succession. - Fetching up-to-date
    information about the user and available products and converting it into features
    for the model (in some cases, this may be performed by an online feature store).
    - Combining the results of multiple models using manual rules such as filtering
    to the top results or selecting the model with highest confidence.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Implementing an online inference API requires the ability to integrate all
    of these pieces together into one unified service. Therefore, it’s important to
    have the flexibility to compose multiple models together along with custom business
    logic. These pieces can’t really be viewed completely in isolation: the “glue”
    logic often needs to evolve alongside the models themselves.'
  prefs: []
  type: TYPE_NORMAL
- en: An Introduction to Ray Serve
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Ray Serve is a scalable compute layer for serving machine learning models on
    top of Ray. Serve is framework-agnostic, meaning that it isn’t tied to a specific
    machine learning library, but rather treats models as ordinary Python code. Additionally,
    it allows you to flexibly combine normal Python business logic alongside machine
    learning models. This makes it possible to build online inference services completely
    end-to-end: a Serve application could validate user input, query a database, perform
    inference scalably across multiple ML models, and combine, filter, and validate
    the output all in the process of handling a single inference request. Indeed,
    combining the results of multiple machine learning models is one of the key strengths
    of Ray Serve, as you’ll see later in the chapter as we explore common multi-model
    serving patterns.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While being flexible in nature, Serve has purpose-built features for compute-heavy
    machine learning models, enabling dynamic scaling and resource allocation to ensure
    that request load can be handled efficiently across many CPUs and/or GPUs. Here,
    Serve inherits a lot of benefits from being built on top of Ray: it’s scalable
    to hundreds of machines, offers flexible scheduling policies, and offers low-overhead
    communication across processes using Ray’s core APIs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This section will incrementally introduce core funcionality from Ray Serve
    with a focus on how it helps to address the challenges of online inference as
    outlined above. To follow along with the code samples in this section, you’ll
    need the following Python packages installed locally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Running the examples assumes that you have the code saved locally in a file
    named `app.py` in the current working directory.
  prefs: []
  type: TYPE_NORMAL
- en: Architectural overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ray Serve is built on top of Ray, so it inherits a lot of benefits such as scalability,
    low overhead communication, an API well-suited to parallelism, and the ability
    to leverage shared memory via the object store. The core primitive in Ray Serve
    is a **deployment**, which you can think of as a managed group of Ray actors that
    can be addressed together and will handle requests load-balanced across them.
    Each actor in a deployment is called a **replica** in Ray Serve. Often, a deployment
    will map one-to-one with a machine learning model, but deployments can contain
    arbitrary Python code so they might also house business logic.
  prefs: []
  type: TYPE_NORMAL
- en: Ray Serve enables exposing deployments over HTTP and defining the input parsing
    and output logic. However, one of the most important features of Ray Serve is
    that deployments can also call into each other directly using a native Python
    API, which will translate to direct actor calls between the replicas. This enables
    flexible, high performance composition of models and business logic; you’ll see
    this in action later in the section.
  prefs: []
  type: TYPE_NORMAL
- en: Under the hood, the deployments making up a Ray Serve application are managed
    by a centralized **controller** actor. This is a detached actor that is managed
    by Ray and will be restarted upon failure. The controller is in charge of creating
    and updating replica actors, broadcasting updates to other actors in the system,
    and performing health checking and failure recovery. If a replica or an entire
    Ray node crashes for any reason, the controller will detect the failures and ensure
    that the actors are recovered and can continue serving traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Defining a basic HTTP endpoint
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section will introduce Ray Serve by defining a simple HTTP endpoint wrapping
    a single ML model. The model we’ll deploy is a sentiment classifier: given a text
    input, it will predict if the output had a positive or negative sentiment. We’ll
    be using a pretrained sentiment classifier from the [Hugging Face](https://huggingface.co/)
    `transformers` library, which provides a simple Python API for pretrained models
    that will abstract away the details of the model and allow us to focus on the
    serving logic. To deploy this model using Ray Serve, we need to define a Python
    class and turn it into a Serve **deployment** using the `@serve.deployment` decorator.
    The decorator allows us to pass a number of useful options to configure the deployment;
    we will explore some of those options later in the chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-1\.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'There are a few important points to note here. First, we instantiate our model
    in the constructor of the class. This model may be very large, so downloading
    it and loading it into memory can be slow (10s of seconds or longer). In Ray Serve,
    the code in the constructor will only be run once in each replica on startup and
    any properties can be cached for future use. Second, we define the logic to handle
    a request in the `__call__` method. This takes a `Starlette` HTTP request as input
    and can return any JSON-serializable output. In this case, we’ll return a single
    string from the output of our model: `"POSITIVE"` or `"NEGATIVE"`.'
  prefs: []
  type: TYPE_NORMAL
- en: Once a deployment is defined, we use the `.bind()` API to instantiate a copy
    of it. This is where we can pass optional arguments to the constructor to configure
    the deployment (such as a remote path to download model weights from). Note that
    this doesn’t actually run the deployment, but just packages it up with its arguments
    (this will be more important later when we combine multiple models together).
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-2\.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can run the bound deployment using the `serve.run` Python API or corresponding
    `serve run` CLI command. Assuming you save the above code in a file called `app.py`,
    you can run it locally with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This will instantiate a single replica of our deployment and host it behind
    a local HTTP server. To test it, we can use the Python `requests` package:'
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-3\.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Testing the sentiment classifier on a sample input text of `"Hello friend!"`,
    it correctly classifies the text as positive!
  prefs: []
  type: TYPE_NORMAL
- en: 'This example is effectively the “hello world” of Ray Serve: we deployed a single
    model behind a basic HTTP endpoint. Note, however, that we had to manually parse
    the input HTTP request and feed it into our model. For this basic example it was
    just a single line of code, but real world applications often take a more complex
    schema as input and hand-writing HTTP logic can be tedious and error prone. To
    enable writing more expressive HTTP APIs, Serve integrates with the [FastAPI](https://fastapi.tiangolo.com/)
    Python framework.'
  prefs: []
  type: TYPE_NORMAL
- en: A Serve deployment can wrap a `FastAPI` app, making use of its expressive APIs
    for parsing inputs and configuring HTTP behavior. In the following example, we
    rely on `FastAPI` to handle parsing the `input_text` query parameter for us, allowing
    to remove the boilerplate parsing code.
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-4\.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The modified deployment should have exactly the same behavior on the example
    above (try it out using `serve run`!), but will gracefully handle invalid inputs.
    These may look like small benefits for this simple example, but for more complex
    APIs this can make a world of difference. We won’t delve deeper into the details
    of FastAPI here, but for more details on its features and syntax check out their
    excellent [documentation](https://fastapi.tiangolo.com/).
  prefs: []
  type: TYPE_NORMAL
- en: Scaling and resource allocation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As mentioned above, machine learning models are often compute hungry. Therefore,
    it’s important to be able to allocate the correct amount of resources to your
    ML application to handle request load while minimizing cost. Ray Serve allows
    you to adjust the resources dedicated to a deployment in two ways: by tuning the
    number of **replicas** of the deployment and tuning the resources allocated to
    each replica. By default, a deployment consists of a single replica that uses
    a single CPU but these parameters can be adjusted in the `@serve.deployment` decorator
    (or using the corresponding `deployment.options` API).'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s modify the `SentimentClassifier` example from above to scale out to multiple
    replicas and adjust the resource allocation so that each replica uses 2 CPUs instead
    of 1 (in practice, you would want to profile and understand your model in order
    to set this paramter correctly). We’ll also add a print statement to log the process
    ID of the process handling each request to show that the requests are now load
    balanced across two replicas.
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-5\.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this new version of our classifier with `serve run app:scaled_deployment`
    and querying it using `requests` as we did above, you should see that there are
    now two copies of the model handling requests! We could easily scale up to tens
    or hundreds of replicas just by tweaking `num_replicas` in the same way: Ray enables
    scaling to hundreds of machines and thousands of processes in a single cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example we scaled out to a static number of replicas with each replica
    consuming two full CPUs, but Serve also supports more expressive resource allocation
    policies: - Enabling a deployment to use GPUs simpliy requires setting `num_gpus`
    instead of `num_cpus`. Serve supports the same resource types as Ray core, so
    deployments can also use TPUs or other custom resources. - Resources can be **fractional**,
    allowing replicas to be efficiently bin-packed. For example, if a single replica
    doesn’t saturate a full GPU you can allocate `num_gpus=0.5` to it and multiplex
    with another model. - For applications with varying request load, a deployment
    can be configured to dynamically autoscale the number of replicas based on the
    number of requests currently in flight.'
  prefs: []
  type: TYPE_NORMAL
- en: For more details about resource allocation options, refer to the latest Ray
    Serve documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Request batching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Many machine learning models can be efficiently **vectorized**, meaning that
    multiple computations can be run in parallel more efficiently than running them
    sequentially. This is especially beneficial when running models on GPUs which
    are purpose-built for efficiently performing many computations in parallel. In
    the context of online inference this offers a path for optimization: serving multiple
    requests (possibly from different sources) in parallel can drastically improve
    the throughput of the system (and therefore save cost).'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two high-level strategies to take advantage of request batching:
    **client-side** batching and **server-side** batching. In client-side batching,
    the server accepts multiple inputs in a single request and clients include logic
    to send them in batches instead of one at a time. This is mostly useful in situations
    where a single client is frequently sending many inference requests. Server-side
    batching, in contrast, enables the server to batch multiple requests without requiring
    any modification on the client. This can also be used to batch requests across
    multiple clients, which enables efficient batching even in situations with many
    clients that each sends relatively few requests.'
  prefs: []
  type: TYPE_NORMAL
- en: Ray Serve offers a built-in utility for server-side batching, the `@serve.batch`
    decorator, that requires just a few code changes. This batching support uses Python’s
    `asyncio` capabilities to enqueue multiple requests into a single function call.
    The function should take in a list of inputs and return the corresponding list
    of outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Once again, let’s revisit the sentiment classifier from earlier and this time
    modify it to perform server-side batching. The underlying Hugging Face `pipeline`
    supports vectorized inference, all we need to do is pass a list of inputs and
    it will return the corresponding list of outputs. We’ll split out the call to
    the classifier into a new method, `classify_batched`, that will take a list of
    input texts as input, perform inference across them, and return the outputs in
    a formatted list. `classify_batched` will use the `@serve.batch` decorator to
    automatically perform batching. The behavior can be configured using the `max_batch_size`
    and `batch_timeout_wait_s` parameters, here we’ll set the max batch size to 10
    and wait for up to 100ms.
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-6\.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Notice that both the `classify` and `classify_batched` methods now use Python’s
    `async` and `await` syntax, meaning that many of these calls can run concurrently
    in the same process. To test this behavior, we’ll use the `serve.run` Python API
    to send requests using the Python-native handle to our deployment.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The handle returned by `serve.run` can be used to send multiple requests in
    parallel: here, we send 10 requests in parallel and wait for them all to return.
    Without batching, each request would be handled sequentially, but because we enabled
    batching we should see the requests handled all at once (evidenced by batch size
    printed in the `classify_batched` method). Running on a CPU, this might be marginally
    faster than running sequentially, but running the same handler on a GPU we would
    observe a significant speedup for the batched version.'
  prefs: []
  type: TYPE_NORMAL
- en: Multi-model inference graphs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Up until now, we’ve been deploying and querying a single Serve deployment wrapping
    one ML model. As described earlier, machine learning models are not often useful
    in isolation: many applications require multiple models to be composed together
    and for business logic to be intertwined with machine learning. The real power
    of Ray Serve is in its ability to compose multiple models along with regular Python
    logic into a single application. This is possible by instantiating many different
    deployments and passing reference between them. Each of these deployments can
    use all of the features we’ve discussed up until this point: they can be independently
    scaled, perform request batching, and use flexible resource allocations.'
  prefs: []
  type: TYPE_NORMAL
- en: This section provides illustrative examples of common multi-model serving patterns
    but doesn’t actually contain any ML models in order to focus on the core capabilities
    that Serve provides. Later in the chapter we will explore an end-to-end multi-model
    inference graph containing ML models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Core feature: binding multiple deployments'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'All types of multi-model inference graphs in Ray Serve center around the ability
    to pass a reference to one deployment into the constructor of another. In order
    to do this, we use another feature of the `.bind()` API: a bound deployment can
    be passed to another call to `.bind()` and this will resolve to a “handle” to
    the deployment at runtime. This enables deployments to be deployed and instantiated
    independently and then call each other at runtime. Below is the most basic example
    of a multi-deployment Serve application.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-7\.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the downstream model is passed into the “driver” deployment.
    Then at runtime the driver deployment calls into the downstream model. The driver
    could take any number of models passed in, and the downstream model could even
    take other downstream models of its own.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pattern 1: Pipelining'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first common multi-model pattern among machine learning applications is
    “pipelining:” calling multiple models in sequence, where the input of one model
    depends on the output of the previous. Image processing, for example, often consists
    of a pipeline consistenting of multiple stages of transformations such as cropping,
    segmentation, and object recognition or optical character recognition (OCR). Each
    of these models may have different properties with some of them being lightweight
    transformations that can run on a CPU and others being heavyweight deep learning
    models that run on a CPU.
  prefs: []
  type: TYPE_NORMAL
- en: Such pipelines can easily be expressed using Serve’s API. Each stage of the
    pipeline is defined as an independent deployment and each deployment is passed
    into a top-level “pipeline driver.” In the example below, we pass two deployments
    into a top-level driver and the driver calls them in sequence. Note that there
    could be many requests to the driver happening concurrently, therefore it is possible
    to efficiently saturate all stages of the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-8\.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'To test this example, you can once again use the `serve run` API. Sending a
    test request to the pipeline returns `"''input|val1|val2''"` as output: each downstream
    “model” appended its own value to construct the final result. In practice, each
    of these deployments could be wrapping its own ML model and a single request may
    flow across many physical nodes in a cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pattern 2: Broadcasting'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In addition to sequentially chaining models together, it’s often useful to
    perform inference on multiple models in parallel. This could be to perform “ensembling,”
    or combining the results of multiple independent models into a single result,
    or in a situation where different models may perform better on different inputs.
    Often the results of the models need to be combined in some way into a final result:
    either simply concatenated together or maybe a single result chosen from the lot.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is expressed very similarly to the pipelining example: a number of downstream
    models are passed into a top-level “driver.” In this case, it’s important that
    we call the models in parallel: waiting for the result of each before calling
    the next would dramatically increase the overall latency of the system.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-9\.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Testing this endpoint after running it once again with `serve run` returns `'["val1",
    "val2"]'`, the combined output of the two models called in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pattern 3: Conditional logic'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finally, while many machine learning applications fit roughly into one of the
    above patterns, often having static control flow can be very limiting. Take, for
    instance, the example of building a service to extract license plate numbers from
    user-uploaded images. In this case, we’ll likely need to build an image processing
    pipeline as discussed above, but we also don’t simply want to feed any image into
    the pipeline blindly. If the user uploads something other than a car or with an
    image that is low quality, we likely want to short circuit, avoid calling into
    the heavy-weight and expensive pipeline, and provide a useful error message. Similarly,
    in a product recommendation use case we may want to select a downstream model
    based on user input or the result of an intermediate model. Each of these examples
    requires embedding custom logic alongside our ML models.
  prefs: []
  type: TYPE_NORMAL
- en: We can accomplish this trivially using Serve’s multi-model API because our computation
    graph is defined as ordinary Python logic rather than as a statically-defined
    graph. For instance, in the example below, we use a simple random number generator
    (RNG) to decide which of two downstream models to call into. In a real-world example,
    the RNG could be replaced with business logic, a database query, or the result
    of an intermediate model.
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-10\.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Each call to this endpoint returns either `"val1"` or `"val2"` with 50/50 probability.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying on Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'TODO: the Kubernetes deployment story for Ray Serve is currently being reworked,
    will update this section once it’s finalized.'
  prefs: []
  type: TYPE_NORMAL
- en: 'End-to-end Example: Building an NLP-powered API'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll use Ray Serve to build an end-to-end natural language
    processing (NLP) pipeline hosted for online inference. Our goal will be to provide
    a Wikipedia summarization endpoint that will leverage multiple NLP models and
    some custom logic to provide a succinct summary of the most relevant Wikipedia
    page for a given search term.
  prefs: []
  type: TYPE_NORMAL
- en: 'This task will bring together many of the concepts and features discussed above:
    - We’ll be combining custom business logic along with multiple machine learning
    models. - The inference graph will consist of all three multi-model patterns discussed
    above: pipelining, broadcasting, and conditional logic. - Each model will be hosted
    as a separate Serve deployment, so they can be independently scaled and given
    their own resource allocation. - One of the models will leverage vectorized computation
    via batching. - The API will be defined using Ray Serve’s `FastAPI` for input
    parsing and defining our output schema.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our online inference pipeline will be structured as follows: - The user will
    provide a keyword search term. - We’ll fetch the content for the most relevant
    Wikipedia article for the search term. - A sentiment analysis model will be applied
    to the article. Anything with a “negative” sentiment will be rejected and we’ll
    return early. - The article content will be broadcast to summarizer and named
    entity recognition models. - We’ll return a composed result based on the summarizer
    and named entity recognition outputs.'
  prefs: []
  type: TYPE_NORMAL
- en: This pipeline will be exposed over HTTP and return the results in a structured
    format. By the end of this section, we’ll have the pipeline running end-to-end
    locally and ready to scale up on a cluster. Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 0: Install dependencies'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we dive into the code, you’ll need the following Python packages installed
    locally in order to follow along:.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Additionally, in this section we’ll assume that all of the code samples are
    available locally in a file called `app.py` so we can run the deployments using
    `serve run` from the same directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Fetching content & preprocessing'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step is to fetch the most relevant page from Wikipedia given a user-provided
    search term. For this, we will leverage the `wikipedia` package on PyPI to do
    the heavy lifting. We’ll first search for the term, then select the top result
    and return its page content. If no results are found, we’ll return `None` — this
    edge case will be handled later when we define the API.
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-11\.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 2: NLP models'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, we need to define the ML models that will do the heavy lifting of our
    API. As we did in the introduction section, we’ll be using the [Hugging Face](https://huggingface.co/)
    `transformers` library as it provides convenient APIs to pretrained state-of-the-art
    ML models, so we can focus on the serving logic.
  prefs: []
  type: TYPE_NORMAL
- en: The first model we’ll use is a sentiment classifier, the same one we used in
    the examples above. The deployment for this model will take advantage of vectorized
    computations using Serve’s batching API.
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-12\.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We’ll also use a text summarization model to provide a succinct summary for
    the selected article. This model takes an optional “max_length” argument to cap
    the length of the summary. Because we know this is the most computationally expensive
    of the models, we set `num_replicas=2` — that way, if we have many requests coming
    in at the same time, it can keep up with the throughput of the other models. In
    practice, we may need more replicas to keep up with the input load, but we could
    only know that from profiling and monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-13\.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The final model in our pipeline will be a named entity recognition model: this
    will attempt extract named entities from the text. Each result will have a confidence
    score, so we can set a threshold to only accept results above a certain threshold.
    We may also want to cap the total number of entities returned. The request handler
    for this deployment calls the model, then uses some basic business logic to enforce
    the provided confidence threshold and limit on the number of entities.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-14\.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 3: HTTP handling and driver logic'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the input preprocessing and ML models defined, we’re ready to define the
    HTTP API and driver logic. First, we define the schema of the response that we’ll
    return from the API using [Pydantic](https://pydantic-docs.helpmanual.io/). The
    response includes whether or not the request was successful and a status message
    in addition to our summary and named entities. This will allow us to return a
    helpful response in error conditions such as when no result is found or the sentiment
    analysis comes back as negative.
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-15\.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Next, we need to define the actual control flow logic that will run in the driver
    deployment. The driver will not do any of the actual heavy lifting itself, but
    instead call into our three downstream model deployments and interpret their results.
    It will also house the FastAPI app definition, parsing the input and returning
    the correct `Response` model based on the results of the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-16\.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'In the body of the main handler, we first fetch the page content using our
    `fetch_wikipedia_page` logic (if no result is found, an error is returned). Then,
    we call into the sentiment analysis model. If this returns negative, we terminate
    early and return an error to avoid calling the other expensive ML models.. Finally,
    we broadcast the article contents to both the summary and named entity recognition
    models in parallel. The results of the two models are stitched together into the
    final response and we return success. Remember that we may have many calls to
    this handler running concurrently: the calls to the downstream models don’t block
    the driver and it could coordinate calls to many replicas of the heavyweight models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: Putting it all together'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At this point, all of the core logic is defined. All that’s left is to bind
    the graph of deployments together and run it!
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-17\.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we need to instantiate each of the deployments with any relevant input
    arguments. For example, here we pass a threshold and limit for the entity recognition
    model. The most important piece is we pass a reference to each of the three models
    into the driver so it can coordinate the computation. Now that we’ve defined the
    full NLP pipeline, we can run it using `serve run`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This will deploy each of the four deployments locally and make the driver available
    at `http://localhost:8000/`. We can query the pipeline using the `requests` to
    see it in action. First, let’s try querying for an entry on Ray Serve.
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-18\.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Unfortunately, this page doesn’t exist yet! The first chunk of validation business
    logic kicks in and returns a “No pages found” message. Let’s try finding looking
    for something more common:'
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-19\.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Maybe we were just interested in learning about history, but this article was
    a bit too negative for our sentiment classifier. Let’s try something more neutral
    this time — what about science?
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-20\.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This example successfully ran through the full pipeline: the API responded
    with a cogent summary of the article and a list of relevant named entities.'
  prefs: []
  type: TYPE_NORMAL
- en: To recap, in this section we were able to build an online natural language processing
    API using Ray Serve. This inference graph consisted of multiple machine learning
    models in addition to custom business logic and dynamic control flow. Each model
    can be independently scaled and have its own resource allocation, and we can exploit
    vectorized computations using server-side batching. Now that we were able to test
    the API locally, the next step would be to deploy to production. Ray Serve makes
    it easy to deploy on Kubernetes or other cloud provider offerings using the Ray
    cluster launcher, and we could easily scale up to handle many users by tweaking
    the resource allocations for our deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: About the Author
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Max Pumperla** is a data science professor and software engineer located
    in Hamburg, Germany. He’s an active open source contributor, maintainer of several
    Python packages, author of machine learning books and speaker at international
    conferences. As head of product research at Pathmind Inc. he’s developing reinforcement
    learning solutions for industrial applications at scale using Ray. Pathmind works
    closely with the AnyScale team and is a power user of Ray’s RLlib, Tune and Serve
    libraries. Max has been a core developer of DL4J at Skymind, helped grow and extend
    the Keras ecosystem, and is a Hyperopt maintainer.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Edward Oakes**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Richard Liaw**'
  prefs: []
  type: TYPE_NORMAL
