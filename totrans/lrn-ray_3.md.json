["```py\npip install \"ray[rllib]\"==1.9.0\n```", "```py\nimport gym\n\nclass Env:\n\n    action_space: gym.spaces.Space\n    observation_space: gym.spaces.Space  ![1](assets/1.png)\n\n    def step(self, action):  ![2](assets/2.png)\n        ...\n\n    def reset(self):  ![3](assets/3.png)\n        ...\n\n    def render(self, mode=\"human\"):  ![4](assets/4.png)\n        ...\n```", "```py\n# Original definition of `Environment` and `Discrete` go here.\n\nimport gym\nfrom gym.spaces import Discrete  ![1](assets/1.png)\n\nclass GymEnvironment(Environment, gym.Env):  ![2](assets/2.png)\n    def __init__(self, *args, **kwargs):\n        \"\"\"Make our original `Environment` a gym `Env`.\"\"\"\n        super().__init__(*args, **kwargs)\n\ngym_env = GymEnvironment()\n```", "```py\n# maze.yml\nmaze_env:\n    env: maze_gym_env.GymEnvironment  ![1](assets/1.png)\n    run: DQN\n    checkpoint_freq: 1 ![2](assets/2.png)\n    stop:\n        timesteps_total: 10000  ![3](assets/3.png)\n```", "```py\n rllib train -f maze.yml\n```", "```py\nrllib evaluate ~/ray_results/maze_env/DQN_maze_gym_env.Environment_0ae8d_00000_\n0_2022-02-08_13-52-59/checkpoint_000010/checkpoint-10\\\n  --run DQN\\\n  --env maze_gym_env.Environment\\\n  --steps 100\n```", "```py\nEpisode #1: reward: 1.0\nEpisode #2: reward: 1.0\nEpisode #3: reward: 1.0\n...\nEpisode #13: reward: 1.0\n```", "```py\nfrom ray.tune.logger import pretty_print\nfrom maze_gym_env import GymEnvironment\nfrom ray.rllib.agents.dqn import DQNTrainer\n\ntrainer = DQNTrainer(env=GymEnvironment, config={\"num_workers\": 4})  ![1](assets/1.png)\n\nconfig = trainer.get_config()  ![2](assets/2.png)\nprint(pretty_print(config))\n\nfor i in range(10):\n    result = trainer.train()  ![3](assets/3.png)\n\nprint(pretty_print(result))  ![4](assets/4.png)\n```", "```py\n...\nepisode_reward_max: 1.0\nepisode_reward_mean: 1.0\nepisode_reward_min: 1.0\nepisodes_this_iter: 15\nepisodes_total: 19\n...\ntimesteps_total: 10000\ntraining_iteration: 10\n...\n```", "```py\ncheckpoint = trainer.save()  ![1](assets/1.png)\nprint(checkpoint)\n\nevaluation = trainer.evaluate(checkpoint)  ![2](assets/2.png)\nprint(pretty_print(evaluation))\n\nrestored_trainer = DQNTrainer(env=GymEnvironment)\nrestored_trainer.restore(checkpoint)  ![3](assets/3.png)\n```", "```py\n~/ray_results/DQN_GymEnvironment_2022-02-09_10-19-301o3m9r6d/checkpoint_000010/\ncheckpoint-10 evaluation:\n  ...\n  episodes_this_iter: 5\n  hist_stats:\n    episode_lengths:\n    - 8\n    - 8\n    ...\n```", "```py\nenv = GymEnvironment()\ndone = False\ntotal_reward = 0\nobservations = env.reset()\n\nwhile not done:\n    action = trainer.compute_single_action(observations)  ![1](assets/1.png)\n    observations, reward, done, info = env.step(action)\n    total_reward += reward\n```", "```py\naction = trainer.compute_actions({\"obs_1\": observations, \"obs_2\": observations})\nprint(action)\n# {'obs_1': 0, 'obs_2': 1}\n```", "```py\npolicy = trainer.get_policy()\nprint(policy.get_weights())\n\nmodel = policy.model\n```", "```py\nworkers = trainer.workers\nworkers.foreach_worker(lambda remote_trainer: remote_trainer.get_policy().get_weights())\n```", "```py\nmodel.base_model.summary()\n```", "```py\nModel: \"model\"\n____________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to\n====================================================================================\nobservations (InputLayer)       [(None, 25)]         0\n____________________________________________________________________________________\nfc_1 (Dense)                    (None, 256)          6656        observations[0][0]\n____________________________________________________________________________________\nfc_out (Dense)                  (None, 256)          65792       fc_1[0][0]\n____________________________________________________________________________________\nvalue_out (Dense)               (None, 1)            257         fc_1[0][0]\n====================================================================================\nTotal params: 72,705\nTrainable params: 72,705\nNon-trainable params: 0\n____________________________________________________________________________________\n```", "```py\nfrom ray.rllib.models.preprocessors import get_preprocessor\nenv = GymEnvironment()\nobs_space = env.observation_space\npreprocessor = get_preprocessor(obs_space)(obs_space)  ![1](assets/1.png)\n\nobservations = env.reset()\ntransformed = preprocessor.transform(observations).reshape(1, -1)  ![2](assets/2.png)\n\nmodel_output, _ = model.from_batch({\"obs\": transformed})  ![3](assets/3.png)\n```", "```py\nq_values = model.get_q_value_distributions(model_output)  ![1](assets/1.png)\nprint(q_values)\n\naction_distribution = policy.dist_class(model_output, model)  ![2](assets/2.png)\nsample = action_distribution.sample()  ![3](assets/3.png)\nprint(sample)\n```", "```py\nfrom ray.rllib.env.multi_agent_env import MultiAgentEnv\nfrom gym.spaces import Discrete\nimport os\n\nclass MultiAgentMaze(MultiAgentEnv):\n\n    agents = {1: (4, 0), 2: (0, 4)}  ![1](assets/1.png)\n    goal = (4, 4)\n    info = {1: {'obs': agents[1]}, 2: {'obs': agents[2]}}  ![2](assets/2.png)\n\n    def __init__(self,  *args, **kwargs):  ![3](assets/3.png)\n        self.action_space = Discrete(4)\n        self.observation_space = Discrete(5*5)\n\n    def reset(self):\n        self.agents = {1: (4, 0), 2: (0, 4)}\n\n        return {1: self.get_observation(1), 2: self.get_observation(2)}  ![4](assets/4.png)\n```", "```py\n    def get_observation(self, agent_id):  ![1](assets/1.png)![2](assets/2.png)\n        seeker = self.agents[agent_id]\n        return 5 * seeker[0] + seeker[1]\n\n    def get_reward(self, agent_id):\n        return 1 if self.agents[agent_id] == self.goal else 0\n\n    def is_done(self, agent_id):\n        return self.agents[agent_id] == self.goal\n```", "```py\n    def step(self, action):  ![1](assets/1.png)\n        agent_ids = action.keys()\n\n        for agent_id in agent_ids:\n            seeker = self.agents[agent_id]\n            if action[agent_id] == 0:  # move down\n                seeker = (min(seeker[0] + 1, 4), seeker[1])\n            elif action[agent_id] == 1:  # move left\n                seeker = (seeker[0], max(seeker[1] - 1, 0))\n            elif action[agent_id] == 2:  # move up\n                seeker = (max(seeker[0] - 1, 0), seeker[1])\n            elif action[agent_id] == 3:  # move right\n                seeker = (seeker[0], min(seeker[1] + 1, 4))\n            else:\n                raise ValueError(\"Invalid action\")\n            self.agents[agent_id] = seeker  ![2](assets/2.png)\n\n        observations = {i: self.get_observation(i) for i in agent_ids}  ![3](assets/3.png)\n        rewards = {i: self.get_reward(i) for i in agent_ids}\n        done = {i: self.is_done(i) for i in agent_ids}\n\n        done[\"__all__\"] = all(done.values())  ![4](assets/4.png)\n\n        return observations, rewards, done, self.info\n```", "```py\n    def render(self, *args, **kwargs):\n        os.system('cls' if os.name == 'nt' else 'clear')\n        grid = [['| ' for _ in range(5)] + [\"|\\n\"] for _ in range(5)]\n        grid[self.goal[0]][self.goal[1]] = '|G'\n        grid[self.agents[1][0]][self.agents[1][1]] = '|1'\n        grid[self.agents[2][0]][self.agents[2][1]] = '|2'\n        print(''.join([''.join(grid_row) for grid_row in grid]))\n```", "```py\nimport time\n\nenv = MultiAgentMaze()\n\nwhile True:\n    obs, rew, done, info = env.step(\n        {1: env.action_space.sample(), 2: env.action_space.sample()}\n    )\n    time.sleep(0.1)\n    env.render()\n    if any(done.values()):\n        break\n```", "```py\nfrom ray.rllib.agents.dqn import DQNTrainer\n\nsimple_trainer = DQNTrainer(env=MultiAgentMaze)\nsimple_trainer.train()\n```", "```py\ntrainer = DQNTrainer(env=MultiAgentMaze, config={\n    \"multiagent\": {\n        \"policies\": {  ![1](assets/1.png)\n            \"policy_1\": (None, env.observation_space, env.action_space, {\"gamma\": 0.80}),\n            \"policy_2\": (None, env.observation_space, env.action_space, {\"gamma\": 0.95}),\n        },\n        \"policy_mapping_fn\": lambda agent_id: f\"policy_{agent_id}\",  ![2](assets/2.png)\n    },\n})\n\nprint(trainer.train())\n```", "```py\n# policy_server.py\nimport ray\nfrom ray.rllib.agents.dqn import DQNTrainer\nfrom ray.rllib.env.policy_server_input import PolicyServerInput\nimport gym\n\nray.init()\n\ndef policy_input(context):\n    return PolicyServerInput(context, \"localhost\", 9900)  ![1](assets/1.png)\n\nconfig = {\n    \"env\": None,  ![2](assets/2.png)\n    \"observation_space\": gym.spaces.Discrete(5*5),\n    \"action_space\": gym.spaces.Discrete(4),\n    \"input\": policy_input,  ![3](assets/3.png)\n    \"num_workers\": 0,\n    \"input_evaluation\": [],\n    \"log_level\": \"INFO\",\n}\n\ntrainer = DQNTrainer(config=config)\n```", "```py\n# policy_server.py\nif __name__ == \"__main__\":\n\n    time_steps = 0\n    for _ in range(100):\n        results = trainer.train()\n        checkpoint = trainer.save()  ![1](assets/1.png)\n        if time_steps >= 10.000:  ![2](assets/2.png)\n            break\n        time_steps += results[\"timesteps_total\"]\n```", "```py\n# policy_client.py\nimport gym\nfrom ray.rllib.env.policy_client import PolicyClient\nfrom maze_gym_env import GymEnvironment\n\nif __name__ == \"__main__\":\n    env = GymEnvironment()\n    client = PolicyClient(\"http://localhost:9900\", inference_mode=\"remote\")  ![1](assets/1.png)\n\n    obs = env.reset()\n    episode_id = client.start_episode(training_enabled=True)  ![2](assets/2.png)\n\n    while True:\n        action = client.get_action(episode_id, obs)  ![3](assets/3.png)\n\n        obs, reward, done, info = env.step(action)\n\n        client.log_returns(episode_id, reward, info=info)  ![4](assets/4.png)\n\n        if done:\n            client.end_episode(episode_id, obs)  ![5](assets/5.png)\n            obs = env.reset()\n\n            exit(0)  ![6](assets/6.png)\n```", "```py\nfrom gym.spaces import Discrete\nimport random\nimport os\n\nclass AdvancedEnv(GymEnvironment):\n\n    def __init__(self, seeker=None, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.maze_len = 11\n        self.action_space = Discrete(4)\n        self.observation_space = Discrete(self.maze_len * self.maze_len)\n\n        if seeker:  ![1](assets/1.png)\n            assert 0 <= seeker[0] < self.maze_len and 0 <= seeker[1] < self.maze_len\n            self.seeker = seeker\n        else:\n            self.reset()\n\n        self.goal = (self.maze_len-1, self.maze_len-1)\n        self.info = {'seeker': self.seeker, 'goal': self.goal}\n\n        self.punish_states = [  ![2](assets/2.png)\n            (i, j) for i in range(self.maze_len) for j in range(self.maze_len)\n            if i % 2 == 1 and j % 2 == 0\n        ]\n```", "```py\n    def reset(self):\n        \"\"\"Reset seeker position randomly, return observations.\"\"\"\n        self.seeker = (random.randint(0, self.maze_len - 1), random.randint(0, self.maze_len - 1))\n        return self.get_observation()\n\n    def get_observation(self):\n        \"\"\"Encode the seeker position as integer\"\"\"\n        return self.maze_len * self.seeker[0] + self.seeker[1]\n\n    def get_reward(self):\n        \"\"\"Reward finding the goal and punish forbidden states\"\"\"\n        reward = -1 if self.seeker in self.punish_states else 0\n        reward += 5 if self.seeker == self.goal else 0\n        return reward\n\n    def render(self, *args, **kwargs):\n        \"\"\"Render the environment, e.g. by printing its representation.\"\"\"\n        os.system('cls' if os.name == 'nt' else 'clear')\n        grid = [['| ' for _ in range(self.maze_len)] + [\"|\\n\"] for _ in range(self.maze_len)]\n        for punish in self.punish_states:\n            grid[punish[0]][punish[1]] = '|X'\n        grid[self.goal[0]][self.goal[1]] = '|G'\n        grid[self.seeker[0]][self.seeker[1]] = '|S'\n        print(''.join([''.join(grid_row) for grid_row in grid]))\n```", "```py\nfrom ray.rllib.env.apis.task_settable_env import TaskSettableEnv\n\nclass CurriculumEnv(AdvancedEnv, TaskSettableEnv):\n\n    def __init__(self, *args, **kwargs):\n        AdvancedEnv.__init__(self)\n\n    def difficulty(self):\n        return abs(self.seeker[0] - self.goal[0]) + abs(self.seeker[1] - self.goal[1])\n\n    def get_task(self):\n        return self.difficulty()\n\n    def set_task(self, task_difficulty):\n        while not self.difficulty() <= task_difficulty:\n            self.reset()\n```", "```py\ndef curriculum_fn(train_results, task_settable_env, env_ctx):\n    time_steps = train_results.get(\"timesteps_total\")\n    difficulty = time_steps // 1000\n    print(f\"Current difficulty: {difficulty}\")\n    return difficulty\n```", "```py\nconfig = {\n    \"env\": CurriculumEnv,\n    \"env_task_fn\": curriculum_fn,\n    \"output\": \"/tmp/env-out\",\n}\n\nfrom ray.rllib.agents.dqn import DQNTrainer\n\ntrainer = DQNTrainer(env=CurriculumEnv, config=config)\n\nfor i in range(15):\n    trainer.train()\n```", "```py\ninput_config = {\n    \"input\": \"/tmp/env-out\",\n    \"input_evaluation\": [],\n    \"explore\": False\n}\n```", "```py\nimitation_trainer = DQNTrainer(env=AdvancedEnv, config=input_config)\nfor i in range(10):\n    imitation_trainer.train()\n\nimitation_trainer.evaluate()\n```"]