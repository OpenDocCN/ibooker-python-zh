["```py\nconda create -n dask python=3.8.6  mamba -y\nconda activate dask\nmamba install --yes python==3.8.6 cytoolz dask==2021.7.0 numpy \\\n      pandas==1.3.0 beautifulsoup4 requests\n```", "```py\nimport dask\nfrom dask.distributed import Client\nclient = Client() # Here we could specify a cluster, defaults to local mode\n```", "```py\nimport timeit\n\ndef slow_task(x):\n    import time\n    time.sleep(2) # Do something sciency/business\n    return x\n\nthings = range(10)\n\nvery_slow_result = map(slow_task, things)\nslowish_result = map(dask.delayed(slow_task), things)\n\nslow_time = timeit.timeit(lambda: list(very_slow_result), number=1)\nfast_time = timeit.timeit(\n    lambda: list(\n        dask.compute(\n            *slowish_result)),\n    number=1)\nprint(\"In sequence {}, in parallel {}\".format(slow_time, fast_time))\n```", "```py\n@dask.delayed\ndef crawl(url, depth=0, maxdepth=1, maxlinks=4):\n    links = []\n    link_futures = []\n    try:\n        import requests\n        from bs4 import BeautifulSoup\n        f = requests.get(url)\n        links += [(url, f.text)]\n        if (depth > maxdepth):\n            return links # base case\n        soup = BeautifulSoup(f.text, 'html.parser')\n        c = 0\n        for link in soup.find_all('a'):\n            if \"href\" in link:\n                c = c + 1\n                link_futures += crawl(link[\"href\"],\n                                      depth=(depth + 1),\n                                      maxdepth=maxdepth)\n                # Don't branch too much; we're still in local mode and the web is\n                # big\n                if c > maxlinks:\n                    break\n        for r in dask.compute(link_futures):\n            links += r\n        return links\n    except requests.exceptions.InvalidSchema:\n        return [] # Skip non-web links\n\ndask.compute(crawl(\"http://holdenkarau.com/\"))\n```", "```py\nimport dask.array as da\ndistributed_array = da.from_array(list(range(0, 1000)))\navg = dask.compute(da.average(distributed_array))\n```", "```py\nimport dask.bag as db\ngithubs = [\n    \"https://github.com/scalingpythonml/scalingpythonml\",\n    \"https://github.com/dask/distributed\"]\ninitial_bag = db.from_delayed(map(crawl, githubs))\n```", "```py\nwords_bag = initial_bag.map(\n    lambda url_contents: url_contents[1].split(\" \")).flatten()\n```", "```py\ndask.compute(words_bag.frequencies())\n```", "```py\ndef make_word_tuple(w):\n    return (w, 1)\n\ndef get_word(word_count):\n    return word_count[0]\n\ndef sum_word_counts(wc1, wc2):\n    return (wc1[0], wc1[1] + wc2[1])\n\nword_count = words_bag.map(make_word_tuple).foldby(get_word, sum_word_counts)\n```", "```py\nimport dask.dataframe as dd\n\n@dask.delayed\ndef crawl_to_df(url, depth=0, maxdepth=1, maxlinks=4):\n    import pandas as pd\n    crawled = crawl(url, depth=depth, maxdepth=maxdepth, maxlinks=maxlinks)\n    return pd.DataFrame(crawled.compute(), columns=[\n                        \"url\", \"text\"]).set_index(\"url\")\n\ndelayed_dfs = map(crawl_to_df, githubs)\ninitial_df = dd.from_delayed(delayed_dfs)\nwc_df = initial_df.text.str.split().explode().value_counts()\n\ndask.compute(wc_df)\n```"]