- en: Chapter 17\. Avoiding Scraping Traps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Few things are more frustrating than scraping a site, viewing the output, and
    not seeing the data that’s so clearly visible in your browser. Or submitting a
    form that should be perfectly fine but gets denied by the web server. Or getting
    your IP address blocked by a site for unknown reasons.
  prefs: []
  type: TYPE_NORMAL
- en: These are some of the most difficult bugs to solve, not only because they can
    be so unexpected (a script that works just fine on one site might not work at
    all on another, seemingly identical, site), but because they purposefully don’t
    have any telltale error messages or stack traces to use. You’ve been identified
    as a bot, rejected, and you don’t know why.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this book, I’ve written about a lot of ways to do tricky things on websites,
    including submitting forms, extracting and cleaning difficult data, and executing
    JavaScript. This chapter is a bit of a catchall in that the techniques stem from
    a wide variety of subjects. However, they all have something in common: they are
    meant to overcome an obstacle put in place for the sole purpose of preventing
    automated scraping of a site.'
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of how immediately useful this information is to you at the moment,
    I highly recommend you at least skim this chapter. You never know when it might
    help you solve a difficult bug or prevent a problem altogether.
  prefs: []
  type: TYPE_NORMAL
- en: A Note on Ethics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the first few chapters of this book, I discussed the legal gray area that
    web scraping inhabits, as well as some of the ethical and legal guidelines to
    scrape by. To be honest, this chapter is, ethically, perhaps the most difficult
    one for me to write. My websites have been plagued by bots, spammers, web scrapers,
    and all manner of unwanted virtual guests, as perhaps yours have been. So why
    teach people how to build better bots?
  prefs: []
  type: TYPE_NORMAL
- en: 'I believe this chapter is important to include for a few reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: There are perfectly ethical and legally sound reasons to scrape some websites
    that do not want to be scraped. In a previous job I had as a web scraper, I performed
    automated collection of information from websites that were publishing clients’
    names, addresses, telephone numbers, and other personal information to the internet
    without their consent. I used the scraped information to make legal requests to
    the websites to remove this information. To avoid competition, these sites vigilantly
    guarded this information from scrapers. However, my work to ensure the anonymity
    of my company’s clients (some of whom had stalkers, were the victims of domestic
    violence, or had other very good reasons to want to keep a low profile) made a
    compelling case for web scraping, and I was grateful that I had the skills necessary
    to do the job.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although it is almost impossible to build a “scraper proof” site (or at least
    one that can still be easily accessed by legitimate users), I hope that the information
    in this chapter will help those wanting to defend their websites against malicious
    attacks. Throughout, I will point out some of the weaknesses in each web scraping
    technique, which you can use to defend your own site. Keep in mind that most bots
    on the web today are merely doing a broad scan for information and vulnerabilities;
    employing even a couple of simple techniques described in this chapter likely
    will thwart 99% of them. However, they are getting more sophisticated every month,
    and it’s best to be prepared.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Like most programmers, I don’t believe that withholding any sort of educational
    information is a net positive thing to do.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'While you’re reading this chapter, keep in mind that many of these scripts
    and described techniques should not be run against every site you can find. Not
    only is it not a nice thing to do, but you could wind up receiving a cease and
    desist letter or worse (for more information about what to do if you receive such
    a letter, see [Chapter 2](ch02.html#c-2)). But I’m not going to pound you over
    the head with this every time I discuss a new technique. So, for the rest of this
    chapter, as the philosopher Gump once said: “That’s all I have to say about that.”'
  prefs: []
  type: TYPE_NORMAL
- en: Looking Like a Human
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The fundamental challenge for sites that do not want to be scraped is figuring
    out how to tell bots from humans. Although many of the techniques sites use (such
    as CAPTCHAs) can be difficult to fool, you can do a few fairly easy things to
    make your bot look more human.
  prefs: []
  type: TYPE_NORMAL
- en: Adjust Your Headers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Throughout the book, you’ve used the Python Requests library to create, send,
    and receive HTTP requests, such as handling forms on a website in [Chapter 13](ch13.html#c-13).
    The Requests library is also excellent for setting headers. HTTP headers are lists
    of attributes, or preferences, sent by you every time you make a request to a
    web server. HTTP defines dozens of obscure header types, most of which are not
    commonly used. The following seven fields, however, are consistently used by most
    major browsers when initiating any connection (shown with example data from my
    own browser):'
  prefs: []
  type: TYPE_NORMAL
- en: '| `Host` | https://www.google.com/ |'
  prefs: []
  type: TYPE_TB
- en: '| `Connection` | keep-alive |'
  prefs: []
  type: TYPE_TB
- en: '| `Accept` | text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7
    |'
  prefs: []
  type: TYPE_TB
- en: '| `User-Agent` | Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36
    (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36 |'
  prefs: []
  type: TYPE_TB
- en: '| `Referrer` | https://www.google.com/ |'
  prefs: []
  type: TYPE_TB
- en: '| `Accept-Encoding` | gzip, deflate, sdch |'
  prefs: []
  type: TYPE_TB
- en: '| `Accept-Language` | en-US,en;q=0.8 |'
  prefs: []
  type: TYPE_TB
- en: 'And here are the headers that a typical Python scraper using the default urllib
    library might send:'
  prefs: []
  type: TYPE_NORMAL
- en: '| `Accept-Encoding` | identity |'
  prefs: []
  type: TYPE_TB
- en: '| `User-Agent` | Python-urllib/3.9 |'
  prefs: []
  type: TYPE_TB
- en: If you’re a website administrator trying to block scrapers, which one are you
    more likely to let through?
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, headers can be completely customized using the Requests library.
    The website [*https://www.whatismybrowser.com*](https://www.whatismybrowser.com)
    is great for testing browser properties viewable by servers. You’ll scrape this
    website to verify your cookie settings with the following script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The output should show that the headers are now the same ones set in the `headers`
    dictionary object in the code.
  prefs: []
  type: TYPE_NORMAL
- en: Although it is possible for websites to check for “humanness” based on any of
    the properties in HTTP headers, I’ve found that typically the only setting that
    really matters is the `User-Agent`. It’s a good idea to keep this one set to something
    more inconspicuous than `Python-urllib/3.9`, regardless of what project you are
    working on. In addition, if you ever encounter an extremely suspicious website,
    populating one of the commonly used but rarely checked headers such as `Accept-Language`
    might be the key to convincing it you’re a human.
  prefs: []
  type: TYPE_NORMAL
- en: Handling Cookies with JavaScript
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Handling cookies correctly can alleviate many scraping problems, although cookies
    can also be a double-edged sword. Websites that track your progression through
    a site using cookies might attempt to cut off scrapers that display abnormal behavior,
    such as completing forms too quickly, or visiting too many pages. Although these
    behaviors can be disguised by closing and reopening connections to the site, or
    even changing your IP address (see [Chapter 20](ch20.html#c-20) for more information
    on how to do that), if your cookie gives your identity away, your efforts at disguise
    might be futile.
  prefs: []
  type: TYPE_NORMAL
- en: Cookies can also be necessary to scrape a site. As shown in [Chapter 13](ch13.html#c-13),
    staying logged in on a site requires that you be able to hold and present a cookie
    from page to page. Some websites don’t even require that you actually log in and
    get a new version of a cookie every time—merely holding an old copy of a “logged-in”
    cookie and visiting the site is enough.
  prefs: []
  type: TYPE_NORMAL
- en: If you are scraping a single targeted website or a small number of targeted
    sites, I recommend examining the cookies generated by those sites and considering
    which ones you might want your scraper to handle. Various browser plug-ins can
    show you how cookies are being set as you visit and move around a site. [EditThisCookie](http://www.editthiscookie.com),
    a Chrome extension, is one of my favorites.
  prefs: []
  type: TYPE_NORMAL
- en: Check out the code samples in [“Handling Logins and Cookies”](ch13.html#handling_logins_cookies)
    in [Chapter 13](ch13.html#c-13) for more information about handling cookies by
    using the Requests library. Of course, because it is unable to execute JavaScript,
    the Requests library will be unable to handle many of the cookies produced by
    modern tracking software, such as Google Analytics, which are set only after the
    execution of client-side scripts (or sometimes based on page events, such as button
    clicks, that happen while browsing the page). To handle these, you need to use
    the Selenium and Chrome WebDriver packages (I covered their installation and basic
    usage in [Chapter 14](ch14.html#c-14)).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can view cookies by visiting any site ([*http://pythonscraping.com*](http://pythonscraping.com),
    in this example) and calling `get_cookies()` on the webdriver:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This provides the fairly typical array of Google Analytics cookies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'To manipulate cookies, you can call the `delete_cookie()`, `add_cookie()`,
    and `delete_all_cookies()` functions. In addition, you can save and store cookies
    for use in other web scrapers. Here’s an example to give you an idea of how these
    functions work together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the first webdriver retrieves a website, prints the cookies,
    and then stores them in the variable `savedCookies`. The second webdriver loads
    the same website, deletes its own cookies, and adds the cookies from the first
    webdriver.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the second webdriver must load the website first before the cookies
    are added. This is so Selenium knows which domain the cookies belong to, even
    if the act of loading the website does nothing useful for the scraper.
  prefs: []
  type: TYPE_NORMAL
- en: After this is done, the second webdriver should have the same cookies as the
    first. According to Google Analytics, this second webdriver is now identical to
    the first one, and they will be tracked in the same way. If the first webdriver
    was logged in to a site, the second webdriver will be as well.
  prefs: []
  type: TYPE_NORMAL
- en: TLS Fingerprinting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the early 2000s, many large tech companies liked to ask prospective programmers
    riddles during job interviews. This mostly fell out of fashion when hiring managers
    realized two things: candidates were sharing and memorizing riddle solutions,
    and the “ability to solve riddles” doesn’t correlate well to job performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, one of these classic job interview riddles still has value as a metaphor
    for the Transport Layer Security protocol. It goes like this:'
  prefs: []
  type: TYPE_NORMAL
- en: You need to ship a top secret message to a friend via a dangerous route where
    any unlocked message-containing boxes are intercepted by spies (padlocked message
    boxes, however, are safe if the spies do not have the keys). You place the message
    in a box that can be locked with multiple padlocks. While you have padlocks with
    corresponding keys and your friend also has their own padlocks with corresponding
    keys, none of your friend’s keys work on your padlocks and vice versa. How do
    you ensure that your friend is able to unlock the box on their end and receive
    the message securely?
  prefs: []
  type: TYPE_NORMAL
- en: Note that shipping a key that unlocks your padlock, even as a separate shipment,
    will not work. The spies will intercept and make copies of these keys and save
    them for later use. Also, shipping a key afterwards will not work (although this
    is where “riddle as metaphor” breaks down a bit) because the spies can make copies
    of the *box itself* and, if a key is sent later, unlock their box copy.
  prefs: []
  type: TYPE_NORMAL
- en: 'One solution is this: you place your padlock on the box and ship it to your
    friend. Your friend receives the locked box, places their own padlock on it (so
    now the box has two padlocks), and ships it back. You remove your padlock and
    ship it to your friend with only their padlock remaining. Your friend receives
    the box and unlocks it.'
  prefs: []
  type: TYPE_NORMAL
- en: This is, essentially, how secure communications are established over an untrusted
    network. Over a secure communication protocol, like HTTPS, all messages are encrypted
    and decrypted with a key. If an attacker obtains the key (represented by the secret
    message in the riddle), then they are able to read any messages being sent.
  prefs: []
  type: TYPE_NORMAL
- en: So how do you send your friend the key that you’re going to use to encrypt and
    decrypt future messages without that key being intercepted and used by attackers?
    Encrypt it with your own “padlock,” send it to the friend, the friend adds their
    own “padlock,” you remove your “padlock,” and send it back for the friend to “unlock.”
    In this way, the secret key is exchanged securely.
  prefs: []
  type: TYPE_NORMAL
- en: This entire process of “locking,” sending, adding another “lock,” etc., is handled
    by the Transport Layer Security protocol, or TLS. This process of securely establishing
    a mutually known key is called the *TLS handshake*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to a establishing a mutually known key, or *master secret*, many
    other things are established during the handshake:'
  prefs: []
  type: TYPE_NORMAL
- en: The highest version of the TLS protocol supported by both parties (which will
    be the version used during the rest of the handshake)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which encryption library will be used
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which compression method will be used
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The identity of the server, represented by its public certificate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Verifications that the master secret is working for both parties and that the
    communication is now secure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This entire TLS handshake is done every time you contact a new web server and
    any time you need to establish a new HTTP session with that web server (see [Chapter 1](ch01.html#c-1)
    for more information about sessions). The exact messages that are sent by your
    computer for the TLS handshake are determined by the application that is making
    the connection. For example, Chrome may support slightly different TLS versions
    or encryption libraries than Microsoft Edge, so the messages sent by it in the
    TLS handshake will be different.
  prefs: []
  type: TYPE_NORMAL
- en: Because the TLS handshake is so lengthy, and the variables involved in its negotiations
    so numerous, clever server administrators realized that the messages sent by clients
    during the TLS handshake were somewhat unique to each application. The messages
    created a sort of *TLS fingerprint* that revealed whether the messages were being
    generated by Chrome, Microsoft Edge, Safari, or even the Python Requests library.
  prefs: []
  type: TYPE_NORMAL
- en: You can see some of the information generated by your TLS handshake by visiting
    (or scraping) [*https://tools.scrapfly.io/api/fp/ja3?extended=1*](https://tools.scrapfly.io/api/fp/ja3?extended=1).
    To make TLS fingerprints more manageable and easy to compare, a hashing method
    called JA3 is often used, the results of which are shown in this API response.
    JA3 hashed fingerprints are catalogued in large databases and used for lookup
    when an application needs to be identified later.
  prefs: []
  type: TYPE_NORMAL
- en: A TLS fingerprint is a bit like a User-Agent cookie in that it is a long string
    that identifies the application you’re using to send data. But, unlike a User-Agent
    cookie, it’s not easy to modify. In Python, TLS is controlled by the [SSL library](https://github.com/python/cpython/blob/3.11/Lib/ssl.py).
    In theory, perhaps you could rewrite the SSL library. With hard work and dedication,
    you may be able to modify the TLS fingerprint that Python is sending from your
    computer just enough to make the JA3 hash unrecognizable to servers seeking to
    block Python bots. With harder work and more dedication, you might impersonate
    an innocuous browser! Some projects, such as [*https://github.com/lwthiker/curl-impersonate*](https://github.com/lwthiker/curl-impersonate), 
    are seeking to do just that.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately the nature of this TLS fingerprint problem means that any impersonation
    libraries will require frequent maintenance by volunteers and are prone to quick
    degradation. Until these projects gain more mainstream traction and reliability,
    there is a much easier way to subvert TLS fingerprinting and blocking: Selenium.'
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this book, I’ve warned against using an automated browser to solve
    your problems when alternative solutions exist. Browsers use lots of memory, often
    load unnecessary pages, and require extra dependencies that all need upkeep and
    maintenance to keep your web crawler running. But when it comes to TLS fingerprinting,
    it just makes sense to avoid the headache and use a browser.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that your TLS fingerprint will be the same whether you’re using
    the headless or nonheadless version of your browser. So feel free to turn off
    the graphics and use best practices to load only the data that you need—the target
    server isn’t going to know (based on your TLS data, at least)!
  prefs: []
  type: TYPE_NORMAL
- en: Timing Is Everything
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some well-protected websites might prevent you from submitting forms or interacting
    with the site if you do it too quickly. Even if these security features aren’t
    in place, downloading lots of information from a website significantly faster
    than a normal human might is a good way to get yourself noticed and blocked.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, although multithreaded programming might be a great way to load
    pages faster—allowing you to process data in one thread while repeatedly loading
    pages in another—it’s a terrible policy for writing good scrapers. You should
    always try to keep individual page loads and data requests to a minimum. If possible,
    try to space them out by a few seconds, even if you have to add in an extra:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Whether or not you need this extra few seconds between page loads is often found
    experimentally. Many times I’ve struggled to scrape data from a website, having
    to prove myself as “not a robot” every few minutes (solving the CAPTCHA by hand,
    pasting my newly obtained cookies back over to the scraper so the website viewed
    the scraper itself as having “proven its humanness”), but adding a `time.sleep`
    solved my problems and allowed me to scrape indefinitely.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes you have to slow down to go fast!
  prefs: []
  type: TYPE_NORMAL
- en: Common Form Security Features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many litmus tests have been used over the years, and continue to be used, with
    varying degrees of success, to separate web scrapers from browser-using humans.
    Although it’s not a big deal if a bot downloads some articles and blog posts that
    were available to the public anyway, it is a big problem if a bot creates thousands
    of user accounts and starts spamming all of your site’s members. Web forms, especially
    forms that deal with account creation and logins, pose a significant threat to
    security and computational overhead if they’re vulnerable to indiscriminate use
    by bots, so it’s in the best interest of many site owners (or at least they think
    it is) to try to limit access to the site.
  prefs: []
  type: TYPE_NORMAL
- en: These antibot security measures centered on forms and logins can pose a significant
    challenge to web scrapers.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that this is only a partial overview of some of the security measures
    you might encounter when creating automated bots for these forms. Review [Chapter 16](ch16.html#c-16),
    on dealing with CAPTCHAs and image processing, as well as [Chapter 13](ch13.html#c-13),
    on dealing with headers and IP addresses, for more information on dealing with
    well-protected forms.
  prefs: []
  type: TYPE_NORMAL
- en: Hidden Input Field Values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '“Hidden” fields in HTML forms allow the value contained in the field to be
    viewable by the browser but invisible to the user (unless they look at the site’s
    source code). With the increase in use of cookies to store variables and pass
    them around on websites, hidden fields fell out of a favor for a while before
    another excellent purpose was discovered for them: preventing scrapers from submitting
    forms.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 17-1](#facebook_login_form) shows an example of these hidden fields
    at work on a LinkedIn login page. Although the form has only three visible fields
    (Username, Password, and a Submit button), it conveys a great deal of information
    to the server behind the scenes.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Alt Text](assets/wsp3_1701.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17-1\. The LinkedIn login form has a few hidden fields
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Hidden fields are used to prevent web scraping in two main ways: a field can
    be populated with a randomly generated variable on the form page that the server
    is expecting to be posted to the form-processing page. If this value is not present
    in the form, the server can reasonably assume that the submission did not originate
    organically from the form page but was posted by a bot directly to the processing
    page. The best way to get around this measure is to scrape the form page first,
    collect the randomly generated variable, and then post to the processing page
    from there.'
  prefs: []
  type: TYPE_NORMAL
- en: The second method is a “honeypot” of sorts. If a form contains a hidden field
    with an innocuous name, such as Username or Email Address, a poorly written bot
    might fill out the field and attempt to submit it, regardless of whether it is
    hidden to the user. Any hidden fields with actual values (or values that are different
    from their defaults on the form submission page) should be disregarded, and the
    user may even be blocked from the site.
  prefs: []
  type: TYPE_NORMAL
- en: 'In short: it is sometimes necessary to check the page that the form is on to
    see whether you missed anything that the server might be expecting. If you see
    several hidden fields, often with large, randomly generated string variables,
    the web server likely will be checking for their existence on form submission.
    In addition, there might be other checks to ensure that the form variables have
    been used only once, are recently generated (this eliminates the possibility of
    simply storing them in a script and using them over and over again over time),
    or both.'
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding Honeypots
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although CSS for the most part makes life extremely easy when it comes to differentiating
    useful information from nonuseful information (e.g., by reading the `id` and `class`
    tags), it can occasionally be problematic for web scrapers. If a field on a web
    form is hidden from a user via CSS, it is reasonable to assume that the average
    user visiting the site will not be able to fill it out because it doesn’t show
    up in the browser. If the form *is* populated, there is likely a bot at work and
    the post will be discarded.
  prefs: []
  type: TYPE_NORMAL
- en: This applies not only to forms but to links, images, files, and any other item
    on the site that can be read by a bot but is hidden from the average user visiting
    the site through a browser. A page visit to a “hidden” link on a site can easily
    trigger a server-side script that will block the user’s IP address, log that user
    out of the site, or take some other action to prevent further access. In fact,
    many business models have been based on exactly this concept.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take, for example, the page located at [*http://pythonscraping.com/pages/itsatrap.html*](http://pythonscraping.com/pages/itsatrap.html).
    This page contains two links, one hidden by CSS and another visible. In addition,
    it contains a form with two hidden fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'These three elements are hidden from the user in three ways:'
  prefs: []
  type: TYPE_NORMAL
- en: The first link is hidden with a simple CSS `display:none` attribute.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The phone field is a hidden input field.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The email field is hidden by moving it 50,000 pixels to the right (presumably
    off the screen of everyone’s monitors) and hiding the telltale scroll bar.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fortunately, because Selenium renders the pages it visits, it is able to distinguish
    between elements that are visually present on the page and those that aren’t.
    Whether the element is present on the page can be determined by the `is_displayed()`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the following code retrieves the previously described page and
    looks for hidden links and form input fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Selenium catches each hidden field, producing the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Although you probably don’t want to visit any hidden links you find, you will
    want to make sure that you submit any prepopulated hidden form values (or have
    Selenium submit them for you) with the rest of the form. To sum up, it is dangerous
    to simply ignore hidden fields, although you must be careful when interacting
    with them.
  prefs: []
  type: TYPE_NORMAL
- en: The Human Checklist
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There’s a lot of information in this chapter, and indeed in this book, about
    how to build a scraper that looks less like a scraper and more like a human. If
    you keep getting blocked by websites and you don’t know why, here’s a checklist
    you can use to remedy the problem:'
  prefs: []
  type: TYPE_NORMAL
- en: First, if the page you are receiving from the web server is blank, missing information,
    or is otherwise not what you expect (or have seen in your own browser), it is
    likely caused by JavaScript being executed on the site to create the page. Review
    [Chapter 14](ch14.html#c-14).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are submitting a form or making a `POST` request to a website, check
    the page to make sure that everything the website is expecting you to submit is
    being submitted and in the correct format. Use a tool such as Chrome’s Inspector
    panel to view an actual `POST` request sent to the site to make sure you have
    everything, and that an “organic” request looks the same as the ones your bot
    is sending.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are trying to log in to a site and can’t make the login “stick,” or the
    website is experiencing other strange “state” behavior, check your cookies. Make
    sure that cookies are being persisted correctly between each page load and that
    your cookies are sent to the site for every request.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you are getting HTTP errors from the client, especially 403 Forbidden errors,
    it might indicate that the website has identified your IP address as a bot and
    is unwilling to accept any more requests. You will need to either wait until your
    IP address is removed from the list or obtain a new IP address (either move to
    a Starbucks or see [Chapter 20](ch20.html#c-20)). To make sure you don’t get blocked
    again, try the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Make sure that your scrapers aren’t moving through the site too quickly. Fast
    scraping is a bad practice that places a heavy burden on the web administrator’s
    servers, can land you in legal trouble, and is the number one cause of scrapers
    getting blacklisted. Add delays to your scrapers and let them run overnight. Remember:
    being in a rush to write programs or gather data is a sign of bad project management;
    plan ahead to avoid messes like this in the first place.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The obvious one: change your headers! Some sites will block anything that advertises
    itself as a scraper. Copy your own browser’s headers if you’re unsure about what
    some reasonable header values are.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Make sure you’re not clicking on or accessing anything that a human normally
    would not be able to (refer to [“Avoiding Honeypots”](#avoiding_honeypots) for
    more information).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If you find yourself jumping through a lot of difficult hoops to gain access,
    consider contacting the website administrator to let them know what you’re doing.
    Try emailing *webmaster@<domain name>* or *admin@<domain name>* for permission
    to use your scrapers. Admins are people too, and you might be surprised at how
    amenable they can be to sharing their data.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
