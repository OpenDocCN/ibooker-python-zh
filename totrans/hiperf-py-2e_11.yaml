- en: Chapter 11\. Using Less RAM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We rarely think about how much RAM we’re using until we run out of it. If you
    run out while scaling your code, it can become a sudden blocker. Fitting more
    into a machine’s RAM means fewer machines to manage, and it gives you a route
    to planning capacity for larger projects. Knowing why RAM gets eaten up and considering
    more efficient ways to use this scarce resource will help you deal with scaling
    issues. We’ll use the Memory Profiler and IPython Memory Usage tools to measure
    the actual RAM usage, along with some tools that introspect objects to try to
    guess how much RAM they’re using.
  prefs: []
  type: TYPE_NORMAL
- en: Another route to saving RAM is to use containers that utilize features in your
    data for compression. In this chapter, we’ll look at a trie (ordered tree data
    structures) and a directed acyclic word graph (DAWG) that can compress a 1.2 GB
    `set` of strings down to just 30 MB with little change in performance. A third
    approach is to trade storage for accuracy. For this we’ll look at approximate
    counting and approximate set membership, which use dramatically less RAM than
    their exact counterparts.
  prefs: []
  type: TYPE_NORMAL
- en: A consideration with RAM usage is the notion that “data has mass.” The more
    there is of it, the slower it moves around. If you can be parsimonious in your
    use of RAM, your data will probably get consumed faster, as it’ll move around
    buses faster and more of it will fit into constrained caches. If you need to store
    it in offline storage (e.g., a hard drive or a remote data cluster), it’ll move
    far more slowly to your machine. Try to choose appropriate data structures so
    all your data can fit onto one machine. We’ll use NumExpr to efficiently calculate
    with NumPy and Pandas with fewer data movements than the more direct method, which
    will save us time and make certain larger calculations feasible in a fixed amount
    of RAM.
  prefs: []
  type: TYPE_NORMAL
- en: Counting the amount of RAM used by Python objects is surprisingly tricky. We
    don’t necessarily know how an object is represented behind the scenes, and if
    we ask the operating system for a count of bytes used, it will tell us about the
    total amount allocated to the process. In both cases, we can’t see exactly how
    each individual Python object adds to the total.
  prefs: []
  type: TYPE_NORMAL
- en: As some objects and libraries don’t report their full internal allocation of
    bytes (or they wrap external libraries that do not report their allocation at
    all), this has to be a case of best-guessing. The approaches explored in this
    chapter can help us to decide on the best way to represent our data so we use
    less RAM overall.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll also look at several lossy methods for storing strings in scikit-learn
    and counts in data structures. This works a little like a JPEG compressed image—we
    lose some information (and we can’t undo the operation to recover it), and we
    gain a lot of compression as a result. By using hashes on strings, we compress
    the time and memory usage for a natural language processing task in scikit-learn,
    and we can count huge numbers of events with only a small amount of RAM.
  prefs: []
  type: TYPE_NORMAL
- en: Objects for Primitives Are Expensive
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s common to work with containers like the `list`, storing hundreds or thousands
    of items. As soon as you store a large number, RAM usage becomes an issue.
  prefs: []
  type: TYPE_NORMAL
- en: A `list` with 100 million items consumes approximately 760 MB of RAM, *if the
    items are the same object*. If we store 100 million *different* items (e.g., unique
    integers), we can expect to use gigabytes of RAM! Each unique object has a memory
    cost.
  prefs: []
  type: TYPE_NORMAL
- en: In [Example 11-1](ch11_split_000.xhtml#lessram-memory-profiler-1e8-same-items),
    we store many `0` integers in a `list`. If you stored 100 million references to
    any object (regardless of how large one instance of that object was), you’d still
    expect to see a memory cost of roughly 760 MB, as the `list` is storing references
    to (not copies of) the object. Refer back to [“Using memory_profiler to Diagnose
    Memory Usage”](ch02.xhtml#memory_profiler) for a reminder of how to use `memory_profiler`;
    here, we load it as a new magic function in IPython using `%load_ext memory_profiler`.
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-1\. Measuring memory usage of 100 million of the same integer in
    a list
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: For our next example, we’ll start with a fresh shell. As the results of the
    first call to `memit` in [Example 11-2](ch11_split_000.xhtml#lessram-memory-profiler-1e8-different-items)
    reveal, a fresh IPython shell consumes approximately 40 MB of RAM. Next, we can
    create a temporary list of 100 million *unique* numbers. In total, this consumes
    approximately 3.8 GB.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Memory can be cached in the running process, so it is always safer to exit and
    restart the Python shell when using `memit` for profiling.
  prefs: []
  type: TYPE_NORMAL
- en: After the `memit` command finishes, the temporary list is deallocated. The final
    call to `memit` shows that the memory usage drops to its previous level.
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-2\. Measuring memory usage of 100 million different integers in a
    list
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: A subsequent `memit` in [Example 11-3](ch11_split_000.xhtml#lessram-memory-profiler-1e8-different-items2)
    to create a second 100-million-item list consumes approximately 3.8 GB again.
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-3\. Measuring memory usage again for 100 million different integers
    in a list
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Next, we’ll see that we can use the `array` module to store 100 million integers
    far more cheaply.
  prefs: []
  type: TYPE_NORMAL
- en: The array Module Stores Many Primitive Objects Cheaply
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `array` module efficiently stores primitive types like integers, floats,
    and characters, but *not* complex numbers or classes. It creates a contiguous
    block of RAM to hold the underlying data.
  prefs: []
  type: TYPE_NORMAL
- en: In [Example 11-4](ch11_split_000.xhtml#lessram-array1), we allocate 100 million
    integers (8 bytes each) into a contiguous chunk of memory. In total, approximately
    760 MB is consumed by the process. The difference between this approach and the
    previous list-of-unique-integers approach is `3100MB - 760MB == 2.3GB`. This is
    a huge savings in RAM.
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-4\. Building an array of 100 million integers with 760 MB of RAM
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note that the unique numbers in the `array` are *not* Python objects; they are
    bytes in the `array`. If we were to dereference any of them, a new Python `int`
    object would be constructed. If you’re going to compute on them, no overall savings
    will occur, but if instead you’re going to pass the array to an external process
    or use only some of the data, you should see a good savings in RAM compared to
    using a `list` of integers.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you’re working with a large array or matrix of numbers with Cython and you
    don’t want an external dependency on `numpy`, be aware that you can store your
    data in an `array` and pass it into Cython for processing without any additional
    memory overhead.
  prefs: []
  type: TYPE_NORMAL
- en: The `array` module works with a limited set of datatypes with varying precisions
    (see [Example 11-5](ch11_split_000.xhtml#lessram-array2)). Choose the smallest
    precision that you need, so that you allocate just as much RAM as needed and no
    more. Be aware that the byte size is platform-dependent—the sizes here refer to
    a 32-bit platform (it states *minimum* size), whereas we’re running the examples
    on a 64-bit laptop.
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-5\. The basic types provided by the `array` module
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'NumPy has arrays that can hold a wider range of datatypes—you have more control
    over the number of bytes per item, and you can use complex numbers and `datetime`
    objects. A `complex128` object takes 16 bytes per item: each item is a pair of
    8-byte floating-point numbers. You can’t store `complex` objects in a Python array,
    but they come for free with `numpy`. If you’d like a refresher on `numpy`, look
    back to [Chapter 6](ch06_split_000.xhtml#matrix_computation).'
  prefs: []
  type: TYPE_NORMAL
- en: In [Example 11-6](ch11_split_000.xhtml#lessram-numpy1), you can see an additional
    feature of `numpy` arrays; you can query for the number of items, the size of
    each primitive, and the combined total storage of the underlying block of RAM.
    Note that this doesn’t include the overhead of the Python object (typically, this
    is tiny in comparison to the data you store in the arrays).
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Be wary of lazy allocation with zeros. In the following example the call to
    `zeros` costs “zero” RAM, whilst the call to `ones` costs 1.5 GB. Both calls will
    ultimately cost 1.5 GB, but the call to `zeros` will allocate the RAM only after
    it is used, so the cost is seen later.
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-6\. Storing more complex types in a `numpy` array
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Using a regular `list` to store many numbers is much less efficient in RAM than
    using an `array` object. More memory allocations have to occur, which each take
    time; calculations also occur on larger objects, which will be less cache friendly,
    and more RAM is used overall, so less RAM is available to other programs.
  prefs: []
  type: TYPE_NORMAL
- en: However, if you do any work on the contents of the `array` in Python numex,
    the primitives are likely to be converted into temporary objects, negating their
    benefit. Using them as a data store when communicating with other processes is
    a great use case for the `array`.
  prefs: []
  type: TYPE_NORMAL
- en: '`numpy` arrays are almost certainly a better choice if you are doing anything
    heavily numeric, as you get more datatype options and many specialized and fast
    functions. You might choose to avoid `numpy` if you want fewer dependencies for
    your project, though Cython works equally well with `array` and `numpy` arrays;
    Numba works with `numpy` arrays only.'
  prefs: []
  type: TYPE_NORMAL
- en: Python provides a few other tools to understand memory usage, as we’ll see in
    the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Using Less RAM in NumPy with NumExpr
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Large vectorized expressions in NumPy (which also happen behind the scenes
    in Pandas) can create intermediate large arrays during complex operations. These
    occur invisibly and may draw attention to themselves only when an out-of-memory
    error occurs. These calculations can also be slow, as large vectors will not be
    cache friendly—a cache can be megabytes or smaller, and large vectors of hundreds
    of megabytes or gigabytes of data will stop the cache from being used effectively.
    NumExpr is a tool that both speeds up and reduces the size of intermediate operations;
    we introduced it in [“numexpr: Making In-Place Operations Faster and Easier”](ch06_split_001.xhtml#matrix_vector_numexpr).'
  prefs: []
  type: TYPE_NORMAL
- en: We’ve also introduced the Memory Profiler before, in [“Using memory_profiler
    to Diagnose Memory Usage”](ch02.xhtml#memory_profiler). Here, we build on it with
    the [IPython Memory Usage tool](https://oreil.ly/i9Vc3), which reports line-by-line
    memory changes inside the IPython shell or in a Jupyter Notebook. Let’s look at
    how these can be used to check that NumExpr is generating a result more efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Remember to install the optional NumExpr when using Pandas. If NumExpr is installed
    in Pandas, calls to `eval` will run more quickly—but note that Pandas does not
    tell you if you *haven’t* installed NumExpr.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll use the cross entropy formula to calculate the error for a machine learning
    classification challenge. *Cross entropy* (or *Log Loss*) is a common metric for
    classification challenges; it penalizes large errors significantly more than small
    errors. Each row in a machine learning problem needs to be scored during the training
    and prediction phases:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="minus l o g upper P left-parenthesis y t vertical-bar y p right-parenthesis
    equals minus left-parenthesis y t l o g left-parenthesis y p right-parenthesis
    plus left-parenthesis 1 minus y t right-parenthesis l o g left-parenthesis 1 minus
    y p right-parenthesis right-parenthesis" display="block"><mrow><mo>-</mo> <mi>l</mi>
    <mi>o</mi> <mi>g</mi> <mi>P</mi> <mo>(</mo> <mi>y</mi> <mi>t</mi> <mo>|</mo> <mi>y</mi>
    <mi>p</mi> <mo>)</mo> <mo>=</mo> <mo>-</mo> <mo>(</mo> <mi>y</mi> <mi>t</mi> <mi>l</mi>
    <mi>o</mi> <mi>g</mi> <mo>(</mo> <mi>y</mi> <mi>p</mi> <mo>)</mo> <mo>+</mo> <mo>(</mo>
    <mn>1</mn> <mo>-</mo> <mi>y</mi> <mi>t</mi> <mo>)</mo> <mi>l</mi> <mi>o</mi> <mi>g</mi>
    <mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>y</mi> <mi>p</mi> <mo>)</mo> <mo>)</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: We’ll use random numbers in the range [0, 1] here to simulate the results of
    a machine learning system from a package like scikit-learn or TensorFlow. [Figure 11-1](ch11_split_000.xhtml#FIG-cross-entropy)
    shows the natural logarithm for the range [0, 1] on the right, and on the left
    it shows the result of calculating the cross entropy for any probability if the
    target is either 0 or 1.
  prefs: []
  type: TYPE_NORMAL
- en: If the target `yt` is 1, the first half of the formula is active and the second
    half goes to zero. If the target is 0, the second half of the formula is active
    and the first part goes to zero. This result is calculated for every row of data
    that needs to be scored and often for many iterations of the machine learning
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '![hpp2 1101](Images/hpp2_1101.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-1\. Cross entropy for `yt` (the “truth”) with values 0 and 1
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In [Example 11-7](ch11_split_000.xhtml#lessram-numexpr), we generate 200 million
    random numbers in the range [0, 1] as `yp`. `yt` is the desired truth—in this
    case, an array of 1s. In a real application, we’d see `yp` generated by a machine
    learning algorithm, and `yt` would be the ground truth mixing 0s and 1s for the
    target we’d be learning provided by the machine learning researcher.
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-7\. The hidden cost of temporaries with large NumPy arrays
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Both `yp` and `yt` take 1.5 GB each, bringing the total RAM usage to just over
    3.1 GB. The `answer` vector has the same dimension as the inputs and thus adds
    a further 1.5 GB. Note that the calculation peaks at 4.5 GB over the current RAM
    usage, so while we end with a 4.6 GB result, we had over 9 GB allocated during
    the calculation. The cross entropy calculation creates several temporaries (notably
    `1 – yt`, `np.log(1 – yp)`, and their multiplication). If you had an 8 GB machine,
    you’d have failed to calculate this result because of memory exhaustion.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [Example 11-8](ch11_split_000.xhtml#lessram-numexpr2), we see the same expression
    placed as a string inside `numexpr.evaluate`. It peaks at 0 GB above the current
    usage—it doesn’t need any additional RAM in this case. Significantly, it also
    calculates much more quickly: the previous direct vector calculation in `In[6]`
    took 18 seconds, while here with NumExpr the same calculation takes 2.6 seconds.'
  prefs: []
  type: TYPE_NORMAL
- en: NumExpr breaks the long vectors into shorter, cache-friendly chunks and processes
    each in series, so local chunks of results are calculated in a cache-friendly
    way. This explains both the requirement for no extra RAM and the increased speed.
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-8\. NumExpr breaks the vectorized calculations into cache-efficient
    chunks
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We can see a similar benefit in Pandas in [Example 11-9](ch11_split_000.xhtml#lessram-numexpr3).
    We construct a DataFrame with the same items as in the preceding example and invoke
    NumExpr by using `df.eval`. The Pandas machinery has to unpack the DataFrame for
    NumExpr, and more RAM is used overall; behind the scenes, NumExpr is still calculating
    the result in a cache-friendly manner. Note that here NumExpr was installed in
    addition to Pandas.
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-9\. Pandas `eval` uses NumExpr if it is available
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Contrast the preceding example with [Example 11-10](ch11_split_000.xhtml#lessram-numexpr4),
    where NumExpr has *not* been installed. The call to `df.eval` falls back on the
    Python interpreter—the same result is calculated but with a 34-second execution
    time (compared to 5.2 seconds before) and a much larger peak memory usage. You
    can test whether NumExpr is installed with `import numexpr`—if this fails, you’ll
    want to install it.
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-10\. Beware that Pandas without NumExpr makes a slow and costly call
    to `eval`!
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Complex vector operations on large arrays will run faster if you can use NumExpr.
    Pandas will not warn you that NumExpr hasn’t been installed, so we recommend that
    you add it as part of your setup if you use `eval`. The IPython Memory Usage tool
    will help you to diagnose where your RAM is being spent if you have large arrays
    to process; this can help you fit more into RAM on your current machine so you
    don’t have to start dividing your data and introducing greater engineering effort.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the RAM Used in a Collection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You may wonder if you can ask Python about the RAM that’s used by each object.
    Python’s `sys.getsizeof(obj)` call will tell us *something* about the memory used
    by an object (most but not all objects provide this). If you haven’t seen it before,
    be warned that it won’t give you the answer you’d expect for a container!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by looking at some primitive types. An `int` in Python is a variable-sized
    object of arbitrary size, well above the range of an 8-byte C integer. The basic
    object costs 24 bytes in Python 3.7 when initialized with 0\. More bytes are added
    as you count to larger numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Behind the scenes, sets of 4 bytes are added each time the size of the number
    you’re counting steps above the previous limit. This affects only the memory usage;
    you don’t see any difference externally.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can do the same check for byte strings. An empty byte sequence costs 33
    bytes, and each additional character adds 1 byte to the cost:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'When we use a list, we see different behavior. `getsizeof` isn’t counting the
    cost of the contents of the list—just the cost of the list itself. An empty list
    costs 64 bytes, and each item in the list takes another 8 bytes on a 64-bit laptop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This is more obvious if we use byte strings—we’d expect to see much larger
    costs than `getsizeof` is reporting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '`getsizeof` reports only some of the cost, and often just for the parent object.
    As noted previously, it also isn’t always implemented, so it can have limited
    usefulness.'
  prefs: []
  type: TYPE_NORMAL
- en: A better tool is `asizeof` in [`pympler`](https://oreil.ly/HGCj5). This will
    walk a container’s hierarchy and make a best guess about the size of each object
    it finds, adding the sizes to a total. Be warned that it is quite slow.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to relying on guesses and assumptions, `asizeof` also cannot count
    memory allocated behind the scenes (such as a module that wraps a C library may
    not report the bytes allocated in the C library). It is best to use this as a
    guide. We prefer to use `memit`, as it gives us an accurate count of memory usage
    on the machine in question.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can check the estimate it makes for a large list—here we’ll use 10 million
    integers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We can validate this estimate by using `memit` to see how the process grew.
    Both reports are approximate—`memit` takes snapshots of the RAM usage reported
    by the operating system while the statement is executing, and `asizeof` asks the
    objects about their size (which may not be reported correctly). We can conclude
    that 10 million integers in a list cost between 320 and 400 MB of RAM.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, the `asizeof` process is slower than using `memit`, but `asizeof`
    can be useful when you’re analyzing small objects. `memit` is probably more useful
    for real-world applications, as the actual memory usage of the process is measured
    rather than inferred.
  prefs: []
  type: TYPE_NORMAL
- en: Bytes Versus Unicode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the (many!) advantages of Python 3.*x* over Python 2.*x* is the switch
    to Unicode-by-default. Previously, we had a mix of single-byte strings and multibyte
    Unicode objects, which could cause a headache during data import and export. In
    Python 3.*x*, all strings are Unicode by default, and if you want to deal in bytes,
    you’ll explicitly create a `byte` sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Unicode objects have more efficient RAM usage in Python 3.7 than in Python 2.*x*.
    In [Example 11-11](ch11_split_000.xhtml#lessram-memory-profiler-stringvsunicode1),
    we can see a 100-million-character sequence being built as a collection of bytes
    and as a Unicode object. The Unicode variant for common characters (here we’re
    assuming UTF 8 as the system’s default encoding) costs the same—a single-byte
    implementation is used for these common characters.
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-11\. Unicode objects can be as cheap as bytes in Python 3.x
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The sigma character (Σ) is more expensive—it is represented in UTF 8 as 2 bytes.
    We gained the flexible Unicode representation from Python 3.3 thanks to [PEP 393](https://oreil.ly/_wNrP).
    It works by observing the range of characters in the string and using a smaller
    number of bytes to represent the lower-order characters, if possible.
  prefs: []
  type: TYPE_NORMAL
- en: The UTF-8 encoding of a Unicode object uses 1 byte per ASCII character and more
    bytes for less frequently seen characters. If you’re not sure about Unicode encodings
    versus Unicode objects, go and watch [Net Batchelder’s “Pragmatic Unicode, or,
    How Do I Stop the Pain?”](https://oreil.ly/udL3A).
  prefs: []
  type: TYPE_NORMAL
- en: Efficiently Storing Lots of Text in RAM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A common problem with text is that it occupies a lot of RAM—but if we want to
    test if we have seen strings before or count their frequency, having them in RAM
    is more convenient than paging them to and from a disk. Storing the strings naively
    is expensive, but tries and directed acyclic word graphs (DAWGs) can be used to
    compress their representation and still allow fast operations.
  prefs: []
  type: TYPE_NORMAL
- en: These more advanced algorithms can save you a significant amount of RAM, which
    means that you might not need to expand to more servers. For production systems,
    the savings can be huge. In this section we’ll look at compressing a `set` of
    strings costing 1.2 GB down to 30 MB using a trie, with only a small change in
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: For this example, we’ll use a text set built from a partial dump of Wikipedia.
    This set contains 11 million unique tokens from a portion of the English Wikipedia
    and takes up 120 MB on disk.
  prefs: []
  type: TYPE_NORMAL
- en: 'The tokens are split on whitespace from their original articles; they have
    variable length and contain Unicode characters and numbers. They look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We’ll use this text sample to test how quickly we can build a data structure
    holding one instance of each unique word, and then we’ll see how quickly we can
    query for a known word (we’ll use the uncommon “Zwiebel,” from the painter Alfred
    Zwiebel). This lets us ask, “Have we seen Zwiebel before?” Token lookup is a common
    problem, and being able to do it quickly is important.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When you try these containers on your own problems, be aware that you will probably
    see different behaviors. Each container builds its internal structures in different
    ways; passing in different types of token is likely to affect the build time of
    the structure, and different lengths of token will affect the query time. Always
    test in a methodical way.
  prefs: []
  type: TYPE_NORMAL
- en: Trying These Approaches on 11 Million Tokens
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Figure 11-2](ch11_split_000.xhtml#FIG-less-ram-8mil_tokens) shows the 11-million-token
    text file (120 MB raw data) stored using a number of containers that we’ll discuss
    in this section. The x-axis shows RAM usage for each container, the y-axis tracks
    the query time, and the size of each point relates to the time taken to build
    the structure (larger means it took longer).'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in this diagram, the `set` and `list` examples use a lot of RAM;
    the `list` example is both large *and* slow! The Marisa trie example is the most
    RAM-efficient for this dataset, while the DAWG runs twice as fast for a relatively
    small increase in RAM usage.
  prefs: []
  type: TYPE_NORMAL
- en: '![DAWG and Tries versus built-in containers for 11 million tokens](Images/hpp2_1102.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-2\. DAWG and tries versus built-in containers
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The figure doesn’t show the lookup time for the naive `list` without sort approach,
    which we’ll introduce shortly, as it takes far too long. Do be aware that you
    must test your problem with a variety of containers—each offers different trade-offs,
    such as construction time and API flexibility.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll build up a process to test the behavior of each container.
  prefs: []
  type: TYPE_NORMAL
- en: list
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s start with the simplest approach. We’ll load our tokens into a `list`
    and then query it using an `O(n)` linear search. You can’t do this on the large
    example that we’ve already mentioned—the search takes far too long—so we’ll demonstrate
    the technique with a much smaller (500,000 tokens) example.
  prefs: []
  type: TYPE_NORMAL
- en: 'In each of the following examples, we use a generator, `text_example.readers`,
    that extracts one Unicode token at a time from the input file. This means that
    the read process uses only a tiny amount of RAM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We’re interested in how quickly we can query this `list`. Ideally, we want
    to find a container that will store our text and allow us to query it and modify
    it without penalty. To query it, we look for a known word a number of times by
    using `timeit`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Our test script reports that approximately 34 MB was used to store the original
    5 MB file as a list, and that the aggregate lookup time was 53 seconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Storing text in an unsorted `list` is obviously a poor idea; the `O(n)` lookup
    time is expensive, as is the memory usage. This is the worst of all worlds! If
    we tried this method on the following larger dataset, we’d expect an aggregate
    lookup time of 25 minutes rather than a fraction of a second for the methods we
    discuss.
  prefs: []
  type: TYPE_NORMAL
- en: We can improve the lookup time by sorting the `list` and using a binary search
    via the [`bisect` module](https://oreil.ly/Uk6ry); this gives us a sensible lower
    bound for future queries. In [Example 11-12](ch11_split_000.xhtml#less_ram_list_sorted_code),
    we time how long it takes to `sort` the `list`. Here, we switch to the larger
    11-million-token set.
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-12\. Timing the `sort` operation to prepare for using `bisect`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we do the same lookup as before, but with the addition of the `index`
    method, which uses `bisect`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In [Example 11-13](ch11_split_000.xhtml#less_ram_bisect_list_output), we see
    that the RAM usage is much larger than before, as we’re loading significantly
    more data. The sort takes a further 0.6 seconds, and the cumulative lookup time
    is 0.01 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-13\. Timings for using `bisect` on a sorted `list`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have a sensible baseline for timing string lookups: RAM usage must get
    better than 871 MB, and the total lookup time should be better than 0.01 seconds.'
  prefs: []
  type: TYPE_NORMAL
- en: set
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using the built-in `set` might seem to be the most obvious way to tackle our
    task. In [Example 11-14](ch11_split_000.xhtml#less_ram_set_code), the `set` stores
    each string in a hashed structure (see [Chapter 4](ch04.xhtml#section-dictionary-sets)
    if you need a refresher). It is quick to check for membership, but each string
    must be stored separately, which is expensive on RAM.
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-14\. Using a `set` to store the data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: As we can see in [Example 11-15](ch11_split_000.xhtml#running_the_set_example),
    the `set` uses more RAM than the `list` by a further 250 MB; however, it gives
    us a very fast lookup time without requiring an additional `index` function or
    an intermediate sorting operation.
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-15\. Running the `set` example
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: If RAM isn’t at a premium, this might be the most sensible first approach.
  prefs: []
  type: TYPE_NORMAL
- en: We have now lost the *ordering* of the original data, though. If that’s important
    to you, note that you could store the strings as keys in a dictionary, with each
    value being an index connected to the original read order. This way, you could
    ask the dictionary if the key is present and for its index.
  prefs: []
  type: TYPE_NORMAL
- en: More efficient tree structures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s introduce a set of algorithms that use RAM more efficiently to represent
    our strings.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 11-3](ch11_split_000.xhtml#FIG-less-ram-trie-dawg-picture) from [Wikimedia
    Commons](http://commons.wikimedia.org) shows the difference in representation
    of four words—“tap”, “taps”, “top”, and “tops”—between a trie and a DAWG.^([1](ch11_split_001.xhtml#idm46122400697944))
    With a `list` or a `set`, each of these words would be stored as a separate string.
    Both the DAWG and the trie share parts of the strings, so that less RAM is used.'
  prefs: []
  type: TYPE_NORMAL
- en: The main difference between these is that a trie shares just common prefixes,
    while a DAWG shares common prefixes and suffixes. In languages (like English)
    that have many common word prefixes and suffixes, this can save a lot of repetition.
  prefs: []
  type: TYPE_NORMAL
- en: Exact memory behavior will depend on your data’s structure. Typically, a DAWG
    cannot assign a value to a key because of the multiple paths from the start to
    the end of the string, but the version shown here can accept a value mapping.
    Tries can also accept a value mapping. Some structures have to be constructed
    in a pass at the start, and others can be updated at any time.
  prefs: []
  type: TYPE_NORMAL
- en: A big strength of some of these structures is that they provide a *common prefix
    search*; that is, you can ask for all words that share the prefix you provide.
    With our list of four words, the result when searching for “ta” would be “tap”
    and “taps.” Furthermore, since these are discovered through the graph structure,
    the retrieval of these results is very fast. If you’re working with DNA, for example,
    compressing millions of short strings by using a trie can be an efficient way
    to reduce RAM usage.
  prefs: []
  type: TYPE_NORMAL
- en: '![DAWG and Trie data structures](Images/hpp2_1103.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-3\. Trie and DAWG structures (image by [Chkno](https://oreil.ly/w71ZI)
    [CC BY-SA 3.0])
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the following sections, we take a closer look at DAWGs, tries, and their
    usage.
  prefs: []
  type: TYPE_NORMAL
- en: Directed acyclic word graph
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The [directed acyclic word graph](https://oreil.ly/4KfVO) (MIT license) attempts
    to efficiently represent strings that share common prefixes and suffixes.
  prefs: []
  type: TYPE_NORMAL
- en: Note that at the time of writing, an [open Pull Request on GitHub](https://oreil.ly/6T5le)
    has to be applied to make this DAWG work with Python 3.7.
  prefs: []
  type: TYPE_NORMAL
- en: In [Example 11-16](ch11_split_000.xhtml#less_ram_dawg_code), you see the very
    simple setup for a DAWG. For this implementation, the DAWG cannot be modified
    after construction; it reads an iterator to construct itself once. The lack of
    post-construction updates might be a deal breaker for your use case. If so, you
    might need to look into using a trie instead. The DAWG does support rich queries,
    including prefix lookups; it also allows persistence and supports storing integer
    indices as values along with byte and record values.
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-16\. Using a DAWG to store the data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in [Example 11-17](ch11_split_000.xhtml#less-ram-dawg-example),
    for the same set of strings it uses significantly less RAM than the earlier `set`
    example during the *construction* phase. More similar input text will cause stronger
    compression.
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-17\. Running the DAWG example
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: More importantly, if we persist the DAWG to disk, as shown in [Example 11-18](ch11_split_000.xhtml#less-ram-dawg-example2),
    and then load it back into a fresh Python instance, we see a dramatic reduction
    in RAM usage—the disk file and the memory usage after loading are both 70 MB;
    this is a significant savings compared to the 1.2 GB `set` variant we built earlier!
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-18\. Loading the DAWG that was built and saved in an earlier session
    is more RAM efficient
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Given that you’ll typically create a DAWG once and then load it many times,
    you’ll benefit from the construction costs repeatedly after you’ve persisted the
    structure to disk.
  prefs: []
  type: TYPE_NORMAL
- en: Marisa trie
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The [Marisa trie](https://oreil.ly/tDvVQ) (dual-licensed LGPL and BSD) is a
    static [trie](https://oreil.ly/suBhE) using Cython bindings to an external library.
    As it is static, it cannot be modified after construction. Like the DAWG, it supports
    storing integer indices as values, as well as byte values and record values.
  prefs: []
  type: TYPE_NORMAL
- en: A key can be used to look up a value, and vice versa. All keys sharing the same
    prefix can be found efficiently. The trie’s contents can be persisted. [Example 11-19](ch11_split_001.xhtml#less_ram_marisa_trie_code)
    illustrates using a Marisa trie to store our sample data.
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-19\. Using a Marisa trie to store the data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: In [Example 11-20](ch11_split_001.xhtml#less-ram-marisa-trie-example), we can
    see that lookup times are slower than those offered by the DAWG.
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-20\. Running the Marisa trie example
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The trie offers a further saving in memory on this dataset. While the lookups
    are a little slower in [Example 11-21](ch11_split_001.xhtml#less-ram-marisa-trie-example2),
    the disk and RAM usage are approximately 30 MB in the following snippet, if we
    save the trie to disk and then load it back into a fresh process; this is twice
    as good as what the DAWG achieved.
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-21\. Loading the Trie that was built and saved in an earlier session
    is more RAM efficient
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The trade-off between storage sizes after construction and lookup times will
    need to be investigated for your application. You may find that using one of these
    “works just well enough,” so you might avoid benchmarking other options and simply
    move on to your next challenge. We suggest that the Marisa trie is your first
    choice in this case; it has more stars than the DAWG on GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: Using tries (and DAWGs) in production systems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The trie and DAWG data structures offer good benefits, but you must still benchmark
    them on your problem rather than blindly adopting them. If you have overlapping
    sequences in your strings, you’ll likely see a RAM improvement.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tries and DAWGs are less well known, but they can provide strong benefits in
    production systems. We have an impressive success story in [“Large-Scale Social
    Media Analysis at Smesh (2014)”](ch12.xhtml#lessons-from-field-alex). Jamie Matthews
    at DabApps (a Python software house based in the United Kingdom) also has a story
    about the use of tries in client systems to enable more efficient and cheaper
    deployments for customers:'
  prefs: []
  type: TYPE_NORMAL
- en: At DabApps, we often try to tackle complex technical architecture problems by
    dividing them into small, self-contained components, usually communicating over
    the network using HTTP. This approach (referred to as a *service-oriented* or
    *microservice* architecture) has all sorts of benefits, including the possibility
    of reusing or sharing the functionality of a single component between multiple
    projects.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'One such task that is often a requirement in our consumer-facing client projects
    is postcode geocoding. This is the task of converting a full UK postcode (for
    example: BN1 1AG) into a latitude and longitude coordinate pair, to enable the
    application to perform geospatial calculations such as distance measurement.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: At its most basic, a geocoding database is a simple mapping between strings
    and can conceptually be represented as a dictionary. The dictionary keys are the
    postcodes, stored in a normalized form (BN11AG), and the values are a representation
    of the coordinates (we used a geohash encoding, but for simplicity imagine a comma-separated
    pair such as 50.822921,-0.142871).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The UK has approximately 1.7 million postcodes. Naively loading the full dataset
    into a Python dictionary, as described previously, uses several hundred megabytes
    of memory. Persisting this data structure to disk using Python’s native Pickle
    format requires an unacceptably large amount of storage space. We knew we could
    do better.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We experimented with several different in-memory and on-disk storage and serialization
    formats, including storing the data externally in databases such as Redis and
    LevelDB, and compressing the key/value pairs. Eventually, we hit on the idea of
    using a trie. Tries are extremely efficient at representing large numbers of strings
    in memory, and the available open source libraries (we chose “marisa-trie”) make
    them very simple to use.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The resulting application, including a tiny web API built with the Flask framework,
    uses only 30 MB of memory to represent the entire UK postcode database, and can
    comfortably handle a high volume of postcode lookup requests. The code is simple;
    the service is very lightweight and painless to deploy and run on a free hosting
    platform such as Heroku, with no external requirements or dependencies on databases.
    Our implementation is open source, available at [*https://github.com/j4mie/postcodeserver*](https://github.com/j4mie/postcodeserver).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Jamie Matthews, technical director of DabApps.com (UK)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: DAWG and tries are powerful data structures that can help you save RAM and time
    in exchange for a little additional effort in preparation. These data structures
    will be unfamiliar to many developers, so consider separating this code into a
    module that is reasonably isolated from the rest of your code to simplify maintenance.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling More Text with Scikit-Learn’s FeatureHasher
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scikit-learn is Python’s best-known machine learning framework, and it has excellent
    support for text-based natural language processing (NLP) challenges. Here, we’ll
    look at classifying public posts from Usenet archives to one of 20 prespecified
    categories; this is similar to the two-category spam classification process that
    cleans up our email inboxes.
  prefs: []
  type: TYPE_NORMAL
- en: One difficulty with text processing is that the vocabulary under analysis quickly
    explodes. The English language uses many nouns (e.g., the names of people and
    places, medical labels, and religious terms) and verbs (the “doing words” that
    often end in “-ing,” like “running,” “taking,” “making,” and “talking”) and their
    conjugations (turning the verb “talk” into “talked,” “talking,” “talks”), along
    with all the other rich forms of language. Punctuation and capitalization add
    an extra nuance to the representation of words.
  prefs: []
  type: TYPE_NORMAL
- en: A powerful and simple technique for classifying text is to break the original
    text into *n-grams*, often unigrams, bigrams, and trigrams (also known as 1-grams,
    2-grams, and 3-grams). A sentence like “there is a cat and a dog” can be turned
    into unigrams (“there,” “is,” “a,” and so on), bigrams (“there is,” “is a,” “a
    cat,” etc.), and trigrams (“there is a,” “is a cat,” “a cat and,” …).
  prefs: []
  type: TYPE_NORMAL
- en: There are 7 unigrams, 6 bigrams, and 5 trigrams for this sentence; in total
    this sentence can be represented in this form by a vocabulary of 6 unique unigrams
    (since the term “a” is used twice), 6 unique bigrams, and 5 unique trigrams, making
    17 descriptive items in total. As you can see, the n-gram vocabulary used to represent
    a sentence quickly grows; some terms are very common, and some are very rare.
  prefs: []
  type: TYPE_NORMAL
- en: There are techniques to control the explosion of a vocabulary, such as eliminating
    stop-words (removing the most common and often uninformative terms, like “a,”
    “the,” and “of”), lowercasing everything, and ignoring less frequent types of
    terms (such as punctuation, numbers, and brackets). If you practice natural language
    processing, you’ll quickly come across these approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing DictVectorizer and FeatureHasher
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we look at the Usenet classification task, let’s look at two of scikit-learn’s
    feature processing tools that help with NLP challenges. The first is `DictVectorizer`,
    which takes a dictionary of terms and their frequencies and converts them into
    a variable-width sparse matrix (we will discuss sparse matrices in [“SciPy’s Sparse
    Matrices”](ch11_split_001.xhtml#SEC-sparse)). The second is `FeatureHasher`, which
    converts the same dictionary of terms and frequencies into a fixed-width sparse
    matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '[Example 11-22](ch11_split_001.xhtml#less_ram_dict_vectorizer) shows two sentences—“there
    is a cat” and “there is a cat and a dog”—in which terms are shared between the
    sentences and the term “a” is used twice in one of the sentences. `DictVectorizer`
    is given the sentences in the call to `fit`; in a first pass it builds a list
    of words into an internal `vocabulary_`, and in a second pass it builds up a sparse
    matrix containing a reference to each term and its count.'
  prefs: []
  type: TYPE_NORMAL
- en: Doing two passes takes longer than the one pass of `FeatureHasher`, and storing
    a vocabulary costs additional RAM. Building a vocabulary is often a serial process;
    by avoiding this stage, the feature hashing can potentially operate in parallel
    for additional speed.
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-22\. Lossless text representation with `DictVectorizer`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: To make the output a little clearer, see the Pandas DataFrame view of the matrix
    `X` in [Figure 11-4](ch11_split_001.xhtml#FIG-less-ram-dictvect-dataframe), where
    the columns are set to the vocabulary. Note that here we’ve made a *dense* representation
    of the matrix—we have 2 rows and 6 columns, and each of the 12 cells contains
    a number. In the sparse form, we store only the 10 counts that are present and
    we do not store anything for the 2 items that are missing. With a larger corpus,
    the larger storage required by a dense representation, containing mostly 0s, quickly
    becomes prohibitive. For NLP the sparse representation is standard.
  prefs: []
  type: TYPE_NORMAL
- en: '![DictVectorizer transformed ouptput](Images/hpp2_1104.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-4\. Transformed output of `DictVectorizer` shown in a Pandas DataFrame
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: One feature of `DictVectorizer` is that we can give it a matrix and reverse
    the process. In [Example 11-23](ch11_split_001.xhtml#less_ram_dict_vectorizer_reverse),
    we use the vocabulary to recover the original frequency representation. Note that
    this does *not* recover the original sentence; there’s more than one way to interpret
    the ordering of words in the first example (both “there is a cat” and “a cat is
    there” are valid interpretations). If we used bigrams, we’d start to introduce
    a constraint on the ordering of words.
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-23\. Reversing the output of matrix `X` to the original dictionary
    representation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '`FeatureHasher` takes the same input and generates a similar output but with
    one key difference: it does not store a vocabulary and instead employs a hashing
    algorithm to assign token frequencies to columns.'
  prefs: []
  type: TYPE_NORMAL
- en: We’ve already looked at hash functions in [“How Do Dictionaries and Sets Work?”](ch04.xhtml#dict_set_how_work).
    A hash converts a unique item (in this case a text token) into a number, where
    multiple unique items might map to the same hashed value, in which case we get
    a collision. Good hash functions cause few collisions. Collisions are inevitable
    if we’re hashing many unique items to a smaller representation. One feature of
    a hash function is that it can’t easily be reversed, so we can’t take a hashed
    value and convert it back to the original token.
  prefs: []
  type: TYPE_NORMAL
- en: In [Example 11-24](ch11_split_001.xhtml#less_ram_featurehasher), we ask for
    a fixed-width 10-column matrix—the default is a fixed-width matrix of 1 million
    elements, but we’ll use a tiny matrix here to show a collision. The default 1-million-element
    width is a sensible default for many applications.
  prefs: []
  type: TYPE_NORMAL
- en: The hashing process uses the fast MurmurHash3 algorithm, which transforms each
    token into a number; this is then converted into the range we specify. Larger
    ranges have few collisions; a small range like our range of 10 will have many
    collisions. Since every token has to be mapped to one of only 10 columns, we’ll
    get many collisions if we add a lot of sentences.
  prefs: []
  type: TYPE_NORMAL
- en: The output `X` has 2 rows and 10 columns; each token maps to one column, and
    we don’t immediately know which column represents each word since hashing functions
    are one-way, so we can’t map the output back to the input. In this case we can
    deduce, using `extra_token_dict`, that the tokens `there` and `is` both map to
    column 8, so we get nine 0s and one count of 2 in column 8.
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-24\. Using a 10-column `FeatureHasher` to show a hash collision
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Despite the occurrence of collisions, more than enough signal is often retained
    in this representation (assuming the default number of columns is used) to enable
    similar quality machine learning results with `FeatureHasher` compared to `DictVectorizer`.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing DictVectorizer and FeatureHasher on a Real Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If we take the full 20 Newsgroups dataset, we have 20 categories, with approximately
    18,000 emails spread across the categories. While some categories such as “sci.med”
    are relatively unique, others like “comp.os.ms-windows.misc” and “comp.windows.x”
    will contain emails that share similar terms. The machine learning task is to
    correctly identify the correct newsgroup from the 20 options for each item in
    the test set. The test set has approximately 4,000 emails; the training set used
    to learn the mapping of terms to the matching category has approximately 14,000
    emails.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this example does *not* deal with some of the necessities of a realistic
    training challenge. We haven’t stripped newsgroup metadata, which can be used
    to overfit on this challenge; rather than generalize just from the text of the
    emails, some extraneous metadata artificially boosts the scores. We have randomly
    shuffled the emails. Here, we’re not trying to achieve a single excellent machine
    learning result; instead, we’re demonstrating that a lossy hashed representation
    can be equivalent to a nonlossy and more memory-hungry variant.
  prefs: []
  type: TYPE_NORMAL
- en: In [Example 11-25](ch11_split_001.xhtml#less_ram_featurehasher_real_world),
    we take 18,846 documents and build up a training and test set representation using
    both `DictVectorizer` and `FeatureHasher` with unigrams, bigrams, and trigrams.
    The `DictVectorizer` sparse array has shape `(14,134, 4,335,793)` for the training
    set, where our 14,134 emails are represented using 4 million tokens. Building
    the vocabulary and transforming the training data takes 42 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: Contrast this with the `FeatureHasher`, which has a fixed 1-million-element-wide
    hashed representation, and where the transformation takes 21 seconds. Note that
    in both cases, roughly 9.8 million nonzero items are stored in the sparse matrices,
    so they’re storing similar quantities of information. The hashed version stores
    approximately 10,000 fewer items because of collisions.
  prefs: []
  type: TYPE_NORMAL
- en: If we’d used a dense matrix, we’d have 14 thousand rows by 10 million columns,
    making 140,000,000,000 cells of 8 bytes each—significantly more RAM than is typically
    available in any current machine. Only a tiny fraction of this matrix would be
    nonzero. Sparse matrices avoid this RAM consumption.
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-25\. Comparing `DictVectorizer` and `FeatureHasher` on a real machine
    learning problem
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Critically, the `LogisticRegression` classifier used on `DictVectorizer` takes
    30% longer to train with 4 million columns compared to the 1 million columns used
    by the `FeatureHasher`. Both show a score of 0.89, so for this challenge the results
    are reasonably equivalent.
  prefs: []
  type: TYPE_NORMAL
- en: Using `FeatureHasher`, we’ve achieved the same score on the test set, built
    our training matrix faster, and avoided building and storing a vocabulary, and
    we’ve trained faster than using the more common `DictVectorizer` approach. In
    exchange, we’ve lost the ability to transform a hashed representation back into
    the original features for debugging and explanation, and since we often want the
    ability to diagnose *why* a decision was made, this might be too costly a trade
    for you to make.
  prefs: []
  type: TYPE_NORMAL
- en: SciPy’s Sparse Matrices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [“Introducing DictVectorizer and FeatureHasher”](ch11_split_001.xhtml#dictvectorizer),
    we created a large feature representation using `DictVectorizer`, which uses a
    sparse matrix in the background. These sparse matrices can be used for general
    computation as well and are extremely useful when working with sparse data.
  prefs: []
  type: TYPE_NORMAL
- en: 'A *sparse matrix* is a matrix in which most matrix elements are 0\. For these
    sorts of matrices, there are many ways to encode the nonzero values and then simply
    say “all other values are zero.” In addition to these memory savings, many algorithms
    have special ways of dealing with sparse matrices that give additional computational
    benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The simplest implementation of this is for `COO` matrices in SciPy, where for
    each non-zero element we store the value in addition to the location of the value.
    This means that for each nonzero value, we store three numbers in total. As long
    as our matrix has at least 66% zero entries, we are reducing the amount of memory
    needed to represent the data as a sparse matrix as opposed to a standard `numpy`
    array. However, `COO` matrices are generally used only to *construct* sparse matrices
    and not for doing actual computation (for that, [CSR/CSC](https://oreil.ly/nHc3h)
    is preferred).
  prefs: []
  type: TYPE_NORMAL
- en: We can see in [Figure 11-5](ch11_split_001.xhtml#FIG-sparse) that for low densities,
    sparse matrices are much faster than their dense counterparts. On top of this,
    they also use much less memory.
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing the runtime speed of sparse matrices vs dense matrices for various
    values for matrix density. Sparse matrix is created with `sparse = scipy.sparse.random(2024,
    2024, density).tocsr()` and dense matrix is created with `dense = sparse.todense()`](Images/hpp2_1105.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-5\. Sparse versus dense matrix multiplication
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In [Figure 11-6](ch11_split_001.xhtml#FIG-sparse-footprint), the dense matrix
    is always using 32.7 MB of memory (2048 × 2048 × 64-bit). However, a sparse matrix
    of 20% density uses only 10 MB, representing a 70% savings! As the density of
    the sparse matrix goes up, `numpy` quickly dominates in terms of speed because
    of the benefits of vectorization and better cache performance.
  prefs: []
  type: TYPE_NORMAL
- en: '![Footprint for 2048 x 2048 dense versus sparse matrices at different densities](Images/hpp2_1106.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-6\. Sparse versus dense memory footprint
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This extreme reduction in memory use is partly why the speeds are so much better.
    In addition to running only the multiplication operation for elements that are
    nonzero (thus reducing the number of operations needed), we also don’t need to
    allocate such a large amount of space to save our result in. This is the push
    and pull of speedups with sparse arrays—it is a balance between losing the use
    of efficient caching and vectorization versus not having to do a lot of the calculations
    associated with the zero values of the matrix.
  prefs: []
  type: TYPE_NORMAL
- en: One operation that sparse matrices are particularly good at is cosine similarity.
    In fact, when creating a `DictVectorizer`, as we did in [“Introducing DictVectorizer
    and FeatureHasher”](ch11_split_001.xhtml#dictvectorizer), it’s common to use cosine
    similarity to see how similar two pieces of text are. In general for these item-to-item
    comparisons (where the value of a particular matrix element is compared to another
    matrix element), sparse matrices do quite well. Since the calls to `numpy` are
    the same whether we are using a normal matrix or a sparse matrix, we can benchmark
    the benefits of using a sparse matrix without changing the code of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: While this is impressive, there are severe limitations. The amount of support
    for sparse matrices is quite low, and unless you are running special sparse algorithms
    or doing only basic operations, you’ll probably hit a wall in terms of support.
    In addition, SciPy’s `sparse` module offers multiple implementations of sparse
    matrices, all of which have different benefits and drawbacks. Understanding which
    is the best one to use and when to use it demands some expert knowledge and often
    leads to conflicting requirements. As a result, sparse matrices probably aren’t
    something you’ll be using often, but when they are the correct tool, they are
    invaluable.
  prefs: []
  type: TYPE_NORMAL
- en: Tips for Using Less RAM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generally, if you can avoid putting it into RAM, do. Everything you load costs
    you RAM. You might be able to load just a part of your data, for example, using
    a [memory-mapped file](https://oreil.ly/l7ekl); alternatively, you might be able
    to use generators to load only the part of the data that you need for partial
    computations rather than loading it all at once.
  prefs: []
  type: TYPE_NORMAL
- en: If you are working with numeric data, you’ll almost certainly want to switch
    to using `numpy` arrays—the package offers many fast algorithms that work directly
    on the underlying primitive objects. The RAM savings compared to using lists of
    numbers can be huge, and the time savings can be similarly amazing. Furthermore,
    if you are dealing with very sparse arrays, using SciPy’s sparse array functionality
    can save incredible amounts of memory, albeit with a reduced feature set as compared
    to normal `numpy` arrays.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re working with strings, stick to `str` rather than `bytes` unless you
    have strong reasons to work at the byte level. Dealing with a myriad set of text
    encodings is painful by hand, and UTF-8 (or other Unicode formats) tends to make
    these problems disappear. If you’re storing many Unicode objects in a static structure,
    you probably want to investigate the DAWG and trie structures that we’ve just
    discussed.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re working with lots of bit strings, investigate `numpy` and the [`bitarray`](https://oreil.ly/Oz4-2)
    package; both have efficient representations of bits packed into bytes. You might
    also benefit from looking at Redis, which offers efficient storage of bit patterns.
  prefs: []
  type: TYPE_NORMAL
- en: 'The PyPy project is experimenting with more efficient representations of homogeneous
    data structures, so long lists of the same primitive type (e.g., integers) might
    cost much less in PyPy than the equivalent structures in CPython. The [MicroPython](http://micropython.org)
    project will be interesting to anyone working with embedded systems: this tiny-memory-footprint
    implementation of Python is aiming for Python 3 compatibility.'
  prefs: []
  type: TYPE_NORMAL
- en: It goes (almost!) without saying that you know you have to benchmark when you’re
    trying to optimize on RAM usage, and that it pays handsomely to have a unit test
    suite in place before you make algorithmic changes.
  prefs: []
  type: TYPE_NORMAL
- en: Having reviewed ways of compressing strings and storing numbers efficiently,
    we’ll now look at trading accuracy for storage space.
  prefs: []
  type: TYPE_NORMAL
- en: Probabilistic Data Structures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Probabilistic data structures allow you to make trade-offs in accuracy for immense
    decreases in memory usage. In addition, the number of operations you can do on
    them is much more restricted than with a `set` or a trie. For example, with a
    single HyperLogLog++ structure using 2.56 KB, you can count the number of unique
    items up to approximately 7,900,000,000 items with 1.625% error.
  prefs: []
  type: TYPE_NORMAL
- en: This means that if we’re trying to count the number of unique license plate
    numbers for cars, and our HyperLogLog++ counter said there were 654,192,028, we
    would be confident that the actual number is between 643,561,407 and 664,822,648\.
    Furthermore, if this accuracy isn’t sufficient, you can simply add more memory
    to the structure and it will perform better. Giving it 40.96 KB of resources will
    decrease the error from 1.625% to 0.4%. However, storing this data in a `set`
    would take 3.925 GB, even assuming no overhead!
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the HyperLogLog++ structure would only be able to count a
    `set` of license plates and merge with another `set`. So, for example, we could
    have one structure for every state, find how many unique license plates are in
    each of those states, and then merge them all to get a count for the whole country.
    If we were given a license plate, we couldn’t tell you with very good accuracy
    whether we’ve seen it before, and we couldn’t give you a sample of license plates
    we have already seen.
  prefs: []
  type: TYPE_NORMAL
- en: Probabilistic data structures are fantastic when you have taken the time to
    understand the problem and need to put something into production that can answer
    a very small set of questions about a very large set of data. Each structure has
    different questions it can answer at different accuracies, so finding the right
    one is just a matter of understanding your requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Much of this section goes into a deep dive into the mechanisms that power many
    of the popular probabilistic data structures. This is useful, because once you
    understand the mechanisms, you can use parts of them in algorithms you are designing.
    If you are just beginning with probabilistic data structures, it may be useful
    to first look at the real-world example ([“Real-World Example”](ch11_split_001.xhtml#pds_example))
    before diving into the internals.
  prefs: []
  type: TYPE_NORMAL
- en: In almost all cases, probabilistic data structures work by finding an alternative
    representation for the data that is more compact and contains the relevant information
    for answering a certain set of questions. This can be thought of as a type of
    lossy compression, where we may lose some aspects of the data but we retain the
    necessary components. Since we are allowing the loss of data that isn’t necessarily
    relevant for the particular set of questions we care about, this sort of lossy
    compression can be much more efficient than the lossless compression we looked
    at before with tries. It is because of this that the choice of which probabilistic
    data structure you will use is quite important—you want to pick one that retains
    the right information for your use case!
  prefs: []
  type: TYPE_NORMAL
- en: Before we dive in, it should be made clear that all the “error rates” here are
    defined in terms of *standard deviations*. This term comes from describing Gaussian
    distributions and says how spread out the function is around a center value. When
    the standard deviation grows, so do the number of values further away from the
    center point. Error rates for probabilistic data structures are framed this way
    because all the analyses around them are probabilistic. So, for example, when
    we say that the HyperLogLog++ algorithm has an error of <math alttext="e r r equals
    StartFraction 1.04 Over StartRoot m EndRoot EndFraction"><mrow><mi>e</mi> <mi>r</mi>
    <mi>r</mi> <mo>=</mo> <mfrac><mrow><mn>1</mn><mo>.</mo><mn>04</mn></mrow> <msqrt><mi>m</mi></msqrt></mfrac></mrow></math>
    , we mean that 68% of the time the error will be smaller than *err*, 95% of the
    time it will be smaller than 2 × *err*, and 99.7% of the time it will be smaller
    than 3 × *err*.^([2](ch11_split_001.xhtml#idm46122399754392))
  prefs: []
  type: TYPE_NORMAL
- en: Very Approximate Counting with a 1-Byte Morris Counter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ll introduce the topic of probabilistic counting with one of the earliest
    probabilistic counters, the Morris counter (by Robert Morris of the NSA and Bell
    Labs). Applications include counting millions of objects in a restricted-RAM environment
    (e.g., on an embedded computer), understanding large data streams, and working
    on problems in AI like image and speech recognition.
  prefs: []
  type: TYPE_NORMAL
- en: The Morris counter keeps track of an exponent and models the counted state as
    <math alttext="2 Superscript e x p o n e n t"><msup><mn>2</mn> <mrow><mi>e</mi><mi>x</mi><mi>p</mi><mi>o</mi><mi>n</mi><mi>e</mi><mi>n</mi><mi>t</mi></mrow></msup></math>
    (rather than a correct count)—it provides an *order of magnitude* estimate. This
    estimate is updated using a probabilistic rule.
  prefs: []
  type: TYPE_NORMAL
- en: We start with the exponent set to 0\. If we ask for the *value* of the counter,
    we’ll be given `pow(2,*exponent*)=1` (the keen reader will note that this is off
    by one—we did say this was an *approximate* counter!). If we ask the counter to
    increment itself, it will generate a random number (using the uniform distribution),
    and it will test if `random.uniform(0, 1)` `<=` `1/pow(2,*exponent*)`, which will
    always be true (`pow(2,0) == 1`). The counter increments, and the exponent is
    set to 1.
  prefs: []
  type: TYPE_NORMAL
- en: The second time we ask the counter to increment itself, it will test if `random.uniform(0,
    1) <= 1/pow(2,1)`. This will be true 50% of the time. If the test passes, the
    exponent is incremented. If not, the exponent is not incremented for this increment
    request.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 11-1](ch11_split_001.xhtml#table_morris_counter) shows the likelihoods
    of an increment occurring for each of the first exponents.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 11-1\. Morris counter details
  prefs: []
  type: TYPE_NORMAL
- en: '| Exponent | pow(2,*exponent*) | P(*increment*) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 2 | 0.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 4 | 0.25 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 8 | 0.125 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 16 | 0.0625 |'
  prefs: []
  type: TYPE_TB
- en: '| … | … | … |'
  prefs: []
  type: TYPE_TB
- en: '| 254 | 2.894802e+76 | 3.454467e-77 |'
  prefs: []
  type: TYPE_TB
- en: The maximum we could approximately count where we use a single unsigned byte
    for the exponent is `math.pow(2,255) == 5e76`. The error relative to the actual
    count will be fairly large as the counts increase, but the RAM savings is tremendous,
    as we use only 1 byte rather than the 32 unsigned bytes we’d otherwise have to
    use. [Example 11-26](ch11_split_001.xhtml#memory_morris_example_code) shows a
    simple implementation of the Morris counter.
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-26\. Simple Morris counter implementation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Using this implementation, we can see in [Example 11-27](ch11_split_001.xhtml#morris_counter_example)
    that the first request to increment the counter succeeds, the second succeeds,
    and the third doesn’t.^([3](ch11_split_001.xhtml#idm46122399671816))
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-27\. Morris counter library example
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: In [Figure 11-7](ch11_split_001.xhtml#FIG-morris-counter), the thick black line
    shows a normal integer incrementing on each iteration. On a 64-bit computer, this
    is an 8-byte integer. The evolution of three 1-byte Morris counters is shown as
    dotted lines; the y-axis shows their values, which approximately represent the
    true count for each iteration. Three counters are shown to give you an idea about
    their different trajectories and the overall trend; the three counters are entirely
    independent of one another.
  prefs: []
  type: TYPE_NORMAL
- en: '![Three 1-byte Morris Counters](Images/hpp2_1107.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-7\. Three 1-byte Morris counters versus an 8-byte integer
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This diagram gives you some idea about the error to expect when using a Morris
    counter. Further details about the error behavior are available [online](http://bit.ly/Morris_error).
  prefs: []
  type: TYPE_NORMAL
- en: K-Minimum Values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the Morris counter, we lose any sort of information about the items we insert.
    That is to say, the counter’s internal state is the same whether we do `.add("micha")`
    or `.add("ian")`. This extra information is useful and, if used properly, could
    help us have our counters count only unique items. In this way, calling `.add("micha")`
    thousands of times would increase the counter only once.
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement this behavior, we will exploit properties of hashing functions
    (see [“Hash Functions and Entropy”](ch04.xhtml#SEC-dict-set-hash-and-entropy)
    for a more in-depth discussion of hash functions). The main property we would
    like to take advantage of is the fact that the hash function takes input and *uniformly*
    distributes it. For example, let’s assume we have a hash function that takes in
    a string and outputs a number between 0 and 1\. For that function to be uniform
    means that when we feed it in a string, we are equally likely to get a value of
    0.5 as a value of 0.2 or any other value. This also means that if we feed it in
    many string values, we would expect the values to be relatively evenly spaced.
    Remember, this is a probabilistic argument: the values won’t always be evenly
    spaced, but if we have many strings and try this experiment many times, they will
    tend to be evenly spaced.'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we took 100 items and stored the hashes of those values (the hashes
    being numbers from 0 to 1). Knowing the spacing is even means that instead of
    saying, “We have 100 items,” we could say, “We have a distance of 0.01 between
    every item.” This is where the K-Minimum Values algorithm finally comes in^([4](ch11_split_001.xhtml#idm46122399315416))--if
    we keep the `k` smallest unique hash values we have seen, we can approximate the
    overall spacing between hash values and infer the total number of items. In [Figure 11-8](ch11_split_001.xhtml#FIG-kmv-hash-density),
    we can see the state of a K-Minimum Values structure (also called a KMV) as more
    and more items are added. At first, since we don’t have many hash values, the
    largest hash we have kept is quite large. As we add more and more, the largest
    of the `k` hash values we have kept gets smaller and smaller. Using this method,
    we can get error rates of <math alttext="upper O left-parenthesis StartRoot StartFraction
    2 Over pi left-parenthesis k minus 2 right-parenthesis EndFraction EndRoot right-parenthesis"><mrow><mi>O</mi>
    <mo>(</mo> <msqrt><mfrac><mn>2</mn> <mrow><mi>π</mi><mo>(</mo><mi>k</mi><mo>-</mo><mn>2</mn><mo>)</mo></mrow></mfrac></msqrt>
    <mo>)</mo></mrow></math> .
  prefs: []
  type: TYPE_NORMAL
- en: The larger `k` is, the more we can account for the hashing function we are using
    not being completely uniform for our particular input and for unfortunate hash
    values. An example of unfortunate hash values would be hashing `['A', 'B', 'C']`
    and getting the values `[0.01, 0.02, 0.03]`. If we start hashing more and more
    values, it is less and less probable that they will clump up.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, since we are keeping only the smallest *unique* hash values, the
    data structure considers only unique inputs. We can see this easily because if
    we are in a state where we store only the smallest three hashes and currently
    `[0.1, 0.2, 0.3]` are the smallest hash values, then if we add in something with
    the hash value of `0.4`, our state also will not change. Similarly, if we add
    more items with a hash value of `0.3`, our state will also not change. This is
    a property called *idempotence*; it means that if we do the same operation, with
    the same inputs, on this structure multiple times, the state will not be changed.
    This is in contrast to, for example, an `append` on a `list`, which will always
    change its value. This concept of idempotence carries on to all of the data structures
    in this section except for the Morris counter.
  prefs: []
  type: TYPE_NORMAL
- en: '[Example 11-28](ch11_split_001.xhtml#memory_simple_kmv) shows a very basic
    K-Minimum Values implementation. Of note is our use of a `sortedset`, which, like
    a set, can contain only unique items. This uniqueness gives our `KMinValues` structure
    idempotence for free. To see this, follow the code through: when the same item
    is added more than once, the `data` property does not change.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Density of hash space for K-Min Value structures](Images/hpp2_1108.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-8\. The value stores in a KMV structure as more elements are added
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Example 11-28\. Simple `KMinValues` implementation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Using the `KMinValues` implementation in the Python package [`countmemaybe`](https://oreil.ly/YF6uO)
    ([Example 11-29](ch11_split_001.xhtml#countmemaybe_kminvalues_implementation)),
    we can begin to see the utility of this data structure. This implementation is
    very similar to the one in [Example 11-28](ch11_split_001.xhtml#memory_simple_kmv),
    but it fully implements the other set operations, such as union and intersection.
    Also note that “size” and “cardinality” are used interchangeably (the word “cardinality”
    is from set theory and is used more in the analysis of probabilistic data structures).
    Here, we can see that even with a reasonably small value for `k`, we can store
    50,000 items and calculate the cardinality of many set operations with relatively
    low error.
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-29\. `countmemaybe` `KMinValues` implementation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](ch11_split_001.xhtml#co_using_less_ram_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: We put 50,000 elements into `kmv1`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](ch11_split_001.xhtml#co_using_less_ram_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: '`kmv2` also gets 50,000 elements, 25,000 of which are also in `kmv1`.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: With these sorts of algorithms, the choice of hash function can have a drastic
    effect on the quality of the estimates. Both of these implementations use [`mmh3`](https://pypi.org/project/mmh3),
    a Python implementation of `murmurhash3` that has nice properties for hashing
    strings. However, different hash functions could be used if they are more convenient
    for your particular dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Bloom Filters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes we need to be able to do other types of set operations, for which
    we need to introduce new types of probabilistic data structures. *Bloom filters*
    were created to answer the question of whether we’ve seen an item before.^([5](ch11_split_001.xhtml#idm46122398960952))
  prefs: []
  type: TYPE_NORMAL
- en: Bloom filters work by having multiple hash values in order to represent a value
    as multiple integers. If we later see something with the same set of integers,
    we can be reasonably confident that it is the same value.
  prefs: []
  type: TYPE_NORMAL
- en: To do this in a way that efficiently utilizes available resources, we implicitly
    encode the integers as the indices of a list. This could be thought of as a list
    of `bool` values that are initially set to `False`. If we are asked to add an
    object with hash values `[10, 4, 7]`, we set the tenth, fourth, and seventh indices
    of the list to `True`. In the future, if we are asked if we have seen a particular
    item before, we simply find its hash values and check if all the corresponding
    spots in the `bool` list are set to `True`.
  prefs: []
  type: TYPE_NORMAL
- en: This method gives us no false negatives and a controllable rate of false positives.
    If the Bloom filter says we have not seen an item before, we can be 100% sure
    that we haven’t seen the item before. On the other hand, if the Bloom filter states
    that we *have* seen an item before, there is a probability that we actually have
    not and we are simply seeing an erroneous result. This erroneous result comes
    from the fact that we will have hash collisions, and sometimes the hash values
    for two objects will be the same even if the objects themselves are not the same.
    However, in practice Bloom filters are set to have error rates below 0.5%, so
    this error can be acceptable.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can simulate having as many hash functions as we want simply by having two
    hash functions that are independent of each other. This method is called *double
    hashing*. If we have a hash function that gives us two independent hashes, we
    can do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The modulo ensures that the resulting hash values are 32-bit (we would modulo
    by `2^64 - 1` for 64-bit hash functions).
  prefs: []
  type: TYPE_NORMAL
- en: 'The exact length of the `bool` list and the number of hash values per item
    we need will be fixed based on the capacity and the error rate we require. With
    some reasonably simple statistical arguments,^([6](ch11_split_001.xhtml#idm46122398840536))
    we see that the ideal values are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block" alttext="n u m normal bar b i t s equals minus c a p a
    c i t y dot StartFraction l o g left-parenthesis e r r o r right-parenthesis Over
    l o g left-parenthesis 2 right-parenthesis squared EndFraction"><mrow><mi>n</mi>
    <mi>u</mi> <mi>m</mi> <mo>_</mo> <mi>b</mi> <mi>i</mi> <mi>t</mi> <mi>s</mi> <mo>=</mo>
    <mo>–</mo> <mi>c</mi> <mi>a</mi> <mi>p</mi> <mi>a</mi> <mi>c</mi> <mi>i</mi> <mi>t</mi>
    <mi>y</mi> <mo>×</mo> <mfrac><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mo>(</mo><mi>e</mi><mi>r</mi><mi>r</mi><mi>o</mi><mi>r</mi><mo>)</mo></mrow>
    <mrow><mi>l</mi><mi>o</mi><mi>g</mi><msup><mrow><mo>(</mo><mn>2</mn><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></mfrac></mrow></math><math display="block" alttext="n
    u m normal bar h a s h e s equals n u m normal bar b i t s dot StartFraction l
    o g left-parenthesis 2 right-parenthesis Over c a p a c i t y EndFraction"><mrow><mi>n</mi>
    <mi>u</mi> <mi>m</mi> <mo>_</mo> <mi>h</mi> <mi>a</mi> <mi>s</mi> <mi>h</mi> <mi>e</mi>
    <mi>s</mi> <mo>=</mo> <mi>n</mi> <mi>u</mi> <mi>m</mi> <mo>_</mo> <mi>b</mi> <mi>i</mi>
    <mi>t</mi> <mi>s</mi> <mo>×</mo> <mfrac><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mo>(</mo><mn>2</mn><mo>)</mo></mrow>
    <mrow><mi>c</mi><mi>a</mi><mi>p</mi><mi>a</mi><mi>c</mi><mi>i</mi><mi>t</mi><mi>y</mi></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: If we wish to store 50,000 objects (no matter how big the objects themselves
    are) at a false positive rate of 0.05% (that is to say, 0.05% of the times we
    say we have seen an object before, we actually have not), it would require 791,015
    bits of storage and 11 hash functions.
  prefs: []
  type: TYPE_NORMAL
- en: To further improve our efficiency in terms of memory use, we can use single
    bits to represent the `bool` values (a native `bool` actually takes 4 bits). We
    can do this easily by using the `bitarray` module. [Example 11-30](ch11_split_001.xhtml#simple_bloom_filter_implemintation)
    shows a simple Bloom filter implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-30\. Simple Bloom filter implementation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: What happens if we insert more items than we specified for the capacity of the
    Bloom filter? At the extreme end, all the items in the `bool` list will be set
    to `True`, in which case we say that we have seen every item. This means that
    Bloom filters are very sensitive to what their initial capacity was set to, which
    can be quite aggravating if we are dealing with a set of data whose size is unknown
    (for example, a stream of data).
  prefs: []
  type: TYPE_NORMAL
- en: One way of dealing with this is to use a variant of Bloom filters called *scalable*
    Bloom filters.^([7](ch11_split_001.xhtml#idm46122398750824)) They work by chaining
    together multiple Bloom filters whose error rates vary in a specific way.^([8](ch11_split_001.xhtml#idm46122398488968))
    By doing this, we can guarantee an overall error rate and add a new Bloom filter
    when we need more capacity. To check if we’ve seen an item before, we iterate
    over all of the sub-Blooms until either we find the object or we exhaust the list.
    A sample implementation of this structure can be seen in [Example 11-31](ch11_split_001.xhtml#memory_scaling_bloom),
    where we use the previous Bloom filter implementation for the underlying functionality
    and have a counter to simplify knowing when to add a new Bloom.
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-31\. Simple scaling Bloom filter implementation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Another way of dealing with this is using a method called *timing Bloom filters*.
    This variant allows elements to be expired out of the data structure, thus freeing
    up space for more elements. This is especially nice for dealing with streams,
    since we can have elements expire after, say, an hour and have the capacity set
    large enough to deal with the amount of data we see per hour. Using a Bloom filter
    this way would give us a nice view into what has been happening in the last hour.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using this data structure will feel much like using a `set` object. In the
    following interaction, we use the scalable Bloom filter to add several objects,
    test if we’ve seen them before, and then try to experimentally find the false
    positive rate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also do unions with Bloom filters to join multiple sets of items:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](ch11_split_001.xhtml#co_using_less_ram_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The value of `51` is not in `bloom_a`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](ch11_split_001.xhtml#co_using_less_ram_CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the value of `24` is not in `bloom_b`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](ch11_split_001.xhtml#co_using_less_ram_CO2-3)'
  prefs: []
  type: TYPE_NORMAL
- en: However, the `bloom` object contains all the objects in both `bloom_a` and `bloom_b`!
  prefs: []
  type: TYPE_NORMAL
- en: One caveat is that you can take the union of only two Blooms with the same capacity
    and error rate. Furthermore, the final Bloom’s used capacity can be as high as
    the sum of the used capacities of the two Blooms unioned to make it. This means
    that you could start with two Bloom filters that are a little more than half full
    and, when you union them together, get a new Bloom that is over capacity and not
    reliable!
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A [Cuckoo filter](https://oreil.ly/oD6UM) is a modern Bloom filter–like data
    structure that provides similar functionality to a Bloom filter with the addition
    of better object deletion. Furthermore, the Cuckoo filter in most cases has lower
    overhead, leading to better space efficiencies than the Bloom filter. When a fixed
    number of objects needs to be kept track of, it is often a better option. However,
    its performance degrades dramatically when its load limit is reached and there
    are no options for automatic scaling of the data structure (as we saw with the
    scaling Bloom filter).
  prefs: []
  type: TYPE_NORMAL
- en: The work of doing fast set inclusion in a memory-efficient way is a very important
    and active part of database research. Cuckoo filters, [Bloomier filters](https://arxiv.org/abs/0807.0928),
    [Xor filters](https://arxiv.org/abs/1912.08258), and more are being constantly
    released. However, for most applications, it is still best to stick with the well-known,
    well-supported Bloom filter.
  prefs: []
  type: TYPE_NORMAL
- en: LogLog Counter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[LogLog-type counters](http://bit.ly/LL-type_counters) are based on the realization
    that the individual bits of a hash function can also be considered random. That
    is to say, the probability of the first bit of a hash being `1` is 50%, the probability
    of the first two bits being `01` is 25%, and the probability of the first three
    bits being `001` is 12.5%. Knowing these probabilities, and keeping the hash with
    the most `0`s at the beginning (i.e., the least probable hash value), we can come
    up with an estimate of how many items we’ve seen so far.'
  prefs: []
  type: TYPE_NORMAL
- en: A good analogy for this method is flipping coins. Imagine we would like to flip
    a coin 32 times and get heads every time. The number 32 comes from the fact that
    we are using 32-bit hash functions. If we flip the coin once and it comes up tails,
    we will store the number `0`, since our best attempt yielded 0 heads in a row.
    Since we know the probabilities behind this coin flip, we can also tell you that
    our longest series was `0` long, and you can estimate that we’ve tried this experiment
    `2^0 = 1` time. If we keep flipping our coin and we’re able to get 10 heads before
    getting a tail, then we would store the number `10`. Using the same logic, you
    could estimate that we’ve tried the experiment `2^10 = 1024` times. With this
    system, the highest we could count would be the maximum number of flips we consider
    (for 32 flips, this is `2^32 = 4,294,967,296`).
  prefs: []
  type: TYPE_NORMAL
- en: 'To encode this logic with LogLog-type counters, we take the binary representation
    of the hash value of our input and see how many `0`s there are before we see our
    first `1`. The hash value can be thought of as a series of 32 coin flips, where
    `0` means a flip for heads and `1` means a flip for tails (i.e., `000010101101`
    means we flipped four heads before our first tails, and `010101101` means we flipped
    one head before flipping our first tail). This gives us an idea of how many tries
    happened before this hash value was reached. The mathematics behind this system
    is almost equivalent to that of the Morris counter, with one major exception:
    we acquire the “random” values by looking at the actual input instead of using
    a random number generator. This means that if we keep adding the same value to
    a LogLog counter, its internal state will not change. [Example 11-32](ch11_split_001.xhtml#simple_implementation_of_loglog_register)
    shows a simple implementation of a LogLog counter.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-32\. Simple implementation of LogLog register
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The biggest drawback of this method is that we may get a hash value that increases
    the counter right at the beginning and skews our estimates. This would be similar
    to flipping 32 tails on the first try. To remedy this, we should have many people
    flipping coins at the same time and combine their results. The law of large numbers
    tells us that as we add more and more flippers, the total statistics become less
    affected by anomalous samples from individual flippers. The exact way that we
    combine the results is the root of the difference between LogLog-type methods
    (classic LogLog, SuperLogLog, HyperLogLog, HyperLogLog++, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: We can accomplish this “multiple flipper” method by taking the first couple
    of bits of a hash value and using that to designate which of our flippers had
    that particular result. If we take the first 4 bits of the hash, this means we
    have `2^4 = 16` flippers. Since we used the first 4 bits for this selection, we
    have only 28 bits left (corresponding to 28 individual coin flips per coin flipper),
    meaning each counter can count only up to `2^28 = 268,435,456`. In addition, there
    is a constant (alpha) that depends on the number of flippers, which normalizes
    the estimation.^([9](ch11_split_001.xhtml#idm46122397477912)) All of this together
    gives us an algorithm with <math alttext="1.05 slash StartRoot m EndRoot"><mrow><mn>1</mn>
    <mo>.</mo> <mn>05</mn> <mo>/</mo> <msqrt><mi>m</mi></msqrt></mrow></math> accuracy,
    where *m* is the number of registers (or flippers) used. [Example 11-33](ch11_split_001.xhtml#simple_implementation_of_loglog_example11-26)
    shows a simple implementation of the LogLog algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-33\. Simple implementation of LogLog
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: In addition to this algorithm deduplicating similar items by using the hash
    value as an indicator, it has a tunable parameter that can be used to dial whatever
    sort of accuracy versus storage compromise you are willing to make.
  prefs: []
  type: TYPE_NORMAL
- en: In the `__len__` method, we are averaging the estimates from all of the individual
    LogLog registers. This, however, is not the most efficient way to combine the
    data! This is because we may get some unfortunate hash values that make one particular
    register spike up while the others are still at low values. Because of this, we
    are able to achieve an error rate of only <math alttext="upper O left-parenthesis
    StartFraction 1.30 Over StartRoot m EndRoot EndFraction right-parenthesis"><mrow><mi>O</mi>
    <mo>(</mo> <mfrac><mrow><mn>1</mn><mo>.</mo><mn>30</mn></mrow> <msqrt><mi>m</mi></msqrt></mfrac>
    <mo>)</mo></mrow></math> , where *m* is the number of registers used.
  prefs: []
  type: TYPE_NORMAL
- en: SuperLogLog was devised as a fix to this problem.^([10](ch11_split_001.xhtml#idm46122397252728))
    With this algorithm, only the lowest 70% of the registers were used for the size
    estimate, and their value was limited by a maximum value given by a restriction
    rule. This addition decreased the error rate to <math alttext="upper O left-parenthesis
    StartFraction 1.05 Over StartRoot m EndRoot EndFraction right-parenthesis"><mrow><mi>O</mi>
    <mo>(</mo> <mfrac><mrow><mn>1</mn><mo>.</mo><mn>05</mn></mrow> <msqrt><mi>m</mi></msqrt></mfrac>
    <mo>)</mo></mrow></math> . This is counterintuitive, since we got a better estimate
    by disregarding information!
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, HyperLogLog came out in 2007 and gave us further accuracy gains.^([11](ch11_split_001.xhtml#idm46122397244632))
    It did so by changing the method of averaging the individual registers: instead
    of just averaging, we use a spherical averaging scheme that also has special considerations
    for different edge cases the structure could be in. This brings us to the current
    best error rate of <math alttext="upper O left-parenthesis StartFraction 1.04
    Over StartRoot m EndRoot EndFraction right-parenthesis"><mrow><mi>O</mi> <mo>(</mo>
    <mfrac><mrow><mn>1</mn><mo>.</mo><mn>04</mn></mrow> <msqrt><mi>m</mi></msqrt></mfrac>
    <mo>)</mo></mrow></math> . In addition, this formulation removes a sorting operation
    that is necessary with SuperLogLog. This can greatly speed up the performance
    of the data structure when you are trying to insert items at a high volume. [Example 11-34](ch11_split_001.xhtml#example_11-27)
    shows a basic implementation of HyperLogLog.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-34\. Simple implementation of HyperLogLog
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: The only further increase in accuracy was given by the HyperLogLog++ algorithm,
    which increases the accuracy of the data structure while it is relatively empty.
    When more items are inserted, this scheme reverts to standard HyperLogLog. This
    is actually quite useful, since the statistics of the LogLog-type counters require
    a lot of data to be accurate—having a scheme for allowing better accuracy with
    fewer number items greatly improves the usability of this method. This extra accuracy
    is achieved by having a smaller but more accurate HyperLogLog structure that can
    later be converted into the larger structure that was originally requested. Also,
    some empirically derived constants are used in the size estimates that remove
    biases.
  prefs: []
  type: TYPE_NORMAL
- en: Real-World Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To obtain a better understanding of the data structures, we first created a
    dataset with many unique keys, and then one with duplicate entries. Figures [11-9](ch11_split_001.xhtml#FIG-prob-ds-comparison_repeating)
    and [11-10](ch11_split_001.xhtml#FIG-prob-ds-comparison_unique) show the results
    when we feed these keys into the data structures we’ve just looked at and periodically
    query, “How many unique entries have there been?” We can see that the data structures
    that contain more stateful variables (such as HyperLogLog and `KMinValues`) do
    better, since they more robustly handle bad statistics. On the other hand, the
    Morris counter and the single LogLog register can quickly have very high error
    rates if one unfortunate random number or hash value occurs. For most of the algorithms,
    however, we know that the number of stateful variables is directly correlated
    with the error guarantees, so this makes sense.
  prefs: []
  type: TYPE_NORMAL
- en: '![hpp2 1109](Images/hpp2_1109.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-9\. The approximate count of duplicate data using multiple probabilistic
    data structures. To do this, we generate 60,000 items with many duplicates and
    insert them into the various probabilistic data structures. Graphed is the structures
    prediction of the number of unique items during the process.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Looking just at the probabilistic data structures that have the best performance
    (which really are the ones you will probably use), we can summarize their utility
    and their approximate memory usage (see [Table 11-2](ch11_split_001.xhtml#memory_pd_comparison)).
    We can see a huge change in memory usage depending on the questions we care to
    ask. This simply highlights the fact that when using a probabilistic data structure,
    you must first consider what questions you really need to answer about the dataset
    before proceeding. Also note that only the Bloom filter’s size depends on the
    number of elements. The sizes of the HyperLogLog and `KMinValues` structures are
    sizes dependent *only* on the error rate.
  prefs: []
  type: TYPE_NORMAL
- en: As another, more realistic test, we chose to use a dataset derived from the
    text of Wikipedia. We ran a very simple script in order to extract all single-word
    tokens with five or more characters from all articles and store them in a newline-separated
    file. The question then was, “How many unique tokens are there?” The results can
    be seen in [Table 11-3](ch11_split_001.xhtml#memory_pd_wiki_comparison).
  prefs: []
  type: TYPE_NORMAL
- en: Table 11-2\. Comparison of major probabilistic data structures and the set operations
    available on them
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Size | Union^([a](ch11_split_001.xhtml#idm46122397003048)) | Intersection
    | Contains | Size^([b](ch11_split_001.xhtml#idm46122397001176)) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| HyperLogLog | Yes ( <math alttext="upper O left-parenthesis StartFraction
    1.04 Over StartRoot m EndRoot EndFraction right-parenthesis"><mrow><mi>O</mi>
    <mo>(</mo> <mfrac><mrow><mn>1</mn><mo>.</mo><mn>04</mn></mrow> <msqrt><mi>m</mi></msqrt></mfrac>
    <mo>)</mo></mrow></math> ) | Yes | No^([c](ch11_split_001.xhtml#memory_caveat))
    | No | 2.704 MB |'
  prefs: []
  type: TYPE_TB
- en: '| KMinValues | Yes ( <math alttext="upper O left-parenthesis StartRoot StartFraction
    2 Over pi left-parenthesis m minus 2 right-parenthesis EndFraction EndRoot right-parenthesis"><mrow><mi>O</mi>
    <mo>(</mo> <msqrt><mfrac><mn>2</mn> <mrow><mi>π</mi><mo>(</mo><mi>m</mi><mo>-</mo><mn>2</mn><mo>)</mo></mrow></mfrac></msqrt>
    <mo>)</mo></mrow></math> ) | Yes | Yes | No | 20.372 MB |'
  prefs: []
  type: TYPE_TB
- en: '| Bloom filter | Yes ( <math alttext="upper O left-parenthesis StartFraction
    0.78 Over StartRoot m EndRoot EndFraction right-parenthesis"><mrow><mi>O</mi>
    <mo>(</mo> <mfrac><mrow><mn>0</mn><mo>.</mo><mn>78</mn></mrow> <msqrt><mi>m</mi></msqrt></mfrac>
    <mo>)</mo></mrow></math> ) | Yes | No^([c](ch11_split_001.xhtml#memory_caveat))
    | Yes | 197.8 MB |'
  prefs: []
  type: TYPE_TB
- en: '| ^([a](ch11_split_001.xhtml#idm46122397003048-marker)) Union operations occur
    without increasing the error rate.^([b](ch11_split_001.xhtml#idm46122397001176-marker))
    Size of data structure with 0.05% error rate, 100 million unique elements, and
    using a 64-bit hashing function.^([c](ch11_split_001.xhtml#memory_caveat-marker))
    These operations *can* be done but at a considerable penalty in terms of accuracy.
    |'
  prefs: []
  type: TYPE_TB
- en: 'The major takeaway from this experiment is that if you are able to specialize
    your code, you can get amazing speed and memory gains. This has been true throughout
    the entire book: when we specialized our code in [“Selective Optimizations: Finding
    What Needs to Be Fixed”](ch06_split_001.xhtml#matrix_selective_optimizations),
    we were similarly able to get speed increases.'
  prefs: []
  type: TYPE_NORMAL
- en: '![hpp2 1110](Images/hpp2_1110.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-10\. The approximate count of unique data using multiple probabilistic
    data structures. To do this, we insert the numbers 1 through 100,000 into the
    data structures. Graphed is the structures prediction of the number of unique
    items during the process.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Probabilistic data structures are an algorithmic way of specializing your code.
    We store only the data we need in order to answer specific questions with given
    error bounds. By having to deal with only a subset of the information given, we
    can not only make the memory footprint much smaller, but also perform most operations
    over the structure faster.
  prefs: []
  type: TYPE_NORMAL
- en: Table 11-3\. Size estimates for the number of unique words in Wikipedia
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Elements | Relative error | Processing time^([a](ch11_split_001.xhtml#idm46122396962408))
    | Structure size^([b](ch11_split_001.xhtml#idm46122396961256)) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Morris counter^([c](ch11_split_001.xhtml#idm46122396958776)) | 1,073,741,824
    | 6.52% | 751s | 5 bits |'
  prefs: []
  type: TYPE_TB
- en: '| LogLog register | 1,048,576 | 78.84% | 1,690s | 5 bits |'
  prefs: []
  type: TYPE_TB
- en: '| LogLog | 4,522,232 | 8.76% | 2,112s | 5 bits |'
  prefs: []
  type: TYPE_TB
- en: '| HyperLogLog | 4,983,171 | –0.54% | 2,907s | 40 KB |'
  prefs: []
  type: TYPE_TB
- en: '| KMinValues | 4,912,818 | 0.88% | 3,503s | 256 KB |'
  prefs: []
  type: TYPE_TB
- en: '| Scaling Bloom | 4,949,358 | 0.14% | 10,392s | 11,509 KB |'
  prefs: []
  type: TYPE_TB
- en: '| True value | 4,956,262 | 0.00% | ----- | 49,558 KB^([d](ch11_split_001.xhtml#idm46122396935256))
    |'
  prefs: []
  type: TYPE_TB
- en: '| ^([a](ch11_split_001.xhtml#idm46122396962408-marker)) Processing time has
    been adjusted to remove the time to read the dataset from disk. We also use the
    simple implementations provided earlier for testing.^([b](ch11_split_001.xhtml#idm46122396961256-marker))
    Structure size is theoretical given the amount of data since the implementations
    used were not optimized.^([c](ch11_split_001.xhtml#idm46122396958776-marker))
    Since the Morris counter doesn’t deduplicate input, the size and relative error
    are given with regard to the total number of values.^([d](ch11_split_001.xhtml#idm46122396935256-marker))
    The dataset is 49,558 KB considering only unique tokens, or 8.742 GB with all
    tokens. |'
  prefs: []
  type: TYPE_TB
- en: As a result, whether or not you use probabilistic data structures, you should
    always keep in mind what questions you are going to be asking of your data and
    how you can most effectively store that data in order to ask those specialized
    questions. This may come down to using one particular type of list over another,
    using one particular type of database index over another, or maybe even using
    a probabilistic data structure to throw out all but the relevant data!
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch11_split_000.xhtml#idm46122400697944-marker)) This example is taken
    from the Wikipedia article on the [deterministic acyclic finite state automaton](https://oreil.ly/M_pYe)
    (DAFSA). DAFSA is another name for DAWG. The accompanying image is from Wikimedia
    Commons.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch11_split_001.xhtml#idm46122399754392-marker)) These numbers come from
    the 68-95-99.7 rule of Gaussian distributions. More information can be found in
    the [Wikipedia entry](http://bit.ly/Gaussian).
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch11_split_001.xhtml#idm46122399671816-marker)) A more fully fleshed-out
    implementation that uses an `array` of bytes to make many counters is available
    at [*https://github.com/ianozsvald/morris_counter*](https://github.com/ianozsvald/morris_counter).
  prefs: []
  type: TYPE_NORMAL
- en: '^([4](ch11_split_001.xhtml#idm46122399315416-marker)) Kevin Beyer et al., “On
    Synopses for Distinct-Value Estimation under Multiset Operations,” in *Proceedings
    of the 2007 ACM SIGMOD International Conference on Management of Data* (New York:
    ACM, 2007), 199–210, [*https://doi.org/10.1145/1247480.1247504*](https://doi.org/10.1145/1247480.1247504).'
  prefs: []
  type: TYPE_NORMAL
- en: '^([5](ch11_split_001.xhtml#idm46122398960952-marker)) Burton H. Bloom, “Space/Time
    Trade-Offs in Hash Coding with Allowable Errors,” *Communications of the ACM*
    13, no. 7 (1970): 422–26, [*http://doi.org/10.1145/362686.362692*](http://doi.org/10.1145/362686.362692).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch11_split_001.xhtml#idm46122398840536-marker)) The [Wikipedia page on
    Bloom filters](http://bit.ly/Bloom_filter) has a very simple proof for the properties
    of a Bloom filter.
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch11_split_001.xhtml#idm46122398750824-marker)) Paolo Sérgio Almeida et
    al., “Scalable Bloom Filters,” *Information Processing Letters* 101, no. 6 (2007)
    255–61, [*https://doi.org/10.1016/j.ipl.2006.10.007*](https://doi.org/10.1016/j.ipl.2006.10.007).
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch11_split_001.xhtml#idm46122398488968-marker)) The error values actually
    decrease like the geometric series. This way, when you take the product of all
    the error rates, it approaches the desired error rate.
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch11_split_001.xhtml#idm46122397477912-marker)) A full description of
    the basic LogLog and SuperLogLog algorithms can be found at [*http://bit.ly/algorithm_desc*](http://bit.ly/algorithm_desc).
  prefs: []
  type: TYPE_NORMAL
- en: '^([10](ch11_split_001.xhtml#idm46122397252728-marker)) Marianne Durand and
    Philippe Flajolet, “LogLog Counting of Large Cardinalities,” in *Algorithms—ESA
    2003*, ed. Giuseppe Di Battista and Uri Zwick, vol. 2832 (Berlin, Heidelberg:
    Springer, 2003), 605–17, [*https://doi.org/10.1007/978-3-540-39658-1_55*](https://doi.org/10.1007/978-3-540-39658-1_55).'
  prefs: []
  type: TYPE_NORMAL
- en: '^([11](ch11_split_001.xhtml#idm46122397244632-marker)) Philippe Flajolet et
    al., “HyperLogLog: The Analysis of a Near-Optimal Cardinality Estimation Algorithm,”
    in *AOFA ’07: Proceedings of the 2007 International Conference on Analysis of
    Algorithms*, (AOFA, 2007), 127–46.'
  prefs: []
  type: TYPE_NORMAL
