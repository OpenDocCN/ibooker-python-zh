- en: Chapter 14\. Scraping JavaScript
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Client-side scripting languages are languages that are run in the browser itself,
    rather than on a web server. The success of a client-side language depends on
    your browser’s ability to interpret and execute the language correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'While there are hundreds of server-side programming languages, there’s only
    one client-side programming language. This is because of the difficulty of getting
    every browser manufacturer to agree on a standard. This is a good thing when it
    comes to web scraping: the fewer languages there are to deal with, the better.'
  prefs: []
  type: TYPE_NORMAL
- en: Other Client-Side Programming Languages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Some readers may take issue with the sentence: “There’s only one client-side
    programming language.” Technically, languages such as ActionScript and VBScript
    exist. However, these are no longer supported and, in the case of VBScript, was
    only ever supported by a single browser. For this reason, they are very rarely
    seen.'
  prefs: []
  type: TYPE_NORMAL
- en: If you want to be pedantic about it, absolutely anyone can make a new client-side
    programming language! There are likely many of them out there! The only issue
    is getting widespread support by browsers to make that language effective and
    put it into use by others.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some have also argued that CSS and HTML are programming languages in themselves.
    I agree with this in theory. Lara Schenck has an excellent and entertaining blog
    post on the subject: [*https://notlaura.com/is-css-turing-complete/*](https://notlaura.com/is-css-turing-complete/).'
  prefs: []
  type: TYPE_NORMAL
- en: However, in practice, CSS and HTML are generally treated as markup languages
    separate from “programming languages” and, besides, are covered extensively in
    this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'JavaScript is, by far, the most common and most well-supported client-side
    scripting language on the web today. It can be used to collect information for
    user tracking, submit forms without reloading the page, embed multimedia, and
    even power entire online games. Even deceptively simple-looking pages often contain
    multiple pieces of JavaScript. You can find it embedded between `script` tags
    in the page’s source code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: A Brief Introduction to JavaScript
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having at least some idea of what is going on in the code you are scraping can
    be immensely helpful. With that in mind, it’s a good idea to familiarize yourself
    with JavaScript.
  prefs: []
  type: TYPE_NORMAL
- en: '*JavaScript* is a weakly typed language, with a syntax that is often compared
    to C++ and Java. Although certain elements of the syntax, such as operators, loops,
    and arrays, might be similar, the weak typing and scriptlike nature of the language
    can make it a difficult beast for some programmers to deal with.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the following recursively calculates values in the Fibonacci sequence
    up to 100 and prints them to the browser’s developer console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Notice that all variables are identified by preceding them with `var`. This
    is similar to the `$` sign in PHP or the type declaration (`int`, `String`, `List`, etc.)
    in Java or C++. Python is unusual in that it doesn’t have this sort of explicit
    variable declaration.
  prefs: []
  type: TYPE_NORMAL
- en: 'JavaScript is also good at passing functions around:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This might seem daunting at first, but it becomes simple if you think in terms
    of lambda expressions (covered in [Chapter 5](ch05.html#c-5)). The variable `fibonacci` is
    defined as a function. The value of its function returns a function that prints
    increasingly large values in the Fibonacci sequence. Each time it is called, it
    returns the Fibonacci-calculating function, which executes again and increases
    the values in the function.
  prefs: []
  type: TYPE_NORMAL
- en: 'You might also see functions like these written in the arrow syntax introduced
    in JavaScript ES6 (ECMAScript 6, introduced in 2015):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Here, I’m using the JavaScript keyword `const` to indicate a constant variable
    that will not be reassigned later. You might also see the keyword `let`, indicating
    a variable that may be reassigned. These were also introduced in ES6.
  prefs: []
  type: TYPE_NORMAL
- en: Passing around functions as variables is also extremely useful when it comes
    to handling user actions and callbacks, and it is worth getting comfortable with
    this style of programming when it comes to reading JavaScript.
  prefs: []
  type: TYPE_NORMAL
- en: Common JavaScript Libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although the core JavaScript language is important to know, you can’t get far
    on the modern web without using at least one of the language’s many third-party
    libraries. You might see one or more of these commonly used libraries when looking
    at page source code.
  prefs: []
  type: TYPE_NORMAL
- en: Executing JavaScript by using Python can be extremely time-consuming and processor
    intensive, especially if you’re doing it on a large scale. Knowing your way around
    JavaScript and being able to parse it directly (without needing to execute it
    to acquire the information) can be extremely useful and save you a lot of headaches.
  prefs: []
  type: TYPE_NORMAL
- en: jQuery
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*jQuery* is an extremely common library, used by more than 70% of all websites.^([1](ch14.html#id730)).
    A site using jQuery is readily identifiable because it will contain an import
    to jQuery somewhere in its code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: If jQuery is found on a site, you must be careful when scraping it. jQuery is
    adept at dynamically creating HTML content that appears only after the JavaScript
    is executed. If you scrape the page’s content by using traditional methods, you
    will retrieve only the preloaded page that appears before the JavaScript has created
    the content (this scraping problem is covered in more detail in [“Ajax and Dynamic
    HTML”](#ajax_dynamic_html)).
  prefs: []
  type: TYPE_NORMAL
- en: In addition, these pages are more likely to contain animations, interactive
    content, and embedded media that might make scraping challenging.
  prefs: []
  type: TYPE_NORMAL
- en: Google Analytics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Google Analytics* is used by about 50% of all websites,^([2](ch14.html#id733))
    making it perhaps the most common JavaScript library and the most popular user
    tracking tool on the internet. Both [*http://pythonscraping.com*](http://pythonscraping.com)
    and [*http://www.oreilly.com/*](http://www.oreilly.com/) use Google Analytics.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Determining whether a page is using Google Analytics is easy. It will have
    JavaScript at the bottom, similar to the following (taken from the O’Reilly Media
    site):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This script handles Google Analytics-specific cookies used to track your visit
    from page to page. This can sometimes be a problem for web scrapers that are designed
    to execute JavaScript and handle cookies (such as those that use Selenium, discussed
    later in this chapter).
  prefs: []
  type: TYPE_NORMAL
- en: If a site uses Google Analytics or a similar web analytics system, and you do
    not want the site to know that it’s being crawled or scraped, make sure to discard
    any cookies used for analytics or discard cookies altogether.
  prefs: []
  type: TYPE_NORMAL
- en: Google Maps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you’ve spent any time on the internet, you’ve almost certainly seen *Google
    Maps* embedded in a website. Its API makes it extremely easy to embed maps with
    custom information on any site.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re scraping any sort of location data, understanding how Google Maps
    works makes it easy to obtain well-formatted latitude/longitude coordinates and
    even addresses. One of the most common ways to denote a location in Google Maps
    is through a *marker* (also known as a *pin*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Markers can be inserted into any Google Map by using code such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Python makes it easy to extract all instances of coordinates that occur between
    `google.maps.LatLng(` and `)` to obtain a list of latitude/longitude coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: Using [Google’s Reverse Geocoding API](https://developers.google.com/maps/documentation/javascript/examples/geocoding-reverse),
    you can resolve these coordinate pairs to addresses that are well formatted for
    storage and analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Ajax and Dynamic HTML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Until now, the only way we’ve had of communicating with a web server is to send
    it some sort of HTTP request via the retrieval of a new page. If you’ve ever submitted
    a form or retrieved information from a server without reloading the page, you’ve
    likely used a website that uses Ajax.
  prefs: []
  type: TYPE_NORMAL
- en: Contrary to what some believe, Ajax is not a language but a group of technologies
    used to accomplish a certain task (much like web scraping, come to think of it). *Ajax*
    stands for *Asynchronous JavaScript and XML* and is used to send information to
    and receive it from a web server without making a separate page request.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You should never say, “This website will be written in Ajax.” It would be correct
    to say, “This website will use Ajax to communicate with the web server.”
  prefs: []
  type: TYPE_NORMAL
- en: Like Ajax, *dynamic HTML* (DHTML) is a collection of technologies used for a
    common purpose. DHTML is HTML code, CSS language, or both that changes as client-side
    scripts change HTML elements on the page. A button might appear only after the
    user moves the cursor, a background color might change on a click, or an Ajax
    request might trigger a new block of content to load.
  prefs: []
  type: TYPE_NORMAL
- en: Note that although the word “dynamic” is generally associated with words like
    “moving,” or “changing” the presence of interactive HTML components, moving images,
    or embedded media does not necessarily make a page DHTML, even though it might
    look dynamic. In addition, some of the most boring, static-looking pages on the
    internet can have DHTML processes running behind the scenes that depend on the
    use of JavaScript to manipulate the HTML and CSS.
  prefs: []
  type: TYPE_NORMAL
- en: If you scrape many websites, you will soon run into a situation in which the
    content you are viewing in your browser does not match the content you see in
    the source code you’re retrieving from the site. You might view the output of
    your scraper and scratch your head, trying to figure out where everything you’re
    seeing on the exact same page in your browser has disappeared to.
  prefs: []
  type: TYPE_NORMAL
- en: The web page might also have a loading page that appears to redirect you to
    another page of results, but you’ll notice that the page’s URL never changes when
    this redirect happens.
  prefs: []
  type: TYPE_NORMAL
- en: Both of these are caused by a failure of your scraper to execute the JavaScript
    that is making the magic happen on the page. Without the JavaScript, the HTML
    just sort of sits there, and the site might look very different from what it looks
    like in your web browser, which executes the JavaScript without problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several giveaways that a page might be using Ajax or DHTML to change
    or load the content, but in situations like this, there are only two solutions:
    scrape the content directly from the JavaScript; or use Python packages capable
    of executing the JavaScript itself and scrape the website as you view it in your
    browser.'
  prefs: []
  type: TYPE_NORMAL
- en: Executing JavaScript in Python with Selenium
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Selenium](http://www.seleniumhq.org) is a powerful web scraping tool developed
    originally for website testing. These days, it’s also used when the accurate portrayal
    of websites—as they appear in a browser—is required. Selenium works by automating
    browsers to load the website, retrieve the required data, and even take screenshots
    or assert that certain actions happen on the website.'
  prefs: []
  type: TYPE_NORMAL
- en: Selenium does not contain its own web browser; it requires integration with
    third-party browsers in order to run. If you were to run Selenium with Firefox,
    for example, you would see a Firefox instance open up on your screen, navigate
    to the website, and perform the actions you had specified in the code. Although
    this might be neat to watch, I prefer my scripts to run quietly in the background
    and often use Chrome’s *headless* mode to do that.
  prefs: []
  type: TYPE_NORMAL
- en: A *headless browser* loads websites into memory and executes JavaScript on the
    page, but does it without any graphic rendering of the website to the user. By
    combining Selenium with headless Chrome, you can run an extremely powerful web
    scraper that handles cookies, JavaScript, headers, and everything else you need
    with ease, as if you were using a rendered browser.
  prefs: []
  type: TYPE_NORMAL
- en: Installing and Running Selenium
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can download the Selenium library from [its website](https://pypi.python.org/pypi/selenium) or
    use a third-party installer such as pip to install it from the command line.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Previous versions of Selenium required that you also manually download a webdriver
    file that would allow it to interface with your web browser. This webdriver is
    called such because it is a software *driver* for the web browser. Much like a
    software driver for a hardware device, it allows the Python Selenium package to
    interface with and control your browser.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, because new versions of browsers are released frequently, and
    are much more frequently updated thanks to automatic updates, this meant that
    Selenium drivers also had to be frequently updated. Navigating to the browser
    driver’s website (such as [*http://chromedriver.chromium.org/downloads*](http://chromedriver.chromium.org/downloads)),
    downloading the new file, and replacing the old file was a frequent chore. In
    Selenium 4, released in October 2021, this entire process was replaced by the
    webdriver manager Python package.
  prefs: []
  type: TYPE_NORMAL
- en: 'The webdriver manager can be installed with pip:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'When called, the webdriver manager downloads the latest driver:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Of course, if this script is being run frequently, it’s inefficient to install
    a new driver file each time just in case the Chrome browser was updated since
    the last time it ran. The output of the driver manager installation is simply
    the path in your `driver` directory where the driver is located:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'If you still enjoy downloading files by hand, you can do that by passing your
    own path into the `Service` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Although plenty of pages use Ajax to load data, I’ve created a sample page at
    [*http://pythonscraping.com/pages/javascript/ajaxDemo.html*](http://pythonscraping.com/pages/javascript/ajaxDemo.html) to
    run your scrapers against. This page contains some sample text, hardcoded into
    the page’s HTML, that is replaced by Ajax-generated content after a two-second
    delay. If you were to scrape this page’s data by using traditional methods, you’d
    get only the loading page, without getting the data that you want.
  prefs: []
  type: TYPE_NORMAL
- en: The Selenium library is an API called on the object [`webdriver`](https://selenium-python.readthedocs.io/api.html).
    Note that this is a Python object representing or acting as an interface to the
    webdriver application you downloaded. While the same terms “driver” and “webdriver”
    are often used interchangeably for both things (the Python object and the application
    itself) it’s important to distinguish them conceptually.
  prefs: []
  type: TYPE_NORMAL
- en: The `webdriver` object is a bit like a browser in that it can load websites,
    but it can also be used like a `BeautifulSoup` object to find page elements, interact
    with elements on the page (send text, click, etc.), and do other actions to drive
    the web scrapers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code retrieves text behind an Ajax “wall” on the test page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This creates a new Selenium webdriver, using the Chrome library, which tells
    the webdriver to load a page and then pauses execution for three seconds before
    looking at the page to retrieve the (hopefully loaded) content.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you instantiate a new Chrome webdriver in Python, you can pass it a variety
    of options through the `Options` object. In this case, we’re using the `--headless`
    option to make the webdriver run in the background:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Whether you used the driver manager package to install a driver or downloaded
    it yourself, you must pass this path into the `Service` object, as well as pass
    in your options, to create a new webdriver:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'If everything is configured correctly, the script should take a few seconds
    to run and then result in the following text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Selenium Selectors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In previous chapters, you’ve selected page elements using `BeautifulSoup` selectors,
    such as `find` and `find_all`. Selenium uses a very similar set of methods to
    select elements: `find_element` and `find_elements`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are so many ways to find and select elements from HTML, you might think
    that Selenium would use a wide variety of arguments and keyword arguments for
    these methods. However, for both `find_element` and `find_elements` there are
    only two arguments for both of these functions: the `By` object and the string
    selector.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `By` object specifies how the selector string should be interpreted, with
    the following list of options:'
  prefs: []
  type: TYPE_NORMAL
- en: '`By.ID`'
  prefs: []
  type: TYPE_NORMAL
- en: Used in the example; finds elements by their HTML `id` attribute.
  prefs: []
  type: TYPE_NORMAL
- en: '`By.NAME`'
  prefs: []
  type: TYPE_NORMAL
- en: Finds HTML tags by their `name` attribute. This is handy for HTML forms.
  prefs: []
  type: TYPE_NORMAL
- en: '`By.XPATH`'
  prefs: []
  type: TYPE_NORMAL
- en: Uses an `XPath` expression to select matching elements. The XPath syntax will
    be covered in more detail later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '`By.LINK_TEXT`'
  prefs: []
  type: TYPE_NORMAL
- en: Finds HTML `<a>` tags by the text they contain. For example, a link labeled
    “Next” can be selected using `(By.LINK_TEXT, 'Next')`.
  prefs: []
  type: TYPE_NORMAL
- en: '`By.PARTIAL_LINK_TEXT`'
  prefs: []
  type: TYPE_NORMAL
- en: Similar to `LINK_TEXT` but matches on a partial string.
  prefs: []
  type: TYPE_NORMAL
- en: '`By.TAG_NAME`'
  prefs: []
  type: TYPE_NORMAL
- en: Finds HTML tags by their tag name.
  prefs: []
  type: TYPE_NORMAL
- en: '`By.CLASS_NAME`'
  prefs: []
  type: TYPE_NORMAL
- en: Used to find elements by their HTML `class` attribute. Why is this function `CLASS_NAME` and
    not simply `CLASS`? Using the form `object.CLASS` would create problems for Selenium’s
    Java library, where `.class` is a reserved method. To keep the Selenium syntax
    consistent between languages, `CLASS_NAME` is used instead.
  prefs: []
  type: TYPE_NORMAL
- en: '`By.CSS_SELECTOR`'
  prefs: []
  type: TYPE_NORMAL
- en: Finds elements by their `class`, `id`, or `tag` name, using the `#idName`, `.className`, `tagName` convention.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous example, you used the selector `driver.find_element(By.ID,
    ''content'')`, although the following selectors would have worked as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Of course, if you want to select multiple elements on the page, most of these
    element selectors can return a Python list of elements by using `elements` (i.e.,
    make it plural):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'If you still want to use BeautifulSoup to parse this content, you can, by using
    webdriver’s `page_source` function, which returns the page’s source, as viewed
    by the DOM at that current time, as a string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Waiting to Load
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Note that although the page itself contains an HTML button, Selenium’s `.text` function
    retrieves the text value of the button in the same way that it retrieves all other
    content on the page.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the `time.sleep` pause is changed to one second instead of three, the text
    returned changes to the original:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Although this solution works, it is somewhat inefficient, and implementing it
    could cause problems on a large scale. Page-load times are inconsistent, depending
    on the server load at any particular millisecond, and natural variations occur
    in connection speed. Although this page load should take just over two seconds,
    you’re giving it an entire three seconds to make sure that it loads completely.
    A more efficient solution would repeatedly check for the existence of a particular
    element on a fully loaded page and return only when that element exists.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following program uses the presence of the button with the ID `loadedButton` to
    declare that the page has been fully loaded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This script has several new imports, most notably `WebDriverWait` and `expected​_con⁠ditions`,
    both of which are combined here to form what Selenium calls an* implicit wait*.
  prefs: []
  type: TYPE_NORMAL
- en: 'An implicit wait differs from an explicit wait in that it waits for a certain
    state in the DOM to occur before continuing, while an explicit wait defines a
    hardcoded time as in the previous example, which has a wait of three seconds.
    In an implicit wait, the triggering DOM state is defined by `expected_condition` (note
    that the import is cast to `EC` here, a common convention used for brevity). Expected
    conditions can be many things in the Selenium library, including:'
  prefs: []
  type: TYPE_NORMAL
- en: An alert box pops up.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An element (such as a text box) is put into a *selected* state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The page’s title changes, or text is now displayed on the page or in a specific
    element.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An element is now visible to the DOM, or an element disappears from the DOM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most of these expected conditions require that you specify an element to watch
    for in the first place. Elements are specified using locators. Note that locators
    are not the same as selectors (see [“Selenium Selectors”](#selenium_sidebar) for
    more on selectors). A *locator* is an abstract query language, using the `By` object,
    which can be used in a variety of ways, including to make selectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, a locator is used to find elements with the ID `loadedButton`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Locators also can be used to create selectors, using the `find_element` webdriver
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: If you do not need to use a locator, don’t; it will save you an import. However,
    this handy tool is used for a variety of applications and has a great degree of
    flexibility.
  prefs: []
  type: TYPE_NORMAL
- en: XPath
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*XPath* (short for *XML Path*) is a query language used for navigating and
    selecting portions of an XML document. Founded by the W3C in 1999, it is occasionally
    used in languages such as Python, Java, and C# when dealing with XML documents.'
  prefs: []
  type: TYPE_NORMAL
- en: Although BeautifulSoup does not support XPath, many of the other libraries in
    this book, such as Scrapy and Selenium, do. It often can be used in the same way
    as CSS selectors (such as `mytag#idname`), although it is designed to work with
    more generalized XML documents rather than HTML documents in particular.
  prefs: []
  type: TYPE_NORMAL
- en: 'The XPath syntax has four major concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: Root nodes versus nonroot nodes
  prefs: []
  type: TYPE_NORMAL
- en: '`/div` will select the div node only if it is at the root of the document.'
  prefs: []
  type: TYPE_NORMAL
- en: '`//div` selects all divs anywhere in the document.'
  prefs: []
  type: TYPE_NORMAL
- en: Attribute selection
  prefs: []
  type: TYPE_NORMAL
- en: '`//@href` selects any nodes with the attribute `href`.'
  prefs: []
  type: TYPE_NORMAL
- en: '`//a[@href=''http://google.com'']` selects all links in the document that point
    to Google.'
  prefs: []
  type: TYPE_NORMAL
- en: Selection of nodes by position
  prefs: []
  type: TYPE_NORMAL
- en: '`//a[3]` selects the third link in the document.'
  prefs: []
  type: TYPE_NORMAL
- en: '`//table[last()]` selects the last table in the document.'
  prefs: []
  type: TYPE_NORMAL
- en: '`//a[position() < 3]` selects the first two links in the document.'
  prefs: []
  type: TYPE_NORMAL
- en: Asterisks (*) match any set of characters or nodes and can be used in a variety
    of situations
  prefs: []
  type: TYPE_NORMAL
- en: '`//table/tr/*` selects all children of `tr` tags in all tables (this is good
    for selecting cells using both `th` and `td` tags).'
  prefs: []
  type: TYPE_NORMAL
- en: '`//div[@*]` selects all `div` tags that have any attributes.'
  prefs: []
  type: TYPE_NORMAL
- en: XPath syntax also has many advanced features. Over the years, it has developed
    into a relatively complicated query language, with boolean logic, functions (such
    as `position()`), and a variety of operators not discussed here.
  prefs: []
  type: TYPE_NORMAL
- en: If you have an HTML or XML selection problem that cannot be addressed by the
    functions shown here, see [Microsoft’s XPath Syntax page](https://msdn.microsoft.com/en-us/enus/library/ms256471).
  prefs: []
  type: TYPE_NORMAL
- en: Additional Selenium WebDrivers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, the Chrome WebDriver (ChromeDriver) was used with
    Selenium. In most cases, there is little reason to have a browser pop up on the
    screen and start scraping the web, so running this in headless mode can be convenient.
    However, running in nonheadless mode, and/or using different browser drivers can
    be good practice for a number of reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Troubleshooting. If your code is running in headless mode and fails, the failure
    may be difficult to diagnose without seeing the page in front of you.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can also pause the code execution and interact with the web page or use
    the inspector tools while your scraper is running in order to diagnose problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tests may depend on a specific browser in order to run. A failure in one browser
    but not in another may point at a browser-specific problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In most cases it is preferable to use the webdriver manager to get your browser
    drivers. For instance, you can use the webdriver manager for Firefox and Microsoft
    Edge:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: However, if you need a deprecated browser version or a browser not available
    through the webdriver manager (such as Safari) you may still need to manually
    download the driver files.
  prefs: []
  type: TYPE_NORMAL
- en: Many groups, both official and unofficial, are involved in the creation and
    maintenance of Selenium webdrivers for every major browser today. The Selenium
    group curates a [collection of these webdrivers](http://www.seleniumhq.org/download/) for
    easy reference.
  prefs: []
  type: TYPE_NORMAL
- en: Handling Redirects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Client-side redirects are page redirects that are executed in your browser by
    JavaScript, rather than a redirect performed on the server, before the page content
    is sent. It can sometimes be tricky to tell the difference when visiting a page
    in your web browser. The redirect might happen so fast that you don’t notice any
    delay in loading time and assume that a client-side redirect is actually a server-side
    redirect.
  prefs: []
  type: TYPE_NORMAL
- en: However, when scraping the web, the difference is obvious. A server-side redirect,
    depending on how it is handled, can be traversed easily by Python’s urllib library
    without any help from Selenium (for more information on doing this, see [Chapter 6](ch06.html#c-6)).
    Client-side redirects won’t be handled at all unless something is executing the
    JavaScript.
  prefs: []
  type: TYPE_NORMAL
- en: Selenium is capable of handling these JavaScript redirects in the same way that
    it handles other JavaScript execution; however, the primary issue with these redirects
    is when to stop page execution—that is, how to tell when a page is done redirecting. A
    demo page at [*http://pythonscraping.com/pages/javascript/redirectDemo1.html*](http://pythonscraping.com/pages/javascript/redirectDemo1.html) gives
    an example of this type of redirect, with a two-second pause.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can detect that redirect in a clever way by “watching” an element in the
    DOM when the page initially loads, and then repeatedly calling that element until
    Selenium throws a `StaleElementReferenceException`; the element is no longer attached
    to the page’s DOM and the site has redirected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This script checks the page every half second, with a timeout of 10 seconds,
    although the times used for the checking time and timeout can be easily adjusted
    up or down, as needed.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, you can write a similar loop checking the current URL of the
    page until the URL changes or it matches a specific URL that you’re looking for.
  prefs: []
  type: TYPE_NORMAL
- en: 'Waiting for elements to appear and disappear is a common task in Selenium,
    and you can also use the same `WebDriverWait` function used in the previous button
    loading example. Here you’re providing it a timeout of 15 seconds and an XPath
    selector that looks for the page body content to accomplish the same task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: A Final Note on JavaScript
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most sites today on the internet use JavaScript.^([3](ch14.html#id763)) Fortunately
    for us, in many cases this use of JavaScript will not affect how you scrape the
    page. The JavaScript may be limited to powering a site’s tracking tools, controlling
    a small section of the site, or manipulating a drop-down menu, for example. In
    cases where it does impact how you scrape the site, the JavaScript can be executed
    with tools like Selenium to produce the simple HTML page you’ve been learning
    to scrape in the first part of this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember: just because a site uses JavaScript does not mean that all the traditional
    web scraping tools go out the window. The purpose of JavaScript is ultimately
    to produce HTML and CSS code that can be rendered by the browser, or to communicate
    with the server dynamically, through HTTP requests and responses. Once Selenium
    is used, the HTML and CSS on the page can be read and parsed as you would with
    any other website code, and HTTP requests and responses can be sent and handled
    by your code via the techniques in earlier chapters, even without using Selenium.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, JavaScript can even be a benefit to web scrapers, because its use
    as a “browser-side content management system” may expose useful APIs to the outside
    world, letting you obtain the data more directly. For more information on this,
    see [Chapter 15](ch15.html#c-15).
  prefs: []
  type: TYPE_NORMAL
- en: If you are still having difficulty with a particularly hairy JavaScript situation,
    you can find information on Selenium and interacting directly with dynamic websites,
    including drag-and-drop interfaces, in [Chapter 17](ch17.html#c-17).
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch14.html#id730-marker)) See Web Technology Surveys analysis at [*https://w3techs.com/technologies/details/js-jquery*](https://w3techs.com/technologies/details/js-jquery)
    W3Techs uses web crawlers to monitor trends in technology usage over time.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch14.html#id733-marker)) W3Techs, [“Usage Statistics and Market Share
    of Google Analytics for Websites”](http://w3techs.com/technologies/details/ta-googleanalytics/all/all).
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch14.html#id763-marker)) W3Techs, [“Usage Statistics of JavaScript as
    Client-Side Programming Language on Websites”](http://w3techs.com/technologies/details/cp-javascript/all/all).
  prefs: []
  type: TYPE_NORMAL
