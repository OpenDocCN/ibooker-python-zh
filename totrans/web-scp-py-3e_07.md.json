["```py\nfrom urllib.request import urlopen\nfrom bs4 import BeautifulSoup \n\nhtml = urlopen('http://en.wikipedia.org/wiki/Kevin_Bacon')\nbs = BeautifulSoup(html, 'html.parser')\nfor link in bs.find_all('a'):\n    if 'href' in link.attrs:\n        print(link.attrs['href'])\n\n```", "```py\n//foundation.wikimedia.org/wiki/Privacy_policy\n//en.wikipedia.org/wiki/Wikipedia:Contact_us\n```", "```py\n/wiki/Category:All_articles_with_unsourced_statements\n/wiki/Talk:Kevin_Bacon\n```", "```py\nfrom urllib.request import urlopen \nfrom bs4 import BeautifulSoup \nimport re\n\nhtml = urlopen('http://en.wikipedia.org/wiki/Kevin_Bacon')\nbs = BeautifulSoup(html, 'html.parser')\nfor link in bs.find('div', {'id':'bodyContent'}).find_all(\n    'a', href=re.compile('^(/wiki/)((?!:).)*$')):\n    print(link.attrs['href'])\n\n```", "```py\nfrom urllib.request import urlopen\nfrom bs4 import BeautifulSoup\nimport datetime\nimport random\nimport re\n\nrandom.seed(datetime.datetime.now())\ndef getLinks(articleUrl):\n    html = urlopen('http://en.wikipedia.org{}'.format(articleUrl))\n    bs = BeautifulSoup(html, 'html.parser')\n    return bs.find('div', {'id':'bodyContent'}).find_all('a',\n         href=re.compile('^(/wiki/)((?!:).)*$'))\n\nlinks = getLinks('/wiki/Kevin_Bacon')\nwhile len(links) > 0:\n    newArticle = links[random.randint(0, len(links)-1)].attrs['href']\n    print(newArticle)\n    links = getLinks(newArticle)\n\n```", "```py\nfrom urllib.request import urlopen\nfrom bs4 import BeautifulSoup\nimport re\n\npages = set()\ndef getLinks(pageUrl):\n    html = urlopen('http://en.wikipedia.org{}'.format(pageUrl))\n    bs = BeautifulSoup(html, 'html.parser')\n    for link in bs.find_all('a', href=re.compile('^(/wiki/)')):\n        if 'href' in link.attrs:\n            if link.attrs['href'] not in pages:\n                #We have encountered a new page\n                newPage = link.attrs['href']\n                print(newPage)\n                pages.add(newPage)\n                getLinks(newPage)\ngetLinks('')\n\n```", "```py\nfrom urllib.request import urlopen\nfrom bs4 import BeautifulSoup\nimport re\n\npages = set()\ndef getLinks(pageUrl):\n    html = urlopen('http://en.wikipedia.org{}'.format(pageUrl))\n    bs = BeautifulSoup(html, 'html.parser')\n    try:\n        print(bs.h1.get_text())\n        print(bs.find(id ='mw-content-text').find_all('p')[0])\n        print(bs.find(id='ca-edit').find('span')\n             .find('a').attrs['href'])\n    except AttributeError:\n        print('This page is missing something! Continuing.')\n\n    for link in bs.find_all('a', href=re.compile('^(/wiki/)')):\n        if 'href' in link.attrs:\n            if link.attrs['href'] not in pages:\n                #We have encountered a new page\n                newPage = link.attrs['href']\n                print('-'*20)\n                print(newPage)\n                pages.add(newPage)\n                getLinks(newPage)\ngetLinks('') \n\n```", "```py\n#Retrieves a list of all Internal links found on a page\ndef getInternalLinks(bs, url):\n    netloc = urlparse(url).netloc\n    scheme = urlparse(url).scheme\n    internalLinks = set()\n    for link in bs.find_all('a'):\n        if not link.attrs.get('href'):\n            continue\n        parsed = urlparse(link.attrs['href'])\n        if parsed.netloc == '':\n            l = f'{scheme}://{netloc}/{link.attrs[\"href\"].strip(\"/\")}'\n            internalLinks.add(l)\n        elif parsed.netloc == internal_netloc:\n            internalLinks.add(link.attrs['href'])\n    return list(internalLinks)\n\n```", "```py\n#Retrieves a list of all external links found on a page\ndef getExternalLinks(bs, url):\n    internal_netloc = urlparse(url).netloc\n    externalLinks = set()\n    for link in bs.find_all('a'):\n        if not link.attrs.get('href'):\n            continue\n        parsed = urlparse(link.attrs['href'])\n        if parsed.netloc != '' and parsed.netloc != internal_netloc:\n            externalLinks.add(link.attrs['href'])\n    return list(externalLinks)\n\n```", "```py\ndef getRandomExternalLink(startingPage):\n    bs = BeautifulSoup(urlopen(startingPage), 'html.parser')\n    externalLinks = getExternalLinks(bs, startingPage)\n    if not len(externalLinks):\n        print('No external links, looking around the site for one')\n        internalLinks = getInternalLinks(bs, startingPage)\n        return getRandomExternalLink(random.choice(internalLinks))\n    else:\n        return random.choice(externalLinks)\n\n```", "```py\ndef followExternalOnly(startingSite):\n    externalLink = getRandomExternalLink(startingSite)\n    print(f'Random external link is: {externalLink}')\n    followExternalOnly(externalLink)\n\n```", "```py\nfollowExternalOnly('https://www.oreilly.com/')\n\n```", "```py\nhttp://igniteshow.com/\nhttp://feeds.feedburner.com/oreilly/news\nhttp://hire.jobvite.com/CompanyJobs/Careers.aspx?c=q319\nhttp://makerfaire.com/\n```", "```py\n# Collects a list of all external URLs found on the site\nallExtLinks = []\nallIntLinks = []\n\ndef getAllExternalLinks(url):\n    bs = BeautifulSoup(urlopen(url), 'html.parser')\n    internalLinks = getInternalLinks(bs, url)\n    externalLinks = getExternalLinks(bs, url)\n    for link in externalLinks:\n        if link not in allExtLinks:\n            allExtLinks.append(link)\n            print(link)\n\n    for link in internalLinks:\n        if link not in allIntLinks:\n            allIntLinks.append(link)\n            getAllExternalLinks(link)\n\nallIntLinks.append('https://oreilly.com')\ngetAllExternalLinks('https://www.oreilly.com/')\n\n```"]