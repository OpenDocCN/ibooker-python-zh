- en: Chapter 9\. Migrating Existing Analytic Engineering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章 迁移现有分析工程
- en: Many users will already have analytic work that is currently deployed and that
    they want to migrate over to Dask. This chapter will discuss the considerations,
    challenges, and experiences of users making the switch. The main migration pathway
    explored in the chapter is moving an existing big data engineering job from another
    distributed framework, such as Spark, into Dask.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 许多用户已经部署了当前正在使用的分析工作，他们希望将其迁移到 Dask。本章将讨论用户进行切换时的考虑、挑战和经验。本章主要探讨将现有大数据工程作业从其他分布式框架（如
    Spark）迁移到 Dask 的主要迁移路径。
- en: Why Dask?
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么选择 Dask？
- en: 'Here are some reasons to consider migrating to Dask from an existing job that
    is implemented in pandas, or distributed libraries like PySpark:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是考虑从现有在 pandas 中实现的作业或 PySpark 等分布式库迁移到 Dask 的一些理由：
- en: Python and PyData stack
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Python 和 PyData 堆栈
- en: Many data scientists and developers prefer using a Python-native stack, where
    they don’t have to switch between languages or styles.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 许多数据科学家和开发人员更喜欢使用 Python 本地堆栈，他们不需要在不同语言或风格之间切换。
- en: Richer ML integrations with Dask APIs
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Dask API 更丰富的 ML 集成
- en: Futures, delayed, and ML integrations require less glue code from the developer
    to maintain, and there are performance improvements from the more flexible task
    graph management Dask offers.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Futures、delayed 和 ML 集成要求开发人员减少粘合代码的编写，由于 Dask 提供更灵活的任务图管理，性能有所提升。
- en: Fine-grained task management
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 精细化任务管理
- en: Dask’s task graph is generated and maintained in real time during runtime, and
    users can access the task dictionary synchronously.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 的任务图在运行时实时生成和维护，并且用户可以同步访问任务字典。
- en: Debugging overhead
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 调试开销
- en: Some developer teams prefer the debugging experience in Python, as opposed to
    mixed Python and Java/Scala stacktrace.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 一些开发团队更喜欢 Python 中的调试体验，而不是混合 Python 和 Java/Scala 堆栈跟踪。
- en: Development overhead
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 开发开销
- en: The development step in Dask can be done locally with ease with the developer’s
    laptop, as opposed to needing to connect to a powerful cloud machine in order
    to experiment.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Dask 中进行开发步骤可以轻松在开发者的笔记本电脑上完成，而不需要连接到强大的云机器以进行实验。
- en: Management UX
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 管理用户体验
- en: Dask visualization tools tend to be more visually pleasing and intuitive to
    reason, with native graphviz rendering for task graphs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 的可视化工具往往更具视觉吸引力和直观性，具有用于任务图的本地 graphviz 渲染。
- en: These are not all of the benefits, but if any of them speak to you, it’s probably
    worth investing the time to consider moving the workload to Dask. There are always
    trade-offs involved, so the next section will look at some of the limitations,
    followed by a road map to give you an idea of the scale of work involved in moving
    to Dask.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这些并非所有的优势，但如果其中任何一个对你有说服力，考虑将工作负载转移到 Dask 可能是值得投资时间考虑的。总是会有权衡，因此接下来的部分将讨论一些限制，并提供一个路线图，以便让你了解迁移到
    Dask 所涉及的工作规模。
- en: Limitations of Dask
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dask 的限制
- en: 'Dask is relatively new, and the use of Python data stack to perform large-scale
    extract, transform, load operations is also fairly new. There are limitations
    to Dask, which mainly arise from the fact that PyData stack has traditionally
    not been used to perform large-scale data workloads. At the time of writing, there
    are some limits to the system. However, they are being addressed by developers,
    and a lot of these deficiencies will be filled in. Some of the fine-grained considerations
    you should have are as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 是比较新的技术，使用 Python 数据堆栈执行大规模抽取、转换和加载操作也是相对较新的。Dask 存在一些限制，主要是因为 PyData 堆栈传统上并不用于执行大规模数据工作负载。在撰写本文时，系统存在一些限制。然而，开发人员正在解决这些问题，许多这些不足将会被弥补。你应该考虑一些精细化的注意事项，如下所述：
- en: Parquet scale limits
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 的规模限制
- en: If Parquet data exceeds 10 TB in scale, there are issues at the fastparquet
    and PyArrow level that slow Dask down, and metadata management overhead can be
    overwhelming.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 Parquet 数据超过 10 TB，fastparquet 和 PyArrow 层面会出现问题，这会拖慢 Dask 的速度，并且元数据管理的开销可能会很大。
- en: ETL workloads with Parquet files at 10 TB in scale and beyond, and that include
    a mutation, such as append and update, run into consistency issues.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Parquet 文件达到 10 TB 以上的 ETL 工作负载中，包括追加和更新等变异，会遇到一致性问题。
- en: Weak data lake integrations
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 弱数据湖集成
- en: PyData stack has not engaged much in the big data world traditionally, and the
    integrations on data lake management, such as Apache Iceberg, are missing.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: PyData 堆栈在传统上并没有在大数据领域大量使用，并且在数据湖管理方面的集成，如 Apache Iceberg，尚未完善。
- en: High-level query optimization
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 高级查询优化
- en: Users of Spark would be familiar with the Catalyst optimizer that pushes down
    predicates for optimizing the physical work on the executors. This optimization
    layer is missing in Dask at the moment. Spark in its early years also did not
    have the Catalyst engine written yet, and there is work in progress to build this
    out for Dask.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 的用户可能熟悉 Catalyst 优化器，该优化器推动优化执行器上的物理工作。目前 Dask 还缺少这种优化层。Spark 在早期也没有写
    Catalyst 引擎，目前正在进行相关工作，以为 Dask 构建此功能。
- en: Any list of limitations for a rapidly developing project like Dask may be out
    of date by the time you read it, so if any of these are blockers for your migration,
    make sure to check Dask’s status tracker.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 像 Dask 这样快速发展的项目的任何限制列表，在你阅读时可能已经过时，因此如果这些限制是您迁移的阻碍因素，请确保检查 Dask 的状态跟踪器。
- en: Migration Road Map
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迁移路线图
- en: 'While no engineering work is linear in process, it’s always a good idea to
    have a road map in mind. We’ve laid out an example of migration steps as a non-exhaustive
    list of items a team might want to think through when planning its move:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然没有工程工作是线性进行的，但随时掌握路线图始终是个好主意。我们已经列出了迁移步骤的示例，作为团队在计划迁移时可能需要考虑的非穷尽列表项：
- en: What kind of machines and containerization framework will we want to deploy
    Dask on, and what are their pros and cons?
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将希望在什么类型的机器和容器化框架上部署 Dask，它们各自的优缺点是什么？
- en: Do we have tests to ensure our migration correctness and our desired goals?
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们是否有测试来确保我们的迁移正确性和我们期望的目标？
- en: What type of data is Dask able to ingest, and at what scale, and how does that
    differ from other platforms?
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dask 能够摄取什么类型的数据，在什么规模下，以及这与其他平台有何不同？
- en: What is the computation framework of Dask, and how do we think in Dask and Pythonic
    ways to achieve the task?
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dask 的计算框架是什么，以及我们如何以 Dask 和 Pythonic 的方式思考来完成任务？
- en: How would we monitor and troubleshoot the code at runtime?
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将如何在运行时监控和排除代码问题？
- en: We’ll start by looking at the types of clusters, which goes with the deployment
    framework, as it is often one of the issues requiring collaboration with other
    teams or organizations.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从查看集群类型开始，这与部署框架相关，因为这通常是需要与其他团队或组织合作的问题之一。
- en: Types of Clusters
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集群类型
- en: If you are considering moving your analytic engineering job, you probably have
    a system that’s provisioned to you by your organization. Dask is supported in
    many commonly used deployment and development environments, with some allowing
    more flexibility in scaling, dependency management, and support of heterogeneous
    worker types. We have used Dask on academic environments, on commodity cloud,
    and directly over VMs/containers; we’ve detailed the pros and cons, and some well-used
    and supported environments, in [Appendix A](app01.xhtml#appA).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您考虑迁移您的分析工程工作，您可能拥有一个由您的组织提供的系统。Dask 在许多常用的部署和开发环境中受到支持，其中一些允许更灵活的扩展、依赖管理和支持异构工作类型。我们在学术环境、通用云和直接在虚拟机/容器上使用了
    Dask；我们详细说明了各自的优缺点以及一些广泛使用和支持的环境，详见 [附录 A](app01.xhtml#appA)。
- en: '[Example 9-1](#ex_yarn_deployment_ch09_1685536092648) shows an example of a
    YARN deployment. More examples and in-depth discussion can be found in [Chapter 12](ch12.xhtml#ch12).'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 9-1](#ex_yarn_deployment_ch09_1685536092648) 展示了 YARN 部署的示例。更多示例和深入讨论可见于
    [第 12 章](ch12.xhtml#ch12)。'
- en: Example 9-1\. Deploying Dask on YARN with Dask-Yarn and skein
  id: totrans-38
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-1\. 使用 Dask-Yarn 和 skein 在 YARN 上部署 Dask
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If your organization has multiple clusters that are supported, choosing one
    where you can self-serve dependency management, like Kubernetes, is beneficial.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的组织有多个受支持的集群，选择一个可以自助依赖管理的集群，如 Kubernetes，将是有益的。
- en: For high-performance computing deployments using job queuing systems such as
    PBS, Slurm, MOAB, SGE, LSF, and HTCondor, you should use Dask-jobqueue, as shown
    in [Example 9-2](#ex_slurm_deployment_ch09_1685536141262).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用 PBS、Slurm、MOAB、SGE、LSF 和 HTCondor 等作业队列系统进行高性能计算部署，应使用 Dask-jobqueue，如
    [示例 9-2](#ex_slurm_deployment_ch09_1685536141262) 所示。
- en: Example 9-2\. Deploying Dask using jobqueue over Slurm
  id: totrans-42
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-2\. 使用 jobqueue 在 Slurm 上部署 Dask
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You likely have a shared filesystem already set up by your organization’s admin.
    Enterprise users might be used to already robustly provisioned distributed data
    sources, running on HDFS or blob storage like S3, which Dask works with seamlessly
    (see [Example 9-3](#ex_s3_minio_rw)). Dask also integrates well with networked
    filesystems.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经由你的组织管理员设置了共享文件系统。企业用户可能已经习惯了在HDFS或像S3这样的Blob存储上运行的健全配置的分布式数据源，而Dask能够无缝地与之配合（参见[示例
    9-3](#ex_s3_minio_rw)）。Dask也与网络文件系统良好集成。
- en: Example 9-3\. Reading and writing to blob storage using MinIO
  id: totrans-45
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-3\. 使用MinIO读取和写入Blob存储
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We found that one of the surprisingly useful use cases is connecting directly
    to network storage such as NFS or FTP. When working on an academic dataset that’s
    large and clunky to work with (like a neuroimaging dataset that’s directly hosted
    by another organization), we could connect directly to the source filesystem.
    When using Dask this way, you should test out and consider network timeout allowances.
    Also note that, as of this writing, Dask does not have a connector to data lakes
    such as Iceberg.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现一个令人惊讶地有用的用例是直接连接到网络存储，如NFS或FTP。在处理大型且难以处理的学术数据集时（例如直接由另一个组织托管的神经影像数据集），我们可以直接连接到源文件系统。使用Dask这种方式时，你应该测试并考虑网络超时的允许。此外，请注意，截至本文撰写时，Dask尚未具备与Iceberg等数据湖的连接器。
- en: 'Development: Considerations'
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开发：考虑因素
- en: Translating an existing logic to Dask is a fairly intuitive process. The following
    sections present some considerations if you’re coming from libraries such as R,
    pandas, and Spark, and how Dask might differ from them. Some of these differences
    result from moving from a different low-level implementation, such as Java, and
    others result from moving from single-machine code to a scaled implementation,
    as when you’re coming from pandas.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 将现有逻辑转换为Dask是一个相当直观的过程。以下部分介绍了如果你来自R、pandas和Spark等库，并且Dask可能与它们有何不同的一些考虑因素。其中一些差异来自于从不同的低级实现（如Java）移动，其他差异来自于从单机代码移动到扩展实现，例如从pandas移动而来。
- en: DataFrame performance
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DataFrame性能
- en: If you have a job that you are already running on a different platform, it’s
    likely you are already using columnar storage format, like Parquet, and reading
    at runtime. The data type mapping from Parquet to Python is inherently imprecise.
    It’s a good idea to check data types when reading in any data at runtime, and
    the same applies to DataFrame. If type inference fails, a column would default
    to object. Once you inspect and determine the type inference is imprecise, specifying
    data types can speed up your job a lot. Additionally, it’s always a good idea
    to check strings, floating point numbers, datetime, and arrays. If type errors
    arise, keeping in mind the upstream data sources and their data type is a good
    start. For example, if the Parquet is generated from protocol buffers, depending
    on what encode and decode engine was used, there are differences in null checks,
    float, doubles, and mixed precision types that are introduced in that stack.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经在不同平台上运行作业，很可能已经在运行时使用列存储格式，例如Parquet。从Parquet到Python的数据类型映射固有地不精确。建议在运行时读取任何数据时检查数据类型，DataFrame亦如此。如果类型推断失败，列会默认为对象。一旦检查并确定类型推断不精确，指定数据类型可以显著加快作业速度。此外，检查字符串、浮点数、日期时间和数组总是个好主意。如果出现类型错误，牢记上游数据源及其数据类型是一个好的开始。例如，如果Parquet是从协议缓冲生成的，根据使用的编码和解码引擎，该堆栈中引入了空检查、浮点数、双精度和混合精度类型的差异。
- en: When reading a large file from cloud storage into DataFrame, it may be useful
    to select columns ahead of time at the DataFrame read stage. Users from other
    platforms like Spark would be familiar with predicate push-down, where even if
    you don’t quite specify the columns desired, the platform would optimize and read
    only the required column for computation. Dask doesn’t quite provide that optimization
    yet.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当从云存储读取大文件到DataFrame时，在DataFrame读取阶段预先选择列可能非常有用。来自其他平台（如Spark）的用户可能熟悉谓词下推，即使你没有完全指定所需的列，平台也会优化并仅读取计算所需的列。Dask目前尚未提供这种优化。
- en: Setting smart indices early in the transformation of your DataFrame, prior to
    a complex query, can speed things up. Be aware that multi-indexing is not supported
    by Dask yet. A common workaround for a multi-indexed DataFrame from other platforms
    is mapping as a single concatenated column. For example, a simple workaround when
    coming from a non-Dask columnar dataset, like pandas `pd.MultiIndex` that has
    two columns as its index—say, `col1` and `col2`—would be to introduce a new column
    in Dask DataFrame `col1_col2` as Dask.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DataFrame 转换早期设置智能索引，在复杂查询之前，可以加快速度。请注意，Dask 尚不支持多索引。对于来自其他平台的多索引 DataFrame
    的常见解决方法是映射为单一连接列。例如，从非 Dask 列数据集（如 pandas 的 `pd.MultiIndex`，其索引有两列 `col1` 和 `col2`）来时的一个简单解决方法是在
    Dask DataFrame 中引入一个新列 `col1_col2`。
- en: During the transform stage, calling `.compute()` coalesces a large distributed
    Dask DataFrame to a single partition that should fit in RAM. If it does not, you
    may encounter problems. On the other hand, if you have filtered an input data
    of size 100 GB down to 10 GB (say your RAM is 15 GB), it is probably a good idea
    to reduce the parallelism after the filter operation by invoking `.compute()`.
    You can check your DataFrame’s memory usage by invoking `df.memory_usage(deep=True).sum()`
    to determine if this is the right call. Doing this can be particularly useful
    if, after the filter operation, you have a complex and expensive shuffle operation,
    such as `.join()` with a new larger dataset.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在转换阶段，调用 `.compute()` 方法将大型分布式 Dask DataFrame 合并为一个单一分区，应该可以放入 RAM 中。如果不行，可能会遇到问题。另一方面，如果您已经将大小为
    100 GB 的输入数据过滤到了 10 GB（假设您的 RAM 是 15 GB），那么在过滤操作后减少并行性可能是个好主意，方法是调用 `.compute()`。您可以通过调用
    `df.memory_usage(deep=True).sum()` 来检查 DataFrame 的内存使用情况，以确定是否需要进行此操作。如果在过滤操作后有复杂且昂贵的洗牌操作，比如与新的更大数据集的
    `.join()` 操作，这样做尤其有用。
- en: Tip
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Dask DataFrame is not value-mutable in the way that pandas DataFrame users might
    be familiar with. Since in-memory modification of a particular value is not possible,
    the only way to change a value would be a map operation over the whole column
    of the entire DataFrame. If an in-memory value change is something you have to
    do often, it is better to use an external database.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 与 pandas DataFrame 用户熟悉的内存中值可变不同，Dask DataFrame 不支持这种方式的值可变。由于无法在内存中修改特定值，唯一的改变值的方式将是对整个
    DataFrame 列进行映射操作。如果经常需要进行内存中值的更改，最好使用外部数据库。
- en: Porting SQL to Dask
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将 SQL 迁移到 Dask
- en: Dask does not natively offer a SQL engine, although it does natively offer options
    to read from a SQL database. There are a number of different libraries you can
    use to interact with an existing SQL database, and to treat Dask DataFrame as
    a SQL table and run SQL queries directly (see [Example 9-4](#ex_postgres_dataframe)).
    Some allow you to even build and serve ML models directly using SQL ML syntax
    similar to that of Google’s BigQuery ML. In Examples [11-14](ch11.xhtml#Dask_sql_linear_regression)
    and [11-15](ch11.xhtml#Dask_sql_XGBClassifier), we will show the use of Dask’s
    native `read_sql()` function and running SQL ML using Dask-SQL.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 并不原生支持 SQL 引擎，尽管它原生支持从 SQL 数据库读取数据的选项。有许多不同的库可以用来与现有的 SQL 数据库交互，并且将 Dask
    DataFrame 视为 SQL 表格并直接运行 SQL 查询（参见 [示例 9-4](#ex_postgres_dataframe)）。一些库甚至允许您直接构建和提供
    ML 模型，使用类似于 Google BigQuery ML 的 SQL ML 语法。在示例 [11-14](ch11.xhtml#Dask_sql_linear_regression)
    和 [11-15](ch11.xhtml#Dask_sql_XGBClassifier) 中，我们将展示使用 Dask 的原生 `read_sql()` 函数以及使用
    Dask-SQL 运行 SQL ML 的用法。
- en: Example 9-4\. Reading from a Postgres database
  id: totrans-59
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-4\. 从 Postgres 数据库读取
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: FugueSQL provides SQL compatibility to PyData stack, including Dask. The project
    is in its infancy but seems promising. The main advantage of FugueSQL is that
    the code is portable between pandas, Dask, and Spark, giving a lot more interoperability.
    FugueSQL can run its SQL queries using `DaskExecutionEngine`, or you can run FugueSQL
    queries over a Dask DataFrame you already are using. Alternatively, you can run
    a quick SQL query on Dask DataFrame on your notebook as well. [Example 9-5](#running_sql_over_dask_dataframe_ch09_1685553886459)
    shows an example of using FugueSQL in a notebook. The downside of FugueSQL is
    that it requires the ANTLR library, which in turn requires a Java runtime.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: FugueSQL 为 PyData 栈（包括 Dask）提供了 SQL 兼容性。该项目处于起步阶段，但似乎很有前途。FugueSQL 的主要优势在于代码可以在
    pandas、Dask 和 Spark 之间进行移植，提供了更多的互操作性。FugueSQL 可以使用 `DaskExecutionEngine` 运行其
    SQL 查询，或者在已经使用的 Dask DataFrame 上运行 FugueSQL 查询。或者，你也可以在笔记本上快速在 Dask DataFrame
    上运行 SQL 查询。[示例 9-5](#running_sql_over_dask_dataframe_ch09_1685553886459) 展示了在笔记本中使用
    FugueSQL 的示例。FugueSQL 的缺点是需要 ANTLR 库，而 ANTLR 又依赖于 Java 运行时。
- en: Example 9-5\. Running SQL over Dask DataFrame with FugueSQL
  id: totrans-62
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-5\. 使用 FugueSQL 在 Dask DataFrame 上运行 SQL
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '|  | VendorID | average_fare |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | VendorID | average_fare |'
- en: '| --- | --- | --- |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **0** | 1 | 15.127384 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| **0** | 1 | 15.127384 |'
- en: '| **1** | 2 | 15.775723 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 2 | 15.775723 |'
- en: '[PRE5]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: An alternate method is to use the Dask-SQL library. This package uses Apache
    Calcite to provide the SQL parsing frontend and is used to query Dask DataFrames.
    With that library, you can pass most of the SQL-based operations to the Dask-SQL
    context, and it will be handled. The engine handles standard SQL inputs like `SELECT`,
    `CREATE TABLE`, but also ML model creation, with the `CREATE MODEL` syntax.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是使用 Dask-SQL 库。该软件包使用 Apache Calcite 提供 SQL 解析前端，并用于查询 Dask 数据帧。使用该库，你可以将大多数基于
    SQL 的操作传递给 Dask-SQL 上下文，并进行处理。引擎处理标准 SQL 输入，如 `SELECT`、`CREATE TABLE`，同时还支持使用
    `CREATE MODEL` 语法进行 ML 模型创建。
- en: Deployment Monitoring
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署监控
- en: Like many other distributed libraries, Dask provides logs, and you can configure
    Dask logs to be sent to a storage system. The method will vary by the deployment
    environment, and whether Jupyter is involved.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 像许多其他分布式库一样，Dask 提供日志记录功能，你可以配置 Dask 日志将其发送到存储系统。部署环境会影响方法的选择，以及是否涉及 Jupyter。
- en: The Dask client exposes the `get_worker_logs()` and `get_scheduler_logs()` methods,
    which can be accessed at runtime if desired. Additionally, similar to other distributed
    system logging, you can log events by topic, making them easily accessible by
    event types.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 客户端暴露了 `get_worker_logs()` 和 `get_scheduler_logs()` 方法，如果需要可以在运行时访问。此外，类似于其他分布式系统的日志记录，你可以按主题记录事件，使其易于按事件类型访问。
- en: '[Example 9-6](#ex_basic_logging_ch09_1685536244456) is a toy example of adding
    a custom log event to the client.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 9-6](#ex_basic_logging_ch09_1685536244456) 是在客户端添加自定义日志事件的玩具示例。'
- en: Example 9-6\. Basic logging by topic
  id: totrans-74
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-6\. 按主题进行基本日志记录
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[Example 9-7](#structured-logging-on-workers_ch09_1685536283090) builds on
    the previous example, but swaps in the execution context to a distributed cluster
    setup, for potentially more complex, custom structured events. The Dask client
    listens and accumulates these events, and we can inspect them. We start with a
    Dask DataFrame and then run some compute-heavy task. This example uses a `softmax`
    function, which is a common computation in many ML uses. A common ML dilemma is
    whether to use a more complex activation or loss function for accuracy, sacrificing
    performance (thereby running fewer training epochs but gaining a more stable gradient),
    or vice versa. To figure that out, we insert a code to log custom structured events
    to time the compute overhead of that specific function.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 9-7](#structured-logging-on-workers_ch09_1685536283090) 在前一个示例的基础上构建，但是将执行上下文切换到分布式集群设置中，以处理可能更复杂的自定义结构化事件。Dask
    客户端监听并累积这些事件，我们可以进行检查。我们首先从一个 Dask 数据帧开始，然后执行一些计算密集型任务。本示例使用 `softmax` 函数，这是许多
    ML 应用中常见的计算。常见的 ML 困境是是否使用更复杂的激活或损失函数来提高准确性，牺牲性能（从而运行更少的训练周期，但获得更稳定的梯度），反之亦然。为了弄清楚这一点，我们插入一个代码来记录定制的结构化事件，以计算特定函数的计算开销。'
- en: Example 9-7\. Structured logging on workers
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-7\. 工作节点上的结构化日志
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Conclusion
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this chapter you have reviewed the large questions and considerations of
    migrating existing analytic engineering work. You’ve also learned some of the
    feature differences of Dask compared to Spark, R, and pandas. Some features are
    not yet implemented by Dask, some are more robustly implemented by Dask, and others
    are inherent translational differences when moving a computation from a single
    machine to a distributed cluster. Since large-scale data engineering tends to
    use similar terms and names across many libraries, it’s often easy to overlook
    minute differences that lead to larger performance or correctness issues. Keeping
    them in mind will help you as you take your first journeys in Dask.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您已经审查了迁移现有分析工程工作的重要问题和考虑因素。您还了解了Dask与Spark、R和pandas之间的一些特征差异。一些特性尚未由Dask实现，一些特性则由Dask更为稳健地实现，还有一些是在将计算从单机迁移到分布式集群时固有的翻译差异。由于大规模数据工程倾向于在许多库中使用类似的术语和名称，往往容易忽视导致更大性能或正确性问题的细微差异。记住它们将有助于您在Dask中迈出第一步的旅程。
