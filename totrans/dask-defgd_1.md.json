["```py\nimport pandas as pd\ndf = pd.read_csv(\"0000.csv\")\n```", "```py\n>>> df.head()\n                       id     name         x         y\ntimestamp\n1990-01-01 00:00:00  1066    Edith -0.789146  0.742478\n1990-01-01 00:00:01   988   Yvonne  0.520779 -0.301681\n1990-01-01 00:00:02  1022      Dan  0.523654 -0.438432\n1990-01-01 00:00:03   944      Bob -0.768837  0.537302\n1990-01-01 00:00:04   942   Hannah  0.990359  0.477812\n...                   ...      ...       ...       ...\n1990-01-03 23:59:55   927    Jerry -0.116351  0.456426\n1990-01-03 23:59:56  1002    Laura -0.870446  0.962673\n1990-01-03 23:59:57  1005  Michael -0.481907  0.015189\n1990-01-03 23:59:58   975   Ingrid -0.468270  0.406451\n1990-01-03 23:59:59  1059      Ray -0.739538  0.798155\n```", "```py\n>>> len(df[df.x > 0])\n\n302316\n```", "```py\nimport dask.dataframe as dd\nddf = dd.read_csv(\"6M.csv\")\n```", "```py\n$  ls\n0000.csv\n0001.csv\n0002.csv\n…\n1094.csv\n```", "```py\nddf = dd.read_csv(\"*.csv\")\n```", "```py\n>> ddf.partitions\n1095\n```", "```py\n>>> ddf = dd.read_csv(\"*.csv\", blocksize=\"16MB\")\n>>> ddf.npartitions\n2190\n```", "```py\n\t>> df.dtypes\ntimestamp     \tobject\nid            \tint64\nname         \tobject\nx            \tfloat64\ny            \tfloat64\ndtype: object\n```", "```py\nddf = dd.read_csv(\"*.csv\")\nddf.dtypes\ntimestamp     \tobject\nid             \tint64\nname         \tobject\nx            \tfloat64\ny            \tfloat64\ndtype: object\n```", "```py\n>>> ddf = dd.read_csv(\n    \"*.csv\",\n    dtype={\n        \"name\": \"string[pyarrow]\",\n    },\n)\n>>> ddf.dtypes\ntimestamp     object\nid             int64\nname          string\nx            float64\ny            float64\ndtype: object\n```", "```py\nddf = dd.read_csv(\n    \"s3://coiled-datasets/timeseries/20-years/csv/*.part\"    \n)\n```", "```py\nimport coiled\nimport dask\n```", "```py\ncluster = coiled.Cluster(\n    name=\"demo-cluster\", \n    n_workers=5\n)\n```", "```py\nclient = dask.distributed.Client(cluster)\n```", "```py\n# filter rows\nddf[ddf.x > 0]\n# compute max\nddf.x.max()\n# perform groupby aggregation\nddf.groupby(“id”).x.mean()\n```", "```py\nddf.to_parquet(\"s3://coiled-datasets/timeseries/20-years/parquet\", engine=”pyarrow”)\n```", "```py\n>>> import dask.dataframe as dd\n>>> import pandas as pd\n>>> df = pd.DataFrame({\n\"col1\": [\"a\", \"b\", \"c\", \"d\"], \n\"col2\": [1, 2, 3, 4]\n    })\n>>> ddf = dd.from_pandas(df, npartitions=2)\n>>> ddf.compute()\n  col1  col2\n0    a     1\n1    b     2\n2    c     3\n3    d     4\n```", "```py\n>>> type(ddf.compute())\npandas.core.frame.DataFrame\n```", "```py\ncluster = coiled.Cluster(name=\"demo-cluster\", n_workers=5)\nclient = dask.distributed.Client(cluster)\nddf = dd.read_parquet(\n    \"s3://coiled-datasets/timeseries/20-years/parquet\",\n    storage_options={\"anon\": True, \"use_ssl\": True},\n    engine=\"pyarrow\",\n)\n```", "```py\nres = ddf.loc[ddf[\"id\"] > 1150]\nres.compute()\n```", "```py\n%%time\nid_min = ddf.id.min().compute()\nid_max = ddf.id.max().compute()\nCPU times: user 442 ms, sys: 31.4 ms, total: 473 ms\nWall time: 1min 20s\n```", "```py\n%%time\nid_min, id_max = dask.compute(ddf.id.min(), ddf.id.max())\nCPU times: user 222 ms, sys: 19.1 ms, total: 241 ms\nWall time: 35.5 s\n```", "```py\n>>> import dask.dataframe as dd\n>>> import pandas as pd\n>>> df = pd.DataFrame({\n        \"col1\": [\"a\", \"b\", \"c\", \"d\"], \n   \"col2\": [1, 2, 3, 4]\n    })\n>>> ddf = dd.from_pandas(df, npartitions=2)\n>>> persisted_ddf = ddf.persist()\n>>> len(persisted_ddf)\n4\n```", "```py\nimport coiled\nimport dask\nimport dask.dataframe as dd\ncluster = coiled.Cluster(name=\"powers\", n_workers=5) \n\nclient = dask.distributed.Client(cluster)\nddf = dd.read_parquet(\n    \"s3://coiled-datasets/timeseries/20-years/parquet\",\n    storage_options={\"anon\": True, \"use_ssl\": True},\n    engine=\"pyarrow\",\n)\n```", "```py\n>>> res = ddf.loc[ddf[\"id\"] > 1150]\n>>> len(res)\n87 seconds\n>>> res.name.nunique().compute()\n62 seconds\n```", "```py\n>>> persisted_res = res.persist()\n>>> len(persisted_res)\n2 seconds\n>>> persisted_res.name.nunique().compute()\n2 seconds\n```", "```py\n>>> res.repartition(2).to_parquet(\n        \"s3://coiled-datasets/tmp/matt/disk-persist\", \n        engine=\"pyarrow\"\n    )\n>>>> df = dd.read_parquet(\n        \"s3://coiled-datasets/tmp/matt/disk-persist\",\n        storage_options={\"anon\": True, \"use_ssl\": True},\n        engine=\"pyarrow\",\n    )\n>>> len(df) \n0.4 seconds\n>>> df.name.nunique().compute()\n0.3 seconds\n```", "```py\n>>> res2 = res.repartition(2)\n>>> persisted_res2 = res2.persist()\n>>> len(persisted_res2)\n0.3 seconds\n>>> persisted_res2.name.nunique().compute()\n0.3 seconds\n```", "```py\nimport pandas as pd\nimport dask.dataframe as dd\ndf = pd.DataFrame({\n    \"nums\":[1, 2, 3, 4, 5, 6], \n    \"letters\":[\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"]\n})\nddf = dd.from_pandas(df, npartitions=3)\n```", "```py\n>>>> for i in range(ddf.npartitions):\n        print(ddf.partitions[i].compute())\n  nums letters\n0     1       a\n1     2       b\n   nums letters\n2     3       c\n3     4       d\n   nums letters\n4     5       e\n5     6       f\n```", "```py\n>>> ddf2 = ddf.repartition(2)\n>>> for i in range(ddf2.npartitions):\n        print(ddf2.partitions[i].compute())\n  nums letters\n0     1       a\n1     2       b\n   nums letters\n2     3       c\n3     4       d\n4     5       e\n5     6       f\n```", "```py\n>>> ddf5 = ddf.repartition(5)\n>>> for i in range(ddf5.npartitions):\n        print(ddf5.partitions[i].compute())\n  nums letters\n0     1       a\n   nums letters\n1     2       b\n   nums letters\n2     3       c\n   nums letters\n3     4       d\n   nums letters\n4     5       e\n5     6       f\n```", "```py\nddf.repartition(partition_size=\"100MB\")\n```", "```py\nimport pandas as pd\nimport dask.dataframe as dd\ndf = pd.DataFrame({\n\"nums\":[1, 2, 3, 4, 5, 6], \n\"letters\":[\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"]\n})\nddf = dd.from_pandas(df, npartitions=2)\n```", "```py\n>>> for i in range(ddf.npartitions):\n        print(ddf.partitions[i].compute())\n  nums letters\n0     1       a\n1     2       b\n2     3       c\n   nums letters\n3     4       d\n4     5       e\n5     6       f\n```", "```py\n>>> ddf.loc[[2, 5]].compute()\n  nums letters\n2     3       c\n5     6       f\n```", "```py\n>>> ddf.loc[3:5].compute()\n  nums letters\n3     4       d\n4     5       e\n5     6       f\n```", "```py\n>>> ddf.divisions\n(0, 3, 5)\n```", "```py\n>>> for i in range(ddf.npartitions):\n        print(ddf.partitions[i].compute())\n  nums letters\n0     1       a\n1     2       b\n2     3       c\n   nums letters\n3     4       d\n4     5       e\n5     6       f\n```", "```py\n>>> ddf.loc[ddf[\"nums\"] % 2 == 0].compute()\n  nums letters\n1     2       b\n3     4       d\n5     6       f\n```", "```py\n>>> ddf.loc[(ddf[\"nums\"] % 2 == 0) & (ddf[\"letters\"].isin([\"b\", \"f\"]))].compute()\n  nums letters\n1     2       b\n5     6       f\n```", "```py\nddf = dd.read_parquet(\n    \"s3://coiled-datasets/timeseries/20-years/parquet\",\n    storage_options={\"anon\": True, 'use_ssl': True}\n)\n```", "```py\n>>> ddf.npartitions\n1095\n```", "```py\nres = ddf.loc[ddf[\"id\"] > 1150]\n```", "```py\n>>> len(res)\n1103\n```", "```py\n>>> res.npartitions\n1095\n```", "```py\n>>> res.map_partitions(len).compute()\n0       0\n1       1\n2       0\n3       0\n4       2\n       ..\n1090    0\n1091    2\n1092    0\n1093    0\n1094    0\nLength: 1095, dtype: int64\n```", "```py\nres2 = res.repartition(2).persist()\n```", "```py\nddf2 = ddf.set_index(\"id\")\n```", "```py\ndask_computed_divisions = ddf3.set_index(\"id\").divisions\nunique_divisions =  list(dict.fromkeys(list(dask_computed_divisions)))\nprint(repr(unique_divisions))\n# ^ copy this and reuse\n```", "```py\n>>> # load in small pandas dataframe\n>>> df = pd.read_csv(\"small_df.csv\", index_col=0)\n>>> # merge dask dataframe to pandas dataframe\n>>> join = ddf.merge(\n        df,\n        how=\"left\", \n        on=[\"name\"]\n    )\n>>> # materialize first 5 results\n>>> join.head()\n```", "```py\n>>> large_join = ddf.merge(\n        ddf_2, \n        how=\"left\", \n        left_index=True, \n        right_index=True\n    )\n>>> # materialize first 5 results\n>>> large_join.head()\n```", "```py\ndf = pd.DataFrame({\"a\": [23, 2, 8], \"b\": [99, 6, 1], \"c\": [1, 2, 3]})\n```", "```py\n>>> df\n\n   a   b   c \n0  23  99  1\n1  2   6   2\n2  8   1   3\n```", "```py\ndef minmax(x):\n    return x.max() - x.min()\n```", "```py\n>>> df.apply(minmax, axis=1)\n0  98\n1  4\n2  7\ndtype: int64\n```", "```py\n>> type(df.apply(minmax, axis=1))\npandas.core.series.Series\n```", "```py\nddf = dd.from_pandas(df, npartitions=2)\n```", "```py\ndef minmax2(df):\n    return df.apply(minmax, axis=1)\n```", "```py\n>>> ddf.map_partitions(minmax2, meta=(None, \"int64\")).compute()\n0  98\n1  4\n2  7\ndtype: int64\n```", "```py\n>>> ddf = dd.read_parquet(\n        \"s3://coiled-datasets/timeseries/20-years/parquet\",\n        storage_options={\"anon\": True, 'use_ssl': True}\n    )\n>>> ddf.map_partitions(lambda x: x.memory_usage(deep=True).sum()).compute()\n0       57061027\n1       57060857\n2       57059768\n3       57059342\n4       57060737\n          ...   \n1090    57059834\n1091    57061111\n1092    57061001\n1093    57058404\n1094    57061989\nLength: 1095, dtype: int64\n```", "```py\n>>> ddf.map_partitions(lambda x: dask.sizeof.sizeof(x)).compute()\n0       56822960\n1       57125360\n2       56822960\n3       57246320\n4       57306800\n          ...   \n1090    56974160\n1091    57004400\n1092    57337040\n1093    56822960\n1094    57004400\nLength: 1095, dtype: int64\n```", "```py\nimport coiled\nimport dask\nimport dask.dataframe as dd\ncluster = coiled.Cluster(name=\"demo-cluster\", n_workers=5)\nclient = dask.distributed.Client(cluster)\nddf = dd.read_parquet(\n    \"s3://coiled-datasets/h2o/G1_1e7_1e2_0_0/parquet\",\n    storage_options={\"anon\": True, \"use_ssl\": True},\n    engine=\"pyarrow\",\n)\n```", "```py\n>>> ddf.groupby(\"id\").x.sum().compute()\nid\n858\t-8.741694\n862\t 4.377649\n863\t-0.858438\n866\t-0.332073\n869\t-27.662715\n\t...\n```", "```py\nddf.groupby(\"id\").agg({\"x\": \"sum\"}).compute()\n```", "```py\n>>> ddf.groupby([\"id\", \"name\"]).x.sum().compute()\nid\tname\n858\tXavier    -0.459693\n862\tFrank      0.409465\n\tIngrid     1.067823\n863\tBob        0.048593\n866\tNorbert   -0.051115\n                  ...\n```", "```py\n>>> ddf.groupby(\"id\").agg({\"x\": \"sum\", \"y\": \"mean\"}).compute()\n```", "```py\n>> ddf.npartitions\n1095\n```", "```py\n>>> res = ddf.groupby(\"id\").x.sum()\n>>> res.npartitions\n1\n```", "```py\n>>> res2 = ddf.groupby(\"id\").x.sum(split_out=2)\n>>> res2.npartitions\n2\n```", "```py\nimport pandas as pd\nfrom dask import dataframe as dd \ndf = pd.DataFrame({\n    \"nums\": [1, 2, 3, 4, 5, 6], \n    \"letters\": [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"]\n})\nddf = dd.from_pandas(df, npartitions=2)\n```", "```py\n>>> for i in range(ddf.npartitions):\n        print(ddf.partitions[i].compute())\n  nums letters\n0     1       a\n1     2       b\n2     3       c\n   nums letters\n3     4       d\n4     5       e\n5     6       f\n```", "```py\n>>> ddf.partitions[0].memory_usage(deep=True).compute()\nIndex      128\nletters    174\nnums        24\ndtype: int64\n```", "```py\n>>> ddf.memory_usage_per_partition(deep=True).compute()\n0    326\n1    330\ndtype: int64\n```", "```py\n>>> ddf.memory_usage_per_partition(deep=False).compute()\n0    176\n1    180\ndtype: int64\n```", "```py\n>>> ddf = dd.read_parquet(\n        \"s3://coiled-datasets/timeseries/20-years/parquet\",\n        storage_options={\"anon\": True, 'use_ssl': True}\n    )\n>>> ddf.memory_usage_per_partition(deep=True).compute()\n0       57061027\n1       57060857\n2       57059768\n3       57059342\n4       57060737\n          ...   \n1090    57059834\n1091    57061111\n1092    57061001\n1093    57058404\n1094    57061989\nLength: 1095, dtype: int64\n```", "```py\n>>> from dask.utils import format_bytes\n>>> format_bytes(ddf.memory_usage(deep=True).sum().compute()) \n‘58.19 GiB'\n```", "```py\n>>> ddf.memory_usage(deep=True).compute().apply(format_bytes)\nIndex    4.93 GiB\nid       4.93 GiB\nname    38.45 GiB\nx        4.93 GiB\ny        4.93 GiB\ndtype: object\n```", "```py\n>>> ddf.dtypes\nid      int64\nname   object\nx     float64\ny     float64\ndtype: object\n```", "```py\n>>> ddf.name = ddf.name.astype(\"string[pyarrow]\")\n>>> ddf.memory_usage(deep=True).compute().apply(format_bytes)\nIndex    4.93 GiB\nid       4.93 GiB\nname     5.76 GiB\nx        4.93 GiB\ny        4.93 GiB\ndtype: object\n```", "```py\n>>> dd.compute(ddf.id.min(), ddf.id.max())\n815, 119\n```", "```py\n>>> ddf.id = ddf.id.astype(\"int16\")\n>>> ddf.memory_usage(deep=True).compute().apply(format_bytes)\nIndex    4.93 GiB\nid       1.23 GiB\nname    38.45 GiB\nx        4.93 GiB\ny        4.93 GiB\ndtype: object\n```", "```py\n>>> ddf = dd.read_parquet(\n        \"s3://coiled-datasets/timeseries/20-years/parquet\",\n        storage_options={\"anon\": True, \"use_ssl\": True},\n        engine=\"pyarrow\",\n        columns=[\"x\"],\n    )\n>>> format_bytes(ddf.memory_usage(deep=True).sum().compute())\n9.87 GiB\n```", "```py\n>>> ddf.memory_usage(deep=True).compute().apply(format_bytes)\nIndex    4.93 GiB\nx        4.93 GiB\ndtype: object\n```", "```py\nddf = dd.read_parquet(\n    \"s3://coiled-datasets/timeseries/20-years/parquet\",\n    storage_options={\"anon\": True, \"use_ssl\": True},\n    engine=\"pyarrow\",\n    filters=[[('id', '>', 1170)]],\n)\n```", "```py\n>>> format_bytes(ddf.memory_usage(deep=True).sum().compute())\n'5.99 kiB'\n```", "```py\nimport dask.dataframe as dd\nimport pandas as pd\ndf = pd.DataFrame({\n\"nums\": [1, 2.8, 3, 4, \"hi\", 6], \n\"letters\": [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"]\n})\nddf = dd.from_pandas(df, npartitions=2)\n```", "```py\n>>> print(ddf.compute())\n nums letters\n0    1       a\n1  2.8       b\n2    3       c\n3    4       d\n4   hi       e\n5    6       f\n```", "```py\n>>> ddf.dtypes\nnums       object\nletters    object\n```", "```py\n>>> ddf[\"nums\"] = dd.to_numeric(ddf[\"nums\"], errors=\"coerce\")\n>>> ddf.dtypes\nnums        int64\nletters    object\n>>> print(ddf.compute())\n  nums letters\n0   1.0       a\n1   2.8       b\n2   3.0       c\n3   4.0       d\n4   NaN       e\n5   6.0       f\n```", "```py\n>>> df2 = pd.DataFrame({\n        \"n1\": [\"bye\", 2.8, 3], \n        \"n2\": [\"7.7\", \"8\", 9.2]\n})\n>>> ddf2 = dd.from_pandas(df, npartitions=2)\n>>> print(ddf2.compute())\n   n1    n2\n0  bye  7.7\n1  2.8    8\n2    3  9.2\n```", "```py\n>>> ddf2.dtypes\nn1    object\nn2    object\n```", "```py\n>>> ddf2[\"n2\"] = ddf2[\"n2\"].astype(\"float64\")\n>>> ddf2.dtypes\nn1     object\nn2    float64\ndtype: object\n>>> print(ddf2.compute())\n   n1   n2\n0  bye  7.7\n1  2.8  8.0\n2    3  9.2\n```", "```py\n>>> ddf2[\"n1\"] = ddf2[\"n1\"].astype(\"float64\")\n>>> print(ddf2.compute())\n```", "```py\nValueError: could not convert string to float: 'bye'\n```", "```py\n>>> ddf2[\"n1\"] = dd.to_numeric(ddf[\"n1\"])\n>>> print(ddf2.compute())\n\n“ValueError: Unable to parse string \"bye\" at position 0”.\n```", "```py\n>>> ddf2[\"n1\"] = dd.to_numeric(ddf[\"n1\"], errors=\"coerce\")\n>>> print(ddf2.compute())\n   n1   n2\n0  NaN  7.7\n1  2.8  8.0\n2  3.0  9.2\n```", "```py\nimport dask.dataframe as dd\nimport pandas as pd\ndf = pd.DataFrame({\n    \"nums\": [1, 2, 3, 4, 5, 6], \n    \"letters\": [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"]\n})\nddf1 = dd.from_pandas(df, npartitions=2)\ndf = pd.DataFrame({\"nums\": [88, 99], \"letters\": [\"xx\", \"yy\"]})\nddf2 = dd.from_pandas(df, npartitions=1)\n```", "```py\nddf3 = dd.concat([ddf1, ddf2])\n```", "```py\n>>> print(ddf3.compute())\n  nums letters\n0     1       a\n1     2       b\n2     3       c\n3     4       d\n4     5       e\n5     6       f\n0    88      xx\n1    99      yy\n```", "```py\n>>> ddf3.npartitions\n3\n```", "```py\n>>> df = pd.DataFrame({\n        \"nums\": [1, 2, 3, 4, 5, 6], \n        \"letters\": [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"]\n})\nddf1 = dd.from_pandas(df, npartitions=2)\n>>> ddf1.divisions\n(0, 3, 5)\n```", "```py\n>>> def print_partitions(ddf):\n        for i in range(ddf.npartitions):\n            print(ddf.partitions[i].compute())\n>>> print_partitions(ddf1)\n  nums letters\n0     1       a\n1     2       b\n2     3       c\n   nums letters\n3     4       d\n4     5       e\n5     6       f\n```", "```py\n>>> df = pd.DataFrame({\"nums\": [88, 99], \"letters\": [\"xx\", \"yy\"]})\n>>> ddf2 = dd.from_pandas(df, npartitions=1)\n>>> ddf2.divisions\n(0, 1)\n```", "```py\n>>> print_partitions(ddf2)\n   nums letters\n0    88      xx\n1    99      yy\n```", "```py\n>>> ddf3 = dd.concat([ddf1, ddf2])\n>>> ddf3.divisions\n(None, None, None, None)\n```", "```py\n>>> ddf3_interleave = dd.concat([ddf1, ddf2], interleave_partitions=True)\n>>> ddf3_interleave.divisions\n(0, 1, 3, 5)\n```", "```py\n>>> print_partitions(ddf3_interleave)\n  nums letters\n0     1       a\n0    88      xx\n   nums letters\n1     2       b\n2     3       c\n1    99      yy\n   nums letters\n3     4       d\n4     5       e\n5     6       f\n```", "```py\ndf = pd.DataFrame(\n    {\n        \"animal\": [\"cat\", \"dolphin\", \"shark\", \"starfish\"],\n        \"is_mammal\": [True, True, False, False],\n    }\n)\nddf1 = dd.from_pandas(df, npartitions=2)\ndf = pd.DataFrame({\"animal\": [\"hippo\", \"lion\"], \"likes_water\": [True, False]})\nddf2 = dd.from_pandas(df, npartitions=1)\n```", "```py\n>>> ddf3 = dd.concat([ddf1, ddf2])\n>>> print(ddf3.compute())\n    animal is_mammal likes_water\n0       cat      True         NaN\n1   dolphin      True         NaN\n2     shark     False         NaN\n3  starfish     False         NaN\n0     hippo       NaN        True\n1      lion       NaN       False\n```", "```py\nimport coiled\nimport dask\ncluster = coiled.Cluster(name=\"concat-cluster\", n_workers=5)\nclient = dask.distributed.Client(cluster)\nddf1990s = dd.read_parquet(\n    \"s3://coiled-datasets/timeseries/7d/parquet/1990s\",\n    storage_options={\"anon\": True, \"use_ssl\": True},\n    engine=\"pyarrow\"\n)\n```", "```py\n>>> ddf1990s.head()\n```", "```py\n>>> len(ddf1990s)\n311,449,600\n>>> ddf1990s.npartitions\n552\n```", "```py\n>>> len(ddf)\n661,449,600\n>>> ddf.npartitions\n1050\n```", "```py\n>>> ddf = dd.concat([ddf1990s, ddf])\n>>> len(ddf)\n972,899,200\n\n>>> ddf.npartitions\n1602\n>>> ddf.divisions\n(Timestamp('1990-01-01 00:00:00'),\n Timestamp('1990-01-08 00:00:00'),\n Timestamp('1990-01-15 00:00:00'),\n …\n Timestamp('2020-12-17 00:00:00'),\n Timestamp('2020-12-24 00:00:00'),\n Timestamp('2020-12-30 23:59:59'))\n```", "```py\nddf.to_parquet(“filename.parquet”)\nddf.to_csv(“filename.csv”)\n```", "```py\n>>> df = pd.DataFrame({\n            \"nums\": [1, 2, 3, 4, 5, 6], \n            \"letters\": [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"]\n    })\n>>> ddf = dd.from_pandas(df, npartitions=2)\n>>> ddf.npartitions\n2\n```", "```py\n>>> ddf.partitions[0].compute()\n     nums   letters\n0    1      a\n1    2      b\n2    3      c\n```", "```py\nddf.to_csv(\"data.csv\")\n```", "```py\n>>> $ cd data.csv\n>>> $ ls\n0.part\t1.part\n```", "```py\n>>> ddf_1 = dd.read_csv('data.csv/0.part')\n>>> ddf_1.compute()\n     0: unnamed   nums   letters\n0    0            1      a\n1    1            2      b\n2    2            3      c\n```", "```py\n>>> ddf.to_csv(\"data.csv\", index=False)\n>>> ddf_1 = dd.read_csv('data.csv/0.part')\n>>> ddf_1.compute()\n     nums   letters\n0    1      a\n1    2      b\n2    3      c\n```", "```py\n>>> ddf = dd.read_csv('data.csv/*')\n>>> ddf.compute()\n     nums   letters\n0    1      a\n1    2      b\n2    3      c\n0    4      d\n1    5      e\n2    6      f\n```", "```py\nddf.to_csv(\n\"single_file.csv\", \nindex=False, \nsingle_file=True\n)\n```", "```py\ndf = pd.DataFrame({\n\"letter\": [\"a\", \"b\", \"c\", \"a\", \"a\", \"d\"], \n\"number\": [1, 2, 3, 4, 5, 6]\n})\nddf = dd.from_pandas(df, npartitions=3)\nddf.to_parquet(\n\"output/partition_on\", \npartition_on=\"letter\"\n)\n```", "```py\noutput/partition_on\n  letter=a/\n    part.0.parquet\n    part.1.parquet\n    part.2.parquet\n  letter=b/\n    part.0.parquet\n  letter=c/\n    part.1.parquet\n  letter=d/\n    part.2.parquet\n```", "```py\n>>> ddf = dd.read_parquet(\n       \"tmp/partition/1\", \n       engine=\"pyarrow\", \n       filters=[(\"letter\", \"==\", \"a\")]\n    )\n>>> print(ddf.compute())\n  number letter\n0       1      a\n3       4      a\n4       5      a\n```", "```py\nddf = dd.read_parquet(\n   \"output/partition_on\", \n   engine=\"pyarrow\", \n   filters=[(\"letter\", \"==\", \"a\")]\n)\n```", "```py\nddf = dd.read_parquet(\n    \"output/partition_on\", \n    engine=\"pyarrow\"\n)\nddf.loc[ddf[\"number\"] == 2]\n```"]