["```py\nfrom ray.data import from_pandas\nimport sklearn.datasets\n\ndata_raw = sklearn.datasets.load_breast_cancer(as_frame=True)  ![1](assets/1.png)\n\ndataset_df = data_raw[\"data\"]\npredict_ds = from_pandas(dataset_df)  ![2](assets/2.png)\n\ndataset_df[\"target\"] = data_raw[\"target\"]\ndataset = from_pandas(dataset_df)\n```", "```py\npreprocessor = Chain(  ![1](assets/1.png)\n    Scaler([\"worst radius\", \"worst area\"]),  ![2](assets/2.png)\n    Repartitioner(num_partitions=2)  ![3](assets/3.png)\n)\n```", "```py\ntrainer = XGBoostTrainer(\n    scaling_config={  ![1](assets/1.png)\n        \"num_actors\": 2,\n        \"gpus_per_actor\": 0,\n        \"cpus_per_actor\": 2,\n    },\n    label=\"target\",  ![2](assets/2.png)\n    params={  ![3](assets/3.png)\n        \"tree_method\": \"approx\",\n        \"objective\": \"binary:logistic\",\n        \"eval_metric\": [\"logloss\", \"error\"],\n    },\n)\n\nresult = trainer.fit(dataset=dataset, preprocessor=preprocessor)  ![4](assets/4.png)\n\nprint(result)\n```", "```py\nresult = trainer.fit(dataset=dataset, preprocessor=preprocessor)\n```", "```py\npickle.dumps(prep)  ![1](assets/1.png)\n```", "```py\nimport torch\nimport torch.nn as nn\n\nnum_samples = 20\ninput_size = 10\nlayer_size = 15\noutput_size = 5\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.layer1 = nn.Linear(input_size, layer_size)\n        self.relu = nn.ReLU()\n        self.layer2 = nn.Linear(layer_size, output_size)\n\n    def forward(self, input_data):\n        return self.layer2(self.relu(self.layer1(input_data)))\n\ninput = torch.randn(num_samples, input_size)  ![1](assets/1.png)\nlabels = torch.randn(num_samples, output_size)\n\ndef train_func():  ![2](assets/2.png)\n    num_epochs = 3\n    model = NeuralNetwork()\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(model.parameters(), lr=0.1)\n    for epoch in range(num_epochs):\n        output = model(input)\n        loss = loss_fn(output, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n```", "```py\nimport ray.train.torch\n\ndef train_func_distributed():\n    num_epochs = 3\n    model = NeuralNetwork()\n    model = train.torch.prepare_model(model)  ![1](assets/1.png)\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(model.parameters(), lr=0.1)\n    for epoch in range(num_epochs):\n        output = model(input)\n        loss = loss_fn(output, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n```", "```py\nfrom ray.train import Trainer\n\ntrainer = Trainer(backend=\"torch\", num_workers=4, use_gpu=False)  ![1](assets/1.png)\ntrainer.start()\nresults = trainer.run(train_func_distributed)\ntrainer.shutdown()\n```", "```py\nscaling_config = {\"num_workers\": 10, \"use_gpu\": True}\n\ntrainer = ray.train.integrations.XGBoostTrainer(\n    scaling_config=scaling_config,\n    # ...\n)\n```", "```py\n# Connect to a large Ray cluster\nray.init(address=\"auto\")\n\nscaling_config = {\"num_workers\": 200, \"use_gpu\": True}\n\ntrainer = ray.train.integrations.XGBoostTrainer(\n    scaling_config=scaling_config,\n    # ...\n)\n```", "```py\nfrom typing import Dict\nimport torch\nimport torch.nn as nn\n\nimport ray\nimport ray.train as train\nfrom ray.train import Trainer\n\ndef get_datasets(a=5, b=10, size=1000, split=0.8):\n\n    def get_dataset(a, b, size):\n        items = [i / size for i in range(size)]\n        dataset = ray.data.from_items([{\"x\": x, \"y\": a * x + b} for x in items])\n        return dataset\n\n    dataset = get_dataset(a, b, size)\n    split_index = int(dataset.count() * split)\n    train_dataset, validation_dataset = dataset.random_shuffle().split_at_indices(\n        [split_index]\n    )\n    train_dataset_pipeline = train_dataset.repeat().random_shuffle_each_window()\n    validation_dataset_pipeline = validation_dataset.repeat()\n    datasets = {\n        \"train\": train_dataset_pipeline,\n        \"validation\": validation_dataset_pipeline,\n    }\n    return datasets\n```", "```py\ndef train_func(config):\n    batch_size = config[\"batch_size\"]\n    # hidden_size = config[\"hidden_size\"]\n    # lr = config.get(\"lr\", 1e-2)\n    epochs = config.get(\"epochs\", 3)\n\n    train_dataset_pipeline_shard = train.get_dataset_shard(\"train\")\n    validation_dataset_pipeline_shard = train.get_dataset_shard(\"validation\")\n    train_dataset_iterator = train_dataset_pipeline_shard.iter_epochs()\n    validation_dataset_iterator = validation_dataset_pipeline_shard.iter_epochs()\n\n    for _ in range(epochs):\n        train_dataset = next(train_dataset_iterator)\n        validation_dataset = next(validation_dataset_iterator)\n        train_torch_dataset = train_dataset.to_torch(\n            label_column=\"y\",\n            feature_columns=[\"x\"],\n            label_column_dtype=torch.float,\n            feature_column_dtypes=torch.float,\n            batch_size=batch_size,\n        )\n        validation_torch_dataset = validation_dataset.to_torch(\n            label_column=\"y\",\n            feature_columns=[\"x\"],\n            label_column_dtype=torch.float,\n            feature_column_dtypes=torch.float,\n            batch_size=batch_size,\n        )\n        # ... training\n\n    return results\n```", "```py\nnum_workers = 4\nuse_gpu = False\n\ndatasets = get_datasets()\ntrainer = Trainer(\"torch\", num_workers=num_workers, use_gpu=use_gpu)\nconfig = {\"lr\": 1e-2, \"hidden_size\": 1, \"batch_size\": 4, \"epochs\": 3}\ntrainer.start()\nresults = trainer.run(\n   train_func,\n   config,\n   dataset=datasets,\n   callbacks=[JsonLoggerCallback(), TBXLoggerCallback()],\n)\ntrainer.shutdown()\nprint(results)\n```", "```py\nresult = trainer.fit()\nmodel: ray.train.Model = result.checkpoint.load_model()\n```", "```py\nchkpt = Checkpoint.from_directory(dir)\nchkpt.to_bytes() -> bytes\n```", "```py\n# Run the training function, logging all the intermediate results\n# to MLflow and Tensorboard.\n\nresult = trainer.run(\n    train_func,\n    callbacks=[\n        MLflowLoggerCallback(experiment_name=\"train_experiment\"),\n        TBXLoggerCallback(),\n    ],\n)\n```", "```py\nfrom ray import tune\n\nfail_after_finished = True\nprep_v1 = preprocessor\nprep_v2 = preprocessor\n\nparam_space = {\n    \"scaling_config\": {\n        \"num_actors\": tune.grid_search([2, 4]),\n        \"cpus_per_actor\": 2,\n        \"gpus_per_actor\": 0,\n    },\n    \"preprocessor\": tune.grid_search([prep_v1, prep_v2]),\n    # \"datasets\": {\n    #     \"train_dataset\": tune.grid_search([dataset_v1, dataset_v2]),\n    # },\n    \"params\": {\n        \"objective\": \"binary:logistic\",\n        \"tree_method\": \"approx\",\n        \"eval_metric\": [\"logloss\", \"error\"],\n        \"eta\": tune.loguniform(1e-4, 1e-1),\n        \"subsample\": tune.uniform(0.5, 1.0),\n        \"max_depth\": tune.randint(1, 9),\n    },\n}\nif fail_after_finished > 0:\n    callbacks = [StopperCallback(fail_after_finished=fail_after_finished)]\nelse:\n    callbacks = None\ntuner = tune.Tuner(\n    XGBoostTrainer(\n        run_config={\"max_actor_restarts\": 1},\n        scaling_config=None,\n        resume_from_checkpoint=None,\n        label=\"target\",\n    ),\n    run_config={},\n    param_space=param_space,\n    name=\"tuner_resume\",\n    callbacks=callbacks,\n)\nresults = tuner.fit(datasets={\"train_dataset\": dataset})\nprint(results.results)\n```", "```py\nresult = trainer.fit(dataset=dataset, preprocessor=preprocessor)\nprint(result)\n\nthis_checkpoint = result.checkpoint\nthis_model = this_checkpoint.load_model()\npredicted = this_model.predict(predict_ds)\nprint(predicted.to_pandas())\n```"]