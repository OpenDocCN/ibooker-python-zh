- en: Chapter 2\. Getting Started with Ray (Locally)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we’ve discussed, Ray is useful for managing resources from a single computer
    up to a cluster. It is simpler to get started with a local installation, which
    leverages the parallelism of multicore/multi-CPU machines. Even when deploying
    to a cluster, you’ll want to have Ray installed locally for development. Once
    you’ve installed Ray, we’ll show you how to make and call your first asynchronous
    parallelized function and store state in an actor.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you are in a hurry, you can also use [Gitpod on the book’s GitHub repo](https://oreil.ly/7YUbX)
    to get a web environment with the examples, or check out [Anyscale’s managed Ray](https://oreil.ly/UacuZ).
  prefs: []
  type: TYPE_NORMAL
- en: Installation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Installing Ray, even on a single machine, can range from relatively straightforward
    to fairly complicated. Ray publishes wheels to the Python Package Index (PyPI)
    following a normal release cadence as well as in nightly releases. These wheels
    are currently available for only x86 users, so ARM users will mostly need to build
    Ray from source.^([1](ch02.html#idm45354787565408))
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: M1 ARM users on macOS can use the x86 packages with Rosetta. Some performance
    degradation occurs, but it’s a much simpler setup. To use the x86s package, install
    Anaconda for macOS.
  prefs: []
  type: TYPE_NORMAL
- en: Installing for x86 and M1 ARM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most users can run `pip install -U ray` to automatically install Ray from PyPI.
    When you go to distribute your computation on multiple machines, it’s often easier
    to have been working in a Conda environment so you can match Python versions with
    your cluster and know your package dependencies. The commands in [Example 2-1](#ex_ray_conda)
    set up a fresh Conda environment with Python and install Ray with minimal dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-1\. [Installing Ray inside a Conda environment](https://oreil.ly/rxdEC)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Installing (from Source) for ARM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For ARM users or any users with a system architecture that does not have a prebuilt
    wheel available, you will need to build Ray from the source. On our ARM Ubuntu
    system, we need to install additional packages, as shown in [Example 2-2](#debian_ray_arms_pkgs).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-2\. [Installing Ray from source](https://oreil.ly/k97Lt)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: If you are an M1 Mac user who doesn’t want to use Rosetta, you’ll need to install
    some dependencies. You can install them with Homebrew and `pip`, as shown in [Example 2-3](#m1_ray_arms_pkgs).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-3\. [Installing extra dependencies needed on the M1](https://oreil.ly/4KDxL)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You need to build some of the Ray components separately because they are written
    in different languages. This does make installation more complicated, but you
    can follow the steps in [Example 2-4](#build_ray).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-4\. [Installing the build tools for Ray’s native build toolchain](https://oreil.ly/k97Lt)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The slowest part of the build is compiling the C++ code, which can easily take
    up to an hour even on modern machines. If you have a cluster with numerous ARM
    machines, building a wheel once and reusing it on your cluster is often worthwhile.
  prefs: []
  type: TYPE_NORMAL
- en: Hello Worlds
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you have Ray installed, it’s time to learn about some of the Ray APIs.
    We’ll cover these APIs in more detail later, so don’t get too hung up on the details
    now.
  prefs: []
  type: TYPE_NORMAL
- en: Ray Remote (Task/Futures) Hello World
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the core building blocks of Ray is that of remote functions, which return
    futures. The term *remote* here indicates *remote to our main process*, and can
    be on the same or a different machine.
  prefs: []
  type: TYPE_NORMAL
- en: To understand this better, you can write a function that returns the location
    where it is running. Ray distributes work among multiple processes and, when in
    distributed mode, multiple hosts. A local (non-Ray) version of this function is
    shown in [Example 2-5](#ex_local_fun).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-5\. [A local (regular) function](https://oreil.ly/perip)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: You can use the `ray.remote` decorator to create a remote function. Calling
    remote functions is a bit different from calling local ones and is done by calling
    `.remote` on the function. Ray will immediately return a future when you call
    a remote function instead of blocking for the result. You can use `ray.get` to
    get the values returned in those futures. To convert [Example 2-5](#ex_local_fun)
    to a remote function, all you need to do is use the `ray.remote` decorator, as
    shown in [Example 2-6](#ex_remote_fun).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-6\. [Turning the previous function into a remote function](https://oreil.ly/perip)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: When you run these two examples, you’ll see that the first is executed in the
    same process, and that Ray schedules the second one in another process. When we
    run the two examples, we get `Running on jupyter-holdenk in pid 33` and `Running
    on jupyter-holdenk in pid 173`, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Sleepy task
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An easy (although artificial) way to understand how remote futures can help
    is by making an intentionally slow function (in our case, `slow_task`) and having
    Python compute in regular function calls and Ray remote calls. See [Example 2-7](#sleepy_task).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-7\. [Using Ray to parallelize an intentionally slow function](https://oreil.ly/perip)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: When you run this code, you’ll see that by using Ray remote functions, your
    code is able to execute multiple remote functions at the same time. While you
    can do this without Ray by using `multiprocessing`, Ray handles all of the details
    for you and can also eventually scale up to multiple machines.
  prefs: []
  type: TYPE_NORMAL
- en: Nested and chained tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Ray is notable in the distributed processing world for allowing nested and chained
    tasks. Launching more tasks inside other tasks can make certain kinds of recursive
    algorithms easier to implement.
  prefs: []
  type: TYPE_NORMAL
- en: One of the more straightforward examples using nested tasks is a web crawler.
    In the web crawler, each page we visit can launch multiple additional visits to
    the links on that page, as shown in [Example 2-8](#nested_task).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-8\. [Web crawler with nested tasks](https://oreil.ly/perip)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Many other systems require that all tasks launch on a central coordinator node.
    Even those that support launching tasks in a nested fashion still usually depend
    on a central scheduler.
  prefs: []
  type: TYPE_NORMAL
- en: Data Hello World
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ray has a somewhat limited dataset API for working with structured data. Apache
    Arrow powers Ray’s Datasets API. Arrow is a column-oriented, language-independent
    format with some popular operations. Many popular tools support Arrow, allowing
    easy transfer between them (such as Spark, Ray, Dask, and TensorFlow).
  prefs: []
  type: TYPE_NORMAL
- en: Ray only recently added keyed aggregations on datasets with version 1.9\. The
    most popular distributed data example is a word count, which requires aggregates.
    Instead of using these, we can perform embarrassingly parallel tasks, such as
    map transformations, by constructing a dataset of web pages, shown in [Example 2-9](#ds_hello).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-9\. [Constructing a dataset of web pages](https://oreil.ly/perip)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Ray 1.9 added `GroupedDataset` for supporting various kinds of aggregations.
    By calling `groupby` with either a column name or a function that returns a key,
    you get a `GroupedDataset`. `GroupedDataset` has built-in support for `count`,
    `max`, `min`, and other common aggregations. You can use `GroupedDataset` to extend
    [Example 2-9](#ds_hello) into a word-count example, as shown in [Example 2-10](#ds_wc).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-10\. [Converting a dataset of web pages into words](https://oreil.ly/perip)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: When you need to go beyond the built-in operations, Ray supports custom aggregations,
    provided you implement its interface. We will cover more on datasets, including
    aggregate functions, in [Chapter 9](ch09.html#ch09).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Ray uses *blocking evaluation* for its Dataset API. When you call a function
    on a Ray dataset, it will wait until it completes the result instead of returning
    a future. The rest of the Ray Core API uses futures.
  prefs: []
  type: TYPE_NORMAL
- en: If you want a full-featured DataFrame API, you can convert your Ray dataset
    into Dask. [Chapter 9](ch09.html#ch09) covers how to use Dask for more complex
    operations. If you are interested in learning more about Dask, check out [*Scaling
    Python with Dask*](https://oreil.ly/LKMlO) (O’Reilly), which Holden coauthored
    with Mika Kimmins.
  prefs: []
  type: TYPE_NORMAL
- en: Actor Hello World
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the unique parts of Ray is its emphasis on actors. Actors give you tools
    to manage the execution state, which is one of the more challenging parts of scaling
    systems. Actors send and receive messages, updating their state in response. These
    messages can come from other actors, programs, or your main execution thread with
    the Ray client.
  prefs: []
  type: TYPE_NORMAL
- en: For every actor, Ray starts a dedicated process. Each actor has a mailbox of
    messages waiting to be processed. When you call an actor, Ray adds a message to
    the corresponding mailbox, which allows Ray to serialize message processing, thus
    avoiding expensive distributed locks. Actors can return values in response to
    messages, so when you send a message to an actor, Ray immediately returns a future
    so you can fetch the value when the actor is done processing your message.
  prefs: []
  type: TYPE_NORMAL
- en: Ray actors are created and called similarly to remote functions but use Python
    classes, which gives the actor a place to store state. You can see this in action
    by modifying the classic “Hello World” example to greet you in sequence, as shown
    in [Example 2-11](#actor_hello_world).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-11\. [Actor Hello World](https://oreil.ly/perip)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This example is fairly basic; it lacks any fault tolerance or concurrency within
    each actor. We’ll explore those more in [Chapter 4](ch04.html#ch04).
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you installed Ray on your local machine and used many of its
    core APIs. For the most part, you can continue to run the examples we’ve picked
    for this book in local mode. Naturally, local mode can limit your scale or take
    longer to run.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll look at some of the core concepts behind Ray. One
    of the concepts (fault tolerance) will be easier to illustrate with a cluster
    or cloud. So if you have access to a cloud account or a cluster, now would be
    an excellent time to jump over to [Appendix B](app02.html#appB) and look at the
    deployment options.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch02.html#idm45354787565408-marker)) As ARM grows in popularity, Ray is
    more likely to add ARM wheels, so this is hopefully temporary.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch02.html#idm45354786831024-marker)) Actors are still more expensive than
    lock-free remote functions, which can be scaled horizontally. For example, lots
    of workers calling the same actor to update model weights will still be slower
    than embarrassingly parallel operations.
  prefs: []
  type: TYPE_NORMAL
