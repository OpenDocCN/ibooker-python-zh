<html><head></head><body><section data-pdf-bookmark="Chapter 12. Container Orchestration: Kubernetes" data-type="chapter" epub:type="chapter"><div class="chapter" id="containers-kubernetes">&#13;
<h1><span class="label">Chapter 12. </span>Container Orchestration: Kubernetes</h1>&#13;
&#13;
&#13;
<p><a data-primary="Kubernetes" data-type="indexterm" id="ix_ch12-asciidoc0"/>If you are experimenting with Docker, or if running a set of Docker containers on a single machine is all you need, then Docker and Docker Compose would be sufficient for your needs. However, as soon as you move from the number <code>1</code> (single machine) to the number <code>2</code> (multiple machines), you need to start worrying about orchestrating the containers across the network. For production scenarios, this is a given. You need at least two machines to achieve fault tolerance/high availability.</p>&#13;
&#13;
<p>In our age of cloud computing, the recommended way of scaling an infrastructure is “out” (also referred to as “horizontal scalability”), by adding more instances to your overall system, as opposed to the older way of scaling “up” (or “vertical scalability”), by adding more CPUs and memory to a single instance. A Docker orchestration platform uses these many instances or nodes as sources of raw resources (CPU, memory, network) that it then allocates to individual containers running within the platform. This ties into what we mentioned in <a data-type="xref" href="ch11.html#containers-docker">Chapter 11</a> in regards to the advantages of using containers over classic virtual machines (VMs): the raw resources at your disposal will be better utilized because containers can get these resources allocated to them on a much more granular basis than VMs, and you will&#13;
get more bang for your infrastructure buck.</p>&#13;
&#13;
<p>There has also been a shift from provisioning servers for specific purposes and running specific software packages on each instance (such as web server software, cache software, database software) to provisioning them as generic units of resource allocation and running Docker containers on them, coordinated by a Docker orchestration platform. You may be familiar with the distinction between looking at servers as “pets” versus looking at them as “cattle.” In the early days of infrastructure design, each server had a definite function (such as the mail server), and many times there was only one server for each specific function. There were naming schemes for such servers (Grig remembers using a planetary system naming scheme in the dot-com days), and a lot of time was spent on their care and feeding, hence the pet designation. When configuration management tools such as Puppet, Chef, and Ansible burst onto the scene, it became easier to provision multiple servers of the same type (for example, a web server farm) at the same time, by using an identical installation procedure on each server. This coincided with the rise of cloud computing, with the concept of horizontal scalability mentioned previously, and also with more concern for fault tolerance and high availability as critical properties of well-designed system infrastructure. The servers or cloud instances were considered cattle, disposable units that have value in their aggregate.</p>&#13;
&#13;
<p>The age of containers and serverless computing also brought about another designation, “insects.” Indeed, one can look at the coming and going of containers as a potentially short existence, like an ephemeral insect. Functions-as-a-service are even more fleeting than Docker containers, with a short but intense life coinciding with the duration of their call.</p>&#13;
&#13;
<p>In the case of containers, their ephemerality makes their orchestration and interoperability hard to achieve at a large scale. This is exactly the need that has been filled by container orchestration platforms. There used to be multiple Docker orchestration platforms to choose from, such as Mesosphere and Docker Swarm, but these days we can safely say that Kubernetes has won that game. The rest of the chapter is dedicated to a short overview of Kubernetes, followed by an example of running the same application described in <a data-type="xref" href="ch11.html#containers-docker">Chapter 11</a> and porting it from <code>docker-compose</code> to Kubernetes. We will also show how to use Helm, a Kubernetes package manager, to install packages called charts for the monitoring and dashboarding tools Prometheus and Grafana, and how to customize these charts.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Short Overview of Kubernetes Concepts" data-type="sect1"><div class="sect1" id="idm46691321626696">&#13;
<h1>Short Overview of Kubernetes Concepts</h1>&#13;
&#13;
<p><a data-primary="Kubernetes" data-secondary="overview of concepts" data-type="indexterm" id="idm46691321625240"/>The best starting point for understanding the many parts comprising a Kubernetes cluster is <a href="https://oreil.ly/TYpdE">the official Kubernetes documentation</a>.</p>&#13;
&#13;
<p>At a high level, a Kubernetes cluster consists of nodes that can be equated to servers, be they bare-metal or virtual machines running in a cloud. Nodes run pods, which are collections of Docker containers. A pod is the unit of deployment in Kubernetes. All containers in a pod share the same network and can refer to each other as if they were running on the same host. There are many situations in which it is advantageous to run more than one container in a pod. Typically, your application container runs as the main container in the pod, and if needed you will run one or more so-called “sidecar” containers for functionality, such as logging or monitoring. One particular case of sidecar containers is an “init container,” which is guaranteed to run first and can be used for housekeeping tasks, such as running database migrations. We’ll explore this later in this chapter.</p>&#13;
&#13;
<p>An application will typically use more than one pod for fault tolerance and performance purposes. The Kubernetes object responsible for launching and maintaining the desired number of pods is called a deployment. For pods to communicate with other pods, Kubernetes provides another kind of object called a service. Services are tied to deployments through selectors. Services are also exposed to external clients, either by exposing a NodePort as a static port on each Kubernetes node, or by creating a LoadBalancer object that corresponds to an actual load balancer, if it is supported by the cloud provider running the Kubernetes cluster.</p>&#13;
&#13;
<p>For managing sensitive information such as passwords, API keys, and other credentials, Kubernetes offers the Secret object. We will see an example of using a Secret for storing a database password.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Using Kompose to Create Kubernetes Manifests from docker-compose.yaml" data-type="sect1"><div class="sect1" id="idm46691321620168">&#13;
<h1>Using Kompose to Create Kubernetes Manifests from docker-compose.yaml</h1>&#13;
&#13;
<p><a data-primary="Kubernetes" data-secondary="using Kompose to create Kubernetes manifests from docker-compose.yaml" data-type="indexterm" id="idm46691321618920"/>Let’s take another look at the <em>docker_compose.yaml</em> file for the Flask example application discussed in <a data-type="xref" href="ch11.html#containers-docker">Chapter 11</a>:</p>&#13;
&#13;
<pre data-type="programlisting">$ cat docker-compose.yaml&#13;
version: "3"&#13;
services:&#13;
  app:&#13;
    image: "griggheo/flask-by-example:v1"&#13;
    command: "manage.py runserver --host=0.0.0.0"&#13;
    ports:&#13;
      - "5000:5000"&#13;
    environment:&#13;
      APP_SETTINGS: config.ProductionConfig&#13;
      DATABASE_URL: postgresql://wordcount_dbadmin:$DBPASS@db/wordcount&#13;
      REDISTOGO_URL: redis://redis:6379&#13;
    depends_on:&#13;
      - db&#13;
      - redis&#13;
  worker:&#13;
    image: "griggheo/flask-by-example:v1"&#13;
    command: "worker.py"&#13;
    environment:&#13;
      APP_SETTINGS: config.ProductionConfig&#13;
      DATABASE_URL: postgresql://wordcount_dbadmin:$DBPASS@db/wordcount&#13;
      REDISTOGO_URL: redis://redis:6379&#13;
    depends_on:&#13;
      - db&#13;
      - redis&#13;
  migrations:&#13;
    image: "griggheo/flask-by-example:v1"&#13;
    command: "manage.py db upgrade"&#13;
    environment:&#13;
      APP_SETTINGS: config.ProductionConfig&#13;
      DATABASE_URL: postgresql://wordcount_dbadmin:$DBPASS@db/wordcount&#13;
    depends_on:&#13;
      - db&#13;
  db:&#13;
    image: "postgres:11"&#13;
    container_name: "postgres"&#13;
    ports:&#13;
      - "5432:5432"&#13;
    volumes:&#13;
      - dbdata:/var/lib/postgresql/data&#13;
  redis:&#13;
    image: "redis:alpine"&#13;
    ports:&#13;
      - "6379:6379"&#13;
volumes:&#13;
  dbdata:</pre>&#13;
&#13;
<p>We will use a tool called <code>Kompose</code> to translate this YAML file into a set of Kubernetes manifests.</p>&#13;
&#13;
<p>To get a new version of <code>Kompose</code> on a macOS machine, first download it from <a href="https://oreil.ly/GUqaq">the Git repository</a>, then move it to <em>/usr/local/bin/kompose</em>, and make it executable. Note that if you rely on your operating system’s package management system (for example, <code>apt</code> on Ubuntu systems or <code>yum</code> on Red Hat systems) for installing <code>Kompose</code>, you may get a much older version that may not be compatible to these instructions.</p>&#13;
&#13;
<p>Run the <code>kompose convert</code> command to create the Kubernetes manifest files from the existing <em>docker-compose.yaml</em> file:</p>&#13;
&#13;
<pre data-type="programlisting">$ kompose convert&#13;
INFO Kubernetes file "app-service.yaml" created&#13;
INFO Kubernetes file "db-service.yaml" created&#13;
INFO Kubernetes file "redis-service.yaml" created&#13;
INFO Kubernetes file "app-deployment.yaml" created&#13;
INFO Kubernetes file "db-deployment.yaml" created&#13;
INFO Kubernetes file "dbdata-persistentvolumeclaim.yaml" created&#13;
INFO Kubernetes file "migrations-deployment.yaml" created&#13;
INFO Kubernetes file "redis-deployment.yaml" created&#13;
INFO Kubernetes file "worker-deployment.yaml" created</pre>&#13;
&#13;
<p>At this point, remove the <em>docker-compose.yaml</em> file:</p>&#13;
&#13;
<pre data-type="programlisting">$ rm docker-compose.yaml</pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Deploying Kubernetes Manifests to a Local Kubernetes Cluster Based on minikube" data-type="sect1"><div class="sect1" id="idm46691321605832">&#13;
<h1>Deploying Kubernetes Manifests to a Local Kubernetes Cluster Based on minikube</h1>&#13;
&#13;
<p><a data-primary="Kubernetes" data-secondary="deploying Kubernetes manifests to a local Kubernetes cluster based on minikube" data-type="indexterm" id="ix_ch12-asciidoc1"/><a data-primary="minikube" data-type="indexterm" id="ix_ch12-asciidoc2"/>Our next step is to deploy the Kubernetes manifests to a local Kubernetes cluster based on <code>minikube</code>.</p>&#13;
&#13;
<p>A prerequisite to running <code>minikube</code> on macOS is to install <em>VirtualBox</em>. Download the VirtualBox package for macOS from its <a href="https://oreil.ly/BewRq">download page</a>, install it, and then move it to <em>/usr/local/bin/minikube</em> to make it executable. Note that at the time of this writing, <code>minikube</code> installed a Kubernetes cluster with version 1.15. If you want to follow along with these examples, specify the version of Kubernetes you want to install with <code>minikube</code>:</p>&#13;
<pre data-type="programlisting">&#13;
$ minikube start --kubernetes-version v1.15.0&#13;
<img src="assets/smiley.svg"/> minikube v1.2.0 on darwin (amd64)&#13;
<img src="assets/fire.svg"/> Creating virtualbox VM (CPUs=2, Memory=2048MB, Disk=20000MB) ...&#13;
<img src="assets/whale.svg"/> Configuring environment for Kubernetes v1.15.0 on Docker 18.09.6&#13;
<img src="assets/floppy.svg"/> Downloading kubeadm v1.15.0&#13;
<img src="assets/floppy.svg"/> Downloading kubelet v1.15.0&#13;
<img src="assets/tractor.svg"/> Pulling images ...&#13;
<img src="assets/rocket.svg"/> Launching Kubernetes ...&#13;
<img src="assets/hourglass.svg"/> Verifying: apiserver proxy etcd scheduler controller dns&#13;
<img src="assets/surfer.svg"/> Done! kubectl is now configured to use "minikube"&#13;
</pre>&#13;
&#13;
<p>The main command for interacting with a Kubernetes cluster is <code>kubectl</code>.</p>&#13;
&#13;
<p>Install <code>kubectl</code> on a macOS machine by downloading it from <a href="https://oreil.ly/f9Wv0">the release page</a>, then moving it to <em>/usr/local/bin/kubectl</em> and making it <span class="keep-together">executable.</span></p>&#13;
&#13;
<p>One of the main concepts you will use when running <code>kubectl</code> commands is <a data-primary="context, Kubernetes cluster and" data-type="indexterm" id="idm46691321584632"/><em>context</em>, which signifies a Kubernetes cluster that you want to interact with. The installation process for <code>minikube</code> already created a context for us called <em>minikube</em>. One way to point <code>kubectl</code> to a specific context is with the following command:</p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl config use-context minikube&#13;
Switched to context "minikube".</pre>&#13;
&#13;
<p>A different, and more handy, way is to install the <code>kubectx</code> utility from <a href="https://oreil.ly/SIf1U">the Git repository</a>, then run:</p>&#13;
&#13;
<pre data-type="programlisting">$ kubectx minikube&#13;
Switched to context "minikube".</pre>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>Another handy client utility for your Kubernetes work is <a href="https://oreil.ly/AcE32">kube-ps1</a>. For a macOS setup based on Zsh, add this snippet to the file <em>~/.zshrc</em>:</p>&#13;
&#13;
<pre data-type="programlisting">source "/usr/local/opt/kube-ps1/share/kube-ps1.sh"&#13;
PS1='$(kube_ps1)'$PS1</pre>&#13;
&#13;
<p>These lines change the shell prompt to show the current Kubernetes context and namespace. As you start interacting with multiple Kubernetes clusters, this will be a lifesaver for distinguishing between a production and a staging cluster.</p>&#13;
</div>&#13;
&#13;
<p>Now run <code>kubectl</code> commands against the local <code>minikube</code> cluster. For example, the <code>kubectl get nodes</code> command shows the nodes that are part of the cluster. In this case, there is only one node with the role of <code>master</code>:</p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl get nodes&#13;
NAME       STATUS   ROLES    AGE     VERSION&#13;
minikube   Ready    master   2m14s   v1.15.0</pre>&#13;
&#13;
<p>Start by creating the Persistent Volume Claim (PVC) object from the file <em>dbdata-persistentvolumeclaim.yaml</em> that was created by <code>Kompose</code>, and which corresponds to the local volume allocated for the PostgreSQL database container, when running it with <code>docker-compose</code>:</p>&#13;
&#13;
<pre data-type="programlisting">$ cat dbdata-persistentvolumeclaim.yaml&#13;
apiVersion: v1&#13;
kind: PersistentVolumeClaim&#13;
metadata:&#13;
  creationTimestamp: null&#13;
  labels:&#13;
    io.kompose.service: dbdata&#13;
  name: dbdata&#13;
spec:&#13;
  accessModes:&#13;
  - ReadWriteOnce&#13;
  resources:&#13;
    requests:&#13;
      storage: 100Mi&#13;
status: {}</pre>&#13;
&#13;
<p>To create this object in Kubernetes, use the <a data-primary="kubectl create command" data-type="indexterm" id="idm46691321568584"/><code>kubectl create</code> command and specify the file name of the manifest with the <code>-f</code> flag:</p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl create -f dbdata-persistentvolumeclaim.yaml&#13;
persistentvolumeclaim/dbdata created</pre>&#13;
&#13;
<p>List all the PVCs with the <a data-primary="kubectl get pvc command" data-type="indexterm" id="idm46691321565704"/><code>kubectl get pvc</code> command to verify that our PVC is there:</p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl get pvc&#13;
NAME     STATUS   VOLUME                                     CAPACITY&#13;
ACCESS MODES   STORAGECLASS   AGE&#13;
dbdata   Bound    pvc-39914723-4455-439b-a0f5-82a5f7421475   100Mi&#13;
RWO            standard       1m</pre>&#13;
&#13;
<p>The next step is to create the Deployment object for PostgreSQL. Use the manifest file <em>db-deployment.yaml</em> created previously by the <code>Kompose</code> utility:</p>&#13;
&#13;
<pre data-type="programlisting">$ cat db-deployment.yaml&#13;
apiVersion: extensions/v1beta1&#13;
kind: Deployment&#13;
metadata:&#13;
  annotations:&#13;
    kompose.cmd: kompose convert&#13;
    kompose.version: 1.16.0 (0c01309)&#13;
  creationTimestamp: null&#13;
  labels:&#13;
    io.kompose.service: db&#13;
  name: db&#13;
spec:&#13;
  replicas: 1&#13;
  strategy:&#13;
    type: Recreate&#13;
  template:&#13;
    metadata:&#13;
      creationTimestamp: null&#13;
      labels:&#13;
        io.kompose.service: db&#13;
    spec:&#13;
      containers:&#13;
      - image: postgres:11&#13;
        name: postgres&#13;
        ports:&#13;
        - containerPort: 5432&#13;
        resources: {}&#13;
        volumeMounts:&#13;
        - mountPath: /var/lib/postgresql/data&#13;
          name: dbdata&#13;
      restartPolicy: Always&#13;
      volumes:&#13;
      - name: dbdata&#13;
        persistentVolumeClaim:&#13;
          claimName: dbdata&#13;
status: {}</pre>&#13;
&#13;
<p>To create the deployment, use the <a data-primary="kubectl create -f command" data-type="indexterm" id="idm46691321560264"/><code>kubectl create -f</code> command and point it to the manifest file:</p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl create -f db-deployment.yaml&#13;
deployment.extensions/db created</pre>&#13;
&#13;
<p>To verify that the deployment was created, list all deployments in the cluster and list the pods that were created as part of the deployment:</p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl get deployments&#13;
NAME     READY   UP-TO-DATE   AVAILABLE   AGE&#13;
db       1/1     1            1           1m&#13;
&#13;
$ kubectl get pods&#13;
NAME                  READY   STATUS    RESTARTS   AGE&#13;
db-67659d85bf-vrnw7   1/1     Running   0          1m</pre>&#13;
&#13;
<p class="pagebreak-before">Next, create the database for the example Flask application. Use a similar command to <code>docker exec</code> to run the <code>psql</code> command inside a running Docker container. The form of the command in the case of a Kubernetes cluster is <code>kubectl exec</code>:</p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl exec -it db-67659d85bf-vrnw7 -- psql -U postgres&#13;
psql (11.4 (Debian 11.4-1.pgdg90+1))&#13;
Type "help" for help.&#13;
&#13;
postgres=# create database wordcount;&#13;
CREATE DATABASE&#13;
postgres=# \q&#13;
&#13;
$ kubectl exec -it db-67659d85bf-vrnw7 -- psql -U postgres wordcount&#13;
psql (11.4 (Debian 11.4-1.pgdg90+1))&#13;
Type "help" for help.&#13;
&#13;
wordcount=# CREATE ROLE wordcount_dbadmin;&#13;
CREATE ROLE&#13;
wordcount=# ALTER ROLE wordcount_dbadmin LOGIN;&#13;
ALTER ROLE&#13;
wordcount=# ALTER USER wordcount_dbadmin PASSWORD 'MYPASS';&#13;
ALTER ROLE&#13;
wordcount=# \q</pre>&#13;
&#13;
<p>The next step is to create the Service object corresponding to the <code>db</code> deployment, that will expose the deployment to the other services running inside the cluster, such as the Redis worker service and the main application service. Here is the manifest file for the <code>db</code> service:</p>&#13;
&#13;
<pre data-type="programlisting">$ cat db-service.yaml&#13;
apiVersion: v1&#13;
kind: Service&#13;
metadata:&#13;
  annotations:&#13;
    kompose.cmd: kompose convert&#13;
    kompose.version: 1.16.0 (0c01309)&#13;
  creationTimestamp: null&#13;
  labels:&#13;
    io.kompose.service: db&#13;
  name: db&#13;
spec:&#13;
  ports:&#13;
  - name: "5432"&#13;
    port: 5432&#13;
    targetPort: 5432&#13;
  selector:&#13;
    io.kompose.service: db&#13;
status:&#13;
  loadBalancer: {}</pre>&#13;
&#13;
<p class="pagebreak-before">One thing to note is the following section:</p>&#13;
&#13;
<pre data-type="programlisting">  labels:&#13;
    io.kompose.service: db</pre>&#13;
&#13;
<p>This section appears in both the deployment manifest and the service manifest and is indeed the way to tie the two together. A service will be associated with any deployment that has the same label.</p>&#13;
&#13;
<p>Create the Service object with the <code>kubectl create -f</code> command:</p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl create -f db-service.yaml&#13;
service/db created</pre>&#13;
&#13;
<p>List all services and notice that the <code>db</code> service was created:</p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl get services&#13;
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE&#13;
db           ClusterIP   10.110.108.96   &lt;none&gt;        5432/TCP   6s&#13;
kubernetes   ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP    4h45m</pre>&#13;
&#13;
<p>The next service to deploy is Redis. Create the Deployment and Service objects based on the manifest files generated by <code>Kompose</code>:</p>&#13;
&#13;
<pre data-type="programlisting">$ cat redis-deployment.yaml&#13;
apiVersion: extensions/v1beta1&#13;
kind: Deployment&#13;
metadata:&#13;
  annotations:&#13;
    kompose.cmd: kompose convert&#13;
    kompose.version: 1.16.0 (0c01309)&#13;
  creationTimestamp: null&#13;
  labels:&#13;
    io.kompose.service: redis&#13;
  name: redis&#13;
spec:&#13;
  replicas: 1&#13;
  strategy: {}&#13;
  template:&#13;
    metadata:&#13;
      creationTimestamp: null&#13;
      labels:&#13;
        io.kompose.service: redis&#13;
    spec:&#13;
      containers:&#13;
      - image: redis:alpine&#13;
        name: redis&#13;
        ports:&#13;
        - containerPort: 6379&#13;
        resources: {}&#13;
      restartPolicy: Always&#13;
status: {}&#13;
&#13;
$ kubectl create -f redis-deployment.yaml&#13;
deployment.extensions/redis created&#13;
&#13;
$ kubectl get pods&#13;
NAME                    READY   STATUS    RESTARTS   AGE&#13;
db-67659d85bf-vrnw7     1/1     Running   0          37m&#13;
redis-c6476fbff-8kpqz   1/1     Running   0          11s&#13;
&#13;
$ kubectl create -f redis-service.yaml&#13;
service/redis created&#13;
&#13;
$ cat redis-service.yaml&#13;
apiVersion: v1&#13;
kind: Service&#13;
metadata:&#13;
  annotations:&#13;
    kompose.cmd: kompose convert&#13;
    kompose.version: 1.16.0 (0c01309)&#13;
  creationTimestamp: null&#13;
  labels:&#13;
    io.kompose.service: redis&#13;
  name: redis&#13;
spec:&#13;
  ports:&#13;
  - name: "6379"&#13;
    port: 6379&#13;
    targetPort: 6379&#13;
  selector:&#13;
    io.kompose.service: redis&#13;
status:&#13;
  loadBalancer: {}&#13;
&#13;
$ kubectl get services&#13;
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE&#13;
db           ClusterIP   10.110.108.96   &lt;none&gt;        5432/TCP   84s&#13;
kubernetes   ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP    4h46m&#13;
redis        ClusterIP   10.106.44.183   &lt;none&gt;        6379/TCP   10s</pre>&#13;
&#13;
<p>So far, the two services that have been deployed, <code>db</code> and <code>redis</code>, are independent of each other. The next part of the application is the worker process, which needs to talk to both PostgreSQL and Redis. This is where the advantage of using Kubernetes services comes into play. The worker deployment can refer to the endpoints for PostgreSQL and Redis by using the service names. Kubernetes knows how to route the requests from the client (the containers running as part of the pods in the worker deployment) to the servers (the PostgreSQL and Redis containers running as part of the pods in the <code>db</code> and <code>redis</code> deployments, respectively).</p>&#13;
&#13;
<p>One of the environment variables used in the worker deployment is <code>DATABASE_URL</code>. It contains the database password used by the application. The password should not be exposed in clear text in the deployment manifest file, because this file needs to be checked into version control. Instead, create a Kubernetes Secret object.</p>&#13;
&#13;
<p>First, encode the password string in <code>base64</code>:</p>&#13;
&#13;
<pre data-type="programlisting">$ echo MYPASS | base64&#13;
MYPASSBASE64</pre>&#13;
&#13;
<p>Then, create a manifest file describing the Kubernetes Secret object that you want to create. Since the <code>base64</code> encoding of the password is not secure, use <code>sops</code> to edit and save an encrypted manifest file <em>secrets.yaml.enc</em>:</p>&#13;
&#13;
<pre data-type="programlisting">$ sops --pgp E14104A0890994B9AC9C9F6782C1FF5E679EFF32 secrets.yaml.enc</pre>&#13;
&#13;
<p>Inside the editor, add these lines:</p>&#13;
&#13;
<pre data-type="programlisting">apiVersion: v1&#13;
kind: Secret&#13;
metadata:&#13;
  name: fbe-secret&#13;
type: Opaque&#13;
data:&#13;
  dbpass: MYPASSBASE64</pre>&#13;
&#13;
<p>The <em>secrets.yaml.enc</em> file can now be checked in because it contains the encrypted version of the <code>base64</code> value of the password.</p>&#13;
&#13;
<p>To decrypt the encrypted file, use the <code>sops -d</code> command:</p>&#13;
&#13;
<pre data-type="programlisting">$ sops -d secrets.yaml.enc&#13;
apiVersion: v1&#13;
kind: Secret&#13;
metadata:&#13;
  name: fbe-secret&#13;
type: Opaque&#13;
data:&#13;
  dbpass: MYPASSBASE64</pre>&#13;
&#13;
<p>Pipe the output of <code>sops -d</code> to <code>kubectl create -f</code> to create the Kubernetes Secret object:</p>&#13;
&#13;
<pre data-type="programlisting">$ sops -d secrets.yaml.enc | kubectl create -f -&#13;
secret/fbe-secret created</pre>&#13;
&#13;
<p>Inspect the Kubernetes Secrets and describe the Secret that was created:</p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl get secrets&#13;
NAME                  TYPE                                  DATA   AGE&#13;
default-token-k7652   kubernetes.io/service-account-token   3      3h19m&#13;
fbe-secret            Opaque                                1      45s&#13;
&#13;
$ kubectl describe secret fbe-secret&#13;
Name:         fbe-secret&#13;
Namespace:    default&#13;
Labels:       &lt;none&gt;&#13;
Annotations:  &lt;none&gt;&#13;
&#13;
Type:  Opaque&#13;
&#13;
Data&#13;
dbpass:  12 bytes</pre>&#13;
&#13;
<p>To get the <code>base64</code>-encoded Secret back, use:</p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl get secrets fbe-secret -ojson | jq -r ".data.dbpass"&#13;
MYPASSBASE64</pre>&#13;
&#13;
<p>To get the plain-text password back, use the following command on a macOS machine:</p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl get secrets fbe-secret -ojson | jq -r ".data.dbpass" | base64 -D&#13;
MYPASS</pre>&#13;
&#13;
<p>On a Linux machine, the proper flag for <code>base64</code> decoding is <code>-d</code>, so the correct command would be:</p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl get secrets fbe-secret -ojson | jq -r ".data.dbpass" | base64 -d&#13;
MYPASS</pre>&#13;
&#13;
<p>The secret can now be used in the deployment manifest of the worker. Modify the <em>worker-deployment.yaml</em> file generated by the <code>Kompose</code> utility and add two environment variables:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p><code>DBPASS</code> is the database password that will be retrieved from the fbe-secret Secret object.</p>&#13;
</li>&#13;
<li>&#13;
<p><code>DATABASE_URL</code> is the full database connection string for PostgreSQL, which includes the database password and references it as <code>${DBPASS}</code>.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>This is the modified version of <em>worker-deployment.yaml</em>:</p>&#13;
&#13;
<pre data-type="programlisting">$ cat worker-deployment.yaml&#13;
apiVersion: extensions/v1beta1&#13;
kind: Deployment&#13;
metadata:&#13;
  annotations:&#13;
    kompose.cmd: kompose convert&#13;
    kompose.version: 1.16.0 (0c01309)&#13;
  creationTimestamp: null&#13;
  labels:&#13;
    io.kompose.service: worker&#13;
  name: worker&#13;
spec:&#13;
  replicas: 1&#13;
  strategy: {}&#13;
  template:&#13;
    metadata:&#13;
      creationTimestamp: null&#13;
      labels:&#13;
        io.kompose.service: worker&#13;
    spec:&#13;
      containers:&#13;
      - args:&#13;
        - worker.py&#13;
        env:&#13;
        - name: APP_SETTINGS&#13;
          value: config.ProductionConfig&#13;
        - name: DBPASS&#13;
          valueFrom:&#13;
            secretKeyRef:&#13;
              name: fbe-secret&#13;
              key: dbpass&#13;
        - name: DATABASE_URL&#13;
          value: postgresql://wordcount_dbadmin:${DBPASS}@db/wordcount&#13;
        - name: REDISTOGO_URL&#13;
          value: redis://redis:6379&#13;
        image: griggheo/flask-by-example:v1&#13;
        name: worker&#13;
        resources: {}&#13;
      restartPolicy: Always&#13;
status: {}</pre>&#13;
&#13;
<p>Create the worker Deployment object in the same way as for the other deployments, by calling&#13;
<code>kubectl create -f</code>:</p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl create -f worker-deployment.yaml&#13;
deployment.extensions/worker created</pre>&#13;
&#13;
<p>List the pods:</p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl get pods&#13;
NAME                      READY   STATUS              RESTARTS   AGE&#13;
db-67659d85bf-vrnw7       1/1     Running             1          21h&#13;
redis-c6476fbff-8kpqz     1/1     Running             1          21h&#13;
worker-7dbf5ff56c-vgs42   0/1     Init:ErrImagePull   0          7s</pre>&#13;
&#13;
<p>Note that the worker pod is shown with status <code>Init:ErrImagePull</code>. To see details about this status, run <code>kubectl describe</code>:</p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl describe pod worker-7dbf5ff56c-vgs42 | tail -10&#13;
                 node.kubernetes.io/unreachable:NoExecute for 300s&#13;
Events:&#13;
  Type     Reason     Age                  From               Message&#13;
  ----     ------     ----                 ----               -------&#13;
  Normal   Scheduled  2m51s                default-scheduler&#13;
  Successfully assigned default/worker-7dbf5ff56c-vgs42 to minikube&#13;
&#13;
  Normal   Pulling    76s (x4 over 2m50s)  kubelet, minikube&#13;
  Pulling image "griggheo/flask-by-example:v1"&#13;
&#13;
  Warning  Failed     75s (x4 over 2m49s)  kubelet, minikube&#13;
  Failed to pull image "griggheo/flask-by-example:v1": rpc error:&#13;
  code = Unknown desc = Error response from daemon: pull access denied for&#13;
  griggheo/flask-by-example, repository does not exist or may require&#13;
  'docker login'&#13;
&#13;
  Warning  Failed     75s (x4 over 2m49s)  kubelet, minikube&#13;
  Error: ErrImagePull&#13;
&#13;
  Warning  Failed     62s (x6 over 2m48s)  kubelet, minikube&#13;
  Error: ImagePullBackOff&#13;
&#13;
  Normal   BackOff    51s (x7 over 2m48s)  kubelet, minikube&#13;
  Back-off pulling image "griggheo/flask-by-example:v1"</pre>&#13;
&#13;
<p>The deployment tried to pull the <code>griggheo/flask-by-example:v1</code> private Docker image from Docker Hub, and it lacked the appropriate credentials to access the private Docker registry. Kubernetes includes a special type of object for this very scenario, called an <em>imagePullSecret</em>.</p>&#13;
&#13;
<p>Create an encrypted file with <code>sops</code> containing the Docker Hub credentials and the call to <code>kubectl create secret</code>:</p>&#13;
&#13;
<pre data-type="programlisting">$ sops --pgp E14104A0890994B9AC9C9F6782C1FF5E679EFF32 \&#13;
create_docker_credentials_secret.sh.enc</pre>&#13;
&#13;
<p>The contents of the file are:</p>&#13;
&#13;
<pre data-type="programlisting">DOCKER_REGISTRY_SERVER=docker.io&#13;
DOCKER_USER=Type your dockerhub username, same as when you `docker login`&#13;
DOCKER_EMAIL=Type your dockerhub email, same as when you `docker login`&#13;
DOCKER_PASSWORD=Type your dockerhub pw, same as when you `docker login`&#13;
&#13;
kubectl create secret docker-registry myregistrykey \&#13;
--docker-server=$DOCKER_REGISTRY_SERVER \&#13;
--docker-username=$DOCKER_USER \&#13;
--docker-password=$DOCKER_PASSWORD \&#13;
--docker-email=$DOCKER_EMAIL</pre>&#13;
&#13;
<p>Decode the encrypted file with <code>sops</code> and run it through <code>bash</code>:</p>&#13;
&#13;
<pre data-type="programlisting">$ sops -d create_docker_credentials_secret.sh.enc | bash -&#13;
secret/myregistrykey created</pre>&#13;
&#13;
<p>Inspect the Secret:</p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl get secrets myregistrykey -oyaml&#13;
apiVersion: v1&#13;
data:&#13;
  .dockerconfigjson: eyJhdXRocyI6eyJkb2NrZXIuaW8iO&#13;
kind: Secret&#13;
metadata:&#13;
  creationTimestamp: "2019-07-17T22:11:56Z"&#13;
  name: myregistrykey&#13;
  namespace: default&#13;
  resourceVersion: "16062"&#13;
  selfLink: /api/v1/namespaces/default/secrets/myregistrykey&#13;
  uid: 47d29ffc-69e4-41df-a237-1138cd9e8971&#13;
type: kubernetes.io/dockerconfigjson</pre>&#13;
&#13;
<p>The only change to the worker deployment manifest is to add these lines:</p>&#13;
&#13;
<pre data-type="programlisting">      imagePullSecrets:&#13;
      - name: myregistrykey</pre>&#13;
&#13;
<p>Include it right after this line:</p>&#13;
&#13;
<pre data-type="programlisting">     restartPolicy: Always</pre>&#13;
&#13;
<p>Delete the worker deployment and recreate it:</p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl delete -f worker-deployment.yaml&#13;
deployment.extensions "worker" deleted&#13;
&#13;
$ kubectl create -f worker-deployment.yaml&#13;
deployment.extensions/worker created</pre>&#13;
&#13;
<p>Now the worker pod is in a Running state, with no errors:</p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl get pods&#13;
NAME                      READY   STATUS    RESTARTS   AGE&#13;
db-67659d85bf-vrnw7       1/1     Running   1          22h&#13;
redis-c6476fbff-8kpqz     1/1     Running   1          21h&#13;
worker-7dbf5ff56c-hga37   1/1     Running   0          4m53s</pre>&#13;
&#13;
<p>Inspect the worker pod’s logs with the <a data-primary="kubectl logs command" data-type="indexterm" id="idm46691321471976"/><code>kubectl logs</code> command:</p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl logs worker-7dbf5ff56c-hga37&#13;
20:43:13 RQ worker 'rq:worker:040640781edd4055a990b798ac2eb52d'&#13;
started, version 1.0&#13;
20:43:13 *** Listening on default...&#13;
20:43:13 Cleaning registries for queue: default</pre>&#13;
&#13;
<p>The next step is to tackle the application deployment. When the application was deployed in a <code>docker-compose</code> setup in <a data-type="xref" href="ch11.html#containers-docker">Chapter 11</a>, a separate Docker container was employed to run the migrations necessary to update the Flask database. This type of task is a good candidate for running as a sidecar container in the same pod as the main application container. The sidecar will be defined as a Kubernetes <a href="https://oreil.ly/80L5L"><code>initContainer</code></a> inside the application deployment manifest. This type of container is guaranteed to run inside the pod it belongs to before the start of the other containers included in the pod.</p>&#13;
&#13;
<p>Add this section to the <em>app-deployment.yaml</em> manifest file that was generated by the <code>Kompose</code> utility, and delete the <em>migrations-deployment.yaml</em> file:</p>&#13;
&#13;
<pre data-type="programlisting">      initContainers:&#13;
      - args:&#13;
        - manage.py&#13;
        - db&#13;
        - upgrade&#13;
        env:&#13;
        - name: APP_SETTINGS&#13;
          value: config.ProductionConfig&#13;
        - name: DATABASE_URL&#13;
          value: postgresql://wordcount_dbadmin:@db/wordcount&#13;
        image: griggheo/flask-by-example:v1&#13;
        name: migrations&#13;
        resources: {}&#13;
&#13;
$ rm migrations-deployment.yaml</pre>&#13;
&#13;
<p>Reuse the <code>fbe-secret</code> Secret object created for the worker deployment in the application deployment manifest:</p>&#13;
&#13;
<pre data-type="programlisting">$ cat app-deployment.yaml&#13;
apiVersion: extensions/v1beta1&#13;
kind: Deployment&#13;
metadata:&#13;
  annotations:&#13;
    kompose.cmd: kompose convert&#13;
    kompose.version: 1.16.0 (0c01309)&#13;
  creationTimestamp: null&#13;
  labels:&#13;
    io.kompose.service: app&#13;
  name: app&#13;
spec:&#13;
  replicas: 1&#13;
  strategy: {}&#13;
  template:&#13;
    metadata:&#13;
      creationTimestamp: null&#13;
      labels:&#13;
        io.kompose.service: app&#13;
    spec:&#13;
      initContainers:&#13;
      - args:&#13;
        - manage.py&#13;
        - db&#13;
        - upgrade&#13;
        env:&#13;
        - name: APP_SETTINGS&#13;
          value: config.ProductionConfig&#13;
        - name: DBPASS&#13;
          valueFrom:&#13;
            secretKeyRef:&#13;
              name: fbe-secret&#13;
              key: dbpass&#13;
        - name: DATABASE_URL&#13;
          value: postgresql://wordcount_dbadmin:${DBPASS}@db/wordcount&#13;
        image: griggheo/flask-by-example:v1&#13;
        name: migrations&#13;
        resources: {}&#13;
      containers:&#13;
      - args:&#13;
        - manage.py&#13;
        - runserver&#13;
        - --host=0.0.0.0&#13;
        env:&#13;
        - name: APP_SETTINGS&#13;
          value: config.ProductionConfig&#13;
        - name: DBPASS&#13;
          valueFrom:&#13;
            secretKeyRef:&#13;
              name: fbe-secret&#13;
              key: dbpass&#13;
        - name: DATABASE_URL&#13;
          value: postgresql://wordcount_dbadmin:${DBPASS}@db/wordcount&#13;
        - name: REDISTOGO_URL&#13;
          value: redis://redis:6379&#13;
        image: griggheo/flask-by-example:v1&#13;
        name: app&#13;
        ports:&#13;
        - containerPort: 5000&#13;
        resources: {}&#13;
      restartPolicy: Always&#13;
status: {}</pre>&#13;
&#13;
<p>Create the application deployment with <code>kubectl create -f</code>, then list the pods and describe the application pod:</p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl create -f app-deployment.yaml&#13;
deployment.extensions/app created&#13;
&#13;
$ kubectl get pods&#13;
NAME                      READY   STATUS    RESTARTS   AGE&#13;
app-c845d8969-l8nhg       1/1     Running   0          7s&#13;
db-67659d85bf-vrnw7       1/1     Running   1          22h&#13;
redis-c6476fbff-8kpqz     1/1     Running   1          21h&#13;
worker-7dbf5ff56c-vgs42   1/1     Running   0          4m53s</pre>&#13;
&#13;
<p>The last piece in the deployment of the application to <code>minikube</code> is to ensure that a Kubernetes service is created for the application and that it is declared as type <code>LoadBalancer</code>, so it can be accessed from outside the cluster:</p>&#13;
&#13;
<pre data-type="programlisting">$ cat app-service.yaml&#13;
apiVersion: v1&#13;
kind: Service&#13;
metadata:&#13;
  annotations:&#13;
    kompose.cmd: kompose convert&#13;
    kompose.version: 1.16.0 (0c01309)&#13;
  creationTimestamp: null&#13;
  labels:&#13;
    io.kompose.service: app&#13;
  name: app&#13;
spec:&#13;
  ports:&#13;
  - name: "5000"&#13;
    port: 5000&#13;
    targetPort: 5000&#13;
  type: LoadBalancer&#13;
  selector:&#13;
    io.kompose.service: app&#13;
status:&#13;
  loadBalancer: {}</pre>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Similar to the <code>db</code> service, the <code>app</code> service is tied to the <code>app</code> deployment through a label declaration that exists in both the deployment and the service manifest for the application:</p>&#13;
&#13;
<pre data-type="programlisting">  labels:&#13;
    io.kompose.service: app</pre>&#13;
</div>&#13;
&#13;
<p>Create the service with <a data-primary="kubectl create command" data-type="indexterm" id="idm46691321452072"/><code>kubectl create</code>:</p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl create -f app-service.yaml&#13;
service/app created&#13;
&#13;
$ kubectl get services&#13;
NAME         TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE&#13;
app          LoadBalancer   10.99.55.191    &lt;pending&gt;     5000:30097/TCP   2s&#13;
db           ClusterIP      10.110.108.96   &lt;none&gt;        5432/TCP         21h&#13;
kubernetes   ClusterIP      10.96.0.1       &lt;none&gt;        443/TCP          26h&#13;
redis        ClusterIP      10.106.44.183   &lt;none&gt;        6379/TCP         21h</pre>&#13;
&#13;
<p>Next, run:</p>&#13;
&#13;
<pre data-type="programlisting">$ minikube service app</pre>&#13;
&#13;
<p>This command opens the default browser with the URL <a href="http://192.168.99.100:30097/"><em class="hyperlink">http://192.168.99.100:30097/</em></a> and shows the home page of the Flask site.</p>&#13;
&#13;
<p>In our next section, we will take the same Kubernetes manifest files for our application and deploy them to a Kubernetes cluster that will be provisioned with Pulumi in the Google Cloud Platform (GCP).<a data-startref="ix_ch12-asciidoc2" data-type="indexterm" id="idm46691321446264"/><a data-startref="ix_ch12-asciidoc1" data-type="indexterm" id="idm46691321445592"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Launching a GKE Kubernetes Cluster in GCP with Pulumi" data-type="sect1"><div class="sect1" id="idm46691321602968">&#13;
<h1>Launching a GKE Kubernetes Cluster in GCP with Pulumi</h1>&#13;
&#13;
<p><a data-primary="GCP (Google Cloud Platform), launching GKE Kubernetes cluster in" data-type="indexterm" id="ix_ch12-asciidoc3"/><a data-primary="GKE Kubernetes cluster" data-secondary="launching in GCP with Pulumi" data-type="indexterm" id="ix_ch12-asciidoc4"/><a data-primary="Google Cloud Platform (GCP), launching GKE Kubernetes cluster in" data-type="indexterm" id="ix_ch12-asciidoc5"/><a data-primary="Kubernetes" data-secondary="launching GKE cluster in GCP with Pulumi" data-type="indexterm" id="ix_ch12-asciidoc6"/><a data-primary="Pulumi" data-secondary="launching GKE Kubernetes cluster in GCP" data-type="indexterm" id="ix_ch12-asciidoc7"/>In this section, we’ll make use of the <a href="https://oreil.ly/VGBfF">Pulumi GKE example</a>&#13;
and also of the <a href="https://oreil.ly/kRsFA">GCP setup documentation</a>, so use these links to obtain the necessary documents before proceeding.</p>&#13;
&#13;
<p>Start by creating a new directory:</p>&#13;
&#13;
<pre data-type="programlisting">$ mkdir pulumi_gke&#13;
$ cd pulumi_gke</pre>&#13;
&#13;
<p>Set up the Google Cloud SDK using the <a href="https://oreil.ly/f4pPs">macOS instructions</a>.</p>&#13;
&#13;
<p>Initialize the GCP environment using the <code>gcloud init</code> command. Create a new configuration and a new project named <em>pythonfordevops-gke-pulumi</em>:</p>&#13;
&#13;
<pre data-type="programlisting">$ gcloud init&#13;
Welcome! This command will take you through the configuration of gcloud.&#13;
&#13;
Settings from your current configuration [default] are:&#13;
core:&#13;
  account: grig.gheorghiu@gmail.com&#13;
  disable_usage_reporting: 'True'&#13;
  project: pulumi-gke-testing&#13;
&#13;
Pick configuration to use:&#13;
 [1] Re-initialize this configuration [default] with new settings&#13;
 [2] Create a new configuration&#13;
Please enter your numeric choice:  2&#13;
&#13;
Enter configuration name. Names start with a lower case letter and&#13;
contain only lower case letters a-z, digits 0-9, and hyphens '-':&#13;
pythonfordevops-gke-pulumi&#13;
Your current configuration has been set to: [pythonfordevops-gke-pulumi]&#13;
&#13;
Pick cloud project to use:&#13;
 [1] pulumi-gke-testing&#13;
 [2] Create a new project&#13;
Please enter numeric choice or text value (must exactly match list&#13;
item):  2&#13;
&#13;
Enter a Project ID. pythonfordevops-gke-pulumi&#13;
Your current project has been set to: [pythonfordevops-gke-pulumi].</pre>&#13;
&#13;
<p>Log in to the GCP account:</p>&#13;
&#13;
<pre data-type="programlisting">$ gcloud auth login</pre>&#13;
&#13;
<p>Log in to the default application, which is <code>pythonfordevops-gke-pulumi</code>:</p>&#13;
&#13;
<pre data-type="programlisting">$ gcloud auth application-default login</pre>&#13;
&#13;
<p>Create a new Pulumi project by running the <code>pulumi new</code> command, specifying <em>gcp-python</em> as your template and <em>pythonfordevops-gke-pulumi</em> as the name of the project:</p>&#13;
<pre data-type="programlisting">&#13;
$ pulumi new&#13;
Please choose a template: gcp-python&#13;
A minimal Google Cloud Python Pulumi program&#13;
This command will walk you through creating a new Pulumi project.&#13;
&#13;
Enter a value or leave blank to accept the (default), and press &lt;ENTER&gt;.&#13;
Press ^C at any time to quit.&#13;
&#13;
project name: (pulumi_gke_py) pythonfordevops-gke-pulumi&#13;
project description: (A minimal Google Cloud Python Pulumi program)&#13;
Created project 'pythonfordevops-gke-pulumi'&#13;
&#13;
stack name: (dev)&#13;
Created stack 'dev'&#13;
&#13;
gcp:project: The Google Cloud project to deploy into: pythonfordevops-gke-pulumi&#13;
Saved config&#13;
&#13;
Your new project is ready to go! <img src="assets/sparkle.svg"/>&#13;
&#13;
To perform an initial deployment, run the following commands:&#13;
&#13;
   1. virtualenv -p python3 venv&#13;
   2. source venv/bin/activate&#13;
   3. pip3 install -r requirements.txt&#13;
&#13;
Then, run 'pulumi up'.&#13;
</pre>&#13;
&#13;
<p>The following files were created by the <a data-primary="pulumi new command" data-type="indexterm" id="idm46691321425384"/><code>pulumi new</code> command:</p>&#13;
&#13;
<pre data-type="programlisting">$ ls -la&#13;
ls -la&#13;
total 40&#13;
drwxr-xr-x  7 ggheo  staff  224 Jul 16 15:08 .&#13;
drwxr-xr-x  6 ggheo  staff  192 Jul 16 15:06 ..&#13;
-rw-------  1 ggheo  staff   12 Jul 16 15:07 .gitignore&#13;
-rw-r--r--  1 ggheo  staff   50 Jul 16 15:08 Pulumi.dev.yaml&#13;
-rw-------  1 ggheo  staff  107 Jul 16 15:07 Pulumi.yaml&#13;
-rw-------  1 ggheo  staff  203 Jul 16 15:07 __main__.py&#13;
-rw-------  1 ggheo  staff   34 Jul 16 15:07 requirements.txt</pre>&#13;
&#13;
<p>We are going to make use of the <code>gcp-py-gke</code> example from the <a href="https://oreil.ly/SIT-v">Pulumi examples</a> GitHub repository.</p>&#13;
&#13;
<p>Copy <em>*.py</em> and <em>requirements.txt</em> from <em>examples/gcp-py-gke</em> to our current directory:</p>&#13;
&#13;
<pre data-type="programlisting">$ cp ~/pulumi-examples/gcp-py-gke/*.py .&#13;
$ cp ~/pulumi-examples/gcp-py-gke/requirements.txt .</pre>&#13;
&#13;
<p>Configure GCP-related variables needed for Pulumi to operate in GCP:</p>&#13;
&#13;
<pre data-type="programlisting">$ pulumi config set gcp:project pythonfordevops-gke-pulumi&#13;
$ pulumi config set gcp:zone us-west1-a&#13;
$ pulumi config set password --secret PASS_FOR_KUBE_CLUSTER</pre>&#13;
&#13;
<p>Create and use a Python <code>virtualenv</code>, install the dependencies declared in <em>requirements.txt</em>, and then bring up the GKE cluster defined in <em>mainpy</em> by running the <code>pulumi up</code> command:</p>&#13;
&#13;
<pre data-type="programlisting">$ virtualenv -p python3 venv&#13;
$ source venv/bin/activate&#13;
$ pip3 install -r requirements.txt&#13;
$ pulumi up</pre>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>Make sure you enable the Kubernetes Engine API by associating it with a Google billing account in the GCP web console.</p>&#13;
</div>&#13;
&#13;
<p>The GKE cluster can now be seen in the <a href="https://oreil.ly/Su5FZ">GCP console</a>.</p>&#13;
&#13;
<p>To interact with the newly provisioned GKE cluster, generate the proper <code>kubectl</code> configuration and use it. Handily, the <code>kubectl</code> configuration is being exported as <code>output</code> by the Pulumi program:</p>&#13;
&#13;
<pre data-type="programlisting">$ pulumi stack output kubeconfig &gt; kubeconfig.yaml&#13;
$ export KUBECONFIG=./kubeconfig.yaml</pre>&#13;
&#13;
<p>List the nodes comprising the GKE cluster:<a data-startref="ix_ch12-asciidoc7" data-type="indexterm" id="idm46691321408808"/><a data-startref="ix_ch12-asciidoc6" data-type="indexterm" id="idm46691321408104"/><a data-startref="ix_ch12-asciidoc5" data-type="indexterm" id="idm46691321407432"/><a data-startref="ix_ch12-asciidoc4" data-type="indexterm" id="idm46691321406760"/><a data-startref="ix_ch12-asciidoc3" data-type="indexterm" id="idm46691321406088"/></p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl get nodes&#13;
NAME                                                 STATUS   ROLES    AGE&#13;
   VERSION&#13;
gke-gke-cluster-ea17e87-default-pool-fd130152-30p3   Ready    &lt;none&gt;   4m29s&#13;
   v1.13.7-gke.8&#13;
gke-gke-cluster-ea17e87-default-pool-fd130152-kf9k   Ready    &lt;none&gt;   4m29s&#13;
   v1.13.7-gke.8&#13;
gke-gke-cluster-ea17e87-default-pool-fd130152-x9dx   Ready    &lt;none&gt;   4m27s&#13;
   v1.13.7-gke.8</pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Deploying the Flask Example Application to GKE" data-type="sect1"><div class="sect1" id="idm46691321602056">&#13;
<h1>Deploying the Flask Example Application to GKE</h1>&#13;
&#13;
<p><a data-primary="GKE Kubernetes cluster" data-secondary="deploying flask example application to" data-type="indexterm" id="ix_ch12-asciidoc8"/><a data-primary="Kubernetes" data-secondary="deploying flask example application to GKE" data-type="indexterm" id="ix_ch12-asciidoc9"/>Take the same Kubernetes manifests used in the <code>minikube</code> example and deploy them to the Kubernetes cluster in GKE, via the <code>kubectl</code> command. Start by creating the <code>redis</code> deployment and service:</p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl create -f redis-deployment.yaml&#13;
deployment.extensions/redis created&#13;
&#13;
$ kubectl get pods&#13;
NAME                             READY   STATUS    RESTARTS   AGE&#13;
canary-aqw8jtfo-f54b9749-q5wqj   1/1     Running   0          5m57s&#13;
redis-9946db5cc-8g6zz            1/1     Running   0          20s&#13;
&#13;
$ kubectl create -f redis-service.yaml&#13;
service/redis created&#13;
&#13;
$ kubectl get service redis&#13;
NAME    TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE&#13;
redis   ClusterIP   10.59.245.221   &lt;none&gt;        6379/TCP   18s</pre>&#13;
&#13;
<p class="pagebreak-before">Create a PersistentVolumeClaim to be used as the data volume for the PostgreSQL database:</p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl create -f dbdata-persistentvolumeclaim.yaml&#13;
persistentvolumeclaim/dbdata created&#13;
&#13;
$ kubectl get pvc&#13;
NAME     STATUS   VOLUME                                     CAPACITY&#13;
dbdata   Bound    pvc-00c8156c-b618-11e9-9e84-42010a8a006f   1Gi&#13;
   ACCESS MODES   STORAGECLASS   AGE&#13;
   RWO            standard       12s</pre>&#13;
&#13;
<p>Create the <code>db</code> deployment:</p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl create -f db-deployment.yaml&#13;
deployment.extensions/db created&#13;
&#13;
$ kubectl get pods&#13;
NAME                             READY   STATUS             RESTARTS  AGE&#13;
canary-aqw8jtfo-f54b9749-q5wqj   1/1     Running            0         8m52s&#13;
db-6b4fbb57d9-cjjxx              0/1     CrashLoopBackOff   1         38s&#13;
redis-9946db5cc-8g6zz            1/1     Running            0         3m15s&#13;
&#13;
$ kubectl logs db-6b4fbb57d9-cjjxx&#13;
&#13;
initdb: directory "/var/lib/postgresql/data" exists but is not empty&#13;
It contains a lost+found directory, perhaps due to it being a mount point.&#13;
Using a mount point directly as the data directory is not recommended.&#13;
Create a subdirectory under the mount point.</pre>&#13;
&#13;
<p>We hit a snag when trying to create the <code>db</code> deployment. GKE provisioned a persistent volume that was mounted as <em>/var/lib/postgresql/data</em>, and according to the error message above, was not empty.</p>&#13;
&#13;
<p>Delete the failed <code>db</code> deployment:</p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl delete -f db-deployment.yaml&#13;
deployment.extensions "db" deleted</pre>&#13;
&#13;
<p>Create a new temporary pod used to mount the same <code>dbdata</code> PersistentVolumeClaim as <em>/data</em> inside the pod, so its filesystem can be inspected. Launching this type of temporary pod for troubleshooting purposes is a useful technique to know about:</p>&#13;
&#13;
<pre data-type="programlisting">$ cat pvc-inspect.yaml&#13;
kind: Pod&#13;
apiVersion: v1&#13;
metadata:&#13;
  name: pvc-inspect&#13;
spec:&#13;
  volumes:&#13;
    - name: dbdata&#13;
      persistentVolumeClaim:&#13;
        claimName: dbdata&#13;
  containers:&#13;
    - name: debugger&#13;
      image: busybox&#13;
      command: ['sleep', '3600']&#13;
      volumeMounts:&#13;
        - mountPath: "/data"&#13;
          name: dbdata&#13;
&#13;
$ kubectl create -f pvc-inspect.yaml&#13;
pod/pvc-inspect created&#13;
&#13;
$ kubectl get pods&#13;
NAME                             READY   STATUS    RESTARTS   AGE&#13;
canary-aqw8jtfo-f54b9749-q5wqj   1/1     Running   0          20m&#13;
pvc-inspect                      1/1     Running   0          35s&#13;
redis-9946db5cc-8g6zz            1/1     Running   0          14m</pre>&#13;
&#13;
<p>Use <code>kubectl exec</code> to open a shell inside the pod so <em>/data</em> can be inspected:</p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl exec -it pvc-inspect -- sh&#13;
/ # cd /data&#13;
/data # ls -la&#13;
total 24&#13;
drwx------    3 999      root          4096 Aug  3 17:57 .&#13;
drwxr-xr-x    1 root     root          4096 Aug  3 18:08 ..&#13;
drwx------    2 999      root         16384 Aug  3 17:57 lost+found&#13;
/data # rm -rf lost\+found/&#13;
/data # exit</pre>&#13;
&#13;
<p>Note how <em>/data</em> contained a directory called <em>lost+found</em> that needed to be removed.</p>&#13;
&#13;
<p>Delete the temporary pod:</p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl delete pod pvc-inspect&#13;
pod "pvc-inspect" deleted</pre>&#13;
&#13;
<p>Create the <code>db</code> deployment again, which completes successfully this time:</p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl create -f db-deployment.yaml&#13;
deployment.extensions/db created&#13;
&#13;
$ kubectl get pods&#13;
NAME                             READY   STATUS    RESTARTS   AGE&#13;
canary-aqw8jtfo-f54b9749-q5wqj   1/1     Running   0          23m&#13;
db-6b4fbb57d9-8h978              1/1     Running   0          19s&#13;
redis-9946db5cc-8g6zz            1/1     Running   0          17m&#13;
&#13;
$ kubectl logs db-6b4fbb57d9-8h978&#13;
PostgreSQL init process complete; ready for start up.&#13;
&#13;
2019-08-03 18:12:01.108 UTC [1]&#13;
LOG:  listening on IPv4 address "0.0.0.0", port 5432&#13;
2019-08-03 18:12:01.108 UTC [1]&#13;
LOG:  listening on IPv6 address "::", port 5432&#13;
2019-08-03 18:12:01.114 UTC [1]&#13;
LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"&#13;
2019-08-03 18:12:01.135 UTC [50]&#13;
LOG:  database system was shut down at 2019-08-03 18:12:01 UTC&#13;
2019-08-03 18:12:01.141 UTC [1]&#13;
LOG:  database system is ready to accept connections</pre>&#13;
&#13;
<p>Create <code>wordcount</code> database and role:</p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl exec -it db-6b4fbb57d9-8h978 -- psql -U postgres&#13;
psql (11.4 (Debian 11.4-1.pgdg90+1))&#13;
Type "help" for help.&#13;
&#13;
postgres=# create database wordcount;&#13;
CREATE DATABASE&#13;
postgres=# \q&#13;
&#13;
$ kubectl exec -it db-6b4fbb57d9-8h978 -- psql -U postgres wordcount&#13;
psql (11.4 (Debian 11.4-1.pgdg90+1))&#13;
Type "help" for help.&#13;
&#13;
wordcount=# CREATE ROLE wordcount_dbadmin;&#13;
CREATE ROLE&#13;
wordcount=# ALTER ROLE wordcount_dbadmin LOGIN;&#13;
ALTER ROLE&#13;
wordcount=# ALTER USER wordcount_dbadmin PASSWORD 'MYNEWPASS';&#13;
ALTER ROLE&#13;
wordcount=# \q</pre>&#13;
&#13;
<p>Create the <code>db</code> service:</p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl create -f db-service.yaml&#13;
service/db created</pre>&#13;
&#13;
<pre data-type="programlisting">$ kubectl describe service db&#13;
Name:              db&#13;
Namespace:         default&#13;
Labels:            io.kompose.service=db&#13;
Annotations:       kompose.cmd: kompose convert&#13;
                   kompose.version: 1.16.0 (0c01309)&#13;
Selector:          io.kompose.service=db&#13;
Type:              ClusterIP&#13;
IP:                10.59.241.181&#13;
Port:              5432  5432/TCP&#13;
TargetPort:        5432/TCP&#13;
Endpoints:         10.56.2.5:5432&#13;
Session Affinity:  None&#13;
Events:            &lt;none&gt;</pre>&#13;
&#13;
<p>Create the Secret object based on the <code>base64</code> value of the database password. The plain-text value for the password is stored in a file encrypted with <code>sops</code>:</p>&#13;
&#13;
<pre data-type="programlisting">$ echo MYNEWPASS | base64&#13;
MYNEWPASSBASE64&#13;
&#13;
$ sops secrets.yaml.enc&#13;
&#13;
apiVersion: v1&#13;
kind: Secret&#13;
metadata:&#13;
  name: fbe-secret&#13;
type: Opaque&#13;
data:&#13;
  dbpass: MYNEWPASSBASE64&#13;
&#13;
$ sops -d secrets.yaml.enc | kubectl create -f -&#13;
secret/fbe-secret created&#13;
&#13;
kubectl describe secret fbe-secret&#13;
Name:         fbe-secret&#13;
Namespace:    default&#13;
Labels:       &lt;none&gt;&#13;
Annotations:  &lt;none&gt;&#13;
&#13;
Type:  Opaque&#13;
&#13;
Data&#13;
===&#13;
dbpass:  21 bytes</pre>&#13;
&#13;
<p>Create another Secret object representing the Docker Hub credentials:</p>&#13;
&#13;
<pre data-type="programlisting">$ sops -d create_docker_credentials_secret.sh.enc | bash -&#13;
secret/myregistrykey created</pre>&#13;
&#13;
<p>Since the scenario under consideration is a production-type deployment of the appication to GKE, set <code>replicas</code> to <code>3</code> in <em>worker-deployment.yaml</em> to ensure that three worker pods are running at all times:</p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl create -f worker-deployment.yaml&#13;
deployment.extensions/worker created</pre>&#13;
&#13;
<p>Make sure that three worker pods are running:</p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl get pods&#13;
NAME                             READY   STATUS    RESTARTS   AGE&#13;
canary-aqw8jtfo-f54b9749-q5wqj   1/1     Running   0          39m&#13;
db-6b4fbb57d9-8h978              1/1     Running   0          16m&#13;
redis-9946db5cc-8g6zz            1/1     Running   0          34m&#13;
worker-8cf5dc699-98z99           1/1     Running   0          35s&#13;
worker-8cf5dc699-9s26v           1/1     Running   0          35s&#13;
worker-8cf5dc699-v6ckr           1/1     Running   0          35s&#13;
&#13;
$ kubectl logs worker-8cf5dc699-98z99&#13;
18:28:08 RQ worker 'rq:worker:1355d2cad49646e4953c6b4d978571f1' started,&#13;
 version 1.0&#13;
18:28:08 *** Listening on default...</pre>&#13;
&#13;
<p class="pagebreak-before">Similarly, set <code>replicas</code> to two in <em>app-deployment.yaml</em>:</p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl create -f app-deployment.yaml&#13;
deployment.extensions/app created</pre>&#13;
&#13;
<p>Make sure that two app pods are running:</p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl get pods&#13;
NAME                             READY   STATUS    RESTARTS   AGE&#13;
app-7964cff98f-5bx4s             1/1     Running   0          54s&#13;
app-7964cff98f-8n8hk             1/1     Running   0          54s&#13;
canary-aqw8jtfo-f54b9749-q5wqj   1/1     Running   0          41m&#13;
db-6b4fbb57d9-8h978              1/1     Running   0          19m&#13;
redis-9946db5cc-8g6zz            1/1     Running   0          36m&#13;
worker-8cf5dc699-98z99           1/1     Running   0          2m44s&#13;
worker-8cf5dc699-9s26v           1/1     Running   0          2m44s&#13;
worker-8cf5dc699-v6ckr           1/1     Running   0          2m44s</pre>&#13;
&#13;
<p>Create the <code>app</code> service:</p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl create -f app-service.yaml&#13;
service/app created</pre>&#13;
&#13;
<p>Note that a service of type LoadBalancer was created:</p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl describe service app&#13;
Name:                     app&#13;
Namespace:                default&#13;
Labels:                   io.kompose.service=app&#13;
Annotations:              kompose.cmd: kompose convert&#13;
                          kompose.version: 1.16.0 (0c01309)&#13;
Selector:                 io.kompose.service=app&#13;
Type:                     LoadBalancer&#13;
IP:                       10.59.255.31&#13;
LoadBalancer Ingress:     34.83.242.171&#13;
Port:                     5000  5000/TCP&#13;
TargetPort:               5000/TCP&#13;
NodePort:                 5000  31305/TCP&#13;
Endpoints:                10.56.1.6:5000,10.56.2.12:5000&#13;
Session Affinity:         None&#13;
External Traffic Policy:  Cluster&#13;
Events:&#13;
Type    Reason                Age   From                Message&#13;
----    ------                ----  ----                -------&#13;
Normal  EnsuringLoadBalancer  72s   service-controller  Ensuring load balancer&#13;
Normal  EnsuredLoadBalancer   33s   service-controller  Ensured load balancer</pre>&#13;
&#13;
<p>Test the application by accessing the endpoint URL based on the IP address corresponding to <code>LoadBalancer Ingress</code>: <a href="http://34.83.242.171:5000"><em class="hyperlink">http://34.83.242.171:5000</em></a>.</p>&#13;
&#13;
<p>We have demonstrated how to create Kubernetes objects such as Deployments, Services, and Secrets from raw Kubernetes manifest files. As your application becomes more complicated, this approach will start showing its limitations, because it will get harder to customize these files per environment (for example, for staging versus integration versus production). Each environment will have its own set of environment values and secrets that you will need to keep track of. In general, it will become more and more complicated to keep track of which manifests have been installed at a given time. Many solutions to this problem exist in the Kubernetes ecosystem, and one of the most common ones is to use the <a href="https://oreil.ly/duKVw">Helm</a> package manager. Think of Helm as the Kubernetes equivalent of the <code>yum</code> and <code>apt</code> package managers.</p>&#13;
&#13;
<p>The next section shows how to use Helm to install and customize Prometheus and Grafana inside the GKE cluster.<a data-startref="ix_ch12-asciidoc9" data-type="indexterm" id="idm46691321352312"/><a data-startref="ix_ch12-asciidoc8" data-type="indexterm" id="idm46691321351608"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Installing Prometheus and Grafana Helm Charts" data-type="sect1"><div class="sect1" id="idm46691321402808">&#13;
<h1>Installing Prometheus and Grafana Helm Charts</h1>&#13;
&#13;
<p><a data-primary="Grafana Helm charts" data-type="indexterm" id="ix_ch12-asciidoc10"/><a data-primary="Kubernetes" data-secondary="installing Prometheus and Grafana Helm charts" data-type="indexterm" id="ix_ch12-asciidoc11"/><a data-primary="Prometheus" data-type="indexterm" id="ix_ch12-asciidoc12"/>In its current version (v2 as of this writing), Helm has a server-side component called Tiller that needs certain permissions inside the Kubernetes cluster.</p>&#13;
&#13;
<p>Create a new Kubernetes Service Account for Tiller and give it the proper <span class="keep-together">permissions:</span></p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl -n kube-system create sa tiller&#13;
&#13;
$ kubectl create clusterrolebinding tiller \&#13;
  --clusterrole cluster-admin \&#13;
  --serviceaccount=kube-system:tiller&#13;
&#13;
$ kubectl patch deploy --namespace kube-system \&#13;
tiller-deploy -p  '{"spec":{"template":{"spec":{"serviceAccount":"tiller"}}}}'</pre>&#13;
&#13;
<p>Download and install the Helm binary for your operating system from the official <a href="https://oreil.ly/sPwDO">Helm release</a> page, and then install Tiller with the <code>helm init</code> command:</p>&#13;
&#13;
<pre data-type="programlisting">$ helm init</pre>&#13;
&#13;
<p>Create a namespace called <code>monitoring</code>:</p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl create namespace monitoring&#13;
namespace/monitoring created</pre>&#13;
&#13;
<p>Install the Prometheus <a href="https://oreil.ly/CSaSo">Helm chart</a> in the <code>monitoring</code> <span class="keep-together">namespace:</span></p>&#13;
&#13;
<pre data-type="programlisting">$ helm install --name prometheus --namespace monitoring stable/prometheus&#13;
NAME:   prometheus&#13;
LAST DEPLOYED: Tue Aug 27 12:59:40 2019&#13;
NAMESPACE: monitoring&#13;
STATUS: DEPLOYED</pre>&#13;
&#13;
<p class="pagebreak-before">List pods, services, and configmaps in the <code>monitoring</code> <span class="keep-together">namespace:</span></p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl get pods -nmonitoring&#13;
NAME                                             READY   STATUS    RESTARTS AGE&#13;
prometheus-alertmanager-df57f6df6-4b8lv          2/2     Running   0        3m&#13;
prometheus-kube-state-metrics-564564f799-t6qdm   1/1     Running   0        3m&#13;
prometheus-node-exporter-b4sb9                   1/1     Running   0        3m&#13;
prometheus-node-exporter-n4z2g                   1/1     Running   0        3m&#13;
prometheus-node-exporter-w7hn7                   1/1     Running   0        3m&#13;
prometheus-pushgateway-56b65bcf5f-whx5t          1/1     Running   0        3m&#13;
prometheus-server-7555945646-d86gn               2/2     Running   0        3m&#13;
&#13;
$ kubectl get services -nmonitoring&#13;
NAME                            TYPE        CLUSTER-IP    EXTERNAL-IP  PORT(S)&#13;
   AGE&#13;
prometheus-alertmanager         ClusterIP   10.0.6.98     &lt;none&gt;       80/TCP&#13;
   3m51s&#13;
prometheus-kube-state-metrics   ClusterIP   None          &lt;none&gt;       80/TCP&#13;
   3m51s&#13;
prometheus-node-exporter        ClusterIP   None          &lt;none&gt;       9100/TCP&#13;
   3m51s&#13;
prometheus-pushgateway          ClusterIP   10.0.13.216   &lt;none&gt;       9091/TCP&#13;
   3m51s&#13;
prometheus-server               ClusterIP   10.0.4.74     &lt;none&gt;       80/TCP&#13;
   3m51s&#13;
&#13;
$ kubectl get configmaps -nmonitoring&#13;
NAME                      DATA   AGE&#13;
prometheus-alertmanager   1      3m58s&#13;
prometheus-server         3      3m58s</pre>&#13;
&#13;
<p>Connect to Prometheus UI via the <code>kubectl port-forward</code> command:</p>&#13;
&#13;
<pre data-type="programlisting">$ export PROMETHEUS_POD_NAME=$(kubectl get pods --namespace monitoring \&#13;
-l "app=prometheus,component=server" -o jsonpath="{.items[0].metadata.name}")&#13;
&#13;
$ echo $PROMETHEUS_POD_NAME&#13;
prometheus-server-7555945646-d86gn&#13;
&#13;
$ kubectl --namespace monitoring port-forward $PROMETHEUS_POD_NAME 9090&#13;
Forwarding from 127.0.0.1:9090 -&gt; 9090&#13;
Forwarding from [::1]:9090 -&gt; 9090&#13;
Handling connection for 9090</pre>&#13;
&#13;
<p>Go to localhost:9090 in a browser and see the Prometheus UI.</p>&#13;
&#13;
<p>Install the Grafana <a href="https://oreil.ly/--wEN">Helm chart</a> in the <code>monitoring</code> namespace:</p>&#13;
&#13;
<pre data-type="programlisting">$ helm install --name grafana --namespace monitoring stable/grafana&#13;
NAME:   grafana&#13;
LAST DEPLOYED: Tue Aug 27 13:10:02 2019&#13;
NAMESPACE: monitoring&#13;
STATUS: DEPLOYED</pre>&#13;
&#13;
<p>List Grafana-related pods, services, configmaps, and secrets in the <code>monitoring</code> <span class="keep-together">namespace:</span></p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl get pods -nmonitoring | grep grafana&#13;
grafana-84b887cf4d-wplcr                         1/1     Running   0&#13;
&#13;
$ kubectl get services -nmonitoring | grep grafana&#13;
grafana                         ClusterIP   10.0.5.154    &lt;none&gt;        80/TCP&#13;
&#13;
$ kubectl get configmaps -nmonitoring | grep grafana&#13;
grafana                   1      99s&#13;
grafana-test              1      99s&#13;
&#13;
$ kubectl get secrets -nmonitoring | grep grafana&#13;
grafana                                     Opaque&#13;
grafana-test-token-85x4x                    kubernetes.io/service-account-token&#13;
grafana-token-jw2qg                         kubernetes.io/service-account-token</pre>&#13;
&#13;
<p>Retrieve the password for the <code>admin</code> user for the Grafana web UI:</p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl get secret --namespace monitoring grafana \&#13;
-o jsonpath="{.data.admin-password}" | base64 --decode ; echo&#13;
&#13;
SOMESECRETTEXT</pre>&#13;
&#13;
<p>Connect to the Grafana UI using the <a data-primary="kubectl port-forward command" data-type="indexterm" id="idm46691321322904"/><code>kubectl port-forward</code> command:</p>&#13;
&#13;
<pre data-type="programlisting">$ export GRAFANA_POD_NAME=$(kubectl get pods --namespace monitoring \&#13;
-l "app=grafana,release=grafana" -o jsonpath="{.items[0].metadata.name}")&#13;
&#13;
$ kubectl --namespace monitoring port-forward $GRAFANA_POD_NAME 3000&#13;
Forwarding from 127.0.0.1:3000 -&gt; 3000&#13;
Forwarding from [::1]:3000 -&gt; 3000</pre>&#13;
&#13;
<p>Go to localhost:3000 in a browser and see the Grafana UI. Log in as user admin with the password retrieved above.</p>&#13;
&#13;
<p>List the charts currently installed with <code>helm list</code>. When a chart is installed, the current installation is called a “Helm release”:</p>&#13;
<pre data-type="programlisting">&#13;
$ helm list&#13;
NAME        REVISION  UPDATED                   STATUS    CHART&#13;
    APP VERSION NAMESPACE&#13;
grafana     1         Tue Aug 27 13:10:02 2019  DEPLOYED  grafana-3.8.3&#13;
    6.2.5       monitoring&#13;
prometheus. 1         Tue Aug 27 12:59:40 2019  DEPLOYED  prometheus-9.1.0&#13;
    2.11.1      monitoring&#13;
</pre>&#13;
&#13;
<p>Most of the time you will need to customize a Helm chart. It is easier to do that if you download the chart and install it from the local filesystem with <code>helm</code>.</p>&#13;
&#13;
<p>Get the latest stable Prometheus and Grafana Helm charts with the <code>helm fetch</code> command, which will download <code>tgz</code> archives of the charts:</p>&#13;
&#13;
<pre data-type="programlisting">$ mkdir charts&#13;
$ cd charts&#13;
$ helm fetch stable/prometheus&#13;
$ helm fetch stable/grafana&#13;
$ ls -la&#13;
total 80&#13;
drwxr-xr-x   4 ggheo  staff    128 Aug 27 13:59 .&#13;
drwxr-xr-x  15 ggheo  staff    480 Aug 27 13:55 ..&#13;
-rw-r--r--   1 ggheo  staff  16195 Aug 27 13:55 grafana-3.8.3.tgz&#13;
-rw-r--r--   1 ggheo  staff  23481 Aug 27 13:54 prometheus-9.1.0.tgz</pre>&#13;
&#13;
<p>Unarchive the <code>tgz</code> files, then remove them:</p>&#13;
&#13;
<pre data-type="programlisting">$ tar xfz prometheus-9.1.0.tgz; rm prometheus-9.1.0.tgz&#13;
$ tar xfz grafana-3.8.3.tgz; rm grafana-3.8.3.tgz</pre>&#13;
&#13;
<p>The templatized Kubernetes manifests are stored by default in a directory called <em>templates</em> under the chart directory, so in this case these locations would be <em>prometheus/templates</em> and <em>grafana/templates</em>. The configuration values for a given chart are declared in the <em>values.yaml</em> file in the chart directory.</p>&#13;
&#13;
<p>As an example of a Helm chart customization, let’s add a persistent volume to Grafana, so we don’t lose the data when we restart the Grafana pods.</p>&#13;
&#13;
<p>Modify the file <em>grafana/values.yaml</em> and set the the value of the <code>enabled</code> subkey under the <code>persistence</code> parent key to <code>true</code> (by default it is <code>false</code>) in this section:</p>&#13;
&#13;
<pre data-type="programlisting">## Enable persistence using Persistent Volume Claims&#13;
## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/&#13;
##&#13;
persistence:&#13;
  enabled: true&#13;
  # storageClassName: default&#13;
  accessModes:&#13;
    - ReadWriteOnce&#13;
  size: 10Gi&#13;
  # annotations: {}&#13;
  finalizers:&#13;
    - kubernetes.io/pvc-protection&#13;
  # subPath: ""&#13;
  # existingClaim:</pre>&#13;
&#13;
<p>Upgrade the existing <code>grafana</code> Helm release with the <code>helm upgrade</code> command. The last argument of the command is the name of the local directory containing the chart. Run this command in the parent directory of the <em>grafana</em> chart directory:</p>&#13;
&#13;
<pre data-type="programlisting">$ helm upgrade grafana grafana/&#13;
Release "grafana" has been upgraded. Happy Helming!</pre>&#13;
&#13;
<p>Verify that a PVC has been created for Grafana in the <code>monitoring</code> namespace:</p>&#13;
&#13;
<pre data-type="programlisting">kubectl describe pvc grafana -nmonitoring&#13;
Name:        grafana&#13;
Namespace:   monitoring&#13;
StorageClass:standard&#13;
Status:      Bound&#13;
Volume:      pvc-31d47393-c910-11e9-87c5-42010a8a0021&#13;
Labels:      app=grafana&#13;
             chart=grafana-3.8.3&#13;
             heritage=Tiller&#13;
             release=grafana&#13;
Annotations: pv.kubernetes.io/bind-completed: yes&#13;
             pv.kubernetes.io/bound-by-controller: yes&#13;
             volume.beta.kubernetes.io/storage-provisioner:kubernetes.io/gce-pd&#13;
Finalizers:  [kubernetes.io/pvc-protection]&#13;
Capacity:    10Gi&#13;
Access Modes:RWO&#13;
Mounted By:  grafana-84f79d5c45-zlqz8&#13;
Events:&#13;
Type    Reason                 Age   From                         Message&#13;
----    ------                 ----  ----                         -------&#13;
Normal  ProvisioningSucceeded  88s   persistentvolume-controller  Successfully&#13;
provisioned volume pvc-31d47393-c910-11e9-87c5-42010a8a0021&#13;
using kubernetes.io/gce-pd</pre>&#13;
&#13;
<p>Another example of a Helm chart customization, this time for Prometheus, is modifying the default retention period of 15 days for the data stored in Prometheus.</p>&#13;
&#13;
<p>Change the <code>retention</code> value in <em>prometheus/values.yaml</em> to 30 days:</p>&#13;
&#13;
<pre data-type="programlisting">  ## Prometheus data retention period (default if not specified is 15 days)&#13;
  ##&#13;
  retention: "30d"</pre>&#13;
&#13;
<p>Upgrade the existing Prometheus Helm release by running <code>helm upgrade</code>. Run this command in the parent directory of the <em>prometheus</em> chart directory:</p>&#13;
&#13;
<pre data-type="programlisting">$ helm upgrade prometheus prometheus&#13;
Release "prometheus" has been upgraded. Happy Helming!</pre>&#13;
&#13;
<p>Verify that the retention period was changed to 30 days. Run <code>kubectl describe</code> against the running Prometheus pod in the <code>monitoring</code> namespace and look at the <code>Args</code> section of the output:<a data-startref="ix_ch12-asciidoc12" data-type="indexterm" id="idm46691321294280"/><a data-startref="ix_ch12-asciidoc11" data-type="indexterm" id="idm46691321293544"/><a data-startref="ix_ch12-asciidoc10" data-type="indexterm" id="idm46691321292872"/></p>&#13;
&#13;
<pre data-type="programlisting">$ kubectl get pods -nmonitoring&#13;
NAME                                            READY   STATUS   RESTARTS   AGE&#13;
grafana-84f79d5c45-zlqz8                        1/1     Running  0          9m&#13;
prometheus-alertmanager-df57f6df6-4b8lv         2/2     Running  0          87m&#13;
prometheus-kube-state-metrics-564564f799-t6qdm  1/1     Running  0          87m&#13;
prometheus-node-exporter-b4sb9                  1/1     Running  0          87m&#13;
prometheus-node-exporter-n4z2g                  1/1     Running  0          87m&#13;
prometheus-node-exporter-w7hn7                  1/1     Running  0          87m&#13;
prometheus-pushgateway-56b65bcf5f-whx5t         1/1     Running  0          87m&#13;
prometheus-server-779ffd445f-4llqr              2/2     Running  0          3m&#13;
&#13;
$ kubectl describe pod prometheus-server-779ffd445f-4llqr -nmonitoring&#13;
OUTPUT OMITTED&#13;
      Args:&#13;
      --storage.tsdb.retention.time=30d&#13;
      --config.file=/etc/config/prometheus.yml&#13;
      --storage.tsdb.path=/data&#13;
      --web.console.libraries=/etc/prometheus/console_libraries&#13;
      --web.console.templates=/etc/prometheus/consoles&#13;
      --web.enable-lifecycle</pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Destroying the GKE Cluster" data-type="sect1"><div class="sect1" id="idm46691321349384">&#13;
<h1>Destroying the GKE Cluster</h1>&#13;
&#13;
<p><a data-primary="GKE Kubernetes cluster" data-secondary="destroying" data-type="indexterm" id="idm46691321290152"/><a data-primary="Kubernetes" data-secondary="destroying GKE cluster" data-type="indexterm" id="idm46691321289240"/>It pays (literally) to remember to delete any cloud resources you’ve been using for testing purposes if you do not need them anymore. Otherwise, you may have an unpleasant surprise when you receive the billing statement from your&#13;
cloud provider at the end of the month.</p>&#13;
&#13;
<p>Destroy the GKE cluster via <a data-primary="pulumi destroy" data-type="indexterm" id="idm46691321287448"/><code>pulumi destroy</code>:</p>&#13;
&#13;
<pre data-type="programlisting">$ pulumi destroy&#13;
&#13;
Previewing destroy (dev):&#13;
&#13;
     Type                            Name                            Plan&#13;
 -   pulumi:pulumi:Stack             pythonfordevops-gke-pulumi-dev  delete&#13;
 -   ├─ kubernetes:core:Service      ingress                         delete&#13;
 -   ├─ kubernetes:apps:Deployment   canary                          delete&#13;
 -   ├─ pulumi:providers:kubernetes  gke_k8s                         delete&#13;
 -   ├─ gcp:container:Cluster        gke-cluster                     delete&#13;
 -   └─ random:index:RandomString    password                        delete&#13;
&#13;
Resources:&#13;
    - 6 to delete&#13;
&#13;
Do you want to perform this destroy? yes&#13;
Destroying (dev):&#13;
&#13;
     Type                            Name                            Status&#13;
 -   pulumi:pulumi:Stack             pythonfordevops-gke-pulumi-dev  deleted&#13;
 -   ├─ kubernetes:core:Service      ingress                         deleted&#13;
 -   ├─ kubernetes:apps:Deployment   canary                          deleted&#13;
 -   ├─ pulumi:providers:kubernetes  gke_k8s                         deleted&#13;
 -   ├─ gcp:container:Cluster        gke-cluster                     deleted&#13;
 -   └─ random:index:RandomString    password                        deleted&#13;
&#13;
Resources:&#13;
    - 6 deleted&#13;
&#13;
Duration: 3m18s</pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="pagebreak-before less_space" data-pdf-bookmark="Exercises" data-type="sect1"><div class="sect1" id="idm46691321283976">&#13;
<h1>Exercises</h1>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Use Google Cloud SQL for PostgreSQL, instead of running PostgreSQL in a Docker container in GKE.</p>&#13;
</li>&#13;
<li>&#13;
<p>Use the <a href="https://aws.amazon.com/cdk">AWS Cloud Development Kit</a> to launch an Amazon EKS cluster, and deploy the example application to that cluster.</p>&#13;
</li>&#13;
<li>&#13;
<p>Use Amazon RDS PostgreSQL instead of running PostgreSQL in a Docker container in EKS.</p>&#13;
</li>&#13;
<li>&#13;
<p>Experiment with <a href="https://oreil.ly/ie9n6">Kustomize</a> as an alternative to Helm for managing Kubernetes manifest YAML files.<a data-startref="ix_ch12-asciidoc0" data-type="indexterm" id="idm46691321277144"/></p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>