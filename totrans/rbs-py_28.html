<html><head></head><body>
<section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 24. Mutation Testing" class="preface"><div class="preface" id="mutation">
<h1 class="calibre10" id="calibre_pb_0"><span class="calibre">Chapter 24. </span>Mutation Testing</h1>


<p class="author1">When weaving your safety net of static analysis and tests, how do you know that you are testing as much as you can?<a data-type="indexterm" data-primary="testing" data-secondary="mutation" id="ix_tstmut" class="calibre5"/><a data-type="indexterm" data-primary="mutation testing" id="ix_muttst" class="calibre5"/> Testing absolutely everything is impossible; you need to be smart in what tests you write. Envision each test as a separate strand in your safety net: the more tests you have, the wider your net. However, this doesn’t inherently mean that your net is well-constructed. A safety net with fraying, brittle strands is worse than no safety net at all; it gives the illusion of safety and provides false <span class="calibre">confidence</span>.</p>

<p class="author1">The goal is to strengthen your safety net so that it is not brittle. You need a way to make sure your tests will actually fail when there are bugs in your code. In this chapter, you will learn how to do just that with mutation testing. <a data-type="indexterm" data-primary="mutmut tool" id="idm45644723131960" class="calibre5"/>You’ll learn how to perform mutation testing with a Python tool called <code class="calibre17">mutmut</code>. You’ll use mutation testing to inspect the relation between your tests and code. Finally, you’ll learn about code coverage tools, how best to use those tools, and how to integrate <code class="calibre17">mutmut</code> with your coverage reports. Learning how to do mutation testing will give you a way to measure how effective your tests are.</p>






</div></section>

<section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 24. Mutation Testing" class="preface">
<div class="preface" id="mutation">
<section data-type="sect1" data-pdf-bookmark="What Is Mutation Testing?" class="preface"><div class="preface" id="idm45644723129848">
<h1 class="calibre12" id="calibre_pb_1">What Is Mutation Testing?</h1>

<p class="author1"><em class="calibre6">Mutation testing</em> is the act<a data-type="indexterm" data-primary="mutation testing" data-secondary="about" id="ix_muttstabt" class="calibre5"/> of making changes in your source code with the intent of introducing bugs.<sup class="calibre11"><a data-type="noteref" id="idm45644723126456-marker" href="part0030_split_007.html#idm45644723126456" class="calibre5">1</a></sup> Each change you make in this fashion is known as a <em class="calibre6">mutant</em>. You then run your test suite. If the tests fail, it’s good news; your tests were successful in eliminating the mutant. However, if your tests pass, that means your tests are not robust enough to catch legitimate failures; the mutant survives. Mutation testing is a form of <em class="calibre6">meta-testing</em>, in that you are testing how good your tests are. After all, your test code should be a first-class citizen in your codebase; it requires some level of testing as well.</p>

<p class="author1">Consider a simple calorie-tracking app. A user can input a set of meals and get notified if they exceed their calorie budget for the day. The core functionality is implemented in the following function:</p>

<pre data-type="programlisting" data-code-language="python" class="calibre35"><code class="k">def</code> <code class="nf">check_meals_for_calorie_overage</code><code class="calibre17">(</code><code class="n">meals</code><code class="calibre17">:</code> <code class="nb">list</code><code class="calibre17">[</code><code class="n">Meal</code><code class="calibre17">],</code> <code class="n">target</code><code class="calibre17">:</code> <code class="nb">int</code><code class="calibre17">):</code>
    <code class="k">for</code> <code class="n">meal</code> <code class="calibre19">in</code> <code class="n">meals</code><code class="calibre17">:</code>
        <code class="n">target</code> <code class="calibre17">-=</code> <code class="n">meal</code><code class="calibre17">.</code><code class="n">calories</code>
        <code class="k">if</code> <code class="n">target</code> <code class="calibre17">&lt;</code> <code class="mi">0</code><code class="calibre17">:</code>
            <code class="n">display_warning</code><code class="calibre17">(</code><code class="n">meal</code><code class="calibre17">,</code> <code class="n">WarningType</code><code class="calibre17">.</code><code class="n">OVER_CALORIE_LIMIT</code><code class="calibre17">)</code>
            <code class="k">continue</code>
        <code class="n">display_checkmark</code><code class="calibre17">(</code><code class="n">meal</code><code class="calibre17">)</code></pre>

<p class="author1">Here is a set of tests for this functionality, all of which pass:</p>

<pre data-type="programlisting" data-code-language="python" class="calibre35"><code class="k">def</code> <code class="nf">test_no_warnings_if_under_calories</code><code class="calibre17">():</code>
    <code class="n">meals</code> <code class="calibre17">=</code> <code class="calibre17">[</code><code class="n">Meal</code><code class="calibre17">(</code><code class="s">"Fish 'n' Chips"</code><code class="calibre17">,</code> <code class="mi">1000</code><code class="calibre17">)]</code>
    <code class="n">check_meals_for_calorie_overage</code><code class="calibre17">(</code><code class="n">meals</code><code class="calibre17">,</code> <code class="mi">1200</code><code class="calibre17">)</code>
    <code class="n">assert_no_warnings_displayed_on_meal</code><code class="calibre17">(</code><code class="s">"Fish 'n' Chips"</code><code class="calibre17">)</code>
    <code class="n">assert_checkmark_on_meal</code><code class="calibre17">(</code><code class="s">"Fish 'n' Chips"</code><code class="calibre17">)</code>

<code class="k">def</code> <code class="nf">test_no_exception_thrown_if_no_meals</code><code class="calibre17">():</code>
    <code class="n">check_meals_for_calorie_overage</code><code class="calibre17">([],</code> <code class="mi">1200</code><code class="calibre17">)</code>
    <code class="c"># no explicit assert, just checking for no exceptions</code>

<code class="k">def</code> <code class="nf">test_meal_is_marked_as_over_calories</code><code class="calibre17">():</code>
    <code class="n">meals</code> <code class="calibre17">=</code> <code class="calibre17">[</code><code class="n">Meal</code><code class="calibre17">(</code><code class="s">"Fish 'n' Chips"</code><code class="calibre17">,</code> <code class="mi">1000</code><code class="calibre17">)]</code>
    <code class="n">check_meals_for_calorie_overage</code><code class="calibre17">(</code><code class="n">meals</code><code class="calibre17">,</code> <code class="mi">900</code><code class="calibre17">)</code>
    <code class="n">assert_meal_is_over_calories</code><code class="calibre17">(</code><code class="s">"Fish 'n' Chips"</code><code class="calibre17">)</code>

<code class="k">def</code> <code class="nf">test_meal_going_over_calories_does_not_conflict_with_previous_meals</code><code class="calibre17">():</code>
    <code class="n">meals</code> <code class="calibre17">=</code> <code class="calibre17">[</code><code class="n">Meal</code><code class="calibre17">(</code><code class="s">"Fish 'n' Chips"</code><code class="calibre17">,</code> <code class="mi">1000</code><code class="calibre17">),</code> <code class="n">Meal</code><code class="calibre17">(</code><code class="s">"Banana Split"</code><code class="calibre17">,</code> <code class="mi">400</code><code class="calibre17">)]</code>
    <code class="n">check_meals_for_calorie_overage</code><code class="calibre17">(</code><code class="n">meals</code><code class="calibre17">,</code> <code class="mi">1200</code><code class="calibre17">)</code>
    <code class="n">assert_no_warnings_displayed_on_meal</code><code class="calibre17">(</code><code class="s">"Fish 'n' Chips"</code><code class="calibre17">)</code>
    <code class="n">assert_checkmark_on_meal</code><code class="calibre17">(</code><code class="s">"Fish 'n' Chips"</code><code class="calibre17">)</code>
    <code class="n">assert_meal_is_over_calories</code><code class="calibre17">(</code><code class="s">"Banana Split"</code><code class="calibre17">)</code></pre>

<p class="author1">As a thought exercise, I’d like you to look over these tests  (ignoring the fact that this is a chapter about mutation testing) and ask yourself what your opinion would be if you found these tests in production. How confident are you that they are right? How confident are you that I didn’t miss anything? How confident are you that these tests will catch bugs if the code changes?</p>

<p class="author1">The central theme of this book is that software will always change. You need to make it easy for your future collaborators to maintain your codebase in spite of this change. You need to write tests that catch not only errors in what you wrote, but errors other developers make as they change your code.</p>

<p class="author1">It doesn’t matter if a future developer is refactoring the method to use a common library, changing a single line, or adding more functionality to the code; you want your tests to catch any errors that they introduced. To get into the mindset of mutation testing, you need to think about all the possible changes that can be made to the code and check if your tests would catch any erroneous change. <a data-type="xref" href="part0030_split_001.html#manual_mutation" class="calibre5">Table 24-1</a> breaks down the above code line by line and shows the outcome of the tests if that line is missing.</p>
<table id="manual_mutation" class="calibre45">
<caption class="calibre46"><span class="calibre">Table 24-1. </span>Impact of each line removed</caption>
<thead class="calibre47">
<tr class="calibre48">
<th class="calibre49">Code line</th>
<th class="calibre49">Impact if removed</th>
</tr>
</thead>
<tbody class="calibre50">
<tr class="calibre48">
<td class="calibre51"><p class="author1"><code class="calibre52">for meal in meals:</code></p></td>
<td class="calibre51"><p class="author1">Tests fail: Syntax errors and code does no looping</p></td>
</tr>
<tr class="calibre53">
<td class="calibre51"><p class="author1"><code class="calibre52">target -= meal.calories</code></p></td>
<td class="calibre51"><p class="author1">Tests fail: no warnings are ever displayed</p></td>
</tr>
<tr class="calibre48">
<td class="calibre51"><p class="author1"><code class="calibre52">if target &lt; 0</code></p></td>
<td class="calibre51"><p class="author1">Tests fail: all meals show a warning</p></td>
</tr>
<tr class="calibre53">
<td class="calibre51"><p class="author1"><code class="calibre52">display_warning(meal, WarningType.OVER_CALO⁠RIE_LIMIT)</code></p></td>
<td class="calibre51"><p class="author1">Tests fail: no warnings are shown</p></td>
</tr>
<tr class="calibre48">
<td class="calibre51"><p class="author1"><code class="calibre52">continue</code></p></td>
<td class="calibre51"><p class="author1">Tests pass</p></td>
</tr>
<tr class="calibre53">
<td class="calibre51"><p class="author1"><code class="calibre52">display_checkmark(meal)</code></p></td>
<td class="calibre51"><p class="author1">Tests fail: checkmarks are not displayed on meals</p></td>
</tr>
</tbody>
</table>

<p class="author1">Look at the row in <a data-type="xref" href="part0030_split_001.html#manual_mutation" class="calibre5">Table 24-1</a> for the <code class="calibre17">continue</code> statement. If I delete that line, all tests pass. This means one of three scenarios occurred: the line isn’t needed; the line is needed, but not important enough to test; or there is missing coverage in our test suite.</p>

<p class="author1">The first two scenarios are easy to handle. If the line isn’t needed, delete it. If the line isn’t important enough to test (this is common for things such as debug logging statements or version strings), you can ignore mutation testing on this line. But, if the third scenario is true, you are missing test coverage. You have found a hole in your safety net.</p>

<p class="author1">If <code class="calibre17">continue</code> is removed from the algorithm, both a checkmark and a warning will show up on any meal that is over the calorie limit. This is not ideal behavior; this is a signal that I should have a test to cover for this case. If I were to just add an assertion that meals with warnings also have no checkmarks, then our test suite would have caught this mutant.</p>

<p class="author1">Deleting lines is just one example of a mutation. There are numerous other mutants I could apply to the code above. As a matter of fact, if I change the <code class="calibre17">continue</code> to a <code class="calibre17">break</code>, the tests still pass. Going through every mutation I can think of is tedious, so I want an automated tool to do this process for me. Enter <code class="calibre17">mutmut</code>.<a data-type="indexterm" data-primary="mutation testing" data-secondary="about" data-startref="ix_muttstabt" id="idm45644722900024" class="calibre5"/></p>
</div></section>













</div></section>

<section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 24. Mutation Testing" class="preface">
<div class="preface" id="mutation">
<section data-type="sect1" data-pdf-bookmark="Mutation Testing with mutmut" class="preface"><div class="preface" id="idm45644723129224">
<h1 class="calibre12" id="calibre_pb_2">Mutation Testing with mutmut</h1>

<p class="author1"><a href="https://pypi.org/project/mutmut" class="calibre5"><code class="calibre17">mutmut</code></a> is a Python tool that does mutation testing for you.<a data-type="indexterm" data-primary="mutation testing" data-secondary="using mutmut" id="ix_muttstmut" class="calibre5"/> It comes with a pre-programmed set of mutations to apply to your codebase, such as:</p>

<ul class="printings">
<li class="calibre9">
<p class="author1">Finding integer literals and adding 1 to them to catch off-by-one errors</p>
</li>
<li class="calibre9">
<p class="author1">Changing string literals by inserting text inside them</p>
</li>
<li class="calibre9">
<p class="author1">Exchanging <code class="calibre17">break</code> and <code class="calibre17">continue</code></p>
</li>
<li class="calibre9">
<p class="author1">Exchanging <code class="calibre17">True</code> and <code class="calibre17">False</code></p>
</li>
<li class="calibre9">
<p class="author1">Negating expressions, such as converting <code class="calibre17">x is None</code> to <code class="calibre17">x is not None</code></p>
</li>
<li class="calibre9">
<p class="author1">Changing operators (especially changing from <code class="calibre17">/</code> to <code class="calibre17">//</code>)</p>
</li>
</ul>

<p class="author1">This is by no means a comprehensive list; <code class="calibre17">mutmut</code> has quite a few clever ways of mutating your code. It works by making discrete mutations, running your test suite for you, and then displaying which mutants survived the testing process.</p>

<p class="author1">To get started, you need to install <code class="calibre17">mutmut</code>:</p>

<pre data-type="programlisting" class="calibre35">pip install mutmut</pre>

<p class="author1">Then, you run <code class="calibre17">mutmut</code> against all your tests (warning, this can take some time). You can run <code class="calibre17">mutmut</code> on my code snippet above with the following:</p>

<pre data-type="programlisting" class="calibre35">mutmut run --paths-to-mutate code_examples/chapter24</pre>
<div data-type="tip" class="calibre21"><h6 class="calibre22">Tip</h6>
<p class="author1">For long-running tests and large codebases, you may want to break up your <code class="calibre17">mutmut</code> runs, as they do take some time. However, <code class="calibre17">mutmut</code> is intelligent enough to save its progress in a folder called <em class="calibre6">.mutmut-cache</em>, so if you exit in the middle, it will pick up execution at the same point on future runs.</p>
</div>

<p class="author1"><code class="calibre17">mutmut</code> will display some statistics as it runs, including the number of surviving mutants, the number of eliminated mutants, and which tests were taking a suspiciously long time (such as accidentally introducing an infinite loop).</p>

<p class="author1">Once execution completes, you can view the results with <code class="calibre17">mutmut results</code>. In my code snippet, <code class="calibre17">mutmut</code> identifies three surviving mutants. It will list mutants as numeric IDs, and you can show the specific mutant with the <code class="calibre17">mutmut show &lt;id&gt;</code> <span class="calibre">command</span>.</p>

<p class="author1">Here are the three mutants that survived in my code snippet:</p>

<pre data-type="programlisting" class="calibre35">mutmut show 32
--- code_examples/chapter24/calorie_tracker.py
+++ code_examples/chapter24/calorie_tracker.py
@@ -26,7 +26,7 @@
 def check_meals_for_calorie_overage(meals: list[Meal], target: int):
     for meal in meals:
         target -= meal.calories
-        if target &lt; 0:
+        if target &lt;= 0:
             display_warning(meal, WarningType.OVER_CALORIE_LIMIT)
             continue
         display_checkmark(meal)

mutmut show 33
--- code_examples/chapter24/calorie_tracker.py
+++ code_examples/chapter24/calorie_tracker.py
@@ -26,7 +26,7 @@
 def check_meals_for_calorie_overage(meals: list[Meal], target: int):
     for meal in meals:
         target -= meal.calories
-        if target &lt; 0:
+        if target &lt; 1:
             display_warning(meal, WarningType.OVER_CALORIE_LIMIT)
             continue
         display_checkmark(meal)

mutmut show 34
--- code_examples/chapter24/calorie_tracker.py
+++ code_examples/chapter24/calorie_tracker.py
@@ -28,6 +28,6 @@
         target -= meal.calories
         if target &lt; 0:
             display_warning(meal, WarningType.OVER_CALORIE_LIMIT)
-            continue
+            break
         display_checkmark(meal)</pre>

<p class="author1">In each example, <code class="calibre17">mutmut</code> shows the result in <em class="calibre6">diff notation</em>, which is a way of representing the changes of a file from one changeset to another.<a data-type="indexterm" data-primary="diff notation" id="idm45644722870824" class="calibre5"/> In this case, any line prefixed with a minus sign “-” indicates a line that got changed by <code class="calibre17">mutmut</code>. Lines starting with a plus sign “+” are the change that <code class="calibre17">mutmut</code> made; these are your mutants.</p>

<p class="author1">Each of these cases is a potential hole in my testing. By changing <code class="calibre17">&lt;=</code> to <code class="calibre17">&lt;</code>, I find out I don’t have coverage for when the calories of a meal exactly match the target. By changing <code class="calibre17">0</code> to <code class="calibre17">1</code>, I find out that I don’t have coverage at the boundaries of my input domain (refer back to <a data-type="xref" href="part0029_split_000.html#property" class="calibre5">Chapter 23</a> for discussion of boundary value analysis). By changing a <code class="calibre17">continue</code> to a <code class="calibre17">break</code>, I stop the loop early and potentially miss marking later meals as OK.</p>








</div></section>













</div></section>

<section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 24. Mutation Testing" class="preface">
<div class="preface" id="mutation">
<section data-type="sect1" data-pdf-bookmark="Mutation Testing with mutmut" class="preface">
<div class="preface" id="idm45644723129224">
<section data-type="sect2" data-pdf-bookmark="Fixing Mutants" class="preface"><div class="preface" id="idm45644722864936">
<h2 class="calibre34" id="calibre_pb_3">Fixing Mutants</h2>

<p class="author1">Once you identify mutants, it’s time to fix them.<a data-type="indexterm" data-primary="mutation testing" data-secondary="using mutmut" data-tertiary="fixing mutations" id="idm45644722863368" class="calibre5"/> One of the best ways to do so is to apply the mutants to the files you have on disk. In my previous example, my mutants had the numbers 32, 33, and 34. I can apply them to my codebase like so:</p>

<pre data-type="programlisting" class="calibre35">mutmut apply 32
mutmut apply 33
mutmut apply 34</pre>
<div data-type="warning" epub:type="warning" class="calibre23"><h6 class="calibre24">Warning</h6>
<p class="author1">Only do this on files that are backed up through version control. This makes it easy to revert the mutants when you are done, restoring the original code.</p>
</div>

<p class="author1">Once the mutants have been applied to disk, your goal is to write a failing test. For instance, I can write the following:</p>

<pre data-type="programlisting" data-code-language="python" class="calibre35"><code class="k">def</code> <code class="nf">test_failing_mutmut</code><code class="calibre17">():</code>
    <code class="n">clear_warnings</code><code class="calibre17">()</code>
    <code class="n">meals</code> <code class="calibre17">=</code> <code class="calibre17">[</code><code class="n">Meal</code><code class="calibre17">(</code><code class="s">"Fish 'n' Chips"</code><code class="calibre17">,</code> <code class="mi">1000</code><code class="calibre17">),</code>
             <code class="n">Meal</code><code class="calibre17">(</code><code class="s">"Late-Night Cookies"</code><code class="calibre17">,</code> <code class="mi">300</code><code class="calibre17">),</code>
             <code class="n">Meal</code><code class="calibre17">(</code><code class="s">"Banana Split"</code><code class="calibre17">,</code> <code class="mi">400</code><code class="calibre17">)</code>
             <code class="n">Meal</code><code class="calibre17">(</code><code class="s">"Tub of Cookie Dough"</code><code class="calibre17">,</code> <code class="mi">1000</code><code class="calibre17">)]</code>

    <code class="n">check_meals_for_calorie_overage</code><code class="calibre17">(</code><code class="n">meals</code><code class="calibre17">,</code> <code class="mi">1300</code><code class="calibre17">)</code>

    <code class="n">assert_no_warnings_displayed_on_meal</code><code class="calibre17">(</code><code class="s">"Fish 'n' Chips"</code><code class="calibre17">)</code>
    <code class="n">assert_checkmark_on_meal</code><code class="calibre17">(</code><code class="s">"Fish 'n' Chips"</code><code class="calibre17">)</code>
    <code class="n">assert_no_warnings_displayed_on_meal</code><code class="calibre17">(</code><code class="s">"Late-Night Cookies"</code><code class="calibre17">)</code>
    <code class="n">assert_checkmark_on_meal</code><code class="calibre17">(</code><code class="s">"Late-Night Cookies"</code><code class="calibre17">)</code>
    <code class="n">assert_meal_is_over_calories</code><code class="calibre17">(</code><code class="s">"Banana Split"</code><code class="calibre17">)</code>
    <code class="n">assert_meal_is_over_calories</code><code class="calibre17">(</code><code class="s">"Tub of Cookie Dough"</code><code class="calibre17">)</code></pre>

<p class="author1">You should see this test fail (even if you have only one of the mutations applied). Once you are confident you have caught all mutations, revert the mutants and make sure the tests now pass. Rerunning <code class="calibre17">mutmut</code> should show that you eliminated the mutants as well.</p>
</div></section>













</div></section>













</div></section>

<section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 24. Mutation Testing" class="preface">
<div class="preface" id="mutation">
<section data-type="sect1" data-pdf-bookmark="Mutation Testing with mutmut" class="preface">
<div class="preface" id="idm45644723129224">
<section data-type="sect2" data-pdf-bookmark="Mutation Testing Reports" class="preface"><div class="preface" id="idm45644722855784">
<h2 class="calibre34" id="calibre_pb_4">Mutation Testing Reports</h2>

<p class="author1"><code class="calibre17">mutmut</code> also provides a way to export its results to JUnit report format.<a data-type="indexterm" data-primary="reports on mutation testing" id="idm45644722731704" class="calibre5"/><a data-type="indexterm" data-primary="mutation testing" data-secondary="using mutmut" data-tertiary="mutation testing reports" id="idm45644722730936" class="calibre5"/><a data-type="indexterm" data-primary="JUnit" data-secondary="mutation testing reports" id="idm45644722729704" class="calibre5"/> You’ve seen other tools export to JUnit reports already in this book (such as in <a data-type="xref" href="part0028_split_000.html#bdd" class="calibre5">Chapter 22</a>), and <code class="calibre17">mutmut</code> is no different:</p>

<pre data-type="programlisting" class="calibre35">mutmut junitxml &gt; /tmp/test.xml</pre>

<p class="author1">And just like in <a data-type="xref" href="part0028_split_000.html#bdd" class="calibre5">Chapter 22</a>, I can use <code class="calibre17">junit2html</code> to produce a nice HTML report for the mutation tests, as seen in <a data-type="xref" href="part0030_split_004.html#mutation_junit" class="calibre5">Figure 24-1</a>.</p>

<figure class="calibre36"><div id="mutation_junit" class="figure">
<img src="../images/00042.jpeg" alt="ropy 2401" class="calibre40"/>
<h6 class="calibre37"><span class="calibre">Figure 24-1. </span>Example <code class="calibre17">mutmut</code> report with <code class="calibre17">junit2html</code></h6>
</div></figure>
</div></section>





</div></section>













</div></section>

<section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 24. Mutation Testing" class="preface">
<div class="preface" id="mutation">
<section data-type="sect1" class="preface" data-pdf-bookmark="Adopting Mutation Testing"><div class="preface" id="idm45644722898152">
<h1 class="calibre12" id="calibre_pb_5">Adopting Mutation Testing</h1>

<p class="author1">Mutation testing is not widespread in the software development community today.<a data-type="indexterm" data-primary="mutation testing" data-secondary="using mutmut" data-startref="ix_muttstmut" id="idm45644722719240" class="calibre5"/><a data-type="indexterm" data-primary="mutation testing" data-secondary="adopting" id="ix_muttstadp" class="calibre5"/> I believe this to be for three reasons:</p>

<ul class="printings">
<li class="calibre9">
<p class="author1">People are unaware of it and the benefits it provides.</p>
</li>
<li class="calibre9">
<p class="author1">A codebase’s tests are not mature enough yet for useful mutation testing.</p>
</li>
<li class="calibre9">
<p class="author1">The cost-to-value ratio is too high.</p>
</li>
</ul>

<p class="author1">This book is actively working to improve the first point, but the second and third points certainly have merit.</p>

<p class="author1">If your codebase does not have a mature set of tests, you will see little value in introducing mutation testing. It will end up providing too high of a noise-to-signal ratio. You will see much more value from improving your test suite than trying to find all the mutants. Consider running mutation testing on smaller parts of your codebase that do have mature test suites.</p>

<p class="author1">Mutation testing does have a high cost; it’s important to maximize the value received in order to make mutation testing worth it. Mutation tests are slow, by virtue of running test suites multiple times. Introducing mutation testing to an existing codebase is painful, as well. It is far easier to start on brand-new code from the beginning.</p>

<p class="author1">However, since you are reading a book about improving the robustness of potentially complex codebases, there’s a good chance you are working in an existing codebase. Hope is not lost if you’d like to introduce mutation testing, though. As with any method of improving robustness, the trick is to be selective in where you run mutation testing.</p>

<p class="author1">Look for areas of code that have lots of bugs. Look through bug reports and find trends that indicate that a certain area of code is troublesome. Also consider finding areas of code with high churn, as these are the areas that are most likely to introduce a change that current tests do not fully cover.<sup class="calibre11"><a data-type="noteref" id="idm45644722709960-marker" href="part0030_split_007.html#idm45644722709960" class="calibre5">2</a></sup> Find the areas where mutation testing will pay back the cost multifold. You can use <code class="calibre17">mutmut</code> to run mutation testing selectively on just these areas.<a data-type="indexterm" data-primary="mutmut tool" data-secondary="selective mutation testing with" id="idm45644722707112" class="calibre5"/></p>

<p class="author1">Also, <code class="calibre17">mutmut</code> comes with an option to mutation test only the parts of your codebase that have <em class="calibre6">line coverage</em>.<a data-type="indexterm" data-primary="line coverage" id="idm45644722704600" class="calibre5"/> A line of code has <em class="calibre6">coverage</em> by test suite if it is executed at <span class="calibre">least once</span> by any test. Other coverage types exist, such as API coverage and branch <span class="calibre">coverage</span>, but <code class="calibre17">mutmut</code> focuses on line coverage. <code class="calibre17">mutmut</code> will only generate mutants for code that you actually have tests for in the first place.</p>

<p class="author1">To generate coverage, first install <code class="calibre17">coverage</code>:</p>

<pre data-type="programlisting" class="calibre35">pip install coverage</pre>

<p class="author1">Then run your test suite with the <code class="calibre17">coverage</code> command. For the example above, I run:</p>

<pre data-type="programlisting" class="calibre35">coverage run -m pytest code_examples/chapter24</pre>

<p class="author1">Next, all you have to do is pass the <code class="calibre17">--use-coverage</code> flag to your <code class="calibre17">mutmut</code> run:</p>

<pre data-type="programlisting" class="calibre35">mutmut run --paths-to-mutate code_examples/chapter24 --use-coverage</pre>

<p class="author1">With this, <code class="calibre17">mutmut</code> will ignore any untested code, drastically reducing the amount of noise.</p>








</div></section>













</div></section>

<section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 24. Mutation Testing" class="preface">
<div class="preface" id="mutation">
<section data-type="sect1" class="preface" data-pdf-bookmark="Adopting Mutation Testing">
<div class="preface" id="idm45644722898152">
<section data-type="sect2" data-pdf-bookmark="The Fallacy of Coverage (and Other Metrics)" class="preface"><div class="preface" id="idm45644722694760">
<h2 class="calibre34" id="calibre_pb_6">The Fallacy of Coverage (and Other Metrics)</h2>

<p class="author1">Any time a way of measuring code emerges, there is a rush to use that measurement as a <em class="calibre6">metric</em>, or a goal that acts as a proxy predictor of business value. <a data-type="indexterm" data-primary="metrics, fallacy of" id="ix_mtrcs" class="calibre5"/><a data-type="indexterm" data-primary="mutation testing" data-secondary="adopting" data-tertiary="fallacy of coverage and other metrics" id="ix_muttstadpmtrc" class="calibre5"/><a data-type="indexterm" data-primary="code coverage, fallacy of" id="ix_cdcvg" class="calibre5"/>However, there have been numerous ill-advised metrics through software development history, and none more infamous than using lines of code written as an indicator of project progress. The thinking went that if you could directly measure how much code any one person was writing, you would be able to directly measure that person’s productivity. Unfortunately, this led developers to game the system and try to write intentionally verbose code. This backfired as a metric, because the systems ended up complex and bloated, and development slowed due to poor maintainability.</p>

<p class="author1">As an industry, we have moved past measuring lines of code (I hope). However, where one metric disappears, two more come to take its place. I’ve seen other maligned metrics emerge such as number of bugs fixed or number of tests written. At face value, these are good things to be doing, but the problem comes when they are scrutinized as a metric tied to business value. There are ways to manipulate data in each of these metrics. Are you being judged by the number of bugs fixed? Then, just write more bugs in the first place!</p>

<p class="author1">Unfortunately, code coverage has fallen into the same trap in recent years. You hear goals such as “This code should be 100% line covered” or “We should strive for 90% branch coverage.” This is laudable in isolation, but it falls short of predicting business value. It misses the point of <em class="calibre6">why</em> you want these goals in the first place.</p>

<p class="author1">Code coverage is a predictor of the absence of robustness, not quality as many assume. Code with low coverage may or may not do everything you need; you don’t know with any reliability. It is a sign that you will have challenges with modifying the code, as you do not have any sort of safety net built around that part of your system. You should absolutely look for areas with very low coverage and improve the testing story around them.</p>

<p class="author1">Conversely, this causes many people to assume that high coverage is a predictor of robustness, when it really isn’t. You can have every line and every branch covered by tests, and still have abysmal maintainability. The tests could be brittle or even flat-out useless.</p>

<p class="author1">I once worked in a codebase that was beginning to adopt unit testing. I came across a file with the equivalent of the following:</p>

<pre data-type="programlisting" data-code-language="python" class="calibre35"><code class="k">def</code> <code class="nf">test_foo_can_do_something</code><code class="calibre17">():</code>
    <code class="n">foo</code> <code class="calibre17">=</code> <code class="n">Thingamajiggy</code><code class="calibre17">()</code>
    <code class="n">foo</code><code class="calibre17">.</code><code class="n">doSomething</code><code class="calibre17">()</code>
    <code class="k">assert</code> <code class="n">foo</code> <code class="calibre19">is</code> <code class="calibre19">not</code> <code class="nb">None</code>

<code class="k">def</code> <code class="nf">test_foo_parameterized_still_does_the_right_thing</code><code class="calibre17">():</code>
    <code class="n">foo</code> <code class="calibre17">=</code> <code class="n">Thingamajiggy</code><code class="calibre17">(</code><code class="n">y</code><code class="calibre17">=</code><code class="mi">12</code><code class="calibre17">)</code>
    <code class="n">foo</code><code class="calibre17">.</code><code class="n">doSomethingElse</code><code class="calibre17">(</code><code class="mi">15</code><code class="calibre17">)</code>
    <code class="k">assert</code> <code class="n">foo</code> <code class="calibre19">is</code> <code class="calibre19">not</code> <code class="nb">None</code></pre>

<p class="author1">There were<a data-type="indexterm" data-primary="AAA testing" id="idm45644722587240" class="calibre5"/> about 30 of these tests, all with good names and following the AAA pattern (as seen in <a data-type="xref" href="part0027_split_000.html#testing_strategy" class="calibre5">Chapter 21</a>). But they were all effectively useless: all they did was make sure that no exception was thrown. The worst part of all of this was the tests actually had 100% line coverage and near &gt;80% branch coverage. It’s not bad that the tests were checking that no exception was thrown; it was bad that they didn’t actually test the actual functions, despite indicating otherwise.</p>

<p class="author1">Mutation testing is your best defense against poor assumptions about code coverage. When you are measuring the efficacy of your tests, it becomes much harder to write useless, meaningless tests while still eliminating mutants. Mutation testing elevates coverage measurements to become a truer predictor of robustness. Coverage metrics still won’t be a perfect proxy for business value, but mutation testing certainly makes them more valuable as an indicator of robustness.<a data-type="indexterm" data-primary="metrics, fallacy of" data-startref="ix_mtrcs" id="idm45644722626536" class="calibre5"/><a data-type="indexterm" data-primary="mutation testing" data-secondary="adoption of" data-tertiary="fallacy of coverage and other metrics" data-startref="ix_muttstadpmtrc" id="idm45644722625560" class="calibre5"/><a data-type="indexterm" data-primary="code coverage, fallacy of" data-startref="ix_cdcvg" id="idm45644722624104" class="calibre5"/></p>
<div data-type="warning" epub:type="warning" class="calibre23"><h6 class="calibre24">Warning</h6>
<p class="author1">As mutation testing becomes more popular, I fully expect “number of mutants eliminated” to be the new buzzword metric replacing “100% code coverage.” While you definitely want fewer mutants to survive, beware any goal tied to one metric out of context; this number can be gamed just like all the others. You still need a full testing strategy to ensure robustness in your codebase.</p>
</div>
</div></section>





</div></section>













</div></section>

<section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 24. Mutation Testing" class="preface">
<div class="preface" id="mutation">
<section data-type="sect1" data-pdf-bookmark="Closing Thoughts" class="preface"><div class="preface" id="idm45644722720680">
<h1 class="calibre12" id="calibre_pb_7">Closing Thoughts</h1>

<p class="author1">Mutation testing will probably not be the first tool you reach for.<a data-type="indexterm" data-primary="mutation testing" data-secondary="adopting" data-startref="ix_muttstadp" id="idm45644722620328" class="calibre5"/> However, it’s a perfect complement for your testing strategy; it finds holes in your safety net and brings them to your attention. With automated tools such as <code class="calibre17">mutmut</code>, you can leverage your existing test suite to perform mutation testing effortlessly. Mutation testing helps you improve the robustness of your test suite, which in turn will help you write more robust code.</p>

<p class="author1">This concludes <a data-type="xref" href="part0025.html#part_4" class="calibre5">Part IV</a> of this book. You started by learning about static analysis, which provides early feedback at a low cost. You then learned about testing strategies and how to ask yourself what sorts of questions you want your tests to answer. From there, you learned about three specific types of testing: acceptance testing, property-based testing, and mutation testing. All of these serve as ways of enhancing your existing testing strategy, building a denser and stronger safety net around your codebase. With a strong safety net, you will give future developers the confidence and flexibility to evolve your system as they need.</p>

<p class="author1">This also concludes the book as a whole. It’s been a long journey, and you’ve learned a variety of tips, tools, and methods along the way. You’ve dived deep into Python’s type system, learned how writing your own types benefit the codebase, and discovered how to write extensible Python. Each part of this book has given you building blocks that will help your codebase stand the test of time.</p>

<p class="author1">While this is the end of the book, this is not the end of the story of robustness in Python. Our relatively young industry continues to evolve and transform, and as software continues to eat the world, the health and maintainability of complex systems become paramount. I expect continuing changes in how we understand software, and new tools and techniques to build better systems.</p>

<p class="author1">Never stop learning. Python will continue to evolve, adding features and providing new tools. Each one of these has the potential to transform how you write code. I can’t predict the future of Python or its ecosystem. As Python introduces new features, ask yourself about the intentions that feature conveys. What do readers of code assume if they see this new feature? What do they assume if that feature is not used? Understand how developers interact with your codebase, and empathize with them to create systems that are pleasant to develop in.</p>

<p class="author1">Furthermore, take every single thing you’ve read in this book and apply critical thought to it. Ask yourself: what value is provided and what does it cost to implement? The last thing I want readers to do is take the advice in this book as completely prescriptive and use it as a hammer to force codebases to adhere to the standards that “the book said to use” (any developer who worked in the ’90s or ’00s probably remembers “Design Pattern Fever,” where you couldn’t walk 10 steps without running into an <code class="calibre17">AbstractInterfaceFactorySingleton</code>). Each of the concepts in this book should be seen as a tool in a toolbox; my hope is that you’ve learned enough of the background context to make the right decisions about how you use them.</p>

<p class="author1">Above all, remember that you are a human working on a complex system, and other humans will work on these systems with you and after you. Each person has their own motivations, their own goals, their own dreams. Everybody will have their own challenges and struggles. Mistakes will happen. We will never eliminate them all. Instead, I want you to look at these mistakes and push our field forward by learning from them. I want you to help the future build off of your work. In spite of all the changes, all the ambiguities, all the deadlines and scope creep, and all the tribulations of software development, I want you to be able to stand behind your work and say: “I’m proud I built this. This was a good system.”</p>

<p class="author1">Thank you for taking the time to read this book. Now go forth and write awesome code that stands the test of time.<a data-type="indexterm" data-primary="testing" data-secondary="mutation" data-startref="ix_tstmut" id="idm45644722610408" class="calibre5"/><a data-type="indexterm" data-primary="mutation testing" data-startref="ix_muttst" id="idm45644722609160" class="calibre5"/></p>
</div></section>







<div data-type="footnotes" class="calibre25"><p data-type="footnote" id="idm45644723126456" class="calibre26"><sup class="calibre27"><a href="part0030_split_001.html#idm45644723126456-marker" class="calibre5">1</a></sup> Mutation testing was first proposed in 1971 by Richard A. DeMillo, Richard J. Lipton, and Fred G. Sayward in “Hints on Test Data Selection: Help for the Practicing Programmer,” <em class="calibre6">IEEE Computer</em>, 11(4): 34–41, April 1978. The first implementation was developed in 1980 by Tim A. Budd, “Mutation Analysis of Program Test Data,” PhD thesis, Yale University, 1980.</p><p data-type="footnote" id="idm45644722709960" class="calibre26"><sup class="calibre27"><a href="part0030_split_005.html#idm45644722709960-marker" class="calibre5">2</a></sup> You can find code with high churn by measuring files with the highest number of commits. I found the following Git one-liner after a quick Google search: <code class="calibre17">git rev-list --objects --all | awk '$2' | sort -k2 | uniq -cf1 | sort -rn | head</code>. This was provided by <code class="calibre17">sehe</code> on <a href="https://oreil.ly/39UTx" class="calibre5">this Stack Overflow question</a>.</p></div></div></section></body></html>