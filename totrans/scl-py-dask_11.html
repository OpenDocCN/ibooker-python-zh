<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 11. Machine Learning with Dask"><div class="chapter" id="ch11">
<h1><span class="label">Chapter 11. </span>Machine Learning with Dask</h1>


<p>Now that you know Dask’s many different data types, computation patterns, deployment options, and libraries, we are ready to tackle machine learning. You will quickly find that ML with Dask is quite intuitive to use, as it runs on the same Python environment as the many other popular ML libraries. Much of the heavy work is done by Dask’s built-in data types and Dask’s distributed schedulers, making writing code an enjoyable experience for the user.<sup><a data-type="noteref" id="id811-marker" href="ch11.xhtml#id811">1</a></sup></p>

<p>This chapter will primarily <a data-type="indexterm" data-primary="machine learning" data-see="ML" id="id812"/><a data-type="indexterm" data-primary="ML (machine learning)" data-secondary="Dask-ML library" id="id813"/><a data-type="indexterm" data-primary="Dask-ML library" id="id814"/><a data-type="indexterm" data-primary="libraries" data-secondary="Dask-ML" id="id815"/>use the Dask-ML library, a robustly supported ML library from the Dask open source project, but we will also highlight other libraries, such as XGBoost and scikit-learn. The Dask-ML library is designed to run both in clusters and locally.<sup><a data-type="noteref" id="id816-marker" href="ch11.xhtml#id816">2</a></sup> Dask-ML provides familiar interfaces by extending many common ML libraries. ML is different from many of the tasks discussed so far, as it requires the framework (here Dask-ML) to coordinate work more closely. In this chapter we’ll show some of the ways you can use it in your own programs, and we’ll also offer tips.</p>

<p>Since ML is such a wide and varied discipline, we are able to cover only some of the situations where Dask-ML is useful. This chapter will discuss some of the common work patterns, such as exploratory data analysis, random split, featurization, regression, and deep learning inferences, from a practitioner’s perspective on ramping up on Dask. If you don’t see your particular library or use case represented, it may still be possible to accelerate with Dask, and you should look at <a href="https://oreil.ly/eJGHU">Dask-ML’s API guide</a>. However, ML is not Dask’s primary focus, so you may find that you need to use other tools, like Ray.</p>






<section data-type="sect1" data-pdf-bookmark="Parallelizing ML"><div class="sect1" id="id85">
<h1>Parallelizing ML</h1>

<p>Many ML workloads face scaling <a data-type="indexterm" data-primary="ML (machine learning)" data-secondary="parallelizing" id="id817"/><a data-type="indexterm" data-primary="parallelizing ML (machine learning)" id="id818"/>challenges in two dimensions: model size and data size. Training models with large features or components, like many deep learning models, often become compute-bound, where training, predicting, and evaluating the model becomes slow and harder to manage. On the other hand, many ML models, even seemingly simple ones like regression, often get stretched to their limits with large volumes of training datasets that don’t fit into one machine, getting memory-bound in their scaling challenges.</p>

<p>On memory-bound workloads, Dask’s high-level collections that we have covered (such as Dask array, DataFrame, and bag) combine with Dask-ML libraries to offer native scaling. For compute-bound workloads, Dask parallelizes training through integrations such as Dask-ML and Dask-joblib. In the case of scikit-learn, Dask can manage cluster-wide work allocation, using Dask-joblib. You might notice each workflow requires a different library; this is because each ML tool uses its own method of parallelization that Dask extends.</p>

<p>You can use Dask in conjunction with many popular machine learning libraries, including scikit-learn and XGBoost. You may already be familiar with single-machine parallelism inside your favorite ML library. Dask takes these single-machine frameworks, like Dask-joblib, and extends them to machines connected over the network.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="When to Use Dask-ML"><div class="sect1" id="id186">
<h1>When to Use Dask-ML</h1>

<p>Dask excels in parallel tasks with <a data-type="indexterm" data-primary="Dask-ML" data-secondary="use cases" id="dskmlus"/><a data-type="indexterm" data-primary="ML (machine learning)" data-secondary="Dask-ML use cases" id="mldkmlu"/>limited distributed mutable state (like large model weights). Dask is commonly used for inference/predictions on ML models, which is simpler than training. Training models, on the other hand, often require more inter-worker communication in the form of model weight updates and repeated loops, with sometimes variable amounts of computation per training cycle. You can use Dask for both of these use cases, but adoption and tooling for training is not as widespread.</p>

<p>Dask’s integration with common data preparation tools—including pandas, NumPy, PyTorch, and TensorFlow—makes it easier to build inference pipelines. In JVM-based tools, like Spark, working with those libraries comes at a higher overhead.</p>

<p>Another great use case for Dask is feature engineering and plotting large datasets before training. Dask’s pre-processing functions often use the same signatures, and function the same way as scikit-learn, while distributing the work across machines. Similarly with plotting and visualization, Dask is able to generate a beautiful plot of a large dataset beyond the usual limits of matplotlib/seaborn.</p>

<p>For more involved ML and deep learning work, some users opt to generate PyTorch or TensorFlow models separately and then use the models generated for inference workloads using Dask. This keeps the workload on the Dask side embarrassingly parallel. Alternatively, some users opt to write the training data as a Dask DataFrame using the delayed pattern, which is fed into Keras or Torch. Be warned that there is a medium level of effort to do so.</p>

<p>As discussed in previous chapters, the Dask project is still in the early phase of its life, and some of these libraries are still a work in progress and come with disclaimers. We took extra caution to validate most of the numerical methods used in the Dask-ML library to make sure the logic and mathematics are sound and work as expected. However, some dependent libraries come with warnings that it’s not yet ready for prime time, especially as it relates to GPU-aware workloads and massively distributed workloads. We expect these to get sorted out as the community grows <a data-type="indexterm" data-primary="Dask-ML" data-secondary="use cases" data-startref="dskmlus" id="id819"/><a data-type="indexterm" data-primary="ML (machine learning)" data-secondary="Dask-ML use cases" data-startref="mldkmlu" id="id820"/>and users contribute their feedback.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Getting Started with Dask-ML and XGBoost"><div class="sect1" id="id274">
<h1>Getting Started with Dask-ML and XGBoost</h1>

<p>Dask-ML is the officially supported ML library for Dask. Here, we will walk through the functionalities provided in the Dask-ML API; how it connects Dask, pandas, and scikit-learn into its functions; and some differences between Dask and its scikit-learn equivalents. Additionally, we will walk through a few XGBoost gradient boost integrations. We will primarily use the New York City yellow taxicab data we used previously to walk through examples. You can access the dataset directly from the <a href="https://oreil.ly/lbU5V">New York City website</a>.</p>








<section data-type="sect2" data-pdf-bookmark="Feature Engineering"><div class="sect2" id="id86">
<h2>Feature Engineering</h2>

<p>As with any good data science <a data-type="indexterm" data-primary="Dask-ML" data-secondary="Dask-ML API, feature engineering" id="dkmldkp"/><a data-type="indexterm" data-primary="feature engineering, Dask-ML API" id="ftgdkmlp"/>workflow, we start with clean-up, applying scalers, and transforms. Dask-ML has drop-in replacements for most of the pre-processing API from scikit-learn, including <code>StandardScaler</code>, <code>PolynomialFeatures</code>, <code>MinMax​Scaler</code>, etc.</p>

<p>You can pass multiple columns to the transformers, and each will be normalized, resulting in a delayed Dask DataFrame that you should call <code>compute</code> on.</p>

<p>In <a data-type="xref" href="#ex_scaling_variables">Example 11-1</a>, we scale trip distances, which are in miles, and total amount, which is in dollars, to their own scaled variables. This is a continuation of the exploratory data analysis we did in <a data-type="xref" href="ch04.xhtml#ch04">Chapter 4</a>.</p>
<div id="ex_scaling_variables" data-type="example">
<h5><span class="label">Example 11-1. </span>Dask DataFrame pre-processing with <code>StandardScaler</code></h5>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">dask_ml.preprocessing</code> <code class="kn">import</code> <code class="n">StandardScaler</code>
<code class="kn">import</code> <code class="nn">dask.array</code> <code class="k">as</code> <code class="nn">da</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>

<code class="n">df</code> <code class="o">=</code> <code class="n">dd</code><code class="o">.</code><code class="n">read_parquet</code><code class="p">(</code><code class="n">url</code><code class="p">)</code>
<code class="n">trip_dist_df</code> <code class="o">=</code> <code class="n">df</code><code class="p">[[</code><code class="s2">"trip_distance"</code><code class="p">,</code> <code class="s2">"total_amount"</code><code class="p">]]</code>
<code class="n">scaler</code> <code class="o">=</code> <code class="n">StandardScaler</code><code class="p">()</code>

<code class="n">scaler</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">trip_dist_df</code><code class="p">)</code>
<code class="n">trip_dist_df_scaled</code> <code class="o">=</code> <code class="n">scaler</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">trip_dist_df</code><code class="p">)</code>
<code class="n">trip_dist_df_scaled</code><code class="o">.</code><code class="n">head</code><code class="p">()</code></pre></div>

<p>For categorical variables, while <a data-type="indexterm" data-primary="Dask-ML" data-secondary="Categorizer" id="id821"/><a data-type="indexterm" data-primary="Categorizer" id="id822"/>there is <code>OneHotEncoder</code> in Dask-ML, it’s not as efficient or as one-to-one in replacement as its scikit-learn equivalent. At this point we recommend using <code>Categorizer</code> to encode a categorical dtype.<sup><a data-type="noteref" id="id823-marker" href="ch11.xhtml#id823">3</a></sup></p>

<p><a data-type="xref" href="#ex_categorical_variables">Example 11-2</a> shows how you would categorize a particular column while preserving the existing DataFrame. We select <code>payment_type</code>, which is encoded originally as an integer but is really a four-category categorical variable. We call Dask-ML’s 
<span class="keep-together"><code>Categorizer</code>,</span> while using pandas’s <code>CategoricalDtype</code> to give type hints. While Dask does have type inference (e.g., it can auto-infer the type), it is always better to be explicit in your program.</p>
<div id="ex_categorical_variables" data-type="example">
<h5><span class="label">Example 11-2. </span>Dask DataFrame pre-processing as categorical variable using Dask-ML</h5>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">dask_ml.preprocessing</code> <code class="kn">import</code> <code class="n">Categorizer</code>
<code class="kn">from</code> <code class="nn">pandas.api.types</code> <code class="kn">import</code> <code class="n">CategoricalDtype</code>

<code class="n">payment_type_amt_df</code> <code class="o">=</code> <code class="n">df</code><code class="p">[[</code><code class="s2">"payment_type"</code><code class="p">,</code> <code class="s2">"total_amount"</code><code class="p">]]</code>

<code class="n">cat</code> <code class="o">=</code> <code class="n">Categorizer</code><code class="p">(</code><code class="n">categories</code><code class="o">=</code><code class="p">{</code><code class="s2">"payment_type"</code><code class="p">:</code> <code class="n">CategoricalDtype</code><code class="p">([</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">])})</code>
<code class="n">categorized_df</code> <code class="o">=</code> <code class="n">cat</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">payment_type_amt_df</code><code class="p">)</code>
<code class="n">categorized_df</code><code class="o">.</code><code class="n">dtypes</code>
<code class="n">payment_type_amt_df</code><code class="o">.</code><code class="n">head</code><code class="p">()</code></pre></div>

<p>Alternatively, you can opt to use Dask DataFrame’s built-in categorizer. While pandas is permissive with Object and String as categorical data types, Dask will reject these columns unless they are read first as a categorical variable. There are two ways you can do this: declare a column as categorical at reading the data, with <code>dtype={col: categorical}</code>, or convert before invoking <code>get_dummies</code>, by using <code>df​.catego⁠rize(“col1”)</code>. The reasoning here is that Dask is lazily evaluated and cannot create a dummy variable out of a column without having a full list of unique values seen. Calling <code>.categorize()</code> is convenient and allows for dynamic handling of additional categories, but keep in mind that it does need to scan the entire column first to get the categories then do another full scan to transform the column. So if you know the categories already and they won’t change, you should just invoke <code>DummyEncoder</code>.</p>

<p><a data-type="xref" href="#ex_categorical_variables_alt">Example 11-3</a> categorizes multiple columns at once. Nothing is materialized until you call <code>execute</code>, so you can chain many of these pre-processes at once.</p>
<div id="ex_categorical_variables_alt" data-type="example">
<h5><span class="label">Example 11-3. </span>Dask DataFrame pre-processing as categorical variable using the Dask DataFrame built-in</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">train</code> <code class="o">=</code> <code class="n">train</code><code class="o">.</code><code class="n">categorize</code><code class="p">(</code><code class="s2">"VendorID"</code><code class="p">)</code>
<code class="n">train</code> <code class="o">=</code> <code class="n">train</code><code class="o">.</code><code class="n">categorize</code><code class="p">(</code><code class="s2">"passenger_count"</code><code class="p">)</code>
<code class="n">train</code> <code class="o">=</code> <code class="n">train</code><code class="o">.</code><code class="n">categorize</code><code class="p">(</code><code class="s2">"store_and_fwd_flag"</code><code class="p">)</code>

<code class="n">test</code> <code class="o">=</code> <code class="n">test</code><code class="o">.</code><code class="n">categorize</code><code class="p">(</code><code class="s2">"VendorID"</code><code class="p">)</code>
<code class="n">test</code> <code class="o">=</code> <code class="n">test</code><code class="o">.</code><code class="n">categorize</code><code class="p">(</code><code class="s2">"passenger_count"</code><code class="p">)</code>
<code class="n">test</code> <code class="o">=</code> <code class="n">test</code><code class="o">.</code><code class="n">categorize</code><code class="p">(</code><code class="s2">"store_and_fwd_flag"</code><code class="p">)</code></pre></div>

<p><code>DummyEncoder</code> is the Dask-ML <a data-type="indexterm" data-primary="Dask-ML" data-secondary="DummyEncoder" id="id824"/><a data-type="indexterm" data-primary="DummyEncoder" id="id825"/>equivalent to scikit-learn’s <code>OneHotEncoder</code>, which will turn the variables into uint8, an eight-bit unsigned integer, which is more memory efficient.</p>

<p>Again, there is a Dask DataFrame function that gives you a similar result. <a data-type="xref" href="#ex_dummy_variables">Example 11-4</a> demonstrates this on categorical columns, and <a data-type="xref" href="#ex_datetime_dummy_alt">Example 11-5</a> pre-processes datetime. Datetime can be tricky to work with. In this case, Python natively deserializes the datetime. Remember to always check datetime conversion and apply the necessary transforms beforehand.</p>
<div id="ex_dummy_variables" data-type="example">
<h5><span class="label">Example 11-4. </span>Dask DataFrame pre-processing as dummy variable using the Dask DataFrame built-in</h5>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">dask_ml.preprocessing</code> <code class="kn">import</code> <code class="n">DummyEncoder</code>

<code class="n">dummy</code> <code class="o">=</code> <code class="n">DummyEncoder</code><code class="p">()</code>
<code class="n">dummified_df</code> <code class="o">=</code> <code class="n">dummy</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">categorized_df</code><code class="p">)</code>
<code class="n">dummified_df</code><code class="o">.</code><code class="n">dtypes</code>
<code class="n">dummified_df</code><code class="o">.</code><code class="n">head</code><code class="p">()</code></pre></div>
<div id="ex_datetime_dummy_alt" data-type="example">
<h5><span class="label">Example 11-5. </span>Dask DataFrame pre-processing datetime as dummy variable using the Dask DataFrame built-in</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">train</code><code class="p">[</code><code class="s1">'Hour'</code><code class="p">]</code> <code class="o">=</code> <code class="n">train</code><code class="p">[</code><code class="s1">'tpep_pickup_datetime'</code><code class="p">]</code><code class="o">.</code><code class="n">dt</code><code class="o">.</code><code class="n">hour</code>
<code class="n">test</code><code class="p">[</code><code class="s1">'Hour'</code><code class="p">]</code> <code class="o">=</code> <code class="n">test</code><code class="p">[</code><code class="s1">'tpep_pickup_datetime'</code><code class="p">]</code><code class="o">.</code><code class="n">dt</code><code class="o">.</code><code class="n">hour</code>

<code class="n">train</code><code class="p">[</code><code class="s1">'dayofweek'</code><code class="p">]</code> <code class="o">=</code> <code class="n">train</code><code class="p">[</code><code class="s1">'tpep_pickup_datetime'</code><code class="p">]</code><code class="o">.</code><code class="n">dt</code><code class="o">.</code><code class="n">dayofweek</code>
<code class="n">test</code><code class="p">[</code><code class="s1">'dayofweek'</code><code class="p">]</code> <code class="o">=</code> <code class="n">test</code><code class="p">[</code><code class="s1">'tpep_pickup_datetime'</code><code class="p">]</code><code class="o">.</code><code class="n">dt</code><code class="o">.</code><code class="n">dayofweek</code>

<code class="n">train</code> <code class="o">=</code> <code class="n">train</code><code class="o">.</code><code class="n">categorize</code><code class="p">(</code><code class="s2">"dayofweek"</code><code class="p">)</code>
<code class="n">test</code> <code class="o">=</code> <code class="n">test</code><code class="o">.</code><code class="n">categorize</code><code class="p">(</code><code class="s2">"dayofweek"</code><code class="p">)</code>

<code class="n">dom_train</code> <code class="o">=</code> <code class="n">dd</code><code class="o">.</code><code class="n">get_dummies</code><code class="p">(</code>
    <code class="n">train</code><code class="p">,</code>
    <code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s1">'dayofweek'</code><code class="p">],</code>
    <code class="n">prefix</code><code class="o">=</code><code class="s1">'dom'</code><code class="p">,</code>
    <code class="n">prefix_sep</code><code class="o">=</code><code class="s1">'_'</code><code class="p">)</code>
<code class="n">dom_test</code> <code class="o">=</code> <code class="n">dd</code><code class="o">.</code><code class="n">get_dummies</code><code class="p">(</code>
    <code class="n">test</code><code class="p">,</code>
    <code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s1">'dayofweek'</code><code class="p">],</code>
    <code class="n">prefix</code><code class="o">=</code><code class="s1">'dom'</code><code class="p">,</code>
    <code class="n">prefix_sep</code><code class="o">=</code><code class="s1">'_'</code><code class="p">)</code>

<code class="n">hour_train</code> <code class="o">=</code> <code class="n">dd</code><code class="o">.</code><code class="n">get_dummies</code><code class="p">(</code>
    <code class="n">train</code><code class="p">,</code>
    <code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s1">'dayofweek'</code><code class="p">],</code>
    <code class="n">prefix</code><code class="o">=</code><code class="s1">'h'</code><code class="p">,</code>
    <code class="n">prefix_sep</code><code class="o">=</code><code class="s1">'_'</code><code class="p">)</code>
<code class="n">hour_test</code> <code class="o">=</code> <code class="n">dd</code><code class="o">.</code><code class="n">get_dummies</code><code class="p">(</code>
    <code class="n">test</code><code class="p">,</code>
    <code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s1">'dayofweek'</code><code class="p">],</code>
    <code class="n">prefix</code><code class="o">=</code><code class="s1">'h'</code><code class="p">,</code>
    <code class="n">prefix_sep</code><code class="o">=</code><code class="s1">'_'</code><code class="p">)</code>

<code class="n">dow_train</code> <code class="o">=</code> <code class="n">dd</code><code class="o">.</code><code class="n">get_dummies</code><code class="p">(</code>
    <code class="n">train</code><code class="p">,</code>
    <code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s1">'dayofweek'</code><code class="p">],</code>
    <code class="n">prefix</code><code class="o">=</code><code class="s1">'dow'</code><code class="p">,</code>
    <code class="n">prefix_sep</code><code class="o">=</code><code class="s1">'_'</code><code class="p">)</code>
<code class="n">dow_test</code> <code class="o">=</code> <code class="n">dd</code><code class="o">.</code><code class="n">get_dummies</code><code class="p">(</code>
    <code class="n">test</code><code class="p">,</code>
    <code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s1">'dayofweek'</code><code class="p">],</code>
    <code class="n">prefix</code><code class="o">=</code><code class="s1">'dow'</code><code class="p">,</code>
    <code class="n">prefix_sep</code><code class="o">=</code><code class="s1">'_'</code><code class="p">)</code></pre></div>

<p>Dask-ML’s <code>train_test_split</code> method has more <a data-type="indexterm" data-primary="Dask-ML" data-secondary="train_test_split" id="id826"/>flexibility than the Dask DataFrames version. Both are partition-aware, and we using them instead of the scikit-learn equivalent. scikit-learn’s <code>train_test_split</code> can be called here, but it is not partition-aware and can result in a large data movement between workers, whereas the Dask implementations would split the train-test over each partition, avoiding the shuffle (see <a data-type="xref" href="#ex_dask_random_split">Example 11-6</a>).</p>
<div id="ex_dask_random_split" data-type="example">
<h5><span class="label">Example 11-6. </span>Dask DataFrame pseudorandom split</h5>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">dask_ml.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>

<code class="n">X_train</code><code class="p">,</code> <code class="n">X_test</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code>
    <code class="n">df</code><code class="p">[</code><code class="s1">'trip_distance'</code><code class="p">],</code> <code class="n">df</code><code class="p">[</code><code class="s1">'total_amount'</code><code class="p">])</code></pre></div>

<p>As a side effect of random splits happening by each partition block, the random behavior is not uniformly guaranteed over the whole of the DataFrame. If you suspect that some partition may have skews, you should <a data-type="indexterm" data-primary="Dask-ML" data-secondary="Dask-ML API, feature engineering" data-startref="dkmldkp" id="id827"/><a data-type="indexterm" data-primary="feature engineering, Dask-ML API" data-startref="ftgdkmlp" id="id828"/>compute, redistribute, and then shuffle-split.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Model Selection and Training"><div class="sect2" id="id87">
<h2>Model Selection and Training</h2>

<p>Many of scikit-learn’s model-selection-related functions, including cross-validation, hyperparameter <a data-type="indexterm" data-primary="Dask-ML" data-secondary="models" data-tertiary="selecting" id="dmdlsc"/><a data-type="indexterm" data-primary="Dask-ML" data-secondary="models" data-tertiary="training" id="dkmdl"/>search, clustering, regression, imputation, and scoring methods, are ported into Dask as a drop-in replacement. There are a few noteworthy improvements that make the functions run more efficiently than a simple parallel computing architecture, by using Dask’s task-graph views.</p>

<p>Most regression-based models have been implemented for Dask use and can be used as a replacement for scikit-learn.<sup><a data-type="noteref" id="id829-marker" href="ch11.xhtml#id829">4</a></sup> Many scikit-learn users would be familiar with the task of <code>.reshape()</code> for pandas, needing them to convert a pandas DataFrame into a 2D array in order for scikit-learn to work. For some Dask-ML functions you still need to also call <code>ddf.to_dask_array()</code> in order to convert a DataFrame to an array before training. Lately, some Dask-ML has been improved to directly work on Dask DataFrames, but not all libraries.</p>

<p><a data-type="xref" href="#linear_regression">Example 11-7</a> runs through a <a data-type="indexterm" data-primary="Dask-ML" data-secondary="linear regression" id="id830"/><a data-type="indexterm" data-primary="linear regression, Dask-ML and" id="id831"/><a data-type="indexterm" data-primary="regression, linear" id="id832"/>straightforward multi-variate linear regression using Dask-ML. Say you want to build a regression model on two predictor columns and one output column. You would apply <code>.to_array()</code> to convert the data type to Dask arrays and then pass them into Dask-ML’s implementation of <code>LinearRegression</code>. Note how we needed to materialize the conversion into arrays, and we gave explicit chunk size. This is because Dask-ML’s underlying implementation with linear models is not quite fully able to infer chunk sizes from previous steps. We also purposefully use scikit-learn’s scoring library, not Dask-ML. We are noticing implementation issues where Dask-ML doesn’t play well with chunk sizes.<sup><a data-type="noteref" id="id833-marker" href="ch11.xhtml#id833">5</a></sup> Thankfully, at this point, this calculation is a reduce step, which works without any Dask-specific logic.<sup><a data-type="noteref" id="id834-marker" href="ch11.xhtml#id834">6</a></sup></p>
<div id="linear_regression" data-type="example">
<h5><span class="label">Example 11-7. </span>Linear regression with Dask-ML</h5>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">dask_ml.linear_model</code> <code class="kn">import</code> <code class="n">LinearRegression</code>
<code class="kn">from</code> <code class="nn">dask_ml.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>

<code class="n">regr_df</code> <code class="o">=</code> <code class="n">df</code><code class="p">[[</code><code class="s1">'trip_distance'</code><code class="p">,</code> <code class="s1">'total_amount'</code><code class="p">]]</code><code class="o">.</code><code class="n">dropna</code><code class="p">()</code>
<code class="n">regr_X</code> <code class="o">=</code> <code class="n">regr_df</code><code class="p">[[</code><code class="s1">'trip_distance'</code><code class="p">]]</code>
<code class="n">regr_y</code> <code class="o">=</code> <code class="n">regr_df</code><code class="p">[[</code><code class="s1">'total_amount'</code><code class="p">]]</code>

<code class="n">X_train</code><code class="p">,</code> <code class="n">X_test</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code>
    <code class="n">regr_X</code><code class="p">,</code> <code class="n">regr_y</code><code class="p">)</code>

<code class="n">X_train</code> <code class="o">=</code> <code class="n">X_train</code><code class="o">.</code><code class="n">to_dask_array</code><code class="p">(</code><code class="n">lengths</code><code class="o">=</code><code class="p">[</code><code class="mi">100</code><code class="p">])</code><code class="o">.</code><code class="n">compute</code><code class="p">()</code>
<code class="n">X_test</code> <code class="o">=</code> <code class="n">X_test</code><code class="o">.</code><code class="n">to_dask_array</code><code class="p">(</code><code class="n">lengths</code><code class="o">=</code><code class="p">[</code><code class="mi">100</code><code class="p">])</code><code class="o">.</code><code class="n">compute</code><code class="p">()</code>
<code class="n">y_train</code> <code class="o">=</code> <code class="n">y_train</code><code class="o">.</code><code class="n">to_dask_array</code><code class="p">(</code><code class="n">lengths</code><code class="o">=</code><code class="p">[</code><code class="mi">100</code><code class="p">])</code><code class="o">.</code><code class="n">compute</code><code class="p">()</code>
<code class="n">y_test</code> <code class="o">=</code> <code class="n">y_test</code><code class="o">.</code><code class="n">to_dask_array</code><code class="p">(</code><code class="n">lengths</code><code class="o">=</code><code class="p">[</code><code class="mi">100</code><code class="p">])</code><code class="o">.</code><code class="n">compute</code><code class="p">()</code>

<code class="n">reg</code> <code class="o">=</code> <code class="n">LinearRegression</code><code class="p">()</code>
<code class="n">reg</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
<code class="n">y_pred</code> <code class="o">=</code> <code class="n">reg</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code>

<code class="n">r2_score</code><code class="p">(</code><code class="n">y_test</code><code class="p">,</code> <code class="n">y_pred</code><code class="p">)</code></pre></div>

<p>Note that function parameters for models for scikit-learn and Dask-ML are identical, but some are not supported as of now. For example, <code>LogisticRegression</code> is available in Dask-ML, but multi-class solver is not supported, meaning that there is no exact equivalent for multi-class solvers implemented in Dask-ML yet. So, if you want to use multinomial loss solver newton-cg or newton-cholesky, it might not work. For most uses of <code>LogisticRegression</code>, a default liblinear solver would do the trick. In practice, this limitation would pertain only to more niche and advanced use cases.</p>

<p>For hyperparameter search, Dask-ML has the scikit-learn equivalent of <code>GridSearchCV</code> for exhaustive search over parameter values, and <code>RandomizedSearchCV</code> for randomly trying hyperparameters from a list. These can be run directly, similar to the scikit-learn variant, if the data and resulting model do not require much scaling.</p>

<p>Cross-validation and <a data-type="indexterm" data-primary="Dask-ML" data-secondary="hyperparameter tuning" id="id835"/><a data-type="indexterm" data-primary="hyperparameter tuning, Dask-ML" id="id836"/>hyperparameter tuning often is a costly process even with a smaller dataset, as anyone who has run the scikit-learn cross-validate would attest. Dask users often deal with datasets large enough that use of exhaustive search algorithms is not feasible. As an alternative, Dask-ML implements several additional adaptive algorithms and hyperband-based methods that approach the tuned parameter more quickly with robust mathematical foundation.<sup><a data-type="noteref" id="id837-marker" href="ch11.xhtml#id837">7</a></sup> The <a data-type="indexterm" data-primary="Dask-ML" data-secondary="models" data-tertiary="selecting" data-startref="dmdlsc" id="id838"/><a data-type="indexterm" data-primary="Dask-ML" data-secondary="models" data-tertiary="training" data-startref="dkmdl" id="id839"/>authors of the <code>HyperbandSearchCV</code> methods do ask that the use be cited.<sup><a data-type="noteref" id="id840-marker" href="ch11.xhtml#id840">8</a></sup></p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="When There Is No Dask-ML Equivalent"><div class="sect2" id="id187">
<h2>When There Is No Dask-ML Equivalent</h2>

<p>If there is a function that exists <a data-type="indexterm" data-primary="Dask-ML" data-secondary="distributed version of code" id="id841"/>in scikit-learn or other data science libraries but not in Dask-ML, you can write the distributed version of your desired code. After all, Dask-ML can be thought of as a convenience wrapper around scikit-learn.</p>

<p><a data-type="xref" href="#ex_daskml_port">Example 11-8</a> uses scikit-learn’s learning functions <code>SGDRegressor</code> and <code>LinearRegression</code>, and uses <code>dask.delayed</code> to wrap the delayed functionality around the method. You can do this over any piece of code you may want to parallelize.</p>
<div id="ex_daskml_port" data-type="example">
<h5><span class="label">Example 11-8. </span>Linear regression with Dask-ML</h5>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">LinearRegression</code> <code class="k">as</code> <code class="n">ScikitLinearRegression</code>
<code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">SGDRegressor</code> <code class="k">as</code> <code class="n">ScikitSGDRegressor</code>

<code class="n">estimators</code> <code class="o">=</code> <code class="p">[</code><code class="n">ScikitLinearRegression</code><code class="p">(),</code> <code class="n">ScikitSGDRegressor</code><code class="p">()]</code>
<code class="n">run_tasks</code> <code class="o">=</code> <code class="p">[</code><code class="n">dask</code><code class="o">.</code><code class="n">delayed</code><code class="p">(</code><code class="n">estimator</code><code class="o">.</code><code class="n">fit</code><code class="p">)(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
             <code class="k">for</code> <code class="n">estimator</code> <code class="ow">in</code> <code class="n">estimators</code><code class="p">]</code>
<code class="n">run_tasks</code></pre></div>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Use with Dask’s joblib"><div class="sect2" id="id88">
<h2>Use with Dask’s joblib</h2>

<p>Alternatively, you can use scikit-learn <a data-type="indexterm" data-primary="Dask-ML" data-secondary="joblib and" id="id842"/><a data-type="indexterm" data-primary="joblib" id="id843"/>along with joblib (see <a data-type="xref" href="#ex_joblib">Example 11-9</a>), a package that can take any Python function as pipelined steps to be computed on a single machine. Joblib works well with a lot of parallel computations that are not dependent on each other. In this case, having hundreds of cores on a single machine would be helpful. While a typical laptop does not have hundreds of cores, using the four or so that it does have can still be beneficial. With Dask’s version of joblib you can use cores from multiple machines. This can work for ML workloads that are compute-bound on a single machine.</p>
<div id="ex_joblib" data-type="example">
<h5><span class="label">Example 11-9. </span>Parallelizing computation using joblib</h5>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">dask.distributed</code> <code class="kn">import</code> <code class="n">Client</code>
<code class="kn">from</code> <code class="nn">joblib</code> <code class="kn">import</code> <code class="n">parallel_backend</code>

<code class="n">client</code> <code class="o">=</code> <code class="n">Client</code><code class="p">(</code><code class="s1">'127.0.0.1:8786'</code><code class="p">)</code>

<code class="n">X</code><code class="p">,</code> <code class="n">y</code> <code class="o">=</code> <code class="n">load_my_data</code><code class="p">()</code>
<code class="n">net</code> <code class="o">=</code> <code class="n">get_that_net</code><code class="p">()</code>

<code class="n">gs</code> <code class="o">=</code> <code class="n">GridSearchCV</code><code class="p">(</code>
    <code class="n">net</code><code class="p">,</code>
    <code class="n">param_grid</code><code class="o">=</code><code class="p">{</code><code class="s1">'lr'</code><code class="p">:</code> <code class="p">[</code><code class="mf">0.01</code><code class="p">,</code> <code class="mf">0.03</code><code class="p">]},</code>
    <code class="n">scoring</code><code class="o">=</code><code class="s1">'accuracy'</code><code class="p">,</code>
<code class="p">)</code>

<code class="n">XGBClassifier</code><code class="p">()</code>

<code class="k">with</code> <code class="n">parallel_backend</code><code class="p">(</code><code class="s1">'dask'</code><code class="p">):</code>
    <code class="n">gs</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">gs</code><code class="o">.</code><code class="n">cv_results_</code><code class="p">)</code></pre></div>
</div></section>








<section data-type="sect2" data-pdf-bookmark="XGBoost with Dask"><div class="sect2" id="id89">
<h2>XGBoost with Dask</h2>

<p>XGBoost is a popular Python gradient boosting library, used for parallel tree boosting. Well-known gradient <a data-type="indexterm" data-primary="XGBoost, Dask-ML and" id="xgbkm"/><a data-type="indexterm" data-primary="Dask-ML" data-secondary="XGBoost and" id="dmlxgb"/>boosting methods include bootstrap aggregation (bagging). Various gradient boosting methods have been used in high-energy physics data analysis at the Large Hadron Collider, used to train deep neural networks to confirm the discovery of the Higgs boson. Gradient boosting methods are currently used in scientific areas such as geological or climate studies. Given its importance, we found XGBoost on Dask-ML to be a well-implemented library, ready for users.</p>

<p>Dask-ML has built-in support for XGBoost to work with Dask arrays and DataFrames. By using XGBClassifiers within Dask-ML, you will be setting up XGBoost in distributed mode, which works with your Dask cluster. When you do so, XGBoost’s master process lives in Dask scheduler, and XGBoost’s worker processes will be on Dask’s worker processes. The data distribution is handled using Dask DataFrame, split into pandas DataFrame, and is talking between Dask worker and XGBoost worker on the same machine.</p>

<p>XGBoost uses a <code>DMatrix</code> (data matrix) as the <a data-type="indexterm" data-primary="DMatrix, XGBoost" id="id844"/>standard data format it works with. XGBoost has a built-in Dask-compatible <code>DMatrix</code>, which takes in Dask array and Dask DataFrame. Once the Dask environment is set up, the use of gradient booster is as you would expect. Specify the learning rate, threads, and objective functions, as usual. <a data-type="xref" href="#ex_xgb_basic_usage">Example 11-10</a> works with a Dask CUDA cluster and runs a standard gradient booster training.</p>
<div id="ex_xgb_basic_usage" data-type="example">
<h5><span class="label">Example 11-10. </span>Gradient-boosted trees with Dask-ML</h5>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">xgboost</code> <code class="k">as</code> <code class="nn">xgb</code>
<code class="kn">from</code> <code class="nn">dask_cuda</code> <code class="kn">import</code> <code class="n">LocalCUDACluster</code>
<code class="kn">from</code> <code class="nn">dask.distributed</code> <code class="kn">import</code> <code class="n">Client</code>

<code class="n">n_workers</code> <code class="o">=</code> <code class="mi">4</code>
<code class="n">cluster</code> <code class="o">=</code> <code class="n">LocalCUDACluster</code><code class="p">(</code><code class="n">n_workers</code><code class="p">)</code>
<code class="n">client</code> <code class="o">=</code> <code class="n">Client</code><code class="p">(</code><code class="n">cluster</code><code class="p">)</code>

<code class="n">dtrain</code> <code class="o">=</code> <code class="n">xgb</code><code class="o">.</code><code class="n">dask</code><code class="o">.</code><code class="n">DaskDMatrix</code><code class="p">(</code><code class="n">client</code><code class="p">,</code> <code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>

<code class="n">booster</code> <code class="o">=</code> <code class="n">xgb</code><code class="o">.</code><code class="n">dask</code><code class="o">.</code><code class="n">train</code><code class="p">(</code>
    <code class="n">client</code><code class="p">,</code>
    <code class="p">{</code><code class="s2">"booster"</code><code class="p">:</code> <code class="s2">"gbtree"</code><code class="p">,</code> <code class="s2">"verbosity"</code><code class="p">:</code> <code class="mi">2</code><code class="p">,</code> <code class="s2">"nthread"</code><code class="p">:</code> <code class="mi">4</code><code class="p">,</code> <code class="s2">"eta"</code><code class="p">:</code> <code class="mf">0.01</code><code class="p">,</code> <code class="n">gamma</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code>
     <code class="s2">"max_depth"</code><code class="p">:</code> <code class="mi">5</code><code class="p">,</code> <code class="s2">"tree_method"</code><code class="p">:</code> <code class="s2">"auto"</code><code class="p">,</code> <code class="s2">"objective"</code><code class="p">:</code> <code class="s2">"reg:squarederror"</code><code class="p">},</code>
    <code class="n">dtrain</code><code class="p">,</code>
    <code class="n">num_boost_round</code><code class="o">=</code><code class="mi">4</code><code class="p">,</code>
    <code class="n">evals</code><code class="o">=</code><code class="p">[(</code><code class="n">dtrain</code><code class="p">,</code> <code class="s2">"train"</code><code class="p">)])</code></pre></div>

<p>In <a data-type="xref" href="#ex_xgb_train_plot_importance">Example 11-11</a>, we run a simple training run and plot feature importance. Note when we define <code>DMatrix</code>, we explicitly specify the labels, and the label names are taken from Dask DataFrame into <code>DMatrix</code>.</p>
<div id="ex_xgb_train_plot_importance" data-type="example">
<h5><span class="label">Example 11-11. </span>Dask-ML using the XGBoost library</h5>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">xgboost</code> <code class="k">as</code> <code class="nn">xgb</code>

<code class="n">dtrain</code> <code class="o">=</code> <code class="n">xgb</code><code class="o">.</code><code class="n">DMatrix</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="n">y_train</code><code class="p">,</code> <code class="n">feature_names</code><code class="o">=</code><code class="n">X_train</code><code class="o">.</code><code class="n">columns</code><code class="p">)</code>
<code class="n">dvalid</code> <code class="o">=</code> <code class="n">xgb</code><code class="o">.</code><code class="n">DMatrix</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="n">y_test</code><code class="p">,</code> <code class="n">feature_names</code><code class="o">=</code><code class="n">X_test</code><code class="o">.</code><code class="n">columns</code><code class="p">)</code>
<code class="n">watchlist</code> <code class="o">=</code> <code class="p">[(</code><code class="n">dtrain</code><code class="p">,</code> <code class="s1">'train'</code><code class="p">),</code> <code class="p">(</code><code class="n">dvalid</code><code class="p">,</code> <code class="s1">'valid'</code><code class="p">)]</code>
<code class="n">xgb_pars</code> <code class="o">=</code> <code class="p">{</code>
    <code class="s1">'min_child_weight'</code><code class="p">:</code> <code class="mi">1</code><code class="p">,</code>
    <code class="s1">'eta'</code><code class="p">:</code> <code class="mf">0.5</code><code class="p">,</code>
    <code class="s1">'colsample_bytree'</code><code class="p">:</code> <code class="mf">0.9</code><code class="p">,</code>
    <code class="s1">'max_depth'</code><code class="p">:</code> <code class="mi">6</code><code class="p">,</code>
    <code class="s1">'subsample'</code><code class="p">:</code> <code class="mf">0.9</code><code class="p">,</code>
    <code class="s1">'lambda'</code><code class="p">:</code> <code class="mf">1.</code><code class="p">,</code>
    <code class="s1">'nthread'</code><code class="p">:</code> <code class="o">-</code><code class="mi">1</code><code class="p">,</code>
    <code class="s1">'booster'</code><code class="p">:</code> <code class="s1">'gbtree'</code><code class="p">,</code>
    <code class="s1">'silent'</code><code class="p">:</code> <code class="mi">1</code><code class="p">,</code>
    <code class="s1">'eval_metric'</code><code class="p">:</code> <code class="s1">'rmse'</code><code class="p">,</code>
    <code class="s1">'objective'</code><code class="p">:</code> <code class="s1">'reg:linear'</code><code class="p">}</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">xgb</code><code class="o">.</code><code class="n">train</code><code class="p">(</code><code class="n">xgb_pars</code><code class="p">,</code> <code class="n">dtrain</code><code class="p">,</code> <code class="mi">10</code><code class="p">,</code> <code class="n">watchlist</code><code class="p">,</code> <code class="n">early_stopping_rounds</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code>
                  <code class="n">maximize</code><code class="o">=</code><code class="kc">False</code><code class="p">,</code> <code class="n">verbose_eval</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="s1">'Modeling RMSLE </code><code class="si">%.5f</code><code class="s1">'</code> <code class="o">%</code> <code class="n">model</code><code class="o">.</code><code class="n">best_score</code><code class="p">)</code>

<code class="n">xgb</code><code class="o">.</code><code class="n">plot_importance</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">max_num_features</code><code class="o">=</code><code class="mi">28</code><code class="p">,</code> <code class="n">height</code><code class="o">=</code><code class="mf">0.7</code><code class="p">)</code>

<code class="n">pred</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">dtest</code><code class="p">)</code>
<code class="n">pred</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">exp</code><code class="p">(</code><code class="n">pred</code><code class="p">)</code> <code class="o">-</code> <code class="mi">1</code></pre></div>

<p>Putting the previous examples together, you can now compose a function that can fit a model, provide early stopping arguments, and also define <a data-type="indexterm" data-primary="XGBoost, Dask-ML and" data-startref="xgbkm" id="id845"/><a data-type="indexterm" data-primary="Dask-ML" data-secondary="XGBoost and" data-startref="dmlxgb" id="id846"/>predictions using XGBoost for Dask (see <a data-type="xref" href="#ex_xgb_early_stopping">Example 11-12</a>). These would be called in your main client code.</p>
<div id="ex_xgb_early_stopping" data-type="example">
<h5><span class="label">Example 11-12. </span>Gradient-boosted tree training and inference using the Dask 
<span class="keep-together">XGBoost library</span></h5>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">xgboost</code> <code class="k">as</code> <code class="nn">xgb</code>
<code class="kn">from</code> <code class="nn">dask_cuda</code> <code class="kn">import</code> <code class="n">LocalCUDACluster</code>
<code class="kn">from</code> <code class="nn">dask.distributed</code> <code class="kn">import</code> <code class="n">Client</code>

<code class="n">n_workers</code> <code class="o">=</code> <code class="mi">4</code>
<code class="n">cluster</code> <code class="o">=</code> <code class="n">LocalCUDACluster</code><code class="p">(</code><code class="n">n_workers</code><code class="p">)</code>
<code class="n">client</code> <code class="o">=</code> <code class="n">Client</code><code class="p">(</code><code class="n">cluster</code><code class="p">)</code>


<code class="k">def</code> <code class="nf">fit_model</code><code class="p">(</code><code class="n">client</code><code class="p">,</code> <code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">X_valid</code><code class="p">,</code> <code class="n">y_valid</code><code class="p">,</code>
              <code class="n">early_stopping_rounds</code><code class="o">=</code><code class="mi">5</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">xgb</code><code class="o">.</code><code class="n">Booster</code><code class="p">:</code>
    <code class="n">Xy_valid</code> <code class="o">=</code> <code class="n">dxgb</code><code class="o">.</code><code class="n">DaskDMatrix</code><code class="p">(</code><code class="n">client</code><code class="p">,</code> <code class="n">X_valid</code><code class="p">,</code> <code class="n">y_valid</code><code class="p">)</code>
    <code class="c1"># train the model</code>
    <code class="n">booster</code> <code class="o">=</code> <code class="n">xgb</code><code class="o">.</code><code class="n">dask</code><code class="o">.</code><code class="n">train</code><code class="p">(</code>
        <code class="n">client</code><code class="p">,</code>
        <code class="p">{</code><code class="s2">"booster"</code><code class="p">:</code> <code class="s2">"gbtree"</code><code class="p">,</code> <code class="s2">"verbosity"</code><code class="p">:</code> <code class="mi">2</code><code class="p">,</code> <code class="s2">"nthread"</code><code class="p">:</code> <code class="mi">4</code><code class="p">,</code> <code class="s2">"eta"</code><code class="p">:</code> <code class="mf">0.01</code><code class="p">,</code> <code class="n">gamma</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code>
         <code class="s2">"max_depth"</code><code class="p">:</code> <code class="mi">5</code><code class="p">,</code> <code class="s2">"tree_method"</code><code class="p">:</code> <code class="s2">"gpu_hist"</code><code class="p">,</code> <code class="s2">"objective"</code><code class="p">:</code> <code class="s2">"reg:squarederror"</code><code class="p">},</code>
        <code class="n">dtrain</code><code class="p">,</code>
        <code class="n">num_boost_round</code><code class="o">=</code><code class="mi">500</code><code class="p">,</code>
        <code class="n">early_stopping_rounds</code><code class="o">=</code><code class="n">early_stopping_rounds</code><code class="p">,</code>
        <code class="n">evals</code><code class="o">=</code><code class="p">[(</code><code class="n">dtrain</code><code class="p">,</code> <code class="s2">"train"</code><code class="p">)])[</code><code class="s2">"booster"</code><code class="p">]</code>
    <code class="k">return</code> <code class="n">booster</code>


<code class="k">def</code> <code class="nf">predict</code><code class="p">(</code><code class="n">client</code><code class="p">,</code> <code class="n">model</code><code class="p">,</code> <code class="n">X</code><code class="p">):</code>
    <code class="n">predictions</code> <code class="o">=</code> <code class="n">xgb</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">client</code><code class="p">,</code> <code class="n">model</code><code class="p">,</code> <code class="n">X</code><code class="p">)</code>
    <code class="k">assert</code> <code class="nb">isinstance</code><code class="p">(</code><code class="n">predictions</code><code class="p">,</code> <code class="n">dd</code><code class="o">.</code><code class="n">Series</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">predictions</code></pre></div>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="ML Models with Dask-SQL"><div class="sect1" id="id188">
<h1>ML Models with Dask-SQL</h1>

<p>A much newer addition is another library, Dask-SQL, that provides a convenient wrapper around simple ML model <a data-type="indexterm" data-primary="ML (machine learning)" data-secondary="Dask-SQL" id="mldsql"/><a data-type="indexterm" data-primary="Dask-SQL" data-secondary="ML (machine learning) models" id="dskqmlm"/>training workloads. <a data-type="xref" href="#Dask_sql_define_tables">Example 11-13</a> loads the same NYC yellow taxicab data as a Dask DataFrame and then registers the view to Dask-SQL context.</p>
<div id="Dask_sql_define_tables" data-type="example">
<h5><span class="label">Example 11-13. </span>Registering datasets into Dask-SQL</h5>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">dask.dataframe</code> <code class="k">as</code> <code class="nn">dd</code>
<code class="kn">import</code> <code class="nn">dask.datasets</code>
<code class="kn">from</code> <code class="nn">dask_sql</code> <code class="kn">import</code> <code class="n">Context</code>

<code class="c1"># read dataset</code>
<code class="n">taxi_df</code> <code class="o">=</code> <code class="n">dd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code><code class="s1">'./data/taxi_train_subset.csv'</code><code class="p">)</code>
<code class="n">taxi_test</code> <code class="o">=</code> <code class="n">dd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code><code class="s1">'./data/taxi_test.csv'</code><code class="p">)</code>

<code class="c1"># create a context to register tables</code>
<code class="n">c</code> <code class="o">=</code> <code class="n">Context</code><code class="p">()</code>
<code class="n">c</code><code class="o">.</code><code class="n">create_table</code><code class="p">(</code><code class="s2">"taxi_test"</code><code class="p">,</code> <code class="n">taxi_test</code><code class="p">)</code>
<code class="n">c</code><code class="o">.</code><code class="n">create_table</code><code class="p">(</code><code class="s2">"taxicab"</code><code class="p">,</code> <code class="n">taxi_df</code><code class="p">)</code></pre></div>

<p>Dask-SQL implements similar ML SQL language to BigQuery ML, allowing you to simply define models, define the training data as a SQL select statement, and then run inference on a different select statement as well.</p>

<p>You can define the model with most of the ML models we discussed, and this will run the scikit-learn ML models in the background. In <a data-type="xref" href="#Dask_sql_linear_regression">Example 11-14</a>, we train the <code>LinearRegression</code> model we trained earlier, using Dask-SQL. We first define the model, telling it to use scikit-learn’s <code>LinearRegression</code>, and the target column. Then we feed the training data with requisite columns. You can inspect the model trained using the <code>DESCRIBE</code> statement; then you can see in the <code>FROM PREDICT</code> statement how the model is used to run inference on another SQL-defined dataset.</p>
<div id="Dask_sql_linear_regression" data-type="example">
<h5><span class="label">Example 11-14. </span>Defining, training, and predicting a linear regression on Dask-SQL</h5>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">dask.dataframe</code> <code class="k">as</code> <code class="nn">dd</code>
<code class="kn">import</code> <code class="nn">dask.datasets</code>
<code class="kn">from</code> <code class="nn">dask_sql</code> <code class="kn">import</code> <code class="n">Context</code>

<code class="n">c</code> <code class="o">=</code> <code class="n">Context</code><code class="p">()</code>
<code class="c1"># define model</code>
<code class="n">c</code><code class="o">.</code><code class="n">sql</code><code class="p">(</code>
    <code class="sd">"""</code>
<code class="sd">CREATE MODEL fare_linreg_model WITH (</code>
<code class="sd">    model_class = 'LinearRegression',</code>
<code class="sd">    wrap_predict = True,</code>
<code class="sd">    target_column = 'fare_amount'</code>
<code class="sd">) AS (</code>
<code class="sd">    SELECT passenger_count, fare_amount</code>
<code class="sd">    FROM taxicab</code>
<code class="sd">    LIMIT 1000</code>
<code class="sd">)</code>
<code class="sd">"""</code>
<code class="p">)</code>

<code class="c1"># describe model</code>
<code class="n">c</code><code class="o">.</code><code class="n">sql</code><code class="p">(</code>
    <code class="sd">"""</code>
<code class="sd">DESCRIBE MODEL fare_linreg_model</code>
<code class="sd">    """</code>
<code class="p">)</code><code class="o">.</code><code class="n">compute</code><code class="p">()</code>

<code class="c1"># run inference</code>
<code class="n">c</code><code class="o">.</code><code class="n">sql</code><code class="p">(</code>
    <code class="sd">"""</code>
<code class="sd">SELECT</code>
<code class="sd">    *</code>
<code class="sd">FROM PREDICT(MODEL fare_linreg_model,</code>
<code class="sd">    SELECT * FROM taxi_test</code>
<code class="sd">)</code>
<code class="sd">    """</code>
<code class="p">)</code><code class="o">.</code><code class="n">compute</code><code class="p">()</code></pre></div>

<p>Similarly, as shown in <a data-type="xref" href="#Dask_sql_XGBClassifier">Example 11-15</a>, you can run classification models, similar to the XGBoost model we have discussed <a data-type="indexterm" data-primary="ML (machine learning)" data-secondary="Dask-SQL" data-startref="mldsql" id="id847"/><a data-type="indexterm" data-primary="Dask-SQL" data-secondary="ML (machine learning) models" data-startref="dskqmlm" id="id848"/>earlier using the Dask-ML library.</p>
<div id="Dask_sql_XGBClassifier" data-type="example">
<h5><span class="label">Example 11-15. </span>Defining, training, and predicting a classifier built using XGBoost 
<span class="keep-together">with Dask-SQL</span></h5>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">dask.dataframe</code> <code class="k">as</code> <code class="nn">dd</code>
<code class="kn">import</code> <code class="nn">dask.datasets</code>
<code class="kn">from</code> <code class="nn">dask_sql</code> <code class="kn">import</code> <code class="n">Context</code>

<code class="n">c</code> <code class="o">=</code> <code class="n">Context</code><code class="p">()</code>
<code class="c1"># define model</code>
<code class="n">c</code><code class="o">.</code><code class="n">sql</code><code class="p">(</code>
    <code class="sd">"""</code>
<code class="sd">CREATE MODEL classify_faretype WITH (</code>
<code class="sd">    model_class = 'XGBClassifier',</code>
<code class="sd">    target_column = 'fare_type'</code>
<code class="sd">) AS (</code>
<code class="sd">    SELECT airport_surcharge, passenger_count, fare_type</code>
<code class="sd">    FROM taxicab</code>
<code class="sd">    LIMIT 1000</code>
<code class="sd">)</code>
<code class="sd">"""</code>
<code class="p">)</code>

<code class="c1"># describe model</code>
<code class="n">c</code><code class="o">.</code><code class="n">sql</code><code class="p">(</code>
    <code class="sd">"""</code>
<code class="sd">DESCRIBE MODEL classify_faretype</code>
<code class="sd">    """</code>
<code class="p">)</code><code class="o">.</code><code class="n">compute</code><code class="p">()</code>

<code class="c1"># run inference</code>
<code class="n">c</code><code class="o">.</code><code class="n">sql</code><code class="p">(</code>
    <code class="sd">"""</code>
<code class="sd">SELECT</code>
<code class="sd">    *</code>
<code class="sd">FROM PREDICT(MODEL classify_faretype,</code>
<code class="sd">    SELECT airport_surcharge, passenger_count, FROM taxi_test</code>
<code class="sd">)</code>
<code class="sd">    """</code>
<code class="p">)</code><code class="o">.</code><code class="n">compute</code><code class="p">()</code></pre></div>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Inference and Deployment"><div class="sect1" id="id275">
<h1>Inference and Deployment</h1>

<p>Regardless of the libraries chosen to train and validate your model (which could be using some of the Dask-ML libraries, or trained without using Dask at all), here are some of the considerations to keep in mind when using Dask for model inference deployment.</p>








<section data-type="sect2" data-pdf-bookmark="Distributing Data and Models Manually"><div class="sect2" id="id90">
<h2>Distributing Data and Models Manually</h2>

<p>When loading data and pre-trained models <a data-type="indexterm" data-primary="Dask-ML" data-secondary="inference deployment" id="id849"/><a data-type="indexterm" data-primary="inference deployment, Dask-ML" id="id850"/>to Dask workers, <code>dask.delayed</code> is the main tool (see <a data-type="xref" href="#dask_delayed_load">Example 11-16</a>). When distributing data, you should choose to use Dask’s collections: array and DataFrame. As you recall from <a data-type="xref" href="ch04.xhtml#ch04">Chapter 4</a>, each Dask DataFrame is made up of a pandas DataFrame. This is useful since you can write a method that takes each smaller DataFrame and returns a computed output. Custom functions and tasks can also be given per partition using Dask DataFrame’s <code>map​_par⁠titions</code> function.</p>

<p>Remember to use delayed notation if you are reading in a large dataset, to delay materialization and avoid reading in unnecessarily early.</p>
<div data-type="tip"><h6>Tip</h6>
<p><code>map_partitions</code> passes in a row-wise operation that is meant to be fit into a serializable code that is marshaled to workers. You can define a custom class that handles inference to be called, but a static method needs to be called, not an instance-dependent method. We covered this further in <a data-type="xref" href="ch04.xhtml#ch04">Chapter 4</a>.</p>
</div>
<div id="dask_delayed_load" data-type="example">
<h5><span class="label">Example 11-16. </span>Loading large files on Dask workers</h5>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">skimage.io</code> <code class="kn">import</code> <code class="n">imread</code>
<code class="kn">from</code> <code class="nn">skimage.io.collection</code> <code class="kn">import</code> <code class="n">alphanumeric_key</code>
<code class="kn">from</code> <code class="nn">dask</code> <code class="kn">import</code> <code class="n">delayed</code>
<code class="kn">import</code> <code class="nn">dask.array</code> <code class="k">as</code> <code class="nn">da</code>
<code class="kn">import</code> <code class="nn">os</code>

<code class="n">root</code><code class="p">,</code> <code class="n">dirs</code><code class="p">,</code> <code class="n">filenames</code> <code class="o">=</code> <code class="n">os</code><code class="o">.</code><code class="n">walk</code><code class="p">(</code><code class="n">dataset_dir</code><code class="p">)</code>
<code class="c1"># sample first file</code>
<code class="n">imread</code><code class="p">(</code><code class="n">filenames</code><code class="p">[</code><code class="mi">0</code><code class="p">])</code>


<code class="nd">@dask</code><code class="o">.</code><code class="n">delayed</code>
<code class="k">def</code> <code class="nf">lazy_reader</code><code class="p">(</code><code class="n">file</code><code class="p">):</code>
    <code class="k">return</code> <code class="n">imread</code><code class="p">(</code><code class="n">file</code><code class="p">)</code>


<code class="c1"># we have a bunch of delayed readers attached to the files</code>
<code class="n">lazy_arrays</code> <code class="o">=</code> <code class="p">[</code><code class="n">lazy_reader</code><code class="p">(</code><code class="n">file</code><code class="p">)</code> <code class="k">for</code> <code class="n">file</code> <code class="ow">in</code> <code class="n">filenames</code><code class="p">]</code>

<code class="c1"># read individual files from reader into a dask array</code>
<code class="c1"># particularly useful if each image is a large file like DICOM radiographs</code>
<code class="c1"># mammography dicom tends to be extremely large</code>
<code class="n">dask_arrays</code> <code class="o">=</code> <code class="p">[</code>
    <code class="n">da</code><code class="o">.</code><code class="n">from_delayed</code><code class="p">(</code><code class="n">delayed_reader</code><code class="p">,</code> <code class="n">shape</code><code class="o">=</code><code class="p">(</code><code class="mi">4608</code><code class="p">,</code> <code class="mi">5200</code><code class="p">,),</code> <code class="n">dtype</code><code class="o">=</code><code class="n">np</code><code class="o">.</code><code class="n">float32</code><code class="p">)</code>
    <code class="k">for</code> <code class="n">delayed_reader</code> <code class="ow">in</code> <code class="n">lazy_arrays</code>
<code class="p">]</code></pre></div>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Large-Scale Inferences with Dask"><div class="sect2" id="id189">
<h2>Large-Scale Inferences with Dask</h2>

<p>When using Dask for inference on scale, you <a data-type="indexterm" data-primary="Dask-ML" data-secondary="inference deployment" data-tertiary="large-scale inferences" id="dskmlfgcf"/><a data-type="indexterm" data-primary="inference deployment, Dask-ML" data-secondary="large-scale inferences" id="ifdlgsmkm"/>would distribute trained models to each worker, and then distribute Dask collections (DataFrame or array) over these partitions to work on a portion of the collection at a time, parallelizing the workflow. This strategy would work well in a straightforward inference deployment. We will cover one of the ways to achieve this: defining the workflow manually using 
<span class="keep-together"><code>map_partitions</code>,</span> and then wrapping existing functions with PyTorch or Keras/​Ten⁠sor⁠Flow models. For PyTorch-based models, you can wrap Skorch with the model, which allows it to be used with the Dask-ML API. For TensorFlow models, you would use SciKeras to create a scikit-learn-compatible model, which would allow it to be used for Dask-ML. For PyTorch, the dask-pytorch-ddp library from SaturnCloud is currently the most widely used. As for Keras and TensorFlow, be aware that while it’s doable, there are some issues with TensorFlow not liking some of its threads being moved to other workers.</p>

<p>The most generic way to deploy inference is using Dask DataFrame’s <code>map_partitions</code> (see <a data-type="xref" href="#Dask_DataFrame_map_partition_inference">Example 11-17</a>). You can take your custom inference function that will be run on each row, with the data mapped over each worker by partition.</p>
<div id="Dask_DataFrame_map_partition_inference" data-type="example">
<h5><span class="label">Example 11-17. </span>Distributed inference using Dask DataFrame</h5>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">dask.dataframe</code> <code class="k">as</code> <code class="nn">dd</code>
<code class="kn">import</code> <code class="nn">dask.bag</code> <code class="k">as</code> <code class="nn">db</code>

<code class="k">def</code> <code class="nf">rowwise_operation</code><code class="p">(</code><code class="n">row</code><code class="p">,</code> <code class="n">arg</code> <code class="o">*</code><code class="p">):</code>
    <code class="c1"># row-wise compute</code>
    <code class="k">return</code> <code class="n">result</code>


<code class="k">def</code> <code class="nf">partition_operation</code><code class="p">(</code><code class="n">df</code><code class="p">):</code>
    <code class="c1"># partition wise logic</code>
    <code class="n">result</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="n">col1</code><code class="p">]</code><code class="o">.</code><code class="n">apply</code><code class="p">(</code><code class="n">rowwise_operation</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">result</code>


<code class="n">ddf</code> <code class="o">=</code> <code class="n">dd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code><code class="err">“</code><code class="n">metadata_of_files</code><code class="err">”</code><code class="p">)</code>
<code class="n">results</code> <code class="o">=</code> <code class="n">ddf</code><code class="o">.</code><code class="n">map_partitions</code><code class="p">(</code><code class="n">partition_operation</code><code class="p">)</code>
<code class="n">results</code><code class="o">.</code><code class="n">compute</code><code class="p">()</code>

<code class="c1"># An alternate way, but note the .apply() here becomes a pandas apply, not</code>
<code class="c1"># Dask .apply(), and you must define axis = 1</code>
<code class="n">ddf</code><code class="o">.</code><code class="n">map_partitions</code><code class="p">(</code>
    <code class="k">lambda</code> <code class="n">partition</code><code class="p">:</code> <code class="n">partition</code><code class="o">.</code><code class="n">apply</code><code class="p">(</code>
        <code class="k">lambda</code> <code class="n">row</code><code class="p">:</code> <code class="n">rowwise_operation</code><code class="p">(</code><code class="n">row</code><code class="p">),</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">),</code> <code class="n">meta</code><code class="o">=</code><code class="p">(</code>
            <code class="s1">'ddf'</code><code class="p">,</code> <code class="nb">object</code><code class="p">))</code></pre></div>

<p>One of the interesting ways that Dask offers more than other scalable libraries is flexibility in parallel behavior. In the preceding example, we define a function that works row-wise and then give that function to a partition-wise logic that will be run by each partition over the entire DataFrame. We can use this as a boilerplate to define more fine-grained batched functions (see <a data-type="xref" href="#batched_operations">Example 11-18</a>). Keep in mind that behaviors you define within the row-wise function should be free of side effects, as in, you should avoid mutating the inputs to the function, as is the general best practice in Dask distributed delayed computations. Also, as the <a data-type="indexterm" data-primary="Dask-ML" data-secondary="inference deployment" data-tertiary="large-scale inferences" data-startref="dskmlfgcf" id="id851"/><a data-type="indexterm" data-primary="inference deployment, Dask-ML" data-secondary="large-scale inferences" data-startref="ifdlgsmkm" id="id852"/>comments in the preceding example say, if you do .apply within a partition-wise lambda, this calls <code>.apply()</code> from pandas. Within Pandas, <code>.apply()</code> defaults to <code>axis = 0</code>, so if you want otherwise, you should remember to specify <code>axis = 1</code>.</p>
<div id="batched_operations" data-type="example">
<h5><span class="label">Example 11-18. </span>Distributed inference using Dask DataFrame</h5>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">handle_batch</code><code class="p">(</code><code class="n">batch</code><code class="p">,</code> <code class="n">conn</code><code class="p">,</code> <code class="n">nlp_model</code><code class="p">):</code>
    <code class="c1"># run_inference_here.</code>
    <code class="n">conn</code><code class="o">.</code><code class="n">commit</code><code class="p">()</code>


<code class="k">def</code> <code class="nf">handle_partition</code><code class="p">(</code><code class="n">df</code><code class="p">):</code>
    <code class="n">worker</code> <code class="o">=</code> <code class="n">get_worker</code><code class="p">()</code>
    <code class="n">conn</code> <code class="o">=</code> <code class="n">connect_to_db</code><code class="p">()</code>
    <code class="k">try</code><code class="p">:</code>
        <code class="n">nlp_model</code> <code class="o">=</code> <code class="n">worker</code><code class="o">.</code><code class="n">roberta_model</code>
    <code class="k">except</code> <code class="ne">BaseException</code><code class="p">:</code>
        <code class="n">nlp_model</code> <code class="o">=</code> <code class="n">load_model</code><code class="p">()</code>
        <code class="n">worker</code><code class="o">.</code><code class="n">nlp_model</code> <code class="o">=</code> <code class="n">nlp_model</code>
    <code class="n">result</code><code class="p">,</code> <code class="n">batch</code> <code class="o">=</code> <code class="p">[],</code> <code class="p">[]</code>
    <code class="k">for</code> <code class="n">_</code><code class="p">,</code> <code class="n">row</code> <code class="ow">in</code> <code class="n">part</code><code class="o">.</code><code class="n">iterrows</code><code class="p">():</code>
        <code class="k">if</code> <code class="nb">len</code><code class="p">(</code><code class="n">batch</code><code class="p">)</code> <code class="o">%</code> <code class="n">batch_size</code> <code class="o">==</code> <code class="mi">0</code> <code class="ow">and</code> <code class="nb">len</code><code class="p">(</code><code class="n">batch</code><code class="p">)</code> <code class="o">&gt;</code> <code class="mi">0</code><code class="p">:</code>
            <code class="n">batch_results</code> <code class="o">=</code> <code class="n">handle_batch</code><code class="p">(</code><code class="n">batch</code><code class="p">,</code> <code class="n">conn</code><code class="p">,</code> <code class="n">nlp_model</code><code class="p">)</code>
            <code class="n">result</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">batch_results</code><code class="p">)</code>
            <code class="n">batch</code> <code class="o">=</code> <code class="p">[]</code>
        <code class="n">batch</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="n">row</code><code class="o">.</code><code class="n">doc_id</code><code class="p">,</code> <code class="n">row</code><code class="o">.</code><code class="n">sent_id</code><code class="p">,</code> <code class="n">row</code><code class="o">.</code><code class="n">utterance</code><code class="p">))</code>
    <code class="k">if</code> <code class="nb">len</code><code class="p">(</code><code class="n">batch</code><code class="p">)</code> <code class="o">&gt;</code> <code class="mi">0</code><code class="p">:</code>
        <code class="n">batch_results</code> <code class="o">=</code> <code class="n">handle_batch</code><code class="p">(</code><code class="n">batch</code><code class="p">,</code> <code class="n">conn</code><code class="p">,</code> <code class="n">nlp_model</code><code class="p">)</code>
        <code class="n">result</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">batch_results</code><code class="p">)</code>
    <code class="n">conn</code><code class="o">.</code><code class="n">close</code><code class="p">()</code>
    <code class="k">return</code> <code class="n">result</code>


<code class="n">ddf</code> <code class="o">=</code> <code class="n">dd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code><code class="s2">"metadata.csv”)</code><code class="w"/>
<code class="n">results</code> <code class="o">=</code> <code class="n">ddf</code><code class="o">.</code><code class="n">map_partitions</code><code class="p">(</code><code class="n">handle_partition</code><code class="p">)</code>
<code class="n">results</code><code class="o">.</code><code class="n">compute</code><code class="p">()</code></pre></div>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Conclusion"><div class="sect1" id="id276">
<h1>Conclusion</h1>

<p>In this chapter, you have learned how to use the building blocks of Dask to write data science and ML workflows, combining core Dask libraries with other ML libraries you might be familiar with to achieve your desired task. You have also learned how you can use Dask to scale both compute- and memory-bound ML workloads.</p>

<p>Dask-ML provides an almost functionally equivalent library to scikit-learn, oftentimes calling scikit-learn with the additional awareness of task and data parallelism that Dask brings. Dask-ML is actively being developed by the community and will evolve to add more use cases and examples. Check the Dask documentation for the latest updates.</p>

<p>In addition, you have learned methods of parallelizing ML training with models from scikit-learn libraries by using joblib for compute-intensive workloads, and batched operations for data-intensive workloads, so that you can write any custom implementations yourself.</p>

<p>Finally, you have learned the use cases for Dask-SQL and its SQL ML statements in providing high-level abstraction for model creation, hyperparameter tuning, and inference.</p>

<p>Since ML can be very computation- and memory-heavy, it’s important to deploy your ML work on a correctly configured cluster and monitor the progress and output closely. We will cover deployment, profiling, and troubleshooting in the next chapter.</p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="id811"><sup><a href="ch11.xhtml#id811-marker">1</a></sup> For those inclined to think that writing data engineering code is “fun.”</p><p data-type="footnote" id="id816"><sup><a href="ch11.xhtml#id816-marker">2</a></sup> This is especially important for non-batch inference, where being able to use the same code can be of great benefit.</p><p data-type="footnote" id="id823"><sup><a href="ch11.xhtml#id823-marker">3</a></sup> For performance reasons—at the time of writing, Dask’s <code>OneHotEncoder</code> calls the <code>get_dummies</code> method from pandas, which is a slower implementation than scikit-learn’s <code>OneHotEncoder</code>. <code>Categorizer</code>, on the other hand, uses a Dask DataFrame aggregation method to scan through categories efficiently.</p><p data-type="footnote" id="id829"><sup><a href="ch11.xhtml#id829-marker">4</a></sup> Most linear models in Dask-ML use a base implementation of the Generalized Linear Model library that has been implemented for Dask. We have verified the code for mathematical correctness, but the writers of this library have not endorsed the use of their GLM library for prime time yet.</p><p data-type="footnote" id="id833"><sup><a href="ch11.xhtml#id833-marker">5</a></sup> Dask-ML version 2023.3.24; some of the generalized linear models used rely on dask-glm 0.1.0.</p><p data-type="footnote" id="id834"><sup><a href="ch11.xhtml#id834-marker">6</a></sup> Because it’s a simple reduce operation, we don’t need to preserve the chunking from previous steps.</p><p data-type="footnote" id="id837"><sup><a href="ch11.xhtml#id837-marker">7</a></sup> Dask-ML’s own documentation has more info on adaptive and approximation CV methods implemented and use cases.</p><p data-type="footnote" id="id840"><sup><a href="ch11.xhtml#id840-marker">8</a></sup> They note in the documentation that the following paper should be cited if using this method: S. Sievert, T. Augspurger, and M. Rocklin, “Better and Faster Hyperparameter Optimization with Dask,” <em>Proceedings of the 18th Python in Science Conference</em> (2019), <em><a href="https://doi.org/10.25080/Majora-7ddc1dd1-011" class="bare"><em class="hyperlink">https://doi.org/10.25080/Majora-7ddc1dd1-011</em></a></em>.</p></div></div></section></div></body></html>