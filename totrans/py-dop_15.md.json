["```py\nwith open(\"containers.txt\", \"w\") as file_to_write:\n  file_to_write.write(\"Pod/n\")\n  file_to_write.write(\"Service/n\")\n  file_to_write.write(\"Volume/n\")\n  file_to_write.write(\"Namespace/n\")\n```", "```py\ncat containers.txt\n\nPod\nService\nVolume\nNamespace\n```", "```py\nwith open(\"containers.txt\") as file_to_read:\n  lines = file_to_read.readlines()\n  print(lines)\n```", "```py\n['Pod\\n', 'Service\\n', 'Volume\\n', 'Namespace\\n']\n```", "```py\ndef process_file_lazily():\n  \"\"\"Uses generator to lazily process file\"\"\"\n\n  with open(\"containers.txt\") as file_to_read:\n    for line in file_to_read.readlines():\n      yield line\n```", "```py\n# Create generator object\npipeline = process_file_lazily()\n# convert to lowercase\nlowercase = (line.lower() for line in pipeline)\n# print first processed line\nprint(next(lowercase))\n```", "```py\npod\n```", "```py\nimport yaml\n\nkubernetes_components = {\n    \"Pod\": \"Basic building block of Kubernetes.\",\n    \"Service\": \"An abstraction for dealing with Pods.\",\n    \"Volume\": \"A directory accessible to containers in a Pod.\",\n    \"Namespaces\": \"A way to divide cluster resources between users.\"\n}\n\nwith open(\"kubernetes_info.yaml\", \"w\") as yaml_to_write:\n  yaml.safe_dump(kubernetes_components, yaml_to_write, default_flow_style=False)\n```", "```py\ncat kubernetes_info.yaml\n\nNamespaces: A way to divide cluster resources between users.\nPod: Basic building block of Kubernetes.\nService: An abstraction for dealing with Pods.\nVolume: A directory accessible to containers in a Pod.\n```", "```py\nimport yaml\n\nwith open(\"kubernetes_info.yaml\", \"rb\") as yaml_to_read:\n  result = yaml.safe_load(yaml_to_read)\n```", "```py\nimport pprint\npp = pprint.PrettyPrinter(indent=4)\npp.pprint(result)\n{   'Namespaces': 'A way to divide cluster resources between users.',\n    'Pod': 'Basic building block of Kubernetes.',\n    'Service': 'An abstraction for dealing with Pods.',\n    'Volume': 'A directory accessible to containers in a Pod.'}\n```", "```py\ntime shuf -n 100000 en.openfoodfacts.org.products.tsv\\\n    > 10k.sample.en.openfoodfacts.org.products.tsv\n    1.89s user 0.80s system 97% cpu 2.748 total\n```", "```py\nwc -l en.openfoodfacts.org.products.tsv\n  356002 en.openfoodfacts.org.products.tsv\n```", "```py\ndu -sh 10k.sample.en.openfoodfacts.org.products.tsv\n272M    10k.sample.en.openfoodfacts.org.products.tsv\n```", "```py\nimport asyncio\n\ndef send_async_firehose_events(count=100):\n    \"\"\"Async sends events to firehose\"\"\"\n\n    start = time.time()\n    client = firehose_client()\n    extra_msg = {\"aws_service\": \"firehose\"}\n    loop = asyncio.get_event_loop()\n    tasks = []\n    LOG.info(f\"sending aysnc events TOTAL {count}\",extra=extra_msg)\n    num = 0\n    for _ in range(count):\n        tasks.append(asyncio.ensure_future(put_record(gen_uuid_events(),\n                                                      client)))\n        LOG.info(f\"sending aysnc events: COUNT {num}/{count}\")\n        num +=1\n    loop.run_until_complete(asyncio.wait(tasks))\n    loop.close()\n    end = time.time()\n    LOG.info(\"Total time: {}\".format(end - start))\n```", "```py\npip3 install boto3 --target ../\npip3 install python-json-logger --target ../\n```", "```py\n'''\nDynamo to SQS\n'''\n\nimport boto3\nimport json\nimport sys\nimport os\n\nDYNAMODB = boto3.resource('dynamodb')\nTABLE = \"fang\"\nQUEUE = \"producer\"\nSQS = boto3.client(\"sqs\")\n\n#SETUP LOGGING\nimport logging\nfrom pythonjsonlogger import jsonlogger\n\nLOG = logging.getLogger()\nLOG.setLevel(logging.INFO)\nlogHandler = logging.StreamHandler()\nformatter = jsonlogger.JsonFormatter()\nlogHandler.setFormatter(formatter)\nLOG.addHandler(logHandler)\n\ndef scan_table(table):\n    '''Scans table and return results'''\n\n    LOG.info(f\"Scanning Table {table}\")\n    producer_table = DYNAMODB.Table(table)\n    response = producer_table.scan()\n    items = response['Items']\n    LOG.info(f\"Found {len(items)} Items\")\n    return items\n\ndef send_sqs_msg(msg, queue_name, delay=0):\n    '''Send SQS Message\n\n Expects an SQS queue_name and msg in a dictionary format.\n Returns a response dictionary.\n '''\n\n    queue_url = SQS.get_queue_url(QueueName=queue_name)[\"QueueUrl\"]\n    queue_send_log_msg = \"Send message to queue url: %s, with body: %s\" %\\\n        (queue_url, msg)\n    LOG.info(queue_send_log_msg)\n    json_msg = json.dumps(msg)\n    response = SQS.send_message(\n        QueueUrl=queue_url,\n        MessageBody=json_msg,\n        DelaySeconds=delay)\n    queue_send_log_msg_resp = \"Message Response: %s for queue url: %s\" %\\\n        (response, queue_url)\n    LOG.info(queue_send_log_msg_resp)\n    return response\n\ndef send_emissions(table, queue_name):\n    '''Send Emissions'''\n\n    items = scan_table(table=table)\n    for item in items:\n        LOG.info(f\"Sending item {item} to queue: {queue_name}\")\n        response = send_sqs_msg(item, queue_name=queue_name)\n        LOG.debug(response)\n\ndef lambda_handler(event, context):\n    '''\n Lambda entrypoint\n '''\n\n    extra_logging = {\"table\": TABLE, \"queue\": QUEUE}\n    LOG.info(f\"event {event}, context {context}\", extra=extra_logging)\n    send_emissions(table=TABLE, queue_name=QUEUE)\n\n```", "```py\n\nThis code does the following:\n\n1.  Grabs company names from Amazon DynamoDB.\n\n2.  Puts the names into Amazon SQS.\n\nTo test it, you can do a local test in Cloud9 ([Figure 15-10](#Figure-15-10)).\n\n![pydo 1510](assets/pydo_1510.png)\n\n###### Figure 15-10\\. Local test in Cloud9\n\nNext you can verify messages in SQS, as shown in [Figure 15-11](#Figure-15-11).\n\n![pydo 1511](assets/pydo_1511.png)\n\n###### Figure 15-11\\. SQS verification\n\nDon’t forget to set the correct IAM role! You need to assign the lambda an IAM role that can write messages to SQS, as shown in [Figure 15-12](#Figure-15-12).\n\n![pydo 1512](assets/pydo_1512.png)\n\n###### Figure 15-12\\. Permission error\n\n## Wiring Up CloudWatch Event Trigger\n\nThe final step to enable CloudWatch trigger does the following: enable timed execution of producer, and verify that messages flow into SQS, as shown in [Figure 15-13](#Figure-15-13).\n\n![pydo 1513](assets/pydo_1513.png)\n\n###### Figure 15-13\\. Configure timer\n\nYou can now see messages in the SQS queue ([Figure 15-14](#Figure-15-14)).\n\n![pydo 1514](assets/pydo_1514.png)\n\n###### Figure 15-14\\. SQS queue\n\n## Creating Event-Driven Lambdas\n\nWith the producer lambda out of the way, next up is to create an event-driven lambda that fires asynchronously upon every message in SQS (the consumer). The Lambda function can now fire in response to every SQS message ([Figure 15-15](#Figure-15-15)).\n\n![pydo 1515](assets/pydo_1515.png)\n\n###### Figure 15-15\\. Fire on SQS event\n\n## Reading Amazon SQS Events from AWS Lambda\n\nThe only task left is to write the code to consume the messages from SQS, process them using our API, and then write the results to S3:\n\n```", "```py\n\nYou can see that one easy way to download the files is to use the AWS CLI:\n\n```"]