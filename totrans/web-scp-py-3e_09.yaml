- en: Chapter 8\. Scrapy
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章 Scrapy
- en: '[Chapter 7](ch07.html#c-7) presented some techniques and patterns for building
    large, scalable, and (most important!) maintainable web crawlers. Although this
    is easy enough to do by hand, many libraries, frameworks, and even GUI-based tools
    will do this for you or at least try to make your life a little easier.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '[第7章](ch07.html#c-7)介绍了构建大型、可扩展且（最重要的！）可维护网络爬虫的一些技术和模式。尽管手动操作很容易做到，但许多库、框架甚至基于GUI的工具都可以为您完成这些工作，或者至少试图让您的生活变得更轻松。'
- en: Since its release in 2008, Scrapy has quickly grown into the largest and best-maintained
    web scraping framework in Python. It is currently maintained by Zyte (formerly
    Scrapinghub).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 自2008年发布以来，Scrapy迅速发展成为Python中最大且维护最好的网络爬虫框架。目前由Zyte（前身为Scrapinghub）维护。
- en: 'One of the challenges of writing web crawlers is that you’re often performing
    the same tasks again and again: find all links on a page, evaluate the difference
    between internal and external links, and go to new pages. These basic patterns
    are useful to know and to be able to write from scratch, but the Scrapy library
    handles many of these details for you.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 编写网络爬虫的一个挑战是经常要重复执行相同的任务：查找页面上的所有链接，评估内部和外部链接之间的差异，并浏览新页面。这些基本模式是有用的，也可以从头编写，但Scrapy库可以为您处理其中的许多细节。
- en: Of course, Scrapy isn’t a mind reader. You still need to define page templates,
    give it locations to start scraping from, and define URL patterns for the pages
    that you’re looking for. But in these cases, it provides a clean framework to
    keep your code organized.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，Scrapy不能读心。您仍然需要定义页面模板，指定要开始抓取的位置，并为您正在寻找的页面定义URL模式。但在这些情况下，它提供了一个清晰的框架来保持代码的组织。
- en: Installing Scrapy
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Scrapy
- en: Scrapy offers the tool for [download](http://scrapy.org/download/) from its
    website, as well as instructions for installing Scrapy with third-party installation
    managers such as pip.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Scrapy提供了从其网站[下载](http://scrapy.org/download/)的工具，以及使用pip等第三方安装管理器安装Scrapy的说明。
- en: 'Because of its relatively large size and complexity, Scrapy is not usually
    a framework that can be installed in the traditional way with:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其相对较大的大小和复杂性，Scrapy通常不是可以通过传统方式安装的框架：
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note that I say “usually” because, though it is theoretically possible, I usually
    run into one or more tricky dependency issues, version mismatches, and unsolvable
    bugs.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我说“通常”是因为尽管理论上可能存在，但我通常会遇到一个或多个棘手的依赖问题、版本不匹配和无法解决的错误。
- en: If you’re determined to install Scrapy from pip, using a virtual environment
    is highly recommended (see [“Keeping Libraries Straight with Virtual Environments”](ch04.html#KLSwVE_01)
    for more on virtual environments).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您决定从pip安装Scrapy，则强烈建议使用虚拟环境（有关虚拟环境的更多信息，请参阅[“使用虚拟环境保持库的整洁”](ch04.html#KLSwVE_01)）。
- en: The installation method I prefer is through the [Anaconda package manager](https://docs.continuum.io/anaconda/).
    Anaconda is a product from the company Continuum, designed to reduce friction
    when it comes to finding and installing popular Python data science packages.
    Many of the packages it manages, such as NumPy and NLTK, will be used in later
    chapters as well.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我偏爱的安装方法是通过[Anaconda包管理器](https://docs.continuum.io/anaconda/)。Anaconda是由Continuum公司推出的产品，旨在简化查找和安装流行的Python数据科学包的过程。后续章节将使用它管理的许多包，如NumPy和NLTK。
- en: 'After you install Anaconda, you can install Scrapy by using this command:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完Anaconda后，您可以使用以下命令安装Scrapy：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If you run into issues, or need up-to-date information, check out the [Scrapy](https://doc.scrapy.org/en/latest/intro/install.html)
    installation guide for more information.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果遇到问题或需要获取最新信息，请查阅[Scrapy](https://doc.scrapy.org/en/latest/intro/install.html)安装指南获取更多信息。
- en: Initializing a New Spider
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初始化新的Spider
- en: Once you’ve installed the Scrapy framework, a small amount of setup needs to
    be done for each spider. A *spider* is a Scrapy project that, like its arachnid
    namesake, is designed to crawl webs. Throughout this chapter, I use “spider” to
    describe a Scrapy project in particular, and “crawler” to mean “any generic program
    that crawls the web, using Scrapy or not.”
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完Scrapy框架后，每个Spider需要进行少量设置。*Spider*是一个Scrapy项目，类似于其命名的蜘蛛，专门设计用于爬取网络。在本章中，我使用“Spider”来描述特定的Scrapy项目，“crawler”指“使用Scrapy或不使用任何通用程序爬取网络”的任何通用程序。
- en: 'To create a new spider in the current directory, run the following from the
    command line:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 要在当前目录下创建一个新的Spider，请从命令行运行以下命令：
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This creates a new subdirectory in the directory the project was created in,
    with the title *wikiSpider*. Inside this directory is the following file structure:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这会在项目创建的目录中创建一个新的子目录，名为*wikiSpider*。在这个目录中，有以下文件结构：
- en: '*scrapy.cfg*'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*scrapy.cfg*'
- en: '*wikiSpider*'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*wikiSpider*'
- en: '*spiders*'
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*spiders*'
- en: '*__init.py__*'
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*__init.py__*'
- en: '*items.py*'
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*items.py*'
- en: '*middlewares.py*'
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*middlewares.py*'
- en: '*pipelines.py*'
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*pipelines.py*'
- en: '*settings.py*'
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*settings.py*'
- en: '*__init.py__*'
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*__init.py__*'
- en: These Python files are initialized with stub code to provide a fast means of
    creating a new spider project. Each section in this chapter works with this *wikiSpider*
    project.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这些Python文件以存根代码初始化，以提供创建新爬虫项目的快速方式。本章中的每个部分都与这个*wikiSpider*项目一起工作。
- en: Writing a Simple Scraper
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编写一个简单的爬虫
- en: 'To create a crawler, you will add a new file inside the child *spiders* directory
    at *wiki​S⁠pider/wikiSpider/spiders/article.py*. This is where all the spiders,
    or things that extend scrapy.Spider will go. In your newly created *article.py* file,
    write:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个爬虫，您将在*wiki​S⁠pider/wikiSpider/spiders/article.py*的子*spiders*目录中添加一个新文件。这是所有爬虫或扩展scrapy.Spider的内容的地方。在您新创建的*article.py*文件中，写入：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The name of this class (`ArticleSpider`) does not reference “wiki” or “Wikipedia”
    at all, indicating that this class in particular is responsible for spidering
    through only article pages, under the broader category of *wikiSpider*, which
    you may later want to use to search for other page types.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 此类的名称（`ArticleSpider`）根本没有涉及“wiki”或“维基百科”，这表明此类特别负责爬取文章页面，属于更广泛的*wikiSpider*类别，稍后您可能希望使用它来搜索其他页面类型。
- en: For large sites with many types of content, you might have separate Scrapy items
    for each type (blog posts, press releases, articles, etc.), each with different
    fields but all running under the same Scrapy project. The name of each spider
    must be unique within the project.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于内容类型繁多的大型网站，您可能会为每种类型（博客文章、新闻稿、文章等）设置单独的Scrapy项目。每个爬虫的名称在项目内必须是唯一的。
- en: 'The other key things to notice about this spider are the two functions `start_requests` and `parse`:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 关于此爬虫的另外两个关键点是函数`start_requests`和`parse`：
- en: '`start_requests`'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`start_requests`'
- en: A Scrapy-defined entry point to the program used to generate `Request` objects
    that Scrapy uses to crawl the website.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Scrapy定义的程序入口点用于生成`Request`对象，Scrapy用它来爬取网站。
- en: '`parse`'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`parse`'
- en: A callback function defined by the user and passed to the `Request` object with `callback=self.parse`.
    Later, you’ll look at more powerful things that can be done with the `parse` function,
    but for now it prints the title of the page.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 由用户定义的回调函数，并通过`Request`对象传递给`callback=self.parse`。稍后，您将看到更多可以使用`parse`函数完成的功能，但现在只打印页面的标题。
- en: 'You can run this `article` spider by navigating to the outer *wikiSpider* directory
    and running:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过转到外部*wikiSpider*目录并运行以下命令来运行这个`article`爬虫：
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The default Scrapy output is fairly verbose. Along with debugging information,
    this should print out lines like:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的Scrapy输出相当冗长。除了调试信息外，这应该会打印出类似以下的行：
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The scraper goes to the three pages listed as the URLs, gathers information,
    and then terminates.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 爬虫访问列出为URL的三个页面，收集信息，然后终止。
- en: Spidering with Rules
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用规则进行爬取
- en: The spider in the previous section isn’t much of a crawler, confined to scraping
    only the list of URLs it’s provided. It has no ability to seek new pages on its
    own. To turn it into a fully fledged crawler, you need to use the `CrawlSpider`
    class provided by Scrapy.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节中的爬虫并不是一个真正的爬虫，只能限于仅抓取提供的URL列表。它没有能力自行查找新页面。要将其转变为一个完全成熟的爬虫，你需要使用Scrapy提供的`CrawlSpider`类。
- en: Code Organization Within the GitHub Repository
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在GitHub仓库中的代码组织
- en: Unfortunately, the Scrapy framework cannot be run easily from within a Jupyter
    notebook, making a linear progression of code difficult to capture. For the purpose
    of presenting all code samples in the text, the scraper from the previous section
    is stored in the *article.py* file, while the following example, creating a Scrapy
    spider that traverses many pages, is stored in *articles.py* (note the use of
    the plural).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，Scrapy框架不能在Jupyter笔记本中轻松运行，这使得线性编码难以实现。为了在文本中展示所有代码示例的目的，前一节中的爬虫存储在*article.py*文件中，而下面的示例，创建一个遍历多个页面的Scrapy爬虫，存储在*articles.py*中（注意使用了复数形式）。
- en: Later examples will also be stored in separate files, with new filenames given
    in each section. Make sure you are using the correct filename when running these
    examples.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 后续示例也将存储在单独的文件中，每个部分都会给出新的文件名。运行这些示例时，请确保使用正确的文件名。
- en: 'This class can be found in the spiders file *articles.py* in the GitHub repository:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 此类可以在GitHub仓库的spiders文件*articles.py*中找到：
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This new `ArticleSpider` extends the `CrawlSpider` class. Rather than providing
    a `start_requests` function, it provides a list of `start_urls` and `allowed_domains`.
    This tells the spider where to start crawling from and whether it should follow
    or ignore a link based on the domain.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这个新的`ArticleSpider`扩展了`CrawlSpider`类。它不是提供一个`start_requests`函数，而是提供一个`start_urls`和`allowed_domains`列表。这告诉爬虫从哪里开始爬取，并根据域名是否应跟踪或忽略链接。
- en: A list of `rules` is also provided. This provides further instructions on which
    links to follow or ignore (in this case, you are allowing all URLs with the regular
    expression `.*`).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 还提供了一个`rules`列表。这提供了进一步的说明，指导哪些链接应该跟踪或忽略（在本例中，允许所有使用正则表达式`.*`的URL）。
- en: In addition to extracting the title and URL on each page, a couple of new items
    have been added. The text content of each page is extracted using an XPath selector.
    XPath is often used when retrieving text content including text in child tags
    (for example, an `<a>` tag inside a block of text). If you use the CSS selector
    to do this, all text within child tags will be ignored.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 除了提取每个页面的标题和URL之外，还添加了一些新的项目。使用XPath选择器提取每个页面的文本内容。XPath在检索文本内容时经常被使用，包括文本在子标签中的情况（例如，在一段文本中的`<a>`标签内）。如果您使用CSS选择器来做这件事，所有子标签中的文本都将被忽略。
- en: The last updated date string is also parsed from the page footer and stored
    in the `lastUpdated` variable.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 也从页脚解析并存储了最后更新日期字符串到`lastUpdated`变量中。
- en: 'You can run this example by navigating to the *wikiSpider *directory and running:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过导航到*wikiSpider*目录并运行以下命令来运行此示例：
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Warning: This Will Run Forever'
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 警告：这将永远运行下去
- en: While this new spider runs in the command line in the same way as the simple
    spider built in the previous section, it will not terminate (at least not for
    a very, very long time) until you halt execution by using Ctrl-C or by closing
    the terminal. Please be kind to Wikipedia’s server load and do not run it for
    long.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个新的爬虫在命令行中的运行方式与前一节中构建的简单爬虫相同，但它不会终止（至少不会很长时间），直到您使用Ctrl-C终止执行或关闭终端为止。请注意，对Wikipedia服务器负载要友善，不要长时间运行它。
- en: 'When run, this spider traverses *wikipedia.org*, following all links under
    the domain *wikipedia.org*, printing titles of pages, and ignoring all external
    (offsite) links:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此爬虫时，它将遍历*wikipedia.org*，跟踪* wikipedia.org*域下的所有链接，打印页面标题，并忽略所有外部（站外）链接：
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This is a pretty good crawler so far, but it could use a few limits. Instead
    of just visiting article pages on Wikipedia, it’s free to roam to nonarticle pages
    as well, such as:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，这是一个相当不错的爬虫，但它可以使用一些限制。它不仅可以访问维基百科上的文章页面，还可以自由漫游到非文章页面，例如：
- en: '[PRE9]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let’s take a closer look at the line by using Scrapy’s `Rule` and `LinkExtractor`:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用Scrapy的`Rule`和`LinkExtractor`来仔细查看这一行：
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This line provides a list of Scrapy `Rule` objects that define the rules that
    all found links are filtered through. When multiple rules are in place, each link
    is checked against the rules, in order. The first rule that matches is the one
    that is used to determine how the link is handled. If the link doesn’t match any
    rules, it is ignored.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 此行提供了一个Scrapy `Rule`对象的列表，该列表定义了所有找到的链接通过的规则。当存在多个规则时，每个链接都会根据规则进行检查，按顺序进行。第一个匹配的规则将用于确定如何处理链接。如果链接不符合任何规则，则会被忽略。
- en: 'A `Rule` can be provided with four arguments:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 可以为`Rule`提供四个参数：
- en: '`link_extractor`'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`link_extractor`'
- en: The only mandatory argument, a `LinkExtractor` object.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一必需的参数，一个`LinkExtractor`对象。
- en: '`callback`'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`callback`'
- en: The function that should be used to parse the content on the page.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 应该用于解析页面内容的函数。
- en: '`cb_kwargs`'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`cb_kwargs`'
- en: 'A dictionary of arguments to be passed to the callback function. This dictionary
    is formatted as `{arg_name1: arg_value1, arg_name2: arg_value2}` and can be a
    handy tool for reusing the same parsing functions for slightly different tasks.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '要传递给回调函数的参数字典。这个字典格式为`{arg_name1: arg_value1, arg_name2: arg_value2}`，对于稍微不同的任务重用相同的解析函数非常方便。'
- en: '`follow`'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`follow`'
- en: Indicates whether you want links found at that page to be included in a future
    crawl. If no callback function is provided, this defaults to `True` (after all,
    if you’re not doing anything with the page, it makes sense that you’d at least
    want to use it to continue crawling through the site). If a callback function
    is provided, this defaults to `False`.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 指示是否希望在未来的爬行中包含在该页面上找到的链接。如果未提供回调函数，则默认为`True`（毕竟，如果您对页面没有做任何事情，那么至少您可能想要继续通过站点进行爬行）。如果提供了回调函数，则默认为`False`。
- en: '`LinkExtractor` is a simple class designed solely to recognize and return links
    in a page of HTML content based on the rules provided to it. It has a number of
    arguments that can be used to accept or deny a link based on CSS and XPath selectors,
    tags (you can look for links in more than just anchor tags!), domains, and more.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`LinkExtractor`是一个简单的类，专门设计用于识别并返回基于提供的规则在HTML内容页中的链接。它具有许多参数，可用于接受或拒绝基于CSS和XPath选择器、标签（您不仅可以查找锚点标签中的链接！）、域名等的链接。'
- en: The `LinkExtractor` class can even be extended, and custom arguments can be
    created. See Scrapy’s [documentation on link extractors](https://doc.scrapy.org/en/latest/topics/link-extractors.html)
    for more information.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`LinkExtractor`类甚至可以扩展，并且可以创建自定义参数。详见Scrapy的[链接提取器文档](https://doc.scrapy.org/en/latest/topics/link-extractors.html)获取更多信息。'
- en: 'Despite all the flexible features of the `LinkExtractor` class, the most common
    arguments you’ll use are these:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管`LinkExtractor`类具有灵活的特性，但您最常使用的参数是这些：
- en: '`allow`'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`allow`'
- en: Allow all links that match the provided regular expression.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 允许所有符合提供的正则表达式的链接。
- en: '`deny`'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`deny`'
- en: Deny all links that match the provided regular expression.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 拒绝所有符合提供的正则表达式的链接。
- en: 'Using two separate `Rule` and `LinkExtractor` classes with a single parsing
    function, you can create a spider that crawls Wikipedia, identifying all article
    pages and flagging nonarticle pages (*articleMoreRules.py*):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 使用两个单独的`Rule`和`LinkExtractor`类以及一个解析函数，您可以创建一个爬虫，该爬虫可以爬取维基百科，识别所有文章页面，并标记非文章页面（*articleMoreRules.py*）：
- en: '[PRE11]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Recall that the rules are applied to each link in the order that they are presented
    in the list. All article pages (pages that start with */wiki/* and do not contain
    a colon) are passed to the `parse_items` function first with the default parameter
    `is_article=True`. Then all the other nonarticle links are passed to the `parse_items`
    function with the argument `is_article=False`.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，规则适用于列表中呈现的每个链接。首先将所有文章页面（以*/wiki/*开头且不包含冒号的页面）传递给`parse_items`函数，并使用默认参数`is_article=True`。然后将所有其他非文章链接传递给`parse_items`函数，并使用参数`is_article=False`。
- en: Of course, if you’re looking to collect only article-type pages and ignore all
    others, this approach would be impractical. It would be much easier to ignore
    pages that don’t match the article URL pattern and leave out the second rule (and
    the `is_article` variable) altogether. However, this type of approach may be useful
    in odd cases where information from the URL, or information collected during crawling,
    impacts the way the page should be parsed.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，如果您只想收集文章类型页面并忽略所有其他页面，这种方法将不太实际。忽略不匹配文章URL模式的页面，并完全省略第二个规则（以及`is_article`变量），会更加简单。但在URL中收集的信息或在爬取过程中收集的信息影响页面解析方式的奇特情况下，这种方法可能会很有用。
- en: Creating Items
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建项目
- en: So far, you’ve looked at many ways of finding, parsing, and crawling websites
    with Scrapy, but Scrapy also provides useful tools to keep your collected items
    organized and stored in custom objects with well-defined fields.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经学习了许多在Scrapy中查找、解析和爬取网站的方法，但Scrapy还提供了有用的工具，可以将您收集的项目组织并存储在具有明确定义字段的自定义对象中。
- en: To help organize all the information you’re collecting, you need to create an
    `Article` object. Define a new item called `Article` inside the *items.py* file.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助组织您收集的所有信息，您需要创建一个名为`Article`的对象。在*items.py*文件内定义一个新的`Article`项目。
- en: 'When you open the *items.py* file, it should look like this:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 当您打开*items.py*文件时，它应该看起来像这样：
- en: '[PRE12]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Replace this default `Item` stub with a new `Article` class extending `scrapy.Item`:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 用一个新的扩展`scrapy.Item`的`Article`类替换此默认的`Item`存根：
- en: '[PRE13]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You are defining four fields that will be collected from each page: URL, title,
    text content, and the date the page was last edited.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 您正在定义将从每个页面收集的四个字段：URL、标题、文本内容和页面最后编辑日期。
- en: If you are collecting data for multiple page types, you should define each separate
    type as its own class in *items.py*. If your items are large, or you start to
    move more parsing functionality into your item objects, you may also wish to extract
    each item into its own file. While the items are small, however, I like to keep
    them in a single file.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在收集多种页面类型的数据，应将每种单独类型定义为*items.py*中的一个单独类。如果您的项目很大，或者您开始将更多的解析功能移动到项目对象中，您可能还希望将每个项目提取到自己的文件中。然而，当项目很小时，我喜欢将它们保存在单个文件中。
- en: 'In the file *articleItems.py*, note the changes that were made to the `ArticleSpider`
    class in order to create the new `Article` item:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在文件*articleItems.py*中，请注意在 `ArticleSpider` 类中所做的更改，以创建新的 `Article` 项目：
- en: '[PRE14]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: When this file is run with
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此文件时
- en: '[PRE15]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'it will output the usual Scrapy debugging data along with each article item
    as a Python dictionary:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 它将输出通常的 Scrapy 调试数据以及每个文章项目作为 Python 字典：
- en: '[PRE16]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Using Scrapy `Items` isn’t just for promoting good code organization or laying
    things out in a readable way. Items provide many tools for outputting and processing
    data, covered in the next sections.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Scrapy `Items` 并不仅仅是为了促进良好的代码组织或以可读的方式布置事物。项目提供了许多工具，用于输出和处理数据，这些将在接下来的部分中详细介绍。
- en: Outputting Items
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输出项目
- en: 'Scrapy uses the `Item` objects to determine which pieces of information it
    should save from the pages it visits. This information can be saved by Scrapy
    in a variety of ways, such as CSV, JSON, or XML files, using the following commands:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Scrapy 使用 `Item` 对象来确定应从其访问的页面中保存哪些信息。这些信息可以由 Scrapy 以各种方式保存，例如CSV、JSON或XML文件，使用以下命令：
- en: '[PRE17]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Each of these runs the scraper `articleItems` and writes the output in the specified
    format to the provided file. This file will be created if it does not exist already.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 每次运行抓取器 `articleItems` 并将输出按指定格式写入提供的文件。如果不存在，则将创建此文件。
- en: You may have noticed that in the articles the spider created in previous examples,
    the text variable is a list of strings rather than a single string. Each string
    in this list represents text inside a single HTML element, whereas the content
    inside `<div id="mw-content-text">`, from which you are collecting the text data,
    is composed of many child elements.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到，在先前示例中爬虫创建的文章中，文本变量是一个字符串列表，而不是单个字符串。此列表中的每个字符串表示单个HTML元素内的文本，而`<div
    id="mw-content-text">`内的内容（您正在收集的文本数据）由许多子元素组成。
- en: Scrapy manages these more complex values well. In the CSV format, for example,
    it converts lists to strings and escapes all commas so that a list of text displays
    in a single CSV cell.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Scrapy 很好地管理这些更复杂的值。例如，在CSV格式中，它将列表转换为字符串，并转义所有逗号，以便文本列表显示在单个CSV单元格中。
- en: 'In XML, each element of this list is preserved inside child value tags:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在XML中，此列表的每个元素都被保留在子值标签中：
- en: '[PRE18]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In the JSON format, lists are preserved as lists.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在JSON格式中，列表被保留为列表。
- en: Of course, you can use the `Item` objects yourself and write them to a file
    or a database in whatever way you want, simply by adding the appropriate code
    to the parsing function in the crawler.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，您可以自己使用 `Item` 对象，并通过将适当的代码添加到爬虫的解析函数中，以任何您想要的方式将它们写入文件或数据库。
- en: The Item Pipeline
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 项目管道
- en: Although Scrapy is single threaded, it is capable of making and handling many
    requests asynchronously. This makes it faster than the scrapers written so far
    in this book, although I have always been a firm believer that faster is not always
    better when it comes to web scraping.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Scrapy 是单线程的，但它能够异步地进行许多请求的处理。这使其比本书中迄今编写的爬虫更快，尽管我一直坚信，在涉及网络抓取时，更快并不总是更好。
- en: The web server for the site you are trying to scrape must handle each of these
    requests, and it’s important to be a good citizen and evaluate whether this sort
    of server hammering is appropriate (or even wise for your own self-interests,
    as many websites have the ability and the will to block what they might see as
    malicious scraping activity). For more information about the ethics of web scraping,
    as well as the importance of appropriately throttling scrapers, see [Chapter 19](ch19.html#c-19).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 您正在尝试抓取的网站的网络服务器必须处理这些请求中的每一个，评估这种服务器压力是否合适（甚至对您自己的利益是否明智，因为许多网站有能力和意愿阻止它们可能认为是恶意的抓取活动）。有关网络抓取伦理以及适当地调节抓取程序重要性的更多信息，请参阅[第19章](ch19.html#c-19)。
- en: With that said, using Scrapy’s item pipeline can improve the speed of your web
    scraper even further by performing all data processing while waiting for requests
    to be returned, rather than waiting for data to be processed before making another
    request. This type of optimization can even be necessary when data processing
    requires a great deal of time or processor-heavy calculations.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，使用 Scrapy 的项目管道可以通过在等待请求返回时执行所有数据处理来进一步提高网络爬虫的速度，而不是等待数据处理完成后再发出另一个请求。当数据处理需要大量时间或处理器密集型计算时，这种优化甚至可能是必需的。
- en: 'To create an item pipeline, revisit the *settings.py* file created at the beginning
    of the chapter. You should see the following commented lines:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个项目管道，请重新访问本章开头创建的 *settings.py* 文件。您应该看到以下被注释的行：
- en: '[PRE19]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Uncomment the last three lines and replace them with:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 取消对最后三行的注释，并替换为：
- en: '[PRE20]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This provides a Python class, `wikiSpider.pipelines.WikispiderPipeline`, that
    will be used to process the data, as well as an integer that represents the order
    in which to run the pipeline if there are multiple processing classes. Although
    any integer can be used here, the numbers 0–1,000 are typically used and will
    be run in ascending order.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这提供了一个 Python 类 `wikiSpider.pipelines.WikispiderPipeline`，用于处理数据，以及一个表示运行管道顺序的整数。虽然可以使用任何整数，但通常使用
    0 到 1000 的数字，并将按升序运行。
- en: 'Now you need to add the pipeline class and rewrite your original spider so
    that the spider collects data and the pipeline does the heavy lifting of the data
    processing. It might be tempting to write the `parse_items` method in your original
    spider to return the response and let the pipeline create the `Article` object:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您需要添加管道类并重写原始爬虫，以便爬虫收集数据，管道执行数据处理的繁重工作。也许会诱人地在原始爬虫中编写 `parse_items` 方法以返回响应并让管道创建
    `Article` 对象：
- en: '[PRE21]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'However, the Scrapy framework does not allow this, and an `Item` object (such
    as an `Article`, which extends `Item`) must be returned. So the goal of `parse_items`
    is now to extract the raw data, doing as little processing as possible, so that
    it can be passed to the pipeline:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Scrapy 框架不允许这样做，必须返回一个 `Item` 对象（例如扩展 `Item` 的 `Article`）。因此，现在 `parse_items`
    的目标是提取原始数据，尽可能少地进行处理，以便可以传递到管道：
- en: '[PRE22]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This file is saved as *articlePipelines.py* in the GitHub repository.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 此文件保存为 *articlePipelines.py* 在 GitHub 仓库中。
- en: 'Of course, now you need to tie the *pipelines.py* file and the updated spider
    together by adding the pipeline. When the Scrapy project was first initialized,
    a file was created at *wikiSpider/wikiSpider/pipelines.py*:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，现在您需要通过添加管道将 *pipelines.py* 文件和更新的爬虫联系在一起。当最初初始化 Scrapy 项目时，会在 *wikiSpider/wikiSpider/pipelines.py*
    创建一个文件：
- en: '[PRE23]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This stub class should be replaced with your new pipeline code. In previous
    sections, you’ve been collecting two fields in a raw format, and these could use
    additional processing: `lastUpdated` (which is a badly formatted string object
    representing a date) and `text` (a messy array of string fragments).'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这个存根类应该用您的新管道代码替换。在之前的章节中，您已经以原始格式收集了两个字段，并且这些字段可能需要额外的处理：`lastUpdated`（一个糟糕格式化的表示日期的字符串对象）和
    `text`（一个杂乱的字符串片段数组）。
- en: The following should be used to replace the stub code in *wikiSpider/wikiSpider/​pipe⁠lines.py:*
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 以下应该用于替换 *wikiSpider/wikiSpider/pipelines.py* 中的存根代码：
- en: '[PRE24]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The class `WikispiderPipeline` has a method `process_item` that takes in an
    `Article` object, parses the `lastUpdated` string into a Python `datetime` object,
    and cleans and joins the text into a single string from a list of strings.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 类 `WikispiderPipeline` 具有一个 `process_item` 方法，接受一个 `Article` 对象，将 `lastUpdated`
    字符串解析为 Python 的 `datetime` 对象，并从字符串列表中清理和连接文本为单个字符串。
- en: '`process_item` is a mandatory method for every pipeline class. Scrapy uses
    this method to asynchronously pass `Items` that are collected by the spider. The
    parsed `Article` object that is returned here will be logged or printed by Scrapy
    if, for example, you are outputting items to JSON or CSV, as was done in the previous
    section.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`process_item` 是每个管道类的必需方法。Scrapy 使用此方法异步传递由爬虫收集的 `Items`。例如，如果您在上一节中输出到 JSON
    或 CSV，这里返回的解析的 `Article` 对象将由 Scrapy 记录或打印。'
- en: 'You now have two choices when it comes to deciding where to do your data processing:
    the `parse_items` method in the spider, or the `process_items` method in the pipeline.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在决定数据处理位置时，您有两个选择：在爬虫中的 `parse_items` 方法或在管道中的 `process_items` 方法。
- en: 'Multiple pipelines with different tasks can be declared in the *settings.py*
    file. However, Scrapy passes all items, regardless of item type, to each pipeline
    in order. Item-specific parsing may be better handled in the spider, before the
    data hits the pipeline. However, if this parsing takes a long time, you may want
    to consider moving it to the pipeline (where it can be processed asynchronously)
    and adding a check on the item type:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在 *settings.py* 文件中声明具有不同任务的多个管道。但是，Scrapy 将所有项目不论类型都传递给每个管道以便处理。在数据传入管道之前，可能更好地在爬虫中处理特定项目的解析。但是，如果此解析需要很长时间，可以考虑将其移到管道中（可以异步处理），并添加一个项目类型检查：
- en: '[PRE25]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Which processing to do and where to do it is an important consideration when
    it comes to writing Scrapy projects, especially large ones.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写 Scrapy 项目时，特别是在处理大型项目时，确定要执行的处理及其执行位置是一个重要考虑因素。
- en: Logging with Scrapy
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Scrapy 记录日志
- en: 'The debug information generated by Scrapy can be useful, but, as you’ve likely
    noticed, it is often too verbose. You can easily adjust the level of logging by
    adding a line to the *settings.py* file in your Scrapy project:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Scrapy 生成的调试信息可能很有用，但你可能已经注意到，它通常过于冗长。你可以通过在 Scrapy 项目的 *settings.py* 文件中添加一行来轻松调整日志级别：
- en: '[PRE26]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Scrapy uses a standard hierarchy of logging levels, as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Scrapy 使用标准的日志级别层次结构，如下：
- en: '`CRITICAL`'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CRITICAL`'
- en: '`ERROR`'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ERROR`'
- en: '`WARNING`'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WARNING`'
- en: '`DEBUG`'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DEBUG`'
- en: '`INFO`'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`INFO`'
- en: If logging is set to `ERROR`, only `CRITICAL` and `ERROR` logs will be displayed.
    If logging is set to `INFO`, all logs will be displayed, and so on.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如果日志级别设置为 `ERROR`，则只会显示 `CRITICAL` 和 `ERROR` 级别的日志。如果设置为 `INFO`，则会显示所有日志，依此类推。
- en: 'In addition to controlling logging through the *settings.py* file, you can
    control where the logs go from the command line. To output logs to a separate
    logfile instead of the terminal, define a logfile when running from the command
    line:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 除了通过 *settings.py* 文件控制日志记录外，还可以通过命令行控制日志输出位置。在命令行中运行时，可以定义一个日志文件，将日志输出到该文件而不是终端：
- en: '[PRE27]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This creates a new logfile, if one does not exist, in your current directory
    and outputs all logs to it, leaving your terminal clear to display only the Python
    print statements you manually add.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在当前目录中创建一个新的日志文件（如果不存在），并将所有日志输出到该文件，使得你的终端只显示手动添加的 Python 打印语句。
- en: More Resources
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多资源
- en: Scrapy is a powerful tool that handles many problems associated with crawling
    the web. It automatically gathers all URLs and compares them against predefined
    rules, makes sure all URLs are unique, normalizes relative URLs where needed,
    and recurses to go more deeply into pages.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: Scrapy 是一个强大的工具，可以处理与网络爬取相关的许多问题。它会自动收集所有 URL，并将它们与预定义的规则进行比较，确保所有 URL 都是唯一的，在必要时会对相对
    URL 进行标准化，并递归深入页面。
- en: I encourage you to check out the [Scrapy documentation](https://doc.scrapy.org/en/latest/news.html)
    as well as [Scrapy’s official tutorial pages](https://docs.scrapy.org/en/latest/intro/tutorial.html),
    which provide a comprehensive discourse on the framework.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 鼓励您查阅[Scrapy 文档](https://doc.scrapy.org/en/latest/news.html)以及[Scrapy 的官方教程页面](https://docs.scrapy.org/en/latest/intro/tutorial.html)，这些资源详细介绍了该框架。
- en: Scrapy is an extremely large, sprawling library with many features. Its features
    work together seamlessly but have many areas of overlap that allow users to easily
    develop their own particular style within it. If there’s something you’d like
    to do with Scrapy that has not been mentioned here, there is likely a way (or
    several) to do it!
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: Scrapy 是一个非常庞大的库，具有许多功能。它的特性可以无缝协同工作，但也存在许多重叠的区域，使用户可以轻松地在其中开发自己的风格。如果你想要使用
    Scrapy 做一些这里未提到的事情，很可能有一种（或几种）方法可以实现！
