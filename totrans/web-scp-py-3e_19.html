<html><head></head><body><section data-pdf-bookmark="Chapter 17. Avoiding Scraping Traps" data-type="chapter" epub:type="chapter"><div class="chapter" id="c-17">&#13;
<h1><span class="label">Chapter 17. </span>Avoiding Scraping Traps</h1>&#13;
&#13;
<p>Few things are more frustrating than scraping a site, viewing the output, and not seeing the data that’s so clearly visible in your browser. Or submitting a form that should be perfectly fine but gets denied by the web server. Or getting your IP address blocked by a site for unknown reasons.</p>&#13;
&#13;
<p>These are some of the most difficult bugs to solve, not only because they can be so unexpected (a script that works just fine on one site might not work at all on another, seemingly identical, site), but because they purposefully don’t have any telltale error messages or stack traces to use. You’ve been identified as a bot, rejected, and you don’t know why.</p>&#13;
&#13;
<p>In this book, I’ve written about a lot of ways to do tricky things on websites, including submitting forms, extracting and cleaning difficult data, and executing JavaScript. This chapter is a bit of a catchall in that the techniques stem from a wide variety of subjects. However, they all have something in common: they are meant to overcome an obstacle put in place for the sole purpose of preventing automated scraping of a site.</p>&#13;
&#13;
<p>Regardless of how immediately useful this information is to you at the moment, I highly recommend you at least skim this chapter. You never know when it might help you solve a difficult bug or prevent a problem altogether.</p>&#13;
&#13;
<section data-pdf-bookmark="A Note on Ethics" data-type="sect1"><div class="sect1" id="id109">&#13;
<h1>A Note on Ethics</h1>&#13;
&#13;
<p>In the first few chapters of <a contenteditable="false" data-primary="ethics" data-type="indexterm" id="etcis"/>this book, I discussed the legal gray area that web scraping inhabits, as well as some of the ethical and legal guidelines to scrape by. To be honest, this chapter is, ethically, perhaps the most difficult one for me to write. My websites have been plagued by bots, spammers, web scrapers, and all manner of unwanted virtual guests, as perhaps yours have been. So why teach people how to build better bots?</p>&#13;
&#13;
<p>I believe this chapter is important to include for a few reasons:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>There are perfectly ethical and legally sound reasons to scrape some websites that do not want to be scraped. In a previous job I had as a web scraper, I performed automated collection of information from websites that were publishing clients’ names, addresses, telephone numbers, and other personal information to the internet without their consent. I used the scraped information to make legal requests to the websites to remove this information. To avoid competition, these sites vigilantly guarded this information from scrapers. However, my work to ensure the anonymity of my company’s clients (some of whom had stalkers, were the victims of domestic violence, or had other very good reasons to want to keep a low profile) made a compelling case for web scraping, and I was grateful that I had the skills necessary to do the job.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Although it is almost impossible to build a “scraper proof” site (or at least one that can still be easily accessed by legitimate users), I hope that the information in this chapter will help those wanting to defend their websites against malicious attacks. Throughout, I will point out some of the weaknesses in each web scraping technique, which you can use to defend your own site. Keep in mind that most bots on the web today are merely doing a broad scan for information and vulnerabilities; employing even a couple of simple techniques described in this chapter likely will thwart 99% of them. However, they are getting more sophisticated every month, and it’s best to be prepared.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Like most programmers, I don’t believe that withholding any sort of educational information is a net positive thing to do.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>While you’re reading this chapter, keep in mind that many of these scripts and described techniques should not be run against every site you can find. Not only is it not a nice thing to do, but you could wind up receiving a cease and desist letter or worse (for more information about what to do if you receive such a letter, see <a data-type="xref" href="ch02.html#c-2">Chapter 2</a>). But I’m not going to pound you over the head with this every time I discuss a new technique. So, for the rest of this chapter, as the <a contenteditable="false" data-primary="ethics" data-startref="etcis" data-type="indexterm" id="id844"/>philosopher Gump once said: “That’s all I have to say about that.”</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Looking Like a Human" data-type="sect1"><div class="sect1" id="id169">&#13;
<h1>Looking Like a Human</h1>&#13;
&#13;
<p>The fundamental challenge for sites that do not want to be scraped is figuring out how to tell bots from humans. Although many of the techniques sites use (such as CAPTCHAs) can be difficult <a contenteditable="false" data-primary="bots" data-secondary="appearing human" data-type="indexterm" id="btpphm"/>to fool, you can do a few fairly easy things to make your bot look more human.</p>&#13;
&#13;
<section data-pdf-bookmark="Adjust Your Headers" data-type="sect2"><div class="sect2" id="id110">&#13;
<h2>Adjust Your Headers</h2>&#13;
&#13;
<p>Throughout the book, you’ve used the Python Requests library to create, send, and receive HTTP requests, such as handling forms <a contenteditable="false" data-primary="headers" data-secondary="setting" data-type="indexterm" id="hdrstg"/><a contenteditable="false" data-primary="Requests library" data-secondary="headers, setting" data-type="indexterm" id="rqbyht"/>on a website in <a data-type="xref" href="ch13.html#c-13">Chapter 13</a>. The Requests library is also excellent for setting headers. HTTP headers are lists of attributes, or preferences, sent by you every time you make a request to a web server. HTTP defines dozens of obscure header types, most of which are not commonly used. The following seven fields, however, are consistently used by most major browsers when initiating any connection (shown with example data from my own browser):</p>&#13;
&#13;
<table>&#13;
	<tbody>&#13;
		<tr>&#13;
			<td><code>Host</code></td>&#13;
			<td>https://www.google.com/</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><code>Connection</code></td>&#13;
			<td>keep-alive</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><code>Accept</code></td>&#13;
			<td>text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><code>User-Agent</code></td>&#13;
			<td>Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><code>Referrer</code></td>&#13;
			<td>https://www.google.com/</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><code>Accept-Encoding</code></td>&#13;
			<td>gzip, deflate, sdch</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><code>Accept-Language</code></td>&#13;
			<td>en-US,en;q=0.8</td>&#13;
		</tr>&#13;
	</tbody>&#13;
</table>&#13;
&#13;
<p>And here are the headers that a typical Python <a contenteditable="false" data-primary="headers" data-secondary="urllib library" data-type="indexterm" id="id845"/><a contenteditable="false" data-primary="urllib library" data-type="indexterm" id="id846"/>scraper using the default urllib library might send:</p>&#13;
&#13;
<table>&#13;
	<tbody>&#13;
		<tr>&#13;
			<td><code>Accept-Encoding</code></td>&#13;
			<td>identity</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><code>User-Agent</code></td>&#13;
			<td>Python-urllib/3.9</td>&#13;
		</tr>&#13;
	</tbody>&#13;
</table>&#13;
&#13;
<p>If you’re a website administrator trying to <a contenteditable="false" data-primary="web crawlers" data-secondary="blocking" data-type="indexterm" id="id847"/>block scrapers, which one are you more likely to let through?</p>&#13;
&#13;
<p>Fortunately, headers can be completely customized using the Requests  library. The website <a href="https://www.whatismybrowser.com"><em>https://www.whatismybrowser.com</em></a> is great for testing browser properties viewable by servers. You’ll scrape this website to verify your cookie settings with the following script:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="kn">import</code> <code class="nn">requests</code>&#13;
<code class="kn">from</code> <code class="nn">bs4</code> <code class="kn">import</code> <code class="n">BeautifulSoup</code>&#13;
&#13;
<code class="n">session</code> <code class="o">=</code> <code class="n">requests</code><code class="o">.</code><code class="n">Session</code><code class="p">()</code>&#13;
&#13;
<code class="n">headers</code> <code class="o">=</code> <code class="p">{</code><code class="s1">'User-Agent'</code><code class="p">:</code><code class="s1">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) </code><code class="se">\</code>&#13;
<code class="s1">AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36'</code><code class="p">,</code>&#13;
           <code class="s1">'Accept'</code><code class="p">:</code><code class="s1">'text/html,application/xhtml+xml,application/xml;q=0.9,</code><code class="se">\</code>&#13;
<code class="s1">image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;</code><code class="se">\</code>&#13;
<code class="s1">q=0.7'</code><code class="p">}</code>&#13;
<code class="n">url</code> <code class="o">=</code> <code class="s1">'https://www.whatismybrowser.com/</code><code class="se">\</code>&#13;
<code class="s1">developers/what-http-headers-is-my-browser-sending'</code>&#13;
<code class="n">req</code> <code class="o">=</code> <code class="n">session</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="n">url</code><code class="p">,</code> <code class="n">headers</code><code class="o">=</code><code class="n">headers</code><code class="p">)</code>&#13;
&#13;
<code class="n">bs</code> <code class="o">=</code> <code class="n">BeautifulSoup</code><code class="p">(</code><code class="n">req</code><code class="o">.</code><code class="n">text</code><code class="p">,</code> <code class="s1">'html.parser'</code><code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="n">bs</code><code class="o">.</code><code class="n">find</code><code class="p">(</code><code class="s1">'table'</code><code class="p">,</code> <code class="p">{</code><code class="s1">'class'</code><code class="p">:</code><code class="s1">'table-striped'</code><code class="p">})</code><code class="o">.</code><code class="n">get_text</code><code class="p">)</code>&#13;
</pre>&#13;
&#13;
<p>The output should show that the headers are now the same ones set in the <code>headers</code> dictionary object in the code.</p>&#13;
&#13;
<p>Although it is possible for websites <a contenteditable="false" data-primary="User-Agent" data-type="indexterm" id="id848"/>to check for “humanness” based on any of the properties in HTTP headers, I’ve found that typically the only setting that really matters is the <code>User-Agent</code>. It’s a good idea to keep this one set to something more inconspicuous than <code>Python-urllib/3.9</code>, regardless of what project you are working on. In addition, if you ever encounter an extremely suspicious website, populating <a contenteditable="false" data-primary="bots" data-secondary="appearing human" data-startref="btpphm" data-type="indexterm" id="id849"/><a contenteditable="false" data-primary="headers" data-secondary="setting" data-startref="hdrstg" data-type="indexterm" id="id850"/><a contenteditable="false" data-primary="Requests library" data-secondary="headers, setting" data-startref="rqbyht" data-type="indexterm" id="id851"/>one of the commonly used but rarely checked headers such as <code>Accept-Language</code> might be the key to convincing it you’re a human.</p>&#13;
&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id852">&#13;
<h1>Headers Change the Way You See the World</h1>&#13;
&#13;
<p>Let’s say you want to write a machine-learning language translator for a research project but lack large amounts of <a contenteditable="false" data-primary="headers" data-secondary="language" data-type="indexterm" id="id853"/>translated text to test with. Many large sites present different translations of the same content, based on the indicated language preferences in your headers. Simply changing <code>Accept-Language:en-US</code> to <code>Accept-Language:fr</code> in your headers might get you a “Bonjour” from websites with the scale and budget to handle translation (large international companies are usually a good bet).</p>&#13;
&#13;
<p>Headers also can prompt websites to change the format of the content they are presenting. For instance, mobile devices browsing the web often see a pared-down version of sites, lacking banner ads, Flash, and other distractions. If you try changing your <code>User-Agent</code> to something like the following, you might find that sites get a little easier to scrape!</p>&#13;
&#13;
<pre data-code-language="text" data-type="programlisting">&#13;
User-Agent:Mozilla/5.0 (iPhone; CPU iPhone OS 7_1_2 like Mac OS X) &#13;
AppleWebKit/537.51.2 (KHTML, like Gecko) Version/7.0 Mobile/11D257 &#13;
Safari/9537.53</pre>&#13;
</div></aside>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Handling Cookies with JavaScript" data-type="sect2"><div class="sect2" id="id111">&#13;
<h2>Handling Cookies with JavaScript</h2>&#13;
&#13;
<p>Handling cookies correctly can alleviate many <a contenteditable="false" data-primary="cookies" data-secondary="JavaScript and" data-type="indexterm" id="ckjvcp"/><a contenteditable="false" data-primary="JavaScript" data-secondary="cookies and" data-type="indexterm" id="jvcpks"/><a contenteditable="false" data-primary="headers" data-secondary="cookies and" data-type="indexterm" id="hdrckd"/><a contenteditable="false" data-primary="cookies" data-secondary="headers and" data-type="indexterm" id="ckshds"/>scraping problems, although cookies can also be a double-edged sword. Websites that track your progression through a site using cookies might attempt to cut off scrapers that display abnormal behavior, such as completing forms too quickly, or visiting too many pages. Although these behaviors can be disguised by closing and reopening connections to the site, or even changing your IP address (see <a data-type="xref" href="ch20.html#c-20">Chapter 20</a> for more information on how to do that), if your cookie gives your identity away, your efforts at disguise might be futile.</p>&#13;
&#13;
<p>Cookies can also be necessary to scrape a site. As shown in <a data-type="xref" href="ch13.html#c-13">Chapter 13</a>, staying logged in on a site requires that you be able to hold and present a cookie from page to page. Some websites don’t even require that you actually log in and get a new version of a cookie every time—merely holding an old copy of a “logged-in” cookie and visiting the site is enough.</p>&#13;
&#13;
<p>If you are scraping a single targeted website or a small number of targeted sites, I recommend examining the cookies generated by those sites and considering which ones you might want your scraper to handle. Various browser plug-ins can show you how cookies are being set as you visit <a contenteditable="false" data-primary="plug-ins" data-secondary="EditThisCookie" data-type="indexterm" id="id854"/><a contenteditable="false" data-primary="EditThisCookie plug-in" data-type="indexterm" id="id855"/><a contenteditable="false" data-primary="cookies" data-secondary="plug-ins" data-type="indexterm" id="id856"/>and move around a site. <a href="http://www.editthiscookie.com">EditThisCookie</a>, a Chrome extension, is one of my favorites.</p>&#13;
&#13;
<p>Check out the code samples in <a data-type="xref" href="ch13.html#handling_logins_cookies">“Handling Logins and Cookies”</a> in <a data-type="xref" href="ch13.html#c-13">Chapter 13</a> for more information about handling cookies by using the Requests library. Of course, because it is unable to execute JavaScript, the Requests library will be unable to handle many of the cookies produced by modern tracking software, such as Google Analytics, which are set only after the execution of client-side scripts (or sometimes based on page events, such as button clicks, that happen while browsing the page). To handle these, you need to use the Selenium and Chrome WebDriver packages (I covered their installation and basic usage in <a data-type="xref" href="ch14.html#c-14">Chapter 14</a>).</p>&#13;
&#13;
<p>You can view cookies by <a contenteditable="false" data-primary="cookies" data-secondary="viewing" data-type="indexterm" id="id857"/>visiting any site (<a href="http://pythonscraping.com"><em>http://pythonscraping.com</em></a>, in this example) and calling <code>get_cookies()</code> on the webdriver:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">selenium</code> <code class="kn">import</code> <code class="n">webdriver</code>&#13;
<code class="kn">from</code> <code class="nn">selenium.webdriver.chrome.options</code> <code class="kn">import</code> <code class="n">Options</code>&#13;
<code class="n">chrome_options</code> <code class="o">=</code> <code class="n">Options</code><code class="p">()</code>&#13;
<code class="n">chrome_options</code><code class="o">.</code><code class="n">add_argument</code><code class="p">(</code><code class="s1">'--headless'</code><code class="p">)</code>&#13;
<code class="n">driver</code> <code class="o">=</code> <code class="n">webdriver</code><code class="o">.</code><code class="n">Chrome</code><code class="p">(</code>&#13;
    <code class="n">executable_path</code><code class="o">=</code><code class="s1">'drivers/chromedriver'</code><code class="p">,</code> &#13;
    <code class="n">options</code><code class="o">=</code><code class="n">chrome_options</code><code class="p">)</code>&#13;
<code class="n">driver</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s1">'http://pythonscraping.com'</code><code class="p">)</code>&#13;
<code class="n">driver</code><code class="o">.</code><code class="n">implicitly_wait</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="n">driver</code><code class="o">.</code><code class="n">get_cookies</code><code class="p">())</code></pre>&#13;
&#13;
<p>This provides the fairly typical array <a contenteditable="false" data-primary="cookies" data-secondary="Google Analytics" data-type="indexterm" id="id858"/><a contenteditable="false" data-primary="Google Analytics" data-type="indexterm" id="id859"/>of Google Analytics cookies:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
[{'domain': '.pythonscraping.com', 'expiry': 1722996491, 'httpOnly': False,&#13;
'name': '_ga', 'path': '/', 'sameSite': 'Lax', 'secure': False, 'value': &#13;
'GA1.1.285394841.1688436491'}, {'domain': '.pythonscraping.com', 'expiry': &#13;
1722996491, 'httpOnly': False, 'name': '_ga_G60J5CGY1N', 'path': '/', &#13;
'sameSite': 'Lax', 'secure': False, 'value': &#13;
'GS1.1.1688436491.1.0.1688436491.0.0.0'}]&#13;
</pre>&#13;
&#13;
<p>To manipulate cookies, you can call <a contenteditable="false" data-primary="cookies" data-secondary="functions for manipulating" data-type="indexterm" id="id860"/>the <code>delete_cookie()</code>, <code>add_cookie()</code>, and <code>delete_all_cookies()</code> functions. In addition, you can save and store cookies for use in other web scrapers. Here’s an example to give you an idea of how these functions work together:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">selenium</code> <code class="kn">import</code> <code class="n">webdriver</code>&#13;
<code class="kn">from</code> <code class="nn">selenium.webdriver.chrome.options</code> <code class="kn">import</code> <code class="n">Options</code>&#13;
&#13;
<code class="n">chrome_options</code> <code class="o">=</code> <code class="n">Options</code><code class="p">()</code>&#13;
<code class="n">chrome_options</code><code class="o">.</code><code class="n">add_argument</code><code class="p">(</code><code class="s2">"--headless"</code><code class="p">)</code>&#13;
<code class="n">driver</code> <code class="o">=</code> <code class="n">webdriver</code><code class="o">.</code><code class="n">Chrome</code><code class="p">(</code>&#13;
    <code class="n">service</code><code class="o">=</code><code class="n">Service</code><code class="p">(</code><code class="n">CHROMEDRIVER_PATH</code><code class="p">),</code>&#13;
    <code class="n">options</code><code class="o">=</code><code class="n">chrome_options</code>&#13;
<code class="p">)</code>&#13;
&#13;
<code class="n">driver</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s1">'http://pythonscraping.com'</code><code class="p">)</code>&#13;
<code class="n">driver</code><code class="o">.</code><code class="n">implicitly_wait</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code>&#13;
&#13;
<code class="n">savedCookies</code> <code class="o">=</code> <code class="n">driver</code><code class="o">.</code><code class="n">get_cookies</code><code class="p">()</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="n">savedCookies</code><code class="p">)</code>&#13;
&#13;
<code class="n">driver2</code> <code class="o">=</code> <code class="n">webdriver</code><code class="o">.</code><code class="n">Chrome</code><code class="p">(</code>&#13;
    <code class="n">service</code><code class="o">=</code><code class="n">Service</code><code class="p">(</code><code class="n">CHROMEDRIVER_PATH</code><code class="p">),</code>&#13;
    <code class="n">options</code><code class="o">=</code><code class="n">chrome_options</code>&#13;
<code class="p">)</code>&#13;
&#13;
<code class="n">driver2</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s1">'http://pythonscraping.com'</code><code class="p">)</code>&#13;
<code class="n">driver2</code><code class="o">.</code><code class="n">delete_all_cookies</code><code class="p">()</code>&#13;
<code class="k">for</code> <code class="n">cookie</code> <code class="ow">in</code> <code class="n">savedCookies</code><code class="p">:</code>&#13;
    <code class="n">driver2</code><code class="o">.</code><code class="n">add_cookie</code><code class="p">(</code><code class="n">cookie</code><code class="p">)</code>&#13;
&#13;
<code class="n">driver2</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s1">'http://pythonscraping.com'</code><code class="p">)</code>&#13;
<code class="n">driver</code><code class="o">.</code><code class="n">implicitly_wait</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="n">driver2</code><code class="o">.</code><code class="n">get_cookies</code><code class="p">())</code>&#13;
</pre>&#13;
&#13;
<p>In this example, the first webdriver retrieves a website, prints the cookies, and then stores them in the variable <code>savedCookies</code>. The second webdriver loads the same website, deletes its own cookies, and adds the cookies from the first webdriver.</p>&#13;
&#13;
<p>Note that the second webdriver must load the website first before the cookies are added. This is so Selenium knows which domain the cookies belong to, even if the act of loading the website does nothing useful for the scraper.</p>&#13;
&#13;
<p>After this is done, the second webdriver should have the same cookies as the first. According to Google Analytics, this second webdriver is now identical to the first one, and they will be tracked in the same way. If the first webdriver was logged in to a site, the second webdriver <a contenteditable="false" data-primary="cookies" data-secondary="JavaScript and" data-startref="ckjvcp" data-type="indexterm" id="id861"/><a contenteditable="false" data-primary="JavaScript" data-secondary="cookies and" data-startref="jvcpks" data-type="indexterm" id="id862"/><a contenteditable="false" data-primary="headers" data-secondary="cookies and" data-startref="hdrckd" data-type="indexterm" id="id863"/><a contenteditable="false" data-primary="cookies" data-secondary="headers and" data-startref="ckshds" data-type="indexterm" id="id864"/>will be as well.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="TLS Fingerprinting" data-type="sect2"><div class="sect2" id="id112">&#13;
<h2>TLS Fingerprinting</h2>&#13;
&#13;
<p>In the early 2000s, many large tech <a contenteditable="false" data-primary="TLS (Transport Layer Security)" data-type="indexterm" id="tlstls"/>companies liked to ask prospective programmers riddles during job interviews. This mostly fell out of fashion when hiring managers realized two things: candidates were sharing and memorizing riddle solutions, and the “ability to solve riddles” doesn’t correlate well to job performance.</p>&#13;
&#13;
<p>However, one of these classic job interview riddles still has value as a metaphor for the Transport Layer Security protocol. It goes like this:</p>&#13;
&#13;
<p>You need to ship a top secret message to a friend via a dangerous route where any unlocked message-containing boxes are intercepted by spies (padlocked message boxes, however, are safe if the spies do not have the keys). You place the message in a box that can be locked with multiple padlocks. While you have padlocks with corresponding keys and your friend also has their own padlocks with corresponding keys, none of your friend’s keys work on your padlocks and vice versa. How do you ensure that your friend is able to unlock the box on their end and receive the message securely?</p>&#13;
&#13;
<p>Note that shipping a key that unlocks your padlock, even as a separate shipment, will not work. The spies will intercept and make copies of these keys and save them for later use. Also, shipping a key afterwards will not work (although this is where “riddle as metaphor” breaks down a bit) because the spies can make copies of the <em>box itself</em> and, if a key is sent later, unlock their box copy.</p>&#13;
&#13;
<p>One solution is this: you place your padlock on the box and ship it to your friend. Your friend receives the locked box, places their own padlock on it (so now the box has two padlocks), and ships it back. You remove your padlock and ship it to your friend with only their padlock remaining. Your friend receives the box and unlocks it.</p>&#13;
&#13;
<p>This is, essentially, how secure communications are established over an untrusted network. Over a secure communication protocol, like HTTPS, all messages are encrypted and decrypted with a key. If an attacker obtains the key (represented by the secret message in the riddle), then they are able to read any messages being sent.</p>&#13;
&#13;
<p>So how do you send your friend the key that you’re going to use to encrypt and decrypt future messages without that key being intercepted and used by attackers? Encrypt it with your own “padlock,” send it to the friend, the friend adds their own “padlock,” you remove your “padlock,” and send it back for the friend to “unlock.” In this way, the secret key is exchanged securely.</p>&#13;
&#13;
<p>This entire process of “locking,” sending, adding another “lock,” etc., is handled by the Transport Layer Security protocol, or TLS. This process of securely establishing a mutually known key is called the <em>TLS handshake</em>.</p>&#13;
&#13;
<p>In addition to a establishing a mutually known key, or <em>master secret</em>, many other things are established during the handshake:</p>&#13;
&#13;
<ul>&#13;
	<li>The highest version of the TLS protocol supported by both parties (which will be the version used during the rest of the handshake)</li>&#13;
	<li>Which encryption library will be used </li>&#13;
	<li>Which compression method will be used</li>&#13;
	<li>The identity of the server, represented by its public certificate</li>&#13;
	<li>Verifications that the master secret is working for both parties and that the communication is now secure</li>&#13;
</ul>&#13;
&#13;
<p>This entire TLS handshake is done every time you contact a new web server and any time you need to establish a new HTTP session with that web server (see <a data-type="xref" href="ch01.html#c-1">Chapter 1</a> for more information about sessions). The exact messages that are sent by your computer for the TLS handshake are determined by the application that is making the connection. For example, Chrome may support slightly different TLS versions or encryption libraries than Microsoft Edge, so the messages sent by it in the TLS handshake will be different.</p>&#13;
&#13;
<p>Because the TLS handshake is so lengthy, and the variables involved in its negotiations so numerous, clever server administrators realized that the messages sent by clients during the TLS handshake were somewhat unique to each application. The messages created a sort of <em>TLS fingerprint</em> that revealed whether the messages were being generated by Chrome, Microsoft Edge, Safari, or even the Python Requests library.</p>&#13;
&#13;
<p>You can see some of the information generated by your TLS handshake by visiting (or scraping) <a href="https://tools.scrapfly.io/api/fp/ja3?extended=1"><em class="hyperlink">https://tools.scrapfly.io/api/fp/ja3?extended=1</em></a>. To make TLS fingerprints more manageable and easy to compare, a hashing method called JA3 is often used, the results <a contenteditable="false" data-primary="TLS (Transport Layer Security)" data-startref="tlstls" data-type="indexterm" id="id865"/>of which are shown in this API response. JA3 hashed fingerprints are catalogued in large databases and used for lookup when an application needs to be identified later.</p>&#13;
&#13;
<p>A TLS fingerprint is a bit <a contenteditable="false" data-primary="TLS (Transport Layer Security)" data-secondary="fingerprint" data-type="indexterm" id="tlstfp"/>like a User-Agent cookie in that it is a long string that identifies the application you’re using to send data. But, unlike a User-Agent cookie, it’s not easy to modify. In Python, TLS is <a contenteditable="false" data-primary="Python" data-secondary="TLS" data-type="indexterm" id="id866"/>controlled by the <a href="https://github.com/python/cpython/blob/3.11/Lib/ssl.py">SSL library</a>. In theory, perhaps you could rewrite the SSL library. With hard work and dedication, you may be able to modify the TLS fingerprint that Python is sending from your computer just enough to make the JA3 hash unrecognizable to servers seeking to block Python bots. With harder work and more dedication, you might impersonate an innocuous browser! Some projects, such as <a href="https://github.com/lwthiker/curl-impersonate"><em class="hyperlink">https://github.com/lwthiker/curl-impersonate</em></a>,  are seeking to do just that.</p>&#13;
&#13;
<p>Unfortunately the nature of this TLS fingerprint problem means that any impersonation libraries will require frequent maintenance by volunteers and are prone to quick degradation. Until these projects gain more mainstream traction and reliability, there is a much easier way to subvert TLS fingerprinting and blocking: Selenium.</p>&#13;
&#13;
<p>Throughout this book, I’ve warned against using an automated browser to solve your problems when alternative solutions exist. Browsers use lots of memory, often load unnecessary pages, and require extra dependencies that all need upkeep and <span class="keep-together">maintenance</span> to keep your web crawler running. But when it comes to TLS fingerprinting, it just makes sense to avoid the headache and use a browser.</p>&#13;
&#13;
<p>Keep in mind that your TLS fingerprint will be the same whether you’re using the headless or nonheadless version of your browser. So feel free to turn off the graphics and use best practices to load only <a contenteditable="false" data-primary="TLS (Transport Layer Security)" data-secondary="fingerprint" data-startref="tlstfp" data-type="indexterm" id="id867"/>the data that you need—the target server isn’t going to know (based on your TLS data, at least)!</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Timing Is Everything" data-type="sect2"><div class="sect2" id="id251">&#13;
<h2>Timing Is Everything</h2>&#13;
&#13;
<p>Some well-protected websites might prevent you from submitting forms or interacting with the site if you do it too quickly. Even if these security features aren’t in place, downloading lots of information from a website significantly faster than a normal human might is a good way to get yourself noticed and blocked.</p>&#13;
&#13;
<p>Therefore, although multithreaded programming might be a great way to load pages faster—allowing you to process data in one thread while repeatedly loading pages in another—it’s a terrible policy for writing good scrapers. You should always try to keep individual page loads and data requests to a minimum. If possible, try to space them out by a few seconds, even if you have to add in an extra:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="kn">import</code> <code class="nn">time</code>&#13;
&#13;
<code class="n">time</code><code class="o">.</code><code class="n">sleep</code><code class="p">(</code><code class="mi">3</code><code class="p">)</code></pre>&#13;
&#13;
<p>Whether or not you need this extra few seconds between page loads is often found experimentally. Many times I’ve struggled to scrape data from a website, having to prove myself as “not a robot” every few minutes (solving the CAPTCHA by hand, pasting my newly obtained cookies back over to the scraper so the website viewed the scraper itself as having “proven its humanness”), but adding a <code>time.sleep</code> solved my problems and allowed me to scrape indefinitely.</p>&#13;
&#13;
<p>Sometimes you have to slow down to go fast!</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Common Form Security Features" data-type="sect1"><div class="sect1" id="id170">&#13;
<h1>Common Form Security Features</h1>&#13;
&#13;
<p>Many litmus tests have been used over the years, and <a contenteditable="false" data-primary="forms" data-secondary="security" data-type="indexterm" id="frmscty"/><a contenteditable="false" data-primary="security" data-secondary="forms" data-type="indexterm" id="scrtyfm"/><a contenteditable="false" data-primary="logins" data-secondary="security" data-type="indexterm" id="lgnsscy"/><a contenteditable="false" data-primary="security" data-secondary="logins" data-type="indexterm" id="scygns"/>continue to be used, with varying degrees of success, to separate web scrapers from browser-using humans. Although it’s not a big deal if a bot downloads some articles and blog posts that were available to the public anyway, it is a big problem if a bot creates thousands of user accounts and starts spamming all of your site’s members. Web forms, especially forms that deal with account creation and logins, pose a significant threat to security and computational overhead if they’re vulnerable to indiscriminate use by bots, so it’s in the best interest of many site owners (or at least they think it is) to try to limit access to the site.</p>&#13;
&#13;
<p>These antibot security measures centered on <a contenteditable="false" data-primary="security" data-secondary="antibot measures" data-type="indexterm" id="scyttm"/>forms and logins can pose a significant challenge to web scrapers.</p>&#13;
&#13;
<p>Keep in mind that this is only a partial overview of some of the security measures you might encounter when creating automated bots for these forms. Review <a data-type="xref" href="ch16.html#c-16">Chapter 16</a>, on dealing with CAPTCHAs and image processing, as well as <a data-type="xref" href="ch13.html#c-13">Chapter 13</a>, on dealing with headers and IP addresses, for more information on dealing with well-protected forms.</p>&#13;
&#13;
<section data-pdf-bookmark="Hidden Input Field Values" data-type="sect2"><div class="sect2" id="id113">&#13;
<h2>Hidden Input Field Values</h2>&#13;
&#13;
<p>“Hidden” fields in HTML forms allow the value <a contenteditable="false" data-primary="forms" data-secondary="hidden field values" data-type="indexterm" id="fmshdf"/><a contenteditable="false" data-primary="input fields, hidden values" data-type="indexterm" id="pfdhv"/><a contenteditable="false" data-primary="HTML (HyperText Markup Language)" data-secondary="forms, input fields" data-type="indexterm" id="htxkpf"/>contained in the field to be viewable by the browser but invisible to the user (unless they look at the site’s source code). With the increase in use of cookies to store variables and pass them around on websites, hidden fields fell out of a favor for a while before another excellent purpose was discovered for them: preventing scrapers from submitting forms.</p>&#13;
&#13;
<p><a data-type="xref" href="#facebook_login_form">Figure 17-1</a> shows an example of these hidden fields at work on a LinkedIn login page. Although the form has only three visible fields (Username, Password, and a Submit button), it conveys a great deal of information to the server behind the scenes.</p>&#13;
&#13;
<figure><div class="figure" id="facebook_login_form"><img alt="Alt Text" class="iimagesfacebook_hiddenfieldspng" src="assets/wsp3_1701.png"/>&#13;
<h6><span class="label">Figure 17-1. </span>The LinkedIn login form has a few hidden fields</h6>&#13;
</div></figure>&#13;
&#13;
<p>Hidden fields are used to prevent web scraping in two main ways: a field can be populated with a randomly generated variable on the form page that the server is expecting to be posted to the form-processing page. If this value is not present in the form, the server can reasonably assume that the submission did not originate organically from the form page but was posted by a bot directly to the processing page. The best way to get around this measure is to scrape the form page first, collect the randomly generated variable, and then post to the processing page from there.</p>&#13;
&#13;
<p>The second method is a “honeypot” of sorts. If a form contains a hidden field with an innocuous name, such as Username or Email Address, a poorly written bot might fill out the field and attempt to submit it, regardless of whether it is hidden to the user. Any hidden fields with actual values (or values that are different from their defaults on the form submission page) should be disregarded, and the user may even be blocked from the site.</p>&#13;
&#13;
<p>In short: it is sometimes necessary to check the page that the form is on to see whether you missed anything that the server might be expecting. If you see several hidden fields, often with large, randomly generated string variables, the web server likely will be checking for their existence on form submission. In addition, there might be other checks to ensure that the form variables have been used only once, are recently generated (this eliminates the possibility of simply storing them in a script and using them <a contenteditable="false" data-primary="forms" data-secondary="hidden field values" data-startref="fmshdf" data-type="indexterm" id="id868"/><a contenteditable="false" data-primary="input fields, hidden values" data-startref="pfdhv" data-type="indexterm" id="id869"/><a contenteditable="false" data-primary="HTML (HyperText Markup Language)" data-secondary="forms, input fields" data-startref="htxkpf" data-type="indexterm" id="id870"/>over and over again over time), or both.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Avoiding Honeypots" data-type="sect2"><div class="sect2" id="avoiding_honeypots">&#13;
<h2>Avoiding Honeypots</h2>&#13;
&#13;
<p>Although CSS for the most part makes life <a contenteditable="false" data-primary="security" data-secondary="honeypots, avoiding" data-type="indexterm" id="scyypv"/><a contenteditable="false" data-primary="honeypots" data-type="indexterm" id="hypvd"/>extremely easy when it comes to differentiating useful information from nonuseful information (e.g., by reading the <code>id</code> and <code>class</code> tags), it can occasionally be problematic for web scrapers. If a field on a web form is hidden from a user via CSS, it is reasonable to assume that the average user visiting the site will not be able to fill it out because it doesn’t show up in the browser. If the form <em>is</em> populated, there is likely a bot at work and the post will be discarded.</p>&#13;
&#13;
<p>This applies not only to forms but to links, images, files, and any other item on the site that can be read by a bot but is hidden from the average user visiting the site through a browser. A page visit to a “hidden” link on a site can easily trigger a server-side script that will block the user’s IP address, log that user out of the site, or take some other action to prevent further access. In fact, many business models have been based on exactly this concept.</p>&#13;
&#13;
<p>Take, for example, the page located at <a href="http://pythonscraping.com/pages/itsatrap.html"><em>http://pythonscraping.com/pages/itsatrap.html</em></a>. This page contains two links, one hidden by CSS and another visible. In addition, it contains a form with two hidden fields:</p>&#13;
&#13;
<pre data-code-language="html" data-type="programlisting">&#13;
<code class="p">&lt;</code><code class="nt">html</code><code class="p">&gt;</code>&#13;
<code class="p">&lt;</code><code class="nt">head</code><code class="p">&gt;</code> &#13;
    <code class="p">&lt;</code><code class="nt">title</code><code class="p">&gt;</code>A bot-proof form<code class="p">&lt;/</code><code class="nt">title</code><code class="p">&gt;</code>&#13;
<code class="p">&lt;/</code><code class="nt">head</code><code class="p">&gt;</code>&#13;
<code class="p">&lt;</code><code class="nt">style</code><code class="p">&gt;</code>&#13;
    <code class="nt">body</code> <code class="p">{</code> &#13;
        <code class="k">overflow-x</code><code class="o">:</code><code class="nb">hidden</code><code class="p">;</code>&#13;
    <code class="p">}</code>&#13;
    <code class="nc">.customHidden</code> <code class="p">{</code> &#13;
        <code class="k">position</code><code class="o">:</code><code class="nb">absolute</code><code class="p">;</code> &#13;
        <code class="k">right</code><code class="o">:</code><code class="m">50000px</code><code class="p">;</code>&#13;
    <code class="p">}</code>&#13;
<code class="nt">&lt;/style&gt;</code>&#13;
<code class="p">&lt;</code><code class="nt">body</code><code class="p">&gt;</code> &#13;
    <code class="p">&lt;</code><code class="nt">h2</code><code class="p">&gt;</code>A bot-proof form<code class="p">&lt;/</code><code class="nt">h2</code><code class="p">&gt;</code>&#13;
    <code class="p">&lt;</code><code class="nt">a</code> <code class="na">href</code><code class="o">=</code>&#13;
     <code class="s">"http://pythonscraping.com/dontgohere"</code> <code class="na">style</code><code class="o">=</code><code class="s">"display:none;"</code><code class="p">&gt;</code>Go here!<code class="p">&lt;/</code><code class="nt">a</code><code class="p">&gt;</code>&#13;
    <code class="p">&lt;</code><code class="nt">a</code> <code class="na">href</code><code class="o">=</code><code class="s">"http://pythonscraping.com"</code><code class="p">&gt;</code>Click me!<code class="p">&lt;/</code><code class="nt">a</code><code class="p">&gt;</code>&#13;
    <code class="p">&lt;</code><code class="nt">form</code><code class="p">&gt;</code>&#13;
        <code class="p">&lt;</code><code class="nt">input</code> <code class="na">type</code><code class="o">=</code><code class="s">"hidden"</code> <code class="na">name</code><code class="o">=</code><code class="s">"phone"</code> <code class="na">value</code><code class="o">=</code><code class="s">"valueShouldNotBeModified"</code><code class="p">/&gt;&lt;</code><code class="nt">p</code><code class="p">/&gt;</code>&#13;
        <code class="p">&lt;</code><code class="nt">input</code> <code class="na">type</code><code class="o">=</code><code class="s">"text"</code> <code class="na">name</code><code class="o">=</code><code class="s">"email"</code> <code class="na">class</code><code class="o">=</code><code class="s">"customHidden"</code> &#13;
                  <code class="na">value</code><code class="o">=</code><code class="s">"intentionallyBlank"</code><code class="p">/&gt;&lt;</code><code class="nt">p</code><code class="p">/&gt;</code>&#13;
        <code class="p">&lt;</code><code class="nt">input</code> <code class="na">type</code><code class="o">=</code><code class="s">"text"</code> <code class="na">name</code><code class="o">=</code><code class="s">"firstName"</code><code class="p">/&gt;&lt;</code><code class="nt">p</code><code class="p">/&gt;</code>&#13;
        <code class="p">&lt;</code><code class="nt">input</code> <code class="na">type</code><code class="o">=</code><code class="s">"text"</code> <code class="na">name</code><code class="o">=</code><code class="s">"lastName"</code><code class="p">/&gt;&lt;</code><code class="nt">p</code><code class="p">/&gt;</code>&#13;
        <code class="p">&lt;</code><code class="nt">input</code> <code class="na">type</code><code class="o">=</code><code class="s">"submit"</code> <code class="na">value</code><code class="o">=</code><code class="s">"Submit"</code><code class="p">/&gt;&lt;</code><code class="nt">p</code><code class="p">/&gt;</code>&#13;
   <code class="p">&lt;/</code><code class="nt">form</code><code class="p">&gt;</code>&#13;
<code class="p">&lt;/</code><code class="nt">body</code><code class="p">&gt;</code>&#13;
<code class="p">&lt;/</code><code class="nt">html</code><code class="p">&gt;</code></pre>&#13;
&#13;
<p>These three elements are hidden from the user in three ways:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>The first link is hidden with a simple CSS <code>display:none</code> attribute.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>The phone field is a hidden input field.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>The email field is hidden by moving it 50,000 pixels to the right (presumably off the screen of everyone’s monitors) and hiding the telltale scroll bar.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>Fortunately, because Selenium renders the pages it visits, it is able to distinguish between elements that are visually present on the page and those that aren’t. Whether the element is present on the page can be determined by the <code>is_displayed()</code> <span class="keep-together">function.</span></p>&#13;
&#13;
<p>For example, the following code retrieves the previously described page and looks for hidden links and form input fields:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">selenium</code> <code class="kn">import</code> <code class="n">webdriver</code>&#13;
<code class="kn">from</code> <code class="nn">selenium.webdriver.remote.webelement</code> <code class="kn">import</code> <code class="n">WebElement</code>&#13;
<code class="kn">from</code> <code class="nn">selenium.webdriver.chrome.options</code> <code class="kn">import</code> <code class="n">Options</code>&#13;
<code class="kn">from</code> <code class="nn">selenium.webdriver.common.by</code> <code class="kn">import</code> <code class="n">By</code>&#13;
&#13;
<code class="n">driver</code> <code class="o">=</code> <code class="n">webdriver</code><code class="o">.</code><code class="n">Chrome</code><code class="p">(</code><code class="n">service</code><code class="o">=</code><code class="n">Service</code><code class="p">(</code><code class="n">CHROMEDRIVER_PATH</code><code class="p">))</code>&#13;
&#13;
<code class="n">driver</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s1">'http://pythonscraping.com/pages/itsatrap.html'</code><code class="p">)</code>&#13;
<code class="n">links</code> <code class="o">=</code> <code class="n">driver</code><code class="o">.</code><code class="n">find_elements</code><code class="p">(</code><code class="n">By</code><code class="o">.</code><code class="n">TAG_NAME</code><code class="p">,</code> <code class="s1">'a'</code><code class="p">)</code>&#13;
<code class="k">for</code> <code class="n">link</code> <code class="ow">in</code> <code class="n">links</code><code class="p">:</code>&#13;
    <code class="k">if</code> <code class="ow">not</code> <code class="n">link</code><code class="o">.</code><code class="n">is_displayed</code><code class="p">():</code>&#13;
        <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'The link </code><code class="si">{</code><code class="n">link</code><code class="o">.</code><code class="n">get_attribute</code><code class="p">(</code><code class="s2">"href"</code><code class="p">)</code><code class="si">}</code><code class="s1"> is a trap'</code><code class="p">)</code>&#13;
&#13;
<code class="n">fields</code> <code class="o">=</code> <code class="n">driver</code><code class="o">.</code><code class="n">find_elements</code><code class="p">(</code><code class="n">By</code><code class="o">.</code><code class="n">TAG_NAME</code><code class="p">,</code> <code class="s1">'input'</code><code class="p">)</code>&#13;
<code class="k">for</code> <code class="n">field</code> <code class="ow">in</code> <code class="n">fields</code><code class="p">:</code>&#13;
    <code class="k">if</code> <code class="ow">not</code> <code class="n">field</code><code class="o">.</code><code class="n">is_displayed</code><code class="p">():</code>&#13;
        <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Do not change value of </code><code class="si">{</code><code class="n">field</code><code class="o">.</code><code class="n">get_attribute</code><code class="p">(</code><code class="s2">"name"</code><code class="p">)</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>&#13;
</pre>&#13;
&#13;
<p>Selenium catches each hidden field, producing the following output:</p>&#13;
&#13;
<pre data-code-language="text" data-type="programlisting">&#13;
The link http://pythonscraping.com/dontgohere is a trap&#13;
Do not change value of phone&#13;
Do not change value of email</pre>&#13;
&#13;
<p>Although you probably don’t want to visit any hidden links you find, you will want to make sure that you submit any prepopulated hidden form values (or have Selenium submit them for you) with the rest of the form. To sum up, it is dangerous to simply ignore hidden fields, although <a contenteditable="false" data-primary="forms" data-secondary="security" data-startref="frmscty" data-type="indexterm" id="id871"/><a contenteditable="false" data-primary="security" data-secondary="forms" data-startref="scrtyfm" data-type="indexterm" id="id872"/><a contenteditable="false" data-primary="logins" data-secondary="security" data-startref="lgnsscy" data-type="indexterm" id="id873"/><a contenteditable="false" data-primary="security" data-secondary="logins" data-startref="scygns" data-type="indexterm" id="id874"/><a contenteditable="false" data-primary="security" data-secondary="antibot measures" data-startref="scyttm" data-type="indexterm" id="id875"/><a contenteditable="false" data-primary="security" data-secondary="honeypots, avoiding" data-startref="scyypv" data-type="indexterm" id="id876"/><a contenteditable="false" data-primary="honeypots" data-startref="hypvd" data-type="indexterm" id="id877"/>you must be careful when interacting with them.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="The Human Checklist" data-type="sect1"><div class="sect1" id="id252">&#13;
<h1>The Human Checklist</h1>&#13;
&#13;
<p>There’s a lot of information in this chapter, and indeed in this book, about how to build a scraper that looks less like a scraper and more like a human. If you keep getting blocked by websites and you don’t know why, here’s a checklist you can use to remedy the problem:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>First, if the page you are receiving from the web server is blank, missing information, or is otherwise not what you expect (or have seen in your own browser), it is likely caused by JavaScript being executed on the site to create the page. Review <a data-type="xref" href="ch14.html#c-14">Chapter 14</a>.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>If you are submitting a form or making a <code>POST</code> request to a website, check the page to make sure that everything the website is expecting you to submit is being submitted and in the correct format. Use a tool such as Chrome’s Inspector panel to view an actual <code>POST</code> request sent to the site to make sure you have everything, and that an “organic” request looks the same as the ones your bot is sending.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>If you are trying to log in to a site and can’t make the login “stick,” or the website is experiencing other strange “state” behavior, check your cookies. Make sure that cookies are being persisted correctly between each page load and that your cookies are sent to the site for every request.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>If you are getting HTTP errors from the client, especially 403 Forbidden errors, it might indicate that the website has identified your IP address as a bot and is unwilling to accept any more requests. You will need to either wait until your IP address is removed from the list or obtain a new IP address (either move to a Starbucks or see <a data-type="xref" href="ch20.html#c-20">Chapter 20</a>). To make sure you don’t get blocked again, try the following:</p>&#13;
&#13;
	<ul>&#13;
		<li>&#13;
		<p>Make sure that your scrapers aren’t moving through the site too quickly. Fast scraping is a bad practice that places a heavy burden on the web administrator’s servers, can land you in legal trouble, and is the number one cause of scrapers getting blacklisted. Add delays to your scrapers and let them run overnight. Remember: being in a rush to write programs or gather data is a sign of bad project management; plan ahead to avoid messes like this in the first place.</p>&#13;
		</li>&#13;
		<li>&#13;
		<p>The obvious one: change your headers! Some sites will block anything that advertises itself as a scraper. Copy your own browser’s headers if you’re unsure about what some reasonable header values are.</p>&#13;
		</li>&#13;
		<li>&#13;
		<p>Make sure you’re not clicking on or accessing anything that a human normally would not be able to (refer to <a data-type="xref" href="#avoiding_honeypots">“Avoiding Honeypots”</a> for more information).</p>&#13;
		</li>&#13;
		<li>&#13;
		<p>If you find yourself jumping through a lot of difficult hoops to gain access, consider contacting the website administrator to let them know what you’re doing. Try emailing <em>webmaster@&lt;domain name&gt;</em> or <em>admin@&lt;domain name&gt;</em> for permission to use your scrapers. Admins are people too, and you might be surprised at how amenable they can be to sharing their data.</p>&#13;
		</li>&#13;
	</ul>&#13;
	</li>&#13;
</ul>&#13;
</div></section>&#13;
</div></section></body></html>