["```py\n# Add the repo\nhelm repo add dask https://helm.dask.org\nhelm repo update\n# Install the operator; you will use this to create clusters\nhelm install --create-namespace -n \\\n     dask-operator --generate-name dask/dask-kubernetes-operator\n```", "```py\nfrom dask_kubernetes.operator import KubeCluster\n\ncluster = KubeCluster(name='simple',\n                      n_workers=1,\n                      resources={\n                          \"requests\": {\"memory\": \"16Gi\"},\n                          \"limits\": {\"memory\": \"16Gi\"}\n                      })\n\ncluster.add_worker_group(name=\"highmem\",\n                         n_workers=0,\n                         resources={\n                             \"requests\": {\"memory\": \"64Gi\"},\n                             \"limits\": {\"memory\": \"64Gi\"}\n                         })\n\ncluster.add_worker_group(name=\"gpu\",\n                         n_workers=0,\n                         resources={\n                             \"requests\": {\"nvidia.com/gpu\": \"1\"},\n                             \"limits\": {\"nvidia.com/gpu\": \"1\"}\n                         })\n# Now you can scale these worker groups up and down as needed\ncluster.scale(\"gpu\", 5, worker_group=\"gpu\")\n# Fancy machine learning logic\ncluster.scale(\"gpu\", , worker_group=\"gpu\")\n# Or just auto-scale\ncluster.adapt(minimum=1, maximum=10)\n```", "```py\nimport dask\n\nenable_dask_on_ray()\nddf_students = ray.data.dataset.Dataset.to_dask(ray_dataset)\nddf_students.head()\n\ndisable_dask_on_ray()\n```", "```py\nfrom dask_yarn import YarnCluster\nfrom dask.distributed import Client\nimport logging\nimport os\nimport sys\nimport time\n\nlogger = logging.getLogger(__name__)\n\nWORKER_ENV = {\n    \"HADOOP_CONF_DIR\": \"/data/app/spark-yarn/hadoop-conf\",\n    \"JAVA_HOME\": \"/usr/lib/jvm/java\"}\n\nlogging.basicConfig(\n    level=logging.DEBUG,\n    format=\"%(asctime)s %(levelname)s %(name)s: %(message)s\")\n\nlogger.info(\"Initializing YarnCluster\")\ncluster_start_time = time.time()\n\n# say your desired conda environment for workers is located at\n# /home/mkimmins/anaconda/bin/python\n# similar syntax for venv and python executable\ncluster = YarnCluster(\n    environment='conda:///home/mkimmins/anaconda/bin/python',\n    worker_vcores=2,\n    worker_memory=\"4GiB\")\n\nlogger.info(\n    \"Initializing YarnCluster: done in %.4f\",\n    time.time() -\n    cluster_start_time)\n\nlogger.info(\"Initializing Client\")\nclient = Client(cluster)\nlogger.info(\n    \"Initializing Client: done in %.4f\",\n    time.time() -\n    client_start_time)\n\n# Important and common misconfig is mismatched versions on nodes\nversions = dask_client.get_versions(check=True)\n```", "```py\nget_ipython().system('dask-yarn submit')\n\n'''\n--environment home/username/anaconda/bin/python --worker-count 20 \\\n--worker-vcores 2 --worker-memory 4GiB your_python_script.py\n'''\n\n# Since we already deployed and ran YARN cluster,\n# we replace YarnCluster(...) with from_current() to reference it\ncluster = YarnCluster.from_current()\n\n# This would give you YARN application ID\n# application_1516806604516_0019\n# status check, kill, view log of application\nget_ipython().system('dask-yarn status application_1516806604516_0019')\nget_ipython().system('dask-yarn kill application_1516806604516_0019')\nget_ipython().system('yarn logs -applicationId application_1516806604516_0019')\n```", "```py\nfrom dask_jobqueue import SLURMCluster\nfrom dask.distributed import Client\n\ndef create_slurm_clusters(cores, processes, workers, memory=\"16GB\",\n                          queue='regular', account=\"account\", username=\"user\"):\n    cluster = SLURMCluster(\n        #ensure walltime request is reasonable within your specific cluster\n        walltime=\"04:00:00\",\n        queue=queue,\n        account=account,\n        cores=cores,\n        processes=processes,\n        memory=memory,\n        worker_extra_args=[\"--resources GPU=1\"],\n        job_extra=['--gres=gpu:1'],\n        job_directives_skip=['--mem', 'another-string'],\n        job_script_prologue=[\n            '/your_path/pre_run_script.sh',\n            'source venv/bin/activate'],\n        interface='ib0',\n        log_directory='dask_slurm_logs',\n        python=f'srun -n 1 -c {processes} python',\n        local_directory=f'/dev/{username}',\n        death_timeout=300\n    )\n    cluster.start_workers(workers)\n    return cluster\n\ncluster = create_slurm_clusters(cores=4, processes=1, workers=4)\ncluster.scale(10)\nclient = Client(cluster)\n```", "```py\nimport time\nfrom dask import delayed\nfrom dask.distributed import Client, LocalCluster\n# Note we introduce progress bar for future execution in a distributed\n# context here\nfrom dask.distributed import progress\nfrom dask_jobqueue import SLURMCluster\nimport numpy as np\nimport logging\n\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(\n    level=logging.DEBUG,\n    format=\"%(asctime)s %(levelname)s %(name)s: %(message)s\")\n\ndef visit_url(i):\n    return \"Some fancy operation happened. Trust me.\"\n\n@delayed\ndef crawl(url, depth=0, maxdepth=1, maxlinks=4):\n    # some complicated and async job\n    # refer to Chapter 2 for full implementation of crawl\n    time.sleep(1)\n    some_output = visit_url(url)\n    return some_output\n\ndef main_event(client):\n    njobs = 100\n    outputs = []\n    for i in range(njobs):\n        # assume we have a queue of work to do\n        url = work_queue.deque()\n        output = crawl(url)\n        outputs.append(output)\n\n    results = client.persist(outputs)\n    logger.info(f\"Running main loop...\")\n    progress(results)\n\ndef cli():\n    cluster = create_slurm_clusters(cores=10, processes=10, workers=2)\n    logger.info(f\"Submitting SLURM job with jobscript: {cluster.job_script()}\")\n    client = Client(cluster)\n    main_event(client)\n\nif __name__ == \"__main__\":\n    logger.info(\"Initializing SLURM Cluster\")\n    cli()\n```", "```py\nfrom dask_jobqueue import SLURMCluster\nfrom dask import delayed\nfrom dask.distributed import Client\n\n#we give walltime of 4 hours to the cluster spawn\n#each Dask worker is told they have 5 min less than that for Dask to manage\n#we tell workers to stagger their start and close in a random interval of 5 min\n# some workers will die, but others will be staggered alive, avoiding loss\n# of job\n\ncluster = SLURMCluster(\n    walltime=\"04:00:00\",\n    cores=24,\n    processes=6\n    memory=\"8gb\",\n    #args passed directly to worker\n    worker_extra_args=[\"--lifetime\", \"235m\", \"--lifetime-stagger\", \"5m\"],\n    #path to the interpreter that you want to run the batch submission script\n    shebang='#!/usr/bin/env zsh',\n    #path to desired python runtime if you have a separate one\n    python='~/miniconda/bin/python'\n)\n\nclient = Client(cluster)\n```", "```py\nfrom dask.distributed import performance_report\n\nwith performance_report(filename=\"computation_report.html\"):\n    gnarl = da.random.beta(\n        1, 2, size=(\n            10000, 10000, 10), chunks=(\n            1000, 1000, 5))\n    x = da.random.random((10000, 10000, 10), chunks=(1000, 1000, 5))\n    y = (da.arccos(x) * gnarl).sum(axis=(1, 2))\n    y.compute()\n```", "```py\nfrom dask.distributed import get_task_stream\n\nwith get_task_stream() as ts:\n    gnarl = da.random.beta(1, 2, size=(100, 100, 10), chunks=(100, 100, 5))\n    x = da.random.random((100, 100, 10), chunks=(100, 100, 5))\n    y = (da.arccos(x) * gnarl).sum(axis=(1, 2))\n    y.compute()\nhistory = ts.data\n\n#display the task stream data as dataframe\nhistory_frame = pd.DataFrame(\n    history,\n    columns=[\n        'worker',\n        'status',\n        'nbytes',\n        'thread',\n        'type',\n        'typename',\n        'metadata',\n        'startstops',\n        'key'])\n\n#plot task stream\nts.figure\n```", "```py\nfrom distributed.diagnostics import MemorySampler\nfrom dask_kubernetes import KubeCluster\nfrom distributed import Client\n\ncluster = KubeCluster()\nclient = Client(cluster)\n\nms = MemorySampler()\n\n#some gnarly compute\ngnarl = da.random.beta(1, 2, size=(100, 100, 10), chunks=(100, 100, 5))\nx = da.random.random((100, 100, 10), chunks=(100, 100, 5))\ny = (da.arccos(x) * gnarl).sum(axis=(1, 2))\n\nwith ms.sample(\"memory without adaptive clusters\"):\n    y.compute()\n\n#enable adaptive scaling\ncluster.adapt(minimum=0, maximum=100)\n\nwith ms.sample(\"memory with adaptive clusters\"):\n    y.compute()\n\n#plot the differences\nms.plot(align=True, grid=True)\n```", "```py\nfrom dask.distributed import Client, LocalCluster\n\nclient = Client(cluster)  # Connect to distributed cluster and override default\n\nd = {'x': [3.0, 1.0, 0.2], 'y': [2.0, 0.5, 0.1], 'z': [1.0, 0.2, 0.4]}\nscores_df = dd.from_pandas(pd.DataFrame(data=d), npartitions=1)\n\ndef compute_softmax(partition, axis=0):\n    \"\"\" computes the softmax of the logits\n :param logits: the vector to compute the softmax over\n :param axis: the axis we are summing over\n :return: the softmax of the vector\n \"\"\"\n    if partition.empty:\n        return\n    import timeit\n    x = partition[['x', 'y', 'z']].values.tolist()\n    start = timeit.default_timer()\n    axis = 0\n    e = np.exp(x - np.max(x))\n    ret = e / np.sum(e, axis=axis)\n    stop = timeit.default_timer()\n    partition.log_event(\"softmax\", {\"start\": start, \"x\": x, \"stop\": stop})\n    dask.distributed.get_worker().log_event(\n        \"softmax\", {\"start\": start, \"input\": x, \"stop\": stop})\n    return ret\n\nscores_df.apply(compute_softmax, axis=1, meta=object).compute()\nclient.get_events(\"softmax\")\n```"]