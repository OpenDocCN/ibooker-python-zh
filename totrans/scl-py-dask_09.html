<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 9. Migrating Existing Analytic Engineering"><div class="chapter" id="ch09">
<h1><span class="label">Chapter 9. </span>Migrating Existing Analytic Engineering</h1>


<p>Many users will already have analytic work that is currently deployed and that they want to migrate over to Dask. This chapter will discuss the considerations, challenges, and experiences of users making the switch. The main migration pathway explored in the chapter is moving an existing big data engineering job from another distributed framework, such as Spark, into Dask.</p>






<section data-type="sect1" data-pdf-bookmark="Why Dask?"><div class="sect1" id="id72">
<h1>Why Dask?</h1>

<p>Here are some reasons to consider <a data-type="indexterm" data-primary="migration" id="id757"/>migrating to Dask from an existing job that is implemented in pandas, or distributed libraries like PySpark:</p>
<dl>
<dt>Python and PyData stack</dt>
<dd>
<p>Many data scientists and developers prefer using a Python-native stack, where they don’t have to switch between languages or styles.</p>
</dd>
<dt>Richer ML integrations with Dask APIs</dt>
<dd>
<p>Futures, delayed, and ML integrations require less glue code from the developer to maintain, and there are performance improvements from the more flexible task graph management Dask offers.</p>
</dd>
<dt>Fine-grained task management</dt>
<dd>
<p>Dask’s task graph is generated and maintained in real time during runtime, and users can access the task dictionary synchronously.</p>
</dd>
<dt>Debugging overhead</dt>
<dd>
<p>Some developer teams prefer the debugging experience in Python, as opposed to mixed Python and Java/Scala stacktrace.</p>
</dd>
<dt>Development overhead</dt>
<dd>
<p>The development step in Dask can be done locally with ease with the developer’s laptop, as opposed to needing to connect to a powerful cloud machine in order to experiment.</p>
</dd>
<dt>Management UX</dt>
<dd>
<p>Dask visualization tools tend to be more visually pleasing and intuitive to reason, with native graphviz rendering for task graphs.</p>
</dd>
</dl>

<p>These are not all of the benefits, but if any of them speak to you, it’s probably worth investing the time to consider moving the workload to Dask.
There are always trade-offs involved, so the next section will look at some of the limitations, followed by a road map to give you an idea of the scale of work involved in moving to Dask.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Limitations of Dask"><div class="sect1" id="id179">
<h1>Limitations of Dask</h1>

<p>Dask is relatively new, and the use <a data-type="indexterm" data-primary="migration" data-secondary="Dask limitations" id="id758"/>of Python data stack to perform large-scale extract, transform, load operations is also fairly new. There are limitations to Dask, which mainly arise from the fact that PyData stack has traditionally not been used to perform large-scale data workloads. At the time of writing, there are some limits to the system. However, they are being addressed by developers, and a lot of these deficiencies will be filled in. Some of the fine-grained considerations you should have are as follows:</p>
<dl>
<dt>Parquet scale limits</dt>
<dd>
<p>If Parquet <a data-type="indexterm" data-primary="Parquet" data-secondary="migration and" id="id759"/>data exceeds 10 TB in scale, there are issues at the fastparquet and PyArrow level that slow Dask down, and metadata management overhead can be overwhelming.</p>

<p>ETL workloads with Parquet files at 10 TB in scale and beyond, and that include a mutation, such as append and update, run into consistency issues.</p>
</dd>
<dt>Weak data lake integrations</dt>
<dd>
<p>PyData stack has not engaged much in the big data world traditionally, and the integrations on data lake management, such as Apache Iceberg, are missing.</p>
</dd>
<dt>High-level query optimization</dt>
<dd>
<p>Users of Spark would be familiar with the Catalyst optimizer that pushes down predicates for optimizing the physical work on the executors. This optimization layer is missing in Dask at the moment. Spark in its early years also did not have the Catalyst engine written yet, and there is work in progress to build this out for Dask.</p>
</dd>
</dl>

<p>Any list of limitations for a rapidly developing project like Dask may be out of date by the time you read it, so if any of these are blockers for your migration, make sure to check Dask’s status tracker.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Migration Road Map"><div class="sect1" id="id269">
<h1>Migration Road Map</h1>

<p>While no engineering work is linear in process, it’s always a good idea to have a road map in mind. We’ve laid out an example of migration steps as a non-exhaustive list of items a team might want to think through when planning its move:</p>

<ul>
<li>
<p>What kind of machines and containerization framework will we want to deploy Dask on, and what are their pros and cons?</p>
</li>
<li>
<p>Do we have tests to ensure our migration correctness and our desired goals?</p>
</li>
<li>
<p>What type of data is Dask able to ingest, and at what scale, and how does that differ from other platforms?</p>
</li>
<li>
<p>What is the computation framework of Dask, and how do we think in Dask and Pythonic ways to achieve the task?</p>
</li>
<li>
<p>How would we monitor and troubleshoot the code at runtime?</p>
</li>
</ul>

<p>We’ll start by looking at the types of clusters, which goes with the deployment framework, as it is often one of the issues requiring collaboration with other teams or organizations.</p>








<section data-type="sect2" data-pdf-bookmark="Types of Clusters"><div class="sect2" id="id73">
<h2>Types of Clusters</h2>

<p>If you are considering moving <a data-type="indexterm" data-primary="migration" data-secondary="clusters and" id="mgcl"/><a data-type="indexterm" data-primary="clusters" data-secondary="migration and" id="clmgr"/>your analytic engineering job, you probably have a system that’s provisioned to you by your organization. Dask is supported in many commonly used deployment and development environments, with some allowing more flexibility in scaling, dependency management, and support of heterogeneous worker types. We have used Dask on academic environments, on commodity cloud, and directly over VMs/containers; we’ve detailed the pros and cons, and some well-used and supported environments, in <a data-type="xref" href="app01.xhtml#appA">Appendix A</a>.</p>

<p><a data-type="xref" href="#ex_yarn_deployment_ch09_1685536092648">Example 9-1</a> shows an example of a YARN deployment. More examples and in-depth discussion can be found in <a data-type="xref" href="ch12.xhtml#ch12">Chapter 12</a>.</p>
<div id="ex_yarn_deployment_ch09_1685536092648" data-type="example">
<h5><span class="label">Example 9-1. </span>Deploying Dask on YARN with Dask-Yarn and skein</h5>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">dask_yarn</code> <code class="kn">import</code> <code class="n">YarnCluster</code>
<code class="kn">from</code> <code class="nn">dask.distributed</code> <code class="kn">import</code> <code class="n">Client</code>

<code class="c1"># Create a cluster where each worker has two cores and 8 GiB of memory</code>
<code class="n">cluster</code> <code class="o">=</code> <code class="n">YarnCluster</code><code class="p">(</code>
    <code class="n">environment</code><code class="o">=</code><code class="s1">'your_environment.tar.gz'</code><code class="p">,</code>
    <code class="n">worker_vcores</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code>
    <code class="n">worker_memory</code><code class="o">=</code><code class="s2">"4GiB"</code><code class="p">)</code>

<code class="c1"># Scale out to num_workers such workers</code>
<code class="n">cluster</code><code class="o">.</code><code class="n">scale</code><code class="p">(</code><code class="n">num_workers</code><code class="p">)</code>

<code class="c1"># Connect to the cluster</code>
<code class="n">client</code> <code class="o">=</code> <code class="n">Client</code><code class="p">(</code><code class="n">cluster</code><code class="p">)</code></pre></div>

<p>If your organization has multiple clusters that are supported, choosing one where you can self-serve dependency management, like Kubernetes, is beneficial.</p>

<p>For high-performance computing deployments using job queuing systems such as PBS, Slurm, MOAB, SGE, LSF, and HTCondor, you should <a data-type="indexterm" data-primary="Dask-jobqueue" id="id760"/><a data-type="indexterm" data-primary="job queues, migration and" id="id761"/>use Dask-jobqueue, as shown in <a data-type="xref" href="#ex_slurm_deployment_ch09_1685536141262">Example 9-2</a>.</p>
<div id="ex_slurm_deployment_ch09_1685536141262" data-type="example">
<h5><span class="label">Example 9-2. </span>Deploying Dask using jobqueue over Slurm</h5>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">dask_jobqueue</code> <code class="kn">import</code> <code class="n">SLURMCluster</code>
<code class="kn">from</code> <code class="nn">dask.distributed</code> <code class="kn">import</code> <code class="n">Client</code>

<code class="n">cluster</code> <code class="o">=</code> <code class="n">SLURMCluster</code><code class="p">(</code>
    <code class="n">queue</code><code class="o">=</code><code class="s1">'regular'</code><code class="p">,</code>
    <code class="n">account</code><code class="o">=</code><code class="s2">"slurm_caccount"</code><code class="p">,</code>
    <code class="n">cores</code><code class="o">=</code><code class="mi">24</code><code class="p">,</code>
    <code class="n">memory</code><code class="o">=</code><code class="s2">"500 GB"</code>
<code class="p">)</code>
<code class="n">cluster</code><code class="o">.</code><code class="n">scale</code><code class="p">(</code><code class="n">jobs</code><code class="o">=</code><code class="n">SLURM_JOB_COUNT</code><code class="p">)</code>  <code class="c1"># Ask for N jobs from Slurm</code>

<code class="n">client</code> <code class="o">=</code> <code class="n">Client</code><code class="p">(</code><code class="n">cluster</code><code class="p">)</code>

<code class="c1"># Auto-scale between 10 and 100 jobs</code>
<code class="n">cluster</code><code class="o">.</code><code class="n">adapt</code><code class="p">(</code><code class="n">minimum_jobs</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">maximum_jobs</code><code class="o">=</code><code class="mi">100</code><code class="p">)</code>
<code class="n">cluster</code><code class="o">.</code><code class="n">adapt</code><code class="p">(</code><code class="n">maximum_memory</code><code class="o">=</code><code class="s2">"10 TB"</code><code class="p">)</code>  <code class="c1"># Or use core/memory limits</code></pre></div>

<p>You likely have a shared filesystem already set up by your organization’s admin. Enterprise users might be used to already robustly provisioned distributed data sources, running on HDFS or blob storage like S3, which Dask works with seamlessly (see <a data-type="xref" href="#ex_s3_minio_rw">Example 9-3</a>). Dask also <a data-type="indexterm" data-primary="migration" data-secondary="networked filesystems and" id="id762"/><a data-type="indexterm" data-primary="filesystems" data-secondary="migration and" id="id763"/>integrates well with networked filesystems.</p>
<div id="ex_s3_minio_rw" data-type="example">
<h5><span class="label">Example 9-3. </span>Reading and writing to blob storage using MinIO</h5>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">s3fs</code>
<code class="kn">import</code> <code class="nn">pyarrow</code> <code class="k">as</code> <code class="nn">pa</code>
<code class="kn">import</code> <code class="nn">pyarrow.parquet</code> <code class="k">as</code> <code class="nn">pq</code>

<code class="n">minio_storage_options</code> <code class="o">=</code> <code class="p">{</code>
    <code class="s2">"key"</code><code class="p">:</code> <code class="n">MINIO_KEY</code><code class="p">,</code>
    <code class="s2">"secret"</code><code class="p">:</code> <code class="n">MINIO_SECRET</code><code class="p">,</code>
    <code class="s2">"client_kwargs"</code><code class="p">:</code> <code class="p">{</code>
        <code class="s2">"endpoint_url"</code><code class="p">:</code> <code class="s2">"http://ENDPOINT_URL"</code><code class="p">,</code>
        <code class="s2">"region_name"</code><code class="p">:</code> <code class="s1">'us-east-1'</code>
    <code class="p">},</code>
    <code class="s2">"config_kwargs"</code><code class="p">:</code> <code class="p">{</code><code class="s2">"s3"</code><code class="p">:</code> <code class="p">{</code><code class="s2">"signature_version"</code><code class="p">:</code> <code class="s1">'s3v4'</code><code class="p">}},</code>
<code class="p">}</code>

<code class="n">df</code><code class="o">.</code><code class="n">to_parquet</code><code class="p">(</code><code class="sa">f</code><code class="s1">'s3://s3_destination/</code><code class="si">{</code><code class="n">filename</code><code class="si">}</code><code class="s1">'</code><code class="p">,</code>
              <code class="n">compression</code><code class="o">=</code><code class="s2">"gzip"</code><code class="p">,</code>
              <code class="n">storage_options</code><code class="o">=</code><code class="n">minio_storage_options</code><code class="p">,</code>
              <code class="n">engine</code><code class="o">=</code><code class="s2">"fastparquet"</code><code class="p">)</code>


<code class="n">df</code> <code class="o">=</code> <code class="n">dd</code><code class="o">.</code><code class="n">read_parquet</code><code class="p">(</code>
    <code class="sa">f</code><code class="s1">'s3://s3_source/'</code><code class="p">,</code>
    <code class="n">storage_options</code><code class="o">=</code><code class="n">minio_storage_options</code><code class="p">,</code>
    <code class="n">engine</code><code class="o">=</code><code class="s2">"pyarrow"</code>
<code class="p">)</code></pre></div>

<p>We found that one of the surprisingly useful use cases is connecting directly to network storage such as NFS or FTP. When working on an academic dataset that’s large and clunky to work with (like a neuroimaging dataset that’s directly hosted by another organization), we could connect directly to the source filesystem. When using Dask this way, you should test out and consider network timeout allowances. Also note that, as of this <a data-type="indexterm" data-primary="migration" data-secondary="clusters and" data-startref="mgcl" id="id764"/><a data-type="indexterm" data-primary="clusters" data-secondary="migration and" data-startref="clmgr" id="id765"/>writing, Dask does not have a connector to data lakes such as Iceberg.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Development: Considerations"><div class="sect2" id="id270">
<h2>Development: Considerations</h2>

<p>Translating an existing logic to Dask is a fairly intuitive process. The following sections present some considerations if you’re coming from libraries such as R, pandas, and Spark, and how Dask might differ from them. Some of these differences result from moving from a different low-level implementation, such as Java, and others result from moving from single-machine code to a scaled implementation, as when you’re coming from pandas.</p>










<section data-type="sect3" data-pdf-bookmark="DataFrame performance"><div class="sect3" id="id180">
<h3>DataFrame performance</h3>

<p>If you have a job that you are <a data-type="indexterm" data-primary="migration" data-secondary="DataFrame performance and" id="mgdfpf"/><a data-type="indexterm" data-primary="Dask DataFrames" data-secondary="migration and" id="dfmgr"/>already running on a different platform, it’s likely you are already using columnar storage format, like Parquet, and reading at runtime. The data type mapping from Parquet to Python <a data-type="indexterm" data-primary="Python" data-secondary="data type mapping" id="id766"/>is inherently imprecise. It’s a good idea to check data types when reading in any data at runtime, and the same applies to DataFrame. If type inference fails, a column would default to object. Once you inspect and determine the type inference is imprecise, specifying data types can speed up your job a lot. Additionally, it’s always a good idea to check strings, floating point numbers, datetime, and arrays. If type errors arise, keeping in mind the upstream data sources and their data type is a good start. For example, if the Parquet is generated from protocol buffers, depending on what encode and decode engine was used, there are differences in null checks, float, doubles, and mixed precision types that are introduced in that stack.</p>

<p>When reading a large file from cloud storage into DataFrame, it may be useful to select columns ahead of time at the DataFrame read stage. Users from other platforms like Spark would be familiar with predicate push-down, where even if you don’t quite specify the columns desired, the platform would optimize and read only the required column for computation. Dask doesn’t quite provide that optimization yet.</p>

<p>Setting smart indices early <a data-type="indexterm" data-primary="indexing" data-secondary="migration and" id="id767"/><a data-type="indexterm" data-primary="migration" data-secondary="indexing and" id="id768"/>in the transformation of your DataFrame, prior to a complex query, can speed things up. Be aware that multi-indexing is not supported by Dask yet. A common workaround for a multi-indexed DataFrame from other platforms is mapping as a single concatenated column. For example, a simple workaround when coming from a non-Dask columnar dataset, like pandas <code>pd.MultiIndex</code> that has two columns as its index—say, <code>col1</code> and <code>col2</code>—would be to introduce a new column in Dask DataFrame  <code>col1_col2</code> as Dask.</p>

<p>During the transform stage, calling <code>.compute()</code> coalesces a large distributed Dask DataFrame to a single partition that should fit in RAM. If it does not, you may encounter problems. On the other hand, if you have filtered an input data of size 
<span class="keep-together">100 GB</span> down to 10 GB (say your RAM is 15 GB), it is probably a good idea to reduce the parallelism after the filter operation by invoking <code>.compute()</code>. You can check your DataFrame’s memory usage by invoking <code>df.memory_usage(deep=True).sum()</code> to determine if this is the right call. Doing this can be particularly useful if, after the filter operation, you have a complex and expensive shuffle operation, such as <code>.join()</code> with a new larger dataset.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Dask DataFrame is not value-mutable in the way that pandas DataFrame users might be familiar with. Since in-memory modification of a particular value is not possible, the only way to change a value would be a map operation over the whole column of the entire DataFrame. If an in-memory value change is something you have to do often, it is better to use an <a data-type="indexterm" data-primary="migration" data-secondary="DataFrame performance and" data-startref="mgdfpf" id="id769"/><a data-type="indexterm" data-primary="Dask DataFrames" data-secondary="migration and" data-startref="dfmgr" id="id770"/>external database.</p>
</div>
</div></section>










<section data-type="sect3" data-pdf-bookmark="Porting SQL to Dask"><div class="sect3" id="id74">
<h3>Porting SQL to Dask</h3>

<p>Dask does not natively offer a SQL engine, although <a data-type="indexterm" data-primary="migration" data-secondary="Dask-SQL and" id="mgdksq"/><a data-type="indexterm" data-primary="Dask-SQL" data-secondary="migration and" id="dsksql"/><a data-type="indexterm" data-primary="Structured Query Language" data-see="SQL" id="id771"/><a data-type="indexterm" data-primary="SQL (Structured Query Language)" data-secondary="Dask-SQL" id="id772"/>it does natively offer options to read from a SQL database. There are a number of different libraries you can use to interact with an existing SQL database, and to treat Dask DataFrame as a SQL table and run SQL queries directly (see <a data-type="xref" href="#ex_postgres_dataframe">Example 9-4</a>). Some allow you to even build and serve ML models directly using SQL ML syntax similar to that of Google’s BigQuery ML. In Examples <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch11.xhtml#Dask_sql_linear_regression">11-14</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch11.xhtml#Dask_sql_XGBClassifier">11-15</a>, we will show the use of Dask’s native <code>read_sql()</code> function and running SQL ML using Dask-SQL.</p>
<div id="ex_postgres_dataframe" data-type="example">
<h5><span class="label">Example 9-4. </span>Reading from a Postgres database</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">df</code> <code class="o">=</code> <code class="n">dd</code><code class="o">.</code><code class="n">read_sql_table</code><code class="p">(</code><code class="s1">'accounts'</code><code class="p">,</code> <code class="s1">'sqlite:///path/to/your.db'</code><code class="p">,</code>
                       <code class="n">npartitions</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">index_col</code><code class="o">=</code><code class="s1">'id'</code><code class="p">)</code></pre></div>

<p>FugueSQL provides SQL compatibility <a data-type="indexterm" data-primary="FugueSQL" id="id773"/><a data-type="indexterm" data-primary="SQL (Structured Query Language)" data-secondary="FugueSQL" id="id774"/>to PyData stack, including Dask. The project is in its infancy but seems promising. The main advantage of FugueSQL is that the code is portable between pandas, Dask, and Spark, giving a lot more interoperability. FugueSQL can run its SQL queries using <code>DaskExecutionEngine</code>, or you can run FugueSQL queries over a Dask DataFrame you already are using. Alternatively, you can run a quick SQL query on Dask DataFrame on your notebook as well. <a data-type="xref" href="#running_sql_over_dask_dataframe_ch09_1685553886459">Example 9-5</a> shows an example of using FugueSQL in a notebook. The downside of FugueSQL is that it requires the ANTLR library, which in turn requires a Java runtime.</p>
<div id="running_sql_over_dask_dataframe_ch09_1685553886459" data-type="example">
<h5><span class="label">Example 9-5. </span>Running SQL over Dask DataFrame with FugueSQL</h5>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">fugue_notebook</code> <code class="kn">import</code> <code class="n">setup</code>
<code class="n">setup</code> <code class="p">(</code><code class="n">is_lab</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="n">ur</code> <code class="o">=</code> <code class="p">(</code><code class="s1">'https://d37ci6vzurychx.cloudfront.net/trip-data/'</code>
      <code class="s1">'yellow_tripdata_2018-01.parquet'</code><code class="p">)</code>
<code class="n">df</code> <code class="o">=</code> <code class="n">dd</code><code class="o">.</code><code class="n">read_parquet</code><code class="p">(</code><code class="n">url</code><code class="p">)</code>

<code class="o">%%</code><code class="n">fsql</code> <code class="n">dask</code>
<code class="n">tempdf</code> <code class="o">=</code> <code class="n">SELECT</code> <code class="n">VendorID</code><code class="p">,</code> <code class="n">AVG</code> <code class="p">(</code><code class="n">total_amount</code><code class="p">)</code> <code class="n">AS</code> <code class="n">average_fare</code> <code class="n">FROM</code> <code class="n">df</code>
<code class="n">GROUP</code> <code class="n">BY</code> <code class="n">VendorID</code>

<code class="n">SELECT</code> <code class="o">*</code>
<code class="n">FROM</code> <code class="n">tempdf</code>
<code class="n">ORDER</code> <code class="n">BY</code> <code class="n">average</code> <code class="n">fare</code> <code class="n">DESC</code>
<code class="n">LIMIT</code> <code class="mi">5</code>
<code class="n">PRINT</code></pre></div>
<table>

<thead>
<tr>
<th/>
<th>VendorID</th>
<th>average_fare</th>
</tr>
</thead>
<tbody>
<tr>
<td><p><strong>0</strong></p></td>
<td><p>1</p></td>
<td><p>15.127384</p></td>
</tr>
<tr>
<td><p><strong>1</strong></p></td>
<td><p>2</p></td>
<td><p>15.775723</p></td>
</tr>
</tbody>
</table>

<pre data-type="programlisting" class="example-cont">schema: VendorID:long, average_fare:double</pre>

<p>An alternate method is to use the Dask-SQL library. This package uses Apache Calcite to provide the SQL parsing frontend and is used to query Dask DataFrames. With that library, you can pass most of the SQL-based operations to the Dask-SQL context, and it will be handled. The <a data-type="indexterm" data-primary="migration" data-secondary="Dask-SQL and" data-startref="mgdksq" id="id775"/><a data-type="indexterm" data-primary="Dask-SQL" data-secondary="migration and" data-startref="dsksql" id="id776"/>engine handles standard SQL inputs like <code>SELECT</code>, <code>CREATE TABLE</code>, but also ML model creation, with the <code>CREATE MODEL</code> syntax.</p>
</div></section>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Deployment Monitoring"><div class="sect2" id="id181">
<h2>Deployment Monitoring</h2>

<p>Like many other distributed <a data-type="indexterm" data-primary="migration" data-secondary="deployment monitoring" id="id777"/>libraries, Dask provides logs, and you can configure Dask logs to be sent to a storage system. The method will vary by the deployment environment, and whether Jupyter is involved.</p>

<p>The Dask client exposes the <code>get_worker_logs()</code> and <code>get_scheduler_logs()</code> methods, which can be accessed at runtime if desired. Additionally, similar to other distributed system logging, you can log events by topic, making them easily accessible by event types.</p>

<p><a data-type="xref" href="#ex_basic_logging_ch09_1685536244456">Example 9-6</a> is a toy example of adding a custom log event to the client.</p>
<div id="ex_basic_logging_ch09_1685536244456" data-type="example">
<h5><span class="label">Example 9-6. </span>Basic logging by topic</h5>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">dask.distributed</code> <code class="kn">import</code> <code class="n">Client</code>

<code class="n">client</code> <code class="o">=</code> <code class="n">Client</code><code class="p">()</code>
<code class="n">client</code><code class="o">.</code><code class="n">log_event</code><code class="p">(</code><code class="n">topic</code><code class="o">=</code><code class="s2">"custom_events"</code><code class="p">,</code> <code class="n">msg</code><code class="o">=</code><code class="s2">"hello world"</code><code class="p">)</code>
<code class="n">client</code><code class="o">.</code><code class="n">get_events</code><code class="p">(</code><code class="s2">"custom_events"</code><code class="p">)</code></pre></div>

<p><a data-type="xref" href="#structured-logging-on-workers_ch09_1685536283090">Example 9-7</a> builds on the previous example, but swaps in the execution context to a distributed cluster setup, for potentially more complex, custom structured events. The Dask client listens and accumulates these events, and we can inspect them. We start with a Dask DataFrame and then run some compute-heavy task. This example uses a <code>softmax</code> function, which is a common computation in many ML uses. A common ML dilemma is whether to use a more complex activation or loss function for accuracy, sacrificing performance (thereby running fewer training epochs but gaining a more stable gradient), or vice versa. To figure that out, we insert a code to log custom structured events to time the compute overhead of that specific function.</p>
<div id="structured-logging-on-workers_ch09_1685536283090" data-type="example">
<h5><span class="label">Example 9-7. </span>Structured logging on workers</h5>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">dask.distributed</code> <code class="kn">import</code> <code class="n">Client</code><code class="p">,</code> <code class="n">LocalCluster</code>

<code class="n">client</code> <code class="o">=</code> <code class="n">Client</code><code class="p">(</code><code class="n">cluster</code><code class="p">)</code>  <code class="c1"># Connect to distributed cluster and override default</code>

<code class="n">d</code> <code class="o">=</code> <code class="p">{</code><code class="s1">'x'</code><code class="p">:</code> <code class="p">[</code><code class="mf">3.0</code><code class="p">,</code> <code class="mf">1.0</code><code class="p">,</code> <code class="mf">0.2</code><code class="p">],</code> <code class="s1">'y'</code><code class="p">:</code> <code class="p">[</code><code class="mf">2.0</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">,</code> <code class="mf">0.1</code><code class="p">],</code> <code class="s1">'z'</code><code class="p">:</code> <code class="p">[</code><code class="mf">1.0</code><code class="p">,</code> <code class="mf">0.2</code><code class="p">,</code> <code class="mf">0.4</code><code class="p">]}</code>
<code class="n">scores_df</code> <code class="o">=</code> <code class="n">dd</code><code class="o">.</code><code class="n">from_pandas</code><code class="p">(</code><code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">d</code><code class="p">),</code> <code class="n">npartitions</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>


<code class="k">def</code> <code class="nf">compute_softmax</code><code class="p">(</code><code class="n">partition</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">):</code>
    <code class="sd">""" computes the softmax of the logits</code>
<code class="sd">    :param logits: the vector to compute the softmax over</code>
<code class="sd">    :param axis: the axis we are summing over</code>
<code class="sd">    :return: the softmax of the vector</code>
<code class="sd">    """</code>
    <code class="k">if</code> <code class="n">partition</code><code class="o">.</code><code class="n">empty</code><code class="p">:</code>
        <code class="k">return</code>
    <code class="kn">import</code> <code class="nn">timeit</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">partition</code><code class="p">[[</code><code class="s1">'x'</code><code class="p">,</code> <code class="s1">'y'</code><code class="p">,</code> <code class="s1">'z'</code><code class="p">]]</code><code class="o">.</code><code class="n">values</code><code class="o">.</code><code class="n">tolist</code><code class="p">()</code>
    <code class="n">start</code> <code class="o">=</code> <code class="n">timeit</code><code class="o">.</code><code class="n">default_timer</code><code class="p">()</code>
    <code class="n">axis</code> <code class="o">=</code> <code class="mi">0</code>
    <code class="n">e</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">exp</code><code class="p">(</code><code class="n">x</code> <code class="o">-</code> <code class="n">np</code><code class="o">.</code><code class="n">max</code><code class="p">(</code><code class="n">x</code><code class="p">))</code>
    <code class="n">ret</code> <code class="o">=</code> <code class="n">e</code> <code class="o">/</code> <code class="n">np</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">e</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="n">axis</code><code class="p">)</code>
    <code class="n">stop</code> <code class="o">=</code> <code class="n">timeit</code><code class="o">.</code><code class="n">default_timer</code><code class="p">()</code>
    <code class="n">partition</code><code class="o">.</code><code class="n">log_event</code><code class="p">(</code><code class="s2">"softmax"</code><code class="p">,</code> <code class="p">{</code><code class="s2">"start"</code><code class="p">:</code> <code class="n">start</code><code class="p">,</code> <code class="s2">"x"</code><code class="p">:</code> <code class="n">x</code><code class="p">,</code> <code class="s2">"stop"</code><code class="p">:</code> <code class="n">stop</code><code class="p">})</code>
    <code class="n">dask</code><code class="o">.</code><code class="n">distributed</code><code class="o">.</code><code class="n">get_worker</code><code class="p">()</code><code class="o">.</code><code class="n">log_event</code><code class="p">(</code>
        <code class="s2">"softmax"</code><code class="p">,</code> <code class="p">{</code><code class="s2">"start"</code><code class="p">:</code> <code class="n">start</code><code class="p">,</code> <code class="s2">"input"</code><code class="p">:</code> <code class="n">x</code><code class="p">,</code> <code class="s2">"stop"</code><code class="p">:</code> <code class="n">stop</code><code class="p">})</code>
    <code class="k">return</code> <code class="n">ret</code>


<code class="n">scores_df</code><code class="o">.</code><code class="n">apply</code><code class="p">(</code><code class="n">compute_softmax</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">meta</code><code class="o">=</code><code class="nb">object</code><code class="p">)</code><code class="o">.</code><code class="n">compute</code><code class="p">()</code>
<code class="n">client</code><code class="o">.</code><code class="n">get_events</code><code class="p">(</code><code class="s2">"softmax"</code><code class="p">)</code></pre></div>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Conclusion"><div class="sect1" id="id271">
<h1>Conclusion</h1>

<p>In this chapter you have reviewed the large questions and considerations of migrating existing analytic engineering work. You’ve also learned some of the feature differences of Dask compared to Spark, R, and pandas. Some features are not yet implemented by Dask, some are more robustly implemented by Dask, and others are inherent translational differences when moving a computation from a single machine to a distributed cluster. Since large-scale data engineering tends to use similar terms and names across many libraries, it’s often easy to overlook minute differences that lead to larger performance or correctness issues. Keeping them in mind will help you as you take your first journeys in Dask.</p>
</div></section>
</div></section></div></body></html>