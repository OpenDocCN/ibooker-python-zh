- en: Chapter 9\. Advanced Data with Ray
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 9 章\. 使用 Ray 进行高级数据处理
- en: Despite, or perhaps because of, data ecosystems’ rapid advances, you will likely
    end up needing to use multiple tools as part of your data pipeline. Ray Datasets
    allows data sharing among tools in the data and ML ecosystems. This allows you
    to switch tools without having to copy or move data. Ray Datasets supports Spark,
    Modin, Dask, and Mars and can also be used with ML tools like TensorFlow. You
    can also use Arrow with Ray to allow more tools to work on top of Datasets, such
    as R or even MATLAB. Ray Datasets act as a common format for all steps of your
    ML pipeline, simplifying legacy pipelines.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管数据生态系统的快速发展，您可能最终需要使用多种工具作为数据管道的一部分。Ray Datasets 允许在数据和 ML 生态系统中的工具之间共享数据。这使您可以在不复制或移动数据的情况下切换工具。Ray
    Datasets 支持 Spark、Modin、Dask 和 Mars，还可以与 TensorFlow 等 ML 工具一起使用。您还可以使用 Arrow 与
    Ray 结合使用，使更多工具可以在数据集上运行，如 R 或 MATLAB。Ray Datasets 作为 ML 管道各个步骤的通用格式，简化了传统管道。
- en: 'It all boils down to this: you can use the same dataset in multiple tools without
    worrying about the details. Internally, many of these tools have their own formats,
    but Ray and Arrow manage the translations transparently.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 一切都归结为这一点：您可以在多个工具中使用相同的数据集，而不必担心细节。在内部，许多这些工具都有自己的格式，但 Ray 和 Arrow 会透明地进行转换管理。
- en: In addition to simplifying your use of different tools, Ray also has a growing
    collection of built-in operations for Datasets. These built-in operations are
    being actively developed and are not intended to be as full-featured as those
    of the data tools built on top of Ray.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 除了简化您使用不同工具的过程之外，Ray 还拥有一个不断增长的内置操作集合，专为数据集设计。这些内置操作正在积极开发中，不打算与基于 Ray 构建的数据工具一样全面。
- en: Tip
  id: totrans-4
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: As covered in [“Ray Objects”](ch05.html#ray_objects), Ray Datasets’ default
    behavior may be different than you expect. You can enable object recovery by setting
    `enable_object_reconstruction=True` in `ray.init` to make Ray Datasets more resilient.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 如《光线对象》一文所述（[“Ray Objects”](ch05.html#ray_objects)），Ray Datasets 的默认行为可能与您的期望不同。您可以通过在
    `ray.init` 中设置 `enable_object_reconstruction=True` 来启用对象恢复，使得 Ray Datasets 更加弹性化。
- en: Ray Datasets continues to be an area of active development, including large
    feature additions between minor releases, and more functionality likely will be
    added by the time you are reading this chapter. Regardless, the fundamental principles
    of partitioning and multitool interoperability will remain the same.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Ray Datasets 仍然是积极开发的一个领域，包括在次要版本之间的大型功能添加，到您阅读本章时可能会添加更多功能。尽管如此，分区和多工具互操作性的基本原则将保持不变。
- en: Creating and Saving Ray Datasets
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建和保存 Ray Datasets
- en: As you saw in [Example 2-9](ch02.html#ds_hello), you can create datasets from
    local collections by calling `ray.data.from_items`. However, local collections
    naturally limit the scope of data that you can handle, so Ray supports many other
    options.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在 [Example 2-9](ch02.html#ds_hello) 中看到的，您可以通过调用 `ray.data.from_items` 从本地集合创建数据集。然而，本地集合自然地限制了您可以处理的数据范围，因此
    Ray 支持许多其他选项。
- en: Ray uses [Arrow](https://oreil.ly/GY0at) to load external data into Datasets,
    which support multiple file formats and filesystems. The formats, at present,
    are CSV, JSON, Parquet, NumPy, text, and raw binary. The functions for loading
    data follow the `read_[*format*]` pattern and are in the `ray.data` module, as
    shown in [Example 9-1](#load_csv_local_fs).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Ray 使用 [Arrow](https://oreil.ly/GY0at) 将外部数据加载到数据集中，支持多种文件格式和文件系统。目前支持的格式包括
    CSV、JSON、Parquet、NumPy、文本和原始二进制格式。加载数据的函数遵循 `read_[*format*]` 模式，并位于 `ray.data`
    模块中，如 [Example 9-1](#load_csv_local_fs) 所示。
- en: Example 9-1\. [Loading local data](https://oreil.ly/HP05n)
  id: totrans-10
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 9-1\. [加载本地数据](https://oreil.ly/HP05n)
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: When loading, you can specify a target `parallelism`, but Ray may be limited
    by the number of files being loaded. Picking a good value for your target parallelism
    is complicated and depends on numerous factors. You want to ensure that your data
    can fit easily in memory and take advantage of all of the machines in your cluster,
    while also not picking a number so high that the overhead of launching individual
    tasks exceeds the benefits. Generally, parallelism resulting in splits between
    hundreds of megabytes to tens of gigabytes is often considered a sweet spot.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载时，您可以指定目标 `parallelism`，但 Ray 可能会受到加载文件数量的限制。为您的目标并行度选择一个好的值是复杂的，并取决于多个因素。您希望确保您的数据可以轻松放入内存，并充分利用集群中的所有机器，同时又不要选择一个任务启动开销超过收益的数字。一般来说，导致分割在数百兆到数十吉字节之间的并行性常被认为是一个甜蜜点。
- en: Tip
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If you wish to customize the way Arrow loads your data, you can pass additional
    arguments, like `compression` or `buffer_size`, to Arrow through the `arrow_open_stream_args`
    parameter.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望自定义 Arrow 加载数据的方式，可以通过 `arrow_open_stream_args` 参数向 Arrow 传递额外的参数，如 `compression`
    或 `buffer_size`。
- en: Arrow has built-in native (fast) support for S3, HDFS, and regular filesystems.
    Ray automatically selects the correct built-in filesystem driver based on the
    path.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Arrow 在 S3、HDFS 和常规文件系统上具有内置的本机（快速）支持。Ray 会根据路径自动选择正确的内置文件系统驱动程序。
- en: Warning
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: When loading from a local filesystem, it is up to you to ensure that the file
    is available on all of the workers when running in distributed mode.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当从本地文件系统加载时，在分布式模式下运行时，您需要确保文件在所有工作器上都可用。
- en: Arrow, and by extension Ray, also uses [`fsspec`](https://oreil.ly/Tz32F), which
    supports a wider array of filesystems, including HTTPS (when aiohttp is installed).
    Unlike with the “built-in” filesystems, you need to manually specify the filesystem,
    as shown in [Example 9-2](#ex_load_from_https).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Arrow，以及其扩展 Ray，还使用 [`fsspec`](https://oreil.ly/Tz32F)，它支持更广泛的文件系统，包括 HTTPS（当安装
    aiohttp 时）。与“内置”文件系统不同，您需要手动指定文件系统，如 [示例 9-2](#ex_load_from_https) 所示。
- en: Example 9-2\. [Loading data over HTTPS](https://oreil.ly/HP05n)
  id: totrans-19
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-2\. [通过 HTTPS 加载数据](https://oreil.ly/HP05n)
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Warning
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: At present, the protocol is incorrectly stripped off, so you need to put it
    in twice. For example, when loading data from an HTTPS website, you would load
    from `https://https://[someurlhere].com`.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，协议被错误地剥离了，所以您需要将其重复两次。例如，当从 HTTPS 网站加载数据时，您应该从 `https://https://[someurlhere].com`
    加载。
- en: 'Ray has the ability to write in all the formats it can read from. The writing
    functions, like the reading functions, follow a pattern of `write_[*format*]`.
    A few minor differences exist between the read path and the write path. Instead
    of taking in a `parallelism` parameter, the write path always writes with the
    parallelism of the input dataset:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Ray 有能力以其可以从中读取的所有格式进行写入。写入函数与读取函数类似，遵循 `write_[*format*]` 模式。读取路径和写入路径之间存在一些细微差异。写入路径始终使用输入数据集的并行性，而不是接收
    `parallelism` 参数：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If Ray does not have I/O support for your desired format or filesystem, you
    should check to see whether any of the other tools that Ray supports does. Then,
    as covered in the next section, you can convert your dataset from/to the desired
    tool.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 Ray 没有对您期望的格式或文件系统提供 I/O 支持，请检查 Ray 支持的其他工具中是否有适用的。然后，如下一节所述，您可以将数据集从/转换为所需的工具。
- en: Using Ray Datasets with Different Tools
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用不同工具的 Ray 数据集
- en: Ray has built-in tooling to share data among the various data tools running
    on Ray. Most of these tools have their own internal representations of the data,
    but Ray handles converting the data as needed. Before you first use a dataset
    with Spark or Dask, you need to run a bit of setup code so that they delegate
    their execution to Ray, as shown in Examples [9-3](#ex_setup_dask) and [9-4](#ex_setup_spark).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Ray 具有内置的工具，可在运行在 Ray 上的各种数据工具之间共享数据。这些工具中的大多数都有其自己的数据内部表示，但 Ray 会根据需要转换数据。在首次使用数据集与
    Spark 或 Dask 之前，您需要运行一些设置代码，以便它们将其执行委托给 Ray，如示例 [9-3](#ex_setup_dask) 和 [9-4](#ex_setup_spark)
    所示。
- en: Example 9-3\. [Setting up Dask on Ray](https://oreil.ly/HP05n)
  id: totrans-28
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-3\. [在 Ray 上设置 Dask](https://oreil.ly/HP05n)
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Example 9-4\. [Setting up Dask on Spark](https://oreil.ly/HP05n)
  id: totrans-30
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-4\. [在 Spark 上设置 Dask](https://oreil.ly/HP05n)
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As with functions for reading and loading datasets, transfer-to-Ray functions
    are defined on the `ray.data` module and follow the `from_[*x*]` pattern, where
    `[*x*]` is the tool name. Similar to writing data, we convert datasets to a tool
    with a `to_[*x*]` function defined on the dataset, where `[*x*]` is the tool name.
    [Example 9-5](#raydataset0905) shows how to use this pattern to convert a Ray
    dataset into a Dask DataFrame.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 与用于读取和加载数据集的功能类似，将数据传输至 Ray 的功能在 `ray.data` 模块上定义，并遵循 `from_[*x*]` 模式，其中 `[*x*]`
    是工具名称。与写入数据类似，我们使用数据集上定义的 `to_[*x*]` 函数将数据集转换为工具，其中 `[*x*]` 是工具名称。[示例 9-5](#raydataset0905)
    展示了如何使用此模式将 Ray 数据集转换为 Dask DataFrame。
- en: Note
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Datasets do not use Ray’s runtime environments for dependencies, so you must
    have your desired tools installed in your worker image; see [Appendix B](app02.html#appB).
    This is more involved for Spark, as it requires the Java Virtual Machine (JVM)
    and other non-Python components.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集不使用 Ray 的运行时环境来处理依赖关系，因此您必须在工作器映像中安装所需的工具；请参阅 [附录 B](app02.html#appB)。对于
    Spark 而言，这更加复杂，因为它需要 Java 虚拟机 (JVM) 和其他非 Python 组件。
- en: Example 9-5\. [Ray dataset in Dask](https://oreil.ly/HP05n)
  id: totrans-35
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-5\. [Dask 中的 Ray 数据集](https://oreil.ly/HP05n)
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You are not limited to the tools that are built into Ray. If you have a new
    tool that supports Arrow, and you are using [Arrow-supported types](https://oreil.ly/qNbFA),
    `to_arrow_refs` gives you a zero-copy Arrow representation of your dataset. You
    can then use this list of Ray Arrow objects to pass into your tool, whether for
    model training or any other purpose. You will learn more about this in [“Using
    Built-in Ray Dataset Operations”](#builtinRayDSops).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 您不仅限于 Ray 内置的工具。如果您有一个支持 Arrow 的新工具，并且正在使用 [支持 Arrow 的类型](https://oreil.ly/qNbFA)，`to_arrow_refs`
    可以为您的数据集提供零拷贝的 Arrow 表示。然后，您可以使用 Ray Arrow 对象列表将其传递给您的工具，无论是用于模型训练还是其他任何目的。您将在
    [“使用内置 Ray 数据集操作”](#builtinRayDSops) 中了解更多信息。
- en: 'Many tools and languages can be connected with Arrow and Ray, including:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 许多工具和语言可以与 Arrow 和 Ray 连接，包括：
- en: '[Apache Spark](https://oreil.ly/o2m4s)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Apache Spark](https://oreil.ly/o2m4s)'
- en: '[Dask](https://oreil.ly/tqbJY)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Dask](https://oreil.ly/tqbJY)'
- en: '[Apache Parquet](https://oreil.ly/Mj0N8)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Apache Parquet](https://oreil.ly/Mj0N8)'
- en: '[Modin](https://oreil.ly/uSMt5)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Modin](https://oreil.ly/uSMt5)'
- en: '[pandas](https://oreil.ly/oX5FO)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[pandas](https://oreil.ly/oX5FO)'
- en: '[TensorFlow](https://oreil.ly/7sweI)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TensorFlow](https://oreil.ly/7sweI)'
- en: '[R](https://oreil.ly/41btG)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[R](https://oreil.ly/41btG)'
- en: '[JSON](https://oreil.ly/shYH5)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[JSON](https://oreil.ly/shYH5)'
- en: '[MATLAB](https://oreil.ly/tqqR2)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MATLAB](https://oreil.ly/tqqR2)'
- en: Tip
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Dask and Spark both have non-DataFrame collections—bags, arrays, and resilient
    distributed datasets (RDDs)—that cannot be converted with these APIs.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 和 Spark 都有非 DataFrame 集合——bags、arrays 和 resilient distributed datasets
    (RDDs)，这些无法使用这些 API 转换。
- en: Using Tools on Ray Datasets
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Ray 数据集上使用工具
- en: This section assumes you have a good understanding of the data-wrangling tools
    you’re going to use with Ray—either pandas or Spark. Pandas is ideal for users
    scaling Python analytics, and Spark is well suited for users connecting to big
    data tools. If you’re not familiar with the pandas APIs, you should check out
    [*Python for Data Analysis*](https://oreil.ly/IoSNu) by Wes McKinney (O’Reilly).
    New Spark users should check out [*Learning Spark*](https://oreil.ly/wmypt) by
    Jules Damji et al. (O’Reilly). If you want to go super deep, Holden recommends
    [*High Performance Spark*](https://oreil.ly/lyZ99) by Holden and Rachel Warren
    (O’Reilly).^([1](ch09.html#idm45354767432992))
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 本节假定您已经对将要在 Ray 上使用的数据整理工具——无论是 pandas 还是 Spark——有很好的理解。Pandas 对于扩展 Python 分析是理想的选择，而
    Spark 则非常适合连接大数据工具的用户。如果您对 pandas API 不熟悉，建议查阅 [*Python 数据分析*](https://oreil.ly/IoSNu)（Wes
    McKinney 著，O’Reilly 出版）。新的 Spark 用户应查阅 [*学习 Spark*](https://oreil.ly/wmypt)（Jules
    Damji 等著，O’Reilly 出版）。如果您想深入了解，Holden 推荐 [*高性能 Spark*](https://oreil.ly/lyZ99)（Holden
    和 Rachel Warren 著，O’Reilly 出版）。^([1](ch09.html#idm45354767432992))
- en: pandas-like DataFrames with Dask
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Dask 的类似 pandas 的 DataFrame
- en: '[Dask](https://oreil.ly/ylNqR) on Ray is an excellent choice for data preparation
    for ML, or scaling existing pandas code. Many initial Dask developers also worked
    on pandas, leading to a comparatively solid distributed pandas interface.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[Ray 上的 Dask](https://oreil.ly/ylNqR) 是为 ML 数据准备或扩展现有 pandas 代码的极佳选择。许多最初的
    Dask 开发者也参与了 pandas 的开发，因此具有相对稳固的分布式 pandas 接口。'
- en: Note
  id: totrans-54
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Portions of this section are based on the DataFrame chapter in [*Scaling Python
    with Dask*](https://oreil.ly/Fk0I6).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 本节部分内容基于 [*Scaling Python with Dask*](https://oreil.ly/Fk0I6) 中的 DataFrame 章节。
- en: Dask on Ray benefits from using Ray’s per node/container shared memory storage
    for data. This is especially important when doing operations like broadcast joins;
    in Dask the same data will need to be stored in each worker process.^([2](ch09.html#idm45354767410480))
    However, in Ray, it needs to be stored only once per node or container.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Ray 上使用 Dask 可以从 Ray 的每个节点/容器共享内存存储数据中获益。在执行像广播连接这样的操作时尤为重要；在 Dask 中，相同的数据将需要存储在每个工作进程中。^([2](ch09.html#idm45354767410480))
    然而，在 Ray 中，只需要在每个节点或容器中存储一次。
- en: Warning
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Unlike Ray, Dask is generally lazy, meaning it does not evaluate data until
    forced. This can make debugging a challenge as errors may appear several lines
    removed from their root cause.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Ray 不同，Dask 通常是惰性的，这意味着它在被迫之前不会评估数据。这可能会增加调试的难度，因为错误可能出现在距离其根本原因几行的地方。
- en: Most of the distributed components of Dask’s DataFrames use the three core building
    blocks `map_partitions`, `reduction`, and `rolling`. You mostly won’t need to
    call these functions directly; instead, you will use higher-level APIs, but understanding
    them and how they work is important to understanding how Dask works. `shuffle`
    is a critical building block of distributed DataFrames for reorganizing your data.
    Unlike the other building blocks, you may use it directly more frequently as Dask
    is unable to abstract away partitioning.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 的 DataFrame 的大多数分布式组件使用三个核心构建块 `map_partitions`、`reduction` 和 `rolling`。你通常不需要直接调用这些函数；而是会使用更高级别的
    API，但理解它们以及它们的工作原理对于理解 Dask 的工作方式至关重要。`shuffle` 是重新组织数据的分布式 DataFrame 的关键构建块。与其他构建块不同，你可能更频繁地直接使用它，因为
    Dask 无法抽象分区。
- en: Indexing
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 索引
- en: Indexing into a DataFrame is one of the powerful features of pandas, but comes
    with some restrictions when moving into a distributed system like Dask. Since
    Dask does not, by default, track the size of each partition, positional indexing
    by row is not supported. You can use positional indexing into columns, as well
    as label indexing for columns or rows.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在 pandas 中，对 DataFrame 进行索引是其强大功能之一，但在进入像 Dask 这样的分布式系统时，会有一些限制。由于 Dask 默认不跟踪每个分区的大小，因此不支持按行进行位置索引。你可以使用对列的位置索引，以及对列或行的标签索引。
- en: Indexing is frequently used to filter data to have only the components you need.
    We did this for San Francisco COVID-19 data by looking at just the case rates
    for people of all vaccine statuses, as shown in [Example 9-6](#index_covid_data).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 索引经常用于过滤数据，仅保留你需要的组件。我们通过查看只包括所有疫苗接种状态的病例率的方式，在 San Francisco COVID-19 数据中执行了这项工作，如
    [示例 9-6](#index_covid_data) 所示。
- en: Example 9-6\. [Dask DataFrame indexing](https://oreil.ly/IJaQ2)
  id: totrans-63
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-6\. [Dask DataFrame 索引](https://oreil.ly/IJaQ2)
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: If you truly need positional indexing by row, you can implement your own by
    computing the size of each partition and using this to select the desired partition
    subsets. This is very inefficient, so Dask avoids implementing directly so you
    make an intentional choice before doing this.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你确实需要按行进行位置索引，可以通过计算每个分区的大小并使用它来选择所需的分区子集来实现自己的方法。这种方法非常低效，因此 Dask 避免直接实现，所以在执行之前你需要做出有意识的选择。
- en: Shuffles
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Shuffles
- en: As mentioned in the previous chapter, shuffles are expensive. The primary causes
    of the expensive nature of shuffles are the comparative slowness of network speed
    (relative to to reading data from memory) and serialization overhead. These costs
    scale as the amount of data being shuffled increases, so Dask has techniques to
    reduce the amount of data being shuffled. These techniques depend on certain data
    properties or on the operation being performed.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一章节所述，shuffles 是昂贵的。造成 shuffle 昂贵的主要原因是网络速度相对于从内存读取数据而言较慢，以及序列化开销。随着被洗牌的数据量增加，这些成本会按比例增加，因此
    Dask 有技术手段来减少被洗牌的数据量。这些技术依赖于某些数据属性或正在执行的操作。
- en: Note
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: While understanding shuffles is important for performance, feel free to skip
    this section if your code is working well enough.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然理解 shuffle 对性能很重要，但如果你的代码运行良好，可以跳过本节。
- en: Rolling windows and map_overlap
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 滚动窗口和 map_overlap
- en: One situation that can trigger the need for a shuffle is a rolling window, where
    at the edges of a partition your function needs some records from its neighbors.
    A Dask DataFrame has a special `map_overlap` function in which you can specify
    a *look-after* window (also called a *look-ahead* window) and a *look-before*
    window (also called a *look-back* window) of rows to transfer (either an integer
    or a time delta). The simplest example taking advantage of this is a rolling average,
    shown in [Example 9-7](#rolling_date_ex).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 触发需要 shuffle 的一种情况是滚动窗口，在分区的边缘，你的函数需要一些来自其邻居的记录。Dask DataFrame 具有一个特殊的 `map_overlap`
    函数，你可以在其中指定一个*向后查看*窗口（也称为*向前查看*窗口）和一个*向前查看*窗口（也称为*向后查看*窗口）的行来传输（可以是整数或时间增量）。利用这一点的最简单示例是滚动平均，如
    [示例 9-7](#rolling_date_ex) 所示。
- en: Example 9-7\. [Dask DataFrame rolling average](https://oreil.ly/IJaQ2)
  id: totrans-72
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-7\. [Dask DataFrame 滚动平均](https://oreil.ly/IJaQ2)
- en: '[PRE7]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Using `map_overlap` allows Dask to transfer only the data needed. For this implementation
    to work correctly, your minimum partition size needs to be larger than your largest
    window.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `map_overlap` 允许 Dask 仅传输所需的数据。为了确保该实现能够正确运行，你的最小分区大小需要大于最大的窗口大小。
- en: Warning
  id: totrans-75
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Dask’s rolling windows will not cross multiple partitions. If your DataFrame
    is partitioned in such a way that the look-after or look-back is greater than
    the length of the neighbor’s partition, the results will either fail or be incorrect.
    Dask validates this for timedelta look-afters, but no such checks are performed
    for look-backs or integer look-afters.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 的滚动窗口不会跨越多个分区。如果你的 DataFrame 分区方式使得向后或向前查看的长度大于相邻分区的长度，结果将会失败或者不正确。Dask
    对于时间增量的向后查看进行了验证，但对于向前查看或整数增量则没有进行此类检查。
- en: Aggregations
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 聚合
- en: Aggregations are another special case that can reduce the amount of data that
    needs to be transferred over the network. Aggregations are functions that combine
    records. If you are coming from a map/reduce or Spark background, `reduceByKey`
    is the classic aggregation. Aggregations can either be *by key* or global across
    an entire DataFrame.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合是另一种可以减少需要通过网络传输的数据量的特殊情况。聚合是结合记录的函数。如果你来自于 map/reduce 或 Spark 的背景，`reduceByKey`
    是经典的聚合方法。聚合可以是*按键*进行的，也可以是整个 DataFrame 的全局聚合。
- en: To aggregate by key, you first need to call `groupby` with the column(s) representing
    the key, or the keying function to aggregate on. For example, calling `df.groupby("PostCode")`
    groups your DataFrame by postal code, or calling `df.groupby(["PostCode", "SicCodes"])`
    uses a combination of columns for grouping. Function-wise, many of the same pandas
    aggregates are available, but the performance of aggregates in Dask are very different
    than with local pandas DataFrames.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 要按键进行聚合，首先需要使用表示键的列（或用于聚合的键函数）调用 `groupby`。例如，调用 `df.groupby("PostCode")` 将根据邮政编码对
    DataFrame 进行分组，或者调用 `df.groupby(["PostCode", "SicCodes"])` 使用多列组合进行分组。在函数上，许多与
    pandas 相同的聚合函数是可用的，但在 Dask 中聚合的性能与本地 pandas DataFrame 有很大不同。
- en: Tip
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If you’re aggregating by partition key, Dask can compute the aggregation without
    needing a shuffle.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果按分区键进行聚合，Dask 可以在不需要洗牌的情况下计算聚合结果。
- en: The first way to speed up your aggregations is to reduce the columns that you
    are aggregating on, since the fastest data to process is no data. Finally, when
    possible, doing multiple aggregations at the same time reduces the number of times
    the same data needs to be shuffled. Therefore, you need to compute the average
    and the max, you should compute both at the same time, as shown in [Example 9-8](#max_mean).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 加速聚合的第一种方法是减少进行聚合的列，因为处理速度最快的数据是没有数据。最后，如果可能，同时执行多个聚合可以减少多次洗牌相同数据的次数。因此，如果需要计算平均值和最大值，应该像
    [示例 9-8](#max_mean) 中显示的那样同时计算两者。
- en: Example 9-8\. [Computing a Dask DataFrame max and mean together](https://oreil.ly/IJaQ2)
  id: totrans-83
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-8\. [同时计算 Dask DataFrame 的最大值和平均值](https://oreil.ly/IJaQ2)
- en: '[PRE8]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: For distributed systems like Dask, if an aggregation can be partially evaluated
    and then merged, you can potentially combine some records pre-shuffle. Not all
    partial aggregations are created equal. What matters with partial aggregations
    is the amount of data reduced when merging values with the same key, as compared
    to the storage space used by the original multiple values.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像 Dask 这样的分布式系统，如果可以部分评估然后合并聚合，你可以在预洗牌前合并一些记录。并非所有部分聚合都是相等的。部分聚合重要的是当合并具有相同键的值时减少的数据量，与原始多个值使用的存储空间相比。
- en: The most efficient aggregations take a sublinear amount of space regardless
    of the number of records. Some of these can take constant space such as sum, count,
    first, minimum, maximum, mean, and standard deviation. More complicated tasks,
    like quantiles and distinct counts, also have sublinear approximation options.
    These approximation options can be great, as exact answers can require linear
    growth in storage.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最有效的聚合方式可以在不考虑记录数量的情况下占用亚线性的空间量。其中一些可以占用恒定空间，如 sum、count、first、minimum、maximum、mean
    和 standard deviation。更复杂的任务，如分位数和不同计数，也有亚线性近似选项。这些近似选项非常有效，因为精确答案可能需要线性增长的存储空间。
- en: Some aggregation functions are not sublinear in growth, but “tend to” or “might”
    not grow too quickly. Counting the distinct values is in this group, but if all
    your values are unique, there is no space-saving.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一些聚合函数的增长不是亚线性的，但“倾向于”或“可能”增长不会太快。计算不同值的数量属于此类别，但如果所有值都是唯一的，则没有节省空间。
- en: To take advantage of efficient aggregations, you need to use a built-in aggregation
    from Dask, or write your own using Dask’s aggregation class. Whenever you can,
    use a built-in. Built-ins not only require less effort but also are often faster.
    Not all of the pandas aggregates are directly supported in Dask, so sometimes
    your only choice is to write your own aggregate.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 要利用高效的聚合功能，你需要使用 Dask 提供的内置聚合，或者使用 Dask 的聚合类自己编写。在可能的情况下，使用内置聚合。内置聚合不仅需要更少的工作量，而且通常更快。并非所有的
    pandas 聚合在 Dask 中都直接支持，因此有时你的唯一选择是编写自己的聚合。
- en: 'If you choose to write your own aggregate, you have three functions to define:
    `chunk` for handling each group-partition/chunk, `agg` to combine the results
    of `chunk` between partitions, and (optionally) `finalize` to take the result
    of `agg` and produce a final value.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果选择编写自己的聚合，你需要定义三个函数：`chunk` 处理每个组/分区块，`agg` 组合分区之间 `chunk` 的结果，以及（可选的）`finalize`
    用于获取 `agg` 的结果并生成最终值。
- en: The fastest way to understand how to use partial aggregation is by looking at
    an example that uses all three functions. Using weighted average in [Example 9-9](#weight_avg)
    can help you think of what is needed for each function. The first function needs
    to compute the weighted values and the weights. The `agg` function combines these
    by summing each side part of the tuple. Finally, the `finalize` function divides
    the total by the weights.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 理解如何使用部分聚合的最快方法是查看一个使用所有三个函数的示例。在 [Example 9-9](#weight_avg) 中使用加权平均值可以帮助你思考每个函数所需的内容。第一个函数需要计算加权值和权重。`agg`
    函数通过对元组的各部分求和来组合这些值。最后，`finalize` 函数将总和除以权重。
- en: Example 9-9\. [Dask custom aggregate](https://oreil.ly/IJaQ2)
  id: totrans-91
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 9-9\. [Dask 自定义聚合](https://oreil.ly/IJaQ2)
- en: '[PRE9]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In some cases, like a pure summation, you don’t need to do any post-processing
    on `agg`’s output, so you can skip the `finalize` function.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，比如纯求和，你不需要在 `agg` 的输出上进行任何后处理，因此可以跳过 `finalize` 函数。
- en: Not all aggregations must be by key; you can also compute aggregations across
    all rows. Dask’s custom aggregation interface, however, is only exposed with by-key
    operations.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有的聚合都必须按键进行；你还可以跨所有行计算聚合。然而，Dask 的自定义聚合接口仅在按键操作中暴露。
- en: Dask’s built-in full DataFrame aggregations use a lower-level interface called
    `apply_contact_apply`, for partial aggregations. Rather than learn two different
    APIs for partial aggregations, we prefer to do a static `groupby` by providing
    a constant grouping function. This way, we have to know only one interface for
    aggregations. You can use this to find the aggregate COVID-19 numbers across the
    DataFrame, as shown in [Example 9-10](#agg_entire).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 的内置完整 DataFrame 聚合使用一个称为 `apply_contact_apply` 的低级接口，用于部分聚合。与学习两种不同的部分聚合
    API 相比，我们更倾向于通过提供一个常量分组函数来进行静态 `groupby`。这样，我们只需了解一个聚合接口。你可以使用这个方法在整个 DataFrame
    中找到聚合 COVID-19 数字的方式，如 [Example 9-10](#agg_entire) 所示。
- en: Example 9-10\. [Aggregating the entire DataFrame](https://oreil.ly/IJaQ2)
  id: totrans-96
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 9-10\. [聚合整个 DataFrame](https://oreil.ly/IJaQ2)
- en: '[PRE10]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'If built-in aggregations exist, they will likely be better than anything we
    may be able to write. Sometimes a partial aggregation is partially implemented,
    as in the case of Dask’s HyperLogLog: it is implemented only for full DataFrames.
    You can often translate simple aggregations by using `apply_contact_apply` or
    `aca` by copying the `chunk` function, using the `combine` parameter for `agg`,
    and using the `aggregate` parameter for `finalize`. This is shown via porting
    Dask’s HyperLogLog implementation in [Example 9-11](#custom_agg_hyperloglog).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果存在内置的聚合方法，它们很可能比我们能写的任何东西都要好。有时部分聚合是部分实现的，就像 Dask 的 HyperLogLog 一样：它仅适用于完整的
    DataFrames。你可以通过复制 `chunk` 函数，使用 `agg` 的 `combine` 参数，以及使用 `finalize` 的 `aggregate`
    参数来转换简单的聚合。这在将 Dask 的 HyperLogLog 实现移植到 [Example 9-11](#custom_agg_hyperloglog)
    中有所展示。
- en: Example 9-11\. [Wrapping Dask’s HyperLogLog in `dd.Aggregation`](https://oreil.ly/IJaQ2)
  id: totrans-99
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 9-11\. [将 Dask 的 HyperLogLog 封装在 `dd.Aggregation` 中](https://oreil.ly/IJaQ2)
- en: '[PRE11]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Slow/inefficient aggregations (or those likely to cause an out-of-memory exception)
    use storage proportional to the records being aggregated. Examples from this slow
    group include making a list and naively computing exact quantiles.^([3](ch09.html#idm45354766947536))
    With these slow aggregates, using Dask’s aggregation class has no benefit over
    the `apply` API, which you may wish to use for simplicity. For example, if you
    just wanted a list of employer IDs by postal code, rather than having to write
    three functions, you could use a one-liner like `df.groupby("PostCode")["EmployerId"].apply(lambda
    g: list(g))`. Dask implements the `apply` function as a full shuffle, which is
    covered in the next section.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '慢/低效的聚合操作（或者可能导致内存不足异常的操作）会使用与正在聚合的记录成比例的存储空间。这些慢操作的例子包括制作列表和简单地计算精确分位数。^([3](ch09.html#idm45354766947536))
    对于这些慢聚合操作，使用Dask的聚合类并不能比`apply` API带来好处，后者可能更简单。例如，如果你只想通过邮政编码获取雇主ID列表，而不想编写三个函数，你可以使用像`df.groupby("PostCode")["EmployerId"].apply(lambda
    g: list(g))`这样的一行代码。Dask将`apply`函数实现为完全的洗牌操作，这将在下一节中讨论。'
- en: Warning
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Dask is unable to apply partial aggregations when you use the `apply` function.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用`apply`函数时，Dask无法应用部分聚合。
- en: Full shuffles and partitioning
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 完全洗牌和分区化
- en: Sorting is inherently expensive in distributed systems because it most often
    requires a *full shuffle*. Full shuffles are sometimes an unavoidable part of
    working in Dask. Counterintuitively, while full shuffles are themselves slow,
    you can use them to speed up future operations that are all happening on the same
    grouping key(s). As mentioned in the aggregation section, one of the ways a full
    shuffle is triggered is by using the `apply` method when partitioning is not aligned.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式系统中，排序通常是昂贵的，因为它通常需要进行*完全洗牌*。有时，完全洗牌是使用Dask时不可避免的部分。有趣的是，虽然完全洗牌本身是慢的，但你可以用它来加速未来在相同分组键上进行的操作。正如在聚合部分提到的，通过在分区不对齐时使用`apply`方法，就会触发完全洗牌的一种方式。
- en: Partitioning
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分区
- en: You will most commonly use full shuffles to repartition your data. It’s important
    to have the right partitioning when dealing with aggregations, rolling windows,
    or look-ups/indexing. As discussed in [“Rolling windows and map_overlap”](#rollingWindowsMapOL),
    Dask cannot do more than one partition’s worth of look-ahead or look-behind, so
    having the right partitioning is required to get correct results. For most other
    operations, having incorrect partitioning will slow down your job.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在重新分区数据时，你最常使用完全洗牌。在处理聚合、滚动窗口或查找/索引时，拥有正确的分区是很重要的。如“滚动窗口和map_overlap”章节中讨论的，Dask不能做超过一个分区的前视或后视，因此需要正确的分区才能获得正确的结果。对于大多数其他操作来说，分区不正确会减慢作业速度。
- en: 'Dask has three primary methods for controlling the partitioning of a DataFrame:
    `set_index`, `repartition`, and `shuffle`. You use `set_index` when the partitioning
    is being changed to a new key/index. `repartition` keeps the same key/index but
    changes the splits. `repartition` and `set_index` take similar parameters, with
    `repartition` not taking an index key name. `shuffle` is a bit different since
    it does not produce a “known” partitioning scheme that operations like `groupby`
    can take advantage of.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Dask有三种主要方法来控制DataFrame的分区：`set_index`、`repartition`和`shuffle`。当分区正在更改为新的键/索引时，使用`set_index`。`repartition`保持相同的键/索引，但更改分片。`repartition`和`set_index`使用类似的参数，但`repartition`不接受索引键名。`shuffle`有些不同，因为它不生成“已知”的分区方案，像`groupby`这样的操作无法利用它。
- en: The first step of getting the right partitioning for your DataFrame is to decide
    whether you want an index. Indexes are useful for pretty much any by-key type
    of operation, including filtering data, grouping, and, of course, indexing. One
    such by-key operation would be a `groupby`; the column being grouped on could
    be a good candidate for the key. If you use a rolling window over a column, that
    column must be the key, which makes choosing the key relatively easy. Once you’ve
    decided on an index, you can call `set_index` with the column name of the index
    (e.g., `set_index("PostCode")`). This will, under most circumstances, result in
    a shuffle, so it’s a good time to size your partitions.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 获取DataFrame的正确分区的第一步是决定是否需要索引。索引对于几乎任何按键类型的操作都很有用，包括数据过滤、分组和当然索引。一个这样的按键操作可能是`groupby`；正在分组的列可能是一个很好的键的候选。如果在某列上使用滚动窗口，则该列必须是键，这使得选择键相对容易。一旦确定了索引，可以使用索引列名调用`set_index`（例如，`set_index("PostCode")`）。在大多数情况下，这将导致重新分区，因此现在是调整分区大小的好时机。
- en: Tip
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If you’re unsure of the current key used for partitioning, you can check the
    `index` property to see the partitioning key.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不确定当前用于分区的键，可以检查`index`属性来查看分区键。
- en: 'Once you’ve chosen your key, the next question is how to size your partitions.
    The advice in [“Partitioning”](#basic_partitioning) generally applies here: shoot
    for enough partitions to keep each machine busy, but keep in mind the general
    sweet spot of 100 MB to 1 GB. Dask generally computes pretty even splits if you
    give it a target number of partitions.^([4](ch09.html#idm45354766886064)) Thankfully,
    `set_index` will also take `npartitions`. To repartition the data by postal code,
    with 10 partitions, you would add `set_index("PostCode", npartitions=10)`; otherwise,
    Dask will default to the number of input partitions.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦选择了键，下一个问题是如何确定分区的大小。在“分区”中的建议通常适用于这里：努力使每台计算机保持忙碌，但要记住一般的最佳区间为100 MB到1 GB。如果给定目标分区数，Dask通常会计算出相当均匀的分割。^([4](ch09.html#idm45354766886064))
    幸运的是，`set_index`也会接受`npartitions`。要通过邮政编码重新分区数据，并设定为10个分区，可以添加`set_index("PostCode",
    npartitions=10)`；否则，Dask将默认使用输入分区的数量。
- en: If you plan to use rolling windows, you will likely need to ensure that you
    have the right size (in terms of key range) covered in each partition. To do this
    as part of `set_index`, you would need to compute your own divisions to ensure
    that each partition has the right range of records present. Divisions are specified
    as a list starting from the minimal value of the first partition up to the maximum
    value of the last partition. Each value in between is a “cut” point for between
    the pandas DataFrames that make up the Dask DataFrame. To make a DataFrame with
    `[0, 100) [100, 200), (300, 500]`, you would write `df.set_index("Num​Employees",
    divisions=[0, 100, 200, 300, 500])`. Similarly for the date range, to support
    a rolling window of up to seven days, from the start of the pandemic to this writing,
    is shown in [Example 9-12](#set_index_with_rolling_window).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您计划使用滚动窗口，可能需要确保每个分区中涵盖了正确大小的（按键范围）记录。为了在`set_index`的一部分中执行此操作，您需要计算自己的分区，以确保每个分区中包含正确范围的记录。分区被指定为一个列表，从第一个分区的最小值到最后一个分区的最大值。每个值之间是用于分割构成Dask
    DataFrame的pandas DataFrame的“切点”。要创建一个包含`[0, 100) [100, 200), (300, 500]`的DataFrame，您可以编写`df.set_index("Num​Employees",
    divisions=[0, 100, 200, 300, 500])`。类似地，为了支持从大流行开始到本文撰写时长达七天的滚动窗口，日期范围如示例9-12所示，需要写成[Example 9-12](#set_index_with_rolling_window)。
- en: Example 9-12\. [Dask DataFrame rolling window with `set_index`](https://oreil.ly/IJaQ2)
  id: totrans-114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-12\. [使用`set_index`的Dask DataFrame滚动窗口](https://oreil.ly/IJaQ2)
- en: '[PRE12]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Warning
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Dask, including for rolling time windows, assumes that your partition index
    is *monotonically increasing*—strictly increasing with no repeated values (e.g.,
    1, 4, 7 is monotically increasing, but 1, 4, 4, 7 is not).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Dask，包括滚动时间窗口，在处理时假定您的分区索引是*单调递增*的——严格递增，没有重复的值（例如，1、4、7是单调递增的，但1、4、4、7不是）。
- en: So far, you’ve had to specify the number of partitions, or the specific divisions,
    but you might be wondering if Dask can just figure this out itself. Thankfully,
    Dask’s repartition function has the ability to pick divisions from a target size.
    However, doing this is a nontrivial cost as Dask must evaluate the DataFrame as
    well as the repartition itself. [Example 9-13](#repartition_ex) shows how to have
    Dask calculate the divisions from a desired partition size in bytes.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您必须指定分区的数量或特定的分区，但您可能想知道 Dask 是否可以自行确定这一点。幸运的是，Dask 的 `repartition` 函数可以从目标大小选择分区。然而，这样做是一个不小的成本，因为
    Dask 必须评估 DataFrame 以及重新分区本身。[示例 9-13](#repartition_ex) 展示了如何让 Dask 根据所需的分区大小（以字节为单位）计算分区。
- en: Example 9-13\. [Dask DataFrame automatic partitioning](https://oreil.ly/IJaQ2)
  id: totrans-119
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-13\. [Dask DataFrame 自动分区](https://oreil.ly/IJaQ2)
- en: '[PRE13]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Warning
  id: totrans-121
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Dask’s `set_index` has a similar `partition_size` parameter, but as of the writing,
    it does not work.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 截至撰写时，Dask 的 `set_index` 有一个类似的 `partition_size` 参数，但尚不起作用。
- en: As you’ve seen at the start of this chapter when writing DataFrames, each partition
    is given its own file, but sometimes this can result in files that are too big
    or too small. Some tools can accept only one file as input, so you need to repartition
    everything into a single partition. Other times, the data storage system is optimized
    for certain file sizes, like the Hadoop Distributed File System (HDFS) default
    block size of 128 MB. The good news is you can use `repartition` or `set_index`
    to get your desired output structure.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写 DataFrames 时，每个分区都有自己的文件，但有时这可能导致文件过大或过小。有些工具只能接受单个文件作为输入，因此您需要将所有内容重新分区为单个分区。其他情况下，数据存储系统针对特定文件大小进行了优化，例如
    Hadoop 分布式文件系统（HDFS）的默认块大小为 128 MB。好消息是，您可以使用 `repartition` 或 `set_index` 来获得所需的输出结构。
- en: Embarrassingly Parallel Operations
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 尴尬并行操作
- en: Dask’s `map_partitions` function applies a function to each of the partitions
    underlying pandas DataFrames, and the result is also a pandas DataFrame. Functions
    implemented with `map_partitions` are embarrassingly parallel since they don’t
    require any inter-worker transfer of data. In [embarrassingly parallel problems](https://oreil.ly/NFYHB),
    the overhead of distributed computing and communication is low.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 的 `map_partitions` 函数将函数应用于每个底层 pandas DataFrame 的分区，结果也是一个 pandas DataFrame。使用
    `map_partitions` 实现的函数由于不需要任何工作节点之间的数据传输，因此是尴尬并行的。在 [尴尬并行问题](https://oreil.ly/NFYHB)
    中，分布式计算和通信的开销很低。
- en: '`map_partitions` implements `map`, and many row-wise operations. If you want
    to use a row-wise operation that you find missing, you can implement it yourself,
    as shown in [Example 9-14](#filna_ex).'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`map_partitions` 实现了 `map` 和许多逐行操作。如果您想使用一个在逐行操作中找不到的函数，您可以像 [示例 9-14](#filna_ex)
    中所示自行实现它。'
- en: Example 9-14\. [Dask DataFrame `fillna`](https://oreil.ly/IJaQ2)
  id: totrans-127
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-14\. [Dask DataFrame `fillna`](https://oreil.ly/IJaQ2)
- en: '[PRE14]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: You aren’t limited to calling pandas built-ins as in this example. Provided
    that your function takes and returns a DataFrame, you can do pretty much anything
    you want inside `map_partitions`.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 您不仅限于像本例中调用 pandas 内置函数。只要您的函数接受并返回 DataFrame，您几乎可以在 `map_partitions` 中实现任何想做的事情。
- en: The full pandas API is too long to cover in this chapter, but if a function
    can operate on a row-by-row basis without any knowledge of the rows before or
    after, it may already be implemented in Dask DataFrames using `map_partitions`.
    If not, you can also implement it yourself using the pattern from [Example 9-14](#filna_ex).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 本章节中无法覆盖完整的 pandas API，但如果一个函数可以在逐行处理时不需要了解前后的行，则可能已经在 Dask DataFrames 中使用 `map_partitions`
    实现。如果没有，您也可以使用 [示例 9-14](#filna_ex) 中的模式自行实现。
- en: When using `map_partitions` on a DataFrame, you can change anything about each
    row, including the key that it is partitioned on. If you *are* changing the values
    in the partition key, you *must* either clear the partitioning information on
    the resulting DataFrame with `clear_divisions` *or* specify the correct indexing
    with `set_index`, which you’ll learn more about in the next section.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 当在 DataFrame 上使用 `map_partitions` 时，您可以更改每行的任何内容，包括其分区的键。如果您更改了分区键中的值，您必须使用
    `clear_divisions` 清除生成的 DataFrame 上的分区信息，或者使用 `set_index` 指定正确的索引，关于这一点您将在下一节中了解更多。
- en: Warning
  id: totrans-132
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Incorrect partitioning information can result in incorrect results, not just
    exceptions, as Dask may miss relevant data.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 不正确的分区信息可能导致不正确的结果，而不仅仅是异常，因为 Dask 可能会错过相关数据。
- en: Working with Multiple DataFrames
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理多个 DataFrames
- en: pandas and Dask have four common functions for combining DataFrames. At the
    root is the `concat` function, which allows joining DataFrames on any axis. Concatenating
    DataFrames is generally slower in Dask since it involves inter-worker communication.
    The other three functions are `join`, `merge`, and `append`, which all implement
    special cases for common situations on top of `concat`, and have slightly different
    performance considerations. Having good divisions/partitioning, in terms of key
    selection and number of partitions, makes a huge difference when working on multiple
    DataFrames.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: pandas和Dask有四个常见的函数用于合并DataFrames。在根上是`concat`函数，它允许在任何轴上连接DataFrames。在Dask中，连接DataFrames通常较慢，因为它涉及工作节点之间的通信。另外三个函数是`join`，`merge`和`append`，它们都在`concat`之上实现了常见情况的特殊情况，并且具有略有不同的性能考虑。在处理多个DataFrames时，良好的分区和划分选择对性能有很大影响。
- en: Dask’s `join` and `merge` functions take most of the standard pandas arguments
    along with an extra, optional, one. `npartitions` specifies a target number of
    output partitions, but is used for only hash-based joins (which you’ll learn about
    in [“Multi-DataFrame internals”](#multiDFI)). Both `join` and `merge` automatically
    repartition your input DataFrames if needed. This is great, as you might not know
    the partitioning, but since repartitioning can be slow, explicitly using the lower-level
    `concat` function when you don’t expect any partitioning changes to be needed
    can help catch performance problems early. Dask’s join takes only more than two
    DataFrames at a time when doing a left or outer join type.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Dask的`join`和`merge`函数除了标准的pandas参数外，还有一个额外的可选参数。`npartitions`指定了目标输出分区的数量，但仅用于基于哈希的连接（您将在[“多DataFrame内部”](#multiDFI)中了解到）。`join`和`merge`会根据需要自动重新分区输入的DataFrames。这很好，因为您可能不知道分区情况，但由于重新分区可能很慢，明确使用较低级别的`concat`函数可以帮助及早发现性能问题。Dask的join在进行左连接或外连接时一次只能接受两个以上的DataFrames。
- en: Tip
  id: totrans-137
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Dask has special logic to speed up multi-DataFrame joins, so in most cases,
    rather than do `a.join(b).join(c).join(d).join(e)`, you will benefit from doing
    `a.join([b, c, d, e])`. However, if you are performing a left join with a small
    dataset, the first syntax may be more efficient.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Dask具有专门的逻辑来加速多DataFrame的连接，因此在大多数情况下，与`a.join(b).join(c).join(d).join(e)`相比，使用`a.join([b,
    c, d, e])`将更加有利。但是，如果您执行与小数据集的左连接，则第一种语法可能更高效。
- en: When you combine (via `concat`) DataFrames by row (similar to a SQL UNION) the
    performance depends on whether divisions of the DataFrames being combined are
    well ordered. We call the divisions of a series of DataFrames *well ordered* if
    all the divisions are known, and the highest division of the previous DataFrame
    is below that of the lowest division of the next. If any input has an unknown
    division, Dask will produce an output without known partitioning. With all known
    partitions, Dask treats row-based concatenations as a metadata-only change and
    will not perform any shuffle. This requires that no overlap between the divisions
    exists. In addition, an extra `interleave_partitions` parameter will change the
    join type for row-based combinations to one without the input partitioning restriction
    and will result in a known partitioner.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 当您通过行（类似于SQL UNION）合并（通过`concat`）DataFrames时，性能取决于要合并的DataFrames的分区是否有序。我们称一系列DataFrames的分区为*有序*，如果所有分区都是已知的，并且前一个DataFrame的最高分区低于下一个DataFrame的最低分区。如果任何输入具有未知分区，则Dask将生成一个没有已知分区的输出。具有所有已知分区时，Dask将行基础的连接视为仅元数据更改，并且不会执行任何洗牌操作。这要求分区之间不存在重叠。此外，额外的`interleave_partitions`参数将行基础组合的连接类型更改为无输入分区限制，并将导致已知分区器。
- en: Dask’s column-based `concat` (similar to a SQL JOIN) also has restrictions around
    the divisions/partitions of the DataFrames it is combining. Dask’s version of
    concat supports only inner or full outer joins, not left or right. Column-based
    joins require that all inputs have known partitioners and also result in a DataFrame
    with known partitioning. Having a known partitioner can be useful for subsequent
    joins.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Dask的基于列的`concat`（类似于SQL JOIN）在组合的DataFrames的分区/分割方面也有限制。Dask的concat版本仅支持内部或完全外连接，不支持左连接或右连接。基于列的连接要求所有输入都有已知的分区器，并且结果是具有已知分区的DataFrame。具有已知分区器对于后续连接非常有用。
- en: Warning
  id: totrans-141
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Don’t use Dask’s `concat` when operating by row on a DataFrame with unknown
    divisions, as it will likely return incorrect results. Dask assumes indices are
    aligned no indices are present.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在对具有未知分区的 DataFrame 按行操作时，不要使用 Dask 的 `concat`，因为它可能会返回不正确的结果。Dask 假设索引是对齐的，如果没有索引，则不会存在。
- en: Multi-DataFrame internals
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多 DataFrame 内部
- en: Dask uses four techniques—hash, broadcast, partitioned, and `stack_partitions`—to
    combine DataFrames, and each results in very different performance. Dask chooses
    the technique based on the indexes, divisions, and requested join type (e.g.,
    outer/left/inner). The three column-based join techniques are hash joins, broadcast,
    and partitioned joins. When doing row-based combinations (e.g., `append`), Dask
    has a special technique called `stack_partitions` that is extra fast. It’s important
    that you understand the performance of each of these techniques and the conditions
    that will cause Dask to pick which approach.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 使用四种技术——哈希、广播、分区和 `stack_partitions`——来合并 DataFrame，每种技术的性能差异很大。Dask 根据索引、分区和请求的连接类型（例如，outer/left/inner）选择合适的技术。三种基于列的连接技术是哈希连接、广播连接和分区连接。在执行基于行的组合（例如
    `append`）时，Dask 使用一种特殊的技术称为 `stack_partitions`，速度非常快。重要的是，您了解每种技术的性能以及引起 Dask
    选择哪种方法的条件。
- en: '*Hash* joins are the default that Dask uses when no other join technique is
    suit­able. Hash joins shuffle the data for all the input DataFrames to partition
    on the target key. Hash joins use the hash values of keys, which results in a
    DataFrame that is not in any particular order. As such, the result of hash joins
    do not have any known divisions.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '*哈希* 连接是 Dask 在没有其他适合的连接技术时使用的默认技术。哈希连接会为所有输入 DataFrame 的数据洗牌，以在目标键上进行分区。哈希连接使用键的哈希值，导致生成的
    DataFrame 没有任何特定顺序。因此，哈希连接的结果没有已知的分区。'
- en: '*Broadcast* joins are ideal for joining large DataFrames with small DataFrames.
    In a broadcast join, Dask takes the smaller DataFrame and distributes it to all
    of the workers. This means that the smaller DataFrame must be able to fit in memory.
    To tell Dask that a DataFrame is a good candidate for broadcasting, you make sure
    it is all stored in one partition—for example, call `repartition(npartitions=1)`.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '*广播* 连接非常适合将大 DataFrame 与小 DataFrame 进行连接。在广播连接中，Dask 将较小的 DataFrame 分发给所有工作进程。这意味着较小的
    DataFrame 必须能够放入内存中。为了告诉 Dask 一个 DataFrame 适合广播，确保它全部存储在一个分区中，例如调用 `repartition(npartitions=1)`。'
- en: '*Partitioned* joins happen when combining DataFrames along an index where the
    partitions are known for all the DataFrames. Since the input partitions are known,
    Dask is able to align the partitions between the DataFrames, involving less data
    transfer as each output partition has a smaller than full set of inputs.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '*分区* 连接发生在沿着索引组合 DataFrame 时，其中所有 DataFrame 的分区都是已知的。由于输入分区已知，Dask 能够在 DataFrame
    之间对齐分区，涉及的数据传输更少，因为每个输出分区具有比完整输入集更小的集合。'
- en: Since partition and broadcast joins are faster, doing some work to help Dask
    can be worthwhile. For example, concatenating several DataFrames with known and
    aligned partitions, and one DataFrame which is unaligned, will result in an expensive
    hash join. Instead, try to either set the index and partition on the remaining
    DataFrame, or join the less expensive DataFrames first and then perform the expensive
    join after.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 由于分区和广播连接速度更快，帮助 Dask 完成一些工作是值得的。例如，连接几个已知且对齐分区的 DataFrame，以及一个未对齐的 DataFrame，将导致昂贵的哈希连接。相反，尝试在剩余的
    DataFrame 上设置索引并分区，或者先连接较便宜的 DataFrame，然后再执行昂贵的连接。
- en: Using `stack_partitions` is different from all of the other options since it
    doesn’t involve any movement of data. Instead, the resulting DataFrame partitions
    list is a union of the upstream partitions from the input DataFrames. Dask uses
    `stack​_partiti⁠ons` for most row-based combinations except when all of the input
    DataFrame divisions are known and they are not well ordered and you ask Dask to
    `interleave_partitions`. The `stack_partitions` function is able to provide only
    known partitioning in its output when the input divisions are known and well ordered.
    If all the divisions are known but not well ordered, and you set `interleave_partitions`,
    Dask will use a partitioned join instead. While this approach is compa⁠r­atively
    inexpensive, it is not free and can result in an excessively large number of partitions
    requiring you to repartition anyway.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `stack_partitions` 不同于所有其他选项，因为它不涉及任何数据的移动。相反，生成的 DataFrame 分区列表是输入 DataFrames
    的上游分区的联合。Dask 在大多数基于行的组合中使用 `stack​_partiti⁠ons`，除非所有输入 DataFrame 的分区都已知且未正确排序，并且您请求
    Dask `interleave_partitions`。当输入分区已知且正确排序时，`stack_partitions` 函数能够仅在其输出中提供已知分区。如果所有分区都已知但未正确排序，并且您设置了
    `interleave_partitions`，Dask 将使用分区连接。虽然这种方法比较便宜，但并非免费，可能导致分区数量过多，需要重新分区。
- en: Missing functionality
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 缺失功能
- en: Not all multi-DataFrame operations are implemented; `compare` is one such operation,
    which leads into the next section about the limitations of Dask DataFrames.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有的多 DataFrame 操作都已实现；`compare` 就是这样一个操作，这导致了关于 Dask DataFrames 限制的下一节。
- en: What Does Not Work
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么不起作用
- en: Dask’s DataFrame implements most, but not all, of the pandas DataFrame API.
    Some of the pandas API is not implemented in Dask because of the development time
    involved. Other parts are not used in order to avoid exposing an API that would
    be unexpectedly slow.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 的 DataFrame 实现了大部分，但并非全部 pandas DataFrame API。由于开发时间的关系，某些 pandas API 在
    Dask 中并未实现。其他部分则是为了避免暴露出意外缓慢的 API 而未使用。
- en: Sometimes the API is just missing small parts, as both pandas and Dask are under
    active development. An example is the `split` function. In local pandas, instead
    of doing `split().explode`, you could have called `split(expand=true)`. Some of
    these can be excellent places to get involved and [contribute to the Dask project](https://oreil.ly/OHPqQ)
    if you are interested.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 有时 API 只是缺少一些小部分，因为 pandas 和 Dask 都在积极开发中。例如 `split` 函数就是一个例子。在本地 pandas 中，你可以调用
    `split(expand=true)`，而不是进行 `split().explode`。其中一些可以是参与并[为 Dask 项目做贡献](https://oreil.ly/OHPqQ)的好地方，如果您感兴趣的话。
- en: Some libraries do not parallelize as well as others. In these cases, a common
    approach is to try to filter or aggregate the data down enough that it can be
    represented locally and then apply the local libraries to the data. For example,
    with graphing, it’s common to pre-aggregate the counts or take a random sample
    and graph the result.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 有些库的并行效果不如其他库好。在这些情况下，一种常见的方法是尝试对数据进行足够的筛选或聚合，以便可以在本地表示数据，然后将本地库应用于数据。例如，在绘图时，通常会预先聚合计数或进行随机抽样，然后绘制结果。
- en: While much of the pandas DataFrame API will work out of the box, before you
    swap in Dask DataFrame, it’s important to make sure you have good test coverage
    to catch the situations where it does not.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然大部分 pandas DataFrame API 都可以直接使用，但在切换到 Dask DataFrame 之前，重要的是确保您有良好的测试覆盖率，以捕捉它不适用的情况。
- en: What’s Slower
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么更慢
- en: Usually, using Dask DataFrames will improve performance, but not always. Generally,
    smaller datasets will perform better in local pandas. As discussed, anything involving
    shuffles is generally slower in a distributed system than in a local one. Iterative
    algorithms can also produce large graphs of operations, which are slow to evaluate
    in Dask compared to traditional greedy evaluation.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，使用 Dask DataFrames 可以提高性能，但并非总是如此。一般来说，较小的数据集在本地 pandas 中的表现更好。正如讨论的那样，任何涉及洗牌的操作在分布式系统中通常比在本地系统中慢。迭代算法还可以产生大量操作的图形，这些操作在
    Dask 中评估速度比传统的贪婪评估要慢。
- en: Some problems are generally unsuitable for data-parallel computing. For example,
    writing out to a data store with a single lock that has more parallel writers
    will increase the lock contention and may make it slower than if a single thread
    was doing the writing. In these situations, you can sometimes repartition your
    data or write individual partitions to avoid lock contention.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 一些问题通常不适合数据并行计算。例如，使用具有更多并行写入者的单个锁的数据存储写入时会增加锁争用，并可能比单线程写入更慢。在这些情况下，你可以有时重新分区数据或将单个分区写入以避免锁争用。
- en: Handling Recursive Algorithms
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理递归算法
- en: Dask’s lazy evaluation, powered by its lineage graph, is normally beneficial,
    allowing it to combine steps automatically. However, when the graph gets too large,
    Dask can struggle to manage it, which often shows up as a slow driver process
    or notebook, and sometimes an out-of-memory exception. Thankfully, you can work
    around this by writing out your DataFrame and reading it back in. Generally, Parquet
    is the best format for doing this as it is space-efficient and self-describing,
    so no schema inference is required.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 的惰性评估，由其谱系图提供支持，通常是有益的，使其能够自动合并步骤。然而，当图变得太大时，Dask 可能会在管理上遇到困难，这通常表现为驱动程序或笔记本变慢，有时会出现内存不足的异常。幸运的是，你可以通过将
    DataFrame 写出并重新读入来解决这个问题。一般来说，Parquet 是执行此操作的最佳格式，因为它占用空间小且自描述，因此不需要进行模式推断。
- en: What Other Functions Are Different
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他函数有何不同
- en: 'For performance reasons, various parts of Dask DataFrames behave a little differently
    from local DataFrames:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 由于性能原因，Dask DataFrames 的各个部分与本地 DataFrames 行为略有不同。
- en: '`reset_index`'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '`reset_index`'
- en: The index will start back over at zero on each partition.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 索引将在每个分区上重新从零开始。
- en: '`kurtosis`'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '`kurtosis`'
- en: Does not filter out not-a-number (NaN) values and uses SciPy defaults.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 不会过滤掉非数字（NaN）值并使用 SciPy 默认值。
- en: '`concat`'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '`concat`'
- en: Instead of coercing category types, each category type is expanded to the union
    of all of the categories it is concatenated with.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 不同于强制执行类别类型，每个类别类型都扩展为其与所有连接的类别的并集。
- en: '`sort_values`'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`sort_values`'
- en: Dask supports only single-column sorts.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 仅支持单列排序。
- en: Joins
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 连接
- en: When joining more than two DataFrames at the same time, the join type must be
    either outer or left.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 当同时连接两个以上的 DataFrames 时，连接类型必须是 outer 或 left。
- en: Tip
  id: totrans-174
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If you are interested in going deeper with Dask, several Dask-focused books
    are in active development. Much of the material in this chapter is based on [*Scaling
    Python with Dask*](https://oreil.ly/Fk0I6).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对深入了解 Dask 感兴趣，有几本专注于 Dask 的书籍正在积极开发中。本章中的大部分材料基于 [*Scaling Python with Dask*](https://oreil.ly/Fk0I6)。
- en: pandas-like DataFrames with Modin
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 类似 pandas 的 Modin DataFrames
- en: '[Modin](https://oreil.ly/KR1wT), like Dask DataFrames, is designed to largely
    be a plug-in replacement for pandas DataFrames. Modin DataFrames follow the same
    general performance as Dask DataFrames, with a few caveats. Modin offers less
    control over internals, which can limit performance for some applications. Since
    Modin and Dask DataFrames are sufficiently similar, we won’t cover it here except
    to say it’s another option if Dask doesn’t meet your needs.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '[Modin](https://oreil.ly/KR1wT)，就像 Dask DataFrames 一样，旨在大部分替代 pandas DataFrames。Modin
    DataFrames 的整体性能与 Dask DataFrames 相似，但有几个注意事项。Modin 对内部的控制较少，这可能限制某些应用的性能。由于 Modin
    和 Dask DataFrames 相似度足够高，我们在这里不会详细介绍，只是说如果 Dask 不能满足你的需求，Modin 是另一个选择。'
- en: Note
  id: totrans-178
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: '*Modin* is a new library designed to accelerate pandas by automatically distributing
    the computation across all of the system’s available CPU cores. Modin claims to
    be able to get nearly linear speedup to the number of CPU cores on your system
    for pandas DataFrames of any size.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '*Modin* 是一个旨在通过自动分布计算加速 pandas 的新库，可以利用系统上所有可用的 CPU 核心。Modin 声称可以几乎线性加速系统上
    pandas DataFrames 的计算，无论大小如何。'
- en: Since Modin on Ray is so similar to Dask DataFrames, we’ve decided to skip repeating
    the examples from Dask on Ray as they would not change substantially.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Modin 在 Ray 上与 Dask DataFrames 如此相似，我们决定跳过重复 Dask 在 Ray 上的示例，因为它们在本质上并无大变化。
- en: Warning
  id: totrans-181
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: When you look at Dask and Modin’s documentation side by side, you may get the
    impression that Dask is earlier in its development cycle. In our opinion, this
    is not the case; rather, the Dask documentation takes a more conservative approach
    to marking features as ready.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 当你并排查看 Dask 和 Modin 的文档时，你可能会觉得 Dask 在其开发周期中较早。在我们看来，事实并非如此；相反，Dask 文档采用更为保守的方法来标记功能是否就绪。
- en: Big Data with Spark
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Spark 进行大数据处理
- en: If you’re working with an existing big data infrastructure (such as Apache Hive,
    Iceberg, or HBase), Spark is an excellent choice. Spark has optimizations like
    filter push-down, which can dramatically improve performance. Spark has a more
    traditional, big data DataFrame interface.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在使用现有的大数据基础设施（如Apache Hive、Iceberg或HBase），Spark是一个极好的选择。Spark具有诸如过滤器推送等优化功能，可以显著提高性能。Spark具有更传统的大数据DataFrame接口。
- en: Spark’s strong suit is in the data ecosystem of which it is a part. As a Java-based
    tool, with a Python API, Spark plugs into much of the traditional big data ecosystem.
    Spark supports the widest array of formats and filesystems, making it an excellent
    choice for the initial stages of many pipelines.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的强项在于其所属的数据生态系统。作为一个基于Java的工具，带有Python API，Spark与传统的大数据生态系统高度集成。Spark支持最广泛的格式和文件系统，使其成为许多流水线初始阶段的优秀选择。
- en: 'While Spark continues to add more pandas-like functionality, its DataFrames
    started from more of a SQL-inspired design. You have several options to learn
    about Spark, including some O’Reilly books: [*Learning Spark*](https://oreil.ly/LearningSpark2)
    by Jules Damji, [*High Performance Spark*](https://oreil.ly/highperfSpark) by
    Holden and Rachel Warren, and [*Spark: The Definitive Guide*](https://oreil.ly/sparkTDG)
    by Bill Chambers and Matei Zaharia.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '虽然Spark继续添加更多类似于pandas的功能，但其DataFrame最初是基于SQL设计的。您有几种选项可以了解Spark，包括一些O''Reilly的书籍：[*Learning
    Spark*](https://oreil.ly/LearningSpark2) by Jules Damji，[*High Performance Spark*](https://oreil.ly/highperfSpark)
    by Holden and Rachel Warren，以及[*Spark: The Definitive Guide*](https://oreil.ly/sparkTDG)
    by Bill Chambers and Matei Zaharia。'
- en: Warning
  id: totrans-187
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Unlike Ray, Spark is generally lazy, meaning it does not evaluate data until
    forced. This can make debugging a challenge as errors may appear several lines
    removed from their root cause.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 与Ray不同，Spark通常是惰性的，这意味着它不会在被强制之前评估数据。这可能会使调试变得具有挑战性，因为错误可能会出现在其根本原因几行之外。
- en: Working with Local Tools
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用本地工具
- en: Some tools are not well suited to distributed operation. Thankfully, provided
    your dataset is filtered down small enough, you can convert it into a variety
    of local in-process formats. If the entire dataset can fit in memory, `to_pandas`
    and `to_arrow` are the simplest ways to convert a dataset to a local object. For
    larger objects, where each partition may fit in memory but the entire dataset
    may not, `iter_batches` will give you a generator/iterator to consume one partition
    at a time. The `iter_batches` function takes a `batch_format` parameter to switch
    between `pandas` or `pyarrow`. If possible, `pyarrow` is generally more efficient
    than `pandas`.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 有些工具不太适合分布式操作。幸运的是，只要您的数据集足够小，您可以将其转换为各种本地进程格式。如果整个数据集可以放入内存中，`to_pandas`和`to_arrow`是将数据集转换为本地对象的最简单方法。对于较大的对象，每个分区可能适合内存，但整个数据集可能不适合，`iter_batches`将为您提供一个生成器/迭代器，以逐个分区消耗数据。`iter_batches`函数接受`batch_format`参数，在`pandas`和`pyarrow`之间进行切换。如果可能，`pyarrow`通常比`pandas`更高效。
- en: Using Built-in Ray Dataset Operations
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用内置Ray Dataset操作
- en: In addition to allowing you to move data among various tools, Ray also has some
    built-in operations. Ray Datasets does not attempt to match any particular existing
    API, but rather expose basic building blocks you can use when the existing libraries
    do not meet your needs.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 除了允许您在各种工具之间传输数据外，Ray还具有一些内置操作。Ray Datasets并不试图匹配任何特定的现有API，而是暴露基本的构建块，当现有库不满足您的需求时可以使用这些构建块。
- en: Ray Datasets has support for basic data operations. Ray Datasets does not aim
    to expose a pandas-like API; rather, it focuses on providing basic primitives
    to build on top of. The Dataset API is functionally inspired, along with partition-oriented
    functions. Ray also recently added `groupBy`s and aggregates.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: Ray Datasets支持基本的数据操作。Ray Datasets并不旨在提供类似于pandas的API；相反，它专注于提供基本的原语来构建。Dataset
    API的功能受到启发，同时具有面向分区的函数。Ray还最近添加了`groupBy`和聚合功能。
- en: The core building block of most of the dataset operations is `map_batches`.
    By default, `map_batches` executes the function you provide across the blocks
    or batches that make up a dataset and uses the results to make a new dataset.
    The `map_batches` function is used to implement `filter`, `flat_map`, and `map`.
    You can see the flexibility of `map_batches` by looking at the word-count example
    rewritten to directly use `map_batches` as well as drop any word that shows up
    only once, as shown in [Example 9-15](#ray_wordcount_on_ds_filter_only_once_with_batches).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数数据集操作的核心构建块是 `map_batches`。默认情况下，`map_batches` 在构成数据集的块或批次上执行您提供的函数，并使用结果生成新数据集。`map_batches`
    函数用于实现 `filter`、`flat_map` 和 `map`。通过查看将单词计数示例重写为直接使用 `map_batches` 的示例，您可以看到
    `map_batches` 的灵活性，同时删除仅出现一次的单词，如[Example 9-15](#ray_wordcount_on_ds_filter_only_once_with_batches)所示。
- en: Example 9-15\. [Ray word count with `map_batches`](https://oreil.ly/HP05n)
  id: totrans-195
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 9-15\. 使用 `map_batches` 进行Ray单词计数的示例，详见[Ray word count with `map_batches`](https://oreil.ly/HP05n)
- en: '[PRE15]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The `map_batches` function takes parameters to customize its behavior. For stateful
    operations, you can change the `compute` strategy to `actors` from its default
    `tasks`. The previous example uses the default format, which is Ray’s internal
    format, but you can also convert the data to `pandas` or `pyarrow`. You can see
    this in [Example 9-16](#batch_op_on_pandas) in which Ray converts the data to
    pandas for us.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '`map_batches` 函数接受参数以定制其行为。对于有状态的操作，您可以将计算策略从默认的`tasks`更改为`actors`。前面的示例使用了默认格式，即Ray的内部格式，但您也可以将数据转换为`pandas`或`pyarrow`。您可以在[Example 9-16](#batch_op_on_pandas)中看到Ray将数据转换为pandas的示例。'
- en: Example 9-16\. [Using Ray `map_batches` with pandas to update a column](https://oreil.ly/HP05n)
  id: totrans-198
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 9-16\. 使用Ray的 `map_batches` 与pandas更新列的示例，详见[Ray `map_batches` with
    pandas to update a column](https://oreil.ly/HP05n)
- en: '[PRE16]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Tip
  id: totrans-200
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: The result you return must be a list, `pandas`, or `pyarrow`, and it does not
    need to match the same type that you take in.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 您返回的结果必须是列表、`pandas` 或 `pyarrow`，并且不需要与接收的相同类型匹配。
- en: Ray Datasets does not have a built-in way to specify additional libraries to
    be installed. You can use `map_batches` along with a task to accomplish this,
    as shown in [Example 9-17](#more_awesome_wordcount_with_batches), which installs
    extra libraries to parse the HTML.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: Ray数据集没有内置的方法来指定要安装的附加库。您可以使用 `map_batches` 和任务来完成此操作，如[Example 9-17](#more_awesome_wordcount_with_batches)所示，它安装额外的库以解析HTML。
- en: Example 9-17\. [Using Ray `map_batches` with extra libraries](https://oreil.ly/HP05n)
  id: totrans-203
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 9-17\. 使用Ray的 `map_batches` 与额外库的示例，详见[Using Ray `map_batches` with
    extra libraries](https://oreil.ly/HP05n)
- en: '[PRE17]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: For operations needing shuffles, Ray has `GroupedDataset`, which behaves a bit
    differently. Unlike the rest of the Datasets API, `groupby` is lazily evaluated
    in Ray. The `groupby` function takes a column name or function, where records
    with the same value will be aggregated together. Once you have the `GroupedDataset`,
    you can then pass in multiple aggregates to the `aggregate` function. Ray’s `AggregateFn`
    class is conceptually similar to Dask’s `Aggregation` class except it operates
    by row. Since it operates by row, you need to provide an `init` function for when
    a new key value is found. Instead of `chunk` for each new chunk, you provide `accumulate`
    to add each new element. You still provide a method of combining the aggregators,
    called `merge` instead of `agg`, and both have the same optional `finalize`. To
    understand the differences, we rewrote the Dask weighted average example to Ray
    in [Example 9-18](#ray_basic_agg).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 对于需要洗牌的操作，Ray拥有 `GroupedDataset`，其行为略有不同。与其余的Datasets API不同，Ray中的 `groupby`
    是惰性评估的。`groupby` 函数接受列名或函数，其中具有相同值的记录将被聚合在一起。一旦您有了 `GroupedDataset`，您就可以将多个聚合传递给
    `aggregate` 函数。Ray的 `AggregateFn` 类在概念上类似于Dask的 `Aggregation` 类，只是它是按行操作的。由于它是按行操作的，所以当发现新的键值时，您需要提供一个
    `init` 函数。对于每个新元素，您提供 `accumulate` 而不是每个新块提供 `chunk`。您仍然提供一种组合聚合器的方法，称为 `merge`
    而不是 `agg`，两者都有可选的 `finalize`。为了理解差异，我们将Dask加权平均示例改写为Ray，如[Example 9-18](#ray_basic_agg)所示。
- en: Example 9-18\. [Ray weighted average aggregation](https://oreil.ly/HP05n)
  id: totrans-206
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 9-18\. Ray加权平均聚合的示例，详见[Ray weighted average aggregation](https://oreil.ly/HP05n)
- en: '[PRE18]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note
  id: totrans-208
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Full dataset aggregation is implemented using `None` since all records then
    have the same key.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `None` 实现完整数据集聚合，因为所有记录都具有相同的键。
- en: Ray’s parallelism control does not have the same flexibility as indexes in Dask
    or partitioning in Spark. You can control the target number of partitions—but
    not the way the data is spread out.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: Ray的并行控制不像Dask的索引或Spark的分区那样灵活。您可以控制目标分区的数量，但无法控制数据的分布方式。
- en: Note
  id: totrans-211
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Ray does not currently take advantage of the concept of known partitioning to
    minimize shuffles.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: Ray 目前没有利用已知分区概念以最小化洗牌操作。
- en: Implementing Ray Datasets
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现 Ray 数据集
- en: Ray datasets are built using the tools you have been working with in the previous
    chapters. Ray splits each dataset into many smaller components. These smaller
    components are called both *blocks* and *partitions* inside the Ray code. Each
    partition contains an Arrow dataset representing a slice of the entire Ray dataset.
    Since Arrow does not support all of the types from Ray, if you have unsupported
    types, each partition also contains a list of the unsupported types.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: Ray 数据集是使用您在前几章中使用的工具构建的。Ray 将每个数据集分割成许多较小的组件。这些较小的组件在 Ray 代码内部被称为 *blocks*
    和 *partitions*。每个分区包含一个 Arrow 数据集，表示整个 Ray 数据集的一个切片。由于 Arrow 不支持 Ray 的所有类型，如果有不支持的类型，每个分区还包含一个不支持类型的列表。
- en: The data inside each dataset is stored in the standard Ray object store. Each
    partition is stored as a separate object, since Ray is not able to split up individual
    objects. This also means that you can use the underlying Ray objects as parameters
    to Ray remote functions and actors. The dataset contains references to these objects
    as well as schema information.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 每个数据集内部的数据存储在标准的 Ray 对象存储中。由于 Ray 不能分割单个对象，每个分区都存储为一个单独的对象。这也意味着您可以将底层的 Ray
    对象用作 Ray 远程函数和 actors 的参数。数据集包含对这些对象的引用以及模式信息。
- en: Tip
  id: totrans-216
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Since the dataset contains the schema information, loading a dataset blocks
    on the first partition so that the schema information can be determined. The remainder
    of the partitions are eagerly loaded, but in a nonblocking fashion like the rest
    of Ray’s operations.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据集包含模式信息，加载数据集会阻塞在第一个分区上，以便确定模式信息。其余分区会急切加载，但像 Ray 的其他操作一样不会阻塞。
- en: In keeping with the rest of Ray, datasets are immutable. When you want to do
    an operation on a dataset, you apply a transformation, like `filter`, `join`,
    or `map`, and Ray returns a new dataset with the result.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Ray 的其余部分保持一致，数据集是不可变的。当您想对数据集执行操作时，您会应用一个转换，比如 `filter`、`join` 或 `map`，Ray
    返回一个包含结果的新数据集。
- en: Ray datasets can use either tasks (aka remote functions) or actors for processing
    transformations. Some libraries built on top of Ray datasets, like Modin, depend
    on using actor processing so they can implement certain ML tasks involving state.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: Ray 数据集可以使用任务（也称为远程函数）或者 actors 来进行转换处理。像 Modin 这样构建在 Ray 数据集之上的库依赖于使用 actors
    进行处理，以便能够实现涉及状态的某些 ML 任务。
- en: Conclusion
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Ray’s transparent handling of moving data among tools makes it an excellent
    choice for building end-to-end ML pipelines when compared with traditional techniques
    where the communication barrier between tools is much higher. Two separate frameworks,
    Modin and Dask, both offer a pandas-like experience on top of Ray Datasets, making
    it easy to scale existing data science workflows. Spark on Ray (known as *RayDP*)
    provides an easy integration path for those working in organizations with existing
    big-data tools.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: Ray 在处理工具之间透明地处理数据移动方面表现出色，与传统技术相比，在工具之间的通信障碍要高得多。两个独立的框架，Modin 和 Dask，都在 Ray
    数据集之上提供了类似于 pandas 的体验，这使得扩展现有的数据科学工作流程变得简单。在 Ray 数据集上的 Spark（称为 *RayDP*）为那些在具有现有大数据工具的组织中工作的人提供了一条简便的集成路径。
- en: In this chapter, you learned to effectively process data with Ray to power your
    ML and other needs. In the next chapter, you will learn to use Ray to power ML.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您学会了如何使用 Ray 有效地处理数据，以支持您的机器学习和其他需求。在下一章中，您将学习如何使用 Ray 来支持机器学习。
- en: ^([1](ch09.html#idm45354767432992-marker)) This is like a Ford dealer recommending
    a Ford, so take this advice with a grain of salt.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch09.html#idm45354767432992-marker)) 这就像一家福特经销商建议购买福特车一样，所以接受这些建议时要持保留态度。
- en: ^([2](ch09.html#idm45354767410480-marker)) Operations in native code can avoid
    this problem in Dask by using multithreading, but the details are beyond the scope
    of this book.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch09.html#idm45354767410480-marker)) 在 Dask 中，通过使用多线程可以避免原生代码中出现这个问题，但是具体细节超出了本书的范围。
- en: ^([3](ch09.html#idm45354766947536-marker)) Alternate algorithms for exact quantiles
    depend on more shuffles to reduce the space overhead.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch09.html#idm45354766947536-marker)) 对精确分位数的备用算法依赖于更多的洗牌操作以减少空间开销。
- en: ^([4](ch09.html#idm45354766886064-marker)) Key-skew can make this impossible
    for a known partitioner.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch09.html#idm45354766886064-marker)) 键偏斜可以使得已知的分区器无法执行此操作。
