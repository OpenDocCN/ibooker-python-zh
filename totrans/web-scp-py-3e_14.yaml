- en: Chapter 12\. Reading and Writing Natural Languages
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12章 读写自然语言
- en: So far, the data you have worked with in this book has been in the form of numbers
    or countable values. In most cases, you’ve simply stored the data without conducting
    any analysis after the fact. This chapter attempts to tackle the tricky subject
    of the English language.^([1](ch12.html#id650))
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您在本书中处理的数据形式大多是数字或可计数值。在大多数情况下，您只是存储数据而没有进行后续分析。本章尝试解决英语这一棘手的主题。^([1](ch12.html#id650))
- en: How does Google know what you’re looking for when you type “cute kitten” into
    its image search? Because of the text that surrounds the cute kitten images. How
    does YouTube know to bring up a certain Monty Python sketch when you type “dead
    parrot” into its search bar? Because of the title and description text that accompanies
    each uploaded video.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 当您在其图像搜索中输入“可爱小猫”时，Google是如何知道您正在寻找什么？因为围绕可爱小猫图像的文本。当您在YouTube的搜索栏中输入“死鹦鹉”时，YouTube是如何知道要播放某个蒙提·派森的段子？因为每个上传视频的标题和描述文本。
- en: In fact, even typing in terms such as “deceased bird monty python” immediately
    brings up the same “Dead Parrot” sketch, even though the page itself contains
    no mention of the words “deceased” or “bird.” Google knows that a “hot dog” is
    a food and that a “boiling puppy” is an entirely different thing. How? It’s all
    statistics!
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，即使输入“已故鸟蒙提·派森”等术语，也会立即出现相同的“死鹦鹉”段子，尽管页面本身没有提到“已故”或“鸟”这两个词。Google知道“热狗”是一种食物，而“煮狗幼犬”则完全不同。为什么？这都是统计数据！
- en: Although you might not think that text analysis has anything to do with your
    project, understanding the concepts behind it can be extremely useful for all
    sorts of machine learning, as well as the more general ability to model real-world
    problems in probabilistic and algorithmic terms.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管您可能认为文本分析与您的项目无关，但理解其背后的概念对各种机器学习以及更一般地在概率和算法术语中建模现实世界问题的能力都非常有用。
- en: For instance, the Shazam music service can identify audio as containing a certain
    song recording, even if that audio contains ambient noise or distortion. Google
    is working on automatically captioning images based on nothing but the image itself.^([2](ch12.html#id651))
    By comparing known images of, say, hot dogs to other images of hot dogs, the search
    engine can gradually learn what a hot dog looks like and observe these patterns
    in additional images it is shown.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Shazam音乐服务可以识别包含某个歌曲录音的音频，即使该音频包含环境噪音或失真。Google正在基于图像本身自动为图像添加字幕。^([2](ch12.html#id651))
    通过将已知的热狗图像与其他热狗图像进行比较，搜索引擎可以逐渐学习热狗的外观并观察这些模式在其显示的其他图像中的表现。
- en: Summarizing Data
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据摘要
- en: In [Chapter 11](ch11.html#c-11), you looked at breaking up text content into
    n-grams, or sets of phrases that are *n* words in length. At a basic level, this
    can be used to determine which sets of words and phrases tend to be most commonly
    used in a section of text. In addition, it can be used to create natural-sounding
    data summaries by going back to the original text and extracting sentences around
    some of these most popular phrases.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第11章](ch11.html#c-11)中，您看到了如何将文本内容分解为n-gram，即长度为*n*的短语集。基本上，这可以用于确定哪些词组和短语在文本段落中最常用。此外，它还可用于通过回溯原始文本并提取围绕这些最流行短语之一的句子来创建自然语音数据摘要。
- en: 'One piece of sample text you’ll be using to do this is the inauguration speech
    of the ninth president of the United States, William Henry Harrison. Harrison’s
    presidency set two records in the history of the office: one for the longest inauguration
    speech, and another for the shortest time in office, 32 days.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 您将用美国第九任总统威廉·亨利·哈里森的就职演讲作为本章中许多代码示例的源头。
- en: You’ll use the full text of this [speech](http://pythonscraping.com/files/inaugurationSpeech.txt)
    as the source for many of the code samples in this chapter.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 您将使用此[演讲](http://pythonscraping.com/files/inaugurationSpeech.txt)的完整文本作为本章中许多代码示例的源头。
- en: 'A slightly modified set of functions from the cleaning code in [Chapter 11](ch11.html#c-11) can
    be used to transform this text into a list of sentences ready for splitting into
    n-grams:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第11章](ch11.html#c-11)的清理代码中稍作修改，可以将此文本转换为准备好分割成n-gram的句子列表：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then we fetch the text and call these functions in a particular order:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们获取文本并按特定顺序调用这些函数：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next we use the cleaned text to get a `Counter` object of all 2-grams and find
    the most popular ones:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们使用清理后的文本来获取所有 2-gram 的 `Counter` 对象，并找出最受欢迎的那些：
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This example illustrates the convenience and power of the Python standard library
    collections. No, it wouldn’t be particularly difficult to write a function that
    creates a dictionary counter, sorts it by values, and returns the most popular
    keys for those top values. However, knowing about the built-in collections and
    being able to pick the right one for the task at hand can save you many lines
    of code!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例说明了 Python 标准库 collections 的便利性和强大性。不，编写一个创建字典计数器、按值排序并返回这些顶级值的最受欢迎键的函数并不特别困难。但是，了解内置
    collections 并能够根据手头任务选择合适的工具可以节省很多代码行！
- en: 'The output produces, in part:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的部分包括：
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Of these 2-grams, “the constitution” seems like a reasonably popular subject
    in the speech, but “of the,” “in the,” and “to the” don’t seem especially noteworthy.
    How can you automatically and accurately get rid of unwanted words?
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些二元组中，“宪法”似乎是演讲中一个相当受欢迎的主题，但“of the”、“in the” 和 “to the” 看起来并不特别显着。您如何自动且准确地摆脱不想要的词？
- en: Fortunately, there are people out there who carefully study the differences
    between “interesting” words and “uninteresting” words, and their work can help
    us do just that. Mark Davies, a linguistics professor at Brigham Young University,
    maintains the [Corpus of Contemporary American English](http://corpus.byu.edu/coca/),
    a collection of over 450 million words from the last decade or so of popular American
    publications.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有些人认真研究“有趣”词和“无趣”词之间的差异，他们的工作可以帮助我们做到这一点。布里格姆·杨大学的语言学教授马克·戴维斯维护着[当代美国英语语料库](http://corpus.byu.edu/coca/)，这是一个收集了超过4.5亿字的流行美国出版物最近十年左右的集合。
- en: 'The list of 5,000 most frequently found words is available for free, and fortunately,
    this is more than enough to act as a basic filter to weed out the most common
    2-grams. Just the first one hundred words vastly improves the results, with the
    addition of `isCommon` and `filterCommon` functions:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 5000 最常见的单词列表可以免费获取，幸运的是，这已经足够作为基本过滤器，以清除最常见的二元组。仅前一百个单词就大大改善了结果，同时加入了 `isCommon` 和
    `filterCommon` 函数：
- en: '[PRE4]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This produces the following 2-grams that were found more than twice in the
    text body:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这生成了在文本正文中找到的以下出现超过两次的二元组：
- en: '[PRE5]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Appropriately enough, the first two items in the list are “United States” and
    “executive department,” which you would expect for a presidential inauguration
    speech.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 恰如其分地，列表中的前两项是“美利坚合众国”和“行政部门”，这在总统就职演讲中是可以预期的。
- en: It’s important to note that you are using a list of common words from relatively
    modern times to filter the results, which might not be appropriate given that
    the text was written in 1841\. However, because you’re using only the first one
    hundred or so words on the list—which you can assume are more stable over time
    than, say, the last one hundred words—and you appear to be getting satisfactory
    results, you can likely save yourself the effort of tracking down or creating
    a list of the most common words from 1841 (although such an effort might be interesting).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，您使用的是一份相对现代的常用词列表来过滤结果，这可能并不适合于这段文字是在 1841 年编写的事实。然而，因为您仅使用列表中的前一百个左右单词——可以假设这些单词比后一百个单词更为稳定，且您似乎得到了令人满意的结果，您很可能可以省去追溯或创建
    1841 年最常见单词列表的麻烦（尽管这样的努力可能会很有趣）。
- en: 'Now that some key topics have been extracted from the text, how does this help
    you write text summaries? One way is to search for the first sentence that contains
    each “popular” n-gram, the theory being that the first instance will yield a satisfactory
    overview of the body of the content. The first five most popular 2-grams yield
    these bullet points:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在从文本中提取了一些关键主题，这如何帮助您撰写文本摘要？一种方法是搜索每个“流行”的 n-gram 的第一句话，理论上第一次出现将提供对内容体的令人满意的概述。前五个最受欢迎的
    2-gram 提供了这些要点：
- en: “The Constitution of the United States is the instrument containing this grant
    of power to the several departments composing the Government.”
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “美利坚合众国宪法是包含这一授予各个组成政府部门的权力的工具。”
- en: “Such a one was afforded by the executive department constituted by the Constitution.”
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “联邦政府所构成的行政部门提供了这样一个人。”
- en: “The General Government has seized upon none of the reserved rights of the States.”
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “联邦政府没有侵占任何州保留的权利。”
- en: “Called from a retirement which I had supposed was to continue for the residue
    of my life to fill the chief executive office of this great and free nation, I
    appear before you, fellow-citizens, to take the oaths which the constitution prescribes
    as a necessary qualification for the performance of its duties; and in obedience
    to a custom coeval with our government and what I believe to be your expectations
    I proceed to present to you a summary of the principles which will govern me in
    the discharge of the duties which I shall be called upon to perform.”
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “Called from a retirement which I had supposed was to continue for the residue
    of my life to fill the chief executive office of this great and free nation, I
    appear before you, fellow-citizens, to take the oaths which the constitution prescribes
    as a necessary qualification for the performance of its duties; and in obedience
    to a custom coeval with our government and what I believe to be your expectations
    I proceed to present to you a summary of the principles which will govern me in
    the discharge of the duties which I shall be called upon to perform.”
- en: “The presses in the necessary employment of the Government should never be used
    to ‘clear the guilty or to varnish crime.’”
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “政府必须永远不要使用机器来‘为罪犯洗白或者掩盖罪行’。”
- en: Sure, it might not be published in CliffsNotes anytime soon, but considering
    that the original document was 217 sentences in length, and the fourth sentence
    (“Called from a retirement...”) condenses the main subject down fairly well, it’s
    not too bad for a first pass.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，它可能不会很快出现在 CliffsNotes 上，但考虑到原始文档长达217句，第四句（“Called from a retirement...”）相当简洁地概述了主题，对于第一遍来说还算不错。
- en: With longer blocks of text, or more varied text, it may be worth looking at
    3-grams or even 4-grams when retrieving the “most important” sentences of a passage.
    In this case, only one 3-gram is used multiple times and that is “exclusive metallic
    currency”—referring to the proposal of a gold standard for US currency, which
    was an important issue of the day. With longer passages, using 3-grams may be
    appropriate.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更长的文本块或更多样化的文本，检索一个段落中“最重要”的句子时可能值得查看3-gram甚至4-gram。在这种情况下，只有一个3-gram被多次使用，即“exclusive
    metallic currency”，指的是提出美国货币金本位制的重要问题。对于更长的段落，使用3-gram可能是合适的。
- en: Another approach is to look for sentences that contain the most popular n-grams.
    These will, of course, tend to be longer sentences, so if that becomes a problem,
    you can look for sentences with the highest percentage of words that are popular
    n-grams or create a scoring metric of your own, combining several techniques.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是寻找包含最流行 n-gram 的句子。这些句子通常会比较长，所以如果这成为问题，你可以寻找包含最高比例流行 n-gram 词的句子，或者自行创建一个结合多种技术的评分指标。
- en: Markov Models
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 马尔可夫模型
- en: You might have heard of Markov text generators. They’ve become popular for entertainment
    purposes, as in the [“That can be my next tweet!”](http://yes.thatcan.be/my/next/tweet/)
    app, as well as their use for generating real-sounding spam emails to fool detection
    systems.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能听说过马尔可夫文本生成器。它们已经因娱乐目的而流行起来，例如[“That can be my next tweet!”](http://yes.thatcan.be/my/next/tweet/)
    应用程序，以及它们用于生成听起来真实的垃圾邮件以欺骗检测系统。
- en: All of these text generators are based on the Markov model, which is often used
    to analyze large sets of random events, where one discrete event is followed by
    another discrete event with a certain probability.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些文本生成器都基于马尔可夫模型，该模型经常用于分析大量随机事件集，其中一个离散事件后跟随另一个离散事件具有一定的概率。
- en: For example, you might build a Markov model of a weather system, as illustrated
    in [Figure 12-1](#Markov_model).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您可以构建一个天气系统的马尔可夫模型，如[图12-1](#Markov_model)所示。
- en: '![Alt Text](assets/wsp3_1201.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![Alt Text](assets/wsp3_1201.png)'
- en: Figure 12-1\. Markov model describing a theoretical weather system
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-1。描述理论天气系统的马尔可夫模型
- en: In this model, each sunny day has a 70% chance of the following day also being
    sunny, with a 20% chance of the following day being cloudy with a mere 10% chance
    of rain. If the day is rainy, there is a 50% chance of rain the following day,
    a 25% chance of sun, and a 25% chance of clouds.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个模型中，每个晴天有70%的几率第二天仍然是晴天，有20%的几率第二天是多云，仅有10%的几率下雨。如果今天是雨天，那么第二天有50%的几率还是下雨，25%的几率是晴天，25%的几率是多云。
- en: 'You might note several properties in this Markov model:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会注意到这个马尔可夫模型中的几个属性：
- en: All percentages leading away from any one node must add up to exactly 100%.
    No matter how complicated the system, there must always be a 100% chance that
    it can lead somewhere else in the next step.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有从任何一个节点出发的百分比必须总和为100%。无论系统多么复杂，下一步总是必须有100%的机会能够导向其他地方。
- en: Although there are only three possibilities for the weather at any given time,
    you can use this model to generate an infinite list of weather states.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管每次只有三种天气可能性，您可以使用此模型生成无限的天气状态列表。
- en: 'Only the state of the current node you are on influences where you will go
    to next. If you’re on the Sunny node, it doesn’t matter if the preceding 100 days
    were sunny or rainy—the chances of sun the next day are exactly the same: 70%.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只有当前节点的状态会影响您下一步将去哪里。如果您在晴天节点上，不管前100天是晴天还是雨天，明天出太阳的几率都完全相同：70%。
- en: It might be more difficult to reach some nodes than others. The math behind
    this is reasonably complicated, but it should be fairly easy to see that Rainy
    (with less than “100%” worth of arrows pointing toward it) is a much less likely
    state to reach in this system, at any given point in time, than Sunny or Cloudy.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 达到某些节点可能比其他节点更困难。这背后的数学相当复杂，但可以很容易地看出，雨天（箭头指向少于“100%”）在任何给定时间点上都比晴天或多云状态更不可能到达该状态。
- en: Obviously, this is a simple system, and Markov models can grow arbitrarily large.
    Google’s page-rank algorithm is based partly on a Markov model, with websites
    represented as nodes, and inbound/outbound links represented as connections between
    nodes. The “likelihood” of landing on a particular node represents the relative
    popularity of the site. That is, if our weather system represented an extremely
    small internet, “rainy” would have a low page rank, while “cloudy” would have
    a high page rank.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这是一个简单的系统，马尔可夫模型可以任意扩展。谷歌的页面排名算法部分基于马尔可夫模型，其中网站表示为节点，入站/出站链接表示为节点之间的连接。着陆在特定节点上的“可能性”表示该站点的相对流行程度。也就是说，如果我们的天气系统代表一个极小的互联网，“雨天”将具有较低的页面排名，而“多云”将具有较高的页面排名。
- en: 'With all of this in mind, let’s bring it back down to a more concrete example:
    analyzing and generating text.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，让我们回到一个更具体的例子：分析和生成文本。
- en: 'Again using the inauguration speech of William Henry Harrison analyzed in the
    previous example, you can write the following code that generates arbitrarily
    long Markov chains (with the chain length set to 100) based on the structure of
    its text:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 再次使用前面分析过的威廉·亨利·哈里森的就职演讲，您可以编写以下代码，根据其文本结构生成任意长度的马尔可夫链（链长度设置为100）：
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output of this code changes every time it is run, but here’s an example
    of the uncannily nonsensical text it will generate:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码的输出每次运行时都会发生变化，但以下是它将生成的不可思议的无意义文本的示例：
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: So what’s going on in the code?
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 那么代码中到底发生了什么呢？
- en: 'The function `buildWordDict` takes in the string of text, which was retrieved
    from the internet. It then does some cleaning and formatting, removing quotes
    and putting spaces around other punctuation so it is effectively treated as a
    separate word. After this, it builds a two-dimensional dictionary—a dictionary
    of dictionaries—that has the following form:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`buildWordDict`接收从互联网检索到的文本字符串。然后进行一些清理和格式化，删除引号并在其他标点周围放置空格，以便它有效地被视为一个独立的单词。然后，它构建一个二维字典——字典的字典，其形式如下：
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In this example dictionary, `word_a` was found four times, two instances of
    which were followed by “word_b,” one instance followed by `word_c`, and one instance
    followed by `word_d.` Then “word_e” was followed seven times: five times by `word_b`
    and twice by `word_d`.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例字典中，`word_a`出现了四次，其中两次后跟“word_b”，一次后跟`word_c`，一次后跟`word_d.`然后，“word_e”跟随了七次：五次跟随`word_b`，两次跟随`word_d`。
- en: If we were to draw a node model of this result, the node representing `word_a`
    would have a 50% arrow pointing toward `word_b` (which followed it two out of
    four times), a 25% arrow pointing toward `word_c`, and a 25% arrow pointing toward
    `word_d.`
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们绘制此结果的节点模型，则表示`word_a`的节点将有一个50%的箭头指向`word_b`（它四次中有两次跟随），一个25%的箭头指向`word_c`，和一个25%的箭头指向`word_d.`
- en: 'After this dictionary is built up, it can be used as a lookup table to see
    where to go next, no matter which word in the text you happen to be on.^([3](ch12.html#id656))
    Using the sample dictionary of dictionaries, you might currently be on `word_e`,
    which means that you’ll pass the dictionary `{word_b : 5, word_d: 2}` to the `retrieveRandomWord`
    function. This function in turn retrieves a random word from the dictionary, weighted
    by the number of times it occurs.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '在构建了这个字典之后，它可以作为查找表用于查看下一步该去哪里，无论你当前在文本中的哪个词上。^([3](ch12.html#id656)) 使用字典的字典示例，你目前可能在`word_e`上，这意味着你会将字典`{word_b
    : 5, word_d: 2}`传递给`retrieveRandomWord`函数。该函数反过来按出现次数加权从字典中检索一个随机词。'
- en: By starting with a random starting word (in this case, the ubiquitous “I”),
    you can traverse through the Markov chain easily, generating as many words as
    you like.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 通过从一个随机的起始词（在这种情况下，无处不在的“I”）开始，你可以轻松地遍历马尔可夫链，生成任意数量的单词。
- en: These Markov chains tend to improve in their “realism” as more text is collected,
    especially from sources with similar writing styles. Although this example used
    2-grams to create the chain (where the previous word predicts the next word),
    3-grams or higher-order n-grams can be used, where two or more words predict the
    next word.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 随着收集更多类似写作风格的源文本，这些马尔可夫链的“真实性”会得到改善。尽管此示例使用了2-gram来创建链条（其中前一个词预测下一个词），但也可以使用3-gram或更高阶的n-gram，其中两个或更多词预测下一个词。
- en: Although entertaining, and a great use for megabytes of text that you might
    have accumulated during web scraping, applications like these can make it difficult
    to see the practical side of Markov chains. As mentioned earlier in this section,
    Markov chains model how websites link from one page to the next. Large collections
    of these links as pointers can form weblike graphs that are useful to store, track,
    and analyze. In this way, Markov chains form the foundation for both how to think
    about web crawling and how your web crawlers can think.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些应用程序很有趣，并且是在网络爬取期间积累的大量文本的极好用途，但这些应用程序可能会使人们难以看到马尔可夫链的实际应用。如本节前面提到的，马尔可夫链模拟了网页如何从一个页面链接到下一个页面。这些链接的大量集合可以形成有用的类似网络的图形，用于存储、跟踪和分析。这种方式，马尔可夫链为如何思考网络爬行和你的网络爬虫如何思考奠定了基础。
- en: 'Six Degrees of Wikipedia: Conclusion'
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 维基百科的六度分隔：结论
- en: In [Chapter 6](ch06.html#c-6), you created a scraper that collects links from
    one Wikipedia article to the next, starting with the article on Kevin Bacon, and
    in [Chapter 9](ch09.html#c-9), stored those links in a database. Why am I bringing
    it up again? Because it turns out the problem of choosing a path of links that
    starts on one page and ends up on the target page (i.e., finding a string of pages
    between [*https://en.wikipedia.org/wiki/Kevin_Bacon*](https://en.wikipedia.org/wiki/Kevin_Bacon)
    and *[*https://en.wikipedia.org/wiki/Eric_Idle*](https://en.wikipedia.org/wiki/Eric_Idle)*)
    is the same as finding a Markov chain where both the first word and last word
    are defined.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第六章](ch06.html#c-6)中，你创建了一个从一个维基百科文章到下一个文章的爬虫程序，从凯文·贝肯的文章开始，并在[第九章](ch09.html#c-9)中将这些链接存储到数据库中。为什么我再提一遍呢？因为事实证明，在选择一条从一个页面开始并在目标页面结束的链接路径的问题（即找到从[*https://en.wikipedia.org/wiki/Kevin_Bacon*](https://en.wikipedia.org/wiki/Kevin_Bacon)到[*https://en.wikipedia.org/wiki/Eric_Idle*](https://en.wikipedia.org/wiki/Eric_Idle)的一系列页面）与找到一个首尾都有定义的马尔可夫链是相同的。
- en: These sorts of problems are *directed graph* problems, where A → B does not
    necessarily mean that B → A. The word “football” might often be followed by the
    word “player,” but you’ll find that the word “player” is much less often followed
    by the word “football.” Although Kevin Bacon’s Wikipedia article links to the
    article on his home city, Philadelphia, the article on Philadelphia does not reciprocate
    by linking back to him.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这类问题属于*有向图*问题，其中 A → B 并不一定意味着 B → A。单词“足球”经常会后接“球员”，但你会发现“球员”后接“足球”的次数要少得多。尽管凯文·贝肯的维基百科文章链接到他的家乡费城的文章，但费城的文章并没有回链到他。
- en: In contrast, the original Six Degrees of Kevin Bacon game is an *undirected
    graph* problem. If Kevin Bacon starred in *Flatliners* with Julia Roberts, then
    Julia Roberts necessarily starred in *Flatliners* with Kevin Bacon, so the relationship
    goes both ways (it has no “direction”). Undirected graph problems tend to be less
    common in computer science than directed graph problems, and both are computationally
    difficult to solve.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，原始的“凯文·贝肯的六度分隔游戏”是一个*无向图*问题。如果凯文·贝肯和茱莉亚·罗伯茨在《心灵裂缝》中演出，那么茱莉亚·罗伯茨必然也和凯文·贝肯在《心灵裂缝》中演出，因此关系是双向的（没有“方向”）。与有向图问题相比，无向图问题在计算机科学中较少见，并且两者在计算上都很难解决。
- en: Although much work has been done on these sorts of problems and multitudes of
    variations on them, one of the best and most common ways to find shortest paths
    in a directed graph—and thus find paths between the Wikipedia article on Kevin
    Bacon and all other Wikipedia articles—is through a breadth-first search.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然已经对这类问题进行了大量工作，并且有许多变体，但在有向图中寻找最短路径——从维基百科文章“凯文·贝肯”到所有其他维基百科文章的路径——最好且最常见的方法之一是通过广度优先搜索。
- en: A *breadth-first search* is performed by first searching all links that link
    directly to the starting page. If those links do not contain the target page (the
    page you are searching for), then a second level of links—pages that are linked
    by a page that is linked by the starting page—is searched. This process continues
    until either the depth limit (6 in this case) is reached or the target page is
    found.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 首先搜索直接链接到起始页面的所有链接执行*广度优先搜索*。如果这些链接不包含目标页面（即你正在搜索的页面），则搜索第二层链接——由起始页面链接的页面链接。该过程持续进行，直到达到深度限制（在本例中为6）或找到目标页面为止。
- en: 'A complete solution to the breadth-first search, using a table of links as
    described in [Chapter 9](ch09.html#c-9), is as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 使用如下所述的链接表的广度优先搜索的完整解决方案如下：
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`getUrl` is a helper function that retrieves URLs from the database given a
    page ID. Similarly, `getLinks` takes a `fromPageId` representing the integer ID
    for the current page and fetches a list of all integer IDs for pages it links
    to.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`getUrl`是一个辅助函数，它根据页面ID从数据库中检索URL。类似地，`getLinks`接受一个`fromPageId`，代表当前页面的整数ID，并获取它链接到的所有页面的整数ID列表。'
- en: 'The main function, `searchBreadth`, works recursively to construct a list of
    all possible paths from the search page and stops when it finds a path that has
    reached the target page:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 主函数`searchBreadth`递归地工作以构建从搜索页面到目标页面的所有可能路径列表，并在找到达到目标页面的路径时停止：
- en: It starts with a single path, `[1]`, representing a path in which the user stays
    on the target page with the ID 1 (Kevin Bacon) and follows no links.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它从一个单一路径`[1]`开始，表示用户在目标页面ID为1（凯文·贝肯）的页面上停留并且不跟随任何链接。
- en: For each path in the list of paths (in the first pass, there is only one path,
    so this step is brief), it gets all of the links that link out from the page represented
    by the last page in the path.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于路径列表中的每条路径（在第一次通过中，只有一条路径，因此此步骤很简单），获取表示路径中最后一页的页面外部链接的所有链接。
- en: For each of these outbound links, it checks whether they match the `targetPageId`.
    If there’s a match, that path is returned.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个出站链接，它检查它们是否与`targetPageId`匹配。如果匹配，则返回该路径。
- en: If there’s no match, a new path is added to a new list of (now longer) paths,
    consisting of the old path plus the new outbound page link.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果没有匹配，将一个新路径添加到新的（现在更长的）路径列表中，包含旧路径加上新的出站页面链接。
- en: If the `targetPageId` is not found at this level at all, a recursion occurs
    and `searchBreadth` is called with the same `targetPageId` and a new, longer,
    list of paths.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果在这个级别根本找不到`targetPageId`，则发生递归，并且使用相同的`targetPageId`和新的更长路径列表调用`searchBreadth`。
- en: After the list of page IDs containing a path between the two pages is found,
    each ID is resolved to its actual URL and printed.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 找到包含两个页面之间路径的页面ID列表后，将每个ID解析为其实际URL并打印出来。
- en: 'The output for searching for a link between the page on Kevin Bacon (page ID
    1 in this database) and the page on Eric Idle (page ID 28624 in this database)
    is:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在此数据库中，搜索凯文·贝肯页面（页面ID为1）和搜索埃里克·艾德尔页面（数据库中页面ID为28624）之间链接的输出是：
- en: '[PRE10]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This translates into the relationship of links: Kevin Bacon → Primetime Emmy
    Award → Gary Gilmore → Eric Idle.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这转化为链接的关系：凯文·贝肯 → 黄金时段艾美奖 → 加里·吉尔莫 → 埃里克·艾德尔。
- en: In addition to solving Six Degrees problems and modeling which words tend to
    follow which other words in sentences, directed and undirected graphs can be used
    to model a variety of situations encountered in web scraping. Which websites link
    to which other websites? Which research papers cite which other research papers?
    Which products tend to be shown with which other products on a retail site? What
    is the strength of this link? Is the link reciprocal?
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 除了解决六度问题和建模外，还可以使用有向和无向图来模拟在网页抓取中遇到的各种情况。哪些网站链接到其他网站？哪些研究论文引用了其他研究论文？在零售网站上，哪些产品倾向于与其他产品一起显示？这种链接的强度是多少？这个链接是双向的吗？
- en: Recognizing these fundamental types of relationships can be extremely helpful
    for making models, visualizations, and predictions based on scraped data.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 识别这些基本关系类型对于基于抓取数据生成模型、可视化和预测非常有帮助。
- en: Natural Language Toolkit
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言工具包
- en: So far, this chapter has focused primarily on the statistical analysis of words
    in bodies of text. Which words are most popular? Which words are unusual? Which
    words are likely to come after which other words? How are they grouped together?
    What you are missing is understanding, to the extent that you can, what the words
    represent.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，本章主要集中在对文本体中单词的统计分析上。哪些词最受欢迎？哪些词不寻常？哪些词可能会在哪些其他词之后出现？它们是如何被组合在一起的？您所缺少的是理解，以您能力所及的程度，这些词代表什么。
- en: The *Natural Language Toolkit* (NLTK) is a suite of Python libraries designed
    to identify and tag parts of speech found in natural English text. Its development
    began in 2000, and over the past 20-plus years, dozens of developers around the
    world have contributed to the project. Although the functionality it provides
    is tremendous (entire books are devoted to NLTK), this section focuses on just
    a few of its uses.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*自然语言工具包*（[NLTK](http://www.nltk.org/install.html)）是一套设计用来识别和标记自然英文文本中词性的Python库。其开发始于2000年，过去的20多年里，全球数十名开发者为该项目做出了贡献。尽管它提供的功能非常强大（整本书都可以专门讨论NLTK），本节仅集中介绍其少数用途。'
- en: Installation and Setup
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装和设置
- en: The `nltk` module can be installed in the same way as other Python modules,
    either by downloading the package through the NLTK website directly or by using
    any number of third-party installers with the keyword “nltk.” For complete installation
    instructions and help with troubleshooting, see the [NLTK website](http://www.nltk.org/install.html).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`nltk`模块可以像其他Python模块一样安装，可以直接通过NLTK网站下载包，也可以使用任意数量的第三方安装程序与关键词“nltk”一起安装。有关完整的安装说明和故障排除帮助，请参阅[NLTK网站](http://www.nltk.org/install.html)。'
- en: 'After installing the module, you can browse the extensive collection of text
    corpora available for download and use:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 安装了模块后，您可以浏览广泛的文本语料库，这些语料库可以下载和使用：
- en: '[PRE11]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This opens the NLTK Downloader. You can navigate it in the terminal using the
    commands provided its menu:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打开NLTK下载器。您可以在终端使用其菜单提供的命令进行导航：
- en: '[PRE12]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The last page of the corpora list contains its collections:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 语料库列表的最后一页包含其集合：
- en: '[PRE13]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'For the exercises here, we will be using the book collection. You can download
    it through the downloader interface, or in Python:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里的练习中，我们将使用书籍集合。您可以通过下载器界面或在Python中下载它：
- en: '[PRE14]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Statistical Analysis with NLTK
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用NLTK进行统计分析
- en: NLTK is great for generating statistical information about word counts, word
    frequency, and word diversity in sections of text. If all you need is a relatively
    straightforward calculation (e.g., the number of unique words used in a section
    of text), importing `nltk` might be overkill—it’s a large module. However, if
    you need to do relatively extensive analysis of a text, you have functions at
    your fingertips that will give you just about any metric you want.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK非常适合生成关于文本段落中单词计数、词频和词多样性的统计信息。如果您只需要相对简单的计算（例如，在文本段落中使用的独特单词的数量），那么导入`nltk`可能有些大材小用——它是一个庞大的模块。然而，如果您需要对文本进行相对广泛的分析，您可以轻松获得几乎任何想要的度量函数。
- en: 'Analysis with NLTK always starts with the `Text` object. `Text` objects can
    be created from simple Python strings in the following way:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 使用NLTK进行分析始终从`Text`对象开始。可以通过以下方式从简单的Python字符串创建`Text`对象：
- en: '[PRE15]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The input for the `word_tokenize` function can be any Python text string. Any
    text can be passed in, but the NLTK corpora are handy for playing around with
    the features and for research. You can use the NLTK collection downloaded in the
    previous section by importing everything from the book module:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`word_tokenize`的输入可以是任何Python文本字符串。任何文本都可以传递进去，但是NLTK语料库非常适合用来玩耍和研究功能。你可以通过从book模块导入所有内容来使用前面部分下载的NLTK集合：
- en: '[PRE16]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This loads the nine books:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这加载了九本书：
- en: '[PRE17]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: You will be working with `text6`, “Monty Python and the Holy Grail” (the screenplay
    for the 1975 movie), in all of the following examples.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在所有接下来的例子中使用`text6`，“Monty Python and the Holy Grail”（1975年电影的剧本）。
- en: 'Text objects can be manipulated much like normal Python arrays, as if they
    were an array containing words of the text. Using this property, you can count
    the number of unique words in a text and compare it against the total number of
    words (remember that a Python `set` holds only unique values):'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 文本对象可以像普通的Python数组一样进行操作，就像它们是文本中包含的单词的数组一样。利用这一特性，你可以计算文本中唯一单词的数量，并将其与总单词数进行比较（记住Python的`set`只包含唯一值）：
- en: '[PRE18]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The preceding shows that each word in the script was used about eight times
    on average. You can also put the text into a frequency distribution object to
    determine some of the most common words and the frequencies for various words:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 上面显示了脚本中每个单词平均使用约八次的情况。你还可以将文本放入频率分布对象中，以确定一些最常见的单词和各种单词的频率：
- en: '[PRE19]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Because this is a screenplay, some artifacts of how it is written can pop up.
    For instance, “ARTHUR” in all caps crops up frequently because it appears before
    each of King Arthur’s lines in the script. In addition, a colon (:) appears before
    every single line, acting as a separator between the name of the character and
    the character’s line. Using this fact, we can see that there are 1,197 lines in
    the movie!
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这是一个剧本，所以写作方式可能会显现一些特定的艺术形式。例如，大写的“ARTHUR”经常出现，因为它出现在剧本中亚瑟王每一行的前面。此外，冒号（:）在每一行之前出现，作为角色名和角色台词之间的分隔符。利用这一事实，我们可以看到电影中有1,197行！
- en: 'What we have called 2-grams in previous chapters, NLTK refers to as *bigrams*
    (from time to time, you might also hear 3-grams referred to as *trigrams*, but
    I prefer 2-gram and 3-gram rather than bigram or trigram). You can create, search,
    and list 2-grams extremely easily:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中我们称之为2-grams，在NLTK中称为*bigrams*（有时你也可能听到3-grams被称为*trigrams*，但我更喜欢2-gram和3-gram而不是bigram或trigram）。你可以非常轻松地创建、搜索和列出2-grams：
- en: '[PRE20]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'To search for the 2-grams “Sir Robin,” you need to break it into the tuple
    (“Sir”, “Robin”), to match the way the 2-grams are represented in the frequency
    distribution. There is also a `trigrams` module that works in the same way. For
    the general case, you can also import the `ngrams` module:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 要搜索2元组“Sir Robin”，你需要将其分解为元组（“Sir”，“Robin”），以匹配频率分布中表示2元组的方式。还有一个`trigrams`模块以相同方式工作。对于一般情况，你还可以导入`ngrams`模块：
- en: '[PRE21]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Here, the `ngrams` function is called to break a text object into n-grams of
    any size, governed by the second parameter. In this case, you’re breaking the
    text into 4-grams. Then, you can demonstrate that the phrase “father smelt of
    elderberries” occurs in the screenplay exactly once.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，调用`ngrams`函数将文本对象分解为任意大小的n-grams，由第二个参数控制。在这种情况下，你将文本分解为4-grams。然后，你可以展示短语“father
    smelt of elderberries”在剧本中正好出现一次。
- en: 'Frequency distributions, text objects, and n-grams also can be iterated through
    and operated on in a loop. The following prints out all 4-grams that begin with
    the word “coconut,” for instance:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 频率分布、文本对象和n-grams也可以在循环中进行迭代和操作。例如，以下代码打印出所有以单词“coconut”开头的4-grams：
- en: '[PRE22]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The NLTK library has a vast array of tools and objects designed to organize,
    count, sort, and measure large swaths of text. Although we’ve barely scratched
    the surface of their uses, most of these tools are well designed and operate rather
    intuitively for someone familiar with Python.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK库拥有大量工具和对象，旨在组织、计数、排序和测量大段文本。尽管我们只是初步了解了它们的用途，但这些工具大多设计良好，对熟悉Python的人操作起来相当直观。
- en: Lexicographical Analysis with NLTK
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用NLTK进行词汇分析
- en: So far, you’ve compared and categorized all the words you’ve encountered based
    only on the value they represent by themselves. There is no differentiation between
    homonyms or the context in which the words are used.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你只是根据它们自身的价值比较和分类了遇到的所有单词。没有区分同音异义词或单词使用的上下文。
- en: Although some people might be tempted to dismiss homonyms as rarely problematic,
    you might be surprised at how frequently they crop up. Most native English speakers
    probably don’t often register that a word is a homonym, much less consider that
    it might be confused for another word in a different context.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有些人可能会觉得同音异义词很少会成问题，你也许会惊讶地发现它们出现的频率有多高。大多数以英语为母语的人可能并不经常意识到一个词是同音异义词，更不用说考虑它可能在不同语境中被误认为另一个词了。
- en: “He was objective in achieving his objective of writing an objective philosophy,
    primarily using verbs in the objective case” is easy for humans to parse but might
    make a web scraper think the same word is being used four times and cause it to
    simply discard all the information about the meaning behind each word.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: “他在实现他写一部客观哲学的目标时非常客观，主要使用客观语态的动词”对人类来说很容易解析，但可能会让网页抓取器以为同一个词被使用了四次，导致它简单地丢弃了关于每个词背后意义的所有信息。
- en: In addition to sussing out parts of speech, being able to distinguish between
    a word used in one way versus another might be useful. For example, you might
    want to look for company names made up of common English words, or analyze someone’s
    opinions about a company. “ACME Products is good” and “ACME Products is not bad”
    can have the same basic meaning, even if one sentence uses “good” and the other
    uses “bad.”
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 除了查找词性，能够区分一个词在不同用法下的差异也许会很有用。例如，你可能想要查找由常见英语词组成的公司名称，或分析某人对公司的看法。“ACME产品很好”和“ACME产品不坏”可能有着相同的基本意思，即使一句话使用了“好”而另一句话使用了“坏”。
- en: 'In addition to measuring language, NLTK can assist in finding meaning in the
    words based on context and its own sizable dictionaries. At a basic level, NLTK
    can identify parts of speech:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 除了测量语言之外，NLTK还可以根据上下文和自身庞大的词典帮助找到词语的含义。在基本水平上，NLTK可以识别词性：
- en: '[PRE23]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Each word is separated into a *tuple* containing the word and a tag identifying
    the part of speech (see the preceding sidebar for more information about these
    tags). Although this might seem like a straightforward lookup, the complexity
    needed to perform the task correctly becomes apparent with the following example:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 每个词都被分为一个包含该词和标识其词性的标签的*元组*（有关这些标签的更多信息，请参见前面的侧边栏）。虽然这看起来可能是一个简单的查找，但正确执行这项任务所需的复杂性在以下示例中变得显而易见：
- en: '[PRE24]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Notice that the word “dust” is used twice in the sentence: once as a noun,
    and again as a verb. NLTK identifies both usages correctly, based on their context
    in the sentence. NLTK identifies parts of speech by using a context-free grammar
    defined by the English language. *Context-free grammars* are sets of rules that
    define which things are allowed to follow other things in ordered lists. In this
    case, they define which parts of speech are allowed to follow other parts of speech.
    Whenever an ambiguous word such as “dust” is encountered, the rules of the context-free
    grammar are consulted, and an appropriate part of speech that follows the rules
    is selected.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，“dust”一词在句子中使用了两次：一次作为名词，另一次作为动词。NLTK根据它们在句子中的上下文正确地识别了两种用法。NLTK通过使用由英语语言定义的上下文无关语法来识别词性。*上下文无关语法*是定义哪些东西允许跟随其他东西的规则集合。在这种情况下，它们定义了哪些词性可以跟随其他词性。每当遇到一个模棱两可的词如“dust”时，上下文无关语法的规则被参考，并选择一个符合规则的适当词性。
- en: What’s the point of knowing whether a word is a verb or a noun in a given context?
    It might be neat in a computer science research lab, but how does it help with
    web scraping?
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在特定语境中知道一个词是动词还是名词有什么意义呢？在计算机科学研究实验室里或许很有意思，但它如何帮助网页抓取呢？
- en: 'A common problem in web scraping deals with search. You might be scraping text
    off a site and want to search it for instances of the word “google,” but only
    when it’s being used as a verb, not a proper noun. Or you might be looking only
    for instances of the company Google and don’t want to rely on people’s correct
    use of capitalization in order to find those instances. Here, the `pos_tag` function
    can be extremely useful:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 网页抓取中的一个常见问题涉及搜索。你可能正在从网站上抓取文本，并希望搜索其中“google”一词的实例，但只有在它被用作动词时才这样做，而不是作为专有名词。或者你可能只想寻找公司Google的实例，并且不希望依赖人们对大小写的正确使用来找到这些实例。在这里，`pos_tag`函数可以极为有用：
- en: '[PRE25]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This prints only sentences that contain the word “google” (or “Google”) as some
    sort of a noun, not a verb. Of course, you could be more specific and demand that
    only instances of Google tagged with “NNP” (a proper noun) are printed, but even
    NLTK makes mistakes at times, and it can be good to leave yourself a little wiggle
    room, depending on the application.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这只打印包含“google”（或“Google”）一词的句子（作为某种名词，而不是动词）。当然，您可以更具体地要求只打印带有“NNP”（专有名词）标记的Google实例，但即使是NLTK有时也会出错，因此根据应用程序的不同，留给自己一些灵活性也是有好处的。
- en: Much of the ambiguity of natural language can be resolved using NLTK’s `pos_tag`
    function. By searching text for instances of your target word or phrase *plus*
    its tag, you can greatly increase the accuracy and effectiveness of your scraper’s
    searches.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分自然语言的歧义可以通过 NLTK 的`pos_tag`函数来解决。通过搜索文本中目标单词或短语 *以及* 其标记，您可以极大地提高抓取器搜索的准确性和效率。
- en: Additional Resources
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他资源
- en: Processing, analyzing, and understanding natural language by machine is one
    of the most difficult tasks in computer science, and countless volumes and research
    papers have been written on the subject. I hope that the coverage here will inspire
    you to think beyond conventional web scraping, or at least give you some initial
    direction about where to begin when undertaking a project that requires natural
    language analysis.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 通过机器处理、分析和理解自然语言是计算机科学中最困难的任务之一，关于此主题已经有无数的书籍和研究论文被撰写。我希望这里的涵盖范围能激发您思考超越传统的网络抓取，或者至少给您在开始进行需要自然语言分析的项目时提供一些初始方向。
- en: Many excellent resources are available on introductory language processing and
    Python’s Natural Language Toolkit. In particular, Steven Bird, Ewan Klein, and
    Edward Loper’s book [*Natural Language Processing with Python*](http://oreil.ly/1HYt3vV)
    (O’Reilly) presents both a comprehensive and introductory approach to the topic.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 对于初学者语言处理和Python的自然语言工具包有许多优秀的资源。特别是Steven Bird、Ewan Klein和Edward Loper的书籍[*Natural
    Language Processing with Python*](http://oreil.ly/1HYt3vV)（O'Reilly）对该主题提供了全面和初步的介绍。
- en: In addition, James Pustejovsky and Amber Stubbs’ [*Natural Language Annotations
    for Machine Learning*](http://oreil.ly/S3BudT) (O’Reilly) provides a slightly
    more advanced theoretical guide. You’ll need knowledge of Python to implement
    the lessons; the topics covered work perfectly with Python’s Natural Language
    Toolkit.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，James Pustejovsky和Amber Stubbs的书籍[*Natural Language Annotations for Machine
    Learning*](http://oreil.ly/S3BudT)（O'Reilly）提供了一个略微更高级的理论指南。您需要掌握Python的知识才能实施这些教训；所涵盖的主题与Python的自然语言工具包完全契合。
- en: ^([1](ch12.html#id650-marker)) Although many of the techniques described in
    this chapter can be applied to all or most languages, it’s OK for now to focus
    on natural language processing in English only. Tools such as Python’s Natural
    Language Toolkit, for example, focus on English. Some 53% of the internet is still
    in English (with Spanish following at a mere 5.4%, according to [W3Techs](http://w3techs.com/technologies/overview/content_language/all)).
    But who knows? The hold English has on the majority of the internet will almost
    certainly change in the future, and further updates may be necessary in the next
    few years.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch12.html#id650-marker)) 尽管本章描述的许多技术可以应用于所有或大多数语言，但现在只专注于英语自然语言处理也是可以的。例如，工具如Python的自然语言工具包专注于英语。根据[W3Techs](http://w3techs.com/technologies/overview/content_language/all)，仍有53%的互联网内容是用英语编写的（其次是西班牙语，仅占5.4%）。但谁知道呢？英语在互联网上的主导地位几乎肯定会在未来发生变化，因此在未来几年可能需要进一步更新。
- en: '^([2](ch12.html#id651-marker)) Oriol Vinyals et al, [“A Picture Is Worth a
    Thousand (Coherent) Words: Building a Natural Description of Images”](http://bit.ly/1HEJ8kX),
    *Google Research blog*, November 17, 2014.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch12.html#id651-marker)) Oriol Vinyals等人，[“一幅图值千言（连贯）：构建图片的自然描述”](http://bit.ly/1HEJ8kX)，*Google研究博客*，2014年11月17日。
- en: ^([3](ch12.html#id656-marker)) The exception is the last word in the text, because
    nothing follows the last word. In our example text, the last word is a period
    (.), which is convenient because it has 215 other occurrences in the text and
    so does not represent a dead end. However, in real-world implementations of the
    Markov generator, the last word of the text might be something you need to account
    for.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch12.html#id656-marker)) 例外是文本中的最后一个单词，因为没有任何单词跟随最后一个单词。在我们的示例文本中，最后一个单词是句号（.），这很方便，因为文本中还有215个其他出现，所以不会形成死胡同。但是，在马尔科夫生成器的实际实现中，最后一个单词可能是您需要考虑的内容。
