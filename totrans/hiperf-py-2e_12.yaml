- en: Chapter 12\. Lessons from the Field
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we have collected stories from successful companies that use
    Python in high-data-volume and speed-critical situations. The stories are written
    by key people in each organization who have many years of experience; they share
    not just their technology choices but also some of their hard-won wisdom. We have
    four great new stories for you from other experts in our domain. We’ve also kept
    the “Lessons from the Field” from the first edition of this book; their titles
    are marked “(2014).”
  prefs: []
  type: TYPE_NORMAL
- en: Streamlining Feature Engineering Pipelines with Feature-engine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Soledad Galli (trainindata.com)
  prefs: []
  type: TYPE_NORMAL
- en: Train in Data is an education project led by experienced data scientists and
    AI software engineers. We help professionals improve coding and data science skills
    and adopt machine learning best practices. We create advanced online courses on
    machine learning and AI software engineering and open source libraries, like [Feature-engine](https://feature-engine.readthedocs.io),
    to smooth the delivery of machine learning solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Feature Engineering for Machine Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Machine learning models take in a bunch of input variables and output a prediction.
    In finance and insurance, we build models to predict, for example, the likelihood
    of a loan being repaid, the probability of an application being fraudulent, and
    whether a car should be repaired or replaced after an accident. The data we collect
    and store or recall from third-party APIs is almost never suitable to train machine
    learning models or return predictions. Instead, we transform variables extensively
    before feeding them to machine learning algorithms. We refer to the collection
    of variable transformations as *feature engineering*.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering includes procedures to impute missing data, encode categorical
    variables, transform or discretize numerical variables, put features in the same
    scale, combine features into new variables, extract information from dates, aggregate
    transactional data, and derive features from time series, text, or even images.
    There are many techniques for each of these feature engineering steps, and your
    choice will depend on the characteristics of the variables and the algorithms
    you intend to use. Thus, when feature engineers build and consume machine learning
    in organizations, we do not speak of machine learning models but of machine learning
    pipelines, where a big part of the pipeline is devoted to feature engineering
    and data transformation.
  prefs: []
  type: TYPE_NORMAL
- en: The Hard Task of Deploying Feature Engineering Pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many feature engineering transformations learn parameters from data. I have
    seen organizations utilize config files with hardcoded parameters. These files
    limit versatility and are hard to maintain (every time you retrain your model,
    you need to rewrite the config file with the new parameters). To create highly
    performant feature engineering pipelines, it’s better to develop algorithms that
    automatically learn and store these parameters and can also be saved and loaded,
    ideally as one object.
  prefs: []
  type: TYPE_NORMAL
- en: At Train in Data, we develop machine learning pipelines in the research environment
    and deploy them in the production environment. These pipelines should be reproducible.
    Reproducibility is the ability to duplicate a machine learning model exactly,
    such that, given the same data as input, both models return the same output. Utilizing
    the same code in the research and production environment smooths the deployment
    of machine learning pipelines by minimizing the amount of code to be rewritten,
    maximizing reproducibility.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering transformations need to be tested. Unit tests for each feature
    engineering procedure ensure that the algorithm returns the desired outcome. Extensive
    code refactoring in production to add unit and integration tests is extremely
    time-consuming and provides new opportunities to introduce bugs, or find bugs
    introduced during the research phase due to the lack of testing. To minimize code
    refactoring in production, it is better to introduce unit testing as we develop
    the engineering algorithms in the research phase.
  prefs: []
  type: TYPE_NORMAL
- en: The same feature engineering transformations are used across projects. To avoid
    different code implementations of the same technique, which often occurs in teams
    with many data scientists, and to enhance team performance, speed up model development,
    and smooth model operationalization, we want to reuse code that was previously
    built and tested. The best way to do that is to create in-house packages. Creating
    packages may seem time-consuming, since it involves building tests and documentation.
    But it is more efficient in the long run, because it allows us to enhance code
    and add new features incrementally, while reusing code and functionality that
    has already been developed and tested. Package development can be tracked through
    versioning and even shared with the community as open source, raising the profile
    of the developers and the organization.
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging the Power of Open Source Python Libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It’s important to use established open source projects or thoroughly developed
    in-house libraries. This is more efficient for several reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Well-developed projects tend to be thoroughly documented, so it is clear what
    each piece of code intends to achieve.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Well-established open source packages are tested to prevent the introduction
    of bugs, ensure that the transformation achieves the intended outcome, and maximize
    reproducibility.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Well-established projects have been widely adopted and approved by the community,
    giving you peace of mind that the code is of quality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use the same package in the research and production environment, minimizing
    code refactoring during deployment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Packages are clearly versioned, so you can deploy the version you used when
    developing your pipeline to ensure reproducibility, while newer versions continue
    to add functionality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open source packages can be shared, so different organizations can build tools
    together.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While open source packages are maintained by a group of experienced developers,
    the community can also contribute, providing new ideas and features that raise
    the quality of the package and code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a well-established open source library removes the task of coding from
    our hands, improving team performance, reproducibility, and collaboration, while
    reducing model research and deployment timelines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open source Python libraries like [scikit-learn](https://oreil.ly/j-4ob), [Category
    encoders](https://oreil.ly/DtSL7), and [Featuretools](https://oreil.ly/DOB7V)
    provide feature engineering functionality. To expand on existing functionality
    and smooth the creation and deployment of machine learning pipelines, I created
    the open source Python package [Feature-engine](https://oreil.ly/CZrSB), which
    provides an exhaustive battery of feature engineering procedures and supports
    the implementation of different transformations to distinct feature spaces.
  prefs: []
  type: TYPE_NORMAL
- en: Feature-engine Smooths Building and Deployment of Feature Engineering Pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Feature engineering algorithms need to learn parameters from data automatically,
    return a data format that facilitates use in research and production environments,
    and include an exhaustive battery of transformations to encourage adoption across
    projects. Feature-engine was conceived and designed to fulfill all these requirements.
    Feature-engine transformers—that is, the classes that implement a feature engineering
    transformation—learn and store parameters from data. Feature-engine transformers
    return Pandas DataFrames, which are suitable for data analysis and visualization
    during the research phase. Feature-engine supports the creation and storage of
    an entire end-to-end engineering pipeline in a single object, making deployment
    easier. And to facilitate adoption across projects, it includes an exhaustive
    list of feature transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Feature-engine includes many procedures to impute missing data, encode categorical
    variables, transform and discretize numerical variables, and remove or censor
    outliers. Each transformer can learn, or alternatively have specified, the group
    of variables it should modify. Thus, the transformer can receive the entire dataframe,
    yet it will alter only the selected variable group, removing the need of additional
    transformers or manual work to slice the dataframe and then join it back together.
  prefs: []
  type: TYPE_NORMAL
- en: Feature-engine transformers use the fit/transform methods from scikit-learn
    and expand its functionality to include additional engineering techniques. Fit/transform
    functionality makes Feature-engine transformers usable within the scikit-learn
    pipeline. Thus with Feature-engine we can store an entire machine learning pipeline
    into a single object that can be saved and retrieved or placed in memory for live
    scoring.
  prefs: []
  type: TYPE_NORMAL
- en: Helping with the Adoption of a New Open Source Package
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: No matter how good an open source package is, if no one knows it exists or if
    the community can’t easily understand how to use it, it will not succeed. Making
    a successful open source package entails making code that is performant, well
    tested, well documented, and useful—and then letting the community know that it
    exists, encouraging its adoption by users who can suggest new features, and attracting
    a developer community to add more functionality, improve the documentation, and
    enhance code quality to raise its performance. For a package developer, this means
    that we need to factor in time to develop code and to design and execute a sharing
    strategy. Here are some strategies that have worked for me and for other package
    developers.
  prefs: []
  type: TYPE_NORMAL
- en: We can leverage the power of well-established open source functionality to facilitate
    adoption by the community. Scikit-learn is the reference library for machine learning
    in Python. Thus, adopting scikit-learn fit/transform functionality in a new package
    facilitates an easy and fast adoption by the community. The learning curve to
    use the package is shorter as users are already familiar with this implementation.
    Some packages that leverage the use of fit/transform are [Keras](https://keras.io),
    Category encoders (perhaps the most renowned), and of course Feature-engine.
  prefs: []
  type: TYPE_NORMAL
- en: Users want to know how the package can be used and shared, so include a license
    stating these conditions in the code repository. Users also need instructions
    and examples of the code functionality. Docstrings in code files with information
    about functionality and examples of its use is a good start, but it is not enough.
    Widespread packages include additional documentation (which can be generated with
    ReStructuredText files) with descriptions of code functionality, examples of its
    use and the outputs returned, installation guidelines, the channels where the
    package is available (PyPI, conda), how to get started, changelogs, and more.
    Good documentation should empower users to use the library without reading the
    source code. The documentation of the machine learning visualization library [Yellowbrick](https://oreil.ly/j96lT)
    is a good example. I have adopted this practice for Feature-engine as well.
  prefs: []
  type: TYPE_NORMAL
- en: How can we increase package visibility? How do we reach potential users? Teaching
    an online course can help you reach people, especially if it’s on a prestigious
    online learning platform. In addition, publishing documentation in [Read the Docs](https://readthedocs.org),
    creating YouTube tutorials, and presenting the package at meetups and meetings
    all increase visibility. Presenting the package functionality while answering
    relevant questions in well-established user networks like Stack Overflow, Stack
    Exchange, and Quora can also help. The developers of Featuretools and Yellowbrick
    have leveraged the power of these networks. Creating a dedicated Stack Overflow
    issues list lets users ask questions and shows the package is being actively maintained.
  prefs: []
  type: TYPE_NORMAL
- en: Developing, Maintaining, and Encouraging Contribution to Open Source Libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For a package to be successful and relevant, it needs an active developer community.
    A developer community is composed of one or, ideally, a group of dedicated developers
    or maintainers who will watch for overall functionality, documentation, and direction.
    An active community allows and welcomes additional ad hoc contributors.
  prefs: []
  type: TYPE_NORMAL
- en: One thing to consider when developing a package is code maintainability. The
    simpler and shorter the code, the easier it is to maintain, which makes it more
    likely to attract contributors and maintainers. To simplify development and maintainability,
    Feature-engine leverages the power of scikit-learn base transformers. Scikit-learn
    provides an API with a bunch of base classes that developers can build upon to
    create new transformers. In addition, scikit-learn’s API provides many tests to
    ensure compatibility between packages and also that the transformer delivers the
    intended functionality. By using these, Feature-engine developers and maintainers
    focus only on feature engineering functionality, while the maintenance of base
    code is taken care of by the bigger scikit-learn community. This, of course, has
    a trade-off. If scikit-learn changes its base functionality, we need to update
    our library to ensure it is compatible with the latest version. Other open source
    packages that use scikit-learn API are Yellowbrick and Category encoders.
  prefs: []
  type: TYPE_NORMAL
- en: To encourage developers to collaborate, [NumFOCUS](https://numfocus.org) recommends
    creating a code of conduct and encouraging inclusion and diversity. The project
    needs to be open, which generally means the code should be publicly hosted, with
    guidelines to orient new contributors about project development and discussion
    forums open to public participation, like a mailing list or a Slack channel. While
    some open source Python libraries have their own codes of conduct, others, like
    Yellowbrick and Feature-engine, follow the [Python Community Code of Conduct](https://oreil.ly/8k4Tc).
    Many open source projects, including Feature-engine, are publicly hosted on GitHub.
    Contributing guidelines list ways new contributors can help—for example, by fixing
    bugs, adding new functionality, or enhancing the documentation. Contributing guidelines
    also inform new developers of the contributing cycle, how to fork the repository,
    how to work on the contributing branch, how the code review cycle works, and how
    to make a Pull Request.
  prefs: []
  type: TYPE_NORMAL
- en: Collaboration can raise the quality and performance of the library by enhancing
    code quality or functionality, adding new features, and improving documentation.
    Contributions can be as simple as reporting typos in the documentation, reporting
    code that doesn’t return the intended outcome, or requesting new features. Collaborating
    on open source libraries can also help raise the collaborator’s profile while
    exposing them to new engineering and coding practices, improving their skills.
  prefs: []
  type: TYPE_NORMAL
- en: Many developers and data scientists believe that they need to be top-notch developers
    to contribute to open source projects. I used to believe that myself, and it discouraged
    me from making contributions or requesting features—even though, as a user, I
    had a clear idea of what features were available and which were missing. This
    is far from true. Any user can contribute to the library. And package maintainers
    love contributions.
  prefs: []
  type: TYPE_NORMAL
- en: Useful contributions to Feature-engine have included simple things like adding
    a line to the *.gitignore*, sending a message through LinkedIn to bring typos
    in the docs to my attention, making a PR to correct typos themselves, highlighting
    warning issues raised by newer versions of scikit-learn, requesting new functionality,
    or expanding the battery of unit tests.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to contribute but have no experience, it is useful to go through
    the issues found on the package repository. Issues are lists with the modifications
    to the code that have priority. They are tagged with labels like “code enhancement,”
    “new functionality,” “bug fix,” or “docs.” To start, it is good to go after issues
    tagged as “good first issue” or “good for new contributors”; those tend to be
    smaller code changes and will allow you to get familiar with the contribution
    cycle. Then you can jump into more complex code modifications. Just by solving
    an easy issue, you will learn a lot about software development, Git, and code
    review cycles.
  prefs: []
  type: TYPE_NORMAL
- en: Feature-engine is currently a small package with straightforward code implementations.
    It is easy to navigate and has few dependencies, so it is a good starting point
    for contributing to open source. If you want to get started, do get in touch.
    I will be delighted to hear from you. Good luck!
  prefs: []
  type: TYPE_NORMAL
- en: Highly Performant Data Science Teams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linda Uruchurtu (Fiit)
  prefs: []
  type: TYPE_NORMAL
- en: Data science teams are different from other technical teams, because the scope
    of what they do varies according to where they sit and the type of problems they
    tackle. However, whether the team is responsible for answering “why” and “how”
    questions or simply delivering fully operational ML services, in order to deliver
    successfully, they need to keep the stakeholders happy.
  prefs: []
  type: TYPE_NORMAL
- en: This can be challenging. Most data science projects have a degree of uncertainty
    attached to them, and since there are different types of stakeholders, “happy”
    can mean different things. Some stakeholders might be concerned only with final
    deliverables, whereas others might care about side effects or common interfaces.
    Additionally, some might not be technical or have a limited understanding of the
    specifics of the project. Here I will share some lessons I have learned that make
    a difference in the way projects are carried out and delivered.
  prefs: []
  type: TYPE_NORMAL
- en: How Long Will It Take?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is possibly the most common question data science team leads are asked.
    Picture the following: management asks the project manager (PM), or whoever is
    responsible for delivery, to solve a given problem. The PM goes to the team, presents
    them with this information, and asks them to plan a solution. Cue this question,
    from the PM or from other stakeholders: How long will it take?'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, the team should ask questions to better define the scope of their solutions.
    These might include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Why is this a problem?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the impact of solving this problem?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the definition of done?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the minimal version of a solution that satisfies said definition?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is there a way to validate a solution early on?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Notice that “How long will it take?” isn’t on this list.
  prefs: []
  type: TYPE_NORMAL
- en: The strategy should be twofold. First, get a time-boxed period to ask these
    questions and propose one or more solutions. Once a solution is agreed upon, the
    PM should explain to stakeholders that the team can provide a timeline once the
    work for said solution is planned.
  prefs: []
  type: TYPE_NORMAL
- en: Discovery and Planning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The team has a fixed amount of time to come up with solutions. What’s next?
    They need to generate hypotheses, followed by exploratory work and quick prototyping,
    to keep or discard potential solutions successively.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the solution chosen, other teams may become stakeholders. Development
    teams could have requirements from APIs they hold, or they could become consumers
    of a service; product, operations, customer service, and other teams might use
    visualizations and reports. The PM’s team should discuss their ideas with these
    teams.
  prefs: []
  type: TYPE_NORMAL
- en: Once this process has taken place, the team is usually in a good position to
    determine how much uncertainty and/or risk adheres to each option. The PM can
    now assess which option is preferred.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once an option is chosen, the PM can define a timeline for milestones and deliverables.
    Useful points to raise here are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Can the deliverables be reasonably reviewed and tested?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If work depends on other teams, can work be scheduled so no delay is introduced?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can the team provide value from intermediate milestones?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is there a way to reduce risk from parts of the project that have a significant
    amount of uncertainty?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tasks derived from the plan can then be sized and time-boxed to provide a time
    estimate. It is a good idea to allow for extra time: some people like to double
    or triple the time they think they’ll need!'
  prefs: []
  type: TYPE_NORMAL
- en: Some tasks are frequently underestimated and simplified, including data collection
    and dataset building, testing, and validation. Getting good data for model building
    can often be more complex and expensive than it initially seems. One option may
    be to start with small datasets for prototyping and postpone further collection
    until after proof of concept. Testing, too, is fundamental, both for correctness
    and for reproducibility. Are inputs as expected? Are processing pipelines introducing
    errors? Are outputs correct? Unit testing and integration tests should be part
    of every effort. Finally, validation is important, particularly in the real world.
    Be sure to factor in realistic estimates for all of these tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve done that, the team has not only an answer to the “time” question,
    but also a plan with milestones that everyone can use to understand what work
    is being carried out.
  prefs: []
  type: TYPE_NORMAL
- en: Managing Expectations and Delivery
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Lots of issues can affect the time required before a delivery is achieved.
    Keep an eye on the following points to make sure you manage the team’s expectations:'
  prefs: []
  type: TYPE_NORMAL
- en: Scope creep
  prefs: []
  type: TYPE_NORMAL
- en: Scope creep is the subtle shifting of the scope of work, so that more work is
    expected than was initially planned. Pairing and reviews can help mitigate this.
  prefs: []
  type: TYPE_NORMAL
- en: Underestimating nontechnical tasks
  prefs: []
  type: TYPE_NORMAL
- en: Discussions, user research, documentation, and many other tasks can easily be
    underestimated by those who don’t know them well.
  prefs: []
  type: TYPE_NORMAL
- en: Availability
  prefs: []
  type: TYPE_NORMAL
- en: Team members’ scheduling and availability can also introduce delays.
  prefs: []
  type: TYPE_NORMAL
- en: Issues with data quality
  prefs: []
  type: TYPE_NORMAL
- en: From making sure working datasets are good to go to discovering sources of bias,
    data quality can introduce complications or even invalidate pieces of work.
  prefs: []
  type: TYPE_NORMAL
- en: Alternative options
  prefs: []
  type: TYPE_NORMAL
- en: When unexpected difficulties arise, it might make sense to consider other approaches.
    However, sunk costs might prevent the team from wanting to raise this, which could
    delay the work and risk creating the impression that the team doesn’t know what
    they are doing.
  prefs: []
  type: TYPE_NORMAL
- en: Lack of testing
  prefs: []
  type: TYPE_NORMAL
- en: Sudden changes in data inputs or bugs in data pipelines can invalidate assumptions.
    Having good test coverage from the start will improve team velocity and pay dividends
    at the end.
  prefs: []
  type: TYPE_NORMAL
- en: Difficulty testing or validating
  prefs: []
  type: TYPE_NORMAL
- en: If not enough time is allowed for testing and validating hypotheses, the schedule
    can be delayed. Changes in assumptions can also lead to alterations in the testing
    plan.
  prefs: []
  type: TYPE_NORMAL
- en: Use weekly refinement and planning sessions to spot issues and discuss whether
    tasks need to be added or removed. This will give the PM enough information to
    update the final stakeholders. Prioritization should also happen with the same
    cadence. If opportunities arise to do some tasks earlier than expected, these
    should be pushed forward.
  prefs: []
  type: TYPE_NORMAL
- en: Intermediate deliverables, particularly if they provide value outside the scope
    of the project, continuously justify the work. That’s good for the team, in terms
    of focus and morale, as well as stakeholders, who will have a sense of progress.
    The continuous process of redrawing the game plan and reviewing and adjusting
    iterations will ensure the team has a clear sense of direction and freedom to
    work, while providing enough information and value to keep stakeholders keen on
    continuing to support the project.
  prefs: []
  type: TYPE_NORMAL
- en: To become highly performant while tackling a new project, your data science
    team’s main focus has to be on making data uncertainty and business-need uncertainty
    less risky by delivering lightweight minimum viable product (MVP) solutions (think
    scripts and Python notebooks). The initial conceived MVP might actually turn out
    to be leaner than or different from the first concept, given findings down the
    line or changes in business needs. Only after validation should you proceed with
    a production-ready version.
  prefs: []
  type: TYPE_NORMAL
- en: The discovery and planning process is critical, and so is thinking in terms
    of iterations. Keep in mind that the discovery phase is always ongoing and that
    external events will always affect the plan.
  prefs: []
  type: TYPE_NORMAL
- en: Numba
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Valentin Haenel ([*http://haenel.co*](http://haenel.co))
  prefs: []
  type: TYPE_NORMAL
- en: '*Numba* is an open source, JIT function compiler for numerically focused Python.
    Initially created at Continuum Analytics (now Anaconda Inc) in 2012, it has since
    grown into a mature open source project on GitHub with a large and diverse group
    of contributors. Its primary use case is the acceleration of numerical and/or
    scientific Python code. The main entry point is a decorator—the `@jit` decorator—which
    is used to annotate the specific functions, ideally the bottlenecks of the application,
    that should be compiled. Numba will compile these functions just-in-time, which
    simply means that the function will be compiled at the first, or initial, execution.
    All subsequent executions with the same argument types will then use the compiled
    variant of the function, which should be faster than the original.'
  prefs: []
  type: TYPE_NORMAL
- en: Numba not only can compile Python, but also is NumPy-aware and can handle code
    that uses NumPy. Under the hood, Numba relies on the well-known [LLVM project](https://llvm.org),
    a collection of modular and reusable compiler and toolchain technologies. Last,
    Numba isn’t a fully fledged Python compiler. It can compile only a subset of both
    Python and NumPy—albeit a large enough subset to make it useful in a wide range
    of applications. For more information, please consult the [documentation](https://numba.pydata.org).
  prefs: []
  type: TYPE_NORMAL
- en: A Simple Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As a simple example, let’s use Numba to accelerate a Python implementation
    of an ancient algorithm for finding all prime numbers up to a given maximum (*N*):
    the sieve of Eratosthenes. It works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: First, initialize a Boolean array of length *N* to all true values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then starting with the first prime, 2, cross off (set the position in the Boolean
    list corresponding to that number to false) all multiples of the number up to
    *N*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Proceed to the next number that has not yet been crossed off, in this case 3,
    and again cross off all multiples of it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continue to proceed through the numbers and cross off their multiples until
    you reach *N*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you reach *N*, all the numbers that have not been crossed off are the set
    of prime numbers up to *N*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A reasonably efficient Python implementation might look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'After placing this in a file called *sieve.py*, you can use the `%timeit` magic
    to micro-benchmark the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'That is a speedup of roughly four hundredfold; your mileage may vary. Nonetheless,
    there are a few things of interest here:'
  prefs: []
  type: TYPE_NORMAL
- en: Compilation happened at the function level.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simply adding the decorator `@jit` was enough to instruct Numba to compile the
    function. No further modifications to the function source code, such as type annotations
    of the variables, were needed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Numba is NumPy-aware, so all the NumPy calls in this implementation were supported
    and could be compiled successfully.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The original, pure Python function is available as the `py_func` attribute of
    the compiled function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a faster but less educational version of this algorithm, the implementation
    of which is left to the interested reader.
  prefs: []
  type: TYPE_NORMAL
- en: Best Practices and Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most important recommendations for Numba is to use nopython mode
    whenever possible. To activate this mode, simply use the `nopython=True` option
    with the `@jit` decorator, as shown in the prime number example. Alternatively,
    you can use the `@njit` decorator alias, which is accessible by doing `from numba
    import njit`. In nopython mode, Numba attempts a large number of optimizations
    and can significantly improve performance. However, this mode is very strict;
    for compilation to succeed, Numba needs to be able to infer the types of all the
    variables within your function.
  prefs: []
  type: TYPE_NORMAL
- en: You can also use object mode by doing `@jit(forceobj=True)`. In this mode, Numba
    becomes very permissive about what it can and cannot compile, which limits it
    to performing a minimal set of optimizations. This is likely to have a significant
    negative effect on performance. To take advantage of Numba’s full potential, you
    really should use nopython mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you can’t decide whether you want to use object mode or not, there is the
    option to use an object-mode block. This can come in handy when only a small part
    of your code needs to execute in object mode: for example, if you have a long-running
    loop and would like to use string formatting to print the current progress of
    your program. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Pay attention to the types of the variables that you use. Numba works very
    well with both NumPy arrays and NumPy views on other datatypes. Therefore, if
    you can, use NumPy arrays as your preferred data structure. Tuples, strings, enums,
    and simple scalar types such as int, float, and Boolean are also reasonably well
    supported. Globals are fine for constants, but pass the rest of your data as arguments
    to your function. Python lists and dictionaries are unfortunately not very well
    supported. This largely stems from the fact that they can be type heterogeneous:
    a specific Python list may contain differently typed items; for example, integers,
    floats and strings. This poses a problem for Numba, because it needs the container
    to hold only items of a single type in order to compile it. However, these two
    data structures are probably one of the most used features of the Python language
    and are even one of the first things programmers learn about.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To remedy this shortcoming, Numba supports the so-called typed containers:
    `typed-list` and `typed-dict`. These are homogeneously typed variants of the Python
    list and dict. This means that they may contain only items of a single type: for
    example, a `typed-list` of only integer values. Beyond this limitation, they behave
    much like their Python counterparts and support a largely identical API. Additionally,
    they may be used within regular Python code or within Numba compiled functions,
    and can be passed into and returned from Numba compiled functions. These are available
    from the `numba.typed` submodule. Here is a simple example of a `typed-list`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'While Python does have limitations, you can rethink them and understand which
    ones can be safely disregarded when using Numba. Two specific examples come to
    mind: calling functions and `for` loops. Numba enables a technique called *inlining*
    in the underlying LLVM library to optimize away the overhead of calling functions.
    This means that during compilation, any function calls that are amenable to inlining
    are replaced with a block of code that is the equivalent of the function being
    called. As a result, there is practically no performance impact from breaking
    up a large function into a few or many small ones in order to aid readability
    and comprehensibility.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the main criticisms of Python is that its `for` loops are slow. Many
    people recommend using alternative constructs instead when attempting to improve
    the performance of a Python program: list comprehensions or even NumPy arrays.
    Numba does not suffer from this limitation, and using `for` loops in Numba compiled
    functions is fine. Observe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now benchmark the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, both Numba compiled variants have very similar performance characteristics,
    whereas the pure Python `for` loop implementation is significantly (800 times)
    slower than its compiled counterpart.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are now thinking about rewriting your NumPy array expressions as `for`
    loops, don’t! As shown in the preceding example, Numba is perfectly happy with
    NumPy arrays and their associated functions. In fact, Numba has an additional
    ace up its sleeve: an optimization known as *loop fusion*. Numba performs this
    technique predominantly on array expression operations. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the Numba compiled version is eight times faster than the pure
    NumPy one. What is going on? Without Numba, the array expression will lead to
    several `for` loops and several so-called temporaries in memory. Loosely speaking,
    for each arithmetic operation in the expression, a `for` loop over arrays must
    execute, and the result of each must be stored in a temporary array in memory.
    What loop fusion does is fuse the various loops over arithmetic operations together
    into a single loop, thereby reducing both the total number of memory lookups and
    the overall memory required to compute the result. In fact, the loop-fused variant
    may well look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this will show performance characteristics similar to those of the
    loop-fusion example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Finally, I recommend targeting serial execution initially, but keep parallel
    execution in mind. Don’t assume from the outset that only a parallel version will
    lead to your targeted performance characteristics. Instead, focus on developing
    a clean serial implementation first. Parallelism makes everything harder to reason
    about and can be a source of difficulty when debugging problems. If you are satisfied
    with your results and would still like to investigate parallelizing your code,
    Numba does come with a `parallel=True` option for the `@jit` decorator and a corresponding
    parallel range, the `prange` construct, to make creating parallel loops easier.
  prefs: []
  type: TYPE_NORMAL
- en: Getting Help
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As of early 2020, the two main recommended communication channels for Numba
    are the [GitHub issue tracker](https://oreil.ly/hXGfE) and the [Gitter chat](https://oreil.ly/8YGl1);
    this is where the action happens. There are also a mailing list and a Twitter
    account, but these are fairly low-traffic and mostly used to announce new releases
    and other important project news.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing Versus Thinking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Vincent D. Warmerdam, Senior Person at GoDataDriven ([*http://koaning.io*](http://koaning.io))
  prefs: []
  type: TYPE_NORMAL
- en: This is a story of a team that was solving the wrong problem. We were optimizing
    efficiency while ignoring effectiveness. My hope is that this story is a cautionary
    tale for others. This story actually happened, but I’ve changed parts and kept
    the details vague in the interest of keeping things incognito.
  prefs: []
  type: TYPE_NORMAL
- en: 'I was consulting for a client with a common logistics problem: they wanted
    to predict the number of trucks that would arrive at their warehouses. There was
    a good business case for this. If we knew the number of vehicles, we would know
    how large the workforce had to be to handle the workload for that day.'
  prefs: []
  type: TYPE_NORMAL
- en: The planning department had been trying to tackle this problem for years (using
    Excel). They were skeptical that an algorithm could improve things. Our job was
    to explore if machine learning could help out here.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the start, it was apparent it was a difficult time-series problem:'
  prefs: []
  type: TYPE_NORMAL
- en: There were many (seriously, many!) holidays that we needed to keep in mind since
    the warehouses operated internationally. The effect of the holiday might depend
    on the day of the week, since the warehouses did not open during weekends. Certain
    holidays meant demand would go up, while other holidays meant that the warehouses
    were closed (which would sometimes cause a three-day weekend).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seasonal shifts were not unheard of.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suppliers would frequently enter and leave the market.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The seasonal patterns were always changing because the market was continuously
    evolving.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There were many warehouses, and although they were in separate buildings, there
    was a reason to believe the number of trucks arriving at the different warehouses
    were correlated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The diagram in [Figure 12-1](#FIG-lesson-vincent1) shows the process for the
    algorithm that would calculate the seasonal effect as well as the long-term trend.
    As long as there weren’t any holidays, our method would work. The planning department
    warned us about this; the holidays were the hard part. After spending a lot of
    time collecting relevant features, we ended up building a system that mainly focused
    on trying to deal with the holidays.
  prefs: []
  type: TYPE_NORMAL
- en: '![hpp2 1201](Images/hpp2_1201.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-1\. Seasonal effects and long-term trend
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: So we iterated, did more feature engineering, and designed the algorithm. It
    got to the point where we needed to calculate a time-series model per warehouse,
    which would be post-processed with a heuristic model per holiday per day of the
    week. A holiday just before the weekend would cause a different shift than a holiday
    just after the weekend. As you might imagine, this calculation can get quite expensive
    when you also want to apply a grid search, as shown in [Figure 12-2](#FIG-lesson-vincent2).
  prefs: []
  type: TYPE_NORMAL
- en: '![hpp2 1202](Images/hpp2_1202.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-2\. Many variations cost a lot of compute time
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There were many effects that we had to estimate accurately, including the decay
    of the past measurements, how smooth the seasonal effect is, the regularization
    parameters, and how to tackle the correlation between different warehouses.
  prefs: []
  type: TYPE_NORMAL
- en: 'What didn’t help was that we needed to predict months ahead. Another hard thing
    was the cost function: it was discrete. The planning department did not care about
    (or even appreciate ) mean squared error; they cared only about the number of
    days where the prediction error would exceed 100 trucks.'
  prefs: []
  type: TYPE_NORMAL
- en: You can imagine that, in addition to statistical concerns, the model presented
    performance concerns. To keep this at bay, we restricted ourselves to simpler
    machine learning models. We gained a lot of iteration speed by doing this, which
    allowed us to focus on feature engineering. A few weeks went by before we had
    a version we could demo. We had still made a model that performed well enough,
    except for the holidays.
  prefs: []
  type: TYPE_NORMAL
- en: The model went into a proof-of-concept phase; it performed reasonably well but
    not significantly better than the current planning team’s method. The model was
    useful since it allowed the planning department to reflect if the model disagreed,
    but no one was comfortable having the model automate the planning.
  prefs: []
  type: TYPE_NORMAL
- en: Then it happened. It was my final week with the client, just before a colleague
    would be taking over. I was hanging out at the coffee corner, talking with an
    analyst about a different project for which I needed some data from him. We started
    reviewing the available tables in the database. Eventually, he told me about a
    “carts” table (shown in [Figure 12-3](#FIG-lesson-vincent3)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Me: “A carts table? What’s in there?” Analyst: “Oh, it contains all the orders
    for the carts.” Me: “Suppliers buy them from the warehouse?” Analyst: “No, actually,
    they rent them. They usually rent them three to five days in advance before they
    return them filled with goods for the warehouse.” Me: “All of your suppliers work
    this way?” Analyst: “Pretty much.”'
  prefs: []
  type: TYPE_NORMAL
- en: '![hpp2 1203](Images/hpp2_1203.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-3\. Carts contain the leading information that’s critical to this
    challenge!
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'I spotted the most significant performance issue of them all: we were solving
    the wrong problem. This wasn’t a machine learning problem; it was a SQL problem.
    The number of rented carts was a robust proxy for how many trucks the company
    would send. They wouldn’t need a machine learning model. We could just project
    the number of carts being rented a few days ahead, divide by the number of carts
    that fit into a truck, and get a sensible approximation of what to expect. If
    we had realized this earlier, we would not have needed to optimize the code for
    a gigantic grid search because there would have been no need.'
  prefs: []
  type: TYPE_NORMAL
- en: It is rather straightforward to translate a business case into an analytical
    problem that doesn’t reflect reality. Anything that you can do to prevent this
    will yield the most significant performance benefit you can imagine.
  prefs: []
  type: TYPE_NORMAL
- en: Adaptive Lab’s Social Media Analytics (2014)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ben Jackson ([adaptivelab.com](http://www.adaptivelab.com/))
  prefs: []
  type: TYPE_NORMAL
- en: Adaptive Lab is a product development and innovation company based in London’s
    Tech City area, Shoreditch. We apply our lean, user-centric method of product
    design and delivery collaboratively with a wide range of companies, from start-ups
    to large corporates.
  prefs: []
  type: TYPE_NORMAL
- en: YouGov is a global market research company whose stated ambition is to supply
    a live stream of continuous, accurate data and insight into what people are thinking
    and doing all over the world—and that’s just what we managed to provide for them.
    Adaptive Lab designed a way to listen passively to real discussions happening
    in social media and gain insight into users’ feelings on a customizable range
    of topics. We built a scalable system capable of capturing a large volume of streaming
    information, processing it, storing it indefinitely, and presenting it through
    a powerful, filterable interface in real time. The system was built using Python.
  prefs: []
  type: TYPE_NORMAL
- en: Python at Adaptive Lab
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Python is one of our core technologies. We use it in performance-critical applications
    and whenever we work with clients that have in-house Python skills, so that the
    work we produce for them can be taken on in-house.
  prefs: []
  type: TYPE_NORMAL
- en: Python is ideal for small, self-contained, long-running daemons, and it’s just
    as great with flexible, feature-rich web frameworks like Django and Pyramid. The
    Python community is thriving, which means that there’s a huge library of open
    source tools out there that allow us to build quickly and with confidence, leaving
    us to focus on the new and innovative stuff, solving problems for users.
  prefs: []
  type: TYPE_NORMAL
- en: Across all of our projects, we at Adaptive Lab reuse several tools that are
    built in Python but that can be used in a language-agnostic way. For example,
    we use SaltStack for server provisioning and Mozilla’s Circus for managing long-running
    processes. The benefit to us when a tool is open source and written in a language
    we’re familiar with is that if we find any problems, we can solve them ourselves
    and get those solutions taken up, which benefits the community.
  prefs: []
  type: TYPE_NORMAL
- en: SoMA’s Design
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our Social Media Analytics (SoMA) tool needed to cope with a high throughput
    of social media data and the storage and retrieval in real time of a large amount
    of information. After researching various data stores and search engines, we settled
    on Elasticsearch as our real-time document store. As its name suggests, it’s highly
    scalable, but it is also easy to use and is capable of providing statistical responses
    as well as search—ideal for our application. Elasticsearch itself is built in
    Java, but like any well-architected component of a modern system, it has a good
    API and is well catered to with a Python library and tutorials.
  prefs: []
  type: TYPE_NORMAL
- en: The system we designed uses queues with Celery held in Redis to quickly hand
    a large stream of data to any number of servers for independent processing and
    indexing. Each component of the whole complex system was designed to be small,
    individually simple, and able to work in isolation. Each focused on one task,
    like analyzing a conversation for sentiment or preparing a document for indexing
    into Elasticsearch. Several of these were configured to run as daemons using Mozilla’s
    Circus, which keeps all the processes up and running and allows them to be scaled
    up or down on individual servers.
  prefs: []
  type: TYPE_NORMAL
- en: 'SaltStack is used to define and provision the complex cluster and handles the
    setup of all of the libraries, languages, databases, and document stores. We also
    make use of Fabric, a Python tool for running arbitrary tasks on the command line.
    Defining servers in code has many benefits: complete parity with the production
    environment; version control of the configuration; having everything in one place.
    It also serves as documentation on the setup and dependencies required by a cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: Our Development Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We aim to make it as easy as possible for a newcomer to a project to be able
    to quickly get into adding code and deploying confidently. We use Vagrant to build
    the complexities of a system locally, inside a virtual machine that has complete
    parity with the production environment. A simple `vagrant up` is all a newcomer
    needs to get set up with all the dependencies required for their work.
  prefs: []
  type: TYPE_NORMAL
- en: We work in an agile way, planning together, discussing architecture decisions,
    and determining a consensus on task estimates. For SoMA, we made the decision
    to include at least a few tasks considered as corrections for “technical debt”
    in each sprint. Also included were tasks for documenting the system (we eventually
    established a wiki to house all the knowledge for this ever-expanding project).
    Team members review each other’s code after each task, to sanity check, offer
    feedback, and understand the new code that is about to get added to the system.
  prefs: []
  type: TYPE_NORMAL
- en: A good test suite helped bolster confidence that any changes weren’t going to
    cause existing features to fail. Integration tests are vital in a system like
    SoMA, composed of many moving parts. A staging environment offers a way to test
    the performance of new code; on SoMA in particular, it was only through testing
    against the kind of large datasets seen in production that problems could occur
    and be dealt with, so it was often necessary to reproduce that amount of data
    in a separate environment. Amazon’s Elastic Compute Cloud (EC2) gave us the flexibility
    to do this.
  prefs: []
  type: TYPE_NORMAL
- en: Maintaining SoMA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The SoMA system runs continuously, and the amount of information it consumes
    grows every day. We have to account for peaks in the data stream, network issues,
    and problems in any of the third-party service providers it relies on. So, to
    make things easy on ourselves, SoMA is designed to fix itself whenever it can.
    Thanks to Circus, processes that crash out will come back to life and resume their
    tasks from where they left off. A task will queue up until a process can consume
    it, and there’s enough breathing room there to stack up tasks while the system
    recovers.
  prefs: []
  type: TYPE_NORMAL
- en: We use Server Density to monitor the many SoMA servers. It’s very simple to
    set up, but quite powerful. A nominated engineer can receive a push message via
    phone as soon as a problem is likely to occur, in order to react in time to ensure
    it doesn’t become a problem. With Server Density, it’s also very easy to write
    custom plug-ins in Python, making it possible, for example, to set up instant
    alerts on aspects of Elasticsearch’s behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Advice for Fellow Engineers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Above all, you and your team need to be confident and comfortable that what
    is about to be deployed into a live environment is going to work flawlessly. To
    get to that point, you have to work backward, spending time on all of the components
    of the system that will give you that sense of comfort. Make deployment simple
    and foolproof; use a staging environment to test the performance with real-world
    data; ensure you have a good, solid test suite with high coverage; implement a
    process for incorporating new code into the system; and make sure technical debt
    gets addressed sooner rather than later. The more you shore up your technical
    infrastructure and improve your processes, the happier and more successful at
    engineering the right solutions your team will be.
  prefs: []
  type: TYPE_NORMAL
- en: If a solid foundation of code and ecosystem are not in place but the business
    is pressuring you to get things live, it’s only going to lead to problem software.
    It’s going to be your responsibility to push back and stake out time for incremental
    improvements to the code, and the tests and operations involved in getting things
    out the door.
  prefs: []
  type: TYPE_NORMAL
- en: Making Deep Learning Fly with RadimRehurek.com (2014)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Radim Řehůřek ([*radimrehurek.com*](http://www.radimrehurek.com))
  prefs: []
  type: TYPE_NORMAL
- en: When Ian asked me to write my “lessons from the field” on Python and optimizations
    for this book, I immediately thought, “Tell them how you made a Python port faster
    than Google’s C original!” It’s an inspiring story of making a machine learning
    algorithm, Google’s poster child for deep learning, 12,000× faster than a naive
    Python implementation. Anyone can write bad code and then trumpet about large
    speedups. But the optimized Python port also runs, somewhat astonishingly, almost
    four times faster than the original code written by Google’s team! That is, four
    times faster than opaque, tightly profiled, and optimized C.
  prefs: []
  type: TYPE_NORMAL
- en: But before drawing “machine-level” optimization lessons, some general advice
    about “human-level” optimizations.
  prefs: []
  type: TYPE_NORMAL
- en: The Sweet Spot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I run a small consulting business laser-focused on machine learning, where my
    colleagues and I help companies make sense of the tumultuous world of data analysis,
    in order to make money or save costs (or both). We help clients design and build
    wondrous systems for data processing, especially text data.
  prefs: []
  type: TYPE_NORMAL
- en: The clients range from large multinationals to nascent start-ups, and while
    each project is different and requires a different tech stack, plugging into the
    client’s existing data flows and pipelines, Python is a clear favorite. Not to
    preach to the choir, but Python’s no-nonsense development philosophy, its malleability,
    and the rich library ecosystem make it an ideal choice.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, a few thoughts “from the field” on what works:'
  prefs: []
  type: TYPE_NORMAL
- en: Communication, communication, communication
  prefs: []
  type: TYPE_NORMAL
- en: 'This one’s obvious, but worth repeating. Understand the client’s problem on
    a higher (business) level before deciding on an approach. Sit down and talk through
    what they think they need (based on their partial knowledge of what’s possible
    and/or what they Googled up before contacting you), until it becomes clear what
    they really need, free of cruft and preconceptions. Agree on ways to validate
    the solution beforehand. I like to visualize this process as a long, winding road
    to be built: get the starting line right (problem definition, available data sources)
    and the finish line right (evaluation, solution priorities), and the path in between
    falls into place.'
  prefs: []
  type: TYPE_NORMAL
- en: Be on the lookout for promising technologies
  prefs: []
  type: TYPE_NORMAL
- en: An emergent technology that is reasonably well understood and robust, is gaining
    traction, yet is still relatively obscure in the industry can bring huge value
    to the client (or yourself). As an example, a few years ago, Elasticsearch was
    a little-known and somewhat raw open source project. But I evaluated its approach
    as solid (built on top of Apache Lucene, offering replication, cluster sharding,
    etc.) and recommended its use to a client. We consequently built a search system
    with Elasticsearch at its core, saving the client significant amounts of money
    in licensing, development, and maintenance compared to the considered alternatives
    (large commercial databases). Even more importantly, using a new, flexible, powerful
    technology gave the product a massive competitive advantage. Nowadays, Elasticsearch
    has entered the enterprise market and conveys no competitive advantage at all—everyone
    knows it and uses it. Getting the timing right is what I call hitting the “sweet
    spot,” maximizing the value/cost ratio.
  prefs: []
  type: TYPE_NORMAL
- en: KISS (Keep It Simple, Stupid!)
  prefs: []
  type: TYPE_NORMAL
- en: This is another no-brainer. The best code is code you don’t have to write and
    maintain. Start simple, and improve and iterate where necessary. I prefer tools
    that follow the Unix philosophy of “do one thing, and do it well.” Grand programming
    frameworks can be tempting, with everything imaginable under one roof and fitting
    neatly together. But invariably, sooner or later, you need something the grand
    framework didn’t imagine, and then even modifications that seem simple (conceptually)
    cascade into a nightmare (programmatically). Grand projects and their all-encompassing
    APIs tend to collapse under their own weight. Use modular, focused tools, with
    APIs in between that are as small and uncomplicated as possible. Prefer text formats
    that are open to simple visual inspection, unless performance dictates otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: Use manual sanity checks in data pipelines
  prefs: []
  type: TYPE_NORMAL
- en: When optimizing data processing systems, it’s easy to stay in the “binary mindset”
    mode, using tight pipelines, efficient binary data formats, and compressed I/O.
    As the data passes through the system unseen, and unchecked (except for perhaps
    its type), it remains invisible until something outright blows up. Then debugging
    commences. I advocate sprinkling a few simple log messages throughout the code,
    showing what the data looks like at various internal points of processing, as
    good practice—nothing fancy, just an analogy to the Unix `head` command, picking
    and visualizing a few data points. Not only does this help during the aforementioned
    debugging, but seeing the data in a human-readable format leads to “aha!” moments
    surprisingly often, even when all seems to be going well. Strange tokenization!
    They promised input would always be encoded in latin1! How did a document in this
    language get in there? Image files leaked into a pipeline that expects and parses
    text files! These are often insights that go way beyond those offered by automatic
    type checking or a fixed unit test, hinting at issues beyond component boundaries.
    Real-world data is messy. Catch early even things that wouldn’t necessarily lead
    to exceptions or glaring errors. Err on the side of too much verbosity.
  prefs: []
  type: TYPE_NORMAL
- en: Navigate fads carefully
  prefs: []
  type: TYPE_NORMAL
- en: Just because a client keeps hearing about X and says they must have X too doesn’t
    mean they really need it. It might be a marketing problem rather than a technology
    one, so take care to discern the two and deliver accordingly. X changes over time
    as hype waves come and go; a recent value would be X = big data.
  prefs: []
  type: TYPE_NORMAL
- en: All right, enough business talk—here’s how I got *word2vec* in Python to run
    faster than C.
  prefs: []
  type: TYPE_NORMAL
- en: Lessons in Optimizing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[*word2vec*](https://oreil.ly/SclZ0) is a deep learning algorithm that allows
    detection of similar words and phrases. With interesting applications in text
    analytics and search engine optimization (SEO), and with Google’s lustrous brand
    name attached to it, start-ups and businesses flocked to take advantage of this
    new tool.'
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, the only available code was that produced by Google itself, an
    open source Linux command-line tool written in C. This was a well-optimized but
    rather hard-to-use implementation. The primary reason I decided to port *word2vec*
    to Python was so I could extend *word2vec* to other platforms, making it easier
    to integrate and extend for clients.
  prefs: []
  type: TYPE_NORMAL
- en: The details are not relevant here, but *word2vec* requires a training phase
    with a lot of input data to produce a useful similarity model. For example, the
    folks at Google ran *word2vec* on their GoogleNews dataset, training on approximately
    100 billion words. Datasets of this scale obviously don’t fit in RAM, so a memory-efficient
    approach must be taken.
  prefs: []
  type: TYPE_NORMAL
- en: 'I’ve authored a machine learning library, [`gensim`](https://oreil.ly/6SYgs),
    that targets exactly that sort of memory-optimization problem: datasets that are
    no longer trivial (“trivial” being anything that fits fully into RAM), yet not
    large enough to warrant petabyte-scale clusters of MapReduce computers. This “terabyte”
    problem range fits a surprisingly large portion of real-world cases, *word2vec*
    included.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Details are described [on my blog](http://bit.ly/RR_blog), but here are a few
    optimization takeaways:'
  prefs: []
  type: TYPE_NORMAL
- en: Stream your data, watch your memory
  prefs: []
  type: TYPE_NORMAL
- en: Let your input be accessed and processed one data point at a time, for a small,
    constant memory footprint. The streamed data points (sentences, in the case of
    *word2vec*) may be grouped into larger batches internally for performance (such
    as processing 100 sentences at a time), but a high-level, streamed API proved
    a powerful and flexible abstraction. The Python language supports this pattern
    very naturally and elegantly, with its built-in generators—a truly beautiful problem–tech
    match. Avoid committing to algorithms and tools that load everything into RAM,
    unless you know your data will always remain small, or you don’t mind reimplementing
    a production version yourself later.
  prefs: []
  type: TYPE_NORMAL
- en: Take advantage of Python’s rich ecosystem
  prefs: []
  type: TYPE_NORMAL
- en: I started with a readable, clean port of *word2vec* in `numpy`. `numpy` is covered
    in depth in [Chapter 6](ch06_split_000.xhtml#matrix_computation) of this book,
    but as a short reminder, it is an amazing library, a cornerstone of Python’s scientific
    community and the de facto standard for number crunching in Python. Tapping into
    `numpy`’s powerful array interfaces, memory access patterns, and wrapped BLAS
    routines for ultrafast common vector operations leads to concise, clean, and fast
    code—code that is hundreds of times faster than naive Python code. Normally I’d
    call it a day at this point, but “hundreds of times faster” was still 20× slower
    than Google’s optimized C version, so I pressed on.
  prefs: []
  type: TYPE_NORMAL
- en: Profile and compile hotspots
  prefs: []
  type: TYPE_NORMAL
- en: '*word2vec* is a typical high performance computing app, in that a few lines
    of code in one inner loop account for 90% of the entire training runtime. Here
    I rewrote a single core routine (approximately 20 lines of code) in C, using an
    external Python library, Cython, as the glue. While it’s technically brilliant,
    I don’t consider Cython a particularly convenient tool conceptually—it’s basically
    like learning another language, a nonintuitive mix between Python, `numpy`, and
    C, with its own caveats and idiosyncrasies. But until Python’s JIT technologies
    mature, Cython is probably our best bet. With a Cython-compiled hotspot, performance
    of the Python *word2vec* port is now on par with the original C code. An additional
    advantage of having started with a clean `numpy` version is that we get free tests
    for correctness, by comparing against the slower but correct version.'
  prefs: []
  type: TYPE_NORMAL
- en: Know your BLAS
  prefs: []
  type: TYPE_NORMAL
- en: A neat feature of `numpy` is that it internally wraps Basic Linear Algebra Subprograms
    (BLAS), where available. These are sets of low-level routines, optimized directly
    by processor vendors (Intel, AMD, etc.) in assembly, Fortran, or C, and designed
    to squeeze out maximum performance from a particular processor architecture. For
    example, calling an axpy BLAS routine computes `vector_y += scalar * vector_x`
    way faster than what a generic compiler would produce for an equivalent explicit
    `for` loop. Expressing *word2vec* training as BLAS operations resulted in another
    4× speedup, topping the performance of C *word2vec*. Victory! To be fair, the
    C code could link to BLAS as well, so this is not some inherent advantage of Python
    per se. `numpy` just makes things like this stand out and makes them easy to take
    advantage of.
  prefs: []
  type: TYPE_NORMAL
- en: Parallelization and multiple cores
  prefs: []
  type: TYPE_NORMAL
- en: '`gensim` contains distributed cluster implementations of a few algorithms.
    For *word2vec*, I opted for multithreading on a single machine, because of the
    fine-grained nature of its training algorithm. Using threads also allows us to
    avoid the fork-without-exec POSIX issues that Python’s multiprocessing brings,
    especially in combination with certain BLAS libraries. Because our core routine
    is already in Cython, we can afford to release Python’s GIL (global interpreter
    lock; see [“Parallelizing the Solution with OpenMP on One Machine”](ch07.xhtml#compiling-cython-omp)),
    which normally renders multithreading useless for CPU-intensive tasks. Speedup:
    another 3×, on a machine with four cores.'
  prefs: []
  type: TYPE_NORMAL
- en: Static memory allocations
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we’re processing tens of thousands of sentences per second.
    Training is so fast that even little things like creating a new `numpy` array
    (calling `malloc` for each streamed sentence) slow us down. Solution: preallocate
    a static “work” memory and pass it around, in good old Fortran fashion. Brings
    tears to my eyes. The lesson here is to keep as much bookkeeping and app logic
    in the clean Python code as possible, and to keep the optimized hotspot lean and
    mean.'
  prefs: []
  type: TYPE_NORMAL
- en: Problem-specific optimizations
  prefs: []
  type: TYPE_NORMAL
- en: 'The original C implementation contained specific micro-optimizations, such
    as aligning arrays onto specific memory boundaries or precomputing certain functions
    into memory lookup tables. A nostalgic blast from the past, but with today’s complex
    CPU instruction pipelines, memory cache hierarchies, and coprocessors, such optimizations
    are no longer a clear winner. Careful profiling suggested a few percent improvement,
    which may not be worth the extra code complexity. Takeaway: use annotation and
    profiling tools to highlight poorly optimized spots. Use your domain knowledge
    to introduce algorithmic approximations that trade accuracy for performance (or
    vice versa). But never take it on faith; profile, preferably using real production
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Optimize where appropriate. In my experience, there’s never enough communication
    to fully ascertain the problem scope, priorities, and connection to the client’s
    business goals—a.k.a. the “human-level” optimizations. Make sure you deliver on
    a problem that matters, rather than getting lost in “geek stuff” for the sake
    of it. And when you do roll up your sleeves, make it worth it!
  prefs: []
  type: TYPE_NORMAL
- en: Large-Scale Productionized Machine Learning at Lyst.com (2014)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sebastjan Trepca ([lyst.com](http://www.lyst.com))
  prefs: []
  type: TYPE_NORMAL
- en: Python and Django have been at the heart of Lyst since the site’s creation.
    As internal projects have grown, some of the Python components have been replaced
    with other tools and languages to fit the maturing needs of the system.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster Design
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The cluster runs on Amazon EC2\. In total there are approximately 100 machines,
    including the more recent C3 instances, which have good CPU performance.
  prefs: []
  type: TYPE_NORMAL
- en: Redis is used for queuing with PyRes and storing metadata. The dominant data
    format is JSON, for ease of human comprehension. `supervisord` keeps the processes
    alive.
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch and PyES are used to index all products. The Elasticsearch cluster
    stores 60 million documents across seven machines. Solr was investigated but discounted
    because of its lack of real-time updating features.
  prefs: []
  type: TYPE_NORMAL
- en: Code Evolution in a Fast-Moving Start-Up
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is better to write code that can be implemented quickly so that a business
    idea can be tested than to spend a long time attempting to write “perfect code”
    in the first pass. If code is useful, it can be refactored; if the idea behind
    the code is poor, it is cheap to delete it and remove a feature. This can lead
    to a complicated code base with many objects being passed around, but this is
    acceptable as long as the team makes time to refactor code that is useful to the
    business.
  prefs: []
  type: TYPE_NORMAL
- en: Docstrings are used heavily in Lyst—an external Sphinx documentation system
    was tried but dropped in favor of just reading the code. A wiki is used to document
    processes and larger systems. We also started creating very small services instead
    of chucking everything into one code base.
  prefs: []
  type: TYPE_NORMAL
- en: Building the Recommendation Engine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At first the recommendation engine was coded in Python, using `numpy` and `scipy`
    for computations. Subsequently, performance-critical parts of the recommender
    were sped up using Cython. The core matrix factorization operations were written
    entirely in Cython, yielding an order of magnitude improvement in speed. This
    was mostly due to the ability to write performant loops over `numpy` arrays in
    Python, something that is extremely slow in pure Python and performed poorly when
    vectorized because it necessitated memory copies of `numpy` arrays. The culprit
    was `numpy`’s fancy indexing, which always makes a data copy of the array being
    sliced: if no data copy is necessary or intended, Cython loops will be far faster.'
  prefs: []
  type: TYPE_NORMAL
- en: Over time, the online components of the system (responsible for computing recommendations
    at request time) were integrated into our search component, Elasticsearch. In
    the process, they were translated into Java to allow full integration with Elasticsearch.
    The main reason behind this was not performance, but the utility of integrating
    the recommender with the full power of a search engine, allowing us to apply business
    rules to served recommendations more easily. The Java component itself is extremely
    simple and implements primarily efficient sparse vector inner products. The more
    complex offline component remains written in Python, using standard components
    of the Python scientific stack (mostly Python and Cython).
  prefs: []
  type: TYPE_NORMAL
- en: 'In our experience, Python is useful as more than a prototyping language: the
    availability of tools such as `numpy`, Cython, and `weave` (and more recently
    Numba) allowed us to achieve very good performance in the performance-critical
    parts of the code while maintaining Python’s clarity and expressiveness where
    low-level optimization would be counterproductive.'
  prefs: []
  type: TYPE_NORMAL
- en: Reporting and Monitoring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Graphite is used for reporting. Currently, performance regressions can be seen
    by eye after a deployment. This makes it easy to drill into detailed event reports
    or to zoom out and see a high-level report of the site’s behavior, adding and
    removing events as necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Internally, a larger infrastructure for performance testing is being designed.
    It will include representative data and use cases to properly test new builds
    of the site.
  prefs: []
  type: TYPE_NORMAL
- en: A staging site will also be used to let a small fraction of real visitors see
    the latest version of the deployment—if a bug or performance regression is seen,
    it will have affected only a minority of visitors, and this version can quickly
    be retired. This will make the deployment of bugs significantly less costly and
    problematic.
  prefs: []
  type: TYPE_NORMAL
- en: Sentry is used to log and diagnose Python stack traces.
  prefs: []
  type: TYPE_NORMAL
- en: Jenkins is used for continuous integration (CI) with an in-memory database configuration.
    This enables parallelized testing so that check-ins quickly reveal any bugs to
    the developer.
  prefs: []
  type: TYPE_NORMAL
- en: Some Advice
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It’s really important to have good tools to track the effectiveness of what
    you’re building, and to be super-practical at the beginning. Start-ups change
    constantly, and engineering evolves: you start with a super-exploratory phase,
    building prototypes all the time and deleting code until you hit the gold mine,
    and then you start to go deeper, improving code, performance, etc. Until then,
    it’s all about quick iterations and good monitoring/analytics. I guess this is
    pretty standard advice that has been repeated over and over, but I think many
    don’t really get how important it is.'
  prefs: []
  type: TYPE_NORMAL
- en: I don’t think technologies matter that much nowadays, so use whatever works
    for you. I’d think twice before moving to hosted environments like App Engine
    or Heroku, though.
  prefs: []
  type: TYPE_NORMAL
- en: Large-Scale Social Media Analysis at Smesh (2014)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Alex Kelly ([*sme.sh*](http://www.sme.sh))
  prefs: []
  type: TYPE_NORMAL
- en: At Smesh, we produce software that ingests data from a wide variety of APIs
    across the web; filters, processes, and aggregates it; and then uses that data
    to build bespoke apps for a variety of clients. For example, we provide the tech
    that powers the tweet filtering and streaming in Beamly’s second-screen TV app,
    run a brand and campaign monitoring platform for mobile network EE, and run a
    bunch of Adwords data analysis projects for Google.
  prefs: []
  type: TYPE_NORMAL
- en: To do that, we run a variety of streaming and polling services, frequently polling
    Twitter, Facebook, YouTube, and a host of other services for content and processing
    several million tweets daily.
  prefs: []
  type: TYPE_NORMAL
- en: Python’s Role at Smesh
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We use Python extensively—the majority of our platform and services are built
    with it. The wide variety of libraries, tools, and frameworks available allows
    us to use it across the board for most of what we do.
  prefs: []
  type: TYPE_NORMAL
- en: That variety gives us the ability to (hopefully) pick the right tool for the
    job. For example, we’ve created apps using Django, Flask, and Pyramid. Each has
    its own benefits, and we can pick the one that’s right for the task at hand. We
    use Celery for tasks; Boto for interacting with AWS; and PyMongo, MongoEngine,
    redis-py, Psycopg, etc. for all our data needs. The list goes on and on.
  prefs: []
  type: TYPE_NORMAL
- en: The Platform
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our main platform consists of a central Python module that provides hooks for
    data input, filtering, aggregations and processing, and a variety of other core
    functions. Project-specific code imports functionality from that core and then
    implements more specific data processing and view logic, as each application requires.
  prefs: []
  type: TYPE_NORMAL
- en: This has worked well for us up to now, and allows us to build fairly complex
    applications that ingest and process data from a wide variety of sources without
    much duplication of effort. However, it isn’t without its drawbacks—each app is
    dependent on a common core module, making the process of updating the code in
    that module and keeping all the apps that use it up-to-date a major task.
  prefs: []
  type: TYPE_NORMAL
- en: We’re currently working on a project to redesign that core software and move
    toward more of a service-oriented architecture (SoA) approach. It seems that finding
    the right time to make that sort of architectural change is one of the challenges
    that faces most software teams as a platform grows. There is overhead in building
    components as individual services, and often the deep domain-specific knowledge
    required to build each service is acquired only through an initial iteration of
    development, where that architectural overhead is a hindrance to solving the real
    problem at hand. Hopefully, we’ve chosen a sensible time to revisit our architectural
    choices to move things forward. Time will tell.
  prefs: []
  type: TYPE_NORMAL
- en: High Performance Real-Time String Matching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We consume lots of data from the Twitter Streaming API. As we stream in tweets,
    we match the input strings against a set of keywords so that we know which of
    the terms we’re tracking that each tweet is related to. That’s not such a problem
    with a low rate of input, or a small set of keywords, but doing that matching
    for hundreds of tweets per second, against hundreds or thousands of possible keywords,
    starts to get tricky.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make things even trickier, we’re not interested in simply whether the keyword
    string exists in the tweet, but in more complex pattern matching against word
    boundaries, start and end of line, and optionally the use of # and @ characters
    to prefix the string. The most effective way to encapsulate that matching knowledge
    is using regular expressions. However, running thousands of regex patterns across
    hundreds of tweets per second is computationally intensive. Previously, we had
    to run many worker nodes across a cluster of machines to perform the matching
    reliably in real time.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Knowing this was a major performance bottleneck in the system, we tried a variety
    of things to improve the performance of our matching system: simplifying the regexes,
    running enough processes to ensure we were utilizing all the cores on our servers,
    ensuring all our regex patterns are compiled and cached properly, running the
    matching tasks under PyPy instead of CPython, etc. Each of these resulted in a
    small increase in performance, but it was clear this approach was going to shave
    only a fraction of our processing time. We were looking for an order-of-magnitude
    speedup, not a fractional improvement.'
  prefs: []
  type: TYPE_NORMAL
- en: It was obvious that rather than trying to increase the performance of each match,
    we needed to reduce the problem space before the pattern matching takes place.
    So we needed to reduce either the number of tweets to process, or the number of
    regex patterns we needed to match the tweets against. Dropping the incoming tweets
    wasn’t an option—that’s the data we’re interested in. So we set about finding
    a way to reduce the number of patterns we need to compare an incoming tweet to
    in order to perform the matching.
  prefs: []
  type: TYPE_NORMAL
- en: We started looking at various trie structures for allowing us to do pattern
    matching between sets of strings more efficiently, and came across the Aho-Corasick
    string-matching algorithm. It turned out to be ideal for our use case. The dictionary
    from which the trie is built needs to be static—you can’t add new members to the
    trie after the automaton has been finalized—but for us this isn’t a problem, as
    the set of keywords is static for the duration of a session streaming from Twitter.
    When we change the terms we’re tracking we must disconnect from and reconnect
    to the API, so we can rebuild the Aho-Corasick trie at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: Processing an input against the strings using Aho-Corasick finds all possible
    matches simultaneously, stepping through the input string a character at a time
    and finding matching nodes at the next level down in the trie (or not, as the
    case may be). So we can very quickly find which of our keyword terms may exist
    in the tweet. We still don’t know for sure, as the pure string-in-string matching
    of Aho-Corasick doesn’t allow us to apply any of the more complex logic that is
    encapsulated in the regex patterns, but we can use the Aho-Corasick matching as
    a prefilter. Keywords that don’t exist in the string can’t match, so we know we
    have to try only a small subset of all our regex patterns, based on the keywords
    that do appear in the text. Rather than evaluating hundreds or thousands of regex
    patterns against every input, we rule out the majority and need to process only
    a handful for each tweet.
  prefs: []
  type: TYPE_NORMAL
- en: By reducing the number of patterns that we attempt to match against each incoming
    tweet to just a small handful, we’ve managed to achieve the speedup we were looking
    for. Depending on the complexity of the trie and the average length of the input
    tweets, our keyword matching system now performs somewhere between 10–100× faster
    than the original naive implementation.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re doing a lot of regex processing, or other pattern matching, I highly
    recommend having a dig around the different variations of prefix and suffix tries
    that might help you to find a blazingly fast solution to your problem.
  prefs: []
  type: TYPE_NORMAL
- en: Reporting, Monitoring, Debugging, and Deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We maintain a bunch of different systems running our Python software and the
    rest of the infrastructure that powers it all. Keeping it all up and running without
    interruption can be tricky. Here are a few lessons we’ve learned along the way.
  prefs: []
  type: TYPE_NORMAL
- en: It’s really powerful to be able to see both in real time and historically what’s
    going on inside your systems, whether that be in your own software or the infrastructure
    it runs on. We use Graphite with `collectd` and `statsd` to allow us to draw pretty
    graphs of what’s going on. That gives us a way to spot trends, and to retrospectively
    analyze problems to find the root cause. We haven’t got around to implementing
    it yet, but Etsy’s Skyline also looks brilliant as a way to spot the unexpected
    when you have more metrics than you can keep track of. Another useful tool is
    Sentry, a great system for event logging and keeping track of exceptions being
    raised across a cluster of machines.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment can be painful, no matter what you’re using to do it. We’ve been
    users of Puppet, Ansible, and Salt. They all have pros and cons, but none of them
    will make a complex deployment problem magically go away.
  prefs: []
  type: TYPE_NORMAL
- en: To maintain high availability for some of our systems, we run multiple geographically
    distributed clusters of infrastructure, running one system live and others as
    hot spares, with switchover being done by updates to DNS with low Time-to-Live
    (TTL) values. Obviously that’s not always straightforward, especially when you
    have tight constraints on data consistency. Thankfully, we’re not affected by
    that too badly, making the approach relatively straightforward. It also provides
    us with a fairly safe deployment strategy, updating one of our spare clusters
    and performing testing before promoting that cluster to live and updating the
    others.
  prefs: []
  type: TYPE_NORMAL
- en: Along with everyone else, we’re really excited by the prospect of what can be
    done with [Docker](http://www.docker.com). Also along with pretty much everyone
    else, we’re still just at the stage of playing around with it to figure out how
    to make it part of our deployment processes. However, having the ability to rapidly
    deploy our software in a lightweight and reproducible fashion, with all its binary
    dependencies and system libraries included, seems to be just around the corner.
  prefs: []
  type: TYPE_NORMAL
- en: At a server level, there’s a whole bunch of routine stuff that just makes life
    easier. Monit is great for keeping an eye on things for you. Upstart and `supervisord`
    make running services less painful. Munin is useful for some quick and easy system-level
    graphing if you’re not using a full Graphite/`collectd` setup. And Corosync/Pacemaker
    can be a good solution for running services across a cluster of nodes (for example,
    when you have a bunch of services that you need to run somewhere, but not everywhere).
  prefs: []
  type: TYPE_NORMAL
- en: I’ve tried not to just list buzzwords here, but to point you toward software
    we’re using every day, which is really making a difference in how effectively
    we can deploy and run our systems. If you’ve heard of them all already, I’m sure
    you must have a whole bunch of other useful tips to share, so please drop me a
    line with some pointers. If not, go check them out—hopefully, some of them will
    be as useful to you as they are to us.
  prefs: []
  type: TYPE_NORMAL
- en: PyPy for Successful Web and Data Processing Systems (2014)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Marko Tasic ([*https://github.com/mtasic85*](https://github.com/mtasic85))
  prefs: []
  type: TYPE_NORMAL
- en: Since I had a great experience early on with PyPy, I chose to use it everywhere
    where it was applicable. I have used it from small toy projects where speed was
    essential to medium-sized projects. The first project where I used it was a protocol
    implementation; the protocols we implemented were Modbus and DNP3\. Later, I used
    it for a compression algorithm implementation, and everyone was amazed by its
    speed. The first version I used in production was PyPy 1.2 with JIT out of the
    box, if I recall correctly. By version 1.4, we were sure it was the future of
    all our projects, because many bugs got fixed and the speed just increased more
    and more. We were surprised how simple cases were made 2–3× faster just by upgrading
    PyPy up to the next version.
  prefs: []
  type: TYPE_NORMAL
- en: I will explain two separate but deeply related projects that share 90% of the
    same code here, but to keep the explanation simple to follow, I will refer to
    both of them as “the project.”
  prefs: []
  type: TYPE_NORMAL
- en: The project was to create a system that collects newspapers, magazines, and
    blogs, applies optical character recognition (OCR) if necessary, classifies them,
    translates, applies sentiment analyzing, analyzes the document structure, and
    indexes them for later search. Users can search for keywords in any of the available
    languages and retrieve information about indexed documents. Search is cross-language,
    so users can write in English and get results in French. Additionally, users will
    receive articles and keywords highlighted from the document’s page with information
    about the space occupied and price of publication. A more advanced use case would
    be report generation, where users can see a tabular view of results with detailed
    information on spending by any particular company on advertising in monitored
    newspapers, magazines, and blogs. As well as advertising, it can also “guess”
    if an article is paid or objective and determine its tone.
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Obviously, PyPy was our favorite Python implementation. For the database, we
    used Cassandra and Elasticsearch. Cache servers used Redis. We used Celery as
    a distributed task queue (workers), and for its broker, we used RabbitMQ. Results
    were kept in a Redis backend. Later on, Celery used Redis more exclusively for
    both brokers and backend. The OCR engine used is Tesseract. The language translation
    engine and server used is Moses. We used Scrapy for crawling websites. For distributed
    locking in the whole system, we use a ZooKeeper server, but initially Redis was
    used for that. The web application is based on the excellent Flask web framework
    and many of its extensions, such as Flask-Login, Flask-Principal, etc. The Flask
    application was hosted by Gunicorn and Tornado on every web server, and nginx
    was used as a reverse proxy server for the web servers. The rest of the code was
    written by us and is pure Python that runs on top of PyPy.
  prefs: []
  type: TYPE_NORMAL
- en: The whole project is hosted on an in-house OpenStack private cloud and executes
    between 100 and 1,000 instances of ArchLinux, depending on requirements, which
    can change dynamically on the fly. The whole system consumes up to 200 TB of storage
    every 6–12 months, depending on the mentioned requirements. All processing is
    done by our Python code, except OCR and translation.
  prefs: []
  type: TYPE_NORMAL
- en: The Database
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We developed a Python package that unifies model classes for Cassandra, Elasticsearch,
    and Redis. It is a simple object relational mapper (ORM) that maps everything
    to a dict or list of dicts, in the case where many records are retrieved from
    the database.
  prefs: []
  type: TYPE_NORMAL
- en: Since Cassandra 1.2 did not support complex queries on indices, we supported
    them with join-like queries. However, we allowed complex queries over small datasets
    (up to 4 GB) because much of that had to be processed while held in memory. PyPy
    ran in cases where CPython could not even load data into memory, thanks to its
    strategies applied to homogeneous lists to make them more compact in the memory.
    Another benefit of PyPy is that its JIT compilation kicked in loops where data
    manipulation or analysis happened. We wrote code in such a way that the types
    would stay static inside loops because that’s where JIT-compiled code is especially
    good.
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch was used for indexing and fast searching of documents. It is very
    flexible when it comes to query complexity, so we did not have any major issues
    with it. One of the issues we had was related to updating documents; it is not
    designed for rapidly changing documents, so we had to migrate that part to Cassandra.
    Another limitation was related to facets and memory required on the database instance,
    but that was solved by having more smaller queries and then manually manipulating
    data in Celery workers. No major issues surfaced between PyPy and the PyES library
    used for interaction with Elasticsearch server pools.
  prefs: []
  type: TYPE_NORMAL
- en: The Web Application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we’ve mentioned, we used the Flask framework with its third-party extensions.
    Initially, we started everything in Django, but we switched to Flask because of
    rapid changes in requirements. This does not mean that Flask is better than Django;
    it was just easier for us to follow code in Flask than in Django, since its project
    layout is very flexible. Gunicorn was used as a Web Server Gateway Interface (WSGI)
    HTTP server, and its I/O loop was executed by Tornado. This allowed us to have
    up to one hundred concurrent connections per web server. This was lower than expected
    because many user queries can take a long time—a lot of analyzing happens in user
    requests, and data is returned in user interactions.
  prefs: []
  type: TYPE_NORMAL
- en: Initially, the web application depended on the Python Imaging Library (PIL)
    for article and word highlighting. We had issues with the PIL library and PyPy
    because at that time many memory leaks were associated with PIL. Then we switched
    to Pillow, which was more frequently maintained. In the end, we wrote a library
    that interacted with GraphicsMagick via a subprocess module.
  prefs: []
  type: TYPE_NORMAL
- en: PyPy runs well, and the results are comparable with CPython. This is because
    usually web applications are I/O-bound. However, with the development of STM in
    PyPy, we hope to have scalable event handling on a multicore instance level soon.
  prefs: []
  type: TYPE_NORMAL
- en: OCR and Translation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We wrote pure Python libraries for Tesseract and Moses because we had problems
    with CPython API-dependent extensions. PyPy has good support for the CPython API
    using CPyExt, but we wanted to be more in control of what happens under the hood.
    As a result, we made a PyPy-compatible solution with slightly faster code than
    on CPython. The reason it was not faster is that most of the processing happened
    in the C/C++ code of both Tesseract and Moses. We could only speed up output processing
    and building Python structure of documents. There were no major issues at this
    stage with PyPy compatibility.
  prefs: []
  type: TYPE_NORMAL
- en: Task Distribution and Workers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Celery gave us the power to run many tasks in the background. Typical tasks
    are OCR, translation, analysis, etc. The whole thing could be done using Hadoop
    for MapReduce, but we chose Celery because we knew that the project requirements
    might change often.
  prefs: []
  type: TYPE_NORMAL
- en: We had about 20 workers, and each worker had between 10 and 20 functions. Almost
    all functions had loops, or many nested loops. We cared that types stayed static,
    so the JIT compiler could do its job. The end results were a 2–5× speedup over
    CPython. The reason we did not get better speedups was because our loops were
    relatively small, between 20,000 and 100,000 iterations. In some cases where we
    had to do analysis on the word level, we had over 1 million iterations, and that’s
    where we got over a 10× speedup.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'PyPy is an excellent choice for every pure Python project that depends on speed
    of execution of readable and maintainable large source code. We found PyPy also
    to be very stable. All our programs were long-running with static and/or homogeneous
    types inside data structures, so JIT could do its job. When we tested the whole
    system on CPython, the results did not surprise us: we had roughly a 2× speedup
    with PyPy over CPython. In the eyes of our clients, this meant 2× better performance
    for the same price. In addition to all the good stuff that PyPy has brought to
    us so far, we hope that its software transactional memory (STM) implementation
    will bring to us scalable parallel execution for Python code.'
  prefs: []
  type: TYPE_NORMAL
- en: Task Queues at Lanyrd.com (2014)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Andrew Godwin
  prefs: []
  type: TYPE_NORMAL
- en: Lanyrd is a website for social discovery of conferences—our users sign in, and
    we use their friend graphs from social networks, as well as other indicators like
    their industry of work or their geographic location, to suggest relevant conferences.
  prefs: []
  type: TYPE_NORMAL
- en: The main work of the site is in distilling this raw data down into something
    we can show to the users—essentially, a ranked list of conferences. We have to
    do this offline, because we refresh the list of recommended conferences every
    couple of days and because we’re hitting external APIs that are often slow. We
    also use the Celery task queue for other things that take a long time, like fetching
    thumbnails for links people provide and sending email. There are usually well
    over 100,000 tasks in the queue each day, and sometimes many more.
  prefs: []
  type: TYPE_NORMAL
- en: Python’s Role at Lanyrd
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Lanyrd was built with Python and Django from day one, and virtually every part
    of it is written in Python—the website itself, the offline processing, our statistical
    and analysis tools, our mobile backend servers, and the deployment system. It’s
    a very versatile and mature language and one that’s incredibly easy to write things
    in quickly, mostly thanks to the large amount of libraries available and the language’s
    easily readable and concise syntax, which means it’s easy to update and refactor
    as well as easy to write initially.
  prefs: []
  type: TYPE_NORMAL
- en: The Celery task queue was already a mature project when we evolved the need
    for a task queue (very early on), and the rest of Lanyrd was already in Python,
    so it was a natural fit. As we grew, there was a need to change the queue that
    backed it (which ended up being Redis), but it’s generally scaled very well.
  prefs: []
  type: TYPE_NORMAL
- en: As a start-up, we had to ship some known technical debt in order to make some
    headway—this is something you just have to do, and as long as you know what your
    issues are and when they might surface, it’s not necessarily a bad thing. Python’s
    flexibility in this regard is fantastic; it generally encourages loose coupling
    of components, which means it’s often easy to ship something with a “good enough”
    implementation and then easily refactor a better one in later.
  prefs: []
  type: TYPE_NORMAL
- en: Anything critical, such as payment code, had full unit test coverage, but for
    other parts of the site and task queue flow (especially display-related code)
    things were often moving too fast to make unit tests worthwhile (they would be
    too fragile). Instead, we adopted a very agile approach and had a two-minute deploy
    time and excellent error tracking; if a bug made it into live, we could often
    fix it and deploy within five minutes.
  prefs: []
  type: TYPE_NORMAL
- en: Making the Task Queue Performant
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The main issue with a task queue is throughput. If it gets backlogged, the website
    keeps working but starts getting mysteriously outdated—lists don’t update, page
    content is wrong, and emails don’t get sent for hours.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, though, task queues also encourage a very scalable design; as long
    as your central messaging server (in our case, Redis) can handle the messaging
    overhead of the job requests and responses, for the actual processing you can
    spin up any number of worker daemons to handle the load.
  prefs: []
  type: TYPE_NORMAL
- en: Reporting, Monitoring, Debugging, and Deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We had monitoring that kept track of our queue length, and if it started becoming
    long we would just deploy another server with more worker daemons. Celery makes
    this very easy to do. Our deployment system had hooks where we could increase
    the number of worker threads on a box (if our CPU utilization wasn’t optimal)
    and could easily turn a fresh server into a Celery worker within 30 minutes. It’s
    not like website response times going through the floor—if your task queues suddenly
    get a load spike, you have some time to implement a fix and usually it’ll smooth
    over itself, if you’ve left enough spare capacity.
  prefs: []
  type: TYPE_NORMAL
- en: Advice to a Fellow Developer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: My main advice is to shove as much as you can into a task queue (or a similar
    loosely coupled architecture) as soon as possible. It takes some initial engineering
    effort, but as you grow, operations that used to take half a second can grow to
    half a minute, and you’ll be glad they’re not blocking your main rendering thread.
    Once you’ve got there, make sure you keep a close eye on your average queue latency
    (how long it takes a job to go from submission to completion), and make sure there’s
    some spare capacity for when your load increases.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, be aware that having multiple task queues for different priorities
    of tasks makes sense. Sending email isn’t very high priority; people are used
    to emails taking minutes to arrive. However, if you’re rendering a thumbnail in
    the background and showing a spinner while you do it, you want that job to be
    high priority, as otherwise you’re making the user experience worse. You don’t
    want your 100,000-person mailshot to delay all thumbnailing on your site for the
    next 20 minutes!
  prefs: []
  type: TYPE_NORMAL
