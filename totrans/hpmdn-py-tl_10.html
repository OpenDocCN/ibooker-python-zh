<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 7. Measuring Coverage with Coverage.py"><div class="chapter" id="chapter_coverage">
<h1><span class="label">Chapter 7. </span>Measuring Coverage with Coverage.py</h1>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id297">
<h1>A Note for Early Release Readers</h1>
<p>With Early Release ebooks, you get books in their earliest form—the author’s raw and unedited content as they write—so you can take advantage of these technologies long before the official release of these titles.</p>

<p>This will be the seventh chapter of the final book. Please note that the GitHub repo will be made active later on.</p>

<p>If you have comments about how we might improve the content and/or examples in this book, or if you notice missing material within this chapter, please reach out to the author at <a href="mailto:mail@claudiojolowicz.com">mail@claudiojolowicz.com</a>.</p>
</div></aside>

<p>How confident in a code change are you when your tests pass?</p>

<p>If you look at tests as a way to detect bugs, you can describe their sensitivity
and specificity.</p>

<p>The <em>sensitivity</em> of your test suite is the probability of a test failure when
there’s a defect in the code. If large parts of the code are untested, or if the
tests don’t check for expected behavior, you have low sensitivity.</p>

<p>The <em>specificity</em> of your tests is the probability that they will pass if the
code is free of defects. If your tests are <em>flaky</em> (they fail intermittently) or
<em>brittle</em> (they fail when you change implementation details), then you have low
specificity. Invariably, people stop paying attention to failing tests. This
chapter isn’t about specificity, though.</p>

<p>There’s a great way to boost the sensitivity of your tests: when you add or
change behavior, write a failing test before the code that makes it pass. If you
do this, your test suite will capture your expectations for the code.</p>

<p>Another effective strategy is to test your software with the various inputs and
environmental constraints that you expect it to encounter in the real world.
Cover the edge cases of a function, like empty lists or negative numbers. Test
common error scenarios, not just the “happy path”.</p>

<p><em>Code coverage</em> is a measure of the extent by which the test suite exercises
your code. Full coverage doesn’t guarantee high sensitivity: If your tests cover
every line in your code, you can still have bugs. It’s an upper bound, though.
If your code coverage is 80%, then 20% of your code will <em>never</em> trigger a test
failure, no matter how many bugs creep in. It’s also a quantitative measure
amenable to automated tools. These two properties make coverage a useful proxy
for sensitivity.</p>

<p>In short, coverage tools record each line in your code when you run it. After
completion, they report the overall percentage of executed lines with respect to
the entire codebase.</p>

<p>Coverage tools aren’t limited to measuring test coverage. For example, code
coverage lets you find which modules an API endpoint in a large codebase uses.
Or you could use it to determine the extent to which code examples document your
project.</p>

<p>In this chapter, I’ll explain how to measure code coverage with Coverage.py, a
coverage tool for Python. In the main sections of this chapter, you’ll learn how
to install, configure, and run Coverage.py, and how to identify missing lines of
source code and missing branches in the control flow. I’ll explain how to
measure code coverage across multiple environments and processes. Finally, I’ll
talk about what code coverage you should aim for and how to reach your coverage
target.</p>

<p>How does coverage measurement work in Python? The interpreter lets you register
a callback—​a <em>trace function</em>—using the function <code>sys.settrace</code>. From that
point onwards, the interpreter invokes the callback whenever it executes a line
of code—​as well as in some other situations, like entering or returning from
functions or raising exceptions. Coverage tools register a trace function that
records each executed line of source code in a local database.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="sidebar_coverage_trace">
<h1>The <code>trace</code> module</h1>
<p>Python’s standard library includes a coverage tool in the <code>trace</code> module. Let’s
use it to measure test coverage for the <code>random-wikipedia-article</code> project. Run
the test suite via <code>trace</code> in the active environment, as shown below:</p>
<pre data-type="programlisting">$ <strong>py -m trace --count --summary --missing -C coverage --module pytest</strong>
=========================== test session starts ===========================
platform darwin -- Python 3.12.2, pytest-8.1.1, pluggy-1.4.0
rootdir: ...
configfile: pyproject.toml
plugins: Faker-24.2.0, anyio-4.3.0, xdist-3.5.0, pytest_httpserver-1.0.10
collected 21 items

tests/test_main.py .....................                            [100%]

=========================== 21 passed in 3.75s ============================
lines   cov%   module   (path)
   26    92%   random_wikipedia_article.__init__   (...)
   33   100%   tests.test_main   (...)
...
</pre>

<p>The command asks <code>trace</code> to count how often each line is executed. It writes the
results to <em>&lt;module&gt;.cover</em> files in the <em>coverage</em> directory, marking missed
lines with the string <code>&gt;&gt;&gt;&gt;&gt;&gt;</code>. It also writes a summary to the terminal, with
coverage percentages for every module.</p>

<p>The summary includes modules from the standard library and third-party packages.
It’s easy to miss the fact that your <code>__main__</code> module doesn’t appear at all. If
you’re curious why the <code>__init__</code> module only has 92% coverage, take a look at
the file <em>random_wikipedia_article.__init__.cover</em>. (Bear with me, we’ll
get to those missing lines shortly.)</p>

<p>Measuring coverage with the standard library alone is cumbersome, even for a
simple script like <code>random-wikipedia-article</code>. The <code>trace</code> module is an early
proof of concept for Python code coverage. For real-world projects, you should
use the third-party package Coverage.py.</p>
</div></aside>






<section data-type="sect1" data-pdf-bookmark="Using Coverage.py"><div class="sect1" id="section_coverage_coverage">
<h1>Using Coverage.py</h1>

<p><a href="https://coverage.readthedocs.io/">Coverage.py</a> is a mature and widely used code
coverage tool for Python. Created over two decades ago—​predating PyPI and
setuptools—​and actively maintained ever since, it has measured coverage on
every interpreter since Python 2.1.</p>

<p>Add <code>coverage[toml]</code> to your test dependencies (see
<a data-type="xref" href="ch06.html#section_testing_dependencies">“Managing Test Dependencies”</a>). The <code>toml</code> extra allows Coverage.py to read
its configuration from <em>pyproject.toml</em> on older interpreters. Since Python
3.11, the standard library includes the <code>tomllib</code> module for parsing TOML files.</p>

<p>Measuring coverage is a two-step process. First, you gather coverage data during
a test run with <code>coverage run</code>. Second, you compile an aggregated report from
the data with <code>coverage report</code>. Each command has a table in <em>pyproject.toml</em>
under <code>tool.coverage</code>.</p>

<p>Start by configuring which packages you want to measure—​it lets Coverage.py
report modules that never showed up during execution, like the <code>__main__</code> module
earlier. (Even without the setting, it won’t drown you in reports about the
standard library.) Specify your top-level import package, as well as the test
suite:</p>

<pre data-type="programlisting" data-code-language="toml"><code class="k">[tool.coverage.run]</code><code class="w"/>
<code class="n">source</code><code class="w"> </code><code class="o">=</code><code class="w"> </code><code class="p">[</code><code class="s2">"random_wikipedia_article"</code><code class="p">,</code><code class="w"> </code><code class="s2">"tests"</code><code class="p">]</code><code class="w"/></pre>
<div data-type="tip"><h6>Tip</h6>
<p>Measuring code coverage for your test suite may seem strange—​but you should
always do it. It alerts you when tests don’t run and helps you identify
unreachable code within them. Treat your tests the same way you would treat any
other code.<sup><a data-type="noteref" id="id298-marker" href="ch07.html#id298">1</a></sup></p>
</div>

<p>You can invoke <code>coverage run</code> with a Python script, followed by its command-line
arguments. Alternatively, you can use its <code>-m</code> option with an importable module.
Use the second method—​it ensures that you run pytest from the current
environment:</p>
<pre data-type="programlisting">$ <strong>py -m coverage run -m pytest</strong>
</pre>

<p>After running this command, you’ll find a file named <em>.coverage</em> in the current
directory. Coverage.py uses it to store the coverage data it gathered during the
test run.<sup><a data-type="noteref" id="id299-marker" href="ch07.html#id299">2</a></sup></p>

<p>Coverage reports display the overall percentage of code coverage, as well as a
breakdown per source file. Use the <code>show_missing</code> setting to also include line
numbers for statements missing from coverage:</p>

<pre data-type="programlisting" data-code-language="toml"><code class="k">[tool.coverage.report]</code><code class="w"/>
<code class="n">show_missing</code><code class="w"> </code><code class="o">=</code><code class="w"> </code><code class="kc">true</code><code class="w"/></pre>

<p>Run <code>coverage report</code> to show the coverage report in the terminal:</p>
<pre data-type="programlisting">$ <strong>py -m coverage report</strong>
Name                                       Stmts   Miss  Cover   Missing
------------------------------------------------------------------------
src/random_wikipedia_article/__init__.py      26      2    92%   38-39
src/random_wikipedia_article/__main__.py       2      2     0%   1-3
tests/__init__.py                              0      0   100%
tests/test_main.py                            33      0   100%
------------------------------------------------------------------------
TOTAL                                         61      4    93%
</pre>

<p>Overall, your project has a coverage of 93%—4 statements never showed up during
the tests. The test suite itself has full coverage, as you would expect.</p>

<p>Let’s take a closer look at those missing statements. The <code>Missing</code> column in
the coverage report lists them by line number. You can use your code editor to
display the source code with line numbers, or the standard <code>cat -n</code> command on
Linux and macOS. Again, the entire <code>__main__</code> module is missing from coverage:</p>

<pre data-type="programlisting">   1  from random_wikipedia_article import main  # missing
   2
   3  main()                                     # missing</pre>

<p>The missing lines in <em>__init__.py</em> correspond to the body of the <code>main</code>
function:</p>

<pre data-type="programlisting">  37  def main():
  38      article = fetch(API_URL)   # missing
  39      show(article, sys.stdout)  # missing</pre>

<p>This is surprising—​the end-to-end test from <a data-type="xref" href="ch06.html#example_testing_end_to_end">Example 6-2</a> runs
the entire program, so all of those lines are definitely being tested. For now,
disable coverage measurements for the <code>__main__</code> module:</p>

<pre data-type="programlisting" data-code-language="toml"><code class="k">[tool.coverage.run]</code><code class="w"/>
<code class="n">omit</code><code class="w"> </code><code class="o">=</code><code class="w"> </code><code class="p">[</code><code class="s2">"*/__main__.py"</code><code class="p">]</code><code class="w"/></pre>

<p>You can exclude the <code>main</code> function using a special comment:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">main</code><code class="p">():</code>  <code class="c1"># pragma: no cover</code>
    <code class="n">article</code> <code class="o">=</code> <code class="n">fetch</code><code class="p">(</code><code class="n">API_URL</code><code class="p">)</code>
    <code class="n">show</code><code class="p">(</code><code class="n">article</code><code class="p">,</code> <code class="n">sys</code><code class="o">.</code><code class="n">stdout</code><code class="p">)</code></pre>

<p>If this feels like cheating, bear with me until <a data-type="xref" href="#section_coverage_subprocess">“Measuring in Subprocesses”</a>,
where you’ll re-enable coverage measurements for these lines.</p>

<p>If you run both steps again, Coverage.py will report full code coverage. Let’s
make sure you’ll notice any lines that aren’t exercised by your tests. Configure
Coverage.py to fail if the percentage drops below 100% again:</p>

<pre data-type="programlisting" data-code-language="toml"><code class="k">[tool.coverage.report]</code><code class="w"/>
<code class="n">fail_under</code><code class="w"> </code><code class="o">=</code><code class="w"> </code><code class="mi">100</code><code class="w"/></pre>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Branch Coverage"><div class="sect1" id="section_coverage_branch">
<h1>Branch Coverage</h1>

<p>If an article has an empty summary, <code>random-wikipedia-article</code> prints a trailing
blank line (yikes). Those empty summaries are rare, but they exist, and this
should be a quick fix. <a data-type="xref" href="#example_coverage_show">Example 7-1</a> modifies <code>show</code> to print only
non-empty summaries.</p>
<div id="example_coverage_show" data-type="example">
<h5><span class="label">Example 7-1. </span>Printing only non-empty summaries in <code>show</code></h5>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">show</code><code class="p">(</code><code class="n">article</code><code class="p">,</code> <code class="n">file</code><code class="p">):</code>
    <code class="n">console</code> <code class="o">=</code> <code class="n">Console</code><code class="p">(</code><code class="n">file</code><code class="o">=</code><code class="n">file</code><code class="p">,</code> <code class="n">width</code><code class="o">=</code><code class="mi">72</code><code class="p">,</code> <code class="n">highlight</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code>
    <code class="n">console</code><code class="o">.</code><code class="n">print</code><code class="p">(</code><code class="n">article</code><code class="o">.</code><code class="n">title</code><code class="p">,</code> <code class="n">style</code><code class="o">=</code><code class="s2">"bold"</code><code class="p">,</code> <code class="n">end</code><code class="o">=</code><code class="s2">"</code><code class="se">\n\n</code><code class="s2">"</code><code class="p">)</code>
    <code class="k">if</code> <code class="n">article</code><code class="o">.</code><code class="n">summary</code><code class="p">:</code>
        <code class="n">console</code><code class="o">.</code><code class="n">print</code><code class="p">(</code><code class="n">article</code><code class="o">.</code><code class="n">summary</code><code class="p">)</code></pre></div>

<p>Curiously, the coverage stays at 100%—even though you didn’t write a test first.</p>

<p>By default, Coverage.py measures <em>statement coverage</em>—the percentage of
statements in your modules that the interpreter executed during the tests. If
the summary isn’t empty, every statement in the function gets executed.</p>

<p>On the other hand, the tests only exercised one of two code paths through the
function—​they never skipped the <code>if</code> body. Coverage.py also supports <em>branch
coverage</em>, which looks at all the transitions between statements in your code
and measures the percentage of those traversed during the tests. You should
always enable it, as it’s more precise than statement coverage:</p>

<pre data-type="programlisting" data-code-language="toml"><code class="k">[tool.coverage.run]</code><code class="w"/>
<code class="n">branch</code><code class="w"> </code><code class="o">=</code><code class="w"> </code><code class="kc">true</code><code class="w"/></pre>

<p>Re-run the tests, and you’ll see Coverage.py flag the missing transition from
the <code>if</code> statement on line 34 to the exit of the function:</p>
<pre data-type="programlisting">$ <strong>py -m coverage run -m pytest</strong>
$ <strong>py -m coverage report</strong>
Name                  Stmts   Miss Branch BrPart  Cover   Missing
-----------------------------------------------------------------
src/.../__init__.py      24      0      6      1    97%   34-&gt;exit
tests/__init__.py         0      0      0      0   100%
tests/test_main.py       33      0      6      0   100%
-----------------------------------------------------------------
TOTAL                    57      0     12      1    99%
Coverage failure: total of 99 is less than fail-under=100
</pre>

<p><a data-type="xref" href="#example_coverage_branch">Example 7-2</a> brings coverage back to 100%. It includes an article
with an empty summary and adds the missing test for trailing blank lines.</p>
<div id="example_coverage_branch" data-type="example">
<h5><span class="label">Example 7-2. </span>Testing articles with empty summaries</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">article</code> <code class="o">=</code> <code class="n">parametrized_fixture</code><code class="p">(</code>
    <code class="n">Article</code><code class="p">(</code><code class="s2">"test"</code><code class="p">),</code> <code class="o">*</code><code class="n">ArticleFactory</code><code class="o">.</code><code class="n">build_batch</code><code class="p">(</code><code class="mi">10</code><code class="p">)</code>
<code class="p">)</code>

<code class="k">def</code> <code class="nf">test_trailing_blank_lines</code><code class="p">(</code><code class="n">article</code><code class="p">,</code> <code class="n">file</code><code class="p">):</code>
    <code class="n">show</code><code class="p">(</code><code class="n">article</code><code class="p">,</code> <code class="n">file</code><code class="p">)</code>
    <code class="k">assert</code> <code class="ow">not</code> <code class="n">file</code><code class="o">.</code><code class="n">getvalue</code><code class="p">()</code><code class="o">.</code><code class="n">endswith</code><code class="p">(</code><code class="s2">"</code><code class="se">\n\n</code><code class="s2">"</code><code class="p">)</code></pre></div>

<p>Run the tests again—​and they fail! Can you spot the bug in
<a data-type="xref" href="#example_coverage_show">Example 7-1</a>?</p>

<p>Empty summaries produce two blank lines: one to separate the title and the
summary, and one from printing the empty summary. You’ve only removed the
second one. <a data-type="xref" href="#example_coverage_show_fixed">Example 7-3</a> removes the first one as well.
Thanks, Coverage.py!</p>
<div id="example_coverage_show_fixed" data-type="example">
<h5><span class="label">Example 7-3. </span>Avoiding both trailing blank lines in <code>show</code></h5>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">show</code><code class="p">(</code><code class="n">article</code><code class="p">,</code> <code class="n">file</code><code class="p">):</code>
    <code class="n">console</code> <code class="o">=</code> <code class="n">Console</code><code class="p">(</code><code class="n">file</code><code class="o">=</code><code class="n">file</code><code class="p">,</code> <code class="n">width</code><code class="o">=</code><code class="mi">72</code><code class="p">,</code> <code class="n">highlight</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code>
    <code class="n">console</code><code class="o">.</code><code class="n">print</code><code class="p">(</code><code class="n">article</code><code class="o">.</code><code class="n">title</code><code class="p">,</code> <code class="n">style</code><code class="o">=</code><code class="s2">"bold"</code><code class="p">)</code>
    <code class="k">if</code> <code class="n">article</code><code class="o">.</code><code class="n">summary</code><code class="p">:</code>
        <code class="n">console</code><code class="o">.</code><code class="n">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"</code><code class="se">\n</code><code class="si">{</code><code class="n">article</code><code class="o">.</code><code class="n">summary</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code></pre></div>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Testing in Multiple Environments"><div class="sect1" id="section_coverage_multiple_environments">
<h1>Testing in Multiple Environments</h1>

<p>You will often need to support a variety of Python versions. Python releases
come out every year, while long term support (LTS) distros can reach back a
decade into Python’s history. End-of-life Python versions can have a surprising
afterlife—​distributors may provide security patches years after the core Python
team ends support.</p>

<p>Let’s update <code>random-wikipedia-article</code> to support Python 3.7, which reached its
end of life in June 2023. I’m assuming your project requires Python 3.10, with
lower bounds on all dependencies. First, relax the Python requirement in
<em>pyproject.toml</em>:</p>

<pre data-type="programlisting" data-code-language="toml"><code class="k">[project]</code><code class="w"/>
<code class="n">requires-python</code><code class="w"> </code><code class="o">=</code><code class="w"> </code><code class="s2">"&gt;=3.7"</code><code class="w"/></pre>

<p>Next, check if your dependencies are compatible with the Python version. Use
<code>uv</code> to compile a separate requirements file for a Python 3.7 environment:</p>
<pre data-type="programlisting">$ <strong>uv venv -p 3.7</strong>
$ <strong>uv pip compile --extra=tests pyproject.toml -o py37-dev-requirements.txt</strong>
  × No solution found when resolving dependencies: ...
</pre>

<p>The error indicates that your preferred version of HTTPX has already dropped
Python 3.7. Remove your lower version bound and try again. After a few similar
errors and removing the lower bounds of other packages, dependency resolution
finally succeeds. Restore the lower bounds using the older versions of these
packages.</p>

<p>You’ll also need the backport <code>importlib-metadata</code> (see
<a data-type="xref" href="ch04.html#section_dependencies_environment_markers">“Environment Markers”</a>). Add the following entry to
the <code>project.dependencies</code> field:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">importlib</code><code class="o">-</code><code class="n">metadata</code><code class="o">&gt;=</code><code class="mf">6.7.0</code><code class="p">;</code> <code class="n">python_version</code> <code class="o">&lt;</code> <code class="s1">'3.8'</code></pre>

<p>Update the <em>__init__.py</em> module to fall back to the backport:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">if</code> <code class="n">sys</code><code class="o">.</code><code class="n">version_info</code> <code class="o">&gt;=</code> <code class="p">(</code><code class="mi">3</code><code class="p">,</code> <code class="mi">8</code><code class="p">):</code>
    <code class="kn">from</code> <code class="nn">importlib.metadata</code> <code class="kn">import</code> <code class="n">metadata</code>
<code class="k">else</code><code class="p">:</code>
    <code class="kn">from</code> <code class="nn">importlib_metadata</code> <code class="kn">import</code> <code class="n">metadata</code></pre>

<p>Compile the requirements one more time. Finally, update your project environment:</p>
<pre data-type="programlisting">$ <strong>uv pip sync py37-dev-requirements.txt</strong>
$ <strong>uv pip install -e . --no-deps</strong></pre>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Parallel Coverage"><div class="sect1" id="section_coverage_parallel">
<h1>Parallel Coverage</h1>

<p>If you now re-run Coverage.py under Python 3.7, it reports the first branch of
the <code>if</code> statement as missing. This makes sense: your code executes the <code>else</code>
branch and imports the backport instead of the standard library.</p>

<p>It may be tempting to exclude this line from coverage measurements—​but don’t.
Third-party dependencies like the backport can break your code, too. Instead,
collect coverage data from both environments.</p>

<p>First, switch the environment back to Python 3.12 using your original
requirements:</p>
<pre data-type="programlisting">$ <strong>uv venv -p 3.12</strong>
$ <strong>uv pip sync dev-requirements.txt</strong>
$ <strong>uv pip install -e . --no-deps</strong>
</pre>

<p>By default, <code>coverage run</code> overwrites any existing coverage data—​but you can
tell it to append the data instead. Re-run Coverage.py with the <code>--append</code>
option and confirm you have full test coverage:</p>
<pre data-type="programlisting">$ <strong>py -m coverage run --append -m pytest</strong>
$ <strong>py -m coverage report</strong>
</pre>

<p>With a single file for coverage data, it’s easy to erase data accidentally. If
you forget to pass the <code>--append</code> option, you’ll have to run the tests again.
You could configure Coverage.py to append by default, but that’s error-prone,
too: If you forget to run <code>coverage erase</code> periodically, you’ll end up with
stale data in your report.</p>

<p>There’s a better way to gather coverage across multiple environments.
Coverage.py lets you record coverage data in separate files on each run. Enable
this behavior with the <code>parallel</code> setting:<sup><a data-type="noteref" id="id300-marker" href="ch07.html#id300">3</a></sup></p>

<pre data-type="programlisting" data-code-language="toml"><code class="k">[tool.coverage.run]</code><code class="w"/>
<code class="n">parallel</code><code class="w"> </code><code class="o">=</code><code class="w"> </code><code class="kc">true</code><code class="w"/></pre>

<p>Coverage reports are always based on a single data file, even in parallel mode.
You merge the data files using the command <code>coverage combine</code>. That turns the
two-step process from earlier into a three-step one: <code>coverage run</code> — <code>coverage
combine</code> — <code>coverage report</code>.</p>

<p>Let’s put all of this together. For each Python version, set up the environment
and run the tests, as shown here for Python 3.7:</p>
<pre data-type="programlisting">$ <strong>uv venv -p 3.7</strong>
$ <strong>uv pip sync py37-dev-requirements.txt</strong>
$ <strong>uv pip install -e . --no-deps</strong>
$ <strong>py -m coverage run -m pytest</strong>
</pre>

<p>At this point, you’ll have multiple <em>.coverage.*</em> files in your project.
Aggregate them into a single <em>.coverage</em> file using the command <code>coverage
combine</code>:</p>
<pre data-type="programlisting">$ <strong>py -m run coverage combine</strong>
Combined data file .coverage.somehost.26719.001909
Combined data file .coverage.somehost.26766.146311
</pre>

<p>Finally, produce the coverage report with <code>coverage report</code>:</p>
<pre data-type="programlisting">$ <strong>py -m coverage report</strong></pre>

<p>Does it sound <em>incredibly</em> tedious to gather coverage like this? In
<a data-type="xref" href="ch08.html#chapter_nox">Chapter 8</a>, you’ll learn how to automate testing across multiple Python
environments. You’ll run this entire process using a single three-letter
command.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Measuring in Subprocesses"><div class="sect1" id="section_coverage_subprocess">
<h1>Measuring in Subprocesses</h1>

<p>At the end of <a data-type="xref" href="#section_coverage_coverage">“Using Coverage.py”</a>, you had to disable coverage for the
<code>main</code> function and the <code>__main__</code> module. But the end-to-end test certainly
exercises this code. Let’s remove the <code># pragma</code> comment and the <code>omit</code> setting
and figure this out.</p>

<p>Think about how Coverage.py registers a trace function that records executed
lines. Maybe you can already guess what’s going on here: The end-to-end test
runs your program in a separate process. Coverage.py never registered its trace
function on the interpreter in that process. None of those executed lines were
recorded anywhere.</p>

<p>Coverage.py provides a public API to enable tracing in the current process: the
<code>coverage.process_startup</code> function. You could call the function when your
application starts up. But there must be a better way—​you shouldn’t have to
modify your code to support code coverage.</p>

<p>It turns out you don’t need to. You can place a <em>.pth</em> file in the environment
that calls the function during interpreter startup. This leverages a
little-known Python feature (see <a data-type="xref" href="ch02.html#section_environments_site_packages">“Site Packages”</a>): The
interpreter executes lines in a <em>.pth</em> file if they start with an <code>import</code>
statement.</p>

<p>Install a <em>_coverage.pth</em> file into the <em>site-packages</em> directory of your
environment, with the following contents:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">coverage</code><code class="p">;</code> <code class="n">coverage</code><code class="o">.</code><code class="n">process_startup</code><code class="p">()</code></pre>

<p>You can find the <em>site-packages</em> directory under <em>lib/python3.x</em> on Linux and
macOS, and under <em>Lib</em> on Windows.</p>

<p>Additionally, you need to set the environment variable <code>COVERAGE_PROCESS_START</code>.
On Linux and macOS, use this syntax:</p>
<pre data-type="programlisting">$ <strong>export COVERAGE_PROCESS_START=pyproject.toml</strong>
</pre>

<p>On Windows, use the following syntax instead:</p>
<pre data-type="programlisting">&gt; <strong>$env:COVERAGE_PROCESS_START = 'pyproject.toml'</strong>
</pre>

<p>Re-run the test suite, combine the data files, and display the coverage report.
Thanks to measuring coverage in the subprocess, the program should have full
coverage again.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Measuring coverage in subprocesses only works in parallel mode. Without parallel
mode, the main process overwrites the coverage data from the subprocess, because
both use the same data file.</p>
</div>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id301">
<h1>The <code>pytest-cov</code> plugin</h1>
<p>The pytest plugin <code>pytest-cov</code> enables coverage measurements with Coverage.py
when you run pytest. Add the plugin to your test dependencies and run pytest
with the <code>--cov</code> option to enable it. You can still configure Coverage.py in
<em>pyproject.toml</em>. Alternatively, the plugin exposes <code>--cov-*</code> options for
various configuration settings.</p>

<p>The plugin aims to make everything work out of the box, behind the
scenes—​including subprocess coverage. The convenience comes at the price of a
layer of indirection. Running <code>coverage</code> directly provides finer-grained
control.</p>
</div></aside>
</div></section>






<section data-type="sect1" data-pdf-bookmark="What Coverage to Aim For"><div class="sect1" id="section_coverage_target">
<h1>What Coverage to Aim For</h1>

<p>Any coverage percentage below 100% means your tests won’t detect bugs in some
parts of your codebase. If you’re working on a new project, there isn’t any
other meaningful coverage target.</p>

<p>That doesn’t imply you should test every single line of code. Consider a log
statement for debugging a rare situation. The statement may be difficult to
exercise from a test. At the same time, it’s probably low-risk, trivial code.
Writing that test won’t increase your confidence in the code significantly.
Exclude the line from coverage using a <em>pragma</em> comment:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">if</code> <code class="n">rare_condition</code><code class="p">:</code>
    <code class="nb">print</code><code class="p">(</code><code class="s2">"got rare condition"</code><code class="p">)</code>  <code class="c1"># pragma: no cover</code></pre>

<p>Don’t exclude code from coverage just because it’s cumbersome to test. When you
start working with a new library or interfacing with a new system, it usually
takes some time to figure out how to test your code. But often those tests end
up detecting bugs that would have gone unnoticed and caused problems in
production.</p>

<p>Legacy projects often consist of a large codebase with minimal test coverage. As
a general rule, coverage in such projects should increase <em>monotonically</em>—no
individual change should lead to a drop in coverage.</p>

<p>You’ll often find yourself in a dilemma here: To test, you need to refactor the
code, but refactoring is too risky without tests. Find the minimal safe
refactoring to increase testability. Often, this consists of breaking a
dependency of the code under test.<sup><a data-type="noteref" id="id302-marker" href="ch07.html#id302">4</a></sup></p>

<p>For example, you may be testing a large function that, among other things, also
connects to the production database. Add an optional parameter that lets you
pass the connection from the outside. Tests can then pass a connection to an
in-memory database instead.</p>

<p><a data-type="xref" href="#example_coverage_config">Example 7-4</a> recaps the Coverage.py settings you’ve used in this
chapter.</p>
<div id="example_coverage_config" data-type="example">
<h5><span class="label">Example 7-4. </span>Configuring Coverage.py in <em>pyproject.toml</em></h5>

<pre data-type="programlisting" data-code-language="toml"><code class="k">[tool.coverage.run]</code><code class="w"/>
<code class="n">source</code><code class="w"> </code><code class="o">=</code><code class="w"> </code><code class="p">[</code><code class="s2">"random_wikipedia_article"</code><code class="p">,</code><code class="w"> </code><code class="s2">"tests"</code><code class="p">]</code><code class="w"/>
<code class="n">branch</code><code class="w"> </code><code class="o">=</code><code class="w"> </code><code class="kc">true</code><code class="w"/>
<code class="n">parallel</code><code class="w"> </code><code class="o">=</code><code class="w"> </code><code class="kc">true</code><code class="w"/>
<code class="n">omit</code><code class="w"> </code><code class="o">=</code><code class="w"> </code><code class="p">[</code><code class="s2">"*/__main__.py"</code><code class="p">]</code><code class="w">  </code><code class="c1"># avoid this if you can</code><code class="w"/>

<code class="k">[tool.coverage.report]</code><code class="w"/>
<code class="n">show_missing</code><code class="w"> </code><code class="o">=</code><code class="w"> </code><code class="kc">true</code><code class="w"/>
<code class="n">fail_under</code><code class="w"> </code><code class="o">=</code><code class="w"> </code><code class="mi">100</code><code class="w"/></pre></div>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="id173">
<h1>Summary</h1>

<p>You can measure the extent to which the test suite exercises your project using
Coverage.py. Coverage reports are useful for discovering untested lines. Branch
coverage captures the control flow of your program, instead of isolated lines of
source code. Parallel coverage lets you measure coverage across multiple
environments. You need to combine the data files before reporting. Measuring
coverage in subprocesses requires setting up a <em>.pth</em> file and an environment
variable.</p>

<p>Measuring test coverage effectively for a project requires some amount of
configuration (), as well as the right tool
incantations. In the next chapter, you’ll see how you can automate these steps
with Nox. You’ll set up checks that give you confidence in your changes, while
staying out of your way.</p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="id298"><sup><a href="ch07.html#id298-marker">1</a></sup> Ned Batchelder: <a href="https://nedbatchelder.com/blog/202008/you_should_include_your_tests_in_coverage.html">“You should include your tests in coverage,”</a> August 11, 2020.</p><p data-type="footnote" id="id299"><sup><a href="ch07.html#id299-marker">2</a></sup> Under the hood, the <em>.coverage</em> file is just a SQLite database. Feel free to poke around if you have the <code>sqlite3</code> command-line utility ready on your system.</p><p data-type="footnote" id="id300"><sup><a href="ch07.html#id300-marker">3</a></sup> The name <code>parallel</code> is somewhat misleading; the setting has nothing to do with parallel execution.</p><p data-type="footnote" id="id302"><sup><a href="ch07.html#id302-marker">4</a></sup> Martin Fowler: <a href="https://www.martinfowler.com/bliki/LegacySeam.html">“Legacy Seam,”</a> January 4, 2024.</p></div></div></section></div>
</div>
</body></html>