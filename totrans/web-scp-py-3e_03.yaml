- en: Chapter 2\. The Legalities and Ethics of Web Scraping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In 2010, software engineer Pete Warden built a web crawler to gather data from
    Facebook. He collected data from approximately 200 million Facebook users—names,
    location information, friends, and interests. Of course, Facebook noticed and
    sent him cease and desist letters, which he obeyed. When asked why he complied
    with the cease and desist, he said: “Big data? Cheap. Lawyers? Not so cheap.”'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you’ll look at US laws (and some international ones) that are
    relevant to web scraping and learn how to analyze the legality and ethics of a
    given web scraping situation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before you read the following section, consider the obvious: I am a software
    engineer, not a lawyer. Do not interpret anything you read here or in any other
    chapter of the book as professional legal advice or act on it accordingly. Although
    I believe I’m able to discuss the legalities and ethics of web scraping knowledgeably,
    you should consult a lawyer (not a software engineer) before undertaking any legally
    ambiguous web scraping projects.'
  prefs: []
  type: TYPE_NORMAL
- en: The goal of this chapter is to provide you with a framework for being able to
    understand and discuss various aspects of web scraping legalities, such as intellectual
    property, unauthorized computer access, and server usage, but this should not
    be a substitute for actual legal advice.
  prefs: []
  type: TYPE_NORMAL
- en: Trademarks, Copyrights, Patents, Oh My!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It’s time for a crash course in intellectual property! There are three basic
    types of intellectual property: trademarks (indicated by a ™ or ® symbol), copyrights
    (the ubiquitous ©), and patents (sometimes indicated by text noting that the invention
    is patent protected or a patent number but often by nothing at all).'
  prefs: []
  type: TYPE_NORMAL
- en: Patents are used to declare ownership over inventions only. You cannot patent
    images, text, or any information itself. Although some patents, such as software
    patents, are less tangible than what we think of as “inventions,” keep in mind
    that it is the *thing* (or technique) that is patented—not the data that comprises
    the software. Unless you are either building things from scraped diagrams, or
    someone patents a method of web scraping, you are unlikely to inadvertently infringe
    on a patent by scraping the web.
  prefs: []
  type: TYPE_NORMAL
- en: 'Trademarks also are unlikely to be an issue but still something that must be
    considered. According to the US Patent and Trademark Office:'
  prefs: []
  type: TYPE_NORMAL
- en: A *trademark* is a word, phrase, symbol, and/or design that identifies and distinguishes
    the source of the goods of one party from those of others. A *service mark* is
    a word, phrase, symbol, and/or design that identifies and distinguishes the source
    of a service rather than goods. The term “trademark” is often used to refer to
    both trademarks and service marks.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In addition to the words and symbols that come to mind when you think of trademarks,
    other descriptive attributes can be trademarked. This includes, for example, the
    shape of a container (like Coca-Cola bottles) or even a color (most notably, the
    pink color of Owens Corning’s Pink Panther fiberglass insulation).
  prefs: []
  type: TYPE_NORMAL
- en: Unlike with patents, the ownership of a trademark depends heavily on the context
    in which it is used. For example, if I wish to publish a blog post with an accompanying
    picture of the Coca-Cola logo, I could do that, as long as I wasn’t implying that
    my blog post was sponsored by, or published by, Coca-Cola. If I wanted to manufacture
    a new soft drink with the same Coca-Cola logo displayed on the packaging, that
    would clearly be a trademark infringement. Similarly, although I could package
    my new soft drink in Pink Panther pink, I could not use that same color to create
    a home insulation product.
  prefs: []
  type: TYPE_NORMAL
- en: This brings us to the topic of “fair use,” which is often discussed in the context
    of copyright law but also applies to trademarks. Storing or displaying a trademark
    as a reference to the brand it represents is fine. Using a trademark in a way
    that might mislead the consumer is not. The concept of “fair use” does not apply
    to patents, however. For example, a patented invention in one industry cannot
    be applied to another industry without an agreement with the patent holder.
  prefs: []
  type: TYPE_NORMAL
- en: Copyright Law
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Both trademarks and patents have something in common in that they have to be
    formally registered in order to be recognized. Contrary to popular belief, this
    is not true with copyrighted material. What makes images, text, music, etc., copyrighted?
    It’s not the All Rights Reserved warning at the bottom of the page or anything
    special about “published” versus “unpublished” material. Every piece of material
    you create is automatically subject to copyright law as soon as you bring it into
    existence.
  prefs: []
  type: TYPE_NORMAL
- en: The Berne Convention for the Protection of Literary and Artistic Works, named
    after Berne, Switzerland, where it was first adopted in 1886, is the international
    standard for copyright. This convention says, in essence, that all member countries
    must recognize the copyright protection of the works of citizens of other member
    countries as if they were citizens of their own country. In practice, this means
    that, as a US citizen, you can be held accountable in the United States for violating
    the copyright of material written by someone in, say, France (and vice versa).
  prefs: []
  type: TYPE_NORMAL
- en: Copyright Registration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While it’s true that copyright protections apply automatically and do not require
    any sort of registration, it is also possible to formally register a copyright
    with the US government. This is often done for valuable creative works, such as
    major motion pictures, in order to make any litigation easier later on and create
    a strong paper trail about who owns the work. However, do not let the existence
    of this copyright registration confuse you—all creative works, unless specifically
    part of the public domain, are copyrighted!
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, copyright is more of a concern for web scrapers than trademarks or
    patents. If I scrape content from someone’s blog and publish it on my own blog,
    I could very well be opening myself up to a lawsuit. Fortunately, I have several
    layers of protection that might make my blog-scraping project defensible, depending
    on how it functions.
  prefs: []
  type: TYPE_NORMAL
- en: First, copyright protection extends to creative works only. It does not cover
    statistics or facts. Fortunately, much of what web scrapers are after *are* statistics
    and facts.
  prefs: []
  type: TYPE_NORMAL
- en: A web scraper that gathers poetry from around the web and displays that poetry
    on your own website might be violating copyright law; however, a web scraper that
    gathers information on the frequency of poetry postings over time is not. The
    poetry, in its raw form, is a creative work. The average word count of poems published
    on a website by month is factual data and not a creative work.
  prefs: []
  type: TYPE_NORMAL
- en: Content that is posted verbatim (as opposed to aggregated/calculated content
    from raw scraped data) might not be violating copyright law if that data is prices,
    names of company executives, or some other factual piece of information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Even copyrighted content can be used directly, within reason, under the Digital
    Millennium Copyright Act of 1988\. The DMCA outlines some rules for the automated
    handling of copyrighted material. The DMCA is long, with many specific rules governing
    everything from ebooks to telephones. However, two main points may be of particular
    relevance to web scraping:'
  prefs: []
  type: TYPE_NORMAL
- en: Under the “safe harbor” protection, if you scrape material from a source that
    you are led to believe contains only copyright-free material, but a user has submitted
    copyright material to, you are protected as long as you removed the copyrighted
    material when notified.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You cannot circumvent security measures (such as password protection) in order
    to gather content.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, the DMCA also acknowledges that fair use under 17 U.S. Code § 107
    applies, and that take-down notices may not be issued according to the safe harbor
    protection if the use of the copyrighted material falls under fair use.
  prefs: []
  type: TYPE_NORMAL
- en: In short, you should never directly publish copyrighted material without permission
    from the original author or copyright holder. If you are storing copyrighted material
    that you have free access to in your own nonpublic database for the purposes of
    analysis, that is fine. If you are publishing that database to your website for
    viewing or download, that is not fine. If you are analyzing that database and
    publishing statistics about word counts, a list of authors by prolificacy, or
    some other meta-analysis of the data, that is fine. If you are accompanying that
    meta-analysis with a few select quotes, or brief samples of data to make your
    point, that is likely also fine, but you might want to examine the fair-use clause
    in the US Code to make sure.
  prefs: []
  type: TYPE_NORMAL
- en: Copyright and artificial intelligence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Generative artificial intelligence, or AI programs that generate new “creative”
    works based on a corpus of existing creative works, present unique challenges
    for copyright law.
  prefs: []
  type: TYPE_NORMAL
- en: If the output of the generative AI program resembles an existing work, there
    may be a copyright issue. Many cases have been used as precedent to guide what
    the word “resembles” means here, but, according to the Congressional Research
    Service:^([1](ch02.html#id308))
  prefs: []
  type: TYPE_NORMAL
- en: The substantial similarity test is difficult to define and varies across U.S.
    courts. Courts have variously described the test as requiring, for example, that
    the works have “a substantially similar total concept and feel” or “overall look
    and feel” or that “the ordinary reasonable person would fail to differentiate
    between the two works.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The problem with modern complex algorithms is that it can be  impossible to
    automatically determine if your AI has produced an exciting and novel mash-up
    or something more...directly derivative. The AI may have no way of labeling its
    output as “substantially similar” to a particular input, or even identifying which
    of the inputs it used to generate its creation at all! The first indication that
    anything is wrong at all may come in the form of a cease and desist letter or
    a court summons.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond the issues of copyright infringement over the output of generative AI,
    upcoming court cases are testing whether the training process itself might infringe
    on a copyright holder’s rights.
  prefs: []
  type: TYPE_NORMAL
- en: To train these systems, it is almost always necessary to download, store, and
    reproduce the copyrighted work. While it might not seem like a big deal to download
    a copyrighted image or text, this isn’t much different from downloading a copyrighted
    movie—and you wouldn’t download a movie, would you?
  prefs: []
  type: TYPE_NORMAL
- en: Some claim that this constitutes fair use, and they are not publishing or using
    the content in a way that would impact its market.
  prefs: []
  type: TYPE_NORMAL
- en: As of this writing, OpenAI is arguing before the United States Patent and Trademark
    Office that its use of large volumes of copyrighted material constitutes fair
    use.^([2](ch02.html#id309)) While this argument is primarily in the context of
    AI generative algorithms, I suspect that its outcome will be applicable to web
    scrapers built for a variety of purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Trespass to Chattels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Trespass to chattels* is fundamentally different from what we think of as
    “trespassing laws” in that it applies not to real estate or land but to movable
    property, or *chattels* in legal parlance. It applies when your property is interfered
    with in some way that does not allow you to access or use it.'
  prefs: []
  type: TYPE_NORMAL
- en: In this era of cloud computing, it’s tempting not to think of web servers as
    real, tangible resources. However, not only do servers consist of expensive components,
    but they also need to be stored, monitored, cooled, cleaned, and supplied with
    vast amounts of electricity. By some estimates, 10% of global electricity usage
    is consumed by computers.^([3](ch02.html#id313)) If a survey of your own electronics
    doesn’t convince you, consider Google’s vast server farms, all of which need to
    be connected to large power stations.
  prefs: []
  type: TYPE_NORMAL
- en: Although servers are expensive resources, they’re interesting from a legal perspective
    in that webmasters generally *want* people to consume their resources (i.e., access
    their websites); they just don’t want them to consume their resources *too much.*
    Checking out a website via your browser is fine; launching a full-scale Distributed
    Denial of Service (DDOS) attack against it obviously is not.
  prefs: []
  type: TYPE_NORMAL
- en: 'Three criteria need to be met for a web scraper to violate trespass to chattels:'
  prefs: []
  type: TYPE_NORMAL
- en: Lack of consent
  prefs: []
  type: TYPE_NORMAL
- en: Because web servers are open to everyone, they are generally “giving consent”
    to web scrapers as well. However, many websites’ Terms of Service agreements specifically
    prohibit the use of scrapers. In addition, any cease and desist notices delivered
    to you may revoke this consent.
  prefs: []
  type: TYPE_NORMAL
- en: Actual harm
  prefs: []
  type: TYPE_NORMAL
- en: Servers are costly. In addition to server costs, if your scrapers take a website
    down, or limit its ability to serve other users, this can add to the “harm” you
    cause.
  prefs: []
  type: TYPE_NORMAL
- en: Intentionality
  prefs: []
  type: TYPE_NORMAL
- en: If you’re writing the code, you know what it does! Arguing a lack of intention
    would likely not go well when defending your web scraper.
  prefs: []
  type: TYPE_NORMAL
- en: You must meet all three of these criteria for trespass to chattels to apply.
    However, if you are violating a Terms of Service agreement, but not causing actual
    harm, don’t think that you’re immune from legal action. You might very well be
    violating copyright law, the DMCA, the Computer Fraud and Abuse Act (more on that
    later in this chapter), or one of the other myriad of laws that apply to web scrapers.
  prefs: []
  type: TYPE_NORMAL
- en: The Computer Fraud and Abuse Act
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the early 1980s, computers started moving out of academia and into the business
    world. For the first time, viruses and worms were seen as more than an inconvenience
    (or even a fun hobby) and as a serious criminal matter that could cause monetary
    damages. In 1983, the movie *War Games*, starring Matthew Broderick, also brought
    this issue to the public eye and to the eye of President Ronald Reagan.^([4](ch02.html#id319))
    In response, the Computer Fraud and Abuse Act (CFAA) was created in 1986.
  prefs: []
  type: TYPE_NORMAL
- en: Although you might think that the CFAA applies to only a stereotypical version
    of a malicious hacker unleashing viruses, the act has strong implications for
    web scrapers as well. Imagine a scraper that scans the web looking for login forms
    with easy-to-guess passwords, or collects government secrets accidentally left
    in a hidden but public location. All of these activities are illegal (and rightly
    so) under the CFAA.
  prefs: []
  type: TYPE_NORMAL
- en: 'The act defines seven main criminal offenses, which can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The knowing unauthorized access of computers owned by the US government and
    obtaining information from those computers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The knowing unauthorized access of a computer, obtaining financial information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The knowing unauthorized access of a computer owned by the US government, affecting
    the use of that computer by the government.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knowingly accessing any protected computer with the attempt to defraud.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knowingly accessing a computer without authorization and causing damage to that
    computer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharing or trafficking passwords or authorization information for computers
    used by the US government or computers that affect interstate or foreign commerce.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attempts to extort money or “anything of value” by causing damage, or threatening
    to cause damage, to any protected computer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In short: stay away from protected computers, do not access computers (including
    web servers) that you are not given access to, and especially, stay away from
    government or financial computers.'
  prefs: []
  type: TYPE_NORMAL
- en: robots.txt and Terms of Service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A website’s terms of service and *robots.txt* files are in interesting territory,
    legally speaking. If a website is publicly accessible, the webmaster’s right to
    declare what software can and cannot access it is debatable. Saying that “it is
    fine if you use your browser to view this site, but not if you use a program you
    wrote to view it” is tricky.
  prefs: []
  type: TYPE_NORMAL
- en: Most sites have a link to their Terms of Service (TOS) in the footer on every
    page. The TOS contains more than just the rules for web crawlers and automated
    access; it often has information about what kind of information the website collects,
    what it does with it, and usually a legal disclaimer that the services provided
    by the website come without any express or implied warranty.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are interested in search engine optimization (SEO) or search engine
    technology, you’ve probably heard of the *robots.txt* file. If you go to just
    about any large website and look for its *robots.txt* file, you will find it in
    the root web folder: *http://website.com/robots.txt*.'
  prefs: []
  type: TYPE_NORMAL
- en: The syntax for *robots.txt* files was developed in 1994 during the initial boom
    of web search engine technology. It was about this time that search engines scouring
    the entire internet, such as AltaVista and DogPile, started competing in earnest
    with simple lists of sites organized by subject, such as the one curated by Yahoo!
    This growth of search across the internet meant an explosion not only in the number
    of web crawlers but also in the availability of information collected by those
    web crawlers to the average citizen.
  prefs: []
  type: TYPE_NORMAL
- en: While we might take this sort of availability for granted today, some webmasters
    were shocked when information they published deep in the file structure of their
    website became available on the front page of search results in major search engines.
    In response, the syntax for *robots.txt* files, called the Robots Exclusion Protocol,
    was developed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike the terms of service, which often talks about web crawlers in broad
    terms and in very human language, *robots.txt* can be parsed and used by automated
    programs extremely easily. Although it might seem like the perfect system to solve
    the problem of unwanted bots once and for all, keep in mind that:'
  prefs: []
  type: TYPE_NORMAL
- en: There is no official governing body for the syntax of *robots.txt*. It is a
    commonly used and generally well-followed convention, but there is nothing to
    stop anyone from creating their own version of a *robots.txt* file (apart from
    the fact that no bot will recognize or obey it until it gets popular). That being
    said, it is a widely accepted convention, mostly because it is relatively straightforward,
    and there is no incentive for companies to invent their own standard or try to
    improve on it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no way to legally or technically enforce a *robots.txt* file. It is
    merely a sign that says “Please don’t go to these parts of the site.” Many web
    scraping libraries are available that obey *robots.txt*—although this is usually
    a default setting that can be overridden. Library defaults aside, writing a web
    crawler that obeys *robots.txt* is actually more technically challenging than
    writing one that ignores it altogether. After all, you need to read, parse, and
    apply the contents of *robots.txt* to your code logic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Robot Exclusion Protocol syntax is fairly straightforward. As in Python
    (and many other languages), comments begin with a `#` symbol, end with a newline,
    and can be used anywhere in the file.
  prefs: []
  type: TYPE_NORMAL
- en: The first line of the file, apart from any comments, is started with `User-agent:`,
    which specifies the user to which of the following rules apply. This is followed
    by a set of rules, either `Allow:` or `Disallow:`, depending on whether the bot
    is allowed on that section of the site. An asterisk (*) indicates a wildcard and
    can be used to describe either a `User-agent` or a URL.
  prefs: []
  type: TYPE_NORMAL
- en: 'If a rule follows a rule that it seems to contradict, the last rule takes precedence.
    For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this case, all bots are disallowed from anywhere on the site, except for
    the Googlebot, which is allowed anywhere except for the */private* directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *robots.txt* file of Twitter (also branded as “X”) has explicit instructions
    for the bots of Google, Yahoo!, Yandex (a popular Russian search engine), Microsoft,
    and other bots or search engines not covered by any of the preceding categories.
    The Google section (which looks identical to the permissions allowed to all other
    categories of bots) looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Notice that Twitter restricts access to the portions of its site for which it
    has an API in place. Because Twitter has a well-regulated API (and one that it
    can make money off of by licensing), it is in the company’s best interest to disallow
    any “home-brewed APIs” that gather information by independently crawling its site.
  prefs: []
  type: TYPE_NORMAL
- en: Although a file telling your crawler where it can’t go might seem restrictive
    at first, it can be a blessing in disguise for web crawler development. If you
    find a *robots.txt* file that disallows crawling in a particular section of the
    site, the webmaster is saying, essentially, that they are fine with crawlers in
    all other sections of the site. After all, if they weren’t fine with it, they
    would have restricted access when they were writing *robots.txt* in the first
    place.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the section of Wikipedia’s *robots.txt* file that applies to general
    web scrapers (as opposed to search engines) is wonderfully permissive. It even
    goes as far as containing human-readable text to welcome bots (that’s us!) and
    blocks access to only a few pages, such as the login page, search page, and “random
    article” page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Whether you choose to write web crawlers that obey *robots.txt* is up to you,
    but I highly recommend it, particularly if you have crawlers that indiscriminately
    crawl the web.
  prefs: []
  type: TYPE_NORMAL
- en: Three Web Scrapers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Because web scraping is such a limitless field, there are a staggering number
    of ways to land yourself in legal hot water. This section presents three cases
    that touched on some form of law that generally applies to web scrapers, and how
    it was used in that case.
  prefs: []
  type: TYPE_NORMAL
- en: eBay v. Bidder’s Edge and Trespass to Chattels
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In 1997, the Beanie Baby market was booming, the tech sector was bubbling, and
    online auction houses were the hot new thing on the internet. A company called
    Bidder’s Edge formed and created a new kind of meta-auction site. Rather than
    force you to go from auction site to auction site, comparing prices, it would
    aggregate data from all current auctions for a specific product (say, a hot new
    Furby doll or a copy of *Spice World*) and point you to the site that had the
    lowest price.
  prefs: []
  type: TYPE_NORMAL
- en: Bidder’s Edge accomplished this with an army of web scrapers, constantly making
    requests to the web servers of the various auction sites to get price and product
    information. Of all the auction sites, eBay was the largest, and Bidder’s Edge
    hit eBay’s servers about 100,000 times a day. Even by today’s standards, this
    is a lot of traffic. According to eBay, this was 1.53% of its total internet traffic
    at the time, and it certainly wasn’t happy about it.
  prefs: []
  type: TYPE_NORMAL
- en: eBay sent Bidder’s Edge a cease and desist letter, coupled with an offer to
    license its data. However, negotiations for this licensing failed, and Bidder’s
    Edge continued to crawl eBay’s site.
  prefs: []
  type: TYPE_NORMAL
- en: eBay tried blocking IP addresses used by Bidder’s Edge, blocking 169 IP addresses—although
    Bidder’s Edge was able to get around this by using proxy servers (servers that
    forward requests on behalf of another machine but using the proxy server’s own
    IP address). As I’m sure you can imagine, this was a frustrating and unsustainable
    solution for both parties—Bidder’s Edge was constantly trying to find new proxy
    servers and buy new IP addresses while old ones were blocked, and eBay was forced
    to maintain large firewall lists (and adding computationally expensive IP address-comparing
    overhead to each packet check).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, in December 1999, eBay sued Bidder’s Edge under trespass to chattels.
  prefs: []
  type: TYPE_NORMAL
- en: Because eBay’s servers were real, tangible resources that it owned, and it didn’t
    appreciate Bidder’s Edge’s abnormal use of them, trespass to chattels seemed like
    the ideal law to use. In fact, in modern times, trespass to chattels goes hand
    in hand with web-scraping lawsuits and is most often thought of as an IT law.
  prefs: []
  type: TYPE_NORMAL
- en: 'The courts ruled that for eBay to win its case using trespass to chattels,
    eBay had to show two things:'
  prefs: []
  type: TYPE_NORMAL
- en: Bidder’s Edge knew it was explicitly disallowed from using eBay’s resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: eBay suffered financial loss as a result of Bidder’s Edge’s actions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Given the record of eBay’s cease and desist letters, coupled with IT records
    showing server usage and actual costs associated with the servers, this was relatively
    easy for eBay to do. Of course, no large court battles end easily: countersuits
    were filed, many lawyers were paid, and the matter was eventually settled out
    of court for an undisclosed sum in March 2001.'
  prefs: []
  type: TYPE_NORMAL
- en: So does this mean that any unauthorized use of another person’s server is automatically
    a violation of trespass to chattels? Not necessarily. Bidder’s Edge was an extreme
    case; it was using so many of eBay’s resources that the company had to buy additional
    servers, pay more for electricity, and perhaps hire additional personnel. Although
    the 1.53% increase might not seem like a lot, in large companies, it can add up
    to a significant amount.
  prefs: []
  type: TYPE_NORMAL
- en: 'In 2003, the California Supreme Court ruled on another case, Intel Corp versus
    Hamidi, in which a former Intel employee (Hamidi) sent emails Intel didn’t like,
    across Intel’s servers, to Intel employees. The court said:'
  prefs: []
  type: TYPE_NORMAL
- en: Intel’s claim fails not because e-mail transmitted through the internet enjoys
    unique immunity, but because the trespass to chattels tort—unlike the causes of
    action just mentioned—may not, in California, be proved without evidence of an
    injury to the plaintiff’s personal property or legal interest therein.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Essentially, Intel had failed to prove that the costs of transmitting the six
    emails sent by Hamidi to all employees (each one, interestingly enough, with an
    option to be removed from Hamidi’s mailing list—at least he was polite!) contributed
    to any financial injury for Intel. It did not deprive Intel of any property or
    use of its property.
  prefs: []
  type: TYPE_NORMAL
- en: United States v. Auernheimer and the Computer Fraud and Abuse Act
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If information is readily accessible on the internet to a human using a web
    browser, it’s unlikely that accessing the same exact information in an automated
    fashion would land you in hot water with the Feds. However, as easy as it can
    be for a sufficiently curious person to find a small security leak, that small
    security leak can quickly become a much larger and much more dangerous one when
    automated scrapers enter the picture.
  prefs: []
  type: TYPE_NORMAL
- en: 'In 2010, Andrew Auernheimer and Daniel Spitler noticed a nice feature of iPads:
    when you visited AT&T’s website on them, AT&T would redirect you to a URL containing
    your iPad’s unique ID number:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This page would contain a login form, with the email address of the user whose
    ID number was in the URL. This allowed users to gain access to their accounts
    simply by entering their password.
  prefs: []
  type: TYPE_NORMAL
- en: Although there were a large number of potential iPad ID numbers, it was possible,
    with a web scraper, to iterate through the possible numbers, gathering email addresses
    along the way. By providing users with this convenient login feature, AT&T, in
    essence, made its customer email addresses public to the web.
  prefs: []
  type: TYPE_NORMAL
- en: 'Auernheimer and Spitler created a scraper that collected 114,000 of these email
    addresses, among them the private email addresses of celebrities, CEOs, and government
    officials. Auernheimer (but not Spitler) then sent the list, and information about
    how it was obtained, to Gawker Media, which published the story (but not the list)
    under the headline: “Apple’s Worst Security Breach: 114,000 iPad Owners Exposed.”'
  prefs: []
  type: TYPE_NORMAL
- en: In June 2011, Auernheimer’s home was raided by the FBI in connection with the
    email address collection, although they ended up arresting him on drug charges.
    In November 2012, he was found guilty of identity fraud and conspiracy to access
    a computer without authorization and later sentenced to 41 months in federal prison
    and ordered to pay $73,000 in restitution.
  prefs: []
  type: TYPE_NORMAL
- en: 'His case caught the attention of civil rights lawyer Orin Kerr, who joined
    his legal team and appealed the case to the Third Circuit Court of Appeals. On
    April 11, 2014 (these legal processes can take quite a while), they made the argument:'
  prefs: []
  type: TYPE_NORMAL
- en: Auernheimer’s conviction on Count 1 must be overturned because visiting a publicly
    available website is not unauthorized access under the Computer Fraud and Abuse
    Act, 18 U.S.C. § 1030(a)(2)(C). AT&T chose not to employ passwords or any other
    protective measures to control access to the e-mail addresses of its customers.
    It is irrelevant that AT&T subjectively wished that outsiders would not stumble
    across the data or that Auernheimer hyperbolically characterized the access as
    a “theft.” The company configured its servers to make the information available
    to everyone and thereby authorized the general public to view the information.
    Accessing the e-mail addresses through AT&T’s public website was authorized under
    the CFAA and therefore was not a crime.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Although Auernheimer’s conviction was only overturned on appeal due to lack
    of venue, the Third Circuit Court did seem amenable to this argument in a footnote
    they wrote in their decision:'
  prefs: []
  type: TYPE_NORMAL
- en: Although we need not resolve whether Auernheimer’s conduct involved such a breach,
    no evidence was advanced at trial that the account slurper ever breached any password
    gate or other code-based barrier. The account slurper simply accessed the publicly
    facing portion of the login screen and scraped information that AT&T unintentionally
    published.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: While Auernheimer ultimately was not convicted under the Computer Fraud and
    Abuse Act, he had his house raided by the FBI, spent many thousands of dollars
    in legal fees, and spent three years in and out of courtrooms and prisons.
  prefs: []
  type: TYPE_NORMAL
- en: 'As web scrapers, what lessons can we take away from this to avoid similar situations?
    Perhaps a good start is: don’t be a jerk.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Scraping any sort of sensitive information, whether it’s personal data (in
    this case, email addresses), trade secrets, or government secrets, is probably
    not something you want to do without having a lawyer on speed dial. Even if it’s
    publicly available, think: “Would the average computer user be able to easily
    access this information if they wanted to see it?” or “Is this something the company
    wants users to see?”'
  prefs: []
  type: TYPE_NORMAL
- en: 'I have on many occasions called companies to report security vulnerabilities
    in their web applications. This line works wonders: “Hi, I’m a security professional
    who discovered a potential vulnerability on your website. Could you direct me
    to someone so that I can report it and get the issue resolved?” In addition to
    the immediate satisfaction of recognition for your (white hat) hacking genius,
    you might be able to get free subscriptions, cash rewards, and other goodies out
    of it!'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, Auernheimer’s release of the information to Gawker Media (before
    notifying AT&T) and his showboating around the exploit of the vulnerability also
    made him an especially attractive target for AT&T’s lawyers.
  prefs: []
  type: TYPE_NORMAL
- en: If you find security vulnerabilities in a site, the best thing to do is to alert
    the owners of the site, not the media. You might be tempted to write up a blog
    post and announce it to the world, especially if a fix to the problem is not put
    in place immediately. However, you need to remember that it is the company’s responsibility,
    not yours. The best thing you can do is take your web scrapers (and, if applicable,
    your business) away from the site!
  prefs: []
  type: TYPE_NORMAL
- en: 'Field v. Google: Copyright and robots.txt'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Blake Field, an attorney, filed a lawsuit against Google on the basis that its
    site-caching feature violated copyright law by displaying a copy of his book after
    he had removed it from his website. Copyright law allows the creator of an original
    creative work to have control over the distribution of that work. Field’s argument
    was that Google’s caching (after he had removed it from his website) removed his
    ability to control its distribution.
  prefs: []
  type: TYPE_NORMAL
- en: The Google Web Cache
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When Google web scrapers (also known as *Googlebots*) crawl websites, they
    make a copy of the site and host it on the internet. Anyone can access this cache,
    using the URL format:'
  prefs: []
  type: TYPE_NORMAL
- en: '*http://webcache.googleusercontent.com/search?q=cache:http://pythonscraping.com*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a website you are searching for, or scraping, is unavailable, you might want
    to check there to see if a usable copy exists!
  prefs: []
  type: TYPE_NORMAL
- en: Knowing about Google’s caching feature and not taking action did not help Field’s
    case. After all, he could have prevented the Googlebots from caching his website
    simply by adding the *robots.txt* file, with simple directives about which pages
    should and should not be scraped.
  prefs: []
  type: TYPE_NORMAL
- en: 'More important, the court found that the DMCA Safe Harbor provision allowed
    Google to legally cache and display sites such as Field’s: “[a] service provider
    shall not be liable for monetary relief...for infringement of copyright by reason
    of the intermediate and temporary storage of material on a system or network controlled
    or operated by or for the service provider.”'
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch02.html#id308-marker)) For the full analysis see [“Generative Artificial
    Intelligence and Copyright Law”](https://crsreports.congress.gov/product/pdf/LSB/LSB10922),
    Legal Sidebar, Congressional Research Service. 29 September 2023.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch02.html#id309-marker)) See [“Comment Regarding Request for Comments
    on Intellectual Property Protection for Artificial Intelligence Innovation.”](https://www.uspto.gov/sites/default/files/documents/OpenAI_RFC-84-FR-58141.pdf)
    Docket No. PTO-C-2019-0038, U.S. Patents and Trademark Office.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch02.html#id313-marker)) Bryan Walsh, [“The Surprisingly Large Energy
    Footprint of the Digital Economy [UPDATE]”](http://ti.me/2IFOF3F), TIME.com, August
    14, 2013.
  prefs: []
  type: TYPE_NORMAL
- en: '^([4](ch02.html#id319-marker)) See “‘WarGames’ and Cybersecurity’s Debt to
    a Hollywood Hack,” [*https://oreil.ly/nBCMT*](https://oreil.ly/nBCMT), and “Disloyal
    Computer Use and the Computer Fraud and Abuse Act: Narrowing the Scope,” [*https://oreil.ly/6TWJq*](https://oreil.ly/6TWJq).'
  prefs: []
  type: TYPE_NORMAL
