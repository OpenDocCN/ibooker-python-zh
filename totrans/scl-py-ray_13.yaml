- en: Chapter 12\. Ray in the Enterprise
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12章 Ray在企业中
- en: Deploying software in enterprise environments often comes with additional requirements,
    especially regarding security. Enterprise deployments tend to involve multiple
    stakeholders and need to provide service to a larger group of scientists/engineers.
    While not required, many enterprise clusters tend to have some form of multitenancy
    to allow more efficient use of resources (including human resources, such as operational
    staff).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在企业环境中部署软件通常需要满足额外的要求，特别是在安全方面。企业部署往往涉及多个利益相关者，并且需要为更大的科学家/工程师群体提供服务。虽然不是必需的，但许多企业集群往往具有某种形式的多租户性质，以允许更有效地利用资源（包括人力资源，如运营人员）。
- en: Ray Dependency Security Issues
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Ray依赖项安全问题
- en: Unfortunately, Ray’s default requirements file brings in some insecure libraries.
    Many enterprise environments have some kind of container scanning or similar system
    to detect such issues.^([1](ch12.html#idm45354761688512)) In some cases, you can
    simply remove or upgrade the dependency issues flagged, but when Ray includes
    the dependencies in its wheel (e.g., the Apache Log4j issue), limiting yourself
    to prebuilt wheels has serious drawbacks. If you find a Java or native library
    flagged, you will need to rebuild Ray from source with the version upgraded. Derwen.ai
    has an example of doing this for Docker in its [ray_base repo](https://oreil.ly/Qef7S).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，Ray的默认要求文件引入了一些不安全的库。许多企业环境都有某种容器扫描或类似系统来检测此类问题^([1](ch12.html#idm45354761688512))。在某些情况下，您可以简单地删除或升级标记的依赖项问题，但当Ray将依赖项包含在其wheel中时（例如，Apache
    Log4j问题），限制自己使用预构建的wheel会有严重的缺点。如果发现Java或本地库有问题，则需要使用升级版本从源代码重新构建Ray。Derwen.ai在其[ray_base
    repo](https://oreil.ly/Qef7S)中有一个关于在Docker中执行此操作的示例。
- en: Interacting with the Existing Tools
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与现有工具进行交互
- en: Enterprise deployments often involve interaction with existing tools and the
    data they produce. Some potential points for integration here are using Ray’s
    dataset-generic Arrow interface to interact with other tools. When data is stored
    “at rest,” Parquet is the best format for interaction with other tools.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 企业部署通常涉及与现有工具及其产生的数据的交互。此处进行集成的一些潜在点包括使用Ray的数据集通用Arrow接口与其他工具交互。当数据处于“静止”状态时，Parquet是与其他工具交互的最佳格式。
- en: Using Ray with CI/CD Tools
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Ray与CI/CD工具
- en: When working in large teams, continuous integration and delivery (CI/CD) are
    important parts of effective collaboration on projects. The simplest option for
    using Ray with CI/CD is to use Ray in local mode and treat it as a normal Python
    project. Alternatively, you can submit test jobs by using Ray’s job submission
    API and verify the result. This can allow you to test Ray jobs beyond the scale
    of a single machine. Regardless of whether you use Ray’s job API or Ray’s local
    mode, you can use Ray with any CI/CD tool and virtual environment.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型团队中工作时，持续集成和交付（CI/CD）是项目有效协作的重要组成部分。使用Ray与CI/CD的最简单选择是在本地模式下使用Ray，并将其视为正常的Python项目。另外，您可以通过使用Ray的作业提交API提交测试作业并验证结果。这可以让您测试超出单台计算机规模的Ray作业。无论您使用Ray的作业API还是Ray的本地模式，都可以使用Ray与任何CI/CD工具和虚拟环境。
- en: Authentication with Ray
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与Ray进行身份验证
- en: Ray’s default deployment makes it easy for you to get started, and as such,
    it leaves out any authentication between the client and server. This lack of authentication
    means that anyone who can connect to your Ray server can potentially submit jobs
    and execute arbitrary code. Generally, enterprise environments require a higher
    level of access control than the default configuration provides.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[Ray](https://ray.io)的默认部署使您可以轻松入门，因此，在客户端和服务器之间没有任何身份验证。这种缺乏身份验证意味着任何能连接到您的Ray服务器的人都可能提交作业并执行任意代码。通常，企业环境需要比默认配置提供的更高级别的访问控制。'
- en: Ray’s gRPC endpoints, not the job server, can be configured to use Transport
    Layer Security (TLS) for mutual authentication between the client and the server.
    Ray uses the same TLS communication mechanism between the client and head node
    as between the workers.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Ray的gRPC端点（而不是作业服务器）可以配置为在客户端和服务器之间进行互相认证的传输层安全性（TLS）。Ray在客户端和头节点之间以及工作节点之间使用相同的TLS通信机制。
- en: Warning
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Ray’s TLS implementation requires that the clients have the private key. You
    should consider Ray’s TLS implementation to be akin to shared secret encryption,
    but slower.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Ray的TLS实现要求客户端具有私钥。您应该将Ray的TLS实现视为类似于共享密钥加密，但速度较慢。
- en: 'Another option, which works the job server, is to leave the endpoints insecure
    but restrict who can talk to the endpoint.^([2](ch12.html#idm45354761670192))
    This can be done using ingress controllers, networking rules, or even as an integrated
    part of a virtual private network (VPN) like [Tailscale’s RBAC rules example for
    Grafana](https://oreil.ly/M5O7q).^([3](ch12.html#idm45354761668848)) Thankfully,
    Ray’s dashboard—and by extension, the job server endpoint—already binds to *local​host/127.0.0.1*
    and runs on port 8265\. For example, if you have your Ray head node on Kubernetes
    using Traefik for ingress, you could expose the job API with basic authentication
    as shown here:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选项是使用作业服务器，将端点保持不安全，但限制可以与端点通信的人。这可以通过入口控制器、网络规则甚至作为虚拟私有网络（VPN）的集成部分来完成，例如[Tailscale的Grafana
    RBAC规则示例](https://oreil.ly/M5O7q)。幸运的是，Ray的仪表板——以及作业服务器端点——已绑定到*local​host/127.0.0.1*，并在8265端口上运行。例如，如果你在Kubernetes上使用Traefik作为入口，你可以像这样通过基本身份验证暴露作业API：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Dependence on restricting endpoint access has the downside that anyone who can
    access that computer can submit jobs to your cluster, so it does not work well
    for shared compute resources.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 依赖于限制端点访问的方式存在一个缺点，即任何可以访问该计算机的人都可以向你的集群提交作业，因此对于共享计算资源效果不佳。
- en: Multitenancy on Ray
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Ray上的多租户
- en: Out of the box, Ray clusters support multiple running jobs. When all jobs are
    from the same user and you are not concerned about isolating jobs, you don’t need
    to consider multitenancy implications.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在箱外，Ray集群支持多个运行作业。当所有作业都来自同一用户且你不关心隔离作业时，你不需要考虑多租户的影响。
- en: In our opinion, tenant isolation is less developed than other parts of Ray.
    Ray achieves per user multitenancy security by binding separate workers to a job,
    reducing the chance of accidental information leakage between separate users.
    As with Ray’s execution environments, your users can have different Python libraries
    installed, but Ray does not isolate system-level libraries (like, for example,
    CUDA).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们看来，Ray 的租户隔离相对于其他部分来说发展不足。Ray 通过将独立的工作节点绑定到作业来实现每个用户的多租户安全，从而降低了不同用户之间意外信息泄露的机会。与Ray的执行环境一样，你的用户可以安装不同的Python库，但Ray不会隔离系统级库（例如CUDA）。
- en: We like to think of tenant isolation in Ray as locks on doors. It’s there to
    keep honest people honest and prevent accidental disclosures. However, named resources,
    such as named actors, can be called from any other job. This is an intended function
    of named actors, but as cloudpickle is used frequently throughout Ray, you should
    consider any named actor as having the *potential* of allowing a malicious actor
    on the same cluster to be able to execute arbitrary code in your job.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为Ray中的租户隔离就像门上的锁。它的存在是为了保持诚实的人诚实，并防止意外泄露。然而，像命名的演员这样的命名资源可以从任何其他作业中调用。这是命名演员的预期功能，但由于Ray经常使用cloudpickle，你应该考虑任何命名演员都有允许同一集群上的恶意演员执行任意代码的*潜力*。
- en: Warning
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Named resources break Ray’s tenant isolation.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 命名资源会破坏Ray的租户隔离。
- en: While Ray does have some support for multitenancy, we instead recommend deploying
    multitenant Kubernetes or Yarn clusters. Multitenancy leads nicely into the next
    problem of providing credentials for data sources.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Ray 对多租户有一些支持，但我们建议部署多租户 Kubernetes 或 Yarn 集群。多租户很好地引出了为数据源提供凭据的下一个问题。
- en: Credentials for Data Sources
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据源的凭据
- en: 'Multitenancy complicates credentials for datasources as you cannot fall back
    on instance-based roles/profiles. By adding `env_vars` to your runtime environment,
    you can specify credentials across the entirety of your job. Ideally, you should
    not hardcode these credentials in your source code, but instead, fetch them from
    something like a Kubernetes secret and propagate the values through:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 多租户使得数据源的凭据变得复杂，因为你不能依赖基于实例的角色/配置。通过向运行环境添加`env_vars`，你可以在整个作业中指定凭据。理想情况下，你不应该在源代码中硬编码这些凭据，而是从类似
    Kubernetes 秘钥中获取并传播这些值：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You can also use this same technique to assign credentials per function (e.g.,
    if only one actor should have write permissions) by assigning a runtime environment
    with `.option`. However, in practice, keeping track of the separate credentials
    can become a headache.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用相同的技术为每个函数分配凭据（例如，如果只有一个演员应该具有写权限），通过分配带有`.option`的运行环境。然而，在实践中，跟踪这些单独的凭据可能会成为一个头疼的问题。
- en: Permanent Versus Ephemeral Clusters
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 永久与临时集群
- en: When deploying Ray, you have to choose between permanent and ephemeral clusters.
    With permanent clusters, issues of multitenancy and ensuring that the autoscaler
    can scale down (e.g., no hanging resources) are especially important. However,
    as more enterprises have adopted Kubernetes or other cloud-native technologies,
    we think that ephemeral clusters will increase in appeal.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 部署Ray时，您必须选择永久集群还是瞬时集群。对于永久集群，多租户问题和确保自动缩放器能够缩小（例如，没有悬空资源）尤为重要。然而，随着越来越多的企业采用Kubernetes或其他云原生技术，我们认为瞬时集群的吸引力将增加。
- en: Ephemeral Clusters
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 瞬时集群
- en: Ephemeral clusters have many benefits. Two of the most important are low cost
    and not needing multitenant clusters. Ephemeral clusters allow for resources to
    be fully released when the computation is finished. You can often avoid multitenancy
    issues by provisioning ephemeral clusters, which can reduce the operational burden.
    Ephemeral clusters make experimenting with new versions of Ray and new native
    libraries comparatively lightweight. This can also serve to prevent the issues
    that come with forced migrations, where each team can run its own versions of
    Ray.^([4](ch12.html#idm45354761403136))
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 瞬时集群有许多好处。最重要的两个好处是低成本和不需要多租户集群。瞬时集群允许在计算结束时完全释放资源。通过提供瞬时集群，您可以避免多租户问题，这可以减少操作负担。瞬时集群使得试验新版本的Ray和新的本地库相对轻量化。这也可以防止强制迁移带来的问题，每个团队可以运行自己的Ray版本。^([4](ch12.html#idm45354761403136))
- en: Ephemeral clusters have some drawbacks you should be aware of when making this
    choice. Two of the clearest drawbacks are having to wait for the cluster to start
    up, on top of your application start time, and not being able to use cache/persistence
    on the cluster. Starting an ephemeral cluster depends on being able to allocate
    compute resources, which depending on your environment and budget can take anywhere
    from seconds to days (during cloud issues). If your computations depend on a large
    amount of state or data, each time your application is started on a new cluster,
    it starts by reading back a lot of information, which can be quite slow.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 瞬时集群在做出选择时也有一些您应该注意的缺点。最明显的两个缺点是需要等待集群启动，以及在应用程序启动时间之上，不能在集群上使用缓存/持久性。启动瞬时集群取决于能够分配计算资源，这取决于您的环境和预算，可能需要从几秒到几天的时间（在云问题期间）。如果您的计算依赖于大量状态或数据，每次在新集群上启动应用程序时，它都会先读取大量信息，这可能会相当慢。
- en: Permanent Clusters
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 永久集群
- en: In addition to cost and multitenancy issues, permanent clusters bring additional
    drawbacks. Permanent clusters are more likely to accumulate configuration “artifacts”
    that can be harder to re-create when it comes time to migrate to a new cluster.
    These clusters can become brittle with time as the underlying hardware ages. This
    is true even in the cloud, where long-running instances become increasingly likely
    to experience outages. Long-lived resources in permanent clusters may end up containing
    information that needs to be purged for regulatory reasons.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 除了成本和多租户问题之外，永久集群还带来了额外的缺点。永久集群更容易积累配置“残留物”，当迁移到新集群时可能更难重新创建。随着基础硬件老化，这些集群随时间变得更加脆弱。即使在云中，长时间运行的实例越来越可能遇到故障。永久集群中的长期资源可能最终会包含需要基于监管原因清除的信息。
- en: Permanent clusters also have important benefits that can be useful. From a developer’s
    point of view, one advantage is the ability to have long-lived actors or other
    resources. From an operations point of view, permanent clusters do not take the
    same spin-up time, so if you find yourself needing to do a new task, you don’t
    have to wait for a cluster to become available. [Table 12-1](#comparing-transient-permanent)
    summarizes the differences between transient and permanent clusters.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 永久集群还有一些重要的好处，可以很有用。从开发者的角度来看，一个优势是能够拥有长期存在的参与者或其他资源。从运营的角度来看，永久集群不需要同样的启动时间，因此如果需要执行新任务，你不必等待集群变得可用。[表 12-1](#comparing-transient-permanent)
    总结了瞬时和永久集群之间的差异。
- en: Table 12-1\. Transient- and permanent-cluster comparison chart
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 表 12-1\. 瞬时和永久集群比较表
- en: '|  | Transient/ephemeral clusters | Permanent clusters |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | 瞬时/瞬时集群 | 永久集群 |'
- en: '| --- | --- | --- |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Resource cost** | Normally lower unless running, unless workloads could
    bin-pack or share resources between users | Higher when resource leaks prevent
    the autoscaler from scaling down |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| **资源成本** | 通常较低，除非运行时，工作负载可以进行二进制打包或在用户之间共享资源 | 当资源泄漏阻止自动缩放器缩减时成本较高 |'
- en: '| **Library isolation** | Flexible (including native) | Only venv/Conda env-level
    isolation |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| **库隔离** | 灵活（包括本地） | 仅在 venv/Conda 环境级别隔离 |'
- en: '| **Ability to try new versions of Ray** | Yes, may require code changes for
    new APIs | Higher overhead |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| **尝试新版本 Ray 的能力** | 是，可能需要针对新 API 进行代码更改 | 开销较大 |'
- en: '| **Longest actor life** | Ephemeral (with the cluster) | “Permanent” (excluding
    cluster crashes/redeploys) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| **最长 actor 生命周期** | 短暂（与集群一起） | “永久”（除非集群崩溃/重新部署） |'
- en: '| **Shared actors** | No | Yes |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| **共享 actors** | 否 | 是 |'
- en: '| **Time to launch new application** | Potentially long (cloud-dependent) |
    Varies (if the cluster has nearly instant spare capacity; otherwise, cloud-dependent)
    |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| **启动新应用程序的时间** | 可能较长（依赖云） | 可变（如果集群具有几乎即时的备用容量；否则，依赖于云） |'
- en: '| **Data read amortization** | No (each cluster must read in any shared datasets)
    | Possible (if well structured) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| **数据读取摊销** | 否（每个集群必须读取任何共享数据集） | 可能（如果结构良好） |'
- en: The choice between ephemeral and permanent clusters depends on your use cases
    and requirements. In some deployments, a mix of ephemeral clusters and permanent
    clusters could offer the correct trade-offs.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 使用短暂集群或永久集群的选择取决于您的用例和要求。在某些部署中，短暂集群和永久集群的混合可能提供正确的权衡。
- en: Monitoring
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控
- en: As the size or number of Ray clusters in your organization grows, monitoring
    becomes increasingly important. Ray has built-in metrics reporting through its
    internal dashboard or Prometheus, although Prometheus is disabled by default.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 随着您组织中 Ray 集群的规模或数量增长，监控变得越来越重要。Ray 通过其内部仪表板或 Prometheus 提供内置的度量报告，尽管 Prometheus
    默认情况下是禁用的。
- en: Note
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Ray’s internal dashboard is installed when you install `ray​[default]`, but
    not if you simply install `ray`.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 `ray​[default]` 时会安装 Ray 的内部仪表板，但仅安装 `ray` 不会。
- en: Ray’s dashboard is excellent when you are working by yourself or debugging a
    production issue. If it’s installed, Ray will print an info log message with a
    link to the dashboard (e.g., `View the Ray dashboard at http://127.0.0.1:8265`).
    In addition, the `ray.init` result contains `webui_url`, which points to the metrics
    dashboard. However, Ray’s dashboard does not have the ability to create alerts
    and is therefore helpful only when you know something is wrong. Ray’s dashboard
    UI is being upgraded in Ray 2; [Figure 12-1](#old_dashboard) shows the old dashboard,
    and [Figure 12-2](#new_dashboard) shows the new one.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当您独自工作或调试生产问题时，Ray 的仪表板非常出色。如果安装了仪表板，Ray 将打印一条包含指向仪表板的链接的信息日志（例如，`在 http://127.0.0.1:8265
    查看 Ray 仪表板`）。此外，`ray.init` 的结果包含 `webui_url`，指向度量仪表板。然而，Ray 的仪表板无法创建警报，因此仅在您知道出现问题时才有帮助。Ray
    的仪表板 UI 正在 Ray 2 中升级；[图 12-1](#old_dashboard) 显示旧版仪表板，而 [图 12-2](#new_dashboard)
    显示新版仪表板。
- en: '![spwr 1201](assets/spwr_1201.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![spwr 1201](assets/spwr_1201.png)'
- en: Figure 12-1\. The old (pre-2.0) Ray dashboard
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-1\. 旧版（2.0 之前）Ray 仪表板
- en: '![spwr 1202](assets/spwr_1202.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![spwr 1202](assets/spwr_1202.png)'
- en: Figure 12-2\. The new Ray dashboard
  id: totrans-54
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-2\. 新的 Ray 仪表板
- en: As you can see, the new dashboard did not evolve organically; rather, it was
    intentionally designed and contains new information. Both versions of the dashboard
    contain information about the executor processes and memory usage. The new dashboard
    also has a web UI for looking up objects by ID.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，新仪表板并非自然演化而来；相反，它是经过有意设计的，并包含新信息。两个版本的仪表板均包含有关执行器进程和内存使用情况的信息。新仪表板还具有用于通过
    ID 查找对象的 Web UI。
- en: Warning
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: The dashboard should not be exposed publicly, and the same port is used for
    the job API.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 仪表板不应公开，作业 API 使用相同的端口。
- en: Ray metrics can also be exported to Prometheus, and by default, Ray will pick
    a random port for this. You can find the port by looking at `metrics_export_port`
    in the result of *ray.init*, or specify a fixed port when launching Ray’s head
    node with `--metrics-export-port=`. Ray’s integration with Prometheus not only
    provides integration with metrics visualization tools, like Grafana (see [Figure 12-3](#sample-grafana-dashboard-for-ray)),
    but importantly adds alerting capabilities when some of the parameters are going
    outside predetermined ranges.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Ray 的指标也可以导出到 Prometheus，Ray 默认会选择一个随机端口。您可以通过查看 *ray.init* 的结果中的 `metrics_export_port`
    来找到端口，或者在启动 Ray 的主节点时指定一个固定的端口 `--metrics-export-port=`。Ray 与 Prometheus 的集成不仅提供了与
    Grafana 等指标可视化工具的集成（见[图 12-3](#sample-grafana-dashboard-for-ray)），而且在某些参数超出预定范围时添加了警报功能。
- en: '![spwr 1203](assets/spwr_1203.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![spwr 1203](assets/spwr_1203.png)'
- en: Figure 12-3\. Sample Grafana dashboard for Ray^([5](ch12.html#idm45354761321968))
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-3\. Ray 的示例 Grafana 仪表板^([5](ch12.html#idm45354761321968))
- en: To obtain exported metrics, Prometheus needs to be configured for which hosts
    or pods to scrape. For users with a static cluster, this is as simple as providing
    a host file, but for dynamic users, you have [many options](https://oreil.ly/RR0kf).
    Kubernetes users can use [pod monitors](https://oreil.ly/85MrY) to configure Prometheus
    pod scraping. Because a Ray cluster does not have a unifying label for all nodes,
    here we are using two pod monitors—one for the head node and one for workers.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取导出的指标，需要配置 Prometheus 来抓取哪些主机或 Pod。对于静态集群的用户，只需提供一个主机文件即可；但对于动态用户，有[多种选择](https://oreil.ly/RR0kf)。Kubernetes
    用户可以使用[pod monitors](https://oreil.ly/85MrY)配置 Prometheus 的 Pod 抓取。由于 Ray 集群没有统一的标签适用于所有节点，因此这里我们使用了两个
    Pod Monitor——一个用于主节点，一个用于工作节点。
- en: Non-Kubernetes users can use Prometheus [file-based discovery](https://oreil.ly/eYXbq)
    to use files that Ray automatically generates on the head node at */tmp/ray/prom_metrics_service​_dis⁠covery.json*
    for this.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 非 Kubernetes 用户可以使用 Prometheus 的[file-based discovery](https://oreil.ly/eYXbq)，使用
    Ray 在主节点自动生成的文件*/tmp/ray/prom_metrics_service​_dis⁠covery.json*。
- en: In addition to monitoring Ray itself, you can instrument your code inside Ray.
    You can either add your own metrics to Ray’s Prometheus metrics or integrate with
    OpenTelemetry. The correct metrics and instrumentation largely depend on what
    the rest of your organization uses. Comparing OpenTelemetry and Prometheus is
    beyond the scope of this book.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 除了监控 Ray 本身，您还可以在 Ray 内部对代码进行仪表化。您可以将自己的指标添加到 Ray 的 Prometheus 指标中，或者与 OpenTelemetry
    集成。正确的指标和仪表化主要取决于您的组织其余部分的使用情况。比较 OpenTelemetry 和 Prometheus 超出了本书的范围。
- en: Instrumenting Your Code with Ray Metrics
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Ray 指标仪表化您的代码
- en: Ray’s built-in metrics do an excellent job of reporting cluster health, but
    we often care about application health. For example, a cluster with low memory
    usage because all the jobs are stuck might look good at the cluster level, but
    what we actually care about (serving users, training models, etc.) isn’t happening.
    Thankfully, you can add your own metrics to Ray to monitor your application usage.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Ray 的内置指标很好地报告了集群的健康状况，但我们通常关心的是应用程序的健康状况。例如，由于所有作业都处于停滞状态而导致的低内存使用的集群在集群级别看起来很好，但我们实际关心的（为用户提供服务、训练模型等）并没有发生。幸运的是，您可以向
    Ray 添加自己的指标来监视应用程序的使用情况。
- en: Tip
  id: totrans-66
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Tip
- en: The metrics that you add to Ray metrics are exposed as Prometheus metrics, just
    like Ray’s built in metrics.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 您添加到 Ray 指标的指标会像 Ray 的内置指标一样暴露为 Prometheus 指标。
- en: Ray metrics support the [counter](https://oreil.ly/aW2y9), [gauge](https://oreil.ly/xBLAh),
    and [histogram](https://oreil.ly/tNiTX) metrics types inside `ray.util​.met⁠rics`.
    These metrics objects are not serializable, as they reference C objects. You need
    to explicitly create the metric before you can record any values in it. When creating
    a new metric, you can specify a name, description, and tags. A common tag used
    is the name of the actor a metric is used inside of, for actor sharding. Since
    they are not serializable, you need to either create and use them inside actors,
    as in [Example 12-1](#ray_counters_actor), or use the [lazy singleton pattern](https://oreil.ly/zKck9),
    as in [Example 12-2](#ray_counters_singleton).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Ray 指标支持在 `ray.util.metrics` 内的 [counter](https://oreil.ly/aW2y9), [gauge](https://oreil.ly/xBLAh)
    和 [histogram](https://oreil.ly/tNiTX) 指标类型。这些指标对象不可序列化，因为它们引用了 C 对象。在记录任何值之前，您需要明确创建该指标。在创建新指标时，可以指定名称、描述和标签。一个常用的标签是指标在
    actor 内部使用的 actor 名称，用于 actor 分片。由于它们不可序列化，您需要将它们要么创建并在 actors 内使用，如 [Example 12-1](#ray_counters_actor)，要么使用
    [lazy singleton 模式](https://oreil.ly/zKck9)，如 [Example 12-2](#ray_counters_singleton)。
- en: Example 12-1\. [Using Ray counters inside an actor](https://oreil.ly/LEzXb)
  id: totrans-69
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 12-1\. [在 actor 内部使用 Ray 计数器](https://oreil.ly/LEzXb)
- en: '[PRE2]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Example 12-2\. [Using the global singleton hack to use Ray counters with remote
    functions](https://oreil.ly/LEzXb)
  id: totrans-71
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 12-2\. [使用全局单例方法使 Ray 计数器与远程函数一起使用](https://oreil.ly/LEzXb)
- en: '[PRE3]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: OpenTelemetry is available across many languages, including Python. Ray has
    a basic open-telemetry implementation, but it is not used as widely as its Prometheus
    plug-in.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: OpenTelemetry 可以在包括 Python 在内的多种语言中使用。Ray 具有基本的 OpenTelemetry 实现，但其使用范围不如其 Prometheus
    插件广泛。
- en: Wrapping Custom Programs with Ray
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Ray 包装自定义程序
- en: One of the powerful features of Python is the ability to launch child processes
    using the [subprocess module](https://oreil.ly/rRlWj).^([6](ch12.html#idm45354760850704))
    These processes can be any shell command or any application on your system. This
    capability allows for a lot of interesting options within Ray implementations.
    One of the options, which we will show here, is the ability to run any custom
    Docker image as part of Ray execution.^([7](ch12.html#idm45354760849856)) [Example 12-3](#executing-docker-image-inside-ray-function)
    demonstrates how this can be done.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Python 的一个强大功能是使用 [subprocess 模块](https://oreil.ly/rRlWj)^([6](ch12.html#idm45354760850704))
    启动子进程。这些进程可以是系统上的任何 shell 命令或任何应用程序。这种能力允许在 Ray 实现中有许多有趣的选项。我们将在这里展示其中一个选项，即作为
    Ray 执行的一部分运行任何自定义 Docker 镜像^([7](ch12.html#idm45354760849856))。[Example 12-3](#executing-docker-image-inside-ray-function)
    演示了如何实现这一点。
- en: Example 12-3\. [Executing a Docker image inside a Ray remote function](https://oreil.ly/gacKK)
  id: totrans-76
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 12-3\. [在 Ray 远程函数中执行 Docker 镜像](https://oreil.ly/gacKK)
- en: '[PRE4]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This code contains a simple remote function that executes an external command
    and returns the execution result. The main function passes to it a simple `docker
    run` command and then prints the invocation result.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码包含一个简单的远程函数，执行外部命令并返回执行结果。主函数向其传递一个简单的 `docker run` 命令，然后打印调用结果。
- en: This approach allows you to execute any existing Docker image as part of Ray
    remote function execution, which in turn allows polyglot Ray implementations or
    even executing Python with specific library requirements needing to create a virtual
    environment for this remote function run. It also allows for easy inclusion of
    prebuilt images in the Ray execution.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法允许您在 Ray 远程函数执行的一部分中执行任何现有的 Docker 镜像，这反过来允许多语言 Ray 实现，甚至执行具有特定库需求的 Python
    需要为此远程函数运行创建虚拟环境。它还允许在 Ray 执行中轻松包含预构建的镜像。
- en: Running Docker images is just one of the useful applications of using `subprocess`
    inside Ray. In general, any application installed on the Ray node can be invoked
    using this approach.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Ray 内部使用 `subprocess` 运行 Docker 镜像只是其有用应用之一。一般来说，可以通过这种方法调用安装在 Ray 节点上的任何应用程序。
- en: Conclusion
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'Although Ray was initially created in a research lab, you can start bringing
    Ray to the mainstream enterprise computing infrastructure with the implementation
    enchancements described here. Specifically, be sure to do the following:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Ray 最初是在研究实验室中创建的，您可以通过这里描述的实现增强将 Ray 引入主流企业计算基础设施。具体来说，请确保执行以下操作：
- en: Carefully evaluate the security and multitenancy issues that this can create.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仔细评估此操作可能带来的安全性和多租户问题。
- en: Be mindful of integration with CI/CD and observability tools.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要注意与 CI/CD 和可观察性工具的集成。
- en: Decide whether you need permanent or ephemeral Ray clusters.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决定是否需要永久或短暂的 Ray 集群。
- en: These considerations will change based on your enterprise environment and specific
    use cases for Ray.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这些考虑因您的企业环境和Ray的具体用例而异。
- en: At this point in the book, you should have a solid grasp of all of the Ray basics
    as well as pointers on where to go next. We certainly hope to see you in the Ray
    community and encourage you to check out the [community resources](https://oreil.ly/9xrm8),
    including [Ray’s Slack channel](https://oreil.ly/PnLJO). If you want to see one
    of the ways you can put the pieces of Ray together, [Appendix A](app01.html#appA)
    explores how to build a backend for an open source satellite communication system.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 到达本书的这一点，您应该对所有Ray基础知识有了扎实的掌握，并了解下一步的指引。我们期待您加入Ray社区，并鼓励您查看[社区资源](https://oreil.ly/9xrm8)，包括[Ray的Slack频道](https://oreil.ly/PnLJO)。如果您想看看如何将Ray的各个部分组合起来，附录A探讨了如何为开源卫星通信系统构建后端的一种方式。
- en: ^([1](ch12.html#idm45354761688512-marker)) Some common security scanners include
    Grype, Anchore, and Dagda.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch12.html#idm45354761688512-marker)) 一些常见的安全扫描工具包括Grype、Anchore和Dagda。
- en: ^([2](ch12.html#idm45354761670192-marker)) Making this work with the gRPC client
    is more complicated, as Ray’s workers need to be able to talk to the head node
    and Redis server, which breaks when using localhost for binding.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch12.html#idm45354761670192-marker)) 使其与gRPC客户端配合工作更复杂，因为Ray的工作节点需要能够与头节点和Redis服务器通信，这在使用本地主机进行绑定时会出现问题。
- en: ^([3](ch12.html#idm45354761668848-marker)) One of the authors has friends who
    work at Tailscale, and other solutions are totally OK too.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch12.html#idm45354761668848-marker)) 本书作者中有些人有在Tailscale工作的朋友，其他解决方案也完全可以。
- en: ^([4](ch12.html#idm45354761403136-marker)) In practice, we recommend supporting
    only a few versions of Ray, as it is quickly evolving.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch12.html#idm45354761403136-marker)) 在实际操作中，我们建议仅支持少数几个版本的Ray，因为它在快速发展。
- en: ^([5](ch12.html#idm45354761321968-marker)) See [*Ray metrics-1650932823424.json*](https://oreil.ly/oKtmW)
    for the configuration.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch12.html#idm45354761321968-marker)) 查看[*Ray metrics-1650932823424.json*](https://oreil.ly/oKtmW)获取配置信息。
- en: ^([6](ch12.html#idm45354760850704-marker)) Special thanks to Michael Behrendt
    for suggesting the implementation approach discussed in this section.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch12.html#idm45354760850704-marker)) 特别感谢Michael Behrendt建议本节讨论的实现方法。
- en: ^([7](ch12.html#idm45354760849856-marker)) This will work only for the cloud
    installations where Ray nodes are using Ray installation on the VM. Refer to [Appendix B](app02.html#appB)
    to see how to do this on IBM Cloud and AWS.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch12.html#idm45354760849856-marker)) 这仅适用于使用VM上的Ray安装的云安装环境。参考[附录B](app02.html#appB)了解如何在IBM
    Cloud和AWS上执行此操作。
