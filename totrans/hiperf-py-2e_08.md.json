["```py\nfrom queue import Queue\nfrom functools import partial\n\neventloop = None\n\nclass EventLoop(Queue):\n    def start(self):\n        while True:\n            function = self.get()\n            function()\n\ndef do_hello():\n    global eventloop\n    print(\"Hello\")\n    eventloop.put(do_world)\n\ndef do_world():\n    global eventloop\n    print(\"world\")\n    eventloop.put(do_hello)\n\nif __name__ == \"__main__\":\n    eventloop = EventLoop()\n    eventloop.put(do_hello)\n    eventloop.start()\n```", "```py\nfrom functools import partial\nfrom some_database_library import save_results_to_db\n\ndef save_value(value, callback):\n    print(f\"Saving {value} to database\")\n    save_result_to_db(result, callback)  ![1](Images/1.png)\n\ndef print_response(db_response):\n    print(\"Response from database: {db_response}\")\n\nif __name__ == \"__main__\":\n    eventloop.put(partial(save_value, \"Hello World\", print_response))\n```", "```py\nfrom some_async_database_library import save_results_to_db\n\nasync def save_value(value):\n    print(f\"Saving {value} to database\")\n    db_response = await save_result_to_db(result) ![1](Images/1.png)\n    print(\"Response from database: {db_response}\")\n\nif __name__ == \"__main__\":\n    eventloop.put(\n        partial(save_value, \"Hello World\", print)\n    )\n```", "```py\nimport random\nimport string\n\nimport requests\n\ndef generate_urls(base_url, num_urls):\n    \"\"\"\n We add random characters to the end of the URL to break any caching\n mechanisms in the requests library or the server\n \"\"\"\n    for i in range(num_urls):\n        yield base_url + \"\".join(random.sample(string.ascii_lowercase, 10))\n\ndef run_experiment(base_url, num_iter=1000):\n    response_size = 0\n    for url in generate_urls(base_url, num_iter):\n        response = requests.get(url)\n        response_size += len(response.text)\n    return response_size\n\nif __name__ == \"__main__\":\n    import time\n\n    delay = 100\n    num_iter = 1000\n    base_url = f\"http://127.0.0.1:8080/add?name=serial&delay={delay}&\"\n\n    start = time.time()\n    result = run_experiment(base_url, num_iter)\n    end = time.time()\n    print(f\"Result: {result}, Time: {end - start}\")\n```", "```py\nimport random\nimport string\nimport urllib.error\nimport urllib.parse\nimport urllib.request\nfrom contextlib import closing\n\nimport gevent\nfrom gevent import monkey\nfrom gevent.lock import Semaphore\n\nmonkey.patch_socket()\n\ndef generate_urls(base_url, num_urls):\n    for i in range(num_urls):\n        yield base_url + \"\".join(random.sample(string.ascii_lowercase, 10))\n\ndef download(url, semaphore):\n    with semaphore:  ![2](Images/2.png)\n        with closing(urllib.request.urlopen(url)) as data:\n            return data.read()\n\ndef chunked_requests(urls, chunk_size=100):\n    \"\"\"\n    Given an iterable of urls, this function will yield back the contents of the\n    URLs. The requests will be batched up in \"chunk_size\" batches using a\n    semaphore\n    \"\"\"\n    semaphore = Semaphore(chunk_size)  ![1](Images/1.png)\n    requests = [gevent.spawn(download, u, semaphore) for u in urls]  ![3](Images/3.png)\n    for response in gevent.iwait(requests):\n        yield response\n\ndef run_experiment(base_url, num_iter=1000):\n    urls = generate_urls(base_url, num_iter)\n    response_futures = chunked_requests(urls, 100)  ![4](Images/4.png)\n    response_size = sum(len(r.value) for r in response_futures)\n    return response_size\n\nif __name__ == \"__main__\":\n    import time\n\n    delay = 100\n    num_iter = 1000\n    base_url = f\"http://127.0.0.1:8080/add?name=gevent&delay={delay}&\"\n\n    start = time.time()\n    result = run_experiment(base_url, num_iter)\n    end = time.time()\n    print(f\"Result: {result}, Time: {end - start}\")\n```", "```py\nimport asyncio\nimport random\nimport string\nfrom functools import partial\n\nfrom tornado.httpclient import AsyncHTTPClient\n\nAsyncHTTPClient.configure(\n    \"tornado.curl_httpclient.CurlAsyncHTTPClient\",\n    max_clients=100  ![1](Images/1.png)\n)\n\ndef generate_urls(base_url, num_urls):\n    for i in range(num_urls):\n        yield base_url + \"\".join(random.sample(string.ascii_lowercase, 10))\n\nasync def run_experiment(base_url, num_iter=1000):\n    http_client = AsyncHTTPClient()\n    urls = generate_urls(base_url, num_iter)\n    response_sum = 0\n    tasks = [http_client.fetch(url) for url in urls]  ![2](Images/2.png)\n    for task in asyncio.as_completed(tasks):  ![3](Images/3.png)\n        response = await task  ![4](Images/4.png)\n        response_sum += len(response.body)\n    return response_sum\n\nif __name__ == \"__main__\":\n    import time\n\n    delay = 100\n    num_iter = 1000\n    run_func = partial(\n        run_experiment,\n        f\"http://127.0.0.1:8080/add?name=tornado&delay={delay}&\",\n        num_iter,\n    )\n\n    start = time.time()\n    result = asyncio.run(run_func)  ![5](Images/5.png)\n    end = time.time()\n    print(f\"Result: {result}, Time: {end - start}\")\n```", "```py\nimport asyncio\nimport random\nimport string\n\nimport aiohttp\n\ndef generate_urls(base_url, num_urls):\n    for i in range(num_urls):\n        yield base_url + \"\".join(random.sample(string.ascii_lowercase, 10))\n\ndef chunked_http_client(num_chunks):\n    \"\"\"\n    Returns a function that can fetch from a URL, ensuring that only\n    \"num_chunks\" of simultaneous connects are made.\n    \"\"\"\n    semaphore = asyncio.Semaphore(num_chunks)  ![1](Images/1.png)\n\n    async def http_get(url, client_session):  ![2](Images/2.png)\n        nonlocal semaphore\n        async with semaphore:\n            async with client_session.request(\"GET\", url) as response:\n                return await response.content.read()\n\n    return http_get\n\nasync def run_experiment(base_url, num_iter=1000):\n    urls = generate_urls(base_url, num_iter)\n    http_client = chunked_http_client(100)\n    responses_sum = 0\n    async with aiohttp.ClientSession() as client_session:\n        tasks = [http_client(url, client_session) for url in urls]  ![3](Images/3.png)\n        for future in asyncio.as_completed(tasks):  ![4](Images/4.png)\n            data = await future\n            responses_sum += len(data)\n    return responses_sum\n\nif __name__ == \"__main__\":\n    import time\n\n    loop = asyncio.get_event_loop()\n    delay = 100\n    num_iter = 1000\n\n    start = time.time()\n    result = loop.run_until_complete(\n        run_experiment(\n            f\"http://127.0.0.1:8080/add?name=asyncio&delay={delay}&\", num_iter\n        )\n    )\n    end = time.time()\n    print(f\"Result: {result}, Time: {end - start}\")\n```", "```py\nimport random\nimport string\n\nimport bcrypt\nimport requests\n\ndef do_task(difficulty):\n    \"\"\"\n    Hash a random 10 character string using bcrypt with a specified difficulty\n    rating.\n    \"\"\"\n    passwd = (\"\".join(random.sample(string.ascii_lowercase, 10))  ![1](Images/1.png)\n                .encode(\"utf8\"))\n    salt = bcrypt.gensalt(difficulty)  ![2](Images/2.png)\n    result = bcrypt.hashpw(passwd, salt)\n    return result.decode(\"utf8\")\n\ndef save_result_serial(result):\n    url = f\"http://127.0.0.1:8080/add\"\n    response = requests.post(url, data=result)\n    return response.json()\n\ndef calculate_task_serial(num_iter, task_difficulty):\n    for i in range(num_iter):\n        result = do_task(task_difficulty)\n        save_number_serial(result)\n```", "```py\nimport asyncio\nimport aiohttp\n\nclass AsyncBatcher(object):\n    def __init__(self, batch_size):\n        self.batch_size = batch_size\n        self.batch = []\n        self.client_session = None\n        self.url = f\"http://127.0.0.1:8080/add\"\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args, **kwargs):\n        self.flush()\n\n    def save(self, result):\n        self.batch.append(result)\n        if len(self.batch) == self.batch_size:\n            self.flush()\n\n    def flush(self):\n        \"\"\"\n        Synchronous flush function which starts an IOLoop for the purposes of\n        running our async flushing function\n        \"\"\"\n        loop = asyncio.get_event_loop()\n        loop.run_until_complete(self.__aflush())  ![1](Images/1.png)\n\n    async def __aflush(self):  ![2](Images/2.png)\n        async with aiohttp.ClientSession() as session:\n            tasks = [self.fetch(result, session) for result in self.batch]\n            for task in asyncio.as_completed(tasks):\n                await task\n        self.batch.clear()\n\n    async def fetch(self, result, session):\n        async with session.post(self.url, data=result) as response:\n            return await response.json()\n```", "```py\ndef calculate_task_batch(num_iter, task_difficulty):\n    with AsyncBatcher(100) as batcher: ![1](Images/1.png)\n        for i in range(num_iter):\n            result = do_task(i, task_difficulty)\n            batcher.save(result)\n```", "```py\ndef save_result_aiohttp(client_session):\n    sem = asyncio.Semaphore(100)\n\n    async def saver(result):\n        nonlocal sem, client_session\n        url = f\"http://127.0.0.1:8080/add\"\n        async with sem:\n            async with client_session.post(url, data=result) as response:\n                return await response.json()\n\n    return saver\n\nasync def calculate_task_aiohttp(num_iter, task_difficulty):\n    tasks = []\n    async with aiohttp.ClientSession() as client_session:\n        saver = save_result_aiohttp(client_session)\n        for i in range(num_iter):\n            result = do_task(i, task_difficulty)\n            task = asyncio.create_task(saver(result))  ![1](Images/1.png)\n            tasks.append(task)\n            await asyncio.sleep(0)  ![2](Images/2.png)\n        await asyncio.wait(tasks)  ![3](Images/3.png)\n```"]