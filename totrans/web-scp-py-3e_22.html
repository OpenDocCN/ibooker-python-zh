<html><head></head><body><section data-pdf-bookmark="Chapter 20. Web Scraping Proxies" data-type="chapter" epub:type="chapter"><div class="chapter" id="c-20">&#13;
<h1><span class="label">Chapter 20. </span>Web Scraping Proxies</h1>&#13;
&#13;
<p>That this is the last chapter in the book is somewhat appropriate. Until now you have been running all the Python applications from the command line, within the confines of your home computer. As the saying goes: “If you love something, set it free.”</p>&#13;
&#13;
<p>Although you might be tempted to put off this step as something you don’t <em>need</em> right now, you might be surprised at how much easier your life becomes when you stop trying to run Python scrapers from your laptop.</p>&#13;
&#13;
<p>What’s more, since the first edition of this book was published in 2015, a whole industry of web scraping proxy companies has emerged and flourished. Paying someone to run a web scraper for you used to be a matter of paying for the cloud server instance and running your scraper on it like you would any other software. Now, you can make an API request to, essentially, say “fetch this website,” and a remote program will take care of the details, handle any security issues, and return the data to you (for a fee, of course!).</p>&#13;
&#13;
<p>In this chapter, we’ll look at some methods that will allow you to route your requests through remote IP addresses, host and run your software elsewhere, and even offload the work to a web scraping proxy entirely.</p>&#13;
&#13;
<section data-pdf-bookmark="Why Use Remote Servers?" data-type="sect1"><div class="sect1" id="id126">&#13;
<h1>Why Use Remote Servers?</h1>&#13;
&#13;
<p>Although using a remote server <a contenteditable="false" data-primary="remote servers" data-seealso="proxy servers" data-type="indexterm" id="id920"/><a contenteditable="false" data-primary="remote servers" data-secondary="reasons to use" data-type="indexterm" id="rmsvrsu"/><a contenteditable="false" data-primary="web scraping proxies" data-secondary="remote servers and" data-type="indexterm" id="wbppxrm"/>might seem like an obvious step when launching a web application intended for use by a wide audience, often the tools programmers build for their own purposes are left running locally. In the absence of a motivation for moving the program elsewhere, why do anything? A reason to move it usually falls into one of two camps: the need for an alternate IP address (either because yours is blocked, or to prevent it from getting blocked), and the need for greater power and flexibility.</p>&#13;
&#13;
<section data-pdf-bookmark="Avoiding IP Address Blocking" data-type="sect2"><div class="sect2" id="id174">&#13;
<h2>Avoiding IP Address Blocking</h2>&#13;
&#13;
<p>When building web scrapers, the rule <a contenteditable="false" data-primary="proxy servers" data-secondary="IP address blocking, avoiding" data-type="indexterm" id="pxyvpk"/><a contenteditable="false" data-primary="IP addresses" data-secondary="blocking" data-type="indexterm" id="ipxyv"/>of thumb is: almost everything can be faked. You can send emails from addresses you don’t own, automate mouse-movement data from a command line, or even horrify web administrators by sending their website traffic from Internet Explorer 9.0.</p>&#13;
&#13;
<p>The one thing that cannot be faked is your IP address. In the real world, anyone can send you a letter with the return address: “The President, 1600 Pennsylvania Avenue Northwest, Washington, DC 20500.” However, if the letter is postmarked from Albuquerque, NM, you can be fairly certain you’re not corresponding with the President of the United States.<sup><a data-type="noteref" href="ch20.html#id921" id="id921-marker">1</a></sup></p>&#13;
&#13;
<p>Most efforts to stop scrapers from accessing websites focus on detecting the difference between humans and bots. Going so far as to block IP addresses is a little like a farmer giving up spraying pesticides in favor of just torching the field. It’s a last-ditch but effective method of discarding packets sent from troublesome IP addresses. However, there are problems with this solution:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>IP address access lists are painful to maintain. Although large websites most often have their own programs automating some of the routine management of these lists (bots blocking bots!), someone has to occasionally check them or at least monitor their growth for problems.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Each address adds a tiny amount of processing time to receive packets, as the server must check received packets against the list to decide whether to approve them. Many addresses multiplied by many packets can add up quickly. To save on processing time and complexity, admins often group these IP addresses into blocks and make rules such as “all 256 addresses in this range are blocked” if there are a few tightly clustered offenders. Which leads us to the third point.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>IP address blocking can lead to blocking the “good guys” as well. For example, while I was an undergrad at Olin College of Engineering, one student wrote some software that attempted to rig votes for content on the then-popular <a href="http://digg.com/"><em>http://digg.com</em></a>. This software was blocked, and that single blocked IP address led to an entire dormitory being unable to access the site. The student simply moved his software to another server; in the meantime, Digg lost page visits from many regular users in its prime target demographic.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>Despite its drawbacks, IP address blocking remains an extremely common method for server administrators to stop suspected web scrapers from accessing servers. If an IP address is blocked, the only real solution is to scrape from a different IP address. This can be accomplished by moving the scraper to a new server or routing your traffic through a <a contenteditable="false" data-primary="proxy servers" data-secondary="IP address blocking, avoiding" data-startref="pxyvpk" data-type="indexterm" id="id922"/><a contenteditable="false" data-primary="IP addresses" data-secondary="blocking" data-startref="ipxyv" data-type="indexterm" id="id923"/>different server using a service such as Tor.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Portability and Extensibility" data-type="sect2"><div class="sect2" id="id175">&#13;
<h2>Portability and Extensibility</h2>&#13;
&#13;
<p>Some tasks are too large for a <a contenteditable="false" data-primary="proxy servers" data-secondary="portability and" data-type="indexterm" id="id924"/><a contenteditable="false" data-primary="proxy servers" data-secondary="extensibility" data-type="indexterm" id="id925"/><a contenteditable="false" data-primary="web scraping proxies" data-secondary="remote servers and" data-startref="wbppxrm" data-type="indexterm" id="id926"/>home computer and internet connection. Although you don’t want to put a large load on any single website, you might be collecting data across a wide range of sites and thus require a lot more bandwidth and storage than your current setup can provide.</p>&#13;
&#13;
<p>Moreover, by offloading computationally intensive processing, you can free up your home machine’s cycles for more important tasks (<em>World of Warcraft</em>, anyone?). You don’t have to worry about maintaining power and an internet connection. You can launch your app at a Starbucks, pack up your laptop, and leave knowing that everything’s still running safely. Similarly, later on you can access your collected data anywhere there’s an internet connection.</p>&#13;
&#13;
<p>If you have an application that requires so much computing power that a single Amazon extra-large computing instance won’t satisfy you, you can also look into <em>distributed computing</em>. This allows multiple machines to work in parallel to accomplish your goals. As a simple example, you might have one machine crawl one set of sites and another crawl a second set of sites, and have both of them store collected data in the same database.</p>&#13;
&#13;
<p>Of course, as noted in previous chapters, many can replicate what Google search does, but few can replicate the scale at which Google search does it. Distributed computing is a large field of computer science that is outside the scope of this book. However, learning how to launch your application onto a remote server is a necessary first step, <a contenteditable="false" data-primary="remote servers" data-secondary="reasons to use" data-startref="rmsvrsu" data-type="indexterm" id="id927"/>and you might be surprised at what computers are capable of these days.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Tor" data-type="sect1"><div class="sect1" id="id127">&#13;
<h1>Tor</h1>&#13;
&#13;
<p>The Onion Router network, better <a contenteditable="false" data-primary="proxy servers" data-secondary="Tor" data-type="indexterm" id="pxvtr"/><a contenteditable="false" data-primary="Tor" data-type="indexterm" id="id928"/><a contenteditable="false" data-primary="web scraping proxies" data-secondary="Tor" data-type="indexterm" id="wbpxtr"/>known by the acronym <em>Tor</em>, is a network of volunteer servers set up to route and reroute traffic through many layers (hence the onion reference) of different servers in order to obscure its origin. Data is encrypted before it enters the network so that if any particular server is eavesdropped on, the nature of the communication cannot be revealed. In addition, although the inbound and outbound communications of any particular server can be compromised, one would need to know the details of inbound and outbound communication for <em>all</em> the servers along the path of communication in order to decipher the true start and endpoints of a communication—a near-impossible feat.</p>&#13;
&#13;
<p>Tor is commonly used by human rights workers and political whistleblowers to communicate with journalists, and it receives much of its funding from the US government. Of course, it is also commonly used for illegal activities, and so remains a constant target for government surveillance—although it’s unclear how useful this surveillance is.</p>&#13;
&#13;
<div data-type="warning" epub:type="warning">&#13;
<h1>Limits of Tor Anonymity</h1>&#13;
&#13;
<p>Although the reason you are using Tor in <a contenteditable="false" data-primary="Tor" data-secondary="anonymity" data-type="indexterm" id="id929"/>this book is to change your IP address, not achieve complete anonymity per se, it is worth taking a moment to address some of the strengths and limitations of Tor’s ability to anonymize traffic.</p>&#13;
&#13;
<p>Although you can assume when using Tor that the IP address you are coming from, according to a web server, is not an IP address that can be traced back to you, any information you share with that web server might expose you. For instance, if you log in to your own Gmail account and then make incriminating Google searches, those searches can now be tied back to your identity.</p>&#13;
&#13;
<p>Beyond the obvious, however, even the act of logging in to Tor might be hazardous to your anonymity. In December 2013, a Harvard undergraduate student, in an attempt to get out of final exams, emailed a bomb threat to the school through the Tor network, using an anonymous email account. When the Harvard IT team looked at their logs, they found traffic going out to the Tor network from only a single machine, registered to a known student, during the time that the bomb threat was sent. Although they could not identify the eventual destination of this traffic (only that it was sent across Tor), the fact that the times matched up and only a single machine was logged in at the time was damning enough to prosecute the student.<sup><a data-type="noteref" href="ch20.html#id930" id="id930-marker">2</a></sup></p>&#13;
&#13;
<p>Logging in to Tor is not an automatic invisibility cloak, nor does it give you free rein to do as you please on the internet. Although it is a useful tool, be sure to use it with caution, intelligence, and, of course, morality.</p>&#13;
</div>&#13;
&#13;
<p>Having Tor installed and running is a requirement for using Python with Tor, as you will see in the next section. Fortunately, the Tor service is extremely easy to install and start running with. Just go to the <a href="https://www.torproject.org/download">Tor downloads page</a> and download, install, open, and connect. Keep in mind that your internet speed might appear to be slower while using Tor. Be patient—it might be going around the world several times!</p>&#13;
&#13;
<section data-pdf-bookmark="PySocks" data-type="sect2"><div class="sect2" id="id128">&#13;
<h2>PySocks</h2>&#13;
&#13;
<p>PySocks is a remarkably simple Python <a contenteditable="false" data-primary="Tor" data-secondary="anonymity" data-tertiary="PySocks" data-type="indexterm" id="id931"/><a contenteditable="false" data-primary="PySocks, Tor and" data-type="indexterm" id="pyskt"/>module that is capable of routing traffic through proxy servers and works fantastically in conjunction with Tor. You can download it from <a href="https://pypi.python.org/pypi/PySocks/1.5.0">its website</a> or use any number of third-party module managers to install it.</p>&#13;
&#13;
<p>Although not much in the way of documentation exists for this module, using it is extremely straightforward. The Tor service must be running on port 9150 (the default port) while running this code:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="kn">import</code> <code class="nn">socks</code>&#13;
<code class="kn">import</code> <code class="nn">socket</code>&#13;
<code class="kn">from</code> <code class="nn">urllib.request</code> <code class="kn">import</code> <code class="n">urlopen</code>&#13;
&#13;
&#13;
<code class="n">socks</code><code class="o">.</code><code class="n">set_default_proxy</code><code class="p">(</code><code class="n">socks</code><code class="o">.</code><code class="n">PROXY_TYPE_SOCKS5</code><code class="p">,</code> <code class="s2">"localhost"</code><code class="p">,</code> <code class="mi">9150</code><code class="p">)</code>&#13;
<code class="n">socket</code><code class="o">.</code><code class="n">socket</code> <code class="o">=</code> <code class="n">socks</code><code class="o">.</code><code class="n">socksocket</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="n">urlopen</code><code class="p">(</code><code class="s1">'http://icanhazip.com'</code><code class="p">)</code><code class="o">.</code><code class="n">read</code><code class="p">())</code>&#13;
</pre>&#13;
&#13;
<p>The website <a href="http://icanhazip.com"><em>http://icanhazip.com</em></a> displays only the IP address for the client connecting to the server and can be useful for testing purposes. When this script is run, it should display an IP address that is not your own.</p>&#13;
&#13;
<p>If you want to use Selenium <a contenteditable="false" data-primary="Tor" data-secondary="Selenium and" data-type="indexterm" id="id932"/><a contenteditable="false" data-primary="Tor" data-secondary="ChromeDriver and" data-type="indexterm" id="id933"/><a contenteditable="false" data-primary="Selenium" data-secondary="Tor and" data-type="indexterm" id="id934"/><a contenteditable="false" data-primary="ChromeDriver" data-type="indexterm" id="id935"/>and ChromeDriver with Tor, you don’t need PySocks at all—just make sure that Tor is currently running and add the optional <code>proxy-server</code> Chrome option, specifying that Selenium should connect on the socks5 protocol on port 9150:</p>&#13;
&#13;
<pre class="pre" data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">selenium</code> <code class="kn">import</code> <code class="n">webdriver</code>&#13;
<code class="kn">from</code> <code class="nn">selenium.webdriver.chrome.service</code> <code class="kn">import</code> <code class="n">Service</code>&#13;
<code class="kn">from</code> <code class="nn">selenium.webdriver.chrome.options</code> <code class="kn">import</code> <code class="n">Options</code>&#13;
<code class="kn">from</code> <code class="nn">webdriver_manager.chrome</code> <code class="kn">import</code> <code class="n">ChromeDriverManager</code>&#13;
&#13;
<code class="n">CHROMEDRIVER_PATH</code> <code class="o">=</code> <code class="n">ChromeDriverManager</code><code class="p">()</code><code class="o">.</code><code class="n">install</code><code class="p">()</code>&#13;
<code class="n">driver</code> <code class="o">=</code> <code class="n">webdriver</code><code class="o">.</code><code class="n">Chrome</code><code class="p">(</code><code class="n">service</code><code class="o">=</code><code class="n">Service</code><code class="p">(</code><code class="n">CHROMEDRIVER_PATH</code><code class="p">))</code>&#13;
<code class="n">chrome_options</code> <code class="o">=</code> <code class="n">Options</code><code class="p">()</code>&#13;
<code class="n">chrome_options</code><code class="o">.</code><code class="n">add_argument</code><code class="p">(</code><code class="s1">'--headless'</code><code class="p">)</code>&#13;
<code class="n">chrome_options</code><code class="o">.</code><code class="n">add_argument</code><code class="p">(</code><code class="s1">'--proxy-server=socks5://127.0.0.1:9150'</code><code class="p">)</code>&#13;
<code class="n">driver</code> <code class="o">=</code> <code class="n">webdriver</code><code class="o">.</code><code class="n">Chrome</code><code class="p">(</code>&#13;
    <code class="n">service</code><code class="o">=</code><code class="n">Service</code><code class="p">(</code><code class="n">CHROMEDRIVER_PATH</code><code class="p">),</code>&#13;
    <code class="n">options</code><code class="o">=</code><code class="n">chrome_options</code>&#13;
<code class="p">)</code>&#13;
&#13;
<code class="n">driver</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s1">'http://icanhazip.com'</code><code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="n">driver</code><code class="o">.</code><code class="n">page_source</code><code class="p">)</code>&#13;
<code class="n">driver</code><code class="o">.</code><code class="n">close</code><code class="p">()</code>&#13;
</pre>&#13;
&#13;
<p>Again, this should print out an IP address <a contenteditable="false" data-primary="proxy servers" data-secondary="Tor" data-startref="pxvtr" data-type="indexterm" id="id936"/><a contenteditable="false" data-primary="Tor" data-secondary="anonymity" data-startref="tyyyk" data-tertiary="PySocks" data-type="indexterm" id="id937"/><a contenteditable="false" data-primary="PySocks, Tor and" data-startref="pyskt" data-type="indexterm" id="id938"/><a contenteditable="false" data-primary="web scraping proxies" data-secondary="Tor" data-startref="wbpxtr" data-type="indexterm" id="id939"/>that is not your own but the one that your running Tor client is currently using.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Remote Hosting" data-type="sect1"><div class="sect1" id="id255">&#13;
<h1>Remote Hosting</h1>&#13;
&#13;
<p>Although complete anonymity is lost after you pull out your credit card, hosting your web scrapers remotely may dramatically improve their speed. This is because you’re able to purchase time on much larger machines than you likely own, but also because the connection no longer has to bounce through layers of a Tor network to reach its destination.</p>&#13;
&#13;
<section data-pdf-bookmark="Running from a Website-Hosting Account" data-type="sect2"><div class="sect2" id="id129">&#13;
<h2>Running from a Website-Hosting Account</h2>&#13;
&#13;
<p>If you have a personal or <a contenteditable="false" data-primary="remote hosting" data-secondary="website hosting accounts" data-type="indexterm" id="rhwhcc"/><a contenteditable="false" data-primary="website hosting accounts" data-type="indexterm" id="wbshgcc"/><a contenteditable="false" data-primary="web scraping proxies" data-secondary="website-hosting accounts" data-type="indexterm" id="wpwbhc"/>business website, you might already likely have the means to run your web scrapers from an external server. Even with relatively locked-down web servers, where you have no access to the command line, it is possible to trigger scripts to start and stop through a web interface.</p>&#13;
&#13;
<p>If your website is hosted on a Linux server, the <a contenteditable="false" data-primary="website hosting accounts" data-secondary="Linux server" data-type="indexterm" id="id940"/><a contenteditable="false" data-primary="website hosting accounts" data-secondary="Windows server" data-type="indexterm" id="id941"/>server likely already runs Python. If you’re hosting on a Windows server, you might be out of luck; you’ll need to check specifically to see if Python is installed, or if the server administrator is willing to install it.</p>&#13;
&#13;
<p>Most small web-hosting providers come with software called <em>cPanel</em>, used to provide basic administration services and information about your website and related services. If you have access to cPanel, you can make sure that Python is set up to run on your server by going to Apache Handlers and adding a new handler (if it is not already present):</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
Handler: cgi-script&#13;
Extension(s): .py</pre>&#13;
&#13;
<p>This tells your server that all Python <a contenteditable="false" data-primary="CGI-scripts" data-type="indexterm" id="id942"/><a contenteditable="false" data-primary="CGI (Common Gateway Interface)" data-type="indexterm" id="id943"/>scripts should be executed as a <em>CGI-script</em>. CGI, which stands for <em>Common Gateway Interface</em>, is any program that can be run on a server and dynamically generate content that is displayed on a website. By explicitly defining Python scripts as CGI scripts, you’re giving the server permission to execute them, rather than just display them in a browser or send the user a download.</p>&#13;
&#13;
<p>Write your Python script, upload it to the server, and set the file permissions to 755 to allow it to be executed. To execute the script, navigate to the place you uploaded it to through your browser (or even better, write a scraper to do it for you). If you’re worried about the general public accessing and executing the script, you have two options:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>Store the script at an obscure or hidden URL and make sure to never link to the script from any other accessible URL to avoid search engines indexing it.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Protect the script with a password, or require that a password or secret token be sent to it before it can execute.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>Of course, running a Python script from a service that is specifically designed to display websites is a bit of a hack. For instance, you’ll probably notice that your web scraper-cum-website is a little slow to load. In fact, the page doesn’t actually load (complete with the output of all <code>print</code> statements you might have written in) until the entire scrape is complete. This might take minutes, hours, or never complete at all, depending on how it is written. Although it certainly gets the job done, you might want more real-time output. For that, you’ll need a server that’s <a contenteditable="false" data-primary="remote hosting" data-secondary="website hosting accounts" data-startref="rhwhcc" data-type="indexterm" id="id944"/><a contenteditable="false" data-primary="website hosting accounts" data-startref="wbshgcc" data-type="indexterm" id="id945"/><a contenteditable="false" data-primary="web scraping proxies" data-secondary="website-hosting accounts" data-startref="wpwbhc" data-type="indexterm" id="id946"/>designed for more than just the web.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Running from the Cloud" data-type="sect2"><div class="sect2" id="id130">&#13;
<h2>Running from the Cloud</h2>&#13;
&#13;
<p>Back in the olden days of computing, programmers <a contenteditable="false" data-primary="remote hosting" data-secondary="cloud and" data-type="indexterm" id="rmhcd"/><a contenteditable="false" data-primary="cloud, running from" data-type="indexterm" id="clrfmr"/>paid for or reserved time on computers in order to execute their code. With the advent of personal computers, this became unnecessary—you simply write and execute code on your own computer. Now programmers are once again moving to pay-per-hour computing instances.</p>&#13;
&#13;
<p>This time around, however, users aren’t paying for time on a single, physical machine but on its equivalent computing power, often spread among many machines. The nebulous structure of this system allows computing power to be priced according to times of peak demand. For instance, Amazon allows for bidding on “spot instances” when low costs are more important than immediacy.</p>&#13;
&#13;
<p>Compute instances are also more specialized and can be selected based on the needs of your application, with options like “high memory,” “fast computing,” and “large storage.” Although web scrapers don’t typically use much in the way of memory, you may want to consider large storage or fast computing in lieu of a more general-purpose instance for your scraping application. If you’re doing large amounts of natural language processing, OCR work, or path finding (such as with the Six Degrees of Wikipedia problem), a fast computing instance might work well. If you’re scraping large amounts of data, storing files, or doing large-scale analytics, you might want to go for an instance with storage optimization.</p>&#13;
&#13;
<p>Although the sky is the limit as far as spending goes, at the time of this writing, instances start at just 0.9 cents (less than a penny) an hour for the cheapest Google instance, the f1-micro, and 0.8 cents an hour for a comparable Amazon EC2 micro instance. Thanks to the economies of scale, buying a small compute instance with a large company is almost always cheaper than buying your own physical, dedicated machine. Because now you don’t need to hire an IT guy to keep it running.</p>&#13;
&#13;
<p>Of course, step-by-step instructions for setting up and running cloud computing instances are somewhat outside the scope of this book, but you will likely find that step-by-step instructions are not needed. With both Amazon and Google (not to mention the countless smaller companies in the industry) vying for cloud computing dollars, they’ve made setting up new instances as easy as following a simple prompt, thinking of an app name, and providing a credit card number. As of this writing, both Amazon and Google also offer hundreds of dollars’ worth of free computing hours to further tempt new clients.</p>&#13;
&#13;
<p>If you’re new to cloud computing, DigitalOcean <a contenteditable="false" data-primary="DigitalOcean" data-type="indexterm" id="id947"/>is also a great provider of compute instances (which they call droplets), starting at 0.6 cents an hour. They have an incredibly easy user interface and simply email you the IP address and credentials for any new droplet they create so that you can log in and start running. Although they specialize more in web app hosting, DNS management, and load balancing, you can run anything you want from your instance!</p>&#13;
&#13;
<p>Once you have an instance set up, you should be the proud new owner of an IP address, username, and public/private keys that can be used to connect to your instance through SSH. From there, everything should be the same as working with a server that you physically own—except, of course, you no longer have to worry about hardware maintenance or running your own plethora of advanced monitoring tools.</p>&#13;
&#13;
<p>For quick and dirty jobs, especially if you don’t have a lot of experience dealing with SSH and key pairs, I’ve found that Google’s Cloud Platform instances can be easier to get up and running right <a contenteditable="false" data-primary="remote hosting" data-secondary="cloud and" data-startref="rmhcd" data-type="indexterm" id="id948"/><a contenteditable="false" data-primary="cloud, running from" data-startref="clrfmr" data-type="indexterm" id="id949"/>away. They have a simple launcher and even have a button available after launch to view an SSH terminal right in the browser, as shown in <a data-type="xref" href="#browser-based-terminal">Figure 20-1</a>.</p>&#13;
&#13;
<figure><div class="figure" id="browser-based-terminal"><img alt="" class="iscreen_shot_2018-01-07_at_113758_pmpng" src="assets/wsp3_2001.png"/>&#13;
<h6><span class="label">Figure 20-1. </span>Browser-based terminal from a running Google Cloud Platform VM instance</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Moving Forward" data-type="sect2"><div class="sect2" id="id256">&#13;
<h2>Moving Forward</h2>&#13;
&#13;
<p>The web is constantly changing. The technologies that bring us images, video, text, and other data files are constantly being updated and reinvented. To keep pace, the collection of technologies used to scrape data from the internet must also change.</p>&#13;
&#13;
<p>Who knows? Future versions of this text may omit JavaScript entirely as an obsolete and rarely used technology and instead focus on HTML8 hologram parsing. However, what won’t change is the mindset and general approach needed to successfully scrape any website (or whatever we use for “websites” in the future).</p>&#13;
&#13;
<p>When encountering any web scraping project, you should always ask yourself:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>What is the question I want answered or the problem I want solved?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>What data will help me achieve this and where is it?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>How is the website displaying this data? Can I identify exactly which part of the website’s code contains this information?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>How can I isolate the data and retrieve it?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>What processing or analysis needs to be done to make it more useful?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>How can I make this process better, faster, and more robust?</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>In addition, you need to understand not just how to use the tools presented in this book in isolation but how they can work together to solve a larger problem. Sometimes the data is easily available and well formatted, allowing a simple scraper to do the trick. Other times you have to put some thought into it.</p>&#13;
&#13;
<p>In <a data-type="xref" href="ch16.html#c-16">Chapter 16</a>, for example, you combined the Selenium library to identify Ajax-loaded images on Amazon and Tesseract to use OCR to read them. In the Six Degrees of Wikipedia problem, you used regular expressions to write a crawler that stored link information in a database, and then used a graph-solving algorithm to answer the question, “What is the shortest path of links between Kevin Bacon and Eric Idle?”</p>&#13;
&#13;
<p>There is rarely an unsolvable problem when it comes to automated data collection on the internet. Just remember: the internet is one giant API with a somewhat poor user interface.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Web Scraping Proxies" data-type="sect1"><div class="sect1" id="id131">&#13;
<h1>Web Scraping Proxies</h1>&#13;
&#13;
<p>This book discusses many products <a contenteditable="false" data-primary="web scraping proxies" data-type="indexterm" id="id950"/>and technologies, with a focus on free and open-source software. In cases where paid products are discussed, it’s generally because a free alternative doesn’t exist, isn’t practical, and/or the paid products are so ubiquitous I’d feel remiss not to mention them.</p>&#13;
&#13;
<p>The web scraping proxy and API service industry is an odd one, as far as industries go. It’s new, relatively niche, but still extremely crowded with a low barrier to entry. Because of this, there aren’t any big “household names” yet that all programmers would agree <em>require</em> discussion. Yes, some names are bigger than others, and some services are better than others, but it’s still quite the jungle out there.</p>&#13;
&#13;
<p>Also, because web scraping proxying requires vast amounts of equipment and electricity to run, a viable free alternative does not exist and is unlikely to exist in the future.</p>&#13;
&#13;
<p>This puts me in the precarious position of writing about an assortment of companies that you may not have heard of but that want your money. Rest assured, while I have opinions about these companies, I have not been paid for those opinions. I have used their services, spoken with their representatives, and in several cases been given free account credits for research purposes, but I do not have any incentive to promote them. I am not invested, either financially or emotionally, in any of these companies.</p>&#13;
&#13;
<p>When you read this section, I suggest that you think more generally about the attributes of web scraping proxies and API services, their specialties, your budget, and your project requirements. These profiles are designed to be read as case studies and examples of “what’s out there” rather than specific endorsements. And if you do feel like giving any of these particular companies money, that’s between you and them!</p>&#13;
&#13;
<section data-pdf-bookmark="ScrapingBee" data-type="sect2"><div class="sect2" id="id132">&#13;
<h2>ScrapingBee</h2>&#13;
&#13;
<p>ScrapingBee is the smallest of the companies <a contenteditable="false" data-primary="web scraping proxies" data-secondary="ScrapingBee" data-type="indexterm" id="wbpxcb"/><a contenteditable="false" data-primary="ScrapingBee" data-type="indexterm" id="scrpgb"/><a contenteditable="false" data-primary="APIs (application programming interfaces)" data-secondary="ScrapingBee" data-type="indexterm" id="appcb"/>in this list. It has a strong focus on JavaScript automation, headless browsers, and innocuous-looking IP addresses. Its API is well documented but, if you prefer not to do any reading, ScrapingBee also has an API request generation tool on its website that reduces the problem to button clicking and copy/pasting.</p>&#13;
&#13;
<p>An important feature to consider when evaluating proxy services is the amount of time it takes to return request data to you. Not only does the request have to be routed from your computer to their server to the target’s server and back again, but the proxy service may actually be buffering these requests on its end and not sending them out immediately. It’s not unusual for a request to take a minute or longer to return. During a formal evaluation, it’s important to time these requests throughout the day and time multiple types of requests for any features you might want to use.</p>&#13;
&#13;
<p>Using ScrapingBee’s API directly, we can scrape a product page and print both the results and the time it took to fetch them:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="kn">import</code> <code class="nn">requests</code>&#13;
<code class="kn">import</code> <code class="nn">time</code> &#13;
&#13;
<code class="n">start</code> <code class="o">=</code> <code class="n">time</code><code class="o">.</code><code class="n">time</code><code class="p">()</code>&#13;
<code class="n">params</code> <code class="o">=</code> <code class="p">{</code>&#13;
    <code class="s1">'api_key'</code><code class="p">:</code> <code class="n">SCRAPING_BEE_KEY</code><code class="p">,</code>&#13;
    <code class="s1">'url'</code><code class="p">:</code> <code class="s1">'https://www.target.com/p/-/A-83650487'</code><code class="p">,</code>&#13;
<code class="p">}</code>&#13;
<code class="n">response</code> <code class="o">=</code> <code class="n">requests</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s1">'https://app.scrapingbee.com/api/v1/'</code><code class="p">,</code> <code class="n">params</code><code class="o">=</code><code class="n">params</code><code class="p">)</code>&#13;
&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Time: </code><code class="si">{</code><code class="n">time</code><code class="o">.</code><code class="n">time</code><code class="p">()</code> <code class="o">-</code> <code class="n">start</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'HTTP status: </code><code class="si">{</code><code class="n">response</code><code class="o">.</code><code class="n">status_code</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Response body: </code><code class="si">{</code><code class="n">response</code><code class="o">.</code><code class="n">content</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>&#13;
</pre>&#13;
&#13;
<p>ScrapingBee also has a <a href="https://pypi.org/project/scrapingbee/">Python package</a> that can be installed with pip:</p>&#13;
&#13;
<pre>&#13;
$ pip install scrapingbee</pre>&#13;
&#13;
<p>This is a Software Development Kit (SDK) that let you use various features of the API in a slightly more convenient way. For example, the request above can be written as:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">scrapingbee</code> <code class="kn">import</code> <code class="n">ScrapingBeeClient</code>&#13;
&#13;
<code class="n">start</code> <code class="o">=</code> <code class="n">time</code><code class="o">.</code><code class="n">time</code><code class="p">()</code>&#13;
<code class="n">client</code> <code class="o">=</code> <code class="n">ScrapingBeeClient</code><code class="p">(</code><code class="n">api_key</code><code class="o">=</code><code class="n">SCRAPING_BEE_KEY</code><code class="p">)</code>&#13;
<code class="n">response</code> <code class="o">=</code> <code class="n">client</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s1">'https://www.target.com/p/-/A-83650487'</code><code class="p">)</code>&#13;
&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Time: </code><code class="si">{</code><code class="n">time</code><code class="o">.</code><code class="n">time</code><code class="p">()</code> <code class="o">-</code> <code class="n">start</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'HTTP status: </code><code class="si">{</code><code class="n">response</code><code class="o">.</code><code class="n">status_code</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Response body: </code><code class="si">{</code><code class="n">response</code><code class="o">.</code><code class="n">content</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>&#13;
&#13;
</pre>&#13;
&#13;
<p>Notice that the response is a Python requests response, and it can be used in the same way as in the previous example.</p>&#13;
&#13;
<p>Scraping API services usually deal in units of “credits,” where one basic API request costs one credit. Features such as JavaScript rendering with a headless browser or a residential IP address may cost anywhere from 5 credits to 75 credits. Each paid account level gives you a certain number of credits per month.</p>&#13;
&#13;
<p>While there is a free trial with 1,000 credits, ScrapingBee’s paid subscriptions start at $50/month for 150,000 credits, or 3,000 credits per dollar. Like with most of these services, there are large volume discounts—credits can be 13,000 per dollar or less with greater monthly spend.</p>&#13;
&#13;
<p>If you want to maximize your requests, keep in mind that ScrapingBee charges 5 credits for JavaScript rendering and turns it on by default. This means that the requests above will cost 5 credits each, not 1.</p>&#13;
&#13;
<p>This makes it convenient for customers who may not have read <a data-type="xref" href="ch14.html#c-14">Chapter 14</a> of this book and do not understand why the data appearing in their web browser does not appear in the scraping results coming back from ScrapingBee. If those customers read <a data-type="xref" href="ch15.html#c-15">Chapter 15</a>, they would also understand how to get the data they want without JavaScript rendering at all. If you have read both of these chapters, you can turn off JavaScript rendering and reduce request costs by 80% using:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="n">client</code> <code class="o">=</code> <code class="n">ScrapingBeeClient</code><code class="p">(</code><code class="n">api_key</code><code class="o">=</code><code class="n">SCRAPING_BEE_KEY</code><code class="p">)</code>&#13;
<code class="n">params</code> <code class="o">=</code> <code class="p">{</code><code class="s1">'render_js'</code><code class="p">:</code> <code class="s1">'false'</code><code class="p">}</code>&#13;
<code class="n">response</code> <code class="o">=</code> <code class="n">client</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s1">'https://www.target.com/p/-/A-83650487'</code><code class="p">,</code> <code class="n">params</code><code class="o">=</code><code class="n">params</code><code class="p">)</code></pre>&#13;
&#13;
<p>Like many of these services, ScrapingBee offers the option of using “premium” IP addresses, which may prevent your scrapers from getting blocked by websites wary of IP addresses frequently used by bots. These IP addresses are reported as residential addresses owned by smaller telecommunication companies. If that’s not enough, ScrapingBee also offers a “stealth” IP address for 75 credits per request. The stealth IP addresses I was given were listed as datacenters and VPN servers, so it’s unclear what, exactly, the stealth IP addresses are and what real advantages <a contenteditable="false" data-primary="web scraping proxies" data-secondary="ScrapingBee" data-startref="wbpxcb" data-type="indexterm" id="id951"/><a contenteditable="false" data-primary="ScrapingBee" data-startref="scrpgb" data-type="indexterm" id="id952"/><a contenteditable="false" data-primary="APIs (application programming interfaces)" data-secondary="ScrapingBee" data-startref="appcb" data-type="indexterm" id="id953"/>they offer over the premium addresses.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="ScraperAPI" data-type="sect2"><div class="sect2" id="id133">&#13;
<h2>ScraperAPI</h2>&#13;
&#13;
<p>ScraperAPI, true to its name, has <a contenteditable="false" data-primary="web scraping proxies" data-secondary="ScraperAPI" data-type="indexterm" id="wbpxspapi"/><a contenteditable="false" data-primary="ScraperAPI" data-type="indexterm" id="scrppai"/><a contenteditable="false" data-primary="APIs (application programming interfaces)" data-secondary="ScraperAPI" data-type="indexterm" id="apiscipa"/>a mostly clean and REST-ful API with tons of features. It supports asynchronous requests, which allow you to make the scraping request and fetch the results later in a separate API call. Alternatively, you can provide a webhook endpoint that the results are sent to after the request is complete.</p>&#13;
&#13;
<p>A simple one-credit call with ScraperAPI looks like this:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="kn">import</code> <code class="nn">requests</code>&#13;
<code class="kn">import</code> <code class="nn">time</code> &#13;
&#13;
<code class="n">start</code> <code class="o">=</code> <code class="n">time</code><code class="o">.</code><code class="n">time</code><code class="p">()</code>&#13;
<code class="n">params</code> <code class="o">=</code> <code class="p">{</code>&#13;
    <code class="s1">'api_key'</code><code class="p">:</code> <code class="n">SCRAPER_API_KEY</code><code class="p">,</code>&#13;
    <code class="s1">'url'</code><code class="p">:</code> <code class="s1">'https://www.target.com/p/-/A-83650487'</code>&#13;
<code class="p">}</code>&#13;
<code class="n">response</code> <code class="o">=</code> <code class="n">requests</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s1">'http://api.scraperapi.com'</code><code class="p">,</code> <code class="n">params</code><code class="o">=</code><code class="n">params</code><code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Time: </code><code class="si">{</code><code class="n">time</code><code class="o">.</code><code class="n">time</code><code class="p">()</code> <code class="o">-</code> <code class="n">start</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'HTTP status: </code><code class="si">{</code><code class="n">response</code><code class="o">.</code><code class="n">status_code</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Response body: </code><code class="si">{</code><code class="n">response</code><code class="o">.</code><code class="n">content</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>&#13;
</pre>&#13;
&#13;
<p>ScraperAPI also has an SDK that can be installed with pip:</p>&#13;
&#13;
<pre>&#13;
$ pip install scraperapi-sdk</pre>&#13;
&#13;
<p>Like with most of these SDKs, it is a very thin wrapper around the Python requests library. As with the ScrapingBee API, a Python Requests response is returned:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">scraper_api</code> <code class="kn">import</code> <code class="n">ScraperAPIClient</code>&#13;
&#13;
<code class="n">client</code> <code class="o">=</code> <code class="n">ScraperAPIClient</code><code class="p">(</code><code class="n">SCRAPER_API_KEY</code><code class="p">)</code>&#13;
<code class="n">start</code> <code class="o">=</code> <code class="n">time</code><code class="o">.</code><code class="n">time</code><code class="p">()</code>&#13;
<code class="n">result</code> <code class="o">=</code> <code class="n">client</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s1">'https://www.target.com/p/-/A-83650487'</code><code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Time: </code><code class="si">{</code><code class="n">time</code><code class="o">.</code><code class="n">time</code><code class="p">()</code> <code class="o">-</code> <code class="n">start</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'HTTP status: </code><code class="si">{</code><code class="n">response</code><code class="o">.</code><code class="n">status_code</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Response body: </code><code class="si">{</code><code class="n">response</code><code class="o">.</code><code class="n">content</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>&#13;
</pre>&#13;
&#13;
<p>When evaluating web scraping services, it may be tempting to prefer those that have Python SDKs built around their APIs. However, you should carefully consider how much programming effort it will reduce or convenience it will provide. Technically, a Python “SDK” can be written around any scraping API with very little effort, including your own. This example SDK is written around an imaginary API in just a few lines of code:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="k">class</code> <code class="nc">RyansAPIClient</code><code class="p">:</code>&#13;
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">key</code><code class="p">):</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">key</code> <code class="o">=</code> <code class="n">key</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">api_root</code> <code class="o">=</code> <code class="s1">'http://api.pythonscraping.com/ryansApiPath'</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">get</code><code class="p">(</code><code class="n">url</code><code class="p">):</code>&#13;
        <code class="n">params</code> <code class="o">=</code> <code class="p">{</code><code class="s1">'key'</code><code class="p">:</code> <code class="bp">self</code><code class="o">.</code><code class="n">key</code><code class="p">,</code> <code class="s1">'url'</code><code class="p">:</code> <code class="n">url</code><code class="p">}</code>&#13;
        <code class="k">return</code> <code class="n">requests</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">api_root</code><code class="p">,</code> <code class="n">params</code><code class="o">=</code><code class="n">params</code><code class="p">)</code>&#13;
</pre>&#13;
&#13;
<p>But one unique feature of ScraperAPI is its auto-parsing tools for Amazon products and Google search results. A request for an Amazon product page or an Amazon or Google search results page has a cost of 5 credits, rather the 1 credit for most requests. Although the documentation does mention an explicit call to the Amazon Product Endpoint at <a href="https://api.scraperapi.com/structured/amazon/product"><em class="hyperlink">https://api.scraperapi.com/structured/amazon/product</em></a>, this service appears to be turned on by default:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">scraper_api</code> <code class="kn">import</code> <code class="n">ScraperAPIClient</code>&#13;
&#13;
<code class="n">client</code> <code class="o">=</code> <code class="n">ScraperAPIClient</code><code class="p">(</code><code class="n">SCRAPER_API_KEY</code><code class="p">)</code>&#13;
<code class="n">start</code> <code class="o">=</code> <code class="n">time</code><code class="o">.</code><code class="n">time</code><code class="p">()</code>&#13;
<code class="n">result</code> <code class="o">=</code> <code class="n">client</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s1">'https://www.amazon.com/Web-Scraping-Python-Collecting</code><code class="se">\</code>&#13;
<code class="s1">-Modern/dp/1491985577'</code><code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Time: </code><code class="si">{</code><code class="n">time</code><code class="o">.</code><code class="n">time</code><code class="p">()</code> <code class="o">-</code> <code class="n">start</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'HTTP status: </code><code class="si">{</code><code class="n">response</code><code class="o">.</code><code class="n">status_code</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Response body: </code><code class="si">{</code><code class="n">response</code><code class="o">.</code><code class="n">text</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>&#13;
</pre>&#13;
&#13;
<p>With the response:</p>&#13;
&#13;
<pre>&#13;
Time: 4.672130823135376&#13;
HTTP status: 200&#13;
Response body: {"name":"Web Scraping with Python: Collecting More &#13;
Data from the Modern Web","product_information":{"publisher":&#13;
"‎O'Reilly Media; 2nd edition (May 8, 2018)","language":"‎English",&#13;
"paperback":"‎306 pages","isbn_10":"‎1491985577","isbn_13":&#13;
"‎978-1491985571","item_weight":"‎1.21 pounds" ...</pre>&#13;
&#13;
<p>While writing an Amazon product parsing tool is hardly an insurmountable challenge, offloading the responsibility of testing and maintaining that parsing tool over the years may be well worth the costs.</p>&#13;
&#13;
<p>As mentioned before, ScraperAPI also allows you to make asynchronous requests to its API and fetch the results at a later time. This request takes less than 100 ms to return:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="n">start</code> <code class="o">=</code> <code class="n">time</code><code class="o">.</code><code class="n">time</code><code class="p">()</code>&#13;
<code class="n">params</code> <code class="o">=</code> <code class="p">{</code>&#13;
    <code class="s1">'apiKey'</code><code class="p">:</code> <code class="n">SCRAPER_API_KEY</code><code class="p">,</code>&#13;
    <code class="s1">'url'</code><code class="p">:</code> <code class="s1">'https://www.target.com/p/-/A-83650487'</code>&#13;
<code class="p">}</code>&#13;
<code class="n">response</code> <code class="o">=</code> <code class="n">requests</code><code class="o">.</code><code class="n">post</code><code class="p">(</code><code class="s1">'https://async.scraperapi.com/jobs'</code><code class="p">,</code> <code class="n">json</code><code class="o">=</code><code class="n">params</code><code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Time: </code><code class="si">{</code><code class="n">time</code><code class="o">.</code><code class="n">time</code><code class="p">()</code> <code class="o">-</code> <code class="n">start</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'HTTP status: </code><code class="si">{</code><code class="n">response</code><code class="o">.</code><code class="n">status_code</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Response body: </code><code class="si">{</code><code class="n">response</code><code class="o">.</code><code class="n">content</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code></pre>&#13;
&#13;
<p>Note that this is a <code>POST</code> request rather than a <code>GET</code> request, as shown in previous examples. We are, in a sense, posting data for the creation of a stored entity on Scraper​A⁠PI’s server. Also, the attribute used to send the key changes from <code>api_key</code> to <code>apiKey</code>.</p>&#13;
&#13;
<p>The response body contains only a URL where the job can be fetched:</p>&#13;
&#13;
<pre>&#13;
Time: 0.09664416313171387&#13;
HTTP status: 200&#13;
Response body: b'{"id":"728a365b-3a2a-4ed0-9209-cc4e7d88de96",&#13;
"attempts":0,"status":"running","statusUrl":"https://async.&#13;
scraperapi.com/jobs/728a365b-3a2a-4ed0-9209-cc4e7d88de96",&#13;
"url":"https://www.target.com/p/-/A-83650487"}'</pre>&#13;
&#13;
<p>Calling it does not require an API key—the UUID is sufficient security here—and, assuming the request has been completed on their end, it returns the target’s body:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="n">response</code> <code class="o">=</code> <code class="n">requests</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s1">'https://async.scraperapi.com/jobs/</code><code class="se">\</code>&#13;
<code class="s1">    728a365b-3a2a-4ed0-9209-cc4e7d88de96'</code><code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Response body: </code><code class="si">{</code><code class="n">response</code><code class="o">.</code><code class="n">content</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>&#13;
</pre>&#13;
&#13;
<p>The results of these async requests are stored for up to four hours, or until you retrieve the data. While you could accomplish a similar result at home with a multi-threaded scraper and a little code, you could not easily do it while rotating residential and mobile IP addresses, changing countries of origin, managing session data, rendering all the JavaScript (which will quickly bog down a machine), and tracking all successes and failures in a dashboard.</p>&#13;
&#13;
<p>Asynchronous requests and webhooks (where the proxy service returns the results to the URL you provide) are excellent features in an API service, especially for larger and longer-running scraping projects. ScraperAPI provides this at no extra cost per request, which <a contenteditable="false" data-primary="web scraping proxies" data-secondary="ScraperAPI" data-startref="wbpxspapi" data-type="indexterm" id="id954"/><a contenteditable="false" data-primary="ScraperAPI" data-startref="scrppai" data-type="indexterm" id="id955"/><a contenteditable="false" data-primary="APIs (application programming interfaces)" data-secondary="ScraperAPI" data-startref="apiscipa" data-type="indexterm" id="id956"/>is especially nice.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Oxylabs" data-type="sect2"><div class="sect2" id="id134">&#13;
<h2>Oxylabs</h2>&#13;
&#13;
<p>Oxylabs is a large Lithuanian-based company <a contenteditable="false" data-primary="web scraping proxies" data-secondary="Oxylabs" data-type="indexterm" id="wbpxsxybs"/><a contenteditable="false" data-primary="Oxylabs" data-type="indexterm" id="scrpxylb"/><a contenteditable="false" data-primary="APIs (application programming interfaces)" data-secondary="Oxylabs" data-type="indexterm" id="appxyb"/>with a focus on search engine results page (SERP) and product page scraping. Its product ecosystem and API have a bit of a learning curve. After creating an account, you must activate (either with a one-week trial or paid subscription) every “product” that you want to use and create separate username/password credentials specific to each product. These username/password credentials work a bit like an API key.</p>&#13;
&#13;
<p class="pagebreak-before">The Web Scraper API product allows you to make calls that look like this, with a Web Scraper API username and password:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="kn">import</code> <code class="nn">requests</code>&#13;
<code class="kn">import</code> <code class="nn">time</code>&#13;
&#13;
<code class="n">start</code> <code class="o">=</code> <code class="n">time</code><code class="o">.</code><code class="n">time</code><code class="p">()</code>&#13;
<code class="n">data</code> <code class="o">=</code> <code class="p">{</code>&#13;
    <code class="s1">'url'</code><code class="p">:</code> <code class="s1">'https://www.target.com/p/-/A-83650487'</code><code class="p">,</code>&#13;
    <code class="s1">'source'</code><code class="p">:</code> <code class="s1">'universal'</code><code class="p">,</code>&#13;
<code class="p">}</code>&#13;
&#13;
<code class="n">response</code> <code class="o">=</code> <code class="n">requests</code><code class="o">.</code><code class="n">post</code><code class="p">(</code>&#13;
    <code class="s1">'https://realtime.oxylabs.io/v1/queries'</code><code class="p">,</code>&#13;
    <code class="n">auth</code><code class="o">=</code><code class="p">(</code><code class="n">OXYLABS_USERNAME</code><code class="p">,</code> <code class="n">OXYLABS_PASSWORD</code><code class="p">),</code>&#13;
    <code class="n">json</code><code class="o">=</code><code class="n">data</code>&#13;
<code class="p">)</code>&#13;
&#13;
<code class="n">response</code> <code class="o">=</code> <code class="n">response</code><code class="o">.</code><code class="n">json</code><code class="p">()[</code><code class="s1">'results'</code><code class="p">][</code><code class="mi">0</code><code class="p">]</code>&#13;
&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Time: </code><code class="si">{</code><code class="n">time</code><code class="o">.</code><code class="n">time</code><code class="p">()</code> <code class="o">-</code> <code class="n">start</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'HTTP status: </code><code class="si">{</code><code class="n">response</code><code class="p">[</code><code class="s2">"status_code"</code><code class="p">]</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Response body: </code><code class="si">{</code><code class="n">response</code><code class="p">[</code><code class="s2">"content"</code><code class="p">]</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>&#13;
</pre>&#13;
&#13;
<p>However, the user may be in for a surprise if the target URL is switched to one from the amazon.com domain:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="n">data</code> <code class="o">=</code> <code class="p">{</code>&#13;
    <code class="s1">'url'</code><code class="p">:</code> <code class="s1">'https://www.amazon.com/Web-Scraping-Python-Collecting-Modern</code><code class="se">\</code>&#13;
<code class="s1">-dp-1491985577/dp/1491985577'</code><code class="p">,</code>&#13;
    <code class="s1">'source'</code><code class="p">:</code> <code class="s1">'universal'</code><code class="p">,</code>&#13;
<code class="p">}</code>&#13;
&#13;
<code class="n">response</code> <code class="o">=</code> <code class="n">requests</code><code class="o">.</code><code class="n">post</code><code class="p">(</code>&#13;
    <code class="s1">'https://realtime.oxylabs.io/v1/queries'</code><code class="p">,</code>&#13;
    <code class="n">auth</code><code class="o">=</code><code class="p">(</code><code class="n">OXYLABS_USERNAME</code><code class="p">,</code> <code class="n">OXYLABS_PASSWORD</code><code class="p">),</code>&#13;
<code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="n">response</code><code class="o">.</code><code class="n">json</code><code class="p">())</code>&#13;
</pre>&#13;
&#13;
<p>This code prints an error message:</p>&#13;
&#13;
<pre>&#13;
{'message': 'provided url is not supported'}</pre>&#13;
&#13;
<p>Like ScraperAPI, Oxylabs has parsing tools predesigned for sites like Amazon and Google. However, to scrape those domains—with or without the special parsing <span class="keep-together">tools—you</span> must subscribe specifically to the  SERP Scraper API product (to scrape Google, Bing, Baidu, or Yandex) or the E-Commerce Scraper API product (to scrape Amazon, Aliexpress, eBay, and many others).</p>&#13;
&#13;
<p class="pagebreak-before">If subscribed to the E-Commerce Scraper API product, the Amazon domain can be successfully scraped by changing the <code>source</code> attribute to <code>amazon</code> and passing in the <span class="keep-together">E-Commerce-specific</span> credentials:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="n">data</code> <code class="o">=</code> <code class="p">{</code>&#13;
    <code class="s1">'url'</code><code class="p">:</code> <code class="s1">'https://www.amazon.com/Web-Scraping-Python-Collecting-Modern</code><code class="se">\</code>&#13;
<code class="s1">-dp-1491985577/dp/1491985577'</code><code class="p">,</code>&#13;
    <code class="s1">'source'</code><code class="p">:</code> <code class="s1">'amazon'</code><code class="p">,</code>&#13;
<code class="p">}</code>&#13;
&#13;
<code class="n">response</code> <code class="o">=</code> <code class="n">requests</code><code class="o">.</code><code class="n">post</code><code class="p">(</code>&#13;
    <code class="s1">'https://realtime.oxylabs.io/v1/queries'</code><code class="p">,</code>&#13;
    <code class="n">auth</code><code class="o">=</code><code class="p">(</code><code class="n">OXYLABS_USERNAME_ECOMMERCE</code><code class="p">,</code> <code class="n">OXYLABS_PASSWORD</code><code class="p">),</code>&#13;
    <code class="n">json</code><code class="o">=</code><code class="n">data</code>&#13;
<code class="p">)</code>&#13;
</pre>&#13;
&#13;
<p>This does not do anything special; it simply returns the content of the page as usual. To use the product information formatting templates, we must also set the attribute <code>parse</code> to <code>True</code>:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="n">data</code> <code class="o">=</code> <code class="p">{</code>&#13;
    <code class="s1">'url'</code><code class="p">:</code> <code class="s1">'https://www.amazon.com/Web-Scraping-Python-Collecting-Modern</code><code class="se">\</code>&#13;
<code class="s1">-dp-1491985577/dp/1491985577'</code><code class="p">,</code>&#13;
    <code class="s1">'source'</code><code class="p">:</code> <code class="s1">'amazon'</code><code class="p">,</code>&#13;
    <code class="s1">'parse'</code><code class="p">:</code> <code class="kc">True</code>&#13;
<code class="p">}</code></pre>&#13;
&#13;
<p>This parses the website and returns formatted JSON data:</p>&#13;
&#13;
<pre>&#13;
...&#13;
'page': 1,&#13;
'price': 32.59,&#13;
'stock': 'Only 7 left in stock - order soon',&#13;
'title': 'Web Scraping with Python: Collecting More Data from the Modern Web',&#13;
'buybox': [{'name': 'buy_new', 'price': 32.59, 'condition': 'new'},&#13;
...&#13;
</pre>&#13;
&#13;
<p>It’s important to keep in mind that parsing tools themselves are not specific to the E-Commerce Scraper API product. We can also parse the target.com domain using the regular Web Scraper API product, setting the source back to universal and using the Web Scraper API credentials:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="n">data</code> <code class="o">=</code> <code class="p">{</code>&#13;
    <code class="s1">'url'</code><code class="p">:</code> <code class="s1">'https://www.target.com/p/-/A-83650487'</code><code class="p">,</code>&#13;
    <code class="s1">'source'</code><code class="p">:</code> <code class="s1">'universal'</code><code class="p">,</code>&#13;
    <code class="s1">'parse'</code><code class="p">:</code> <code class="kc">True</code>&#13;
<code class="p">}</code>&#13;
&#13;
<code class="n">response</code> <code class="o">=</code> <code class="n">requests</code><code class="o">.</code><code class="n">post</code><code class="p">(</code>&#13;
    <code class="s1">'https://realtime.oxylabs.io/v1/queries'</code><code class="p">,</code>&#13;
    <code class="n">auth</code><code class="o">=</code><code class="p">(</code><code class="n">OXYLABS_USERNAME</code><code class="p">,</code> <code class="n">OXYLABS_PASSWORD</code><code class="p">),</code>&#13;
    <code class="n">json</code><code class="o">=</code><code class="n">data</code>&#13;
<code class="p">)</code>&#13;
</pre>&#13;
&#13;
<p>Which returns JSON-formatted product data:</p>&#13;
&#13;
<pre>&#13;
'url': 'https://www.target.com/p/-/A-83650487',&#13;
'price': 44.99,&#13;
'title': 'Web Scraping with Python - 2nd Edition by  Ryan Mitchell (Paperback)',&#13;
'category': 'Target/Movies, Music &amp; Books/Books/All Book Genres/Computers &amp; Techn&#13;
ology Books',&#13;
'currency': 'USD',&#13;
'description': 'Error while parsing `description`: `(&lt;class \'AttributeError\'&gt;, &#13;
AttributeError("\'NoneType\' object has no attribute \'xpath\'"))`.', 'rating_sco&#13;
re': 0, 'parse_status_code': 12004</pre>&#13;
&#13;
<p>Because it was attempting to parse pages at the domain target.com automatically, it is liable to run into errors here and there, like it did with the description. Fortunately, users can also write custom parsers, which are compatible with any API product type (Web Scraper API, SERP Scraper API, E-Commerce Scraper API, etc.). These custom parsers take the form of JSON files with a format specified by Oxylabs, which defines the various fields and the XPath selectors that collect data for them.</p>&#13;
&#13;
<p>These custom parsers are essentially the “business logic” of the web scraper itself. It may be worth considering that, if you move to another web scraping API or proxy platform, these templates would be essentially useless and would need to be heavily modified, rewritten, or your new code base would need to be written specifically to work with them. Writing web scraping templates in the Oxylabs-specific language may be somewhat limiting if you choose to go elsewhere.</p>&#13;
&#13;
<p>It’s also important to stress that these different API “products” (which, in fact, use the same API endpoint and call structure) are defined, based not on their particular features but on the domains they’re allowed to send requests to, which could change at any time.</p>&#13;
&#13;
<p>The domains under the purview of a specific product may not necessarily be well-supported by that product either. Oxylab’s SERP Scraping API advertises support for sites such as Baidu and Bing, but it does not have parsing templates developed for them. This “support” may be as simple as the ability to specify a search like:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="n">data</code> <code class="o">=</code> <code class="p">{</code>&#13;
    <code class="s1">'query'</code><code class="p">:</code> <code class="s1">'foo'</code><code class="p">,</code>&#13;
    <code class="s1">'source'</code><code class="p">:</code> <code class="s1">'bing_search'</code><code class="p">,</code>&#13;
<code class="p">}</code></pre>&#13;
&#13;
<p>instead of writing out the full URL:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="n">data</code> <code class="o">=</code> <code class="p">{</code>&#13;
    <code class="s1">'url'</code><code class="p">:</code> <code class="s1">'https://bing.com?q=foo'</code><code class="p">,</code>&#13;
    <code class="s1">'source'</code><code class="p">:</code> <code class="s1">'bing'</code><code class="p">,</code>&#13;
<code class="p">}</code></pre>&#13;
&#13;
<p class="pagebreak-before">Note that, while I am critical of some aspects of Oxylab’s API products, this criticism is not directed at the company per se and should not be interpreted as a comprehensive review or recommendation. I intend it only as a case study, or as an example for consideration, for those who might be evaluating similar products in the future.</p>&#13;
&#13;
<p>When evaluating APIs and web scraping services, it’s always important to consider what is being advertised, what is being provided, and who the target audience is. The structure of an API call may reveal important information about the actual construction of a product, and even the documentation can be misleading.</p>&#13;
&#13;
<p>Oxylabs has many excellent qualities as well. It is one of the best providers of proxy IP addresses. Oxylabs continuously sources a wide variety and large number of IP addresses, listed publicly as being residential, mobile, and data centers. Like other proxy services, these IP addresses are available at a higher cost, depending on the type. However, Oxylabs charges by the gigabyte for these proxy services, rather <a contenteditable="false" data-primary="web scraping proxies" data-secondary="Oxylabs" data-startref="wbpxsxybs" data-type="indexterm" id="id957"/><a contenteditable="false" data-primary="Oxylabs" data-startref="scrpxylb" data-type="indexterm" id="id958"/><a contenteditable="false" data-primary="APIs (application programming interfaces)" data-secondary="Oxylabs" data-startref="appxyb" data-type="indexterm" id="id959"/>than the request. Currently, costs range from $22/GB (low-volume mobile IP addresses) to $8/GB (high-volume residential IP addresses).</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Zyte" data-type="sect2"><div class="sect2" id="id135">&#13;
<h2>Zyte</h2>&#13;
&#13;
<p>Zyte, formerly Scrapinghub, is another <a contenteditable="false" data-primary="web scraping proxies" data-secondary="Zyte" data-type="indexterm" id="wbpxzyet"/><a contenteditable="false" data-primary="Zyte" data-type="indexterm" id="ztyez"/><a contenteditable="false" data-primary="APIs (application programming interfaces)" data-secondary="Zyte" data-type="indexterm" id="apgzyp"/>large web scraping proxy and API service company. It’s also one of the oldest, founded in 2010. While I have no particular attachment to any of these companies, I would be lying if I said that, as the maintainers of Scrapy, Zyte doesn’t stand out from the crowd somewhat. And, beginning in 2019,  it also hosts the <a href="https://www.extractsummit.io">Web Data Extraction Summit</a>.</p>&#13;
&#13;
<p>As a large company, Zyte has most of the features of the previous companies mentioned, and more. Unlike most others, it also sells data outright. If you need, for example, job postings, real estate data, or product information, it can provide those datasets or provide consultants who can build custom datasets.</p>&#13;
&#13;
<p>Zyte maintains Scrapy and has incorporated it into its product lineup in the form of Scrapy Cloud. This tool allows you to deploy and run Scrapy projects in the cloud from either a GitHub repository or from your local machine using the <a href="https://pypi.org/project/shub/">Scrapinghub command-line client</a>. This allows you to keep your web scrapers platform agnostic and portable but still interface tightly with the Zyte ecosystem.</p>&#13;
&#13;
<p>Once a Scrapy project is deployed, Zyte finds all of the spider classes in the project and automatically loads them into your dashboard. You can use Zyte’s dashboard UI to launch and monitor these spiders as they run in the cloud, then view or download the resulting data.</p>&#13;
&#13;
<p class="pagebreak-before">Of course, Zyte also has an API. It is somewhat similar to the other APIs in that it heavily relies on the Python requests package. It is also similar to Oxylab’s API in that it uses the POST method entirely along with HTTP Basic Authentication. However, unlike Oxylab, only a Zyte key is sent over Basic Auth, rather than the username and password:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="kn">import</code> <code class="nn">time</code>&#13;
<code class="kn">from</code> <code class="nn">base64</code> <code class="kn">import</code> <code class="n">b64decode</code>&#13;
<code class="kn">import</code> <code class="nn">requests</code>&#13;
&#13;
<code class="n">json_data</code> <code class="o">=</code> <code class="p">{</code>&#13;
    <code class="s1">'url'</code><code class="p">:</code> <code class="s1">'https://www.target.com/p/-/A-83650487'</code><code class="p">,</code>&#13;
    <code class="s1">'httpResponseBody'</code><code class="p">:</code> <code class="kc">True</code><code class="p">,</code>&#13;
<code class="p">}</code>&#13;
<code class="n">start</code> <code class="o">=</code> <code class="n">time</code><code class="o">.</code><code class="n">time</code><code class="p">()</code>&#13;
<code class="n">response</code> <code class="o">=</code> <code class="n">requests</code><code class="o">.</code><code class="n">post</code><code class="p">(</code><code class="s1">'https://api.zyte.com/v1/extract'</code><code class="p">,</code>&#13;
    <code class="n">auth</code><code class="o">=</code><code class="p">(</code><code class="n">ZYTE_KEY</code><code class="p">,</code> <code class="s1">''</code><code class="p">),</code> <code class="n">json</code><code class="o">=</code><code class="n">json_data</code><code class="p">)</code>&#13;
&#13;
<code class="n">response</code> <code class="o">=</code> <code class="n">response</code><code class="o">.</code><code class="n">json</code><code class="p">()</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Time: </code><code class="si">{</code><code class="n">time</code><code class="o">.</code><code class="n">time</code><code class="p">()</code> <code class="o">-</code> <code class="n">start</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'HTTP status: </code><code class="si">{</code><code class="n">response</code><code class="p">[</code><code class="s2">"statusCode"</code><code class="p">]</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>&#13;
<code class="n">body</code> <code class="o">=</code> <code class="n">b64decode</code><code class="p">(</code><code class="n">response</code><code class="p">[</code><code class="s2">"httpResponseBody"</code><code class="p">])</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Response body: </code><code class="si">{</code><code class="n">body</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>&#13;
</pre>&#13;
&#13;
<p>The other major difference is that all response bodies are returned as base64 encoded strings, rather than HTML or JSON text. This is trivial to handle with Python’s <code>base64</code> package. It also allows you to retrieve binary data, images, and other files just like any other request response by simply decoding the response as that file type.</p>&#13;
&#13;
<p>If you don’t feel like using Scrapy and have a fairly straightforward project, Zyte’s Automatic Extraction API uses AI to detect various fields on a page and return them as JSON-formatted data. Currently, it works with both articles and product types. Obviously, it does not need to use base64 encoding because all the pages it parses must be text:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="n">json_data</code> <code class="o">=</code> <code class="p">[{</code>&#13;
    <code class="s1">'url'</code><code class="p">:</code> <code class="s1">'https://www.target.com/p/-/A-83650487'</code><code class="p">,</code>&#13;
    <code class="s1">'pageType'</code><code class="p">:</code> <code class="s1">'product'</code><code class="p">,</code>&#13;
<code class="p">}]</code>&#13;
&#13;
<code class="n">response</code> <code class="o">=</code> <code class="n">requests</code><code class="o">.</code><code class="n">post</code><code class="p">(</code>&#13;
    <code class="s1">'https://autoextract.zyte.com/v1/extract'</code><code class="p">,</code>&#13;
    <code class="n">auth</code><code class="o">=</code><code class="p">(</code><code class="n">ZYTE_KEY</code><code class="p">,</code> <code class="s1">''</code><code class="p">),</code>&#13;
    <code class="n">json</code><code class="o">=</code><code class="n">json_data</code>&#13;
<code class="p">)</code>&#13;
&#13;
<code class="nb">print</code><code class="p">(</code><code class="n">response</code><code class="o">.</code><code class="n">json</code><code class="p">())</code>&#13;
</pre>&#13;
&#13;
<p class="pagebreak-before">The documentation for Zyte’s Automatic Extraction API provides the URL <em>https://autoextract.scrapinghub.com/v1/extract</em>, as an artifact of their previous name, ScrapingHub. If you see this, know that you can usually replace <code>zyte.com</code> with <code>scrapinghub.com</code> and give your code some backwards compatibility if Zyte decides to shut down the old domain.</p>&#13;
&#13;
<p>Zyte’s products are heavily geared toward developers working in an enterprise environment who want full transparency and control over their scrapers. However, Zyte prefers to take IP address management out of the hands of users with its Zyte Smart Proxy Manager. Zyte controls which IP addresses the traffic is proxied through. IP addresses are maintained between sessions, but IP addresses are switched if one is being blocked. Zyte attempts to use IP address switching to create an organic-looking flow of traffic to a site that avoids suspicion.</p>&#13;
&#13;
<p>Using the Smart Proxy Manager is straightforward, although installing certificates on your machine may add complexity:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="n">response</code> <code class="o">=</code> <code class="n">requests</code><code class="o">.</code><code class="n">get</code><code class="p">(</code>&#13;
    <code class="s1">'https://www.target.com/p/-/A-83650487'</code><code class="p">,</code>&#13;
    <code class="n">proxies</code><code class="o">=</code><code class="p">{</code>&#13;
        <code class="s1">'http'</code><code class="p">:</code> <code class="sa">f</code><code class="s1">'http://</code><code class="si">{</code><code class="n">ZYTE_KEY</code><code class="si">}</code><code class="s1">:@proxy.crawlera.com:8011/'</code><code class="p">,</code>&#13;
        <code class="s1">'https'</code><code class="p">:</code> <code class="sa">f</code><code class="s1">'http://</code><code class="si">{</code><code class="n">ZYTE_KEY</code><code class="si">}</code><code class="s1">:@proxy.crawlera.com:8011/'</code><code class="p">,</code>&#13;
    <code class="p">},</code>&#13;
    <code class="n">verify</code><code class="o">=</code><code class="s1">'/path/to/zyte-proxy-ca.crt'</code> &#13;
<code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="n">response</code><code class="o">.</code><code class="n">text</code><code class="p">)</code></pre>&#13;
&#13;
<p>If you don’t want to use a certificate (although this is not recommended) you can turn off verification in the requests module:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="n">response</code> <code class="o">=</code> <code class="n">requests</code><code class="o">.</code><code class="n">get</code><code class="p">(</code>&#13;
    <code class="o">...</code>&#13;
    <code class="n">verify</code><code class="o">=</code><code class="kc">False</code>&#13;
<code class="p">)</code></pre>&#13;
&#13;
<p>Of course, Zyte also has instructions for integrating its <a href="https://scrapy-zyte-smartproxy.readthedocs.io/en/latest/">proxy services with Scrapy</a>, which can then be run in its Scrapy Cloud.</p>&#13;
&#13;
<p>Proxy requests are around 1,600 per dollar (or less with more expensive monthly plans), API requests start around 12,000 per dollar. The Scrapy Cloud plans are relatively inexpensive, with a generous free tier and a $9/month “Professional” tier. This is likely to encourage <a contenteditable="false" data-primary="web scraping proxies" data-secondary="Zyte" data-startref="wbpxzyet" data-type="indexterm" id="id960"/><a contenteditable="false" data-primary="Zyte" data-startref="ztyez" data-type="indexterm" id="id961"/><a contenteditable="false" data-primary="APIs (application programming interfaces)" data-secondary="Zyte" data-startref="apgzyp" data-type="indexterm" id="id962"/>the use of Scrapy and promote integration with the Zyte <span class="keep-together">platform.</span></p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Additional Resources" data-type="sect1"><div class="sect1" id="id257">&#13;
<h1 class="pagebreak-before">Additional Resources</h1>&#13;
&#13;
<p>Many years ago, running “in the cloud” was mostly the domain of those who felt like slogging through the documentation and already had some server administration experience. Today, the tools have improved dramatically due to increased popularity and competition among cloud computing providers.</p>&#13;
&#13;
<p>Still, for building large-scale or more-complex scrapers and crawlers, you might want a little more guidance on creating a platform for collecting and storing data.</p>&#13;
&#13;
<p><a class="orm:hideurl" href="http://oreil.ly/1FVOw6y"><em>Google Compute Engine</em></a> by Marc Cohen, Kathryn Hurley, and Paul Newson (O’Reilly) is a straightforward resource on using Google Cloud Computing with both Python and JavaScript. It covers not only Google’s user interface but also the command-line and scripting tools that you can use to give your application greater flexibility.</p>&#13;
&#13;
<p>If you prefer to work with Amazon, Mitch Garnaat’s <a class="orm:hideurl" href="http://oreil.ly/VSctQP"><em>Python and AWS Cookbook</em></a> (O’Reilly) is a brief but extremely useful guide that will get you started with Amazon Web Services and show you how to get a scalable application up and running.</p>&#13;
</div></section>&#13;
<div data-type="footnotes"><p data-type="footnote" id="id921"><sup><a href="ch20.html#id921-marker">1</a></sup> Technically, IP addresses can be <a contenteditable="false" data-primary="IP addresses" data-secondary="spoofed" data-type="indexterm" id="id963"/>spoofed in outgoing packets, which is a technique used in distributed denial-of-service attacks, where the attackers don’t care about receiving return packets (which, if sent, will be sent to the wrong address). But web scraping is, by definition, an activity in which a response from the web server is required, so we think of IP addresses as one thing that can’t be faked.</p><p data-type="footnote" id="id930"><sup><a href="ch20.html#id930-marker">2</a></sup> See Nicholas P. Fandos, “Harvard Sophomore Charged in Bomb Threat,” <em>The Harvard Crimson</em>, December 17, 2023, <a href="https://www.thecrimson.com/article/2013/12/17/student-charged-bomb-threat"><em>https://www.thecrimson.com/article/2013/12/17/student-charged-bomb-threat</em></a>.</p></div></div></section></body></html>