- en: Chapter 2\. Profiling to Find Bottlenecks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章。找出瓶颈的性能分析
- en: '*Profiling* lets us find bottlenecks so we can do the least amount of work
    to get the biggest practical performance gain. While we’d like to get huge gains
    in speed and reductions in resource usage with little work, practically you’ll
    aim for your code to run “fast enough” and “lean enough” to fit your needs. Profiling
    will let you make the most pragmatic decisions for the least overall effort.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*性能分析*能帮助我们找出瓶颈，使我们可以尽最少的努力获得最大的实际性能提升。虽然我们希望能够在速度上取得巨大进步并在资源使用上做出大幅度减少，但实际上，您会为了使代码“足够快”和“足够精简”来满足您的需求而努力。性能分析将帮助您为最少的总体工作量做出最为务实的决策。'
- en: Any measurable resource can be profiled (not just the CPU!). In this chapter
    we look at both CPU time and memory usage. You could apply similar techniques
    to measure network bandwidth and disk I/O too.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 任何可测量的资源都可以进行性能分析（不仅仅是CPU！）。在这一章中，我们将研究CPU时间和内存使用情况。您也可以应用类似的技术来测量网络带宽和磁盘I/O。
- en: If a program is running too slowly or using too much RAM, you’ll want to fix
    whichever parts of your code are responsible. You could, of course, skip profiling
    and fix what you *believe* might be the problem—but be wary, as you’ll often end
    up “fixing” the wrong thing. Rather than using your intuition, it is far more
    sensible to first profile, having defined a hypothesis, before making changes
    to the structure of your code.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 如果程序运行太慢或者使用了太多RAM，您将希望修复您代码中负责这些问题的部分。当然，您可以跳过性能分析并修复您*认为*可能是问题的地方，但要小心，因为您往往会“修复”错误的东西。与其依赖您的直觉，最明智的做法是首先进行性能分析，定义了一个假设，然后再对代码结构进行更改。
- en: Sometimes it’s good to be lazy. By profiling first, you can quickly identify
    the bottlenecks that need to be solved, and then you can solve just enough of
    these to achieve the performance you need. If you avoid profiling and jump to
    optimization, you’ll quite likely do more work in the long run. Always be driven
    by the results of profiling.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候偷懒是件好事。通过先进行性能分析，您可以快速识别需要解决的瓶颈，然后您只需解决这些问题中的足够部分以达到所需的性能。如果您避免性能分析而直接进行优化，长远来看您很可能会做更多的工作。始终根据性能分析的结果来驱动您的工作。
- en: Profiling Efficiently
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高效的性能分析
- en: The first aim of profiling is to test a representative system to identify what’s
    slow (or using too much RAM, or causing too much disk I/O or network I/O). Profiling
    typically adds an overhead (10× to 100× slowdowns can be typical), and you still
    want your code to be used in as similar to a real-world situation as possible.
    Extract a test case and isolate the piece of the system that you need to test.
    Preferably, it’ll have been written to be in its own set of modules already.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 性能分析的第一个目标是测试一个代表性系统，以确定什么地方慢（或者使用了太多RAM，或者导致了过多的磁盘I/O或网络I/O）。性能分析通常会增加开销（典型情况下可以是10倍到100倍的减速），而您仍希望您的代码在尽可能接近实际情况的情况下使用。提取一个测试用例，并隔离需要测试的系统部分。最好情况下，它们已经被编写为独立的模块集。
- en: The basic techniques that are introduced first in this chapter include the `%timeit`
    magic in IPython, `time.time()`, and a timing decorator. You can use these techniques
    to understand the behavior of statements and functions.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章首先介绍的基本技术包括IPython中的`%timeit`魔术命令，`time.time()`和一个计时装饰器。您可以使用这些技术来理解语句和函数的行为。
- en: Then we will cover `cProfile` ([“Using the cProfile Module”](#profiling-cprofile)),
    showing you how to use this built-in tool to understand which functions in your
    code take the longest to run. This will give you a high-level view of the problem
    so you can direct your attention to the critical functions.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将介绍`cProfile`（[“使用 cProfile 模块”](#profiling-cprofile)），向您展示如何使用这个内置工具来理解代码中哪些函数运行时间最长。这将为您提供一个高层次的视图，以便您可以将注意力集中在关键函数上。
- en: Next, we’ll look at `line_profiler` ([“Using line_profiler for Line-by-Line
    Measurements”](#profiling-line-profiler)), which will profile your chosen functions
    on a line-by-line basis. The result will include a count of the number of times
    each line is called and the percentage of time spent on each line. This is exactly
    the information you need to understand what’s running slowly and why.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍`line_profiler`（[“使用 line_profiler 进行逐行测量”](#profiling-line-profiler)），它将逐行地对您选择的函数进行性能分析。结果将包括每行被调用的次数和每行所花费的时间百分比。这正是您需要了解什么正在运行缓慢以及为什么的信息。
- en: Armed with the results of `line_profiler`, you’ll have the information you need
    to move on to using a compiler ([Chapter 7](ch07.xhtml#chapter-compiling)).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 有了`line_profiler`的结果，您将拥有继续使用编译器所需的信息（[第7章](ch07.xhtml#chapter-compiling)）。
- en: In [Chapter 6](ch06_split_000.xhtml#matrix_computation), you’ll learn how to
    use `perf stat` to understand the number of instructions that are ultimately executed
    on a CPU and how efficiently the CPU’s caches are utilized. This allows for advanced-level
    tuning of matrix operations. You should take a look at [Example 6-8](ch06_split_000.xhtml#matrix_perf_python_memory)
    when you’re done with this chapter.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第6章](ch06_split_000.xhtml#matrix_computation)中，您将学习如何使用 `perf stat` 来了解在CPU上执行的指令数量以及CPU缓存的有效利用情况。这允许您对矩阵操作进行高级调优。完成本章后，您应该看看[示例6-8](ch06_split_000.xhtml#matrix_perf_python_memory)。
- en: After `line_profiler`, if you’re working with long-running systems, then you’ll
    be interested in `py-spy` to peek into already-running Python processes.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 `line_profiler` 后，如果您正在处理长时间运行的系统，那么您可能会对 `py-spy` 感兴趣，以窥视已经运行的Python进程。
- en: To help you understand why your RAM usage is high, we’ll show you `memory_profiler`
    ([“Using memory_profiler to Diagnose Memory Usage”](#memory_profiler)). It is
    particularly useful for tracking RAM usage over time on a labeled chart, so you
    can explain to colleagues why certain functions use more RAM than expected.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助您理解为什么RAM使用量很高，我们将展示 `memory_profiler`（[“使用memory_profiler诊断内存使用情况”](#memory_profiler)）。它特别适用于在标记的图表上跟踪RAM使用情况，以便您向同事解释为什么某些函数使用的RAM比预期多。
- en: Warning
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Whatever approach you take to profiling your code, you must remember to have
    adequate unit test coverage in your code. Unit tests help you to avoid silly mistakes
    and to keep your results reproducible. Avoid them at your peril.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您采取何种方法来分析您的代码，您必须记住在您的代码中拥有充分的单元测试覆盖率。单元测试帮助您避免愚蠢的错误，并保持您的结果可重现性。不要因为它们而忽视它们。
- en: '*Always* profile your code before compiling or rewriting your algorithms. You
    need evidence to determine the most efficient ways to make your code run faster.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在编译或重写算法之前*始终*对您的代码进行分析。您需要证据来确定使您的代码运行更快的最有效方法。
- en: Next, we’ll give you an introduction to the Python bytecode inside CPython ([“Using
    the dis Module to Examine CPython Bytecode”](#profiling-dis)), so you can understand
    what’s happening “under the hood.” In particular, having an understanding of how
    Python’s stack-based virtual machine operates will help you understand why certain
    coding styles run more slowly than others.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将向您介绍Python字节码在CPython中的使用（[“使用dis模块检查CPython字节码”](#profiling-dis)），以便您了解“底层”发生了什么。特别是，了解Python基于栈的虚拟机如何运行将帮助您理解为什么某些编码风格运行速度较慢。
- en: Before the end of the chapter, we’ll review how to integrate unit tests while
    profiling ([“Unit Testing During Optimization to Maintain Correctness”](#profiling-unit-testing))
    to preserve the correctness of your code while you make it run more efficiently.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束之前，我们将回顾如何在优化过程中集成单元测试以保持代码正确性（[“在优化过程中进行单元测试以保持正确性”](#profiling-unit-testing)）。
- en: We’ll finish with a discussion of profiling strategies ([“Strategies to Profile
    Your Code Successfully”](#profiling-strategies)) so you can reliably profile your
    code and gather the correct data to test your hypotheses. Here you’ll learn how
    dynamic CPU frequency scaling and features like Turbo Boost can skew your profiling
    results, and you’ll learn how they can be disabled.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最后将讨论性能分析策略（[“成功分析代码的策略”](#profiling-strategies)），以便您可以可靠地分析您的代码并收集正确的数据来测试您的假设。在这里，您将了解到动态CPU频率缩放和Turbo
    Boost等功能如何影响您的分析结果，以及如何禁用它们。
- en: To walk through all of these steps, we need an easy-to-analyze function. The
    next section introduces the Julia set. It is a CPU-bound function that’s a little
    hungry for RAM; it also exhibits nonlinear behavior (so we can’t easily predict
    the outcomes), which means we need to profile it at runtime rather than analyzing
    it offline.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 要走完所有这些步骤，我们需要一个易于分析的函数。下一节介绍了朱利亚集。它是一个对RAM需求较高的CPU绑定函数；它还表现出非线性行为（因此我们不能轻易预测结果），这意味着我们需要在运行时而不是离线分析它。
- en: Introducing the Julia Set
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍朱利亚集
- en: The [Julia set](https://oreil.ly/zJ1oB) is an interesting CPU-bound problem
    for us to begin with. It is a fractal sequence that generates a complex output
    image, named after Gaston Julia.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[朱利亚集](https://oreil.ly/zJ1oB) 是一个我们开始的有趣的CPU绑定问题。它是一个产生复杂输出图像的分形序列，以加斯顿·朱利亚命名。'
- en: The code that follows is a little longer than a version you might write yourself.
    It has a CPU-bound component and a very explicit set of inputs. This configuration
    allows us to profile both the CPU usage and the RAM usage so we can understand
    which parts of our code are consuming two of our scarce computing resources. This
    implementation is *deliberately* suboptimal, so we can identify memory-consuming
    operations and slow statements. Later in this chapter we’ll fix a slow logic statement
    and a memory-consuming statement, and in [Chapter 7](ch07.xhtml#chapter-compiling)
    we’ll significantly speed up the overall execution time of this function.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的代码比你自己可能写的版本稍长一些。它包含一个CPU绑定的组件和一个非常明确的输入集。这种配置使我们能够分析CPU和RAM的使用情况，从而理解我们的代码中哪些部分消耗了我们稀缺的计算资源。这个实现故意地不够优化，这样我们可以识别出消耗内存的操作和执行缓慢的语句。在本章的后面，我们将修复一个执行缓慢的逻辑语句和一个消耗内存的语句，在[第7章](ch07.xhtml#chapter-compiling)中，我们将显著加快此函数的总执行时间。
- en: We will analyze a block of code that produces both a false grayscale plot ([Figure 2-1](#FIG-julia-example))
    and a pure grayscale variant of the Julia set ([Figure 2-3](#FIG-julia-example-greyscale)),
    at the complex point `c=-0.62772-0.42193j`. A Julia set is produced by calculating
    each pixel in isolation; this is an “embarrassingly parallel problem,” as no data
    is shared between points.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将分析一个生成假灰度图（[图2-1](#FIG-julia-example)）和朱利亚集的纯灰度变体（[图2-3](#FIG-julia-example-greyscale)）的代码块，复数点为`c=-0.62772-0.42193j`。朱利亚集通过独立计算每个像素来生成；这是一个“尴尬并行问题”，因为点之间没有共享数据。
- en: '![Julia set at -0.62772-0.42193i](Images/hpp2_0201.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![Julia set at -0.62772-0.42193i](Images/hpp2_0201.png)'
- en: Figure 2-1\. Julia set plot with a false gray scale to highlight detail
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-1\. 朱利亚集的绘图，使用灰度假色以突出细节
- en: If we chose a different `c`, we’d get a different image. The location we have
    chosen has regions that are quick to calculate and others that are slow to calculate;
    this is useful for our analysis.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们选择了不同的`c`，我们会得到不同的图像。我们选择的位置有些区域计算迅速，而其他区域计算缓慢；这对我们的分析很有用。
- en: The problem is interesting because we calculate each pixel by applying a loop
    that could be applied an indeterminate number of times. On each iteration we test
    to see if this coordinate’s value escapes toward infinity, or if it seems to be
    held by an attractor. Coordinates that cause few iterations are colored darkly
    in [Figure 2-1](#FIG-julia-example), and those that cause a high number of iterations
    are colored white. White regions are more complex to calculate and so take longer
    to generate.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题很有趣，因为我们通过应用可能无数次的循环来计算每个像素。在每次迭代中，我们测试这个坐标的值是否朝无穷远处逃逸，或者是否被某个吸引子吸引住。导致少量迭代的坐标在[图2-1](#FIG-julia-example)中被着深色，而导致大量迭代的坐标则被着白色。白色区域的计算更复杂，因此生成时间更长。
- en: 'We define a set of *z* coordinates that we’ll test. The function that we calculate
    squares the complex number `z` and adds `c`:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一组将要测试的*z*坐标。我们计算的函数对复数`z`进行平方，并加上`c`：
- en: <math display="block" alttext="f left-parenthesis z right-parenthesis equals
    z squared plus c"><mrow><mi>f</mi> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow>
    <mo>=</mo> <msup><mi>z</mi> <mn>2</mn></msup> <mo>+</mo> <mi>c</mi></mrow></math>
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block" alttext="f left-parenthesis z right-parenthesis equals
    z squared plus c"><mrow><mi>f</mi> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow>
    <mo>=</mo> <msup><mi>z</mi> <mn>2</mn></msup> <mo>+</mo> <mi>c</mi></mrow></math>
- en: We iterate on this function while testing to see if the escape condition holds
    using `abs`. If the escape function is `False`, we break out of the loop and record
    the number of iterations we performed at this coordinate. If the escape function
    is never `False`, we stop after `maxiter` iterations. We will later turn this
    `z`’s result into a colored pixel representing this complex location.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在测试时迭代这个函数，同时使用`abs`检测是否满足逃逸条件。如果逃逸函数为`False`，我们跳出循环并记录在这个坐标处执行的迭代次数。如果逃逸函数从未为`False`，我们将在`maxiter`次迭代后停止。我们随后会将这个`z`的结果转换为一个表示这个复数位置的彩色像素。
- en: 'In pseudocode, it might look like this:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 伪代码可能是这样的：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: To explain this function, let’s try two coordinates.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 要解释这个函数，让我们尝试两个坐标。
- en: 'We’ll use the coordinate that we draw in the top-left corner of the plot at
    `-1.8-1.8j`. We must test `abs(z) < 2` before we can try the update rule:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用绘图左上角的坐标`-1.8-1.8j`。在我们尝试更新规则之前，我们必须测试`abs(z) < 2`：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We can see that for the top-left coordinate, the `abs(z)` test will be `False`
    on the zeroth iteration as `2.54 >= 2.0`, so we do not perform the update rule.
    The `output` value for this coordinate is `0`.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，对于左上角的坐标，`abs(z)`测试在零次迭代时将为`False`，因为`2.54 >= 2.0`，所以我们不执行更新规则。这个坐标的`output`值为`0`。
- en: 'Now let’s jump to the center of the plot at `z = 0 + 0j` and try a few iterations:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们跳到绘图的中心，`z = 0 + 0j`，并尝试几次迭代：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We can see that each update to `z` for these first iterations leaves it with
    a value where `abs(z) < 2` is `True`. For this coordinate we can iterate 300 times,
    and still the test will be `True`. We cannot tell how many iterations we must
    perform before the condition becomes `False`, and this may be an infinite sequence.
    The maximum iteration (`maxiter`) break clause will stop us from iterating potentially
    forever.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，对于这些首次迭代的每次更新`z`，它的值都满足`abs(z) < 2`为`True`。对于这个坐标，我们可以迭代300次，测试仍为`True`。我们无法确定在条件变为`False`之前必须执行多少次迭代，这可能是一个无限序列。最大迭代（`maxiter`）中断子句将阻止我们无限迭代。
- en: In [Figure 2-2](#FIG-julia-non-convergence), we see the first 50 iterations
    of the preceding sequence. For `0+0j` (the solid line with circle markers), the
    sequence appears to repeat every eighth iteration, but each sequence of seven
    calculations has a minor deviation from the previous sequence—we can’t tell if
    this point will iterate forever within the boundary condition, or for a long time,
    or maybe for just a few more iterations. The dashed `cutoff` line shows the boundary
    at `+2`.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 2-2](#FIG-julia-non-convergence)中，我们看到了前50次迭代的序列。对于`0+0j`（实线与圆形标记），序列似乎每8次迭代重复一次，但每个7次计算序列与前一个序列有轻微偏差——我们无法确定这个点是否会在边界条件内永远迭代，或者长时间迭代，或者只是再迭代几次。虚线`cutoff`线显示在`+2`处的边界。
- en: '![julia non convergence](Images/hpp2_0202.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![julia non convergence](Images/hpp2_0202.png)'
- en: Figure 2-2\. Two coordinate examples evolving for the Julia set
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-2\. Julia集的两个坐标示例演变
- en: For `-0.82+0j` (the dashed line with diamond markers), we can see that after
    the ninth update, the absolute result has exceeded the `+2` cutoff, so we stop
    updating this value.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`-0.82+0j`（虚线与菱形标记），我们可以看到在第九次更新后，绝对结果超过了`+2`截止值，因此我们停止更新此值。
- en: Calculating the Full Julia Set
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算完整的Julia集
- en: In this section we break down the code that generates the Julia set. We’ll analyze
    it in various ways throughout this chapter. As shown in [Example 2-1](#profiling-juliaset-intro1),
    at the start of our module we import the `time` module for our first profiling
    approach and define some coordinate constants.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将分解生成Julia集的代码。我们将在本章的各个部分进行多方面的分析。如[示例 2-1](#profiling-juliaset-intro1)所示，在模块的开头，我们导入`time`模块以进行第一个性能分析，并定义一些坐标常量。
- en: Example 2-1\. Defining global constants for the coordinate space
  id: totrans-49
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 2-1\. 为坐标空间定义全局常量
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: To generate the plot, we create two lists of input data. The first is `zs` (complex
    *z* coordinates), and the second is `cs` (a complex initial condition). Neither
    list varies, and we could optimize `cs` to a single `c` value as a constant. The
    rationale for building two input lists is so that we have some reasonable-looking
    data to profile when we profile RAM usage later in this chapter.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成图形，我们创建两个输入数据列表。第一个是`zs`（复数*z*坐标），第二个是`cs`（复杂初始条件）。这两个列表都不变，我们可以将`cs`优化为单个`c`值作为常量。构建两个输入列表的原因是，当我们在本章后面分析RAM使用情况时，我们有一些合理的数据进行性能分析。
- en: To build the `zs` and `cs` lists, we need to know the coordinates for each `z`.
    In [Example 2-2](#profiling-juliaset-intro2), we build up these coordinates using
    `xcoord` and `ycoord` and a specified `x_step` and `y_step`. The somewhat verbose
    nature of this setup is useful when porting the code to other tools (such as `numpy`)
    and to other Python environments, as it helps to have everything *very* clearly
    defined for debugging.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建`zs`和`cs`列表，我们需要知道每个`z`的坐标。在[示例 2-2](#profiling-juliaset-intro2)中，我们使用`xcoord`和`ycoord`以及指定的`x_step`和`y_step`来构建这些坐标。这种设置有点冗长，但在将代码移植到其他工具（如`numpy`）和其他Python环境时很有用，因为它有助于为调试清晰地定义一切。
- en: Example 2-2\. Establishing the coordinate lists as inputs to our calculation
    function
  id: totrans-53
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 2-2\. 将坐标列表建立为我们计算函数的输入
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Having built the `zs` and `cs` lists, we output some information about the size
    of the lists and calculate the `output` list via `calculate_z_serial_purepython`.
    Finally, we `sum` the contents of `output` and `assert` that it matches the expected
    output value. Ian uses it here to confirm that no errors creep into the book.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 构建了`zs`和`cs`列表后，我们输出了关于列表大小的一些信息，并通过`calculate_z_serial_purepython`计算了`output`列表。最后，我们对`output`的内容进行了`sum`并`assert`其与预期输出值匹配。Ian在这里使用它来确认书中没有错误。
- en: As the code is deterministic, we can verify that the function works as we expect
    by summing all the calculated values. This is useful as a sanity check—when we
    make changes to numerical code, it is *very* sensible to check that we haven’t
    broken the algorithm. Ideally, we would use unit tests and test more than one
    configuration of the problem.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 因为代码是确定性的，我们可以通过对所有计算出的值求和来验证函数按预期工作。这在检查代码中的数值变化时非常有用，*非常*明智，以确保算法没有出错。理想情况下，我们会使用单元测试，并测试问题的多种配置。
- en: Next, in [Example 2-3](#profiling-juliaset-intro3), we define the `calculate_z_serial_purepython`
    function, which expands on the algorithm we discussed earlier. Notably, we also
    define an `output` list at the start that has the same length as the input `zs`
    and `cs` lists.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在[示例 2-3](#profiling-juliaset-intro3)中，我们定义了`calculate_z_serial_purepython`函数，该函数扩展了我们之前讨论的算法。值得注意的是，我们还在开头定义了一个与输入`zs`和`cs`列表长度相同的`output`列表。
- en: Example 2-3\. Our CPU-bound calculation function
  id: totrans-58
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 2-3\. 我们的CPU绑定计算函数
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now we call the calculation routine in [Example 2-4](#profiling-juliaset-intro4).
    By wrapping it in a `__main__` check, we can safely import the module without
    starting the calculations for some of the profiling methods. Here, we’re not showing
    the method used to plot the output.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们在[示例 2-4](#profiling-juliaset-intro4)中调用计算例程。通过将其包装在`__main__`检查中，我们可以安全地导入模块，而不会启动某些分析方法的计算。这里我们不展示用于绘制输出的方法。
- en: Example 2-4\. `__main__` for our code
  id: totrans-61
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 2-4\. 我们代码的`__main__`
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Once we run the code, we see some output about the complexity of the problem:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们运行代码，就会看到有关问题复杂性的一些输出：
- en: '[PRE9]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In the false-grayscale plot ([Figure 2-1](#FIG-julia-example)), the high-contrast
    color changes gave us an idea of where the cost of the function was slow changing
    or fast changing. Here, in [Figure 2-3](#FIG-julia-example-greyscale), we have
    a linear color map: black is quick to calculate, and white is expensive to calculate.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在错误的灰度图（[图例 2-1](#FIG-julia-example)）中，高对比度的颜色变化使我们了解了函数成本变化缓慢或迅速的位置。在这里，我们使用线性颜色映射的[图例 2-3](#FIG-julia-example-greyscale)中，黑色计算速度快，白色计算费时。
- en: By showing two representations of the same data, we can see that lots of detail
    is lost in the linear mapping. Sometimes it can be useful to have various representations
    in mind when investigating the cost of a function.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 通过展示相同数据的两种表示，我们可以看到线性映射中丢失了大量细节。在调查函数成本时，有时记住多种表示形式是有用的。
- en: '![Julia set at -0.62772-0.42193i](Images/hpp2_0203.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![Julia集合在-0.62772-0.42193i处](Images/hpp2_0203.png)'
- en: Figure 2-3\. Julia plot example using a pure gray scale
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-3\. 使用纯灰度绘制的Julia图例
- en: Simple Approaches to Timing—print and a Decorator
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间测量的简单方法——打印和装饰器
- en: After [Example 2-4](#profiling-juliaset-intro4), we saw the output generated
    by several `print` statements in our code. On Ian’s laptop, this code takes approximately
    8 seconds to run using CPython 3.7\. It is useful to note that execution time
    always varies. You must observe the normal variation when you’re timing your code,
    or you might incorrectly attribute an improvement in your code to what is simply
    a random variation in execution time.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例 2-4](#profiling-juliaset-intro4)之后，我们看到了代码中几个`print`语句生成的输出。在Ian的笔记本电脑上，此代码使用CPython
    3.7运行大约需要8秒。需要注意的是，执行时间总是会有所变化。在计时代码时，必须注意正常的变化，否则可能会错误地将代码改进归因为执行时间的随机变化。
- en: Your computer will be performing other tasks while running your code, such as
    accessing the network, disk, or RAM, and these factors can cause variations in
    the execution time of your program.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行代码时，您的计算机会执行其他任务，例如访问网络、磁盘或RAM，这些因素会导致程序执行时间的变化。
- en: Ian’s laptop is a Dell 9550 with an Intel Core I7 6700HQ (2.6 GHz, 6 MB cache,
    Quad Core with Hyperthreading) and 32 GB of RAM running Linux Mint 19.1 (Ubuntu
    18.04).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Ian的笔记本电脑是戴尔9550，配有Intel Core I7 6700HQ处理器（2.6 GHz，6 MB缓存，四核支持超线程）和32 GB RAM，运行Linux
    Mint 19.1（Ubuntu 18.04）。
- en: In `calc_pure_python` ([Example 2-2](#profiling-juliaset-intro2)), we can see
    several `print` statements. This is the simplest way to measure the execution
    time of a piece of code *inside* a function. It is a basic approach, but despite
    being quick and dirty, it can be very useful when you’re first looking at a piece
    of code.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在`calc_pure_python`中（[示例 2-2](#profiling-juliaset-intro2)），我们可以看到几个`print`语句。这是测量函数内部代码执行时间的最简单方法。虽然这是一种快速而不太精确的方法，但在初次查看代码时非常有用。
- en: Using `print` statements is commonplace when debugging and profiling code. It
    quickly becomes unmanageable but is useful for short investigations. Try to tidy
    up the `print` statements when you’re done with them, or they will clutter your
    `stdout`.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在调试和性能分析代码时，使用`print`语句很常见。它快速变得难以管理，但对于短期调查非常有用。在完成时，请尽量整理`print`语句，否则它们会杂乱你的`stdout`。
- en: A slightly cleaner approach is to use a *decorator*—here, we add one line of
    code above the function that we care about. Our decorator can be very simple and
    just replicate the effect of the `print` statements. Later, we can make it more
    advanced.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 一种稍微更清晰的方法是使用*装饰器*——在这里，我们在我们关心的函数上面添加一行代码。我们的装饰器可以非常简单，只需复制`print`语句的效果。稍后，我们可以使其更为高级。
- en: 'In [Example 2-5](#profiling-juliaset-timefn), we define a new function, `timefn`,
    which takes a function as an argument: the inner function, `measure_time`, takes
    `*args` (a variable number of positional arguments) and `**kwargs` (a variable
    number of key/value arguments) and passes them through to `fn` for execution.
    Around the execution of `fn`, we capture `time.time()` and then `print` the result
    along with `fn.__name__`. The overhead of using this decorator is small, but if
    you’re calling `fn` millions of times, the overhead might become noticeable. We
    use `@wraps(fn)` to expose the function name and docstring to the caller of the
    decorated function (otherwise, we would see the function name and docstring for
    the decorator, not the function it decorates).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例 2-5](#profiling-juliaset-timefn)中，我们定义了一个新的函数`timefn`，它将一个函数作为参数：内部函数`measure_time`接受`*args`（可变数量的位置参数）和`**kwargs`（可变数量的关键字参数），并将它们传递给`fn`以执行。在执行`fn`周围，我们捕获`time.time()`并且`print`结果以及`fn.__name__`。使用这个装饰器的开销很小，但如果你调用`fn`数百万次，这种开销可能会变得明显。我们使用`@wraps(fn)`来将函数名和文档字符串暴露给装饰函数的调用者（否则，我们将会看到装饰器的函数名和文档字符串，而不是它装饰的函数）。
- en: Example 2-5\. Defining a decorator to automate timing measurements
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 2-5\. 定义一个装饰器来自动化计时测量
- en: '[PRE10]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'When we run this version (we keep the `print` statements from before), we can
    see that the execution time in the decorated version is ever-so-slightly quicker
    than the call from `calc_pure_python`. This is due to the overhead of calling
    a function (the difference is very tiny):'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行这个版本（我们保留了以前的`print`语句）时，我们可以看到装饰版本的执行时间略微比从`calc_pure_python`调用快。这是由于调用函数的开销（差异非常微小）造成的。
- en: '[PRE11]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note
  id: totrans-81
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The addition of profiling information will inevitably slow down your code—some
    profiling options are very informative and induce a heavy speed penalty. The trade-off
    between profiling detail and speed will be something you have to consider.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 添加分析信息无疑会减慢代码执行速度——某些分析选项非常详细，并且会导致严重的速度惩罚。分析详细程度和速度之间的权衡是你必须考虑的事情。
- en: We can use the `timeit` module as another way to get a coarse measurement of
    the execution speed of our CPU-bound function. More typically, you would use this
    when timing different types of simple expressions as you experiment with ways
    to solve a problem.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`timeit`模块作为另一种获取CPU绑定函数执行速度粗略测量的方法。更典型的情况下，你会在尝试解决问题的不同方式时，使用它来计时不同类型的简单表达式。
- en: Warning
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: The `timeit` module temporarily disables the garbage collector. This might impact
    the speed you’ll see with real-world operations if the garbage collector would
    normally be invoked by your operations. See the [Python documentation](https://oreil.ly/2Zvyk)
    for help on this.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`timeit`模块暂时禁用垃圾回收器。如果垃圾回收器通常会被你的操作调用，那么这可能会影响你在实际操作中看到的速度。请参阅[Python文档](https://oreil.ly/2Zvyk)获取相关帮助。'
- en: 'From the command line, you can run `timeit` as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 从命令行，你可以像下面这样运行`timeit`：
- en: '[PRE12]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note that you have to import the module as a setup step using `-s`, as `calc_pure_python`
    is inside that module. `timeit` has some sensible defaults for short sections
    of code, but for longer-running functions it can be sensible to specify the number
    of loops (`-n 5`) and the number of repetitions (`-r 5`) to repeat the experiments.
    The best result of all the repetitions is given as the answer. Adding the verbose
    flag (`-v`) shows the cumulative time of all the loops by each repetition, which
    can help your variability in the results.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，你必须使用`-s`将模块导入为设置步骤，因为`calc_pure_python`就在那个模块里。对于长时间运行的函数，`timeit`有一些合理的默认设置，但对于更长时间的函数，可以明智地指定循环次数（`-n
    5`）和重复次数（`-r 5`）来重复实验。所有重复实验中的最佳结果将作为答案给出。添加详细标志（`-v`）会显示每个重复实验中所有循环的累积时间，这有助于了解结果的变化。
- en: By default, if we run `timeit` on this function without specifying `-n` and
    `-r`, it runs 10 loops with 5 repetitions, and this takes six minutes to complete.
    Overriding the defaults can make sense if you want to get your results a little
    faster.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，如果我们在这个函数上运行 `timeit` 而不指定 `-n` 和 `-r`，它会运行 10 次循环，每次重复 5 次，完成时间大约为 6
    分钟。如果你想更快地获得结果，覆盖默认设置可能是有意义的。
- en: 'We’re interested only in the best-case results, as other results will probably
    have been impacted by other processes:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只对最佳情况的结果感兴趣，因为其他结果可能已经受到其他进程的影响：
- en: '[PRE13]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Try running the benchmark several times to check if you get varying results—you
    may need more repetitions to settle on a stable fastest-result time. There is
    no “correct” configuration, so if you see a wide variation in your timing results,
    do more repetitions until your final result is stable.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试多次运行基准测试以检查是否得到不同的结果——你可能需要更多的重复次数来获得稳定的最快结果时间。没有“正确”的配置，所以如果你看到计时结果有很大的变化，请增加重复次数，直到最终结果稳定。
- en: Our results show that the overall cost of calling `calc_pure_python` is 8.45
    seconds (as the best case), while single calls to `calculate_z_serial_purepython`
    take 8.0 seconds as measured by the `@timefn` decorator. The difference is mainly
    the time taken to create the `zs` and `cs` lists.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的结果显示，调用 `calc_pure_python` 的总耗时为 8.45 秒（作为最佳情况），而单个调用 `calculate_z_serial_purepython`
    由 `@timefn` 装饰器测量的时间为 8.0 秒。差异主要是用于创建 `zs` 和 `cs` 列表的时间。
- en: 'Inside IPython, we can use the magic `%timeit` in the same way. If you are
    developing your code interactively in IPython or in a Jupyter Notebook, you can
    use this:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在 IPython 中，我们可以以同样的方式使用魔术 `%timeit`。如果你在 IPython 或 Jupyter Notebook 中交互式地开发代码，可以使用这个：
- en: '[PRE14]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Warning
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Be aware that “best” is calculated differently by the `timeit.py` approach and
    the `%timeit` approach in Jupyter and IPython. `timeit.py` uses the minimum value
    seen. IPython in 2016 switched to using the mean and standard deviation. Both
    methods have their flaws, but generally they’re both “reasonably good”; you can’t
    compare between them, though. Use one method or the other; don’t mix them.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 要注意，“最佳”是由 `timeit.py` 方法和 Jupyter 和 IPython 中 `%timeit` 方法不同计算得出的。`timeit.py`
    使用最小值。2016 年起，IPython 改为使用均值和标准差。两种方法都有其缺陷，但总体上它们都“相当不错”；尽管如此，不能直接比较它们。选择一种方法使用，不要混用。
- en: It is worth considering the variation in load that you get on a normal computer.
    Many background tasks are running (e.g., Dropbox, backups) that could impact the
    CPU and disk resources at random. Scripts in web pages can also cause unpredictable
    resource usage. [Figure 2-4](#FIG-julia-set-system-monitor-trimmed) shows the
    single CPU being used at 100% for some of the timing steps we just performed;
    the other cores on this machine are each lightly working on other tasks.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 值得考虑的是，普通计算机的负载变化。许多后台任务正在运行（例如 Dropbox、备份），可能会随机影响 CPU 和磁盘资源。网页中的脚本也可能导致不可预测的资源使用。[图 2-4](#FIG-julia-set-system-monitor-trimmed)
    显示此计算机上单个 CPU 在我们刚刚执行的某些计时步骤中使用了 100%，而该机器上的其他核心则轻度工作于其他任务。
- en: '![System Monitor (Ubuntu) showing background CPU usage during timings](Images/hpp2_0204.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![Ubuntu 中的系统监视器显示定时期间的后台 CPU 使用情况](Images/hpp2_0204.png)'
- en: Figure 2-4\. System Monitor on Ubuntu showing variation in background CPU usage
    while we time our function
  id: totrans-100
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-4\. Ubuntu 上的系统监视器显示我们计时函数时后台 CPU 使用变化
- en: Occasionally, the System Monitor shows spikes of activity on this machine. It
    is sensible to watch your System Monitor to check that nothing else is interfering
    with your critical resources (CPU, disk, network).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，系统监视器显示此计算机上的活动突然增加。检查系统监视器以确保没有其他东西干扰你的关键资源（CPU、磁盘、网络）是明智的做法。
- en: Simple Timing Using the Unix time Command
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Unix 时间命令进行简单计时
- en: 'We can step outside of Python for a moment to use a standard system utility
    on Unix-like systems. The following will record various views on the execution
    time of your program, and it won’t care about the internal structure of your code:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以暂时离开 Python，使用 Unix-like 系统上的标准系统实用程序。以下内容将记录程序的执行时间的各种视图，并且不会关心代码的内部结构：
- en: '[PRE15]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note that we specifically use `/usr/bin/time` rather than `time` so we get the
    system’s `time` and not the simpler (and less useful) version built into our shell.
    If you try `time --verbose` quick-and-dirty get an error, you’re probably looking
    at the shell’s built-in `time` command and not the system command.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们特别使用 `/usr/bin/time` 而不是 `time`，以便获取系统的 `time` 而不是我们的 shell 中较简单（且不太有用）的版本。如果你尝试
    `time --verbose` 时出现错误，你可能正在看 shell 的内置 `time` 命令，而不是系统命令。
- en: 'Using the `-p` portability flag, we get three results:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `-p` 可移植性标志，我们得到三个结果：
- en: '`real` records the wall clock or elapsed time.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`real` 记录了实际经过的墙上或经过的时间。'
- en: '`user` records the amount of time the CPU spent on your task outside of kernel
    functions.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`user` 记录了 CPU 在执行您的任务时在内核函数之外所花费的时间。'
- en: '`sys` records the time spent in kernel-level functions.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sys` 记录了在内核级别函数中花费的时间。'
- en: By adding `user` and `sys`, you get a sense of how much time was spent in the
    CPU. The difference between this and `real` might tell you about the amount of
    time spent waiting for I/O; it might also suggest that your system is busy running
    other tasks that are distorting your measurements.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 通过添加 `user` 和 `sys`，您可以了解 CPU 花费了多少时间。这与 `real` 的差异可能告诉您有多少时间花在等待 I/O 上；这也可能表明您的系统正在忙于运行其他扭曲您测量的任务。
- en: '`time` is useful because it isn’t specific to Python. It includes the time
    taken to start the `python` executable, which might be significant if you start
    lots of fresh processes (rather than having a long-running single process). If
    you often have short-running scripts where the startup time is a significant part
    of the overall runtime, then `time` can be a more useful measure.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`time` 非常有用，因为它不特定于 Python。它包括启动 `python` 可执行文件所花费的时间，如果您启动了大量的新进程（而不是运行长时间的单个进程），这可能是重要的。如果您经常运行短暂的脚本，启动时间可能是整体运行时间的一个重要部分，那么
    `time` 可以是一个更有用的度量。'
- en: 'We can add the `--verbose` flag to get even more output:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以添加 `--verbose` 标志来获得更多输出：
- en: '[PRE16]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Probably the most useful indicator here is `Major (requiring I/O) page faults`,
    as this indicates whether the operating system is having to load pages of data
    from the disk because the data no longer resides in RAM. This will cause a speed
    penalty.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这里可能最有用的指标是 `Major (requiring I/O) page faults`，因为这表明操作系统是否不得不从磁盘加载数据页，因为数据不再驻留在
    RAM 中。这将导致速度降低。
- en: In our example, the code and data requirements are small, so no page faults
    occur. If you have a memory-bound process, or several programs that use variable
    and large amounts of RAM, you might find that this gives you a clue as to which
    program is being slowed down by disk accesses at the operating system level because
    parts of it have been swapped out of RAM to disk.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，代码和数据要求很小，因此没有页面错误发生。如果您有一个内存绑定的进程，或者有几个使用变量和大量 RAM 的程序，您可能会发现这为您提供了一个线索，即由于它的部分已经被交换到磁盘而减慢了操作系统级别的磁盘访问速度。
- en: Using the cProfile Module
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 cProfile 模块
- en: '`cProfile` is a built-in profiling tool in the standard library. It hooks into
    the virtual machine in CPython to measure the time taken to run every function
    that it sees. This introduces a greater overhead, but you get correspondingly
    more information. Sometimes the additional information can lead to surprising
    insights into your code.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`cProfile` 是标准库中内置的性能分析工具。它连接到 CPython 虚拟机，测量每个函数的运行时间。这会引入更大的开销，但您将得到更多对应的信息。有时，额外的信息可以带来对代码的惊人洞察。'
- en: '`cProfile` is one of two profilers in the standard library, alongside `profile`.
    `profile` is the original and slower pure Python profiler; `cProfile` has the
    same interface as `profile` and is written in `C` for a lower overhead. If you’re
    curious about the history of these libraries, see [Armin Rigo’s 2005 request](http://bit.ly/cProfile_request)
    to include `cProfile` in the standard library.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`cProfile` 是标准库中的两个性能分析器之一，另一个是 `profile`。`profile` 是原始的纯 Python 性能分析器，速度较慢；`cProfile`
    具有与 `profile` 相同的接口，但由于用 `C` 编写，开销较低。如果您对这些库的历史感兴趣，请参见 [Armin Rigo''s 2005 request](http://bit.ly/cProfile_request)
    来包含 `cProfile` 在标准库中。'
- en: A good practice when profiling is to generate a *hypothesis* about the speed
    of parts of your code before you profile it. Ian likes to print out the code snippet
    in question and annotate it. Forming a hypothesis ahead of time means you can
    measure how wrong you are (and you will be!) and improve your intuition about
    certain coding styles.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析性能时一个好的实践是在分析之前对您的代码的速度生成一个*假设*。Ian 喜欢打印出相关的代码片段并加以注释。提前形成假设意味着您可以测量您的错误程度（而您将会！）并改进您对某些编码风格的直觉。
- en: Warning
  id: totrans-120
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: You should never avoid profiling in favor of a gut instinct (we warn you—you
    *will* get it wrong!). It is definitely worth forming a hypothesis ahead of profiling
    to help you learn to spot possible slow choices in your code, and you should always
    back up your choices with evidence.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 您绝不应该避免使用性能分析来取代直觉（我们警告您——您*肯定会*错！）。提前形成假设以帮助您学会识别代码中可能慢的选择是绝对值得的，您应该始终用证据支持您的选择。
- en: Always be driven by results that you have measured, and always start with some
    quick-and-dirty profiling to make sure you’re addressing the right area. There’s
    nothing more humbling than cleverly optimizing a section of code only to realize
    (hours or days later) that you missed the slowest part of the process and haven’t
    really addressed the underlying problem at all.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 始终依据你测量到的结果进行驱动，并始终从一些快速而粗糙的分析开始，以确保你正在解决正确的问题。没有比聪明地优化代码的一个部分，然后意识到（数小时或数天后）你错过了最慢的部分，实际上根本没有解决底层问题更令人感到羞愧的事情了。
- en: Let’s hypothesize that `calculate_z_serial_purepython` is the slowest part of
    the code. In that function, we do a lot of dereferencing and make many calls to
    basic arithmetic operators and the `abs` function. These will probably show up
    as consumers of CPU resources.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 假设`calculate_z_serial_purepython`是代码中最慢的部分。在这个函数中，我们进行了大量的解引用操作，并调用了许多基本算术运算符和`abs`函数。这些可能会显示为CPU资源的消耗者。
- en: Here, we’ll use the `cProfile` module to run a variant of the code. The output
    is spartan but helps us figure out where to analyze further.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用`cProfile`模块来运行代码的一个变体。输出很简单，但帮助我们找出需要进一步分析的地方。
- en: 'The `-s cumulative` flag tells `cProfile` to sort by cumulative time spent
    inside each function; this gives us a view into the slowest parts of a section
    of code. The `cProfile` output is written to screen directly after our usual `print`
    results:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`-s cumulative`标志告诉`cProfile`按累积时间对每个函数进行排序；这使我们能够查看代码部分中最慢的部分。`cProfile`的输出直接写入屏幕，紧随我们通常的`print`结果之后。'
- en: '[PRE17]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Sorting by cumulative time gives us an idea about where the majority of execution
    time is spent. This result shows us that 36,221,995 function calls occurred in
    just over 12 seconds (this time includes the overhead of using `cProfile`). Previously,
    our code took around 8 seconds to execute—we’ve just added a 4-second penalty
    by measuring how long each function takes to execute.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 按累积时间排序使我们对代码中大部分执行时间都花在哪里有了一个概念。这个结果显示，在12秒多的时间里，执行了36221995次函数调用（这个时间包括使用`cProfile`的开销）。以前，我们的代码执行大约需要8秒钟，现在我们通过测量每个函数执行的时间增加了4秒的惩罚。
- en: We can see that the entry point to the code `julia1_nopil.py` on line 1 takes
    a total of 12 seconds. This is just the `__main__` call to `calc_pure_python`.
    `ncalls` is 1, indicating that this line is executed only once.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到代码的入口点`julia1_nopil.py`在第1行上总共花费了12秒钟。这只是对`calc_pure_python`的`__main__`调用。`ncalls`为1，表示这行代码只执行了一次。
- en: Inside `calc_pure_python`, the call to `calculate_z_serial_purepython` consumes
    11 seconds. Both functions are called only once. We can derive that approximately
    1 second is spent on lines of code inside `calc_pure_python`, separate to calling
    the CPU-intensive `calculate_z_serial_purepython` function. However, we can’t
    derive *which* lines take the time inside the function using `cProfile`.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在`calc_pure_python`内部，调用`calculate_z_serial_purepython`耗时11秒。这两个函数只被调用一次。我们可以推断，在`calc_pure_python`内部的代码行大约花费了1秒钟的时间，与调用CPU密集型的`calculate_z_serial_purepython`函数是分开的。然而，我们无法通过`cProfile`确定在函数内部哪些行花费了时间。
- en: Inside `calculate_z_serial_purepython`, the time spent on lines of code (without
    calling other functions) is 8 seconds. This function makes 34,219,980 calls to
    `abs`, which take a total of 3 seconds, along with other calls that do not cost
    much time.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在`calculate_z_serial_purepython`内部，代码行的时间（不调用其他函数）是8秒。这个函数调用了34219980次`abs`，总共耗时3秒，还有其他调用时间不多的函数。
- en: What about the `{abs}` call? This line is measuring the individual calls to
    the `abs` function inside `calculate_z_serial_purepython`. While the per-call
    cost is negligible (it is recorded as 0.000 seconds), the total time for 34,219,980
    calls is 3 seconds. We couldn’t predict in advance exactly how many calls would
    be made to `abs`, as the Julia function has unpredictable dynamics (that’s why
    it is so interesting to look at).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 那么`{abs}`调用呢？这行代码测量了`calculate_z_serial_purepython`内部调用`abs`函数的次数。虽然每次调用的成本微不足道（记录为0.000秒），但34219980次调用的总时间为3秒。我们无法预测到`abs`会被调用多少次，因为Julia函数具有不可预测的动态特性（这也是我们感兴趣的原因）。
- en: At best we could have said that it will be called a minimum of 1 million times,
    as we’re calculating `1000*1000` pixels. At most it will be called 300 million
    times, as we calculate 1,000,000 pixels with a maximum of 300 iterations. So 34
    million calls is roughly 10% of the worst case.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最好可以说，它将至少被调用100万次，因为我们正在计算`1000*1000`像素。在最坏的情况下，它将被调用3亿次，因为我们计算100万像素最多300次迭代。因此，3420万次调用大约占最坏情况的10%。
- en: If we look at the original grayscale image ([Figure 2-3](#FIG-julia-example-greyscale))
    and, in our mind’s eye, squash the white parts together and into a corner, we
    can estimate that the expensive white region accounts for roughly 10% of the rest
    of the image.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看原始的灰度图像（[图 2-3](#FIG-julia-example-greyscale)）并将白色部分压缩在一起并挤入一个角落，我们可以估计昂贵的白色区域大约占了图像的其余部分的
    10%。
- en: The next line in the profiled output, `{method 'append' of 'list' objects}`,
    details the creation of 2,002,000 list items.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 分析输出中的下一行，`{method 'append' of 'list' objects}`，详细描述了创建了 2,002,000 个列表项。
- en: Tip
  id: totrans-135
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Why 2,002,000 items? Before you read on, think about how many list items are
    being constructed.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么是 2,002,000 个项目？在继续阅读之前，请思考一下有多少个列表项正在被构建。
- en: This creation of 2,002,000 items is occurring in `calc_pure_python` during the
    setup phase.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置阶段，正在发生 2,002,000 个项目的创建，这发生在 `calc_pure_python` 中。
- en: The `zs` and `cs` lists will be `1000*1000` items each (generating 1,000,000
    * 2 calls), and these are built from a list of 1,000 *x* and 1,000 *y* coordinates.
    In total, this is 2,002,000 calls to append.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '`zs` 和 `cs` 列表将分别为 `1000*1000` 个项目（生成 1,000,000 * 2 次调用），这些列表是从 1,000 *x* 和
    1,000 *y* 坐标的列表构建的。总共，这是 2,002,000 次调用 `append`。'
- en: It is important to note that this `cProfile` output is not ordered by parent
    functions; it is summarizing the expense of all functions in the executed block
    of code. Figuring out what is happening on a line-by-line basis is very hard with
    `cProfile`, as we get profile information only for the function calls themselves,
    not for each line within the functions.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，这个 `cProfile` 输出不是按父函数排序的；它在执行的代码块中总结了所有函数的开销。使用 `cProfile` 逐行了解发生的情况非常困难，因为我们只获取函数调用本身的概要信息，而不是函数内的每一行。
- en: Inside `calculate_z_serial_purepython`, we can account for `{abs}`, and in total
    this function costs approximately 3.1 seconds. We know that `calculate_z_serial_purepython`
    costs 11.4 seconds in total.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `calculate_z_serial_purepython` 中，我们可以考虑 `{abs}`，总体而言，这个函数的成本约为 3.1 秒。我们知道
    `calculate_z_serial_purepython` 总共花费了 11.4 秒。
- en: The final line of the profiling output refers to `lsprof`; this is the original
    name of the tool that evolved into `cProfile` and can be ignored.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 分析输出的最后一行提到了 `lsprof`；这是演变为 `cProfile` 的工具的原始名称，可以忽略不计。
- en: 'To get more control over the results of `cProfile`, we can write a statistics
    file and then analyze it in Python:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 要更好地控制 `cProfile` 的结果，我们可以编写一个统计文件，然后在 Python 中进行分析：
- en: '[PRE18]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We can load this into Python as follows, and it will give us the same cumulative
    time report as before:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按照以下方式将其加载到 Python 中，它会给我们与之前相同的累积时间报告：
- en: '[PRE19]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'To trace which functions we’re profiling, we can print the caller information.
    In the following two listings we can see that `calculate_z_serial_purepython`
    is the most expensive function, and it is called from one place. If it were called
    from many places, these listings might help us narrow down the locations of the
    most expensive parents:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 要追踪我们正在分析的函数，我们可以打印调用者信息。在以下两个列表中，我们可以看到 `calculate_z_serial_purepython` 是最昂贵的函数，它只从一个地方调用。如果它被多处调用，这些列表可能会帮助我们缩小最昂贵父函数的位置：
- en: '[PRE20]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We can flip this around the other way to show which functions call other functions:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个过程反过来，显示哪些函数调用其他函数：
- en: '[PRE21]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '`cProfile` is rather verbose, and you need a side screen to see it without
    lots of word wrapping. Since it is built in, though, it is a convenient tool for
    quickly identifying bottlenecks. Tools like `line_profiler` and `memory_profiler`,
    which we discuss later in this chapter, will then help you to drill down to the
    specific lines that you should pay attention to.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`cProfile` 相当冗长，如果不进行很多换行，你需要一个侧屏才能看到它。然而，由于它是内置的，因此它是一个快速识别瓶颈的方便工具。像 `line_profiler`
    和 `memory_profiler` 这样的工具，我们稍后在本章中讨论，将帮助您深入到您应该关注的特定行。'
- en: Visualizing cProfile Output with SnakeViz
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 SnakeViz 可视化 cProfile 输出
- en: '`snakeviz` is a visualizer that draws the output of `cProfile` as a diagram
    in which larger boxes are areas of code that take longer to run. It replaces the
    older `runsnake` tool.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '`snakeviz` 是一个可视化工具，它将 `cProfile` 的输出绘制为一个图表，其中较大的框表示运行时间较长的代码区域。它取代了较旧的 `runsnake`
    工具。'
- en: Use `snakeviz` to get a high-level understanding of a `cProfile` statistics
    file, particularly if you’re investigating a new project for which you have little
    intuition. The diagram will help you visualize the CPU-usage behavior of the system,
    and it may highlight areas that you hadn’t expected to be expensive.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `snakeviz` 可以对 `cProfile` 统计文件进行高层次的理解，特别是当你在调查一个你对其了解较少的新项目时。该图表将帮助你可视化系统的
    CPU 使用行为，并且可能突显出你未曾预料到的开销较大的部分。
- en: To install SnakeViz, use `$ pip install snakeviz`.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装SnakeViz，请使用 `$ pip install snakeviz`。
- en: In [Figure 2-5](#FIG-snakeviz) we have the visual output of the *profile.stats*
    file we’ve just generated. The entry point for the program is shown at the top
    of the diagram. Each layer down is a function called from the function above.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图2-5](#FIG-snakeviz) 中，我们有刚刚生成的 *profile.stats* 文件的视觉输出。图表顶部显示了程序的入口点。每个向下的层级表示从上面的函数调用的函数。
- en: The width of the diagram represents the entire time taken by the program’s execution.
    The fourth layer shows that the majority of the time is spent in `calculate_z_serial_purepython`.
    The fifth layer breaks this down some more—the unannotated block to the right
    occupying approximately 25% of that layer represents the time spent in the `abs`
    function. Seeing these larger blocks quickly brings home how the time is spent
    inside your program.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图表的宽度代表程序执行所需的整体时间。第四层显示大部分时间花费在 `calculate_z_serial_purepython` 中。第五层进一步分解了这一点——右侧未注释的区块大约占据了该层的
    25%，代表在 `abs` 函数中花费的时间。快速看到这些较大的块可以迅速理解程序内部时间的分配情况。
- en: '![snakeviz visualizing profile.stats](Images/hpp2_0205.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![snakeviz 可视化 profile.stats](Images/hpp2_0205.png)'
- en: Figure 2-5\. `snakeviz` visualizing profile.stats
  id: totrans-158
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-5\. `snakeviz` 可视化 profile.stats
- en: The next section down shows a table that is a pretty-printed version of the
    statistics we’ve just been looking at, which you can sort by `cumtime` (cumulative
    time), `percall` (cost per call), or `ncalls` (number of calls altogether), among
    other categories. Starting with `cumtime` will tell you which functions cost the
    most overall. They’re a pretty good place to start your investigations.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节显示的表格是我们刚刚查看过的统计数据的漂亮打印版本，你可以按 `cumtime`（累计时间）、`percall`（每次调用成本）或 `ncalls`（总调用次数）等分类进行排序。从
    `cumtime` 开始将告诉你哪些函数总体成本最高。它们是开始调查的一个很好的起点。
- en: If you’re comfortable looking at tables, the console output for `cProfile` may
    be adequate for you. To communicate to others, we strongly suggest you use diagrams—such
    as this output from `snakeviz`—to help others quickly understand the point you’re
    making.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你习惯查看表格，`cProfile` 的控制台输出可能已经足够了。为了与他人交流，我们强烈建议使用图表，比如来自 `snakeviz` 的此类输出，以帮助其他人快速理解你的观点。
- en: Using line_profiler for Line-by-Line Measurements
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 `line_profiler` 进行逐行测量
- en: In Ian’s opinion, Robert Kern’s `line_profiler` is the strongest tool for identifying
    the cause of CPU-bound problems in Python code. It works by profiling individual
    functions on a line-by-line basis, so you should start with `cProfile` and use
    the high-level view to guide which functions to profile with `line_profiler`.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 依据伊恩的观点，Robert Kern 的 `line_profiler` 是识别 Python 代码中 CPU 绑定问题原因的最强工具。它通过逐行分析单个函数的性能来工作，因此你应该从
    `cProfile` 开始，并使用高层次视图指导选择要使用 `line_profiler` 进行分析的函数。
- en: It is worthwhile printing and annotating versions of the output from this tool
    as you modify your code, so you have a record of changes (successful or not) that
    you can quickly refer to. Don’t rely on your memory when you’re working on line-by-line
    changes.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 当你修改代码时，值得打印并注释此工具输出的版本，这样你就能记录下（成功或失败的）更改，以便快速查阅。在逐行更改代码时，不要依赖于你的记忆。
- en: To install `line_profiler`, issue the command `pip install line_profiler`.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装 `line_profiler`，请输入命令 `pip install line_profiler`。
- en: A decorator (`@profile`) is used to mark the chosen function. The `kernprof`
    script is used to execute your code, and the CPU time and other statistics for
    each line of the chosen function are recorded.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 用装饰器 (`@profile`) 标记选择的函数。使用 `kernprof` 脚本来执行你的代码，并记录选定函数每行的 CPU 时间和其他统计信息。
- en: The arguments are `-l` for line-by-line (rather than function-level) profiling
    and `-v` for verbose output. Without `-v`, you receive an *.lprof* output that
    you can later analyze with the `line_profiler` module. In [Example 2-6](#profiling-kernprof-run1),
    we’ll do a full run on our CPU-bound function.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 参数`-l`表示按行（而不是函数级别）进行分析，`-v`表示详细输出。如果没有`-v`，你将得到一个*.lprof*输出，稍后可以使用`line_profiler`模块进行分析。在[示例 2-6](#profiling-kernprof-run1)中，我们将对我们的CPU绑定函数进行全面运行。
- en: Example 2-6\. Running `kernprof` with line-by-line output on a decorated function
    to record the CPU cost of each line’s execution
  id: totrans-167
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 2-6\. 运行`kernprof`以记录装饰函数的逐行CPU执行成本
- en: '[PRE22]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Introducing `kernprof.py` adds a substantial amount to the runtime. In this
    example, `calculate_z_serial_purepython` takes 49 seconds; this is up from 8 seconds
    using simple `print` statements and 12 seconds using `cProfile`. The gain is that
    we get a line-by-line breakdown of where the time is spent inside the function.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 引入`kernprof.py`会显著增加运行时间。在这个例子中，`calculate_z_serial_purepython`花费了49秒；这比简单使用`print`语句的8秒和使用`cProfile`的12秒要长。好处在于我们可以得到函数内部时间分布的逐行详细信息。
- en: The `% Time` column is the most helpful—we can see that 38% of the time is spent
    on the `while` testing. We don’t know whether the first statement (`abs(z) < 2`)
    is more expensive than the second (`n < maxiter`), though. Inside the loop, we
    see that the update to `z` is also fairly expensive. Even `n += 1` is expensive!
    Python’s dynamic lookup machinery is at work for every loop, even though we’re
    using the same types for each variable in each loop—this is where compiling and
    type specialization ([Chapter 7](ch07.xhtml#chapter-compiling)) give us a massive
    win. The creation of the `output` list and the updates on line 20 are relatively
    cheap compared to the cost of the `while` loop.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`% Time`列是最有帮助的——我们可以看到38%的时间用于`while`测试上。我们不知道第一个语句（`abs(z) < 2`）是否比第二个语句（`n
    < maxiter`）更昂贵。在循环内部，我们还看到对`z`的更新也相当昂贵。甚至`n += 1`也很昂贵！Python的动态查找机制在每个循环中都在工作，即使我们在每个循环中使用相同的类型来定义每个变量——这就是编译和类型专门化（[第7章](ch07.xhtml#chapter-compiling)）给我们带来巨大优势的地方。与`while`循环的成本相比，`output`列表的创建和第20行的更新相对较便宜。'
- en: If you haven’t thought about the complexity of Python’s dynamic machinery before,
    do think about what happens in that `n += 1` operation. Python has to check that
    the `n` object has an `__add__` function (and if it didn’t, it’d walk up any inherited
    classes to see if they provided this functionality), and then the other object
    (`1` in this case) is passed in so that the `__add__` function can decide how
    to handle the operation. Remember that the second argument might be a `float`
    or other object that may or may not be compatible. This all happens dynamically.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你之前没有考虑过Python动态机制的复杂性，请考虑一下`n += 1`操作会发生什么。Python必须检查`n`对象是否有`__add__`函数（如果没有，它会沿袭任何继承类来查看它们是否提供此功能），然后将另一个对象（在本例中为`1`）传入，以便`__add__`函数可以决定如何处理操作。请记住，第二个参数可能是一个`float`或其他可能与之兼容或不兼容的对象。所有这些都是动态发生的。
- en: The obvious way to further analyze the `while` statement is to break it up.
    While there has been some discussion in the Python community around the idea of
    rewriting the *.pyc* files with more detailed information for multipart, single-line
    statements, we are unaware of any production tools that offer a more fine-grained
    analysis than `line_profiler`.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步分析`while`语句的明显方法是将其分解。虽然在Python社区中已经有一些关于重写*.pyc*文件以提供更详细的多部分单行语句信息的讨论，但我们不知道任何生产工具比`line_profiler`提供更精细的分析。
- en: In [Example 2-7](#profiling-kernprof-run2), we break the `while` logic into
    several statements. This additional complexity will increase the runtime of the
    function, as we have more lines of code to execute, but it *might* also help us
    understand the costs incurred in this part of the code.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例 2-7](#profiling-kernprof-run2)中，我们将`while`逻辑分解为几个语句。这种额外的复杂性将增加函数的运行时间，因为我们需要执行更多的代码行，但这*可能*也有助于我们理解代码的这部分所产生的成本。
- en: Tip
  id: totrans-174
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: '*Before you look at the code*, do you think we’ll learn about the costs of
    the fundamental operations this way? Might other factors complicate the analysis?'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '*在查看代码之前*，你认为我们能通过这种方式学习到基本操作的成本吗？其他因素可能会使分析复杂化吗？'
- en: Example 2-7\. Breaking the compound `while` statement into individual statements
    to record the cost of each part of the original statement
  id: totrans-176
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 2-7\. 将复合`while`语句分解为单独的语句，记录原始语句的每个部分的成本
- en: '[PRE23]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This version takes 82 seconds to execute, while the previous version took 49
    seconds. Other factors *did* complicate the analysis. In this case, having extra
    statements that have to be executed 34,219,980 times each slows down the code.
    If we hadn’t used `kernprof.py` to investigate the line-by-line effect of this
    change, we might have drawn other conclusions about the reason for the slowdown,
    as we’d have lacked the necessary evidence.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这个版本的执行时间为82秒，而之前的版本为49秒。其他因素确实*使分析变得复杂*。在这种情况下，必须执行的额外语句次数为34,219,980次，导致代码变慢。如果我们没有使用`kernprof.py`逐行调查这一变化的影响，我们可能会对减速原因得出其他结论，因为我们缺乏必要的证据。
- en: 'At this point it makes sense to step back to the earlier `timeit` technique
    to test the cost of individual expressions:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在回到之前使用`timeit`技术来测试单个表达式的成本是有意义的：
- en: '[PRE24]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: From this simple analysis, it looks as though the logic test on `n` is more
    than two times faster than the call to `abs`. Since the order of evaluation for
    Python statements is both left to right and opportunistic, it makes sense to put
    the cheapest test on the left side of the equation. On 1 in every 301 tests for
    each coordinate, the `n < maxiter` test will be `False`, so Python wouldn’t need
    to evaluate the other side of the `and` operator.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个简单的分析中看来，对于Python语句的逻辑测试来说，`n`的逻辑测试速度要比调用`abs`快两倍以上。由于Python语句的评估顺序是从左到右且机会主义的，因此将最便宜的测试放在等式的左侧是有道理的。对于每个坐标的301次测试中的1次，`n
    < maxiter`测试将为`False`，因此Python不需要评估`and`运算符的另一侧。
- en: We never know whether `abs(z) < 2` will be `False` until we evaluate it, and
    our earlier observations for this region of the complex plane suggest it is `True`
    around 10% of the time for all 300 iterations. If we wanted to have a strong understanding
    of the time complexity of this part of the code, it would make sense to continue
    the numerical analysis. In this situation, however, we want an easy check to see
    if we can get a quick win.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在评估`abs(z) < 2`之前永远不知道它是否为`False`，并且我们对复平面这一区域的早期观察表明，在300次迭代中，大约10%的时间是`True`。如果我们想对代码这部分的时间复杂度有一个深入的理解，继续数值分析是有意义的。然而，在这种情况下，我们希望进行一个简单的检查，看看我们是否能够迅速获得胜利。
- en: We can form a new hypothesis stating, “By swapping the order of the operators
    in the `while` statement, we will achieve a reliable speedup.” We *can* test this
    hypothesis using `kernprof`, but the additional overheads of profiling this way
    might add too much noise. Instead, we can use an earlier version of the code,
    running a test comparing `while abs(z) < 2 and n < maxiter:` against `while n
    < maxiter and abs(z) < 2:`, which we see in [Example 2-8](#profiling-kernprof-run3).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以提出一个新的假设：“通过交换`while`语句中运算符的顺序，我们将实现可靠的加速。”我们*可以*使用`kernprof`来测试这个假设，但是用这种方式进行分析会增加太多的额外开销。相反，我们可以使用代码的早期版本，运行一个测试，比较`while
    abs(z) < 2 and n < maxiter:`与`while n < maxiter and abs(z) < 2:`，我们在[示例2-8](#profiling-kernprof-run3)中看到了这个例子。
- en: Running the two variants *outside* of `line_profiler` means they run at similar
    speeds. The overheads of `line_profiler` also confuse the result, and the results
    on line 17 for both versions are similar. We should reject the hypothesis that
    in Python 3.7 changing the order of the logic results in a consistent speedup—there’s
    no clear evidence for this. Ian notes that with Python 2.7 we *could* accept this
    hypothesis, but with Python 3.7 that’s no longer the case.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在`line_profiler`之外运行这两个变体意味着它们以类似的速度运行。`line_profiler`的开销也会扰乱结果，并且第17行的两个版本的结果相似。我们应该拒绝在Python
    3.7中改变逻辑顺序会导致一致加速的假设——没有明确的证据支持这一点。Ian指出，对于Python 2.7，我们*可以*接受这个假设，但在Python 3.7中不再适用。
- en: Using a more suitable approach to solve this problem (e.g., swapping to using
    Cython or PyPy, as described in [Chapter 7](ch07.xhtml#chapter-compiling)) would
    yield greater gains.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 采用更合适的方法来解决这个问题（例如，切换到使用Cython或PyPy，如[第7章](ch07.xhtml#chapter-compiling)中描述的）将会带来更大的收益。
- en: 'We can be confident in our result because of the following:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对结果感到自信，因为以下原因：
- en: We stated a hypothesis that was easy to test.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们提出了一个易于测试的假设。
- en: We changed our code so that only the hypothesis would be tested (never test
    two things at once!).
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们修改了代码，以便只测试假设（永远不要同时测试两个事物！）。
- en: We gathered enough evidence to support our conclusion.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们收集了足够的证据来支持我们的结论。
- en: For completeness, we can run a final `kernprof` on the two main functions including
    our optimization to confirm that we have a full picture of the overall complexity
    of our code.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 为了全面，我们可以对包括我们的优化在内的两个主要函数运行最后的 `kernprof`，以确认我们对代码的整体复杂性有一个完整的理解。
- en: Example 2-8\. Swapping the order of the compound `while` statement makes the
    function fractionally faster
  id: totrans-191
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 2-8\. 交换复合 `while` 语句的顺序使函数稍微更快
- en: '[PRE25]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: As expected, we can see from the output in [Example 2-9](#profiling-kernprof-run4)
    that `calculate_z_serial_purepython` takes most (97%) of the time of its parent
    function. The list-creation steps are minor in comparison.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，从 [示例 2-9](#profiling-kernprof-run4) 的输出中，我们可以看到 `calculate_z_serial_purepython`
    在其父函数中占用了大部分（97%）的时间。与此相比，列表创建步骤则较小。
- en: Example 2-9\. Testing the line-by-line costs of the setup routine
  id: totrans-194
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 2-9\. 测试设置程序的逐行成本
- en: '[PRE26]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '`line_profiler` gives us a great insight into the cost of lines inside loops
    and expensive functions; even though profiling adds a speed penalty, it is a great
    boon to scientific developers. Remember to use representative data to make sure
    you’re focusing on the lines of code that’ll give you the biggest win.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`line_profiler` 为我们提供了有关循环内行和昂贵函数成本的深刻见解；尽管分析会增加速度惩罚，但它对科学开发人员是一个巨大的帮助。记得使用代表性数据来确保你专注于能为你带来最大收益的代码行。'
- en: Using memory_profiler to Diagnose Memory Usage
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 `memory_profiler` 诊断内存使用
- en: 'Just as Robert Kern’s `line_profiler` package measures CPU usage, the `memory_profiler`
    module by Fabian Pedregosa and Philippe Gervais measures memory usage on a line-by-line
    basis. Understanding the memory usage characteristics of your code allows you
    to ask yourself two questions:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 Robert Kern 的 `line_profiler` 包测量 CPU 使用情况一样，Fabian Pedregosa 和 Philippe
    Gervais 的 `memory_profiler` 模块按行测量内存使用情况。了解代码的内存使用特性使您可以问自己两个问题：
- en: Could we use *less* RAM by rewriting this function to work more efficiently?
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过重写这个函数以更高效地工作，我们能否使用*更少*的 RAM？
- en: Could we use *more* RAM and save CPU cycles by caching?
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过缓存，我们能否使用*更多*的 RAM 并节省 CPU 周期？
- en: '`memory_profiler` operates in a very similar way to `line_profiler` but runs
    far more slowly. If you install the `psutil` package (optional but recommended),
    `memory_profiler` will run faster. Memory profiling may easily make your code
    run 10 to 100 times slower. In practice, you will probably use `memory_profiler`
    occasionally and `line_profiler` (for CPU profiling) more frequently.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '`memory_profiler` 的操作方式与 `line_profiler` 非常相似，但运行速度慢得多。如果安装了 `psutil` 包（可选但建议安装），`memory_profiler`
    将运行更快。内存分析可能会使您的代码运行速度慢 10 到 100 倍。在实际应用中，你可能偶尔会使用 `memory_profiler`，而更频繁地使用 `line_profiler`（用于
    CPU 分析）。'
- en: Install `memory_profiler` with the command `pip install memory_profiler` (and
    optionally with `pip install psutil`).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 使用命令 `pip install memory_profiler` 安装 `memory_profiler`（可选的还可以用 `pip install
    psutil`）。
- en: As mentioned, the implementation of `memory_profiler` is not as performant as
    the implementation of `line_profiler`. It may therefore make sense to run your
    tests on a smaller problem that completes in a useful amount of time. Overnight
    runs might be sensible for validation, but you need quick and reasonable iterations
    to diagnose problems and hypothesize solutions. The code in [Example 2-10](#profiling-memoryprofiler1)
    uses the full 1,000 × 1,000 grid, and the statistics took about two hours to collect
    on Ian’s laptop.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，`memory_profiler` 的实现性能不如 `line_profiler`。因此，对于能在合理时间内完成的较小问题进行测试是有意义的。隔夜运行可能适合验证，但是你需要快速和合理的迭代来诊断问题和假设解决方案。[示例
    2-10](#profiling-memoryprofiler1) 中的代码使用了完整的 1,000 × 1,000 网格，并且统计信息在 Ian 的笔记本电脑上收集大约花费了两个小时。
- en: Note
  id: totrans-204
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The requirement to modify the source code is a minor annoyance. As with `line_profiler`,
    a decorator (`@profile`) is used to mark the chosen function. This will break
    your unit tests unless you make a dummy decorator—see [“No-op @profile Decorator”](#no_op_profile_decorator).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 修改源代码的要求是一个小的烦恼。与 `line_profiler` 类似，一个装饰器（`@profile`）用于标记选择的函数。这将破坏你的单元测试，除非你创建一个虚拟的装饰器—参见
    [“No-op @profile Decorator”](#no_op_profile_decorator)。
- en: When dealing with memory allocation, you must be aware that the situation is
    not as clear-cut as it is with CPU usage. Generally, it is more efficient to overallocate
    memory in a process that can be used at leisure, as memory allocation operations
    are relatively expensive. Furthermore, garbage collection is not instantaneous,
    so objects may be unavailable but still in the garbage collection pool for some
    time.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理内存分配时，您必须意识到情况并不像处理CPU使用率那样明确。一般来说，在可以自由使用的进程中过度分配内存更为高效，因为内存分配操作相对昂贵。此外，垃圾回收并不是即时的，因此对象可能在垃圾回收池中不可用，但仍然存在一段时间。
- en: The outcome of this is that it is hard to really understand what is happening
    with memory usage and release inside a Python program, as a line of code may not
    allocate a deterministic amount of memory *as observed from outside the process*.
    Observing the gross trend over a set of lines is likely to lead to better insight
    than would be gained by observing the behavior of just one line.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，很难真正理解Python程序内存使用和释放的情况，因为一行代码可能不会分配确定量的内存*从外部过程中观察到的情况而言*。通过观察一组行的总体趋势，可能会比仅观察一行行为获得更好的见解。
- en: Let’s take a look at the output from `memory_profiler` in [Example 2-10](#profiling-memoryprofiler1).
    Inside `calculate_z_serial_purepython` on line 12, we see that the allocation
    of 1,000,000 items causes approximately 7 MB of RAM to be added to this process.^([1](ch02.xhtml#idm46122431392152))
    This does not mean that the `output` list is definitely 7 MB in size, just that
    the process grew by approximately 7 MB during the internal allocation of the list.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看`memory_profiler`在[示例 2-10](#profiling-memoryprofiler1)中的输出。在第12行的`calculate_z_serial_purepython`函数内部，我们可以看到分配了100万个项目导致该进程增加了约7
    MB的RAM。^([1](ch02.xhtml#idm46122431392152)) 这并不意味着`output`列表确实是7 MB的大小，只是在列表的内部分配过程中，该进程大约增长了7
    MB。
- en: In the parent function on line 46, we see that the allocation of the `zs` and
    `cs` lists changes the `Mem usage` column from 48 MB to 125 MB (a change of +77
    MB). Again, it is worth noting that this is not necessarily the true size of the
    arrays, just the size that the process grew by after these lists had been created.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在第46行的父函数中，我们看到`zs`和`cs`列表的分配将`Mem usage`列从48 MB变为125 MB（增加了+77 MB）。同样值得注意的是，这并不一定是数组的真实大小，只是在这些列表创建后进程增长的大小。
- en: At the time of writing, the `memory_usage` module exhibits a bug—the `Increment`
    column does not always match the change in the `Mem usage` column. During the
    first edition of this book, these columns were correctly tracked; you might want
    to check the status of this bug on [GitHub](https://oreil.ly/vuQPN). We recommend
    you use the `Mem usage` column, as this correctly tracks the change in process
    size per line of code.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，`memory_usage`模块存在一个bug——`Increment`列并不总是与`Mem usage`列的变化相匹配。在本书第一版中，这些列是正确跟踪的；您可能需要查看在[GitHub](https://oreil.ly/vuQPN)上此bug的状态。我们建议您使用`Mem
    usage`列，因为这可以正确跟踪每行代码的进程大小变化。
- en: Example 2-10\. `memory_profiler`’s result on both of our main functions, showing
    an unexpected memory use in `calculate_z_serial_purepython`
  id: totrans-211
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 2-10。在我们的两个主要函数中，`memory_profiler`的结果显示在`calculate_z_serial_purepython`中存在意外的内存使用。
- en: '[PRE27]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Another way to visualize the change in memory use is to sample over time and
    plot the result. `memory_profiler` has a utility called `mprof`, used once to
    sample the memory usage and a second time to visualize the samples. It samples
    by time and not by line, so it barely impacts the runtime of the code.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种可视化内存使用变化的方法是随时间取样并绘制结果。`memory_profiler`有一个名为`mprof`的实用程序，用于首次取样内存使用情况，并第二次用于可视化这些样本。它通过时间取样，而不是通过行，因此几乎不会影响代码的运行时间。
- en: '[Figure 2-6](#FIG-julia-memoryprofiler-mprof) is created using `mprof run julia1_memoryprofiler.py`.
    This writes a statistics file that is then visualized using `mprof plot`. Our
    two functions are bracketed: this shows where in time they are entered, and we
    can see the growth in RAM as they run. Inside `calculate_z_serial_purepython`,
    we can see the steady increase in RAM usage throughout the execution of the function;
    this is caused by all the small objects (`int` and `float` types) that are created.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2-6](#FIG-julia-memoryprofiler-mprof)是使用`mprof run julia1_memoryprofiler.py`创建的。这会生成一个统计文件，然后可以使用`mprof
    plot`来可视化。我们的两个函数被框定起来：这显示了它们在时间轴上的输入时刻，并且我们可以看到它们运行时RAM的增长。在`calculate_z_serial_purepython`内部，我们可以看到在函数执行期间RAM使用量的稳定增加；这是由创建的所有小对象（`int`和`float`类型）引起的。'
- en: '![](Images/hpp2_0206.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/hpp2_0206.png)'
- en: Figure 2-6\. `memory_profiler` report using `mprof`
  id: totrans-216
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-6\. 使用`mprof`的`memory_profiler`报告
- en: 'In addition to observing the behavior at the function level, we can add labels
    using a context manager. The snippet in [Example 2-11](#profiling-memoryprofiler1-labels)
    is used to generate the graph in [Figure 2-7](#FIG-julia-memoryprofiler-mprof-labels).
    We can see the `create_output_list` label: it appears momentarily at around 1.5
    seconds after `calculate_z_serial_purepython` and results in the process being
    allocated more RAM. We then pause for a second; `time.sleep(1)` is an artificial
    addition to make the graph easier to understand.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 除了观察函数级行为外，我们还可以使用上下文管理器添加标签。在[示例 2-11](#profiling-memoryprofiler1-labels)中的片段用于生成[图 2-7](#FIG-julia-memoryprofiler-mprof-labels)中的图表。我们可以看到`create_output_list`标签：它在`calculate_z_serial_purepython`之后大约1.5秒的时候瞬间出现，并导致进程分配更多RAM。然后我们暂停一秒钟；`time.sleep(1)`是为了使图表更易于理解而添加的人为延迟。
- en: Example 2-11\. Using a context manager to add labels to the `mprof` graph
  id: totrans-218
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 2-11\. 使用上下文管理器为`mprof`图表添加标签
- en: '[PRE28]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: In the `calculate_output` block that runs for most of the graph, we see a very
    slow, linear increase in RAM usage. This will be from all of the temporary numbers
    used in the inner loops. Using the labels really helps us to understand at a fine-grained
    level where memory is being consumed. Interestingly, we see the “peak RAM usage”
    line—a dashed vertical line just before the 10-second mark—occurring before the
    termination of the program. Potentially this is due to the garbage collector recovering
    some RAM from the temporary objects used during `calculate_output`.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行大部分图表的`calculate_output`块中，我们看到RAM使用量呈现非常缓慢的线性增长。这将来自内部循环中使用的所有临时数字。使用标签确实帮助我们在细粒度级别上理解内存的消耗情况。有趣的是，我们看到了“峰值RAM使用量”线——在程序终止之前的虚线垂直线标志出现在10秒标记之前。可能这是由于垃圾收集器在`calculate_output`期间恢复了一些用于临时对象的RAM。
- en: What happens if we simplify our code and remove the creation of the `zs` and
    `cs` lists? We then have to calculate these coordinates inside `calculate_z_serial_purepython`
    (so the same work is performed), but we’ll save RAM by not storing them in lists.
    You can see the code in [Example 2-12](#profiling-memoryprofiler2).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们简化代码并删除对`zs`和`cs`列表的创建，会发生什么？然后我们必须在`calculate_z_serial_purepython`内计算这些坐标（因此执行相同的工作），但通过不存储它们在列表中可以节省内存。你可以在[示例 2-12](#profiling-memoryprofiler2)中看到代码。
- en: In [Figure 2-8](#FIG-julia-memoryprofiler2-removed-large-lists), we see a major
    change in behavior—the overall envelope of RAM usage drops from 140 MB to 60 MB,
    reducing our RAM usage by half!
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 2-8](#FIG-julia-memoryprofiler2-removed-large-lists)中，我们看到行为有了重大变化——RAM使用总体包络从140
    MB降至60 MB，将我们的RAM使用量减少了一半！
- en: '![](Images/hpp2_0207.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/hpp2_0207.png)'
- en: Figure 2-7\. `memory_profiler` report using `mprof` with labels
  id: totrans-224
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-7\. 使用带标签的`mprof`的`memory_profiler`报告
- en: '![](Images/hpp2_0208.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/hpp2_0208.png)'
- en: Figure 2-8\. `memory_profiler` after removing two large lists
  id: totrans-226
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-8\. 移除两个大列表后的`memory_profiler`
- en: Example 2-12\. Creating complex coordinates on the fly to save RAM
  id: totrans-227
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 2-12\. 动态创建复杂坐标以节省内存
- en: '[PRE29]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: If we want to measure the RAM used by several statements, we can use the IPython
    magic `%memit`, which works just like `%timeit`. In [Chapter 11](ch11_split_000.xhtml#chapter-lessram),
    we will look at using `%memit` to measure the memory cost of lists and discuss
    various ways of using RAM more efficiently.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要测量多条语句使用的RAM，可以使用IPython魔术命令`%memit`，它的用法与`%timeit`类似。在[第 11 章](ch11_split_000.xhtml#chapter-lessram)中，我们将探讨使用`%memit`来测量列表的内存成本，并讨论更高效使用RAM的各种方法。
- en: '`memory_profiler` offers an interesting aid to debugging a large process via
    the `--pdb-mmem=*XXX*` flag. The `pdb` debugger will be activated after the process
    exceeds `*XXX*` MB. This will drop you in directly at the point in your code where
    too many allocations are occurring, if you’re in a space-constrained environment.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '`memory_profiler`通过`--pdb-mmem=*XXX*`标志为调试大型进程提供了一个有趣的辅助。当进程超过`*XXX*` MB时，`pdb`调试器将被激活。如果你在空间受限的环境中，这将直接将你放在代码中过多分配内存的地方。'
- en: Introspecting an Existing Process with PySpy
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 PySpy 检查现有进程
- en: '`py-spy` is an intriguing new sampling profiler—rather than requiring any code
    changes, it introspects an already-running Python process and reports in the console
    with a `top`-like display. Being a sampling profiler, it has almost no runtime
    impact on your code. It is written in Rust and requires elevated privileges to
    introspect another process.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '`py-spy` 是一个引人入胜的新型采样分析器——它不需要任何代码更改，而是 introspects 一个已经运行的 Python 进程，并在控制台中以类似
    `top` 的显示形式报告。作为一个采样分析器，它对你的代码几乎没有运行时影响。它是用 Rust 编写的，需要提升权限才能 introspect 另一个进程。'
- en: This tool could be very useful in a production environment with long-running
    processes or complicated installation requirements. It supports Windows, Mac,
    and Linux. Install it using `pip install py-spy` (note the dash in the name—there’s
    a separate `pyspy` project that isn’t related). If your process is already running,
    you’ll want to use `ps` to get its process identifier (the PID); then this can
    be passed into `py-spy` as shown in [Example 2-13](#profiling-pyspy1).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 这个工具在生产环境中有很大用处，特别是对于长期运行的进程或复杂的安装需求。它支持 Windows、Mac 和 Linux。使用 `pip install
    py-spy` 安装（注意名称中的短横线——还有一个与之无关的 `pyspy` 项目）。如果你的进程已经在运行，你需要使用 `ps` 获取其进程标识符（PID）；然后可以将其传递给
    `py-spy`，如 [示例 2-13](#profiling-pyspy1) 所示。
- en: Example 2-13\. Running PySpy at the command line
  id: totrans-234
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 2-13\. 在命令行运行 PySpy
- en: '[PRE30]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: In [Figure 2-9](#FIG-julia-pyspy), you’ll see a static picture of a `top`-like
    display in the console; this updates every second to show which functions are
    currently taking most of the time.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 2-9](#FIG-julia-pyspy) 中，你会看到控制台中类似 `top` 的显示的静态图片；每秒更新一次，显示哪些函数当前耗费的时间最多。
- en: '![Introspecting a Python process using PySpy](Images/hpp2_0209.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![使用 PySpy 进行 Python 进程 introspection](Images/hpp2_0209.png)'
- en: Figure 2-9\. Introspecting a Python process using PySpy
  id: totrans-238
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-9\. 使用 PySpy 进行 Python 进程 introspection
- en: PySpy can also export a flame chart. Here, we’ll run that option while asking
    PySpy to run our code directly without requiring a PID using `$ py-spy --flame
    profile.svg -- python julia1_nopil.py`. You’ll see in [Figure 2-10](#FIG-julia-pyspy-flame)
    that the width of the display represents the entire program’s runtime, and each
    layer moving down the image represents functions called from above.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: PySpy 还可以导出火焰图。这里，我们将在运行 PySpy 时使用该选项，并要求 PySpy 直接运行我们的代码，而不需要 PID，使用 `$ py-spy
    --flame profile.svg -- python julia1_nopil.py`。你将在 [图 2-10](#FIG-julia-pyspy-flame)
    中看到，显示的宽度代表整个程序的运行时间，每一层向下的图像代表从上方调用的函数。
- en: '![Part of a flame chart for PySpy](Images/hpp2_0210.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![PySpy 火焰图的一部分](Images/hpp2_0210.png)'
- en: Figure 2-10\. Part of a flame chart for PySpy
  id: totrans-241
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-10\. PySpy 火焰图的一部分
- en: 'Bytecode: Under the Hood'
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 字节码：引擎盖下
- en: So far we’ve reviewed various ways to measure the cost of Python code (for both
    CPU and RAM usage). We haven’t yet looked at the underlying bytecode used by the
    virtual machine, though. Understanding what’s going on “under the hood” helps
    to build a mental model of what’s happening in slow functions, and it’ll help
    when you come to compile your code. So let’s introduce some bytecode.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 目前我们已经回顾了各种测量 Python 代码成本的方法（包括 CPU 和 RAM 的使用）。但我们还没有探讨虚拟机使用的底层字节码。了解“引擎盖下”的内容有助于建立对缓慢函数的心理模型，当你编译代码时也会有所帮助。所以让我们来介绍一些字节码。
- en: Using the dis Module to Examine CPython Bytecode
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 `dis` 模块检查 CPython 字节码
- en: The `dis` module lets us inspect the underlying bytecode that we run inside
    the stack-based CPython virtual machine. Having an understanding of what’s happening
    in the virtual machine that runs your higher-level Python code will help you to
    understand why some styles of coding are faster than others. It will also help
    when you come to use a tool like Cython, which steps outside of Python and generates
    C code.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '`dis` 模块让我们检查在基于栈的 CPython 虚拟机中运行的底层字节码。了解在运行你更高级别的 Python 代码的虚拟机中发生的事情将帮助你理解为什么某些编码风格比其他风格更快。当你使用像
    Cython 这样的工具时，它会走出 Python，生成 C 代码，这也会有所帮助。'
- en: The `dis` module is built in. You can pass it code or a module, and it will
    print out a disassembly. In [Example 2-14](#profiling-dis1), we disassemble the
    outer loop of our CPU-bound function.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '`dis` 模块是内置的。你可以传递代码或模块，它会打印出反汇编的代码。在 [示例 2-14](#profiling-dis1) 中，我们反汇编了 CPU
    密集型函数的外部循环。'
- en: Tip
  id: totrans-247
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: You should try to disassemble one of your own functions and to follow *exactly*
    how the disassembled code matches to the disassembled output. Can you match the
    following `dis` output to the original function?
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该尝试反汇编你自己的一个函数，并*准确*地跟踪反汇编的代码是如何与反汇编输出匹配的。你能将以下 `dis` 输出与原始函数匹配吗？
- en: Example 2-14\. Using the built-in `dis` to understand the underlying stack-based
    virtual machine that runs our Python code
  id: totrans-249
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 2-14. 使用内置的`dis`来理解运行我们Python代码的基于栈的虚拟机
- en: '[PRE31]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The output is fairly straightforward, if terse. The first column contains line
    numbers that relate to our original file. The second column contains several `>>`
    symbols; these are the destinations for jump points elsewhere in the code. The
    third column is the operation address; the fourth has the operation name. The
    fifth column contains the parameters for the operation. The sixth column contains
    annotations to help line up the bytecode with the original Python parameters.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 输出相当直接，虽然简短。第一列包含行号，与我们原始文件相关。第二列包含多个`>>`符号；这些是代码中其他跳转点的目标。第三列是操作地址；第四列是操作名称。第五列包含操作的参数。第六列包含注释，帮助将字节码与原始Python参数对齐。
- en: Refer back to [Example 2-3](#profiling-juliaset-intro3) to match the bytecode
    to the corresponding Python code. The bytecode starts on Python line 11 by putting
    the constant value 0 onto the stack, and then it builds a single-element list.
    Next, it searches the namespaces to find the `len` function, puts it on the stack,
    searches the namespaces again to find `zs`, and then puts that onto the stack.
    Inside Python line 12, it calls the `len` function from the stack, which consumes
    the `zs` reference in the stack; then it applies a binary multiply to the last
    two arguments (the length of `zs` and the single-element list) and stores the
    result in `output`. That’s the first line of our Python function now dealt with.
    Follow the next block of bytecode to understand the behavior of the second line
    of Python code (the outer `for` loop).
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 参考[示例 2-3](#profiling-juliaset-intro3)以将字节码与相应的Python代码匹配起来。字节码从Python第11行开始，将常量值0放入堆栈，然后构建一个单元素列表。接下来，它搜索命名空间以找到`len`函数，并将其放入堆栈，再次搜索命名空间以找到`zs`，然后将其放入堆栈。在Python第12行内，它调用堆栈中的`len`函数，消耗堆栈中的`zs`引用；然后将最后两个参数（`zs`的长度和单元素列表）应用二进制乘法，并将结果存储在`output`中。现在，我们处理了Python函数的第一行。跟随下一个字节码块以理解第二行Python代码的行为（外部`for`循环）。
- en: Tip
  id: totrans-253
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: The jump points (`>>`) match to instructions like `JUMP_ABSOLUTE` and `POP_JUMP_IF_FALSE`.
    Go through your own disassembled function and match the jump points to the jump
    instructions.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 跳转点（`>>`）对应于指令如`JUMP_ABSOLUTE`和`POP_JUMP_IF_FALSE`。浏览您自己反汇编的函数，并将跳转点与跳转指令匹配起来。
- en: 'Having introduced bytecode, we can now ask: what’s the bytecode and time cost
    of writing a function out explicitly versus using built-ins to perform the same
    task?'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 引入了字节码之后，我们现在可以问：明确编写一个函数的字节码和时间成本与使用内置函数执行相同任务的成本如何？
- en: Different Approaches, Different Complexity
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不同的方法，不同的复杂性
- en: There should be one—and preferably only one—obvious way to do it. Although that
    way may not be obvious at first unless you’re Dutch.^([2](ch02.xhtml#idm46122430772872))
  id: totrans-257
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 应该有一种——最好只有一种——明显的方法来做到这一点。尽管这种方式一开始可能并不明显，除非你是荷兰人。^([2](ch02.xhtml#idm46122430772872))
- en: ''
  id: totrans-258
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Tim Peters, The Zen of Python
  id: totrans-259
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Tim Peters，《Python之禅》
- en: There will be various ways to express your ideas using Python. Generally, the
    most sensible option should be clear, but if your experience is primarily with
    an older version of Python or another programming language, you may have other
    methods in mind. Some of these ways of expressing an idea may be slower than others.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 有各种方式可以用Python表达您的想法。一般来说，最明智的选择应该是明显的，但如果您的经验主要是使用较旧版本的Python或其他编程语言，则可能会有其他方法。某些表达想法的方式可能比其他方式慢。
- en: You probably care more about readability than speed for most of your code, so
    your team can code efficiently without being puzzled by performant but opaque
    code. Sometimes you will want performance, though (without sacrificing readability).
    Some speed testing might be what you need.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数代码而言，你可能更关心的是可读性而不是速度，因此你的团队可以在不被高效但晦涩的代码困扰的情况下进行编码。不过有时你可能需要性能（而不损失可读性）。某些情况下可能需要进行速度测试。
- en: Take a look at the two code snippets in [Example 2-15](#profiling-dis2). Both
    do the same job, but the first generates a lot of additional Python bytecode,
    which will cause more overhead.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[示例 2-15](#profiling-dis2)中的两个代码片段。两者都完成同样的工作，但第一个生成了大量额外的Python字节码，这将导致更多的开销。
- en: Example 2-15\. A naive and a more efficient way to solve the same summation
    problem
  id: totrans-263
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 2-15. 解决相同求和问题的一个天真和一个更有效的方法
- en: '[PRE32]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Both functions calculate the sum of a range of integers. A simple rule of thumb
    (but one you *must* back up using profiling!) is that more lines of bytecode will
    execute more slowly than fewer equivalent lines of bytecode that use built-in
    functions. In [Example 2-16](#profiling-dis3), we use IPython’s `%timeit` magic
    function to measure the best execution time from a set of runs. `fn_terse` runs
    over twice as fast as `fn_expressive`!
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 两个函数都计算一系列整数的和。一个简单的经验法则（但你*必须*通过性能分析来支持！）是，执行更多字节码行的代码比使用内置函数的等效行数执行得更慢。在[示例2-16](#profiling-dis3)中，我们使用IPython的`%timeit`魔术函数来测量一组运行中的最佳执行时间。`fn_terse`的运行速度比`fn_expressive`快了两倍以上！
- en: Example 2-16\. Using `%timeit` to test our hypothesis that using built-in functions
    should be faster than writing our own functions
  id: totrans-266
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 2-16\. 使用`%timeit`来测试我们的假设，即使用内置函数应该比编写我们自己的函数更快
- en: '[PRE33]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: If we use the `dis` module to investigate the code for each function, as shown
    in [Example 2-17](#profiling-dis4), we can see that the virtual machine has 17
    lines to execute with the more expressive function and only 6 to execute with
    the very readable but terser second function.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用`dis`模块来调查每个函数的代码，如示例2-17所示，我们可以看到虚拟机需要执行17行代码来执行更具表达力的函数，而只需要执行6行代码来执行非常易读但更简洁的第二个函数。
- en: Example 2-17\. Using `dis` to view the number of bytecode instructions involved
    in our two functions
  id: totrans-269
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 2-17\. 使用`dis`查看涉及到我们两个函数的字节码指令数量
- en: '[PRE34]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The difference between the two code blocks is striking. Inside `fn_expressive()`,
    we maintain two local variables and iterate over a list using a `for` statement.
    The `for` loop will be checking to see if a `StopIteration` exception has been
    raised on each loop. Each iteration applies the `total.__add__` function, which
    will check the type of the second variable (`n`) on each iteration. These checks
    all add a little expense.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个代码块之间的差异非常明显。在`fn_expressive()`内部，我们维护两个局部变量，并使用`for`语句迭代列表。`for`循环将检查在每次循环中是否引发了`StopIteration`异常。每次迭代都应用`total.__add__`函数，该函数将检查第二个变量`n`的类型。所有这些检查都会增加一些开销。
- en: Inside `fn_terse()`, we call out to an optimized C list comprehension function
    that knows how to generate the final result without creating intermediate Python
    objects. This is much faster, although each iteration must still check for the
    types of the objects that are being added together (in [Chapter 4](ch04.xhtml#section-dictionary-sets),
    we look at ways of fixing the type so we don’t need to check it on each iteration).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在`fn_terse()`内部，我们调用了一个优化的C列表推导函数，它知道如何生成最终结果而不创建中间Python对象。这样做速度要快得多，尽管每次迭代仍然必须检查正在相加的对象的类型（在[第4章](ch04.xhtml#section-dictionary-sets)中，我们讨论了修复类型以便在每次迭代中不需要检查的方法）。
- en: As noted previously, you *must* profile your code—if you just rely on this heuristic,
    you will inevitably write slower code at some point. It is definitely worth learning
    whether a shorter and still readable way to solve your problem is built into Python.
    If so, it is more likely to be easily readable by another programmer, and it will
    *probably* run faster.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前所指出的，你*必须*对你的代码进行性能分析——如果你只依赖这种启发式方法，你最终肯定会写出速度较慢的代码。了解是否Python内置了解决你问题的更短且仍可读的方法绝对是值得的。如果有的话，它更有可能被其他程序员轻松阅读，并且*可能*运行更快。
- en: Unit Testing During Optimization to Maintain Correctness
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化过程中进行单元测试以保持正确性
- en: If you aren’t already unit testing your code, you are probably hurting your
    longer-term productivity. Ian (blushing) is embarrassed to note that he once spent
    a day optimizing his code, having disabled unit tests because they were inconvenient,
    only to discover that his significant speedup result was due to breaking a part
    of the algorithm he was improving. You do not need to make this mistake even once.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有对你的代码进行单元测试，你可能会损害你长期的生产力。伊恩（脸红）很尴尬地注意到，他曾经花了一天的时间优化他的代码，因为他觉得单元测试很不方便，所以禁用了它们，结果发现他显著提速的结果是由于破坏了他正在改进的算法的一部分。你不需要犯这种错误，甚至只有一次。
- en: Tip
  id: totrans-276
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Add unit tests to your code for a saner life. You’ll be giving your current
    self and your colleagues faith that your code works, and you’ll be giving a present
    to your future-self who has to maintain this code later. You really will save
    a lot of time in the long term by adding tests to your code.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更加理智地生活，给你的代码添加单元测试。这样一来，你会给你当前的自己和同事们带来信心，相信你的代码是可靠的；同时，你也会为将来需要维护这段代码的自己送上一份礼物。长远来看，通过给你的代码添加测试，你确实可以节省很多时间。
- en: In addition to unit testing, you should also strongly consider using `coverage.py`.
    It checks to see which lines of code are exercised by your tests and identifies
    the sections that have no coverage. This quickly lets you figure out whether you’re
    testing the code that you’re about to optimize, such that any mistakes that might
    creep in during the optimization process are quickly caught.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 除了单元测试之外，你还应该考虑强烈使用 `coverage.py`。它检查哪些代码行被你的测试覆盖，并识别没有覆盖的部分。这可以快速帮助你确定是否正在测试你即将优化的代码，从而在优化过程中发现可能出现的任何错误。
- en: No-op @profile Decorator
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无操作 `@profile` 装饰器
- en: Your unit tests will fail with a `NameError` exception if your code uses `@profile`
    from `line_profiler` or `memory_profiler`. The reason is that the unit test framework
    will not be injecting the `@profile` decorator into the local namespace. The no-op
    decorator shown here solves this problem. It is easiest to add it to the block
    of code that you’re testing and remove it when you’re done.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的代码使用了`line_profiler`或者`memory_profiler`中的`@profile`装饰器，你的单元测试会因为`NameError`异常而失败。原因在于单元测试框架不会将`@profile`装饰器注入到本地命名空间中。下面展示的无操作装饰器解决了这个问题。将它添加到你正在测试的代码块中，在完成测试后再移除它。
- en: With the no-op decorator, you can run your tests without modifying the code
    that you’re testing. This means you can run your tests after every profile-led
    optimization you make so you’ll never be caught out by a bad optimization step.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 使用无操作装饰器，你可以在不修改正在测试的代码的情况下运行你的测试。这意味着你可以在每次进行基于性能优化的测试后运行你的测试，以便你永远不会因糟糕的优化步骤而受损失。
- en: Let’s say we have the trivial `ex.py` module shown in [Example 2-18](#profiling-noop1).
    It has a test (for `pytest`) and a function that we’ve been profiling with either
    `line_profiler` or `memory_profiler`.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个简单的 `ex.py` 模块，如[示例 2-18](#profiling-noop1)所示。它包含了一个用于`pytest`的测试和一个我们一直在使用`line_profiler`或`memory_profiler`进行性能分析的函数。
- en: Example 2-18\. Simple function and test case where we wish to use `@profile`
  id: totrans-283
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 2-18\. 简单的函数和测试用例，我们希望使用 `@profile`
- en: '[PRE35]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: If we run `pytest` on our code, we’ll get a `NameError`, as shown in [Example 2-19](#profiling-no-decorator).
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在我们的代码上运行 `pytest`，我们将会得到一个 `NameError`，如[示例 2-19](#profiling-no-decorator)所示。
- en: Example 2-19\. A missing decorator during testing breaks out tests in an unhelpful
    way!
  id: totrans-286
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 2-19\. 在测试中缺少装饰器会导致测试以一种无帮助的方式失败！
- en: '[PRE36]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The solution is to add a no-op decorator at the start of our module (you can
    remove it after you’re done with profiling). If the `@profile` decorator is not
    found in one of the namespaces (because `line_profiler` or `memory_profiler` is
    not being used), the no-op version we’ve written is added. If `line_profiler`
    or `memory_profiler` has injected the new function into the namespace, our no-op
    version is ignored.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案是在我们的模块开头添加一个无操作装饰器（在完成性能分析后可以删除它）。如果在命名空间中找不到 `@profile` 装饰器（因为未使用 `line_profiler`
    或 `memory_profiler`），我们编写的无操作版本将被添加进去。如果 `line_profiler` 或 `memory_profiler` 已经将新函数注入到命名空间中，我们编写的无操作版本将被忽略。
- en: For both `line_profiler` and `memory_profiler`, we can add the code in [Example 2-20](#profiling-noop2).
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `line_profiler` 和 `memory_profiler`，我们可以在[示例 2-20](#profiling-noop2)中添加代码。
- en: Example 2-20\. Add a no-op `@profile` decorator to the namespace while unit
    testing
  id: totrans-290
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 2-20\. 在单元测试中向命名空间添加一个无操作 `@profile` 装饰器
- en: '[PRE37]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Having added the no-op decorator, we can now run our `pytest` successfully,
    as shown in [Example 2-21](#profiling-noop3), along with our profilers—with no
    additional code changes.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 添加了无操作装饰器后，我们现在可以成功运行我们的`pytest`，如[示例 2-21](#profiling-noop3)所示，同时还可以使用我们的性能分析器，而不需要额外的代码更改。
- en: Example 2-21\. With the no-op decorator, we have working tests, and both of
    our profilers work correctly
  id: totrans-293
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 2-21\. 使用无操作装饰器，我们的测试工作正常，而且我们的性能分析器也正常工作
- en: '[PRE38]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: You can save yourself a few minutes by avoiding the use of these decorators,
    but once you’ve lost hours making a false optimization that breaks your code,
    you’ll want to integrate this into your workflow.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 通过避免使用这些装饰器，你可以节省几分钟的时间，但是一旦因错误的优化而浪费数小时才会意识到这点，你就会希望将它们整合到你的工作流程中。
- en: Strategies to Profile Your Code Successfully
  id: totrans-296
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 成功分析代码的策略
- en: Profiling requires some time and concentration. You will stand a better chance
    of understanding your code if you separate the section you want to test from the
    main body of your code. You can then unit test the code to preserve correctness,
    and you can pass in realistic fabricated data to exercise the inefficiencies you
    want to address.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 进行性能分析需要一些时间和注意力。如果你将要测试的部分与主体代码分离开来，你将更有可能理解你的代码。然后，你可以进行单元测试以保证正确性，并传入真实的伪造数据来执行你想要解决的低效率部分。
- en: 'Do remember to disable any BIOS-based accelerators, as they will only confuse
    your results. On Ian’s laptop, the Intel Turbo Boost feature can temporarily accelerate
    a CPU above its normal maximum speed if it is cool enough. This means that a cool
    CPU may run the same block of code faster than a hot CPU. Your operating system
    may also control the clock speed—a laptop on battery power is likely to more aggressively
    control CPU speed than a laptop on AC power. To create a more stable benchmarking
    configuration, we do the following:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 记得禁用任何基于 BIOS 的加速器，因为它们只会混淆你的结果。在 Ian 的笔记本电脑上，英特尔 Turbo Boost 功能可以在 CPU 温度足够低时临时加速
    CPU 超过其正常最大速度。这意味着一个冷却的 CPU 可能比一个热的 CPU 更快地运行相同的代码块。你的操作系统也可能控制时钟速度——使用电池供电的笔记本电脑可能会比使用交流电的笔记本电脑更积极地控制
    CPU 速度。为了创建更稳定的基准测试配置，我们执行以下操作：
- en: Disable Turbo Boost in the BIOS.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 BIOS 中禁用 Turbo Boost。
- en: Disable the operating system’s ability to override the SpeedStep (you will find
    this in your BIOS if you’re allowed to control it).
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 禁用操作系统覆盖 SpeedStep 的能力（如果允许你控制的话，你可以在 BIOS 中找到这个选项）。
- en: Use only AC power (never battery power).
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅使用交流电（永不使用电池供电）。
- en: Disable background tools like backups and Dropbox while running experiments.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在运行实验时禁用像备份和 Dropbox 这样的后台工具。
- en: Run the experiments many times to obtain a stable measurement.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多次运行实验以获得稳定的测量结果。
- en: Possibly drop to run level 1 (Unix) so that no other tasks are running.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能会降到运行级别 1（Unix），以确保没有其他任务在运行。
- en: Reboot and rerun the experiments to double-confirm the results.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重启并重新运行实验以双重确认结果。
- en: Try to hypothesize the expected behavior of your code and then validate (or
    disprove!) the hypothesis with the result of a profiling step. Your choices will
    not change (you should only drive your decisions by using the profiled results),
    but your intuitive understanding of the code will improve, and this will pay off
    in future projects as you will be more likely to make performant decisions. Of
    course, you will verify these performant decisions by profiling as you go.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试假设你的代码的预期行为，然后通过分析步骤的结果验证（或驳斥！）这些假设。你的选择不会改变（你应该仅仅通过使用分析结果来推动你的决策），但你对代码的直觉理解将会提高，这将在未来的项目中带来回报，因为你更有可能做出高性能的决策。当然，你将在进行中使用分析来验证这些高性能的决策。
- en: Do not skimp on the preparation. If you try to performance test code deep inside
    a larger project without separating it from the larger project, you are likely
    to witness side effects that will sidetrack your efforts. It is likely to be harder
    to unit test a larger project when you’re making fine-grained changes, and this
    may further hamper your efforts. Side effects could include other threads and
    processes impacting CPU and memory usage and network and disk activity, which
    will skew your results.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 不要在准备工作上吝啬。如果你尝试在较大项目中深入测试代码而没有将其与较大项目分离，很可能会遇到会使你的努力受挫的副作用。在进行精细的更改时，对于较大的项目进行单元测试可能会更加困难，这可能进一步阻碍你的努力。副作用可能包括其他线程和进程影响
    CPU 和内存使用情况以及网络和磁盘活动，这些将使你的结果偏离。
- en: Naturally, you’re already using source code control (e.g., Git or Mercurial),
    so you’ll be able to run multiple experiments in different branches without ever
    losing “the versions that work well.” If you’re *not* using source code control,
    do yourself a huge favor and start to do so!
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你已经在使用源代码控制（例如 Git 或 Mercurial），因此你可以在不丢失“运行良好版本”的情况下，在不同的分支上运行多个实验。如果你还*没有*使用源代码控制，请务必开始使用！
- en: For web servers, investigate `dowser` and `dozer`; you can use these to visualize
    in real time the behavior of objects in the namespace. Definitely consider separating
    the code you want to test out of the main web application if possible, as this
    will make profiling significantly easier.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Web 服务器，请调查`dowser`和`dozer`；你可以使用它们实时可视化命名空间中对象的行为。如果可能的话，确实考虑将你想测试的代码从主
    Web 应用程序中分离出来，这将显著简化分析过程。
- en: Make sure your unit tests exercise all the code paths in the code that you’re
    analyzing. Anything you don’t test that is used in your benchmarking may cause
    subtle errors that will slow down your progress. Use `coverage.py` to confirm
    that your tests are covering all the code paths.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你的单元测试覆盖了你分析的代码中的所有代码路径。你没有测试的任何东西，如果在你的基准测试中使用，可能会导致细微的错误，这些错误会减慢你的进度。使用
    `coverage.py` 确认你的测试覆盖了所有代码路径。
- en: Unit testing a complicated section of code that generates a large numerical
    output may be difficult. Do not be afraid to output a text file of results to
    run through `diff` or to use a `pickled` object. For numeric optimization problems,
    Ian likes to create long text files of floating-point numbers and use `diff`—minor
    rounding errors show up immediately, even if they’re rare in the output.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 对于生成大量数值输出的复杂代码部分进行单元测试可能会很困难。不要害怕生成一个结果文本文件来运行 `diff` 或者使用一个 `pickled` 对象。对于数值优化问题，Ian
    喜欢创建长文本文件包含浮点数，并使用 `diff`——即使在输出中它们很少见，轻微的舍入误差会立即显现。
- en: If your code might be subject to numerical rounding issues due to subtle changes,
    you are better off with a large output that can be used for a before-and-after
    comparison. One cause of rounding errors is the difference in floating-point precision
    between CPU registers and main memory. Running your code through a different code
    path can cause subtle rounding errors that might later confound you—it is better
    to be aware of this as soon as they occur.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的代码由于细微变化可能会受到数值舍入问题的影响，最好生成一个较大的输出，以便进行前后比较。一个造成舍入误差的原因是 CPU 寄存器和主存之间浮点精度的差异。通过不同的代码路径运行你的代码可能会导致细微的舍入误差，这可能会后来让你感到困惑——最好在它们出现时就意识到这一点。
- en: Obviously, it makes sense to use a source code control tool while you are profiling
    and optimizing. Branching is cheap, and it will preserve your sanity.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，在进行性能分析和优化时使用源代码控制工具是有意义的。分支是廉价的，它将保持你的理智。
- en: Wrap-Up
  id: totrans-314
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Having looked at profiling techniques, you should have all the tools you need
    to identify bottlenecks around CPU and RAM usage in your code. Next, we’ll look
    at how Python implements the most common containers, so you can make sensible
    decisions about representing larger collections of data.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 经过性能分析技术的研究，你应该已经掌握了在你的代码中识别 CPU 和 RAM 使用瓶颈所需的所有工具。接下来，我们将看一下 Python 如何实现最常见的容器，这样你就可以对表示更大数据集做出明智的决策。
- en: ^([1](ch02.xhtml#idm46122431392152-marker)) `memory_profiler` measures memory
    usage according to the International Electrotechnical Commission’s MiB (mebibyte)
    of 2^(20) bytes. This is slightly different from the more common but also more
    ambiguous MB (megabyte has two commonly accepted definitions!). 1 MiB is equal
    to 1.048576 (or approximately 1.05) MB. For our discussion, unless we’re dealing
    with very specific amounts, we’ll consider the two equivalent.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch02.xhtml#idm46122431392152-marker)) `memory_profiler` 根据国际电工委员会的 MiB（2^(20)
    字节）来测量内存使用情况。这与更常见但也更模糊的 MB（兆字节有两种通常接受的定义！）稍有不同。1 MiB 等于 1.048576（或大约 1.05）MB。对于我们的讨论来说，除非我们在处理非常具体的数量，否则我们将认为这两者是等效的。
- en: ^([2](ch02.xhtml#idm46122430772872-marker)) The language creator Guido van Rossum
    is Dutch, and not everyone has agreed with his “obvious” choices, but on the whole
    we like the choices that Guido makes!
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch02.xhtml#idm46122430772872-marker)) 语言创造者 Guido van Rossum 是荷兰人，并不是每个人都同意他的“显而易见”的选择，但总体上我们喜欢
    Guido 做出的选择！
