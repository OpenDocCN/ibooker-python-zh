- en: 'Chapter 3\. How Dask Works: The Basics'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三章\. Dask 的工作原理：基础知识
- en: Now that you’ve run your first few tasks with Dask, it’s time to learn a little
    bit about what’s happening behind the scenes. Depending on whether you are using
    Dask locally or in a distributed fashion, the behavior can be a little different.
    While Dask does a good job of abstracting away many of the details of running
    on multiple threads or servers, having a solid grasp of how Dask is working will
    help you better decide both how and when to use it.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经使用 Dask 运行了您的前几个任务，是时候了解一下幕后发生的事情了。根据您是在本地使用 Dask 还是分布式使用，行为可能会有所不同。虽然
    Dask 很好地抽象了在多线程或多服务器上运行的许多细节，但深入了解 Dask 的工作原理将帮助您更好地决定何时以及如何使用它。
- en: 'To be familiar with Dask, you need to understand:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 要熟悉 Dask，您需要了解：
- en: The deployment framework that Dask is able to run on, and its strengths and
    weaknesses
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dask 能够运行的部署框架，以及其优势和劣势
- en: The types of data that Dask is able to read, and how you can interact with the
    data types in Dask
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dask 能够读取的数据类型，以及如何在 Dask 中与这些数据类型进行交互
- en: The computational pattern of Dask, and how to turn your ideas into Dask code
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dask 的计算模式，以及如何将您的想法转化为 Dask 代码
- en: How to monitor and troubleshoot
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何监控和排查故障
- en: In this chapter, we will introduce each of these concepts, and we will expand
    upon them in the rest of the book.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍每一个概念，并在本书的其余部分进行扩展。
- en: Execution Backends
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行后端
- en: Dask has many different execution backends, but we find it easiest to think
    about them in two groups, local and distributed. With local backends, you are
    limited in scale to what a single computer can handle. Local backends also have
    advantages like the avoidance of network overhead, simpler library management,
    and a lower dollar cost.^([1](ch03.xhtml#id374)) Dask’s distributed backend has
    many options for deployment, from cluster managers such as Kubernetes to job queue–like
    systems.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 有许多不同的执行后端，但我们发现最容易将它们归为两组：本地和分布式。使用本地后端，您的规模受限于单台计算机所能处理的范围。本地后端还具有诸如避免网络开销、更简单的库管理和更低的成本等优势。^([1](ch03.xhtml#id374))
    Dask 的分布式后端有多种部署选项，从 Kubernetes 等集群管理器到作业队列式系统。
- en: Local Backends
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本地后端
- en: Dask’s three local backends are single-process, multi-threaded, and multi-process.
    The single-process backend has no parallelism and is mostly useful for validating
    that a problem is caused by concurrency. Multi-threaded and multi-process backends
    are ideal for problems in which the data is small or the cost of copying it would
    be higher than computation time.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 的三个本地后端是单进程、多线程和多进程。单进程后端没有并行性，主要用于验证问题是否由并发引起。多线程和多进程后端适合数据规模较小或复制成本高于计算时间的问题。
- en: Tip
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If you don’t configure a specific local backend, Dask will pick the backend
    based on the library you are working with.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果未配置特定的本地后端，Dask 将根据您正在使用的库选择后端。
- en: The local multi-threaded scheduler is able to avoid having to serialize data
    and interprocess communication costs. The multi-threaded backend is suited for
    tasks in which the majority of the computation is happening in native code, outside
    of Python. This is the case for many numeric libraries, such as pandas and NumPy.
    If that is the case for you, you can configure Dask to use multi-threading, as
    shown in [Example 3-1](#configure_dask_multithreading_ch03_1686241430957).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本地多线程调度器能够避免需要序列化数据和进程间通信成本。多线程后端适用于大部分计算发生在 Python 之外的本地代码的任务。这对于许多数值库（如 pandas
    和 NumPy）是适用的。如果您的情况也是如此，您可以配置 Dask 使用多线程，如 [示例 3-1](#configure_dask_multithreading_ch03_1686241430957)
    所示。
- en: Example 3-1\. Configuring Dask to use multi-threading
  id: totrans-15
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-1\. 配置 Dask 使用多线程
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The local multi-process backend, shown in [Example 3-2](#configuring_dask_multiprocess_ch03_1686241509022),
    has some additional overhead over multi-threaded, although it can be decreased
    on Unix and Unix-like systems.^([2](ch03.xhtml#id375)) The multi-process backend
    is able to avoid Python’s global interpreter lock by launching separate processes.
    Launching a new process is more expensive than a new thread, and Dask needs to
    serialize data that moves between processes.^([3](ch03.xhtml#id376))
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本地多进程后端，如 [示例 3-2](#configuring_dask_multiprocess_ch03_1686241509022) 所示，与多线程相比有一些额外的开销，尽管在
    Unix 和类 Unix 系统上可以减少这些开销。^([2](ch03.xhtml#id375)) 多进程后端通过启动单独的进程来避免 Python 的全局解释器锁。启动新进程比启动新线程更昂贵，而且
    Dask 需要序列化在进程之间传输的数据。^([3](ch03.xhtml#id376))
- en: Example 3-2\. Configuring Dask to use the multi-process backend
  id: totrans-18
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-2\. 配置 Dask 使用多进程后端
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If you are running on a Unix system, you can use the *forkserver*, shown in
    [Example 3-3](#configuring_dask_forkserver_ch03_1686241608423), which will [reduce
    the overhead of starting each Python interpreter](https://oreil.ly/U9voe). Using
    the forkserver will not reduce the communication overhead.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在运行 Unix 系统上，可以使用 *forkserver*，如 [示例 3-3](#configuring_dask_forkserver_ch03_1686241608423)，这将[减少每个
    Python 解释器启动的开销](https://oreil.ly/U9voe)。使用 forkserver 不会减少通信开销。
- en: Example 3-3\. Configuring Dask to use the multi-process forkserver
  id: totrans-21
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-3\. 配置 Dask 使用多进程 forkserver
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This optimization is generally not available on Windows.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 此优化通常不适用于 Windows。
- en: Dask’s local backends are designed for performance rather than for testing that
    your code will work on a distributed scheduler. To test that your code will run
    remotely, you should use Dask’s distributed scheduler with a LocalCluster instead.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 的本地后端旨在提高性能，而不是测试您的代码是否能在分布式调度器上运行。要测试您的代码能否远程运行，应该使用带有 LocalCluster 的
    Dask 分布式调度器。
- en: Distributed (Dask Client and Scheduler)
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分布式（Dask 客户端和调度器）
- en: While Dask can work well locally, its true power comes with the distributed
    scheduler, with which you can scale your problem to multiple machines. Since there
    are physical and financial limits to how much computing power, storage, and memory
    can be put into one machine, using multiple machines is often the most cost-efficient
    solution (and it is sometimes the only solution). Distributed computing is not
    without its drawbacks; as Leslie Lamport famously said, “A distributed system
    is one in which the failure of a computer you didn’t even know existed can render
    your own computer unusable.” While Dask does much to limit these failures (see
    [“Fault Tolerance”](#fault_tolerance_ch03_1688739701115)), you accept some increase
    in complexity when moving to a distributed system.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Dask 在本地可以很好地工作，但其真正的力量来自于分布式调度器，您可以将问题扩展到多台计算机上。由于物理和财务限制限制了可以放入一台机器的计算能力、存储和内存的量，因此使用多台计算机通常是最具成本效益的解决方案（有时甚至是唯一的解决方案）。分布式计算并非没有缺点；正如
    Leslie Lamport 所说，“一个分布式系统是指你甚至都不知道存在的计算机的故障可能使你自己的计算机无法使用。”虽然 Dask 在减少这些故障方面做了很多工作（参见
    [“容错性”](#fault_tolerance_ch03_1688739701115)），但是在转向分布式系统时，您需要接受一些复杂性增加。
- en: Dask has one distributed scheduler backend, and it talks to many different types
    of clusters, including a LocalCluster. Each type of cluster is supported in its
    own library, which schedules the scheduler^([4](ch03.xhtml#id383)) that the Dask
    client then connects to. Using the distributed abstraction `dask.distributed`
    gives you portability between any types of cluster you may be using at the moment,
    including local. If you don’t use `dask​.dis⁠tributed`, Dask can still run perfectly
    well on a local computer, in which case you are using a default single-machine
    scheduler provided in the Dask library.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 有一个分布式调度器后端，它可以与许多不同类型的集群进行通信，包括 LocalCluster。每种类型的集群都在其自己的库中得到支持，这些库安排了调度器^([4](ch03.xhtml#id383))，而
    Dask 客户端则连接到这些调度器。使用分布式抽象 `dask.distributed` 可以使您在任何时候都可以在不同类型的集群之间移植，包括本地集群。如果您不使用
    `dask.distributed`，Dask 也可以在本地计算机上运行得很好，此时您将使用 Dask 库提供的默认单机调度器。
- en: The Dask client is your entry point into the Dask distributed scheduler. In
    this chapter, we will be using Dask with a Kubernetes cluster; if you have another
    type of cluster or want details, please see [Chapter 12](ch12.xhtml#ch12).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 客户端是您进入 Dask 分布式调度器的入口。在本章中，我们将使用 Dask 与 Kubernetes 集群；如果您有其他类型的集群或需要详细信息，请参见
    [第12章](ch12.xhtml#ch12)。
- en: Auto-scaling
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动扩展
- en: With auto-scaling, Dask can increase or decrease the computers/resources being
    used, based on the tasks you have asked it to run.^([5](ch03.xhtml#id386)) For
    example, if you have a program that computes complex aggregations using many computers
    but then mostly operates on the aggregated data, the number of computers you need
    could decrease by a large amount post-aggregation. Many workloads, including machine
    learning, do not need the same amount of resources/computers the entire time.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 使用自动扩展，Dask 可以根据您要求运行的任务增加或减少使用的计算机/资源^([5](ch03.xhtml#id386))。例如，如果您有一个程序，使用许多计算机计算复杂的聚合，但后续大部分操作在聚合数据上进行，则在聚合后，您需要的计算机数量可能会大幅减少。许多工作负载，包括机器学习，不需要在整个时间段内使用相同数量的资源/计算机。
- en: Some of Dask’s cluster backends, including Kubernetes, support auto-scaling,
    which Dask calls *adaptive deployments*. Auto-scaling is useful mostly in situations
    of shared cluster resources, or when running on cloud providers where the underlying
    resources are paid for by the hour.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 的某些集群后端，包括 Kubernetes，支持自动缩放，Dask 称之为*自适应部署*。自动缩放主要在共享集群资源或在云提供商上运行时有用，后者的底层资源是按小时计费的情况下使用。
- en: Important limitations with the Dask client
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Dask 客户端的重要限制
- en: Dask’s client is not fault tolerant, so while Dask can handle the failures of
    its workers, if the connection between the client and the scheduler is broken,
    your application will fail. A common workaround for this is scheduling the client
    within the same environment as the scheduler, although this does somewhat reduce
    the usefulness of having the client and scheduler as separate components.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 的客户端不具备容错性，因此，虽然 Dask 能够处理其工作节点的故障，但如果客户端与调度器之间的连接中断，您的应用程序将会失败。对此的一个常见解决方法是在与调度器相同的环境中调度客户端，尽管这样做会在某种程度上降低将客户端和调度器作为独立组件的实用性。
- en: Libraries and dependencies in distributed clusters
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分布式集群中的库和依赖项
- en: Part of why Dask is so powerful is the Python ecosystem that it is in. While
    Dask will pickle, or serialize (see [“Serialization and Pickling”](#ser_pick_dtl)),
    and send our code to the workers, this doesn’t include the libraries we use.^([6](ch03.xhtml#id400))
    To take advantage of that ecosystem, you need to be able to use additional libraries.
    During the exploration phase, it is common to install packages at runtime as you
    discover that you need them.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 之所以如此强大的一部分原因是它所在的 Python 生态系统。虽然 Dask 将我们的代码 pickle 或序列化（请参阅[“序列化和 Pickling”](#ser_pick_dtl)），并将其发送到工作节点，但这并不包括我们使用的库。^([6](ch03.xhtml#id400))
    要利用该生态系统，您需要能够使用其他库。在探索阶段，常常会在运行时安装包，因为您发现需要它们。
- en: The `PipInstall` worker plug-in takes a list of packages and installs them at
    runtime on all of the workers. Looking back at [Example 2-4](ch02.xhtml#web_crawler_ch02_1688747981454),
    to install bs4 you would call `distributed.diagnostics.plugin.PipInstall(["bs4"])`.
    Any new workers that are launched by Dask then need to wait for the package to
    be installed. The `Pip​Install` plug-in is ideal for quick prototyping when you
    are discovering which packages you need. You can think of `PipInstall` as the
    replacement for `!pip install` in a notebook over having a virtualenv.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`PipInstall` 工作节点插件接受一个包列表，并在所有工作节点上在运行时安装它们。回顾[Example 2-4](ch02.xhtml#web_crawler_ch02_1688747981454)，要安装
    bs4，您将调用 `distributed.diagnostics.plugin.PipInstall(["bs4"])`。然后由 Dask 启动的任何新工作节点都需要等待包被安装。`PipInstall`
    插件非常适合在您发现需要哪些包时进行快速原型设计。您可以将 `PipInstall` 视为在笔记本中使用 `!pip install` 的虚拟环境替代方案。'
- en: To avoid the slowness of having to install packages each time a new worker is
    launched, you should try to pre-install your libraries. Each cluster manager (e.g.,
    YARN, Kubernetes, Coiled, Saturn, etc.) has its own methods for managing dependencies.
    This can happen at runtime or at setup where the packages are pre-installed. The
    specifics for the different cluster managers are covered in [Chapter 12](ch12.xhtml#ch12).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为避免每次启动新工作节点时都需要安装软件包的缓慢性能，您应尝试预先安装您的库。每个集群管理器（例如，YARN、Kubernetes、Coiled、Saturn
    等）都有自己的方法来管理依赖关系。这可以在运行时或设置时进行，其中包已经被预先安装。关于不同集群管理器的具体细节，请参阅[第 12 章](ch12.xhtml#ch12)。
- en: With Kubernetes, for example, the default startup script checks for the presence
    of some key environment variables (`EXTRA_APT_PACKAGES`, `EXTRA_CONDA_PACKAGES`,
    and `EXTRA_PIP_PACKAGES`), which, in conjunction with customized worker specs,
    can be used to add dependencies at runtime. Some of them, like Coiled and Kubernetes,
    allow for adding dependencies when building an image for our workers. Others,
    like YARN, use preallocated conda/virtual environment packing.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在 Kubernetes 中，默认启动脚本会检查某些关键环境变量的存在（`EXTRA_APT_PACKAGES`、`EXTRA_CONDA_PACKAGES`
    和 `EXTRA_PIP_PACKAGES`），结合自定义的工作节点规范，可以在运行时添加依赖项。其中一些，例如 Coiled 和 Kubernetes，允许在为工作节点构建映像时添加依赖项。另一些，例如
    YARN，使用预先分配的 conda/virtual 环境包装。
- en: Warning
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: It is very important to have the same versions of Python and libraries installed
    on all of the workers and your client. Different versions of libraries can lead
    to outright failures or subtler data correctness issues.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有工作节点和客户端上安装相同版本的 Python 和库非常重要。不同版本的库可能会导致彻底失败或更微妙的数据正确性问题。
- en: Dask’s Diagnostics User Interface
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dask 的诊断用户界面
- en: One of your first stops in understanding what your program is doing should be
    Dask’s Diagnostics UI. The UI allows you to see what Dask is executing, the number
    of worker threads/processes/computers, memory utilization information, and much
    more. If you are running Dask locally, you will likely find the UI at *http:​//localhost:8787*.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解程序执行的第一步应该是使用Dask的诊断UI。该UI允许您查看Dask正在执行的操作，工作线程/进程/计算机的数量，内存利用信息等等。如果您在本地运行Dask，则很可能会在*http:​//localhost:8787*找到该UI。
- en: If you’re using the Dask client to connect to a cluster, the UI will be running
    on the scheduler node. You can get the link to the dashboard from `client.dashboard_link`.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用Dask客户端连接到集群，UI将运行在调度器节点上。你可以从`client.dashboard_link`获取仪表板的链接。
- en: Tip
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: For remote notebook users, the hostname of the scheduler node may not be reachable
    directly from your computer. One option is to use the Jupyter proxy; for example,
    one might go to `http://jupyter.example.com/user/username/proxy/dask-head-4c81d51e-3.jhub:8787/status`
    to access the endpoint `dask-head-4c81d51e-3.jhub:8787/status`.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于远程笔记本用户，调度器节点的主机名可能无法直接从您的计算机访问。一种选择是使用Jupyter代理；例如，可以访问`http://jupyter.example.com/user/username/proxy/dask-head-4c81d51e-3.jhub:8787/status`来访问端点`dask-head-4c81d51e-3.jhub:8787/status`。
- en: '[Figure 3-1](#fig_dask_ui) shows the Dask UI during the running of the examples
    in this chapter.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-1](#fig_dask_ui)显示了本章示例中运行时的Dask UI。'
- en: '![spwd 0301](Images/spwd_0301.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![spwd 0301](Images/spwd_0301.png)'
- en: Figure 3-1\. The Dask UI ([digital, color version](https://oreil.ly/PuWRN))
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-1\. Dask UI ([数字，彩色版本](https://oreil.ly/PuWRN))
- en: The UI allows you to see what Dask is doing and what’s being stored on the workers
    and to explore the execution graph. We will revisit the execution graph in [“visualize”](#graph_visualize).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 该UI允许您查看Dask的执行情况以及存储在工作节点上的内容，并探索执行图。我们将在[“visualize”](#graph_visualize)中重新访问执行图。
- en: Serialization and Pickling
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 序列化和Pickling
- en: Distributed and parallel systems depend on serialization, sometimes called *pickling*
    in Python, to share data and functions/code between processes. Dask uses a mixture
    of serialization techniques to match the use case and provides hooks to extend
    by class when the defaults don’t meet your needs.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式和并行系统依赖于序列化，在Python中有时称为*pickling*，用于在进程之间共享数据和函数/代码。Dask使用各种序列化技术来匹配使用情况，并提供扩展钩子以在默认情况不满足需求时进行扩展。
- en: Warning
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: We most often think of serialization when it fails (with an error), but equally
    important can be situations where we end up serializing more data than we need—or
    when the amount of data that needs to be transferred is so large that distributing
    the work is no longer beneficial.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在序列化失败（出现错误）时往往会考虑得更多，但同样重要的是可能会出现序列化了比实际需要更多数据的情况，或者数据量如此之大以至于分布式处理不再具备优势。
- en: Cloudpickle serializes the functions and the generic Python types in Dask. Most
    Python code doesn’t depend on serializing functions, but cluster computing often
    does. Cloudpickle is a project designed for cluster computing and is able to serialize
    and deserialize more functions than Python’s built-in pickle.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Cloudpickle序列化了Dask中的函数和通用Python类型。大多数Python代码不依赖于序列化函数，但集群计算经常需要。Cloudpickle是一个专为集群计算设计的项目，能够序列化和反序列化比Python内置的pickle更多的函数。
- en: Warning
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Dask has its own ability to extend serialization, but the registry methods are
    not automatically sent to the workers, and it’s not always used.^([7](ch03.xhtml#id416))
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Dask具有自己扩展序列化的能力，但是注册方法并不会自动发送到工作节点，并且并非总是使用。^([7](ch03.xhtml#id416))
- en: Dask has built-in special handling for NumPy arrays, sparse, and cuPY. These
    serializations tend to be more space efficient than the default serializers. When
    you make a class that contains one of these types and does not require any special
    initialization, you should call `register_generic(YourClass)` from `dask.distributed.protocol`
    to take advantage of Dask’s special handling.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Dask为NumPy数组、稀疏数组和cuPY构建了内置特殊处理。这些序列化通常比默认的序列化器更节省空间。当您创建一个包含这些类型且不需要任何特殊初始化的类时，应从`dask.distributed.protocol`中调用`register_generic(YourClass)`以利用Dask的特殊处理能力。
- en: If you have a class that is not serializable, as in [Example 3-4](#fail_to_ser),
    you can wrap it to add serialization functions to it, as shown in [Example 3-5](#custom_ser_with_pickle).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个不能序列化的类，如[示例 3-4](#fail_to_ser)，你可以对其进行包装以添加序列化函数，如[示例 3-5](#custom_ser_with_pickle)所示。
- en: Example 3-4\. Dask fails to serialize
  id: totrans-59
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-4\. Dask无法序列化
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Example 3-5\. Custom serialization
  id: totrans-61
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-5\. 自定义序列化
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: If you control the original class, you can also directly add the `getstate`/`setstate`
    methods instead of wrapping it.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您控制原始类，还可以直接添加 `getstate`/`setstate` 方法而不是包装它。
- en: Note
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Dask automatically attempts to compress serialized data, which generally improves
    performance. You can disable this by setting `distributed.comm.compression` to
    `None`.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 自动尝试压缩序列化数据，通常会提高性能。您可以通过将 `distributed.comm.compression` 设置为 `None` 来禁用此功能。
- en: Partitioning/Chunking Collections
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分区/分块集合
- en: Partitioning gives you the ability to control the number of tasks used to process
    your data. If you have billions of rows, using one task for each row would mean
    you spend more time scheduling the tasks than doing the work itself. Understanding
    partitioning is key to being able to make the most efficient use of Dask.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 分区使您能够控制用于处理数据的任务数量。如果有数十亿行数据，使用每行一个任务将意味着您花费更多时间在任务调度上而非实际工作本身。了解分区是能够最有效地使用
    Dask 的关键。
- en: Dask uses slightly different terminology for partitioning in each of its collections.
    In Dask, partitioning impacts how data is located on your cluster, and the right
    partitioning for your problem can make order-of-magnitude improvements. Partitioning
    has a few different aspects, like the size of each partition, the number of partitions,
    and optional properties such as partition key and sorted versus unsorted.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 在其各个集合中为分区使用略有不同的术语。在 Dask 中，分区影响数据在集群上的位置，而对于您的问题来说，选择合适的分区方式可以显著提高性能。分区有几个不同的方面，如每个分区的大小、分区的数量以及可选的属性，如分区键和排序与否。
- en: The number and size of partitions are closely related and impact the maximum
    parallelism. Partitions that are too small or too great in number mean that Dask
    will spend more time scheduling the tasks than running them. A general sweet spot
    for partition size is around 100 MB to 1 GB, but if your computation per element
    is very expensive, smaller partition sizes can perform better.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 分区的数量和大小密切相关，并影响最大并行性能。分区太小或数量过多会导致 Dask 在调度任务而非运行任务时花费更多时间。分区大小的一般最佳范围约为100
    MB到1 GB，但如果每个元素的计算非常昂贵，较小的分区大小可能会表现更好。
- en: Ideally, your partitions should be similar in size to avoid stragglers. A situation
    in which you’ve got partitions of different sizes is called *skewed*. There are
    many different sources of skew, ranging from input file sizes to key skew (when
    keyed). When your data gets too skewed, you will need to repartition the data.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，分区大小应该相似，以避免出现滞后情况。分区大小不同的情况称为 *skewed*。导致数据不平衡的原因有很多，从输入文件大小到键的偏斜（当有键时）。当数据过于不平衡时，您需要重新分区数据。
- en: Tip
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: The Dask UI is a great place to see if you might have stragglers.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Dask UI 是查看是否可能有滞后任务的好地方。
- en: Dask Arrays
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dask 数组
- en: Dask arrays’ partitions are called *chunks* and represent the number of elements.
    Although Dask always knows the number of chunks, when you apply a filter or load
    data, Dask is unaware of the size of each chunk. Indexing or slicing a Dask array
    requires that Dask know the chunk sizes so it can find the chunk(s) with the desired
    elements. Depending on how your Dask array was created, Dask may or may not know
    the size of each chunk. We talk about this more in [Chapter 5](ch05.xhtml#ch05).
    If you want to index into an array where Dask does not know the chunk sizes, you
    will need to first call `compute_chunk_sizes()` on the array. When creating a
    Dask array from a local collection, you can specify the target chunk size, as
    shown in [Example 3-6](#custom_array_chunk_size).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 数组的分区被称为 *chunks*，表示元素的数量。尽管 Dask 总是知道分区的数量，但当您应用过滤器或加载数据时，Dask 不知道每个分区的大小。索引或切片
    Dask 数组需要 Dask 知道分区大小，以便找到包含所需元素的分区。根据创建 Dask 数组的方式，Dask 可能知道每个分区的大小，也可能不知道。我们在[第5章](ch05.xhtml#ch05)中会更详细地讨论这个问题。如果要索引一个
    Dask 数组，而 Dask 不知道分区大小，您需要先在数组上调用 `compute_chunk_sizes()`。当从本地集合创建 Dask 数组时，可以指定目标分区大小，如[示例 3-6](#custom_array_chunk_size)所示。
- en: Example 3-6\. Custom array chunk size
  id: totrans-75
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-6\. 自定义数组分块大小
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Partitions/chunking doesn’t have to be static, and the `rechunk` function allows
    you to change the chunk size of a Dask array.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 分区/分块不一定是静态的，`rechunk` 函数允许您更改 Dask 数组的分块大小。
- en: Dask Bags
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dask Bags
- en: Dask bags’ partitions are called *partitions*. Unlike with Dask arrays, since
    Dask bags do not support indexing, Dask does not track the number of elements
    in each partition. When you use `scatter`, Dask will try to partition the data
    as well as possible, but subsequent iterations can change the number of elements
    inside each partition. Similar to Dask arrays, when creating from a local collection,
    you can specify the number of partitions of a bag, except the parameter is called
    `npartitions` instead of `chunks`.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Dask bags 的分区称为 *partitions*。与 Dask 数组不同，由于 Dask bags 不支持索引，因此 Dask 不跟踪每个分区中的元素数量。当使用
    `scatter` 时，Dask 将尝试尽可能地分区数据，但后续的迭代可能会改变每个分区中的元素数量。与 Dask 数组类似，从本地集合创建时，可以指定 bag
    的分区数量，只是参数称为 `npartitions` 而不是 `chunks`。
- en: You can change the number of partitions in a bag by calling `repartition` with
    either `npartitions` (for a fixed number of partitions) or `partition_size` (for
    a target size of each partition). Specifying `partition_size` is more expensive
    since Dask needs to do some extra computation to determine what the matching number
    of partitions would be.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过调用 `repartition` 来更改 bag 中的分区数量，可以指定 `npartitions`（用于固定数量的分区）或 `partition_size`（用于每个分区的目标大小）。指定
    `partition_size` 更昂贵，因为 Dask 需要进行额外的计算来确定匹配的分区数量。
- en: You can think of data as keyed when there is an index or when the data can be
    looked up by a value, such as in a hashtable. While bags implement keyed operations
    like `groupBy`, where values with the same key are combined, their partitioning
    does not have any idea of key and instead keyed operations always operate on all
    partitions.^([8](ch03.xhtml#id430))
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据具有索引或数据可以通过值进行查找时，可以将数据视为键控。虽然 bags 实现了像 `groupBy` 这样的键控操作，其中具有相同键的值被合并，但其分区并不考虑键，而是总是在所有分区上执行键控操作。^([8](ch03.xhtml#id430))
- en: Dask DataFrames
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dask DataFrames
- en: DataFrames have the most options for partitioning. DataFrames can have partitions
    of different sizes, as well as known or unknown partitioning. With unknown partitioning,
    the data is distributed, but Dask is unable to determine which partition holds
    a particular key. Unknown partitioning happens often, as any operation that could
    change the value of a key results in unknown partitioning. The `known_divisions`
    property on a DataFrame allows you to see whether Dask knows the partitioning,
    and the `index` property shows the splits used and the column.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrames 在分区方面拥有最多的选项。DataFrames 可以具有不同大小的分区，以及已知或未知的分区方式。对于未知的分区方式，数据是分布式的，但
    Dask 无法确定哪个分区持有特定的键。未知的分区方式经常发生，因为任何可能改变键值的操作都会导致未知的分区方式。DataFrame 上的 `known_divisions`
    属性允许您查看 Dask 是否知道分区方式，而 `index` 属性显示了使用的拆分和列。
- en: If a DataFrame has the right partitioning, operations like `groupBy`, which
    would normally involve a lot of internode communication, can be executed with
    less communication. Accessing rows by ID requires that the DataFrame is partitioned
    on that key. If you want to change the column that your DataFrame is partitioned
    on, you can call `set_index` to change the index. Setting the index, like all
    the repartitioning operations, involves copying the data between workers, known
    as a *shuffle*.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 DataFrame 拥有正确的分区方式，则像 `groupBy` 这样的操作，通常涉及大量节点间通信，可以通过较少的通信来执行。通过 ID 访问行需要
    DataFrame 在该键上进行分区。如果要更改 DataFrame 分区的列，可以调用 `set_index` 来更改索引。像所有的重新分区操作一样，设置索引涉及在工作节点之间复制数据，称为
    *shuffle*。
- en: Tip
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: The “right” partitioner for a dataset depends on not only the data but also
    your operations.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据集来说，“正确”的分区器取决于数据本身以及您的操作。
- en: Shuffles
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Shuffles
- en: Shuffling refers to transferring data between different workers to repartition
    the data. Shuffling can be the result of an explicit operation, like calling `repartition`,
    or an implicit one, like grouping data together by key or performing an aggregation.
    Shuffles tend to be relatively expensive, so it’s useful to minimize how often
    they are needed and also to reduce the amount of data they move.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Shuffling 指的是在不同的工作节点之间转移数据以重新分区数据。Shuffling 可能是显式操作的结果，例如调用 `repartition`，也可能是隐式操作的结果，例如按键分组数据或执行聚合操作。Shuffle
    操作往往比较昂贵，因此尽量减少其需要的频率，并减少其移动的数据量是很有用的。
- en: The most straightforward case to understand shuffles is when you explicitly
    ask Dask to repartition your data. In those cases you generally see many-to-many
    worker communication, with the majority of the data needing to be moved over the
    network. This is naturally more expensive than situations in which data is able
    to be processed locally, as the network is much slower than RAM.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 理解洗牌的最直接的情况是当您明确要求 Dask 重新分区数据时。在这些情况下，您通常会看到多对多的工作器通信，大多数数据需要在网络上传输。这自然比能够在本地处理数据的情况更昂贵，因为网络比
    RAM 慢得多。
- en: Another important way that you can trigger shuffles is implicitly through a
    reduction/aggregation. In such cases, if parts of the reduction or aggregation
    can be applied prior to moving the data around, Dask is able to transfer less
    data over the network, making for a faster shuffle.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 触发洗牌的另一个重要方式是通过隐式地进行缩减/聚合。在这种情况下，如果可以在移动数据之前应用部分缩减或聚合，Dask 就能够在网络上传输更少的数据，从而实现更快的洗牌。
- en: Tip
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Sometimes you’ll see things referred to as *map-side* and *reduce-side*; this
    just means before and after the shuffle.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 有时您会看到事物被称为 *map-side* 和 *reduce-side*；这只是洗牌之前和之后的意思。
- en: We’ll explore more about how to minimize the impact of shuffles in the next
    two chapters, where we introduce aggregations.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的两章中更深入地探讨如何最小化洗牌的影响，介绍聚合。
- en: Partitions During Load
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 载入期间的分区
- en: So far you’ve seen how to control partitions when creating from a local collection,
    as well as how to change the partitioning of an existing distributed collection.
    Partitioning during the creation of a collection from delayed tasks is generally
    1:1, with each delayed task being its own partition. When loading data from files,
    partitioning becomes a bit more complicated, involving file layout and compression.
    Generally speaking, it is good practice to look at the partitioning of data you
    have loaded by calling `npartitions` for bags, `chunks` for arrays, or `index`
    for DataFrames.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经看到如何在从本地集合创建时控制分区，以及如何更改现有分布式集合的分区。从延迟任务创建集合时，分区通常是 1:1 的，每个延迟任务都是自己的分区。当从文件加载数据时，分区变得有些复杂，涉及文件布局和压缩。一般来说，查看已加载数据的分区的做法是调用
    bags 的`npartitions`、数组的`chunks`或 DataFrames 的`index`。
- en: Tasks, Graphs, and Lazy Evaluation
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 任务、图和延迟评估
- en: Tasks are the building blocks that Dask uses to implement `dask.delayed`, futures,
    and operations on Dask’s collections. Each task represents a small piece of computation
    that Dask cannot break down any further. Tasks are often fine-grained, and when
    computing a result Dask will try to combine multiple tasks into a single execution.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 任务是 Dask 用于实现`dask.delayed`、futures 和 Dask 集合上的操作的构建块。每个任务代表了 Dask 无法再进一步分解的一小部分计算。任务通常是细粒度的，计算结果时
    Dask 会尝试将多个任务组合成单个执行。
- en: Lazy Evaluation
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 惰性评估
- en: Most of Dask is lazily evaluated, with the exception of Dask futures. Lazy evaluation
    shifts the responsibility for combining computations from you to the scheduler.
    This means that Dask will, when it makes sense, combine multiple function calls.
    Not only that, but if only some parts of a structure are needed, Dask is sometimes
    able to optimize by evaluating just the relevant parts (like `head` or `tail`
    calls).^([9](ch03.xhtml#id439)) Implementing lazy evaluation requires Dask to
    construct a task graph. This task graph is also reused for fault tolerance.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数 Dask 是惰性评估的，除了 Dask futures。惰性评估将组合计算的责任从您转移到调度程序。这意味着 Dask 将在合适时合并多个函数调用。不仅如此，如果只需要结构的一些部分，Dask
    有时能够通过仅评估相关部分（如`head`或`tail`调用）进行优化。实现惰性评估需要 Dask 构建一个任务图。这个任务图也被用于容错。
- en: Unlike most of Dask, futures are eagerly evaluated, which limits the optimizations
    available when chaining them together, as the scheduler has a less complete view
    of the world when it starts executing the first future. Futures still create task
    graphs, and you can verify this by visualizing them in the next section.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 与大多数 Dask 不同，futures 是急切地评估的，这限制了在将它们链接在一起时可用的优化，因为调度程序在开始执行第一个 future 时对世界的视图不够完整。Futures
    仍然创建任务图，您可以通过在下一节中可视化它们来验证这一点。
- en: Unlike the rest of Dask, futures are eagerly evaluated, which limits the optimizations
    available when chaining them together, as the scheduler has a less complete view
    of the world when it starts executing the first future. Futures still create task
    graphs, and you can verify this by visualizing them, as we’ll see in the next
    section.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Dask 的其余部分不同，未来值是急切评估的，这限制了在将它们链接在一起时可用的优化，因为调度程序在执行第一个未来时对世界的视图不完整。未来仍然创建任务图，您可以通过可视化它们来验证，正如我们将在下一节中看到的那样。
- en: Task Dependencies
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 任务依赖
- en: In addition to nested tasks, as seen in [“Nested tasks”](ch02.xhtml#nested_tasks),
    you can also use a `dask.delayed` object as input to another delayed computation
    (see [Example 3-7](#task_dependencies_example_ch03_1686946028901)), and Dask’s
    `submit`/`compute` function will construct a task graph for you.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 除了嵌套任务外，如 [“嵌套任务”](ch02.xhtml#nested_tasks) 中所见，您还可以将 `dask.delayed` 对象作为另一个延迟计算的输入（参见
    [示例 3-7](#task_dependencies_example_ch03_1686946028901)），Dask 的 `submit`/`compute`
    函数将为您构建任务图。
- en: Example 3-7\. Task dependencies
  id: totrans-104
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-7\. 任务依赖
- en: '[PRE6]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now when you go to compute the final combined value, Dask will compute all of
    the other values that are needed for the final function using its implicit task
    graph.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当您计算最终组合值时，Dask 将使用其隐式任务图计算所有其他需要的最终函数值。
- en: Note
  id: totrans-107
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You don’t need to pass around real values. For example, if one function updates
    a database and you want to run another function after that, you can use it as
    a parameter even if you don’t actually need its Python return value.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 您不需要传递真实值。例如，如果一个函数更新数据库，而您希望在此之后运行另一个函数，即使您实际上不需要其 Python 返回值，也可以将其用作参数。
- en: By passing delayed objects into other delayed function calls, you allow Dask
    to re-use shared nodes in the task graph and potentially reduce network overhead.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将延迟对象传递到其他延迟函数调用中，您允许 Dask 重用任务图中的共享节点，从而可能减少网络开销。
- en: visualize
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: visualize
- en: Visualizing the task graph is an excellent tool for you to use while learning
    about task graphs and debugging in the future. The `visualize` function is defined
    both in the Dask library and on all Dask objects. Instead of calling `.visualize`
    separately on multiple objects, you should call `dask.visualize` with the list
    of objects you are planning to compute to see how Dask combines the task graph.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习任务图和未来调试时，可视化任务图是一个优秀的工具。`visualize` 函数在 Dask 库和所有 Dask 对象中都有定义。与在多个对象上单独调用
    `.visualize` 不同，您应该调用 `dask.visualize` 并传递您计划计算的对象列表，以查看 Dask 如何组合任务图。
- en: You should try this out now by visualizing Examples [2-6](ch02.xhtml#make_bag_of_crawler)
    through [2-9](ch02.xhtml#wc_func). When you call `dask.visualize` on `words_bag​.fre⁠quen⁠cies()`,
    you should get a result that looks something like [Figure 3-2](#vis_ex).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该立即通过可视化示例 [2-6](ch02.xhtml#make_bag_of_crawler) 至 [2-9](ch02.xhtml#wc_func)
    来尝试这个。当你在 `words_bag​.fre⁠quen⁠cies()` 上调用 `dask.visualize` 时，你应该得到类似 [图 3-2](#vis_ex)
    的结果。
- en: '![spwd 0302](Images/spwd_0302.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![spwd 0302](Images/spwd_0302.png)'
- en: Figure 3-2\. Visualized word count task graph (redrawn output)
  id: totrans-114
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-2\. 可视化的单词计数任务图（重新绘制输出）
- en: Tip
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: The Dask UI also shows visualized representations of the task graph, without
    the need to modify your code.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Dask UI 还显示任务图的可视化表示，无需修改您的代码。
- en: Intermediate Task Results
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 中间任务结果
- en: Intermediate task results are generally removed as soon as the dependent task
    has started to execute. This can be less than optimal when we need to perform
    multiple computations on the same piece of data. One solution to this is combining
    all our execution together into one call to `dask.compute`, so that Dask is able
    to keep the data around as needed. This breaks down in both the interactive case,
    where we don’t know in advance what our computation is going to be, and iterative
    cases. In those cases, some form of caching or persistence can be beneficial.
    You will learn about how to apply caching later in this chapter.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦依赖任务开始执行，中间任务结果通常会立即被删除。当我们需要对相同数据执行多个计算时，这可能不够优化。解决这个问题的一个方法是将所有执行组合到一个对
    `dask.compute` 的调用中，以便 Dask 可以根据需要保留数据。在交互式案例中，这种方法会失败，因为我们事先不知道计算是什么，也会在迭代案例中出现类似的问题。在这些情况下，某种形式的缓存或持久性可能是有益的。您将在本章后面学习如何应用缓存。
- en: Task Sizing
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 任务大小
- en: Dask uses a centralized scheduler, which is a common technique for many systems.
    It does mean, however, that while the general task scheduling overhead is 1 ms,
    as the number of tasks in a system increases the scheduler can become a bottleneck,
    and the overhead can grow. Counterintuitively, this means that as our system grows
    we may benefit from larger, coarser-grained tasks.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 使用集中式调度器，这是许多系统的常见技术。但这也意味着，尽管一般任务调度的开销只有 1 毫秒，但随着系统中任务数量的增加，调度器可能会成为瓶颈，开销也会增加。令人反直觉的是，这意味着随着我们系统的扩展，我们可能会从更大、更粗粒度的任务中受益。
- en: When Task Graphs Get Too Large
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 当任务图变得太大
- en: Sometimes the task graph itself can become too much for Dask to handle. This
    issue can show up as an out-of-memory exception on the client or scheduler or,
    more commonly, as jobs that slow down with iterations. Most frequently this occurs
    with recursive algorithms. One common example of a situation in which the graph
    can become too expensive to keep is distributed alternating least squares.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 有时任务图本身可能对 Dask 处理过多。这个问题可能表现为客户端或调度器上的内存不足异常，或者更常见的是随迭代变慢的作业。最常见的情况是递归算法。一个常见的示例是分布式交替最小二乘法。
- en: The first step when encountering a situation with a too-large task graph is
    to see if you can reduce the parallelism by using larger chunks of work or by
    switching the algorithm. For example, if we think of Fibonacci numbers computed
    with recursion, a better option would be using a dynamic programming or memoized
    solution instead of trying to distribute the computation task with Dask.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 遇到任务图过大的情况时的第一步是看看是否可以通过使用更大的工作块或切换算法来减少并行性。例如，如果我们考虑使用递归计算斐波那契数列，更好的选择是使用动态规划或记忆化解决方案，而不是尝试使用
    Dask 分发计算任务。
- en: If you have an iterative algorithm, and there isn’t a better way to accomplish
    what you want, you can help Dask out by periodically writing out the intermediate
    work and re-loading it.^([10](ch03.xhtml#id454)) By doing this, Dask does not
    have to keep track of all of the steps involved in creating the data, but instead
    just needs to remember where the data is. The next two chapters will look at how
    to write and load data efficiently for these and other purposes.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个迭代算法，并且没有更好的方法来实现你想要的效果，可以通过定期写入中间结果并重新加载来帮助 Dask。^([10](ch03.xhtml#id454))
    这样一来，Dask 就不必跟踪创建数据的所有步骤，而只需记住数据的位置。接下来的两章将讨论如何有效地为这些及其他目的编写和加载数据。
- en: Tip
  id: totrans-125
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: In Spark the equivalent idea is expressed as *checkpointing*.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 中，等价的概念被称为 *checkpointing*。
- en: Combining Computation
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结合计算
- en: To take the most advantage of Dask’s graph optimization, it’s important to submit
    your work in larger batches. First off, when you’re blocking on the result with
    `dask.compute`, small batches can limit the parallelism. If you have a shared
    parent—say, two results on the same data—submitting the computations together
    allows Dask to share the computation of the underlying data. You can verify whether
    Dask is able to share a common node by calling `visualize` on your list of tasks
    (e.g., if you take Examples [2-8](ch02.xhtml#wc_freq) and [2-9](ch02.xhtml#wc_func)
    and visualize both the tasks together, you’ll see the shared node in [Figure 3-2](#vis_ex)).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 要充分利用 Dask 的图优化，最重要的是以较大的批次提交你的工作。首先，当你在 `dask.compute` 中阻塞在结果上时，小批次会限制并行性。如果有一个共享的父节点——比如说，同一数据上的两个结果——一起提交计算允许
    Dask 共享底层数据的计算。你可以通过在任务列表上调用 `visualize` 来验证 Dask 能否共享一个公共节点（例如，如果你将示例 [2-8](ch02.xhtml#wc_freq)
    和 [2-9](ch02.xhtml#wc_func) 一起可视化，你将在 [Figure 3-2](#vis_ex) 中看到共享节点）。
- en: Sometimes you can’t submit the computations together, but you still know that
    you want to reuse some data. In those cases you should explore persistence.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 有时你不能一起提交计算，但你仍然知道你想要重用一些数据。在这些情况下，你应该探索持久化。
- en: Persist, Caching, and Memoization
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 持久化、缓存和记忆化
- en: Persistence allows you to keep specified Dask collections in memory on the cluster.
    To persist a collection for future reuse, just call `dask.persist` on the collection.
    If you choose persistence, you will be responsible for telling Dask when you are
    done with a distributed collection. Unlike Spark, Dask does not have an easy `unpersist`
    equivalent; instead, you need to release the underlying futures for each partition
    as shown in [Example 3-8](#manual_persist).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 持久化允许你在集群中将指定的Dask集合保留在内存中。要为将来重用持久化一个集合，只需在集合上调用`dask.persist`。如果选择持久化，你需要负责告诉Dask何时完成对分布式集合的使用。与Spark不同，Dask没有简单的`unpersist`等效方法；相反，你需要释放每个分区的底层future，就像在[示例
    3-8](#manual_persist)中所示。
- en: Example 3-8\. Manual persistence and memory management with Dask
  id: totrans-132
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-8. 使用Dask进行手动持久化和内存管理
- en: '[PRE7]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Warning
  id: totrans-134
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: A common mistake is to persist and cache things that are used only once or are
    inexpensive to compute.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的错误是持久化和缓存那些仅被使用一次或计算成本低廉的东西。
- en: Dask’s local mode has a best-effort caching system based on cachey. Since this
    only works in local mode, we won’t go into the details, but if you are running
    in local mode, you can take a look at [the local cache documentation](https://oreil.ly/VFSVQ).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Dask的本地模式具有基于cachey的尽力缓存系统。由于这仅在本地模式下工作，我们不会深入讨论细节，但如果你在本地模式下运行，可以查看[本地缓存文档](https://oreil.ly/VFSVQ)。
- en: Warning
  id: totrans-137
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Dask does not raise an error when you attempt to use Dask caching in a distributed
    fashion; it just won’t work. So when migrating code from local to distributed,
    make sure to check for usage of Dask’s local caching.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 当你尝试在分布式方式下使用Dask缓存时，Dask不会引发错误；它只是不起作用。因此，在从本地迁移到分布式时，请确保检查Dask本地缓存的使用情况。
- en: Fault Tolerance
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容错
- en: In distributed systems like Dask, *fault tolerance* generally refers to how
    a system handles computer, network, or program failures. Fault tolerance becomes
    increasingly important the more computers you use. When you are using Dask on
    a single computer, the concept of fault tolerance is less important, since if
    your one computer fails, there is nothing to recover. However, when you have hundreds
    of machines, the odds of a machine failing go up. Dask’s task graph is used to
    provide its fault tolerance.^([11](ch03.xhtml#id465)) There are many different
    kinds of failures in a distributed system, but thankfully many of them can be
    handled in the same way.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在像Dask这样的分布式系统中，“容错性”通常指的是系统如何处理计算机、网络或程序故障。随着使用计算机数量的增加，容错性变得越来越重要。当你在单台计算机上使用Dask时，容错的概念就不那么重要，因为如果你的计算机失败了，就没有什么可恢复的了。然而，当你有数百台机器时，计算机故障的机率就会增加。Dask的任务图用于提供其容错性。^([11](ch03.xhtml#id465))
    在分布式系统中有许多不同类型的故障，但幸运的是，其中许多可以以相同的方式处理。
- en: Dask automatically retries tasks when the scheduler loses connection to the
    worker. This retry is accomplished by using the same graph of computation Dask
    uses for lazy evaluation.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 当调度器失去与工作节点的连接时，Dask会自动重试任务。这种重试是通过Dask用于惰性评估的计算图来实现的。
- en: Warning
  id: totrans-142
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: The Dask client is *not* fault tolerant to network issues connecting to the
    scheduler. One mitigation technique you can use is to run your client in the same
    network as the scheduler.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: Dask客户端对连接调度器的网络问题不具备容错能力。你可以采用的一种减轻技术是在与调度器相同的网络中运行你的客户端。
- en: Machine failure is a fact of life with distributed systems. When a worker fails,
    Dask will treat it in the same way as a network failure, retrying any necessary
    tasks. However, Dask cannot recover from failures of the scheduler of your client
    code.^([12](ch03.xhtml#id466)) This makes it important that, when you are running
    in a shared environment, you run your client and scheduler nodes at a high priority
    to avoid preemption.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式系统中，机器故障是生活中的一个事实。当一个工作节点失败时，Dask会像处理网络故障一样重新尝试任何必要的任务。然而，Dask无法从客户端代码的调度器失败中恢复。^([12](ch03.xhtml#id466))
    因此，在运行于共享环境时，高优先级运行客户端和调度器节点是非常重要的，以避免被抢占。
- en: Dask automatically retries software failures that exit or crash the worker.
    Much like machine failure, from Dask’s point of view a worker exiting and a network
    failing look the same.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Dask会自动重试由于软件失败而退出或崩溃的工作节点。从Dask的角度来看，工作节点退出和网络故障看起来是一样的。
- en: IOError and OSError exceptions are the only two classes of exceptions Dask will
    retry. If your worker process raises one of these errors, the exception is pickled
    and transferred over to the scheduler. Dask’s scheduler then retries the task.
    If your code encounters an IOError that should not be retried (e.g., a web page
    doesn’t exist), you’ll need to wrap it in another exception to keep Dask from
    retrying it.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: IOError 和 OSError 异常是 Dask 将重试的唯二异常类。如果您的工作进程引发其中一个错误，异常将被 pickle 并传输到调度程序。然后
    Dask 的调度程序会重试任务。如果您的代码遇到不应重试的 IOError（例如，网页不存在），您需要将其包装在另一个异常中，以防止 Dask 重新尝试它。
- en: Since Dask retries failed computation, it’s important to be careful with side
    effects or changing values. For example, if you have a Dask bag of transactions
    and were to update a database as part of a `map`, Dask might re-execute some of
    the operations on that bag multiple times, resulting in the update to the database
    happening more than once. If we think of a withdrawal from an ATM, we can see
    how this would result in some unhappy customers and incorrect data. Instead, if
    you need to mutate small bits of data, you can bring them back to a local collection.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Dask 重试失败的计算，因此在处理副作用或更改值时要小心。例如，如果您有一个 Dask transactions 的 bag，并且在 `map`
    的一部分更新数据库，Dask 可能会多次重新执行该 bag 上的某些操作，导致数据库更新多次发生。如果我们考虑从 ATM 取款，就能看出这将导致一些不满意的客户和不正确的数据。相反，如果您需要改变小数据位，请将它们带回本地集合。
- en: If your program encounters any other exceptions, Dask will return the exception
    to your main thread.^([13](ch03.xhtml#id472))
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的程序遇到其他异常，Dask 将把异常返回到您的主线程。^([13](ch03.xhtml#id472))
- en: Conclusion
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: After this chapter you should have a good grasp of how Dask is able to scale
    your Python code. You should now understand the basics of partitioning, why this
    matters, task sizing, and Dask’s approach to fault tolerance. This will hopefully
    set you up well for deciding when to apply Dask, and for the next few chapters,
    where we do a deeper dive into Dask’s collection libraries. In the next chapter
    we’ll focus on Dask’s DataFrames, as they are the most full-featured of Dask’s
    distributed collections.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本章，您应该对 Dask 如何扩展您的 Python 代码有了很好的掌握。您现在应该了解分区的基础知识，为什么这很重要，任务大小以及 Dask 对容错的方法。这将有助于您决定何时应用
    Dask，并在接下来的几章中深入研究 Dask 的集合库。在下一章中，我们将专注于 Dask 的 DataFrames，因为它们是 Dask 分布式集合中功能最全的。
- en: ^([1](ch03.xhtml#id374-marker)) Unless you work for a cloud provider and computers
    are close to free. If you do work for a cloud provider, please send us cloud credits.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch03.xhtml#id374-marker)) 除非您为云提供商工作并且计算机几乎是免费的。如果您确实为云提供商工作，请发送给我们云积分。
- en: ^([2](ch03.xhtml#id375-marker)) Including OS X and Linux.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch03.xhtml#id375-marker)) 包括 OS X 和 Linux。
- en: ^([3](ch03.xhtml#id376-marker)) This also involves having to store a second
    copy of any object that is in the driver thread and then used in a worker. Since
    Dask shards its collections, this doesn’t generally blow up as quickly as it would
    with normal multi-processing.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch03.xhtml#id376-marker)) 这还涉及必须在驱动程序线程中有对象的第二个副本，然后在工作程序中使用。由于 Dask 对其集合进行分片，这通常不会像正常的多处理那样迅速扩展。
- en: ^([4](ch03.xhtml#id383-marker)) Say that five times fast.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch03.xhtml#id383-marker)) 快速连续说五次。
- en: ^([5](ch03.xhtml#id386-marker)) As in many real-world situations, it’s easier
    to grow and harder to shrink the number of Dask nodes
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch03.xhtml#id386-marker)) 就像许多现实世界的情况一样，增加 Dask 节点比减少更容易。
- en: ^([6](ch03.xhtml#id400-marker)) Automatically picking up and shipping the libraries
    would be very hard and also slow, although it can be done under certain circumstances.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch03.xhtml#id400-marker)) 自动挑选和运输库将非常困难，而且也很慢，尽管在某些情况下可以完成。
- en: ^([7](ch03.xhtml#id416-marker)) See Dask distributed GitHub issues [5561](https://oreil.ly/DmFxp)
    and [2953](https://oreil.ly/RxzGS).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch03.xhtml#id416-marker)) 参见 Dask 分布式 GitHub 问题 [5561](https://oreil.ly/DmFxp)
    和 [2953](https://oreil.ly/RxzGS)。
- en: ^([8](ch03.xhtml#id430-marker)) Coming from databases, you can think of this
    as a “full-scan” or “full-shuffle” for Spark folks with `groupBy`.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch03.xhtml#id430-marker)) 对于来自数据库的人来说，您可以将其视为 Spark 的 `groupBy` 的“全扫描”或“全洗牌”。
- en: ^([9](ch03.xhtml#id439-marker)) When Dask can optimize evaluation here is complicated,
    but remember that a task is the fundamental unit of computation and Dask cannot
    break down compute any further inside the task. So a DataFrame created from many
    individual tasks that you call `head` on is a great candidate for Dask to optimize,
    but for a single task making a large DataFrame, Dask is unable to break “inside.”
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch03.xhtml#id439-marker)) 当Dask可以优化评估时，这里的情况复杂，但请记住，任务是计算的基本单位，Dask无法在任务内进一步分解计算。因此，从许多个体任务创建的DataFrame，当您调用`head`时，是Dask优化的一个很好的候选对象；但对于创建大DataFrame的单个任务，Dask无法内部“深入”分解。
- en: ^([10](ch03.xhtml#id454-marker)) You could also collect and scatter if the dataset
    is small enough.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch03.xhtml#id454-marker)) 如果数据集足够小，您也可以进行收集和分散。
- en: ^([11](ch03.xhtml#id465-marker)) The same technique used in Spark.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch03.xhtml#id465-marker)) 在Spark中使用的相同技术。
- en: ^([12](ch03.xhtml#id466-marker)) This is common for most systems like this.
    Spark does have limited ability to recover from head node failure, but it has
    many restrictions and is not frequently used.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch03.xhtml#id466-marker)) 这在大多数类似系统中很常见。Spark确实具有从头节点故障中恢复的有限能力，但有许多限制，且不经常使用。
- en: ^([13](ch03.xhtml#id472-marker)) For those migrating from Spark, this retry
    behavior is different. Spark will retry most exceptions, whereas Dask will only
    retry errors resulting in a worker exiting, or an IOError or OSError.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ^([13](ch03.xhtml#id472-marker)) 对于从Spark迁移的用户，此重试行为有所不同。Spark会对大多数异常进行重试，而Dask仅在工作节点退出或出现IOError或OSError时重试。
