- en: Chapter 12\. Lessons from the Field
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 12 章。现场经验教训
- en: In this chapter we have collected stories from successful companies that use
    Python in high-data-volume and speed-critical situations. The stories are written
    by key people in each organization who have many years of experience; they share
    not just their technology choices but also some of their hard-won wisdom. We have
    four great new stories for you from other experts in our domain. We’ve also kept
    the “Lessons from the Field” from the first edition of this book; their titles
    are marked “(2014).”
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们收集了一些成功公司的故事，这些公司在高数据量和速度关键的情况下使用 Python。这些故事由每个组织中具有多年经验的关键人员撰写；他们不仅分享了他们的技术选择，还分享了一些宝贵的经验。我们为您带来了来自领域内其他专家的四个新故事。我们还保留了本书第一版的“现场经验教训”，其标题标有“(2014)”。
- en: Streamlining Feature Engineering Pipelines with Feature-engine
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用 Feature-engine 简化特征工程流程
- en: Soledad Galli (trainindata.com)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Soledad Galli (trainindata.com)
- en: Train in Data is an education project led by experienced data scientists and
    AI software engineers. We help professionals improve coding and data science skills
    and adopt machine learning best practices. We create advanced online courses on
    machine learning and AI software engineering and open source libraries, like [Feature-engine](https://feature-engine.readthedocs.io),
    to smooth the delivery of machine learning solutions.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Train in Data 是一个由经验丰富的数据科学家和人工智能软件工程师领导的教育项目。我们帮助专业人士提高编码和数据科学技能，并采用机器学习最佳实践。我们开设先进的机器学习和人工智能软件工程的在线课程，并开发开源库，如
    [Feature-engine](https://feature-engine.readthedocs.io)，以简化机器学习解决方案的交付过程。
- en: Feature Engineering for Machine Learning
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习的特征工程
- en: Machine learning models take in a bunch of input variables and output a prediction.
    In finance and insurance, we build models to predict, for example, the likelihood
    of a loan being repaid, the probability of an application being fraudulent, and
    whether a car should be repaired or replaced after an accident. The data we collect
    and store or recall from third-party APIs is almost never suitable to train machine
    learning models or return predictions. Instead, we transform variables extensively
    before feeding them to machine learning algorithms. We refer to the collection
    of variable transformations as *feature engineering*.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型接收大量输入变量并输出预测结果。在金融和保险领域，我们建立模型来预测诸如贷款偿还的可能性、申请欺诈的概率，以及事故后车辆应当修理还是更换的可能性。我们收集的数据几乎从不适合直接用于训练机器学习模型或返回预测结果。相反，我们在将数据馈送给机器学习算法之前会对变量进行广泛的转换。我们将变量转换的集合称为*特征工程*。
- en: Feature engineering includes procedures to impute missing data, encode categorical
    variables, transform or discretize numerical variables, put features in the same
    scale, combine features into new variables, extract information from dates, aggregate
    transactional data, and derive features from time series, text, or even images.
    There are many techniques for each of these feature engineering steps, and your
    choice will depend on the characteristics of the variables and the algorithms
    you intend to use. Thus, when feature engineers build and consume machine learning
    in organizations, we do not speak of machine learning models but of machine learning
    pipelines, where a big part of the pipeline is devoted to feature engineering
    and data transformation.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程包括缺失数据的填充、分类变量的编码、数值变量的转换或离散化、将特征放置在相同的尺度上、将特征组合成新变量、从日期中提取信息、汇总交易数据，以及从时间序列、文本甚至图像中提取特征。每个特征工程步骤都有许多技术，您的选择将取决于变量的特征和您打算使用的算法。因此，当特征工程师在组织中构建和使用机器学习时，我们不再谈论单一的机器学习模型，而是谈论机器学习流水线，其中流水线的一大部分专注于特征工程和数据转换。
- en: The Hard Task of Deploying Feature Engineering Pipelines
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署特征工程流水线的艰难任务
- en: Many feature engineering transformations learn parameters from data. I have
    seen organizations utilize config files with hardcoded parameters. These files
    limit versatility and are hard to maintain (every time you retrain your model,
    you need to rewrite the config file with the new parameters). To create highly
    performant feature engineering pipelines, it’s better to develop algorithms that
    automatically learn and store these parameters and can also be saved and loaded,
    ideally as one object.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 许多特征工程转换从数据中学习参数。我曾见过一些组织使用硬编码参数的配置文件。这些文件限制了灵活性，且难以维护（每次重新训练模型时，都需要用新的参数重新编写配置文件）。要创建性能高效的特征工程流水线，最好开发算法来自动学习和存储这些参数，而且还可以保存和加载，理想情况下作为一个对象。
- en: At Train in Data, we develop machine learning pipelines in the research environment
    and deploy them in the production environment. These pipelines should be reproducible.
    Reproducibility is the ability to duplicate a machine learning model exactly,
    such that, given the same data as input, both models return the same output. Utilizing
    the same code in the research and production environment smooths the deployment
    of machine learning pipelines by minimizing the amount of code to be rewritten,
    maximizing reproducibility.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Train in Data，我们在研究环境中开发机器学习流水线，并将其部署到生产环境中。这些流水线应该是可重复的。可重复性是准确复制一个机器学习模型的能力，这样，给定相同的数据作为输入，两个模型都会返回相同的输出。在研究和生产环境中利用相同的代码可以通过最小化需要重新编写的代码量，最大化可重复性来平滑地部署机器学习流水线。
- en: Feature engineering transformations need to be tested. Unit tests for each feature
    engineering procedure ensure that the algorithm returns the desired outcome. Extensive
    code refactoring in production to add unit and integration tests is extremely
    time-consuming and provides new opportunities to introduce bugs, or find bugs
    introduced during the research phase due to the lack of testing. To minimize code
    refactoring in production, it is better to introduce unit testing as we develop
    the engineering algorithms in the research phase.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程转换需要进行测试。对每个特征工程过程进行单元测试，以确保算法返回所需的结果。在生产中进行广泛的代码重构以添加单元测试和集成测试非常耗时，并提供了引入错误的新机会，或者在研究阶段由于缺乏测试而引入的错误。为了最小化生产中的代码重构，最好在研究阶段开发工程算法时引入单元测试。
- en: The same feature engineering transformations are used across projects. To avoid
    different code implementations of the same technique, which often occurs in teams
    with many data scientists, and to enhance team performance, speed up model development,
    and smooth model operationalization, we want to reuse code that was previously
    built and tested. The best way to do that is to create in-house packages. Creating
    packages may seem time-consuming, since it involves building tests and documentation.
    But it is more efficient in the long run, because it allows us to enhance code
    and add new features incrementally, while reusing code and functionality that
    has already been developed and tested. Package development can be tracked through
    versioning and even shared with the community as open source, raising the profile
    of the developers and the organization.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 各项目中使用相同的特征工程转换。为了避免在拥有许多数据科学家的团队中经常发生的同一技术的不同代码实现，并增强团队绩效，加快模型开发速度，以及平滑模型操作化，我们希望重用先前构建和测试过的代码。最好的方法是创建内部包。创建包可能看起来很耗时，因为它涉及构建测试和文档。但从长远来看，这是更有效的，因为它允许我们逐步增强代码并添加新功能，同时重用已经开发和测试过的代码和功能。包开发可以通过版本控制来跟踪，甚至可以作为开源与社区共享，提升开发人员和组织的知名度。
- en: Leveraging the Power of Open Source Python Libraries
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用开源 Python 库的力量
- en: 'It’s important to use established open source projects or thoroughly developed
    in-house libraries. This is more efficient for several reasons:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 使用已建立的开源项目或彻底开发的内部库非常重要，原因如下：
- en: Well-developed projects tend to be thoroughly documented, so it is clear what
    each piece of code intends to achieve.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经过良好开发的项目往往有详尽的文档，因此清楚每段代码的目的是什么。
- en: Well-established open source packages are tested to prevent the introduction
    of bugs, ensure that the transformation achieves the intended outcome, and maximize
    reproducibility.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已建立的开源包经过测试，以防止引入错误，确保转换达到预期的结果，并最大程度地实现可重复性。
- en: Well-established projects have been widely adopted and approved by the community,
    giving you peace of mind that the code is of quality.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成熟的项目已被社区广泛采纳和认可，这使您放心代码质量。
- en: You can use the same package in the research and production environment, minimizing
    code refactoring during deployment.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以在研究和生产环境中使用相同的包，最小化部署过程中的代码重构。
- en: Packages are clearly versioned, so you can deploy the version you used when
    developing your pipeline to ensure reproducibility, while newer versions continue
    to add functionality.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包被明确版本化，因此您可以部署您在开发管道时使用的版本，以确保可重现性，而新版本则持续添加功能。
- en: Open source packages can be shared, so different organizations can build tools
    together.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开源包可以共享，因此不同的组织可以共同构建工具。
- en: While open source packages are maintained by a group of experienced developers,
    the community can also contribute, providing new ideas and features that raise
    the quality of the package and code.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然开源包由一组经验丰富的开发者维护，但社区也可以贡献，提供新的想法和功能，从而提升包和代码的质量。
- en: Using a well-established open source library removes the task of coding from
    our hands, improving team performance, reproducibility, and collaboration, while
    reducing model research and deployment timelines.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用一个成熟的开源库减轻了我们编码的负担，提高了团队的表现、可重现性和协作能力，同时缩短了模型研究和部署的时间表。
- en: Open source Python libraries like [scikit-learn](https://oreil.ly/j-4ob), [Category
    encoders](https://oreil.ly/DtSL7), and [Featuretools](https://oreil.ly/DOB7V)
    provide feature engineering functionality. To expand on existing functionality
    and smooth the creation and deployment of machine learning pipelines, I created
    the open source Python package [Feature-engine](https://oreil.ly/CZrSB), which
    provides an exhaustive battery of feature engineering procedures and supports
    the implementation of different transformations to distinct feature spaces.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 像[scikit-learn](https://oreil.ly/j-4ob)，[Category encoders](https://oreil.ly/DtSL7)，和[Featuretools](https://oreil.ly/DOB7V)这样的开源Python库提供了特征工程的功能。为了扩展现有的功能并平滑创建和部署机器学习管道，我创建了开源Python包[Feature-engine](https://oreil.ly/CZrSB)，提供了一系列详尽的特征工程过程，并支持不同转换实施到不同的特征空间。
- en: Feature-engine Smooths Building and Deployment of Feature Engineering Pipelines
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征工程平滑了特征工程管道的构建和部署。
- en: Feature engineering algorithms need to learn parameters from data automatically,
    return a data format that facilitates use in research and production environments,
    and include an exhaustive battery of transformations to encourage adoption across
    projects. Feature-engine was conceived and designed to fulfill all these requirements.
    Feature-engine transformers—that is, the classes that implement a feature engineering
    transformation—learn and store parameters from data. Feature-engine transformers
    return Pandas DataFrames, which are suitable for data analysis and visualization
    during the research phase. Feature-engine supports the creation and storage of
    an entire end-to-end engineering pipeline in a single object, making deployment
    easier. And to facilitate adoption across projects, it includes an exhaustive
    list of feature transformations.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程算法需要自动从数据中学习参数，返回一个有利于在研究和生产环境中使用的数据格式，并包含详尽的变换，以促进在项目中的采纳。Feature-engine的构思和设计就是为了满足所有这些要求。Feature-engine的转换器—即实施特征工程转换的类—从数据中学习并存储参数。Feature-engine的转换器返回适合于数据分析和可视化的Pandas数据框架，这在研究阶段非常有用。Feature-engine支持在单个对象中创建和存储整个端到端的工程管道，使部署更加容易。为了促进在项目中的采纳，它包含了详尽的特征转换列表。
- en: Feature-engine includes many procedures to impute missing data, encode categorical
    variables, transform and discretize numerical variables, and remove or censor
    outliers. Each transformer can learn, or alternatively have specified, the group
    of variables it should modify. Thus, the transformer can receive the entire dataframe,
    yet it will alter only the selected variable group, removing the need of additional
    transformers or manual work to slice the dataframe and then join it back together.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Feature-engine包括许多处理缺失数据、编码分类变量、转换和离散化数值变量以及移除或屏蔽异常值的过程。每个转换器可以自动学习，或者明确指定它应该修改的变量组。因此，转换器可以接收整个数据框架，但仅会改变所选变量组，无需额外的转换器或手动操作来切片数据框架然后重新连接它们。
- en: Feature-engine transformers use the fit/transform methods from scikit-learn
    and expand its functionality to include additional engineering techniques. Fit/transform
    functionality makes Feature-engine transformers usable within the scikit-learn
    pipeline. Thus with Feature-engine we can store an entire machine learning pipeline
    into a single object that can be saved and retrieved or placed in memory for live
    scoring.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Feature-engine的转换器使用scikit-learn的fit/transform方法，并扩展其功能以包括其他工程技术。 Fit/transform功能使Feature-engine的转换器可以在scikit-learn管道中使用。
    因此，使用Feature-engine，我们可以将整个机器学习管道存储为一个单一对象，该对象可以保存和检索或放置在内存中进行实时评分。
- en: Helping with the Adoption of a New Open Source Package
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 协助新开源包的采纳
- en: No matter how good an open source package is, if no one knows it exists or if
    the community can’t easily understand how to use it, it will not succeed. Making
    a successful open source package entails making code that is performant, well
    tested, well documented, and useful—and then letting the community know that it
    exists, encouraging its adoption by users who can suggest new features, and attracting
    a developer community to add more functionality, improve the documentation, and
    enhance code quality to raise its performance. For a package developer, this means
    that we need to factor in time to develop code and to design and execute a sharing
    strategy. Here are some strategies that have worked for me and for other package
    developers.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 无论一个开源包有多好，如果没有人知道它的存在或者社区不能轻松理解如何使用它，它都不会成功。 制作一个成功的开源包意味着制作性能良好，经过充分测试，良好文档和有用的代码—然后让社区知道它的存在，鼓励用户采用并建议新功能，并吸引开发者社区增加更多功能，改进文档和提高代码质量来提高其性能。
    对于包开发者来说，这意味着我们需要考虑开发代码的时间，设计和执行共享策略。 以下是我和其他包开发者的一些有效策略。
- en: We can leverage the power of well-established open source functionality to facilitate
    adoption by the community. Scikit-learn is the reference library for machine learning
    in Python. Thus, adopting scikit-learn fit/transform functionality in a new package
    facilitates an easy and fast adoption by the community. The learning curve to
    use the package is shorter as users are already familiar with this implementation.
    Some packages that leverage the use of fit/transform are [Keras](https://keras.io),
    Category encoders (perhaps the most renowned), and of course Feature-engine.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用成熟的开源功能的力量来促进社区的采用。 Scikit-learn是Python中机器学习的参考库。 因此，在新包中采用scikit-learn的fit/transform功能可以使社区轻松快速地采用。
    使用该实现的用户对使用该包的学习曲线较短。 一些利用fit/transform的包包括[Keras](https://keras.io)，Category
    encoders（也许是最著名的），当然还有Feature-engine。
- en: Users want to know how the package can be used and shared, so include a license
    stating these conditions in the code repository. Users also need instructions
    and examples of the code functionality. Docstrings in code files with information
    about functionality and examples of its use is a good start, but it is not enough.
    Widespread packages include additional documentation (which can be generated with
    ReStructuredText files) with descriptions of code functionality, examples of its
    use and the outputs returned, installation guidelines, the channels where the
    package is available (PyPI, conda), how to get started, changelogs, and more.
    Good documentation should empower users to use the library without reading the
    source code. The documentation of the machine learning visualization library [Yellowbrick](https://oreil.ly/j96lT)
    is a good example. I have adopted this practice for Feature-engine as well.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 用户希望了解如何使用和分享该包，因此在代码存储库中包含声明这些条件的许可证。 用户还需要代码功能的说明和示例。 在代码文件的文档字符串中包含关于功能的信息和使用示例是一个良好的开始，但这还不足够。
    广泛使用的包包括附加文档（可以使用ReStructuredText文件生成），其中包括代码功能的描述，使用示例和返回的输出，安装指南，包可用的渠道（PyPI，conda），如何入门，变更日志等。
    良好的文档应该使用户能够在不阅读源代码的情况下使用库。 机器学习可视化库[Yellowbrick](https://oreil.ly/j96lT)的文档是一个很好的例子。
    我也已经在Feature-engine中采用了这一做法。
- en: How can we increase package visibility? How do we reach potential users? Teaching
    an online course can help you reach people, especially if it’s on a prestigious
    online learning platform. In addition, publishing documentation in [Read the Docs](https://readthedocs.org),
    creating YouTube tutorials, and presenting the package at meetups and meetings
    all increase visibility. Presenting the package functionality while answering
    relevant questions in well-established user networks like Stack Overflow, Stack
    Exchange, and Quora can also help. The developers of Featuretools and Yellowbrick
    have leveraged the power of these networks. Creating a dedicated Stack Overflow
    issues list lets users ask questions and shows the package is being actively maintained.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如何提高软件包的可见性？我们如何接触潜在用户？开设在线课程可以帮助您接触更多人群，特别是在知名的在线学习平台上。此外，在[Read the Docs](https://readthedocs.org)上发布文档，创建YouTube教程，参加meetup和会议等活动都能增加可见度。在像Stack
    Overflow、Stack Exchange和Quora这样的成熟用户网络中，介绍软件包的功能并回答相关问题也很有帮助。Featuretools和Yellowbrick的开发者已经利用了这些网络的力量。创建专用的Stack
    Overflow问题列表可以让用户提问，并显示软件包正在积极维护中。
- en: Developing, Maintaining, and Encouraging Contribution to Open Source Libraries
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开发、维护和鼓励对开源库的贡献
- en: For a package to be successful and relevant, it needs an active developer community.
    A developer community is composed of one or, ideally, a group of dedicated developers
    or maintainers who will watch for overall functionality, documentation, and direction.
    An active community allows and welcomes additional ad hoc contributors.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 要使软件包成功且具有相关性，需要一个活跃的开发者社区。开发者社区由一个或者最好是一组专注的开发者或维护者组成，他们将关注整体功能、文档和发展方向。一个活跃的社区允许并欢迎额外的临时贡献者。
- en: One thing to consider when developing a package is code maintainability. The
    simpler and shorter the code, the easier it is to maintain, which makes it more
    likely to attract contributors and maintainers. To simplify development and maintainability,
    Feature-engine leverages the power of scikit-learn base transformers. Scikit-learn
    provides an API with a bunch of base classes that developers can build upon to
    create new transformers. In addition, scikit-learn’s API provides many tests to
    ensure compatibility between packages and also that the transformer delivers the
    intended functionality. By using these, Feature-engine developers and maintainers
    focus only on feature engineering functionality, while the maintenance of base
    code is taken care of by the bigger scikit-learn community. This, of course, has
    a trade-off. If scikit-learn changes its base functionality, we need to update
    our library to ensure it is compatible with the latest version. Other open source
    packages that use scikit-learn API are Yellowbrick and Category encoders.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发软件包时需要考虑的一件事是代码的可维护性。代码越简单和短小，维护起来就越容易，这样吸引贡献者和维护者的可能性就越大。为了简化开发和维护工作，Feature-engine利用了scikit-learn基础转换器的强大功能。Scikit-learn提供了一个API和一堆基础类，开发者可以在此基础上构建新的转换器。此外，scikit-learn的API提供了许多测试，以确保包与包之间的兼容性，以及转换器提供了预期功能。通过使用这些功能，Feature-engine的开发者和维护者可以专注于特征工程功能，而基础代码的维护则由更大的scikit-learn社区负责。当然，这也有一个权衡。如果scikit-learn改变了其基础功能，我们需要更新我们的库以确保与最新版本兼容。其他使用scikit-learn
    API的开源软件包包括Yellowbrick和Category encoders。
- en: To encourage developers to collaborate, [NumFOCUS](https://numfocus.org) recommends
    creating a code of conduct and encouraging inclusion and diversity. The project
    needs to be open, which generally means the code should be publicly hosted, with
    guidelines to orient new contributors about project development and discussion
    forums open to public participation, like a mailing list or a Slack channel. While
    some open source Python libraries have their own codes of conduct, others, like
    Yellowbrick and Feature-engine, follow the [Python Community Code of Conduct](https://oreil.ly/8k4Tc).
    Many open source projects, including Feature-engine, are publicly hosted on GitHub.
    Contributing guidelines list ways new contributors can help—for example, by fixing
    bugs, adding new functionality, or enhancing the documentation. Contributing guidelines
    also inform new developers of the contributing cycle, how to fork the repository,
    how to work on the contributing branch, how the code review cycle works, and how
    to make a Pull Request.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了鼓励开发者合作，[NumFOCUS](https://numfocus.org) 建议制定行为准则，并鼓励包容和多样性。项目需要开放，通常意味着代码应该是公开托管的，并且有指导新贡献者了解项目开发和讨论的指南，如邮件列表或
    Slack 频道。虽然一些开源 Python 库有自己的行为准则，其他如 Yellowbrick 和 Feature-engine 则遵循 [Python
    Community Code of Conduct](https://oreil.ly/8k4Tc)。许多开源项目，包括 Feature-engine，在
    GitHub 上公开托管。贡献指南列出了新贡献者可以帮助的方式，例如修复错误、添加新功能或增强文档。贡献指南还告知新开发者贡献周期、如何分叉存储库、如何在贡献分支上工作、代码审查周期如何工作以及如何发起
    Pull Request。
- en: Collaboration can raise the quality and performance of the library by enhancing
    code quality or functionality, adding new features, and improving documentation.
    Contributions can be as simple as reporting typos in the documentation, reporting
    code that doesn’t return the intended outcome, or requesting new features. Collaborating
    on open source libraries can also help raise the collaborator’s profile while
    exposing them to new engineering and coding practices, improving their skills.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 合作可以通过提升代码质量或功能，添加新功能以及改进文档，提升库的质量和性能。贡献可以简单到报告文档中的拼写错误，报告未返回预期结果的代码，或请求新功能。在开源库上合作也有助于提升合作者的知名度，同时让他们接触新的工程和编码实践，提高他们的技能。
- en: Many developers and data scientists believe that they need to be top-notch developers
    to contribute to open source projects. I used to believe that myself, and it discouraged
    me from making contributions or requesting features—even though, as a user, I
    had a clear idea of what features were available and which were missing. This
    is far from true. Any user can contribute to the library. And package maintainers
    love contributions.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 许多开发者和数据科学家认为他们需要成为顶尖开发者才能为开源项目做贡献。我曾经也这样认为，这让我不敢贡献或请求功能——尽管作为用户，我清楚地知道哪些功能是可用的，哪些是缺失的。这远非事实。任何用户都可以为库做贡献。而包维护者喜欢贡献。
- en: Useful contributions to Feature-engine have included simple things like adding
    a line to the *.gitignore*, sending a message through LinkedIn to bring typos
    in the docs to my attention, making a PR to correct typos themselves, highlighting
    warning issues raised by newer versions of scikit-learn, requesting new functionality,
    or expanding the battery of unit tests.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Feature-engine 的有用贡献包括简单的事情，比如在 *.gitignore* 中添加一行，通过 LinkedIn 发送消息提醒我文档中的拼写错误，提交
    PR 以纠正拼写错误本身，突出显示由较新版本的 scikit-learn 引起的警告问题，请求新功能或扩展单元测试的电池。
- en: If you want to contribute but have no experience, it is useful to go through
    the issues found on the package repository. Issues are lists with the modifications
    to the code that have priority. They are tagged with labels like “code enhancement,”
    “new functionality,” “bug fix,” or “docs.” To start, it is good to go after issues
    tagged as “good first issue” or “good for new contributors”; those tend to be
    smaller code changes and will allow you to get familiar with the contribution
    cycle. Then you can jump into more complex code modifications. Just by solving
    an easy issue, you will learn a lot about software development, Git, and code
    review cycles.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想贡献但没有经验，查看包仓库中的问题是很有用的。问题是一系列对代码有优先级的修改列表。它们被标记为“代码增强”，“新功能”，“错误修复”或“文档”。首先，最好处理标记为“good
    first issue”或“good for new contributors”的问题；这些问题往往是较小的代码更改，可以让你熟悉贡献周期。然后你可以着手更复杂的代码修改。通过解决一个简单的问题，你将学到很多关于软件开发，Git和代码审查周期的知识。
- en: Feature-engine is currently a small package with straightforward code implementations.
    It is easy to navigate and has few dependencies, so it is a good starting point
    for contributing to open source. If you want to get started, do get in touch.
    I will be delighted to hear from you. Good luck!
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Feature-engine目前是一个功能简单的小型包，代码实现直接。它易于导航，并且依赖较少，因此是贡献到开源项目的良好起点。如果您想开始，请与我联系。我将非常乐意听到您的消息。祝您好运！
- en: Highly Performant Data Science Teams
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高性能数据科学团队
- en: Linda Uruchurtu (Fiit)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Linda Uruchurtu（Fiit）
- en: Data science teams are different from other technical teams, because the scope
    of what they do varies according to where they sit and the type of problems they
    tackle. However, whether the team is responsible for answering “why” and “how”
    questions or simply delivering fully operational ML services, in order to deliver
    successfully, they need to keep the stakeholders happy.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学团队与其他技术团队不同，因为他们的工作范围取决于他们所处的位置和他们解决的问题类型。然而，无论团队负责回答“为什么”和“如何”的问题，还是仅仅交付完全操作的机器学习服务，为了成功交付，他们需要确保利益相关者满意。
- en: This can be challenging. Most data science projects have a degree of uncertainty
    attached to them, and since there are different types of stakeholders, “happy”
    can mean different things. Some stakeholders might be concerned only with final
    deliverables, whereas others might care about side effects or common interfaces.
    Additionally, some might not be technical or have a limited understanding of the
    specifics of the project. Here I will share some lessons I have learned that make
    a difference in the way projects are carried out and delivered.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是具有挑战性的。大多数数据科学项目都存在一定程度的不确定性，而且由于利益相关者的类型不同，“满意”可能意味着不同的事情。有些利益相关者可能只关心最终交付物，而其他人可能关心副作用或通用接口。此外，有些人可能不具备技术背景，对项目的具体细节理解有限。在这里，我将分享一些我学到的在项目执行和交付方式上产生差异的经验教训。
- en: How Long Will It Take?
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 需要多长时间？
- en: 'This is possibly the most common question data science team leads are asked.
    Picture the following: management asks the project manager (PM), or whoever is
    responsible for delivery, to solve a given problem. The PM goes to the team, presents
    them with this information, and asks them to plan a solution. Cue this question,
    from the PM or from other stakeholders: How long will it take?'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是数据科学团队领导经常被问到的问题。想象一下：管理层要求项目经理（PM）或负责交付的人解决一个特定的问题。项目经理去找团队，向他们展示这些信息，并要求他们计划一个解决方案。接着来自项目经理或其他利益相关者的问题：需要多长时间？
- en: 'First, the team should ask questions to better define the scope of their solutions.
    These might include the following:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，团队应该提出问题，以更好地定义他们解决方案的范围。这些问题可能包括以下内容：
- en: Why is this a problem?
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么这是一个问题？
- en: What is the impact of solving this problem?
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决此问题的影响是什么？
- en: What is the definition of done?
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “完成”的定义是什么？
- en: What is the minimal version of a solution that satisfies said definition?
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是满足定义的最小版本解决方案？
- en: Is there a way to validate a solution early on?
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否有办法在早期验证解决方案？
- en: Notice that “How long will it take?” isn’t on this list.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，“需要多长时间？”不在这个列表上。
- en: The strategy should be twofold. First, get a time-boxed period to ask these
    questions and propose one or more solutions. Once a solution is agreed upon, the
    PM should explain to stakeholders that the team can provide a timeline once the
    work for said solution is planned.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 策略应该是双管齐下的。首先，设定一个限时的期间来提出这些问题并提出一个或多个解决方案。一旦达成一致的解决方案，项目经理应该向利益相关者解释，团队可以在计划了解决方案工作后提供一个时间表。
- en: Discovery and Planning
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 发现与规划
- en: The team has a fixed amount of time to come up with solutions. What’s next?
    They need to generate hypotheses, followed by exploratory work and quick prototyping,
    to keep or discard potential solutions successively.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 团队有一定的时间来提出解决方案。接下来呢？他们需要提出假设，进行探索性工作和快速原型设计，依次保留或放弃潜在的解决方案。
- en: Depending on the solution chosen, other teams may become stakeholders. Development
    teams could have requirements from APIs they hold, or they could become consumers
    of a service; product, operations, customer service, and other teams might use
    visualizations and reports. The PM’s team should discuss their ideas with these
    teams.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 根据所选择的解决方案，其他团队可能成为利益相关者。开发团队可能会有来自其持有的API的需求，或者它们可能成为服务的消费者；产品、运营、客户服务和其他团队可能会使用可视化和报告。项目经理的团队应该与这些团队讨论他们的想法。
- en: Once this process has taken place, the team is usually in a good position to
    determine how much uncertainty and/or risk adheres to each option. The PM can
    now assess which option is preferred.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦此过程完成，团队通常能够确定每个选项所带来的不确定性和/或风险有多少。项目经理现在可以评估哪个选项更可取。
- en: 'Once an option is chosen, the PM can define a timeline for milestones and deliverables.
    Useful points to raise here are as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦选择了一个选项，项目经理可以为里程碑和交付定义一个时间表。这里提出的有用点是：
- en: Can the deliverables be reasonably reviewed and tested?
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交付物是否可以合理地审查和测试？
- en: If work depends on other teams, can work be scheduled so no delay is introduced?
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果工作依赖于其他团队，能否安排工作以避免引入延迟？
- en: Can the team provide value from intermediate milestones?
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 团队能否从中间里程碑中提供价值？
- en: Is there a way to reduce risk from parts of the project that have a significant
    amount of uncertainty?
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否有办法减少项目中存在显著不确定性部分的风险？
- en: 'Tasks derived from the plan can then be sized and time-boxed to provide a time
    estimate. It is a good idea to allow for extra time: some people like to double
    or triple the time they think they’ll need!'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 根据计划导出的任务可以进行规模化和时间分配，以提供时间估计。留出额外的时间是个好主意：有些人喜欢将他们认为需要的时间翻倍或翻三倍！
- en: Some tasks are frequently underestimated and simplified, including data collection
    and dataset building, testing, and validation. Getting good data for model building
    can often be more complex and expensive than it initially seems. One option may
    be to start with small datasets for prototyping and postpone further collection
    until after proof of concept. Testing, too, is fundamental, both for correctness
    and for reproducibility. Are inputs as expected? Are processing pipelines introducing
    errors? Are outputs correct? Unit testing and integration tests should be part
    of every effort. Finally, validation is important, particularly in the real world.
    Be sure to factor in realistic estimates for all of these tasks.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一些任务经常被低估和简化，包括数据收集和数据集构建，测试和验证。为模型构建获取良好的数据通常比起初看起来更复杂和昂贵。一种选择可能是从小数据集开始进行原型设计，并推迟进一步的收集工作。测试也是基础，既用于正确性又用于可重复性。输入是否符合预期？处理管道是否引入错误？输出是否正确？单元测试和集成测试应成为每个努力的一部分。最后，验证很重要，特别是在现实世界中。确保为所有这些任务考虑现实的估计。
- en: Once you’ve done that, the team has not only an answer to the “time” question,
    but also a plan with milestones that everyone can use to understand what work
    is being carried out.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成了这些步骤，团队不仅对“时间”问题有了答案，还有了一个每个人都能用来理解正在进行的工作的里程碑计划。
- en: Managing Expectations and Delivery
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管理期望和交付
- en: 'Lots of issues can affect the time required before a delivery is achieved.
    Keep an eye on the following points to make sure you manage the team’s expectations:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在交付之前可能需要的时间会受到许多问题的影响。注意以下几点以确保管理团队的期望：
- en: Scope creep
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 范围蔓延
- en: Scope creep is the subtle shifting of the scope of work, so that more work is
    expected than was initially planned. Pairing and reviews can help mitigate this.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 范围蔓延是工作范围微妙的移动，以便期望比最初计划的工作更多。配对和审查可以帮助减轻这种情况。
- en: Underestimating nontechnical tasks
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 低估非技术任务
- en: Discussions, user research, documentation, and many other tasks can easily be
    underestimated by those who don’t know them well.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论、用户研究、文档编写和许多其他任务往往会被那些不熟悉它们的人低估。
- en: Availability
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 可用性
- en: Team members’ scheduling and availability can also introduce delays.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 团队成员的安排和可用性也可能会引入延迟。
- en: Issues with data quality
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 数据质量问题
- en: From making sure working datasets are good to go to discovering sources of bias,
    data quality can introduce complications or even invalidate pieces of work.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 确保工作数据集可用并发现偏见来源，数据质量可能会引入复杂性，甚至使工作无效。
- en: Alternative options
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 可选方案
- en: When unexpected difficulties arise, it might make sense to consider other approaches.
    However, sunk costs might prevent the team from wanting to raise this, which could
    delay the work and risk creating the impression that the team doesn’t know what
    they are doing.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 当出现意外困难时，考虑其他方法可能是有道理的。然而，沉没成本可能会阻止团队想要提出这一点，这可能会延迟工作，并且有可能给人一种团队不知所措的印象。
- en: Lack of testing
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 缺乏测试
- en: Sudden changes in data inputs or bugs in data pipelines can invalidate assumptions.
    Having good test coverage from the start will improve team velocity and pay dividends
    at the end.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 数据输入的突然变化或数据管道中的错误可能会使假设无效。从一开始就有良好的测试覆盖率将提高团队的速度，并在最后产生回报。
- en: Difficulty testing or validating
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 测试或验证困难
- en: If not enough time is allowed for testing and validating hypotheses, the schedule
    can be delayed. Changes in assumptions can also lead to alterations in the testing
    plan.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有足够的时间进行测试和验证假设，计划可能会延迟。假设变更也可能导致测试计划的更改。
- en: Use weekly refinement and planning sessions to spot issues and discuss whether
    tasks need to be added or removed. This will give the PM enough information to
    update the final stakeholders. Prioritization should also happen with the same
    cadence. If opportunities arise to do some tasks earlier than expected, these
    should be pushed forward.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 使用每周的完善和计划会议来发现问题，并讨论是否需要添加或移除任务。这将为项目经理提供足够的信息来更新最终的利益相关者。优先级也应该以相同的频率进行。如果有机会提前完成某些任务，应该抓住这些机会。
- en: Intermediate deliverables, particularly if they provide value outside the scope
    of the project, continuously justify the work. That’s good for the team, in terms
    of focus and morale, as well as stakeholders, who will have a sense of progress.
    The continuous process of redrawing the game plan and reviewing and adjusting
    iterations will ensure the team has a clear sense of direction and freedom to
    work, while providing enough information and value to keep stakeholders keen on
    continuing to support the project.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 中间交付成果，特别是如果它们在项目范围之外提供价值，将不断证明工作的正当性。这对团队和利益相关者都是有利的，可以保持关注和士气，并让利益相关者感受到进展。持续重新绘制游戏计划、审查和调整迭代过程将确保团队有清晰的方向感和工作自由，同时提供足够的信息和价值，以保持利益相关者继续支持项目。
- en: To become highly performant while tackling a new project, your data science
    team’s main focus has to be on making data uncertainty and business-need uncertainty
    less risky by delivering lightweight minimum viable product (MVP) solutions (think
    scripts and Python notebooks). The initial conceived MVP might actually turn out
    to be leaner than or different from the first concept, given findings down the
    line or changes in business needs. Only after validation should you proceed with
    a production-ready version.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 要在处理新项目时表现出色，你的数据科学团队主要应集中精力降低数据不确定性和业务需求不确定性的风险，通过交付轻量化的最小可行产品（MVP）解决方案（比如脚本和Python笔记本）。最初构想的MVP可能实际上比首个概念更精简或不同，根据随后的发现或业务需求变化。只有经过验证后，你才应继续推进到可投入生产的版本。
- en: The discovery and planning process is critical, and so is thinking in terms
    of iterations. Keep in mind that the discovery phase is always ongoing and that
    external events will always affect the plan.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 发现和规划过程至关重要，迭代思维同样重要。记住发现阶段始终在进行中，外部事件将始终影响计划。
- en: Numba
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Numba
- en: Valentin Haenel ([*http://haenel.co*](http://haenel.co))
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Valentin Haenel ([*http://haenel.co*](http://haenel.co))
- en: '*Numba* is an open source, JIT function compiler for numerically focused Python.
    Initially created at Continuum Analytics (now Anaconda Inc) in 2012, it has since
    grown into a mature open source project on GitHub with a large and diverse group
    of contributors. Its primary use case is the acceleration of numerical and/or
    scientific Python code. The main entry point is a decorator—the `@jit` decorator—which
    is used to annotate the specific functions, ideally the bottlenecks of the application,
    that should be compiled. Numba will compile these functions just-in-time, which
    simply means that the function will be compiled at the first, or initial, execution.
    All subsequent executions with the same argument types will then use the compiled
    variant of the function, which should be faster than the original.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*Numba* 是一个针对数值计算的Python的开源JIT（即时编译）函数编译器。最初在2012年由Continuum Analytics（现在的Anaconda
    Inc）创建，它已经发展成为一个成熟的开源项目，在GitHub上拥有大量和多样化的贡献者。其主要用例是加速数值和/或科学Python代码。主要的入口点是装饰器——`@jit`装饰器，用于注释特定的函数，理想情况下是应用程序的瓶颈，这些函数将被即时编译，这意味着函数将在第一次或初始执行时编译。所有后续具有相同参数类型的执行将使用函数的编译变体，这应该比原始函数更快。'
- en: Numba not only can compile Python, but also is NumPy-aware and can handle code
    that uses NumPy. Under the hood, Numba relies on the well-known [LLVM project](https://llvm.org),
    a collection of modular and reusable compiler and toolchain technologies. Last,
    Numba isn’t a fully fledged Python compiler. It can compile only a subset of both
    Python and NumPy—albeit a large enough subset to make it useful in a wide range
    of applications. For more information, please consult the [documentation](https://numba.pydata.org).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Numba不仅可以编译Python，还能识别NumPy，并处理使用NumPy的代码。在底层，Numba依赖于著名的[LLVM项目](https://llvm.org)，这是一组模块化和可重用的编译器及工具链技术。此外，Numba并非完整的Python编译器。它只能编译Python和NumPy的子集，尽管这个子集足够大，可以在广泛的应用中发挥作用。更多信息，请参阅[文档](https://numba.pydata.org)。
- en: A Simple Example
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个简单的例子
- en: 'As a simple example, let’s use Numba to accelerate a Python implementation
    of an ancient algorithm for finding all prime numbers up to a given maximum (*N*):
    the sieve of Eratosthenes. It works as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个简单的例子，让我们使用Numba来加速一个Python实现的古老算法，用于找出所有不超过给定最大值(*N*)的素数：厄拉托斯特尼筛法。它的工作原理如下：
- en: First, initialize a Boolean array of length *N* to all true values.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，初始化一个长度为*N*的布尔数组，所有值都为true。
- en: Then starting with the first prime, 2, cross off (set the position in the Boolean
    list corresponding to that number to false) all multiples of the number up to
    *N*.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后从第一个素数2开始，划掉（将布尔列表中相应位置设置为false）所有不超过*N*的数字的倍数。
- en: Proceed to the next number that has not yet been crossed off, in this case 3,
    and again cross off all multiples of it.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 继续处理下一个尚未划掉的数字，即3，在这种情况下再次划掉所有它的倍数。
- en: Continue to proceed through the numbers and cross off their multiples until
    you reach *N*.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 继续处理数字并划掉它们的倍数，直到达到*N*。
- en: When you reach *N*, all the numbers that have not been crossed off are the set
    of prime numbers up to *N*.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你达到*N*时，所有未被划掉的数字就是*N*以内的素数集合。
- en: 'A reasonably efficient Python implementation might look something like this:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 一个相对高效的Python实现可能看起来像这样：
- en: '[PRE0]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'After placing this in a file called *sieve.py*, you can use the `%timeit` magic
    to micro-benchmark the code:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 将此放入名为*sieve.py*的文件中后，您可以使用`%timeit`魔法来对代码进行微基准测试：
- en: '[PRE1]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'That is a speedup of roughly four hundredfold; your mileage may vary. Nonetheless,
    there are a few things of interest here:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这使速度提高了大约四百倍；实际效果可能有所不同。尽管如此，这里有一些值得注意的事项：
- en: Compilation happened at the function level.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编译发生在函数级别。
- en: Simply adding the decorator `@jit` was enough to instruct Numba to compile the
    function. No further modifications to the function source code, such as type annotations
    of the variables, were needed.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单地添加装饰器`@jit`就足以指示Numba编译函数。不需要对函数源代码进行其他修改，例如变量的类型注解。
- en: Numba is NumPy-aware, so all the NumPy calls in this implementation were supported
    and could be compiled successfully.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Numba识别NumPy，因此此实现中的所有NumPy调用都得到支持，并且可以成功编译。
- en: The original, pure Python function is available as the `py_func` attribute of
    the compiled function.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始的、纯Python函数在编译函数中作为`py_func`属性可用。
- en: There is a faster but less educational version of this algorithm, the implementation
    of which is left to the interested reader.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个更快但不够教育的算法版本，其实现留给感兴趣的读者。
- en: Best Practices and Recommendations
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳实践和建议
- en: One of the most important recommendations for Numba is to use nopython mode
    whenever possible. To activate this mode, simply use the `nopython=True` option
    with the `@jit` decorator, as shown in the prime number example. Alternatively,
    you can use the `@njit` decorator alias, which is accessible by doing `from numba
    import njit`. In nopython mode, Numba attempts a large number of optimizations
    and can significantly improve performance. However, this mode is very strict;
    for compilation to succeed, Numba needs to be able to infer the types of all the
    variables within your function.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Numba最重要的建议之一是尽可能使用nopython模式。要激活此模式，只需在`@jit`装饰器中使用`nopython=True`选项，就像素数示例中所示的那样。或者，您可以使用`@njit`装饰器别名，通过`from
    numba import njit`访问。在nopython模式下，Numba尝试进行大量优化，可以显著提高性能。但是，此模式非常严格；为了成功编译，Numba需要能够推断函数内所有变量的类型。
- en: You can also use object mode by doing `@jit(forceobj=True)`. In this mode, Numba
    becomes very permissive about what it can and cannot compile, which limits it
    to performing a minimal set of optimizations. This is likely to have a significant
    negative effect on performance. To take advantage of Numba’s full potential, you
    really should use nopython mode.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以通过 `@jit(forceobj=True)` 使用对象模式。在这种模式下，Numba变得非常宽容，可以编译的内容非常有限，这将显著地对性能产生负面影响。为了充分利用Numba的潜力，你应该真正使用nopython模式。
- en: 'If you can’t decide whether you want to use object mode or not, there is the
    option to use an object-mode block. This can come in handy when only a small part
    of your code needs to execute in object mode: for example, if you have a long-running
    loop and would like to use string formatting to print the current progress of
    your program. For example:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不能决定是否要使用对象模式，可以选择使用对象模式块。当只有你的代码中的一小部分需要在对象模式下执行时，这将非常方便：例如，如果你有一个长时间运行的循环，并且希望使用字符串格式化来打印程序的当前进度。例如：
- en: '[PRE2]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Pay attention to the types of the variables that you use. Numba works very
    well with both NumPy arrays and NumPy views on other datatypes. Therefore, if
    you can, use NumPy arrays as your preferred data structure. Tuples, strings, enums,
    and simple scalar types such as int, float, and Boolean are also reasonably well
    supported. Globals are fine for constants, but pass the rest of your data as arguments
    to your function. Python lists and dictionaries are unfortunately not very well
    supported. This largely stems from the fact that they can be type heterogeneous:
    a specific Python list may contain differently typed items; for example, integers,
    floats and strings. This poses a problem for Numba, because it needs the container
    to hold only items of a single type in order to compile it. However, these two
    data structures are probably one of the most used features of the Python language
    and are even one of the first things programmers learn about.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 注意你使用的变量类型。Numba非常适用于NumPy数组和其他数据类型的NumPy视图。因此，如果可以的话，应该将NumPy数组作为首选数据结构。元组、字符串、枚举和简单的标量类型如整数、浮点数和布尔值也得到了合理的支持。全局变量对于常量是可以的，但是将其余数据作为函数参数传递。Python列表和字典不幸地支持得不太好。这在很大程度上源于它们可能是类型异构的：特定的Python列表可能包含不同类型的项；例如，整数、浮点数和字符串。这对Numba来说是个问题，因为它需要容器只包含单一类型的项才能编译它。然而，这两种数据结构可能是Python语言中最常用的特性之一，甚至是程序员最早学习的内容之一。
- en: 'To remedy this shortcoming, Numba supports the so-called typed containers:
    `typed-list` and `typed-dict`. These are homogeneously typed variants of the Python
    list and dict. This means that they may contain only items of a single type: for
    example, a `typed-list` of only integer values. Beyond this limitation, they behave
    much like their Python counterparts and support a largely identical API. Additionally,
    they may be used within regular Python code or within Numba compiled functions,
    and can be passed into and returned from Numba compiled functions. These are available
    from the `numba.typed` submodule. Here is a simple example of a `typed-list`:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 要弥补这个缺点，Numba支持所谓的类型化容器：`typed-list` 和 `typed-dict`。这些是Python列表和字典的同类型变体。这意味着它们只能包含单一类型的项：例如，只包含整数值的
    `typed-list`。除了这个限制，它们的行为与其Python对应物基本相同，并支持大部分相同的API。此外，它们可以在常规Python代码中或在Numba编译的函数中使用，并且可以传递给和从Numba编译的函数中返回。这些功能来自于
    `numba.typed` 子模块。这里是一个 `typed-list` 的简单示例：
- en: '[PRE3]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'While Python does have limitations, you can rethink them and understand which
    ones can be safely disregarded when using Numba. Two specific examples come to
    mind: calling functions and `for` loops. Numba enables a technique called *inlining*
    in the underlying LLVM library to optimize away the overhead of calling functions.
    This means that during compilation, any function calls that are amenable to inlining
    are replaced with a block of code that is the equivalent of the function being
    called. As a result, there is practically no performance impact from breaking
    up a large function into a few or many small ones in order to aid readability
    and comprehensibility.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Python有其局限性，但你可以重新思考它们，并理解在使用Numba时哪些可以安全地忽略。有两个具体的例子值得一提：调用函数和 `for` 循环。Numba在底层LLVM库中启用了称为
    *内联* 的技术，以优化调用函数的开销。这意味着在编译过程中，任何可进行内联的函数调用都将被替换为等效于被调用函数的代码块。因此，将一个大函数分解为几个或多个小函数以增强可读性和可理解性，几乎不会对性能产生影响。
- en: 'One of the main criticisms of Python is that its `for` loops are slow. Many
    people recommend using alternative constructs instead when attempting to improve
    the performance of a Python program: list comprehensions or even NumPy arrays.
    Numba does not suffer from this limitation, and using `for` loops in Numba compiled
    functions is fine. Observe:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Python的一个主要批评是其`for`循环速度较慢。许多人建议在试图改善Python程序性能时使用替代构造：列表推导或甚至NumPy数组。Numba不受此限制，并且在Numba编译函数中使用`for`循环是可以的。观察：
- en: '[PRE4]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can now benchmark the preceding code:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以对上述代码进行基准测试了：
- en: '[PRE5]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As you can see, both Numba compiled variants have very similar performance characteristics,
    whereas the pure Python `for` loop implementation is significantly (800 times)
    slower than its compiled counterpart.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，两个Numba编译的变体具有非常相似的性能特征，而纯Python的`for`循环实现比其编译版本慢得多（慢了800倍）。
- en: 'If you are now thinking about rewriting your NumPy array expressions as `for`
    loops, don’t! As shown in the preceding example, Numba is perfectly happy with
    NumPy arrays and their associated functions. In fact, Numba has an additional
    ace up its sleeve: an optimization known as *loop fusion*. Numba performs this
    technique predominantly on array expression operations. For example:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您现在正在考虑将您的NumPy数组表达式重写为`for`循环，请不要这样做！正如前面的例子所示，Numba完全支持NumPy数组及其相关函数。事实上，Numba还有一个额外的优化技术，称为*循环融合*。Numba主要在数组表达式操作上执行这种技术。例如：
- en: '[PRE6]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As you can see, the Numba compiled version is eight times faster than the pure
    NumPy one. What is going on? Without Numba, the array expression will lead to
    several `for` loops and several so-called temporaries in memory. Loosely speaking,
    for each arithmetic operation in the expression, a `for` loop over arrays must
    execute, and the result of each must be stored in a temporary array in memory.
    What loop fusion does is fuse the various loops over arithmetic operations together
    into a single loop, thereby reducing both the total number of memory lookups and
    the overall memory required to compute the result. In fact, the loop-fused variant
    may well look something like this:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，Numba编译版本比纯NumPy版本快8倍。到底发生了什么？没有Numba，数组表达式将导致多个`for`循环和所谓的临时内存。泛泛地说，对于表达式中的每个算术操作，都必须执行一次数组的`for`循环，并且每次的结果必须存储在内存中的临时数组中。循环融合的作用是将各种算术操作的循环合并成一个单独的循环，从而减少总体内存查找次数和计算结果所需的总内存。实际上，循环融合的变体可能看起来像这样：
- en: '[PRE7]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Running this will show performance characteristics similar to those of the
    loop-fusion example:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码将显示类似于循环融合示例的性能特征：
- en: '[PRE8]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Finally, I recommend targeting serial execution initially, but keep parallel
    execution in mind. Don’t assume from the outset that only a parallel version will
    lead to your targeted performance characteristics. Instead, focus on developing
    a clean serial implementation first. Parallelism makes everything harder to reason
    about and can be a source of difficulty when debugging problems. If you are satisfied
    with your results and would still like to investigate parallelizing your code,
    Numba does come with a `parallel=True` option for the `@jit` decorator and a corresponding
    parallel range, the `prange` construct, to make creating parallel loops easier.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我建议最初专注于串行执行，但要牢记并行执行的可能性。不要从一开始就假设只有并行版本才能达到目标性能特征。相反，专注于首先开发清晰的串行实现。并行化使得所有事情更难推理，并且在调试问题时可能会成为困难的源头。如果您对结果满意，仍然想调查并行化您的代码，Numba确实带有`parallel=True`选项用于`@jit`装饰器以及相应的并行范围，`prange`结构，使得创建并行循环更容易。
- en: Getting Help
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取帮助
- en: As of early 2020, the two main recommended communication channels for Numba
    are the [GitHub issue tracker](https://oreil.ly/hXGfE) and the [Gitter chat](https://oreil.ly/8YGl1);
    this is where the action happens. There are also a mailing list and a Twitter
    account, but these are fairly low-traffic and mostly used to announce new releases
    and other important project news.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 截至2020年初，Numba的两个主要推荐沟通渠道是[GitHub问题跟踪器](https://oreil.ly/hXGfE)和[Gitter聊天室](https://oreil.ly/8YGl1)；这是活动发生的地方。还有一个邮件列表和一个Twitter账户，但这些活动较少，主要用于宣布新发布和其他重要项目新闻。
- en: Optimizing Versus Thinking
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化与思考
- en: Vincent D. Warmerdam, Senior Person at GoDataDriven ([*http://koaning.io*](http://koaning.io))
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Vincent D. Warmerdam，GoDataDriven的高级人员（[*http://koaning.io*](http://koaning.io)）
- en: This is a story of a team that was solving the wrong problem. We were optimizing
    efficiency while ignoring effectiveness. My hope is that this story is a cautionary
    tale for others. This story actually happened, but I’ve changed parts and kept
    the details vague in the interest of keeping things incognito.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个团队解决错误问题的故事。我们在优化效率的同时忽略了效果。我希望这个故事能成为其他人的警示。这个故事实际上发生了，但我已经改变了部分内容，并且为了保持匿名性保留了细节。
- en: 'I was consulting for a client with a common logistics problem: they wanted
    to predict the number of trucks that would arrive at their warehouses. There was
    a good business case for this. If we knew the number of vehicles, we would know
    how large the workforce had to be to handle the workload for that day.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我曾为一家面临常见物流问题的客户提供咨询：他们想预测将会抵达他们仓库的卡车数量。这是一个很好的商业案例。如果我们知道车辆的数量，我们就能知道需要多大的工作人员来处理当天的工作量。
- en: The planning department had been trying to tackle this problem for years (using
    Excel). They were skeptical that an algorithm could improve things. Our job was
    to explore if machine learning could help out here.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 规划部门多年来一直在尝试解决这个问题（使用 Excel）。他们对算法能否改善事物持怀疑态度。我们的工作是探索机器学习是否能在这里提供帮助。
- en: 'From the start, it was apparent it was a difficult time-series problem:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 从一开始就显而易见这是一个困难的时间序列问题：
- en: There were many (seriously, many!) holidays that we needed to keep in mind since
    the warehouses operated internationally. The effect of the holiday might depend
    on the day of the week, since the warehouses did not open during weekends. Certain
    holidays meant demand would go up, while other holidays meant that the warehouses
    were closed (which would sometimes cause a three-day weekend).
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于仓库是国际运营的，我们需要记住许多（真的很多！）假期。假期的影响可能取决于一周中的某一天，因为仓库周末不开放。某些假期意味着需求会增加，而其他假期意味着仓库关闭（有时会导致三天的周末）。
- en: Seasonal shifts were not unheard of.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 季节性变化并不罕见。
- en: Suppliers would frequently enter and leave the market.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 供应商经常进入和退出市场。
- en: The seasonal patterns were always changing because the market was continuously
    evolving.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于市场不断变化，季节性模式始终在变化。
- en: There were many warehouses, and although they were in separate buildings, there
    was a reason to believe the number of trucks arriving at the different warehouses
    were correlated.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有许多仓库，虽然它们位于不同的建筑物中，但有理由相信抵达不同仓库的卡车数量是相关的。
- en: The diagram in [Figure 12-1](#FIG-lesson-vincent1) shows the process for the
    algorithm that would calculate the seasonal effect as well as the long-term trend.
    As long as there weren’t any holidays, our method would work. The planning department
    warned us about this; the holidays were the hard part. After spending a lot of
    time collecting relevant features, we ended up building a system that mainly focused
    on trying to deal with the holidays.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 12-1](#FIG-lesson-vincent1) 中的图示了算法计算季节效应和长期趋势的过程。只要没有假期，我们的方法就可以运行。规划部门警告我们这一点；假期是难点。在花费了大量时间收集相关特征之后，我们最终建立了一个主要专注于尝试应对假期的系统。'
- en: '![hpp2 1201](Images/hpp2_1201.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![hpp2 1201](Images/hpp2_1201.png)'
- en: Figure 12-1\. Seasonal effects and long-term trend
  id: totrans-145
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-1\. 季节效应和长期趋势
- en: So we iterated, did more feature engineering, and designed the algorithm. It
    got to the point where we needed to calculate a time-series model per warehouse,
    which would be post-processed with a heuristic model per holiday per day of the
    week. A holiday just before the weekend would cause a different shift than a holiday
    just after the weekend. As you might imagine, this calculation can get quite expensive
    when you also want to apply a grid search, as shown in [Figure 12-2](#FIG-lesson-vincent2).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们进行了迭代，进行了更多的特征工程，并设计了算法。我们到了需要为每个仓库计算一个时间序列模型的程度，这个模型将通过每个假期每天的启发式模型进行后处理。假期在周末前会引起不同的变化，而在周末后的假期则会引起另一种变化。可以想象，当你想要进行网格搜索时，这个计算变得非常昂贵，正如在
    [图 12-2](#FIG-lesson-vincent2) 中所示。
- en: '![hpp2 1202](Images/hpp2_1202.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![hpp2 1202](Images/hpp2_1202.png)'
- en: Figure 12-2\. Many variations cost a lot of compute time
  id: totrans-148
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-2\. 许多变体消耗了大量计算时间
- en: There were many effects that we had to estimate accurately, including the decay
    of the past measurements, how smooth the seasonal effect is, the regularization
    parameters, and how to tackle the correlation between different warehouses.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须准确估算许多影响因素，包括过去测量数据的衰减、季节效应的平滑程度、正则化参数以及如何处理不同仓库之间的相关性。
- en: 'What didn’t help was that we needed to predict months ahead. Another hard thing
    was the cost function: it was discrete. The planning department did not care about
    (or even appreciate ) mean squared error; they cared only about the number of
    days where the prediction error would exceed 100 trucks.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 预测未来几个月的需要并不是一件容易的事情。另一个困难是成本函数：它是离散的。规划部门并不关心（甚至不欣赏）均方误差，他们只关心预测误差超过100辆卡车的天数。
- en: You can imagine that, in addition to statistical concerns, the model presented
    performance concerns. To keep this at bay, we restricted ourselves to simpler
    machine learning models. We gained a lot of iteration speed by doing this, which
    allowed us to focus on feature engineering. A few weeks went by before we had
    a version we could demo. We had still made a model that performed well enough,
    except for the holidays.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 除了统计问题，您可以想象模型还引发了性能问题。为了避免这种情况，我们限制使用更简单的机器学习模型。通过这样做，我们大大提高了迭代速度，这使我们能够专注于特征工程。几周后，我们得到了一个可以展示的版本。尽管如此，除了假期之外，我们仍然制作了一个表现良好的模型。
- en: The model went into a proof-of-concept phase; it performed reasonably well but
    not significantly better than the current planning team’s method. The model was
    useful since it allowed the planning department to reflect if the model disagreed,
    but no one was comfortable having the model automate the planning.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 模型进入了概念验证阶段；它表现得相当不错，但并没有比当前规划团队的方法显著更好。这个模型很有用，因为它允许规划部门反思模型的意见，但没有人愿意让模型自动化规划。
- en: Then it happened. It was my final week with the client, just before a colleague
    would be taking over. I was hanging out at the coffee corner, talking with an
    analyst about a different project for which I needed some data from him. We started
    reviewing the available tables in the database. Eventually, he told me about a
    “carts” table (shown in [Figure 12-3](#FIG-lesson-vincent3)).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 然后发生了。这是我与客户的最后一周，就在同事接替我的前一刻。我和一位分析师在咖啡角闲聊，讨论我需要他提供数据的另一个项目。我们开始审查数据库中可用的表格。最后，他告诉我一个“卡车”表（如[图12-3](#FIG-lesson-vincent3)所示）。
- en: 'Me: “A carts table? What’s in there?” Analyst: “Oh, it contains all the orders
    for the carts.” Me: “Suppliers buy them from the warehouse?” Analyst: “No, actually,
    they rent them. They usually rent them three to five days in advance before they
    return them filled with goods for the warehouse.” Me: “All of your suppliers work
    this way?” Analyst: “Pretty much.”'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我：“‘卡车’表？里面有什么？” 分析师：“哦，它包含所有卡车的订单。” 我：“供应商从仓库购买它们？” 分析师：“不，实际上是租赁的。他们通常在返回填充货物的几天前租赁它们三到五天。”
    我：“所有的供应商都是这样操作的？” 分析师：“差不多。”
- en: '![hpp2 1203](Images/hpp2_1203.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![hpp2 1203](Images/hpp2_1203.png)'
- en: Figure 12-3\. Carts contain the leading information that’s critical to this
    challenge!
  id: totrans-156
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-3. 卡车包含了这个挑战至关重要的主要信息！
- en: 'I spotted the most significant performance issue of them all: we were solving
    the wrong problem. This wasn’t a machine learning problem; it was a SQL problem.
    The number of rented carts was a robust proxy for how many trucks the company
    would send. They wouldn’t need a machine learning model. We could just project
    the number of carts being rented a few days ahead, divide by the number of carts
    that fit into a truck, and get a sensible approximation of what to expect. If
    we had realized this earlier, we would not have needed to optimize the code for
    a gigantic grid search because there would have been no need.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现了所有问题中最显著的性能问题：我们解决的是错误的问题。这不是一个机器学习问题，而是一个SQL问题。租用的卡车数量是公司将派出多少辆卡车的一个强有力的代理。他们不需要一个机器学习模型。我们可以预测未来几天租用的卡车数量，除以一个卡车能容纳的卡车数量，从而得出一个合理的预测。如果我们早些意识到这一点，我们就不需要为一个巨大的网格搜索优化代码了，因为根本没有这种需要。
- en: It is rather straightforward to translate a business case into an analytical
    problem that doesn’t reflect reality. Anything that you can do to prevent this
    will yield the most significant performance benefit you can imagine.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 将业务案例转化为不反映现实的分析问题相对直接。任何可以防止这种情况发生的措施都将产生你无法想象的最显著的性能优势。
- en: Adaptive Lab’s Social Media Analytics (2014)
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Adaptive Lab 的社交媒体分析（2014）
- en: Ben Jackson ([adaptivelab.com](http://www.adaptivelab.com/))
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Ben Jackson ([adaptivelab.com](http://www.adaptivelab.com/))
- en: Adaptive Lab is a product development and innovation company based in London’s
    Tech City area, Shoreditch. We apply our lean, user-centric method of product
    design and delivery collaboratively with a wide range of companies, from start-ups
    to large corporates.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Adaptive Lab 是一家位于伦敦科技城区域 Shoreditch 的产品开发和创新公司。我们采用精益、用户中心的产品设计和交付方法，与从初创公司到大型企业的广泛合作伙伴共同合作。
- en: YouGov is a global market research company whose stated ambition is to supply
    a live stream of continuous, accurate data and insight into what people are thinking
    and doing all over the world—and that’s just what we managed to provide for them.
    Adaptive Lab designed a way to listen passively to real discussions happening
    in social media and gain insight into users’ feelings on a customizable range
    of topics. We built a scalable system capable of capturing a large volume of streaming
    information, processing it, storing it indefinitely, and presenting it through
    a powerful, filterable interface in real time. The system was built using Python.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: YouGov 是一家全球市场研究公司，其宣称的目标是提供持续、准确的数据和洞察力，了解全球人们的思想和行为，而我们正是为其实现了这一目标。Adaptive
    Lab 设计了一种 passively 监听社交媒体上实时讨论并获取用户对各种主题感受的方法。我们构建了一个可扩展的系统，能够捕获大量流数据，实时处理、长期存储，并通过强大、可过滤的界面实时展示。该系统基于
    Python 构建。
- en: Python at Adaptive Lab
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Python 在 Adaptive Lab 的应用
- en: Python is one of our core technologies. We use it in performance-critical applications
    and whenever we work with clients that have in-house Python skills, so that the
    work we produce for them can be taken on in-house.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Python 是我们的核心技术之一。我们在性能关键的应用程序中使用它，并在与具备内部 Python 技能的客户合作时也同样如此，以便他们能够内部化我们为他们提供的工作。
- en: Python is ideal for small, self-contained, long-running daemons, and it’s just
    as great with flexible, feature-rich web frameworks like Django and Pyramid. The
    Python community is thriving, which means that there’s a huge library of open
    source tools out there that allow us to build quickly and with confidence, leaving
    us to focus on the new and innovative stuff, solving problems for users.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Python 非常适合于小型、独立的长期运行守护进程，也同样适合灵活、功能丰富的 Web 框架，如 Django 和 Pyramid。Python 社区蓬勃发展，这意味着有大量的开源工具库可供使用，让我们能够快速、自信地构建项目，专注于创新和解决用户问题。
- en: Across all of our projects, we at Adaptive Lab reuse several tools that are
    built in Python but that can be used in a language-agnostic way. For example,
    we use SaltStack for server provisioning and Mozilla’s Circus for managing long-running
    processes. The benefit to us when a tool is open source and written in a language
    we’re familiar with is that if we find any problems, we can solve them ourselves
    and get those solutions taken up, which benefits the community.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有项目中，Adaptive Lab 重复使用几个基于 Python 构建的工具，这些工具可以以与语言无关的方式使用。例如，我们使用 SaltStack
    进行服务器配置和 Mozilla 的 Circus 管理长期运行的进程。当一个工具是开源的，并且使用我们熟悉的语言编写时，如果我们遇到任何问题，我们可以自行解决，并将解决方案推广，从而造福社区。
- en: SoMA’s Design
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SoMA 的设计
- en: Our Social Media Analytics (SoMA) tool needed to cope with a high throughput
    of social media data and the storage and retrieval in real time of a large amount
    of information. After researching various data stores and search engines, we settled
    on Elasticsearch as our real-time document store. As its name suggests, it’s highly
    scalable, but it is also easy to use and is capable of providing statistical responses
    as well as search—ideal for our application. Elasticsearch itself is built in
    Java, but like any well-architected component of a modern system, it has a good
    API and is well catered to with a Python library and tutorials.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的社交媒体分析（SoMA）工具需要处理大量社交媒体数据，并实时存储和检索大量信息。在研究了各种数据存储和搜索引擎后，我们选择了 Elasticsearch
    作为我们的实时文档存储。正如其名，它具有高可扩展性，同时易于使用，能够提供统计响应和搜索功能，非常适合我们的应用场景。Elasticsearch 本身基于
    Java 构建，但像现代系统中设计良好的任何组件一样，它具有良好的 API，并且有Python库和教程支持。
- en: The system we designed uses queues with Celery held in Redis to quickly hand
    a large stream of data to any number of servers for independent processing and
    indexing. Each component of the whole complex system was designed to be small,
    individually simple, and able to work in isolation. Each focused on one task,
    like analyzing a conversation for sentiment or preparing a document for indexing
    into Elasticsearch. Several of these were configured to run as daemons using Mozilla’s
    Circus, which keeps all the processes up and running and allows them to be scaled
    up or down on individual servers.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设计的系统使用 Celery 在 Redis 中的队列，快速将大量数据流交给任意数量的服务器进行独立处理和索引。整个复杂系统的每个组件都设计为小型、简单且能够独立工作。每个组件专注于一个任务，例如分析对话的情感或为索引到
    Elasticsearch 准备文档。其中一些配置为使用 Mozilla 的 Circus 作为守护进程运行，它保持所有进程运行并允许根据单个服务器的需要进行扩展或缩减。
- en: 'SaltStack is used to define and provision the complex cluster and handles the
    setup of all of the libraries, languages, databases, and document stores. We also
    make use of Fabric, a Python tool for running arbitrary tasks on the command line.
    Defining servers in code has many benefits: complete parity with the production
    environment; version control of the configuration; having everything in one place.
    It also serves as documentation on the setup and dependencies required by a cluster.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: SaltStack 用于定义和配置复杂的集群，并处理所有库、语言、数据库和文档存储的设置。我们还使用了 Fabric，这是一个用于在命令行上运行任意任务的
    Python 工具。在代码中定义服务器有很多好处：完全与生产环境一致；配置的版本控制；所有内容都在一个地方。它还作为集群设置和依赖项的文档。
- en: Our Development Methodology
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的开发方法论
- en: We aim to make it as easy as possible for a newcomer to a project to be able
    to quickly get into adding code and deploying confidently. We use Vagrant to build
    the complexities of a system locally, inside a virtual machine that has complete
    parity with the production environment. A simple `vagrant up` is all a newcomer
    needs to get set up with all the dependencies required for their work.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是尽可能简化新加入项目的新人快速投入添加代码和自信部署的过程。我们使用 Vagrant 在本地构建系统的复杂性，放在一个完全与生产环境一致的虚拟机中。一个简单的
    `vagrant up` 就是新人启动所需的所有依赖项的全部。
- en: We work in an agile way, planning together, discussing architecture decisions,
    and determining a consensus on task estimates. For SoMA, we made the decision
    to include at least a few tasks considered as corrections for “technical debt”
    in each sprint. Also included were tasks for documenting the system (we eventually
    established a wiki to house all the knowledge for this ever-expanding project).
    Team members review each other’s code after each task, to sanity check, offer
    feedback, and understand the new code that is about to get added to the system.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以敏捷方式工作，共同规划，讨论架构决策，并就任务估算达成共识。对于 SoMA，我们决定每个迭代都包括至少几个被视为技术债务修正的任务。还包括为系统编写文档的任务（我们最终建立了一个维基，用于存放这个不断扩展的项目的所有知识）。团队成员在每个任务之后互相审查代码，进行合理性检查、提供反馈，并理解即将添加到系统中的新代码。
- en: A good test suite helped bolster confidence that any changes weren’t going to
    cause existing features to fail. Integration tests are vital in a system like
    SoMA, composed of many moving parts. A staging environment offers a way to test
    the performance of new code; on SoMA in particular, it was only through testing
    against the kind of large datasets seen in production that problems could occur
    and be dealt with, so it was often necessary to reproduce that amount of data
    in a separate environment. Amazon’s Elastic Compute Cloud (EC2) gave us the flexibility
    to do this.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 一个良好的测试套件有助于增强信心，确保任何更改不会导致现有功能失败。在像 SoMA 这样由许多组件组成的系统中，集成测试至关重要。一个分级环境提供了测试新代码性能的方式；特别是在
    SoMA 中，只有通过针对生产环境中看到的大数据集进行测试，才能发现问题并加以解决，因此通常需要在一个单独的环境中复制该数据量。亚马逊的弹性计算云（EC2）为我们提供了这种灵活性。
- en: Maintaining SoMA
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 维护 SoMA
- en: The SoMA system runs continuously, and the amount of information it consumes
    grows every day. We have to account for peaks in the data stream, network issues,
    and problems in any of the third-party service providers it relies on. So, to
    make things easy on ourselves, SoMA is designed to fix itself whenever it can.
    Thanks to Circus, processes that crash out will come back to life and resume their
    tasks from where they left off. A task will queue up until a process can consume
    it, and there’s enough breathing room there to stack up tasks while the system
    recovers.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: SoMA 系统持续运行，每天消耗的信息量不断增加。我们必须考虑数据流高峰、网络问题以及任何第三方服务提供商可能存在的问题。因此，为了简化操作，SoMA
    被设计成可以自我修复。借助 Circus，崩溃的进程将重新启动并从上次停止的地方继续任务。任务将排队等待进程消耗，系统在恢复期间有足够的时间堆积任务。
- en: We use Server Density to monitor the many SoMA servers. It’s very simple to
    set up, but quite powerful. A nominated engineer can receive a push message via
    phone as soon as a problem is likely to occur, in order to react in time to ensure
    it doesn’t become a problem. With Server Density, it’s also very easy to write
    custom plug-ins in Python, making it possible, for example, to set up instant
    alerts on aspects of Elasticsearch’s behavior.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 Server Density 来监控多台 SoMA 服务器。设置非常简单，但功能强大。一旦可能发生问题，指定工程师可以通过手机即时接收推送消息，以便及时反应，确保问题不会扩大。使用
    Server Density，还可以非常轻松地用 Python 编写自定义插件，例如设置 Elasticsearch 行为的即时警报。
- en: Advice for Fellow Engineers
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 同行工程师的建议
- en: Above all, you and your team need to be confident and comfortable that what
    is about to be deployed into a live environment is going to work flawlessly. To
    get to that point, you have to work backward, spending time on all of the components
    of the system that will give you that sense of comfort. Make deployment simple
    and foolproof; use a staging environment to test the performance with real-world
    data; ensure you have a good, solid test suite with high coverage; implement a
    process for incorporating new code into the system; and make sure technical debt
    gets addressed sooner rather than later. The more you shore up your technical
    infrastructure and improve your processes, the happier and more successful at
    engineering the right solutions your team will be.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是，您和您的团队需要确信和放心，即将部署到实时环境中的内容将完美无缺地运行。为了达到这一点，您必须向后推进，花时间处理系统的所有组件，这些组件将让您感到放心。简化并确保部署无误；使用分段环境测试具有真实数据的性能；确保您拥有一个覆盖率高的良好且稳固的测试套件；实施将新代码整合到系统中的流程；并确保尽早解决技术债务。您加固技术基础并改进流程的越多，您的团队在工程问题上找到正确解决方案的成功和满意程度就越高。
- en: If a solid foundation of code and ecosystem are not in place but the business
    is pressuring you to get things live, it’s only going to lead to problem software.
    It’s going to be your responsibility to push back and stake out time for incremental
    improvements to the code, and the tests and operations involved in getting things
    out the door.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有坚实的代码和生态系统基础，但业务要求您立即上线，这只会导致问题软件。您有责任推迟时间，逐步改进代码、测试和操作，以便顺利完成上线工作。
- en: Making Deep Learning Fly with RadimRehurek.com (2014)
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 让深度学习飞起来（2014）
- en: Radim Řehůřek ([*radimrehurek.com*](http://www.radimrehurek.com))
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: Radim Řehůřek（[*radimrehurek.com*](http://www.radimrehurek.com)）
- en: When Ian asked me to write my “lessons from the field” on Python and optimizations
    for this book, I immediately thought, “Tell them how you made a Python port faster
    than Google’s C original!” It’s an inspiring story of making a machine learning
    algorithm, Google’s poster child for deep learning, 12,000× faster than a naive
    Python implementation. Anyone can write bad code and then trumpet about large
    speedups. But the optimized Python port also runs, somewhat astonishingly, almost
    four times faster than the original code written by Google’s team! That is, four
    times faster than opaque, tightly profiled, and optimized C.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 当 Ian 要求我在这本书上写关于 Python 和优化的“现场经验”时，我立刻想到：“告诉他们如何比 Google 的 C 原版更快地编写 Python
    移植版！”这是一个鼓舞人心的故事，讲述了如何使一个机器学习算法，Google 深度学习的招牌案例，比朴素的 Python 实现快 12,000 倍。任何人都可以写出糟糕的代码，然后大肆宣扬巨大的加速。但优化后的
    Python 移植版本，令人惊讶地运行速度几乎是 Google 团队原始代码的四倍快！也就是说，比 Google 团队编写的不透明、严密优化的 C 代码快了四倍。
- en: But before drawing “machine-level” optimization lessons, some general advice
    about “human-level” optimizations.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 但在得出“机器级”优化经验之前，先谈一些关于“人类级”优化的通用建议。
- en: The Sweet Spot
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 黄金时机
- en: I run a small consulting business laser-focused on machine learning, where my
    colleagues and I help companies make sense of the tumultuous world of data analysis,
    in order to make money or save costs (or both). We help clients design and build
    wondrous systems for data processing, especially text data.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我经营一家专注于机器学习的小型咨询公司，我和我的同事帮助公司理清数据分析的混乱世界，以赚钱或节省成本（或两者兼有）。我们帮助客户设计和构建用于数据处理的奇妙系统，特别是文本数据。
- en: The clients range from large multinationals to nascent start-ups, and while
    each project is different and requires a different tech stack, plugging into the
    client’s existing data flows and pipelines, Python is a clear favorite. Not to
    preach to the choir, but Python’s no-nonsense development philosophy, its malleability,
    and the rich library ecosystem make it an ideal choice.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 客户群体涵盖了从大型跨国公司到新兴初创企业，尽管每个项目都不同且需要不同的技术堆栈，但插入客户现有的数据流和管道时，Python 显然是首选。不是向信徒宣教，但Python
    的务实开发理念、其可塑性以及丰富的库生态使其成为理想的选择。
- en: 'First, a few thoughts “from the field” on what works:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，关于有效的几点看法：“实地”经验告诉我们：
- en: Communication, communication, communication
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 沟通，沟通，沟通
- en: 'This one’s obvious, but worth repeating. Understand the client’s problem on
    a higher (business) level before deciding on an approach. Sit down and talk through
    what they think they need (based on their partial knowledge of what’s possible
    and/or what they Googled up before contacting you), until it becomes clear what
    they really need, free of cruft and preconceptions. Agree on ways to validate
    the solution beforehand. I like to visualize this process as a long, winding road
    to be built: get the starting line right (problem definition, available data sources)
    and the finish line right (evaluation, solution priorities), and the path in between
    falls into place.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这一点显而易见，但值得重复。在决定方法之前，先在更高（商业）层面上了解客户的问题。坐下来讨论他们认为自己需要什么（基于他们对可能发生的事情部分了解和/或在联系你之前通过谷歌搜索到的信息），直到清楚他们真正需要什么，摆脱多余的东西和成见。同意在之前验证解决方案的方法。我喜欢将这个过程形象化为一条漫长曲折的道路：确保起点正确（问题定义、可用数据源），终点正确（评估、解决方案优先级），中间的路径就自然而然地展开。
- en: Be on the lookout for promising technologies
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 保持对有前景的技术保持警惕
- en: An emergent technology that is reasonably well understood and robust, is gaining
    traction, yet is still relatively obscure in the industry can bring huge value
    to the client (or yourself). As an example, a few years ago, Elasticsearch was
    a little-known and somewhat raw open source project. But I evaluated its approach
    as solid (built on top of Apache Lucene, offering replication, cluster sharding,
    etc.) and recommended its use to a client. We consequently built a search system
    with Elasticsearch at its core, saving the client significant amounts of money
    in licensing, development, and maintenance compared to the considered alternatives
    (large commercial databases). Even more importantly, using a new, flexible, powerful
    technology gave the product a massive competitive advantage. Nowadays, Elasticsearch
    has entered the enterprise market and conveys no competitive advantage at all—everyone
    knows it and uses it. Getting the timing right is what I call hitting the “sweet
    spot,” maximizing the value/cost ratio.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 一种新兴技术，被合理理解且稳健，正在逐渐流行，但在行业中仍相对较为陌生，能为客户（或你自己）带来巨大价值。例如，几年前，Elasticsearch 是一个鲜为人知且有些粗糙的开源项目。但我认为它的方法可靠（建立在Apache
    Lucene之上，提供复制、集群分片等功能），并建议客户使用。我们随后构建了以Elasticsearch为核心的搜索系统，与考虑的替代方案（大型商业数据库）相比，为客户节省了大量的授权、开发和维护成本。更重要的是，使用这种新的、灵活且强大的技术为产品赋予了巨大的竞争优势。如今，Elasticsearch
    已进入企业市场，不再具有竞争优势—每个人都知道它并使用它。抓住时机，达到我所说的“黄金时机”，最大化价值/成本比。
- en: KISS (Keep It Simple, Stupid!)
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: KISS（保持简单，愚蠢！）
- en: This is another no-brainer. The best code is code you don’t have to write and
    maintain. Start simple, and improve and iterate where necessary. I prefer tools
    that follow the Unix philosophy of “do one thing, and do it well.” Grand programming
    frameworks can be tempting, with everything imaginable under one roof and fitting
    neatly together. But invariably, sooner or later, you need something the grand
    framework didn’t imagine, and then even modifications that seem simple (conceptually)
    cascade into a nightmare (programmatically). Grand projects and their all-encompassing
    APIs tend to collapse under their own weight. Use modular, focused tools, with
    APIs in between that are as small and uncomplicated as possible. Prefer text formats
    that are open to simple visual inspection, unless performance dictates otherwise.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这是另一个不需要考虑的问题。最好的代码是你不必编写和维护的代码。从简单开始，并在必要时改进和迭代。我更喜欢遵循Unix哲学的工具，“做一件事，并做好它”。大型编程框架可能很诱人，几乎包含了一切，并且整洁地组合在一起。但不可避免地，迟早会需要一些大型框架没有考虑到的东西，然后即使是看似简单的修改（在概念上）也会在程序上演变成噩梦。大型项目及其包罗万象的API往往会因自身臃肿而崩溃。使用模块化的、专注的工具，并尽可能使用小而简单的API之间的接口。除非性能要求不允许，否则请优先选择可以简单视觉检查的文本格式。
- en: Use manual sanity checks in data pipelines
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 使用数据管道中的手动健全性检查
- en: When optimizing data processing systems, it’s easy to stay in the “binary mindset”
    mode, using tight pipelines, efficient binary data formats, and compressed I/O.
    As the data passes through the system unseen, and unchecked (except for perhaps
    its type), it remains invisible until something outright blows up. Then debugging
    commences. I advocate sprinkling a few simple log messages throughout the code,
    showing what the data looks like at various internal points of processing, as
    good practice—nothing fancy, just an analogy to the Unix `head` command, picking
    and visualizing a few data points. Not only does this help during the aforementioned
    debugging, but seeing the data in a human-readable format leads to “aha!” moments
    surprisingly often, even when all seems to be going well. Strange tokenization!
    They promised input would always be encoded in latin1! How did a document in this
    language get in there? Image files leaked into a pipeline that expects and parses
    text files! These are often insights that go way beyond those offered by automatic
    type checking or a fixed unit test, hinting at issues beyond component boundaries.
    Real-world data is messy. Catch early even things that wouldn’t necessarily lead
    to exceptions or glaring errors. Err on the side of too much verbosity.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 当优化数据处理系统时，很容易陷入“二进制思维”模式，使用紧凑的管道、高效的二进制数据格式和压缩的I/O。随着数据在系统中通过，未经检查（除了可能的类型），它仍然是看不见的，直到出现明显问题。然后开始调试。我建议在代码中的各个内部处理点上撒些简单的日志消息，显示数据在不同内部点的样子，这是一种良好的实践——什么都不花哨，只是类似Unix的`head`命令，选择和可视化几个数据点。这不仅有助于前面提到的调试，而且在一切看似顺利的情况下，以人类可读的格式查看数据时，常常会有“啊哈！”的时刻。奇怪的标记化！他们承诺输入始终以latin1编码！图像文件泄漏到期望和解析文本文件的管道中！这些通常是超出自动类型检查或固定单元测试所能提供的洞察，暗示着超出组件边界的问题。现实世界的数据是混乱的。早点捕捉即使不会导致异常或显著错误的事情，也是个好主意。在冗长方面保持谨慎。
- en: Navigate fads carefully
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 谨慎地应对时尚潮流
- en: Just because a client keeps hearing about X and says they must have X too doesn’t
    mean they really need it. It might be a marketing problem rather than a technology
    one, so take care to discern the two and deliver accordingly. X changes over time
    as hype waves come and go; a recent value would be X = big data.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅因为客户一直听说X并说他们也必须要X，并不意味着他们真的需要。这可能是一个市场问题而不是技术问题，因此要小心分辨二者，并相应地交付。X随着时间的推移会发生变化，随着炒作浪潮的来来去去；最近的价值会是X
    = 大数据。
- en: All right, enough business talk—here’s how I got *word2vec* in Python to run
    faster than C.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 行了，足够商业化了——这是我如何让Python中的*word2vec*比C运行得更快的方法。
- en: Lessons in Optimizing
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化的教训
- en: '[*word2vec*](https://oreil.ly/SclZ0) is a deep learning algorithm that allows
    detection of similar words and phrases. With interesting applications in text
    analytics and search engine optimization (SEO), and with Google’s lustrous brand
    name attached to it, start-ups and businesses flocked to take advantage of this
    new tool.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '[*word2vec*](https://oreil.ly/SclZ0) 是一种深度学习算法，允许检测相似的词语和短语。在文本分析和搜索引擎优化（SEO）中有着有趣的应用，并且附有Google光辉的品牌名称，吸引了初创公司和企业纷纷利用这一新工具。'
- en: Unfortunately, the only available code was that produced by Google itself, an
    open source Linux command-line tool written in C. This was a well-optimized but
    rather hard-to-use implementation. The primary reason I decided to port *word2vec*
    to Python was so I could extend *word2vec* to other platforms, making it easier
    to integrate and extend for clients.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，唯一可用的代码是由Google自己生成的，这是一个用C语言编写的开源Linux命令行工具。这是一个经过优化但相当难以使用的实现。我决定将*word2vec*移植到Python的主要原因是为了能够将*word2vec*扩展到其他平台，使其更易于集成和扩展以供客户使用。
- en: The details are not relevant here, but *word2vec* requires a training phase
    with a lot of input data to produce a useful similarity model. For example, the
    folks at Google ran *word2vec* on their GoogleNews dataset, training on approximately
    100 billion words. Datasets of this scale obviously don’t fit in RAM, so a memory-efficient
    approach must be taken.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这里不涉及细节，但*word2vec*需要一个训练阶段，使用大量输入数据来生成一个有用的相似性模型。例如，谷歌的人员在他们的GoogleNews数据集上运行了*word2vec*，大约训练了1000亿字。显然，这种规模的数据集不适合RAM，因此必须采取一种内存高效的方法。
- en: 'I’ve authored a machine learning library, [`gensim`](https://oreil.ly/6SYgs),
    that targets exactly that sort of memory-optimization problem: datasets that are
    no longer trivial (“trivial” being anything that fits fully into RAM), yet not
    large enough to warrant petabyte-scale clusters of MapReduce computers. This “terabyte”
    problem range fits a surprisingly large portion of real-world cases, *word2vec*
    included.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我撰写了一个机器学习库，[`gensim`](https://oreil.ly/6SYgs)，正好解决这种内存优化问题：数据集不再是微不足道的（“微不足道”是指完全适合RAM的任何东西），但也不大到需要PB级MapReduce计算机集群。这种“TB”级问题适用于令人惊讶地大部分真实案例，*word2vec*也包括在内。
- en: 'Details are described [on my blog](http://bit.ly/RR_blog), but here are a few
    optimization takeaways:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 细节在我的博客中有描述，但这里有一些优化的要点：
- en: Stream your data, watch your memory
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 流式处理数据，注意内存使用
- en: Let your input be accessed and processed one data point at a time, for a small,
    constant memory footprint. The streamed data points (sentences, in the case of
    *word2vec*) may be grouped into larger batches internally for performance (such
    as processing 100 sentences at a time), but a high-level, streamed API proved
    a powerful and flexible abstraction. The Python language supports this pattern
    very naturally and elegantly, with its built-in generators—a truly beautiful problem–tech
    match. Avoid committing to algorithms and tools that load everything into RAM,
    unless you know your data will always remain small, or you don’t mind reimplementing
    a production version yourself later.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 让您的输入逐个数据点访问和处理，以获得小而恒定的内存占用。流式数据点（在*word2vec*中为句子）可以在内部分组成更大的批次以提高性能（例如一次处理100个句子），但高级别的流式API证明是一个强大且灵活的抽象。Python语言非常自然和优雅地支持这种模式，借助其内置的生成器——这是一个真正美丽的问题技术匹配。除非您知道数据始终保持较小，或者您不介意以后自己重新实现生产版本，否则应避免依赖于将所有内容加载到RAM的算法和工具。
- en: Take advantage of Python’s rich ecosystem
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 充分利用Python丰富的生态系统
- en: I started with a readable, clean port of *word2vec* in `numpy`. `numpy` is covered
    in depth in [Chapter 6](ch06_split_000.xhtml#matrix_computation) of this book,
    but as a short reminder, it is an amazing library, a cornerstone of Python’s scientific
    community and the de facto standard for number crunching in Python. Tapping into
    `numpy`’s powerful array interfaces, memory access patterns, and wrapped BLAS
    routines for ultrafast common vector operations leads to concise, clean, and fast
    code—code that is hundreds of times faster than naive Python code. Normally I’d
    call it a day at this point, but “hundreds of times faster” was still 20× slower
    than Google’s optimized C version, so I pressed on.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我从一个可读性强、干净的*numpy*端口开始。`numpy`在本书的[第6章](ch06_split_000.xhtml#matrix_computation)中有详细介绍，但简要提醒一下，它是一个了不起的库，是Python科学社区的基石，也是Python中数字计算的事实标准。利用`numpy`强大的数组接口、内存访问模式以及包装的BLAS例程进行超快速的常见向量操作，可以编写简洁、干净和快速的代码——这种代码比天真的Python代码快上数百倍。通常在这一点上我会收工，但是“数百倍快”仍然比谷歌优化的C版本慢20倍，因此我继续努力。
- en: Profile and compile hotspots
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 分析和编译热点代码
- en: '*word2vec* is a typical high performance computing app, in that a few lines
    of code in one inner loop account for 90% of the entire training runtime. Here
    I rewrote a single core routine (approximately 20 lines of code) in C, using an
    external Python library, Cython, as the glue. While it’s technically brilliant,
    I don’t consider Cython a particularly convenient tool conceptually—it’s basically
    like learning another language, a nonintuitive mix between Python, `numpy`, and
    C, with its own caveats and idiosyncrasies. But until Python’s JIT technologies
    mature, Cython is probably our best bet. With a Cython-compiled hotspot, performance
    of the Python *word2vec* port is now on par with the original C code. An additional
    advantage of having started with a clean `numpy` version is that we get free tests
    for correctness, by comparing against the slower but correct version.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '*word2vec* 是一个典型的高性能计算应用程序，其中一个内部循环中几行代码占整个训练运行时间的90%。在这里，我用外部 Python 库 Cython
    重写了一个单核心例程（大约20行代码），作为粘合剂。虽然从技术上讲它很出色，但我认为概念上 Cython 并不是一个特别方便的工具——它基本上就像学习另一种语言，一个不直观的
    Python、`numpy` 和 C 的混合体，有它自己的注意事项和特殊性。但在 Python 的 JIT 技术成熟之前，Cython 可能是我们的最佳选择。通过
    Cython 编译的热点，Python *word2vec* 的性能现在与原始 C 代码相当。从一个干净的 `numpy` 版本开始的额外优势是，通过与较慢但正确的版本进行比较，我们可以获得免费的正确性测试。'
- en: Know your BLAS
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 了解你的 BLAS
- en: A neat feature of `numpy` is that it internally wraps Basic Linear Algebra Subprograms
    (BLAS), where available. These are sets of low-level routines, optimized directly
    by processor vendors (Intel, AMD, etc.) in assembly, Fortran, or C, and designed
    to squeeze out maximum performance from a particular processor architecture. For
    example, calling an axpy BLAS routine computes `vector_y += scalar * vector_x`
    way faster than what a generic compiler would produce for an equivalent explicit
    `for` loop. Expressing *word2vec* training as BLAS operations resulted in another
    4× speedup, topping the performance of C *word2vec*. Victory! To be fair, the
    C code could link to BLAS as well, so this is not some inherent advantage of Python
    per se. `numpy` just makes things like this stand out and makes them easy to take
    advantage of.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '`numpy` 的一个巧妙特性是它在内部包装了基本线性代数子程序（BLAS），如果可用的话。这些是由处理器供应商（Intel、AMD 等）直接优化的低级例程，使用汇编、Fortran
    或 C 设计，旨在从特定处理器架构中挤出最大性能。例如，调用 axpy BLAS 例程计算 `vector_y += scalar * vector_x`
    比一个等效的显式 `for` 循环产生的代码快得多。将 *word2vec* 训练表达为 BLAS 操作导致另外4倍速度提升，超过了 C *word2vec*
    的性能。胜利！公平地说，C 代码也可以链接到 BLAS，所以这并不是 Python 本质上的某种固有优势。`numpy` 只是让这类事情显现并且易于利用。'
- en: Parallelization and multiple cores
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 并行化和多核
- en: '`gensim` contains distributed cluster implementations of a few algorithms.
    For *word2vec*, I opted for multithreading on a single machine, because of the
    fine-grained nature of its training algorithm. Using threads also allows us to
    avoid the fork-without-exec POSIX issues that Python’s multiprocessing brings,
    especially in combination with certain BLAS libraries. Because our core routine
    is already in Cython, we can afford to release Python’s GIL (global interpreter
    lock; see [“Parallelizing the Solution with OpenMP on One Machine”](ch07.xhtml#compiling-cython-omp)),
    which normally renders multithreading useless for CPU-intensive tasks. Speedup:
    another 3×, on a machine with four cores.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '`gensim` 包含了几种算法的分布式集群实现。对于 *word2vec*，我选择在单台机器上进行多线程处理，因为其训练算法的精细化特性。使用线程还允许我们避免
    Python 的 multiprocessing 带来的 fork-without-exec POSIX 问题，特别是与某些 BLAS 库结合使用时。因为我们的核心例程已经在
    Cython 中，我们可以释放 Python 的 GIL（全局解释器锁；参见[“使用 OpenMP 在一台机器上并行化解决方案”](ch07.xhtml#compiling-cython-omp)），通常情况下对于
    CPU 密集型任务来说，多线程是无用的。加速：在四核机器上再次提升3倍。'
- en: Static memory allocations
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 静态内存分配
- en: 'At this point, we’re processing tens of thousands of sentences per second.
    Training is so fast that even little things like creating a new `numpy` array
    (calling `malloc` for each streamed sentence) slow us down. Solution: preallocate
    a static “work” memory and pass it around, in good old Fortran fashion. Brings
    tears to my eyes. The lesson here is to keep as much bookkeeping and app logic
    in the clean Python code as possible, and to keep the optimized hotspot lean and
    mean.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们每秒处理数以万计的句子。训练速度非常快，以至于甚至像创建一个新的 `numpy` 数组（为每个流式句子调用 `malloc`）这样的小事情也会拖慢我们的速度。解决方案：预先分配一个静态的“工作”内存并在代码中传递，像
    Fortran 一样。让我眼泪汪汪。这里的教训是尽可能将尽可能多的簿记和应用逻辑保持在干净的 Python 代码中，并使优化的热点保持简洁高效。
- en: Problem-specific optimizations
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 问题特定优化
- en: 'The original C implementation contained specific micro-optimizations, such
    as aligning arrays onto specific memory boundaries or precomputing certain functions
    into memory lookup tables. A nostalgic blast from the past, but with today’s complex
    CPU instruction pipelines, memory cache hierarchies, and coprocessors, such optimizations
    are no longer a clear winner. Careful profiling suggested a few percent improvement,
    which may not be worth the extra code complexity. Takeaway: use annotation and
    profiling tools to highlight poorly optimized spots. Use your domain knowledge
    to introduce algorithmic approximations that trade accuracy for performance (or
    vice versa). But never take it on faith; profile, preferably using real production
    data.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的 C 实现包含特定的微优化，比如将数组对齐到特定的内存边界或将某些函数预先计算到内存查找表中。这是对过去的怀旧，但是在今天复杂的 CPU 指令流水线、内存高速缓存层次结构和协处理器中，这样的优化已经不再是明显的赢家。仔细的分析表明，有几个百分点的改进，可能并不值得额外的代码复杂性。教训：使用注解和分析工具突出显示优化不佳的地方。利用您的领域知识引入算法近似，以在精度和性能之间进行权衡（或反之）。但千万不要凭信仰；尽量使用真实的生产数据进行分析。
- en: Conclusion
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: Optimize where appropriate. In my experience, there’s never enough communication
    to fully ascertain the problem scope, priorities, and connection to the client’s
    business goals—a.k.a. the “human-level” optimizations. Make sure you deliver on
    a problem that matters, rather than getting lost in “geek stuff” for the sake
    of it. And when you do roll up your sleeves, make it worth it!
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在适当的地方进行优化。根据我的经验，从来没有足够的沟通来完全确定问题范围、优先级和与客户业务目标的联系——即“人为级别”的优化。确保您解决的是一个重要问题，而不是为了“极客”的事情而迷失方向。当您卷起袖子时，确保它是值得的！
- en: Large-Scale Productionized Machine Learning at Lyst.com (2014)
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Lyst.com 上进行大规模生产机器学习（2014 年）
- en: Sebastjan Trepca ([lyst.com](http://www.lyst.com))
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: Sebastjan Trepca（[lyst.com](http://www.lyst.com)）
- en: Python and Django have been at the heart of Lyst since the site’s creation.
    As internal projects have grown, some of the Python components have been replaced
    with other tools and languages to fit the maturing needs of the system.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 自从网站创建以来，Python 和 Django 一直是 Lyst 的核心。随着内部项目的发展，一些 Python 组件已被其他工具和语言替换，以适应系统日益成熟的需求。
- en: Cluster Design
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集群设计
- en: The cluster runs on Amazon EC2\. In total there are approximately 100 machines,
    including the more recent C3 instances, which have good CPU performance.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 集群在 Amazon EC2 上运行。总共有大约 100 台机器，其中包括最近的 C3 实例，其 CPU 性能良好。
- en: Redis is used for queuing with PyRes and storing metadata. The dominant data
    format is JSON, for ease of human comprehension. `supervisord` keeps the processes
    alive.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: Redis 用于使用 PyRes 进行排队和存储元数据。主要数据格式为 JSON，以便人类理解。
- en: Elasticsearch and PyES are used to index all products. The Elasticsearch cluster
    stores 60 million documents across seven machines. Solr was investigated but discounted
    because of its lack of real-time updating features.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch 和 PyES 用于索引所有产品。Elasticsearch 集群在七台机器上存储了 6000 万个文档。已调查 Solr，但由于其缺乏实时更新功能而被排除在外。
- en: Code Evolution in a Fast-Moving Start-Up
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在快速发展的初创公司中的代码演进
- en: It is better to write code that can be implemented quickly so that a business
    idea can be tested than to spend a long time attempting to write “perfect code”
    in the first pass. If code is useful, it can be refactored; if the idea behind
    the code is poor, it is cheap to delete it and remove a feature. This can lead
    to a complicated code base with many objects being passed around, but this is
    acceptable as long as the team makes time to refactor code that is useful to the
    business.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 最好编写可以快速实现的代码，以便测试业务理念，而不是花费很长时间试图在第一次尝试中编写“完美的代码”。如果代码有用，它可以重构；如果代码背后的理念不好，删除它并移除一个功能是很廉价的。这可能会导致一个复杂的代码库，其中传递了许多对象，但只要团队有时间重构对业务有用的代码，这是可以接受的。
- en: Docstrings are used heavily in Lyst—an external Sphinx documentation system
    was tried but dropped in favor of just reading the code. A wiki is used to document
    processes and larger systems. We also started creating very small services instead
    of chucking everything into one code base.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Lyst 中大量使用文档字符串——尝试使用外部 Sphinx 文档系统，但因为只需阅读代码而放弃了。维基用于记录流程和更大的系统。我们还开始创建非常小的服务，而不是将所有内容都放入一个代码库。
- en: Building the Recommendation Engine
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建推荐引擎
- en: 'At first the recommendation engine was coded in Python, using `numpy` and `scipy`
    for computations. Subsequently, performance-critical parts of the recommender
    were sped up using Cython. The core matrix factorization operations were written
    entirely in Cython, yielding an order of magnitude improvement in speed. This
    was mostly due to the ability to write performant loops over `numpy` arrays in
    Python, something that is extremely slow in pure Python and performed poorly when
    vectorized because it necessitated memory copies of `numpy` arrays. The culprit
    was `numpy`’s fancy indexing, which always makes a data copy of the array being
    sliced: if no data copy is necessary or intended, Cython loops will be far faster.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 最初推荐引擎是用 Python 编写的，使用 `numpy` 和 `scipy` 进行计算。随后，推荐器的性能关键部分使用 Cython 加速。核心矩阵因子分解操作完全使用
    Cython 编写，速度提升一个数量级。这主要是由于在 Python 中能够编写高性能的 `numpy` 数组循环，这在纯 Python 中非常慢，并且在向量化时性能表现不佳，因为它需要对
    `numpy` 数组进行内存复制。罪魁祸首是 `numpy` 的花式索引，它总是会对被切片的数组进行数据复制：如果不需要或不打算进行数据复制，Cython
    循环将会快得多。
- en: Over time, the online components of the system (responsible for computing recommendations
    at request time) were integrated into our search component, Elasticsearch. In
    the process, they were translated into Java to allow full integration with Elasticsearch.
    The main reason behind this was not performance, but the utility of integrating
    the recommender with the full power of a search engine, allowing us to apply business
    rules to served recommendations more easily. The Java component itself is extremely
    simple and implements primarily efficient sparse vector inner products. The more
    complex offline component remains written in Python, using standard components
    of the Python scientific stack (mostly Python and Cython).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，系统的在线组件（负责在请求时计算推荐）已经集成到我们的搜索组件 Elasticsearch 中。在这个过程中，它们被转换为 Java，以便与
    Elasticsearch 完全集成。这样做的主要原因不是性能，而是将推荐器与搜索引擎的全部功能结合起来，更容易地应用业务规则来提供推荐。Java 组件本身非常简单，主要实现了高效的稀疏向量内积。更复杂的离线组件仍然使用
    Python 编写，使用了 Python 科学栈的标准组件（主要是 Python 和 Cython）。
- en: 'In our experience, Python is useful as more than a prototyping language: the
    availability of tools such as `numpy`, Cython, and `weave` (and more recently
    Numba) allowed us to achieve very good performance in the performance-critical
    parts of the code while maintaining Python’s clarity and expressiveness where
    low-level optimization would be counterproductive.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的经验，Python 不仅仅是原型设计语言：诸如 `numpy`、Cython 和 `weave`（最近还有 Numba）等工具的可用性，使我们能够在代码的性能关键部分实现非常好的性能，同时保持
    Python 的清晰和表达力，在低级别优化可能适得其反的情况下尤为如此。
- en: Reporting and Monitoring
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 报告和监控
- en: Graphite is used for reporting. Currently, performance regressions can be seen
    by eye after a deployment. This makes it easy to drill into detailed event reports
    or to zoom out and see a high-level report of the site’s behavior, adding and
    removing events as necessary.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: Graphite 用于报告。目前，部署后可以通过肉眼看到性能回归。这使得可以轻松地深入详细的事件报告，或者放大以查看站点行为的高级报告，根据需要添加和删除事件。
- en: Internally, a larger infrastructure for performance testing is being designed.
    It will include representative data and use cases to properly test new builds
    of the site.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部，正在设计一个更大的基础设施用于性能测试。它将包括代表性数据和用例，以适当地测试站点的新版本构建。
- en: A staging site will also be used to let a small fraction of real visitors see
    the latest version of the deployment—if a bug or performance regression is seen,
    it will have affected only a minority of visitors, and this version can quickly
    be retired. This will make the deployment of bugs significantly less costly and
    problematic.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 还将使用一个暂存站点，让少数真实访问者看到最新版本的部署——如果发现错误或性能回归，将只会影响少数访问者，并且可以快速撤销该版本。这将大大减少错误部署的成本和问题。
- en: Sentry is used to log and diagnose Python stack traces.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: Sentry 用于记录和诊断 Python 的堆栈跟踪。
- en: Jenkins is used for continuous integration (CI) with an in-memory database configuration.
    This enables parallelized testing so that check-ins quickly reveal any bugs to
    the developer.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: Jenkins 用于带有内存数据库配置的持续集成（CI）。这使得可以并行测试，以便快速发现开发者的任何错误。
- en: Some Advice
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一些建议
- en: 'It’s really important to have good tools to track the effectiveness of what
    you’re building, and to be super-practical at the beginning. Start-ups change
    constantly, and engineering evolves: you start with a super-exploratory phase,
    building prototypes all the time and deleting code until you hit the gold mine,
    and then you start to go deeper, improving code, performance, etc. Until then,
    it’s all about quick iterations and good monitoring/analytics. I guess this is
    pretty standard advice that has been repeated over and over, but I think many
    don’t really get how important it is.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有好的工具跟踪你所构建内容的有效性非常重要，并且在开始阶段要非常务实。创业公司经常变化，工程也在演进：你从一个超级探索阶段开始，一直建立原型并删除代码，直到找到金矿，然后你开始深入，改进代码、性能等。在那之前，一切都是关于快速迭代和良好的监控/分析。我想这是被反复重复的标准建议，但我认为许多人并不真正理解它的重要性。
- en: I don’t think technologies matter that much nowadays, so use whatever works
    for you. I’d think twice before moving to hosted environments like App Engine
    or Heroku, though.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为现在技术并不那么重要，所以使用能解决问题的任何技术即可。不过，我会再三考虑迁移到像App Engine或Heroku这样的托管环境。
- en: Large-Scale Social Media Analysis at Smesh (2014)
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Smesh进行的大规模社交媒体分析（2014）
- en: Alex Kelly ([*sme.sh*](http://www.sme.sh))
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: Alex Kelly（[*sme.sh*](http://www.sme.sh)）
- en: At Smesh, we produce software that ingests data from a wide variety of APIs
    across the web; filters, processes, and aggregates it; and then uses that data
    to build bespoke apps for a variety of clients. For example, we provide the tech
    that powers the tweet filtering and streaming in Beamly’s second-screen TV app,
    run a brand and campaign monitoring platform for mobile network EE, and run a
    bunch of Adwords data analysis projects for Google.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在Smesh，我们制作软件，从网络上各种API摄取数据；过滤、处理和聚合数据；然后使用这些数据为多样的客户构建定制的应用程序。例如，我们为Beamly的第二屏电视应用提供推文过滤和流服务技术，为移动网络EE运行品牌和活动监控平台，并为Google运行多个Adwords数据分析项目。
- en: To do that, we run a variety of streaming and polling services, frequently polling
    Twitter, Facebook, YouTube, and a host of other services for content and processing
    several million tweets daily.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们运行各种流媒体和轮询服务，频繁地从Twitter、Facebook、YouTube等服务中获取内容，并每天处理数百万条推文。
- en: Python’s Role at Smesh
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Python在Smesh的角色
- en: We use Python extensively—the majority of our platform and services are built
    with it. The wide variety of libraries, tools, and frameworks available allows
    us to use it across the board for most of what we do.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们广泛使用Python——我们的平台和服务的大部分都是用它构建的。可用的各种库、工具和框架使我们能够在大多数工作中使用它。
- en: That variety gives us the ability to (hopefully) pick the right tool for the
    job. For example, we’ve created apps using Django, Flask, and Pyramid. Each has
    its own benefits, and we can pick the one that’s right for the task at hand. We
    use Celery for tasks; Boto for interacting with AWS; and PyMongo, MongoEngine,
    redis-py, Psycopg, etc. for all our data needs. The list goes on and on.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这种多样性使我们能够（希望能够）为工作选择合适的工具。例如，我们使用Django、Flask和Pyramid创建了应用程序。每种工具都有其独特的优点，我们可以根据手头任务选择适合的工具。我们使用Celery处理任务；使用Boto与AWS交互；以及PyMongo、MongoEngine、redis-py、Psycopg等满足所有数据需求。清单还在继续。
- en: The Platform
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 平台
- en: Our main platform consists of a central Python module that provides hooks for
    data input, filtering, aggregations and processing, and a variety of other core
    functions. Project-specific code imports functionality from that core and then
    implements more specific data processing and view logic, as each application requires.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要平台由一个中心Python模块组成，提供数据输入、过滤、聚合和处理的钩子，以及各种其他核心功能。项目特定的代码从核心导入功能，然后实现更具体的数据处理和视图逻辑，因为每个应用程序都有不同的需求。
- en: This has worked well for us up to now, and allows us to build fairly complex
    applications that ingest and process data from a wide variety of sources without
    much duplication of effort. However, it isn’t without its drawbacks—each app is
    dependent on a common core module, making the process of updating the code in
    that module and keeping all the apps that use it up-to-date a major task.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，这种方式对我们运作得很好，并且允许我们构建相当复杂的应用程序，从各种来源摄取和处理数据，减少了大量的重复工作。不过，这并非没有缺点——每个应用程序都依赖于一个共同的核心模块，更新该模块的代码并保持使用它的所有应用程序的更新是一项重大任务。
- en: We’re currently working on a project to redesign that core software and move
    toward more of a service-oriented architecture (SoA) approach. It seems that finding
    the right time to make that sort of architectural change is one of the challenges
    that faces most software teams as a platform grows. There is overhead in building
    components as individual services, and often the deep domain-specific knowledge
    required to build each service is acquired only through an initial iteration of
    development, where that architectural overhead is a hindrance to solving the real
    problem at hand. Hopefully, we’ve chosen a sensible time to revisit our architectural
    choices to move things forward. Time will tell.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们目前正在进行一个项目，重新设计核心软件，并朝着更多的面向服务的架构 (SoA) 方法迈进。看起来，找到进行这种架构变更的合适时机是大多数软件团队在平台发展中面临的挑战之一。构建组件作为单独服务存在开销，并且通常只有通过开发的初步迭代获取构建每个服务所需的深层领域特定知识时，这种架构开销才会成为解决实际问题的障碍。希望我们选择了一个明智的时机来重新审视我们的架构选择，推动事物向前发展。时间将会告诉我们结果。
- en: High Performance Real-Time String Matching
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高性能实时字符串匹配
- en: We consume lots of data from the Twitter Streaming API. As we stream in tweets,
    we match the input strings against a set of keywords so that we know which of
    the terms we’re tracking that each tweet is related to. That’s not such a problem
    with a low rate of input, or a small set of keywords, but doing that matching
    for hundreds of tweets per second, against hundreds or thousands of possible keywords,
    starts to get tricky.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从 Twitter Streaming API 大量获取数据。当我们流式传入推文时，我们会将输入字符串与一组关键词进行匹配，以便知道每条推文与我们正在追踪的哪些术语相关联。当输入速率较低或关键词集较小时，并不是问题，但是每秒处理数百条推文，同时匹配数百甚至数千个可能的关键词，开始变得棘手起来。
- en: 'To make things even trickier, we’re not interested in simply whether the keyword
    string exists in the tweet, but in more complex pattern matching against word
    boundaries, start and end of line, and optionally the use of # and @ characters
    to prefix the string. The most effective way to encapsulate that matching knowledge
    is using regular expressions. However, running thousands of regex patterns across
    hundreds of tweets per second is computationally intensive. Previously, we had
    to run many worker nodes across a cluster of machines to perform the matching
    reliably in real time.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '更棘手的是，我们不仅对推文中的关键词字符串是否存在感兴趣，而是对单词边界、行的起始和结束，以及可选的在字符串前缀使用 # 和 @ 字符进行更复杂的模式匹配感兴趣。封装这些匹配知识的最有效方式是使用正则表达式。然而，每秒在数百条推文上运行成千上万个正则表达式模式是计算密集的。此前，我们必须在一组机器的集群上运行许多工作节点，以确保能够实时可靠地执行匹配。'
- en: 'Knowing this was a major performance bottleneck in the system, we tried a variety
    of things to improve the performance of our matching system: simplifying the regexes,
    running enough processes to ensure we were utilizing all the cores on our servers,
    ensuring all our regex patterns are compiled and cached properly, running the
    matching tasks under PyPy instead of CPython, etc. Each of these resulted in a
    small increase in performance, but it was clear this approach was going to shave
    only a fraction of our processing time. We were looking for an order-of-magnitude
    speedup, not a fractional improvement.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 了解这一点是系统中的一个主要性能瓶颈，我们尝试了多种方法来改善匹配系统的性能：简化正则表达式、运行足够的进程以确保我们充分利用服务器上的所有核心、确保所有的正则表达式模式都得到正确编译和缓存、在
    PyPy 而不是 CPython 下运行匹配任务等。这些方法每种都带来了一点性能提升，但显然这种方法只能节约少量处理时间。我们希望获得数量级的加速，而不是小幅度改善。
- en: It was obvious that rather than trying to increase the performance of each match,
    we needed to reduce the problem space before the pattern matching takes place.
    So we needed to reduce either the number of tweets to process, or the number of
    regex patterns we needed to match the tweets against. Dropping the incoming tweets
    wasn’t an option—that’s the data we’re interested in. So we set about finding
    a way to reduce the number of patterns we need to compare an incoming tweet to
    in order to perform the matching.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，我们需要在进行模式匹配之前，缩小问题空间，而不是试图提高每次匹配的性能。因此，我们需要减少要处理的推文数，或者减少需要对推文进行匹配的正则表达式模式数量。丢弃传入的推文并不是一个选择——那是我们感兴趣的数据。因此，我们着手寻找一种方法，以减少我们需要将传入推文与之比较的模式数量，以进行匹配。
- en: We started looking at various trie structures for allowing us to do pattern
    matching between sets of strings more efficiently, and came across the Aho-Corasick
    string-matching algorithm. It turned out to be ideal for our use case. The dictionary
    from which the trie is built needs to be static—you can’t add new members to the
    trie after the automaton has been finalized—but for us this isn’t a problem, as
    the set of keywords is static for the duration of a session streaming from Twitter.
    When we change the terms we’re tracking we must disconnect from and reconnect
    to the API, so we can rebuild the Aho-Corasick trie at the same time.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始研究各种字典树结构，以更高效地进行字符串集之间的模式匹配，并了解到了Aho-Corasick字符串匹配算法。对于我们的使用场景来说，它非常理想。用于构建字典树的词典必须是静态的——在确定自动机后不能向字典树添加新成员——但对我们来说，这不是问题，因为关键词集合在Twitter会话期间是静态的。当我们改变正在跟踪的术语时，我们必须断开并重新连接API，这样我们可以同时重建Aho-Corasick字典树。
- en: Processing an input against the strings using Aho-Corasick finds all possible
    matches simultaneously, stepping through the input string a character at a time
    and finding matching nodes at the next level down in the trie (or not, as the
    case may be). So we can very quickly find which of our keyword terms may exist
    in the tweet. We still don’t know for sure, as the pure string-in-string matching
    of Aho-Corasick doesn’t allow us to apply any of the more complex logic that is
    encapsulated in the regex patterns, but we can use the Aho-Corasick matching as
    a prefilter. Keywords that don’t exist in the string can’t match, so we know we
    have to try only a small subset of all our regex patterns, based on the keywords
    that do appear in the text. Rather than evaluating hundreds or thousands of regex
    patterns against every input, we rule out the majority and need to process only
    a handful for each tweet.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Aho-Corasick对输入字符串进行处理可以同时找到所有可能的匹配项，逐个字符地遍历输入字符串，并在字典树的下一级找到匹配的节点（或者没有找到）。因此，我们可以非常快速地找到推文中可能存在的关键词。尽管我们仍然不确定，因为Aho-Corasick的纯字符串匹配不能应用任何复杂逻辑，这些逻辑通常包含在正则表达式模式中，但我们可以将Aho-Corasick匹配视为预过滤器。不存在于字符串中的关键词不可能匹配，因此我们只需基于文本中出现的关键词处理少量正则表达式模式，而不是对每个输入都评估数百或数千个正则表达式模式。
- en: By reducing the number of patterns that we attempt to match against each incoming
    tweet to just a small handful, we’ve managed to achieve the speedup we were looking
    for. Depending on the complexity of the trie and the average length of the input
    tweets, our keyword matching system now performs somewhere between 10–100× faster
    than the original naive implementation.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 通过减少我们尝试匹配每条传入推文的模式数量，仅保留少量模式，我们已经成功实现了所期望的加速效果。根据字典树的复杂性和输入推文的平均长度不同，我们的关键词匹配系统现在比原始的简单实现快了10到100倍。
- en: If you’re doing a lot of regex processing, or other pattern matching, I highly
    recommend having a dig around the different variations of prefix and suffix tries
    that might help you to find a blazingly fast solution to your problem.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你经常进行正则表达式处理或其他模式匹配工作，我强烈建议你详细了解不同变体的前缀和后缀字典树，这可能会帮助你找到解决问题的极快速度的解决方案。
- en: Reporting, Monitoring, Debugging, and Deployment
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 报告、监控、调试和部署
- en: We maintain a bunch of different systems running our Python software and the
    rest of the infrastructure that powers it all. Keeping it all up and running without
    interruption can be tricky. Here are a few lessons we’ve learned along the way.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们管理多个运行我们的Python软件和支持其所有基础设施的系统。保持一切运行无中断可能会有些棘手。以下是我们在这个过程中学到的几个经验教训。
- en: It’s really powerful to be able to see both in real time and historically what’s
    going on inside your systems, whether that be in your own software or the infrastructure
    it runs on. We use Graphite with `collectd` and `statsd` to allow us to draw pretty
    graphs of what’s going on. That gives us a way to spot trends, and to retrospectively
    analyze problems to find the root cause. We haven’t got around to implementing
    it yet, but Etsy’s Skyline also looks brilliant as a way to spot the unexpected
    when you have more metrics than you can keep track of. Another useful tool is
    Sentry, a great system for event logging and keeping track of exceptions being
    raised across a cluster of machines.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 能够实时和历史上看到系统内部发生的情况真的非常强大，无论是在你自己的软件内部还是其运行的基础设施上。我们使用 **Graphite** 与 `collectd`
    以及 `statsd` 来绘制系统状态的漂亮图表。这为我们提供了一种发现趋势和回顾性地分析问题以找出根本原因的方法。我们还没有开始实施，但 Etsy 的 **Skyline**
    看起来也是一个很好的工具，用来在你有更多指标需要跟踪时发现意外情况。另一个有用的工具是 **Sentry**，一个非常棒的事件日志系统，用于跟踪一组机器上发生的异常。
- en: Deployment can be painful, no matter what you’re using to do it. We’ve been
    users of Puppet, Ansible, and Salt. They all have pros and cons, but none of them
    will make a complex deployment problem magically go away.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你使用什么来进行部署，部署都可能是痛苦的。我们曾经使用过 **Puppet**，**Ansible** 和 **Salt**。它们各有优缺点，但没有一个能够魔法般地解决复杂的部署问题。
- en: To maintain high availability for some of our systems, we run multiple geographically
    distributed clusters of infrastructure, running one system live and others as
    hot spares, with switchover being done by updates to DNS with low Time-to-Live
    (TTL) values. Obviously that’s not always straightforward, especially when you
    have tight constraints on data consistency. Thankfully, we’re not affected by
    that too badly, making the approach relatively straightforward. It also provides
    us with a fairly safe deployment strategy, updating one of our spare clusters
    and performing testing before promoting that cluster to live and updating the
    others.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持某些系统的高可用性，我们在全球多个地理位置分布的基础设施集群上运行多个实例，将一个系统设为活动实例，其他系统作为热备份，并通过低 TTL（Time-to-Live）值的
    DNS 更新来进行切换。显然，这并不总是简单的，特别是在数据一致性方面有严格的约束时。幸运的是，我们并没有受到太大的影响，使得这种方法相对来说还是相当简单的。这也为我们提供了一个相对安全的部署策略，更新一个备用集群并在晋升该集群为活动实例并更新其他集群之前进行测试。
- en: Along with everyone else, we’re really excited by the prospect of what can be
    done with [Docker](http://www.docker.com). Also along with pretty much everyone
    else, we’re still just at the stage of playing around with it to figure out how
    to make it part of our deployment processes. However, having the ability to rapidly
    deploy our software in a lightweight and reproducible fashion, with all its binary
    dependencies and system libraries included, seems to be just around the corner.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他人一样，我们对使用 [Docker](http://www.docker.com) 能够实现的可能性感到非常兴奋。也和几乎所有人一样，我们仍然处于摸索阶段，试图弄清楚如何将其融入我们的部署流程中。然而，能够以轻量且可重现的方式快速部署我们的软件，包括所有二进制依赖项和系统库，似乎就在不远的将来。
- en: At a server level, there’s a whole bunch of routine stuff that just makes life
    easier. Monit is great for keeping an eye on things for you. Upstart and `supervisord`
    make running services less painful. Munin is useful for some quick and easy system-level
    graphing if you’re not using a full Graphite/`collectd` setup. And Corosync/Pacemaker
    can be a good solution for running services across a cluster of nodes (for example,
    when you have a bunch of services that you need to run somewhere, but not everywhere).
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在服务器级别，有一大堆例行公事能够让生活更轻松。**Monit** 对于帮助你监控事物非常有用。**Upstart** 和 `supervisord`
    则使得运行服务变得更加轻松。如果你没有使用完整的 **Graphite**/**collectd** 设置，**Munin** 对于一些快速简易的系统级图形显示也是非常有用的。而
    **Corosync**/**Pacemaker** 则可以成为在集群节点上运行服务的良好解决方案（例如，当你有一堆需要在某处但不是每处运行的服务时）。
- en: I’ve tried not to just list buzzwords here, but to point you toward software
    we’re using every day, which is really making a difference in how effectively
    we can deploy and run our systems. If you’ve heard of them all already, I’m sure
    you must have a whole bunch of other useful tips to share, so please drop me a
    line with some pointers. If not, go check them out—hopefully, some of them will
    be as useful to you as they are to us.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我尽量不只是列出流行词汇，而是指向我们每天都在使用的软件，这些软件确实在我们能有效部署和运行系统的效率上起到了巨大的作用。如果你已经听说过它们了，我相信你一定还有许多其他有用的建议要分享，所以请给我发封信指点一二。如果还没有，那就去了解一下吧——希望其中一些对你同样有用。
- en: PyPy for Successful Web and Data Processing Systems (2014)
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 成功的 Web 和数据处理系统使用 PyPy（2014）
- en: Marko Tasic ([*https://github.com/mtasic85*](https://github.com/mtasic85))
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: Marko Tasic（[*https://github.com/mtasic85*](https://github.com/mtasic85)）
- en: Since I had a great experience early on with PyPy, I chose to use it everywhere
    where it was applicable. I have used it from small toy projects where speed was
    essential to medium-sized projects. The first project where I used it was a protocol
    implementation; the protocols we implemented were Modbus and DNP3\. Later, I used
    it for a compression algorithm implementation, and everyone was amazed by its
    speed. The first version I used in production was PyPy 1.2 with JIT out of the
    box, if I recall correctly. By version 1.4, we were sure it was the future of
    all our projects, because many bugs got fixed and the speed just increased more
    and more. We were surprised how simple cases were made 2–3× faster just by upgrading
    PyPy up to the next version.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 早期我在PyPy上有很好的经验，所以我选择在适用的地方都使用它。我使用它从小型玩具项目到中型项目。我第一次使用它的项目是一个协议实现；我们实现的协议是Modbus和DNP3。后来，我用它来实现一个压缩算法，每个人都对它的速度感到惊讶。我记得，我在生产中使用的第一个版本是PyPy
    1.2，自带JIT。到了1.4版本，我们确信它是我们所有项目的未来，因为修复了许多错误，速度也越来越快。我们惊讶地发现，仅仅升级PyPy到下一个版本，简单的案例就能快2-3倍。
- en: I will explain two separate but deeply related projects that share 90% of the
    same code here, but to keep the explanation simple to follow, I will refer to
    both of them as “the project.”
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我将在这里解释两个分开但深度相关的项目，它们共享90%的相同代码，但为了使解释易于理解，我将把它们都称为“该项目”。
- en: The project was to create a system that collects newspapers, magazines, and
    blogs, applies optical character recognition (OCR) if necessary, classifies them,
    translates, applies sentiment analyzing, analyzes the document structure, and
    indexes them for later search. Users can search for keywords in any of the available
    languages and retrieve information about indexed documents. Search is cross-language,
    so users can write in English and get results in French. Additionally, users will
    receive articles and keywords highlighted from the document’s page with information
    about the space occupied and price of publication. A more advanced use case would
    be report generation, where users can see a tabular view of results with detailed
    information on spending by any particular company on advertising in monitored
    newspapers, magazines, and blogs. As well as advertising, it can also “guess”
    if an article is paid or objective and determine its tone.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 该项目是创建一个系统，收集报纸、杂志和博客，如有必要进行光学字符识别（OCR），对它们进行分类、翻译、应用情感分析、分析文档结构，并为以后的搜索对它们进行索引。用户可以在任何可用语言中搜索关键字，并检索有关索引文档的信息。搜索是跨语言的，所以用户可以用英语写作并获得法语的结果。此外，用户还将收到来自文档页面的被强调的文章和关键字，以及有关所占空间和刊登价格的信息。一个更高级的用例是报告生成，用户可以查看结果的表格视图，详细了解任何特定公司在受监控的报纸、杂志和博客上的广告支出情况。除了广告，它还可以“猜测”一篇文章是付费还是客观的，并确定其语调。
- en: Prerequisites
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 先决条件
- en: Obviously, PyPy was our favorite Python implementation. For the database, we
    used Cassandra and Elasticsearch. Cache servers used Redis. We used Celery as
    a distributed task queue (workers), and for its broker, we used RabbitMQ. Results
    were kept in a Redis backend. Later on, Celery used Redis more exclusively for
    both brokers and backend. The OCR engine used is Tesseract. The language translation
    engine and server used is Moses. We used Scrapy for crawling websites. For distributed
    locking in the whole system, we use a ZooKeeper server, but initially Redis was
    used for that. The web application is based on the excellent Flask web framework
    and many of its extensions, such as Flask-Login, Flask-Principal, etc. The Flask
    application was hosted by Gunicorn and Tornado on every web server, and nginx
    was used as a reverse proxy server for the web servers. The rest of the code was
    written by us and is pure Python that runs on top of PyPy.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，PyPy是我们最喜欢的Python实现。对于数据库，我们使用了Cassandra和Elasticsearch。缓存服务器使用了Redis。我们使用Celery作为分布式任务队列（工作者），对于其代理，我们使用了RabbitMQ。结果保存在Redis后端。后来，Celery更多地使用Redis作为代理和后端。OCR引擎使用的是Tesseract。语言翻译引擎和服务器使用的是Moses。我们使用Scrapy来爬取网站。对于整个系统的分布式锁定，我们使用了ZooKeeper服务器，但最初使用了Redis。Web应用程序基于出色的Flask
    Web框架及其许多扩展，例如Flask-Login、Flask-Principal等。Flask应用程序由Gunicorn和Tornado在每个Web服务器上托管，nginx被用作Web服务器的反向代理服务器。其余的代码由我们编写，是纯Python代码，运行在PyPy之上。
- en: The whole project is hosted on an in-house OpenStack private cloud and executes
    between 100 and 1,000 instances of ArchLinux, depending on requirements, which
    can change dynamically on the fly. The whole system consumes up to 200 TB of storage
    every 6–12 months, depending on the mentioned requirements. All processing is
    done by our Python code, except OCR and translation.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 整个项目托管在内部 OpenStack 私有云上，并根据需求执行 100 到 1,000 个 ArchLinux 实例，这些需求可以动态地随时更改。整个系统每
    6 到 12 个月消耗高达 200 TB 的存储空间，具体取决于上述需求。除了 OCR 和翻译外，所有处理都由我们的 Python 代码完成。
- en: The Database
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据库
- en: We developed a Python package that unifies model classes for Cassandra, Elasticsearch,
    and Redis. It is a simple object relational mapper (ORM) that maps everything
    to a dict or list of dicts, in the case where many records are retrieved from
    the database.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开发了一个 Python 包，统一了 Cassandra、Elasticsearch 和 Redis 的模型类。它是一个简单的对象关系映射器（ORM），将所有内容映射到字典或字典列表中，以便从数据库中检索多个记录时使用。
- en: Since Cassandra 1.2 did not support complex queries on indices, we supported
    them with join-like queries. However, we allowed complex queries over small datasets
    (up to 4 GB) because much of that had to be processed while held in memory. PyPy
    ran in cases where CPython could not even load data into memory, thanks to its
    strategies applied to homogeneous lists to make them more compact in the memory.
    Another benefit of PyPy is that its JIT compilation kicked in loops where data
    manipulation or analysis happened. We wrote code in such a way that the types
    would stay static inside loops because that’s where JIT-compiled code is especially
    good.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Cassandra 1.2 不支持对索引进行复杂查询，我们通过类似于连接的查询来支持它们。但是，对小数据集（高达 4 GB）进行复杂查询是允许的，因为其中大部分必须在内存中处理。PyPy
    运行在 CPython 无法加载数据到内存的情况下，这要归功于它对同质列表应用的策略，使它们在内存中更加紧凑。PyPy 的另一个好处是，它的即时编译在数据操作或分析发生的循环中起作用。我们编写代码的方式是，类型在循环内部保持静态，因为
    JIT 编译的代码在这里特别有效。
- en: Elasticsearch was used for indexing and fast searching of documents. It is very
    flexible when it comes to query complexity, so we did not have any major issues
    with it. One of the issues we had was related to updating documents; it is not
    designed for rapidly changing documents, so we had to migrate that part to Cassandra.
    Another limitation was related to facets and memory required on the database instance,
    but that was solved by having more smaller queries and then manually manipulating
    data in Celery workers. No major issues surfaced between PyPy and the PyES library
    used for interaction with Elasticsearch server pools.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch 用于文档的索引和快速搜索。在查询复杂性方面非常灵活，因此我们在使用过程中没有遇到任何重大问题。我们遇到的一个问题与更新文档有关；它并不适用于快速变化的文档，因此我们不得不将该部分迁移到
    Cassandra。另一个限制与数据库实例上所需的 facets 和内存有关，但这可以通过使用更多的较小查询，然后在 Celery workers 中手动处理数据来解决。在
    PyPy 和用于与 Elasticsearch 服务器池交互的 PyES 库之间没有出现重大问题。
- en: The Web Application
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Web 应用程序
- en: As we’ve mentioned, we used the Flask framework with its third-party extensions.
    Initially, we started everything in Django, but we switched to Flask because of
    rapid changes in requirements. This does not mean that Flask is better than Django;
    it was just easier for us to follow code in Flask than in Django, since its project
    layout is very flexible. Gunicorn was used as a Web Server Gateway Interface (WSGI)
    HTTP server, and its I/O loop was executed by Tornado. This allowed us to have
    up to one hundred concurrent connections per web server. This was lower than expected
    because many user queries can take a long time—a lot of analyzing happens in user
    requests, and data is returned in user interactions.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，我们使用 Flask 框架及其第三方扩展。最初我们在 Django 中启动了一切，但由于需求的迅速变化，我们转向了 Flask。这并不意味着
    Flask 比 Django 更好；只是对于我们来说，在 Flask 中更容易遵循代码，因为它的项目布局非常灵活。Gunicorn 用作 Web 服务器网关接口（WSGI）HTTP
    服务器，其 I/O 循环由 Tornado 执行。这使我们可以每个 Web 服务器拥有最多一百个并发连接。这比预期的要低，因为许多用户查询可能需要很长时间——在用户请求中进行了大量分析，并在用户交互中返回数据。
- en: Initially, the web application depended on the Python Imaging Library (PIL)
    for article and word highlighting. We had issues with the PIL library and PyPy
    because at that time many memory leaks were associated with PIL. Then we switched
    to Pillow, which was more frequently maintained. In the end, we wrote a library
    that interacted with GraphicsMagick via a subprocess module.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，Web 应用程序依赖于 Python Imaging Library（PIL）进行文章和单词高亮显示。我们在 PIL 库和 PyPy 之间遇到了问题，因为当时与
    PIL 相关的许多内存泄漏。然后我们切换到了 Pillow，它的维护更加频繁。最终，我们编写了一个通过 subprocess 模块与 GraphicsMagick
    交互的库。
- en: PyPy runs well, and the results are comparable with CPython. This is because
    usually web applications are I/O-bound. However, with the development of STM in
    PyPy, we hope to have scalable event handling on a multicore instance level soon.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: PyPy 运行良好，结果与 CPython 可比。这是因为通常 Web 应用程序受 I/O 限制。但是，随着 PyPy 中 STM 的发展，我们希望不久的将来能在多核实例级别上实现可扩展的事件处理。
- en: OCR and Translation
  id: totrans-289
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OCR 和翻译
- en: We wrote pure Python libraries for Tesseract and Moses because we had problems
    with CPython API-dependent extensions. PyPy has good support for the CPython API
    using CPyExt, but we wanted to be more in control of what happens under the hood.
    As a result, we made a PyPy-compatible solution with slightly faster code than
    on CPython. The reason it was not faster is that most of the processing happened
    in the C/C++ code of both Tesseract and Moses. We could only speed up output processing
    and building Python structure of documents. There were no major issues at this
    stage with PyPy compatibility.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为 Tesseract 和 Moses 编写了纯 Python 库，因为我们在 CPython API 依赖扩展方面遇到了问题。PyPy 使用 CPyExt
    对 CPython API 有良好的支持，但我们希望更加控制底层发生的事情。因此，我们制定了一个与 PyPy 兼容的解决方案，其代码略快于 CPython。之所以没有更快，是因为大部分处理发生在
    Tesseract 和 Moses 的 C/C++ 代码中。我们只能加速输出处理和构建文档的 Python 结构。在这个阶段，PyPy 的兼容性没有重大问题。
- en: Task Distribution and Workers
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 任务分配和工作人员
- en: Celery gave us the power to run many tasks in the background. Typical tasks
    are OCR, translation, analysis, etc. The whole thing could be done using Hadoop
    for MapReduce, but we chose Celery because we knew that the project requirements
    might change often.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: Celery 赋予了我们在后台运行多个任务的能力。典型任务包括 OCR、翻译、分析等。我们完全可以使用 Hadoop 进行 MapReduce，但我们选择了
    Celery，因为我们知道项目需求可能经常变化。
- en: We had about 20 workers, and each worker had between 10 and 20 functions. Almost
    all functions had loops, or many nested loops. We cared that types stayed static,
    so the JIT compiler could do its job. The end results were a 2–5× speedup over
    CPython. The reason we did not get better speedups was because our loops were
    relatively small, between 20,000 and 100,000 iterations. In some cases where we
    had to do analysis on the word level, we had over 1 million iterations, and that’s
    where we got over a 10× speedup.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们大约有 20 名工作者，每名工作者都有 10 至 20 个函数。几乎所有函数都有循环或多重嵌套循环。我们关心类型保持静态，以便 JIT 编译器可以发挥其作用。最终结果是比
    CPython 快 2 到 5 倍。我们没有获得更好的加速是因为我们的循环相对较小，介于 20,000 到 100,000 次迭代之间。在一些需要在单词级别进行分析的情况下，我们有超过
    1 百万次迭代，这是我们获得超过 10 倍加速的地方。
- en: Conclusion
  id: totrans-294
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: 'PyPy is an excellent choice for every pure Python project that depends on speed
    of execution of readable and maintainable large source code. We found PyPy also
    to be very stable. All our programs were long-running with static and/or homogeneous
    types inside data structures, so JIT could do its job. When we tested the whole
    system on CPython, the results did not surprise us: we had roughly a 2× speedup
    with PyPy over CPython. In the eyes of our clients, this meant 2× better performance
    for the same price. In addition to all the good stuff that PyPy has brought to
    us so far, we hope that its software transactional memory (STM) implementation
    will bring to us scalable parallel execution for Python code.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: PyPy 是每个依赖于可读性和可维护性的大型源代码速度执行的纯 Python 项目的绝佳选择。我们发现 PyPy 也非常稳定。我们所有的程序都长时间运行，数据结构内部都是静态和/或同质类型，因此
    JIT 可以发挥其作用。当我们在 CPython 上测试整个系统时，结果并不令我们惊讶：PyPy 比 CPython 快了大约 2 倍。在我们客户眼中，这意味着以同样价格获得了
    2 倍的性能提升。除了 PyPy 到目前为止带给我们的所有好处外，我们希望它的软件事务内存（STM）实现能为我们带来 Python 代码的可扩展并行执行。
- en: Task Queues at Lanyrd.com (2014)
  id: totrans-296
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Lanyrd.com 的任务队列（2014年）
- en: Andrew Godwin
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: Andrew Godwin
- en: Lanyrd is a website for social discovery of conferences—our users sign in, and
    we use their friend graphs from social networks, as well as other indicators like
    their industry of work or their geographic location, to suggest relevant conferences.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: Lanyrd 是一个用于社交发现会议的网站——我们的用户登录，我们利用他们在社交网络中的朋友关系图，以及其他指标如他们的行业或地理位置，来推荐相关的会议。
- en: The main work of the site is in distilling this raw data down into something
    we can show to the users—essentially, a ranked list of conferences. We have to
    do this offline, because we refresh the list of recommended conferences every
    couple of days and because we’re hitting external APIs that are often slow. We
    also use the Celery task queue for other things that take a long time, like fetching
    thumbnails for links people provide and sending email. There are usually well
    over 100,000 tasks in the queue each day, and sometimes many more.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 站点的主要工作是将这些原始数据浓缩为我们可以向用户展示的内容——基本上是会议的排名列表。我们必须离线完成这项工作，因为我们每隔几天刷新推荐会议列表，并且因为我们正在访问通常很慢的外部
    API。我们还使用 Celery 任务队列处理其他需要很长时间的任务，比如为用户提供的链接获取缩略图和发送电子邮件。每天队列中通常有超过 100,000 个任务，有时甚至更多。
- en: Python’s Role at Lanyrd
  id: totrans-300
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Python 在 Lanyrd 的角色
- en: Lanyrd was built with Python and Django from day one, and virtually every part
    of it is written in Python—the website itself, the offline processing, our statistical
    and analysis tools, our mobile backend servers, and the deployment system. It’s
    a very versatile and mature language and one that’s incredibly easy to write things
    in quickly, mostly thanks to the large amount of libraries available and the language’s
    easily readable and concise syntax, which means it’s easy to update and refactor
    as well as easy to write initially.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: Lanyrd 从一开始就使用 Python 和 Django 构建，几乎每个部分都是用 Python 编写的——包括网站本身、离线处理、统计分析工具、移动后端服务器以及部署系统。这是一种非常多才多艺且成熟的语言，非常适合快速编写，主要得益于其丰富的库和易于阅读简洁的语法，这使得更新和重构变得容易，初始编写也很轻松。
- en: The Celery task queue was already a mature project when we evolved the need
    for a task queue (very early on), and the rest of Lanyrd was already in Python,
    so it was a natural fit. As we grew, there was a need to change the queue that
    backed it (which ended up being Redis), but it’s generally scaled very well.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: Celery 任务队列在我们早期（非常早期）需要任务队列时已经是一个成熟的项目，而且 Lanyrd 的其余部分已经是 Python，所以它非常合适。随着我们的增长，有必要改变它的后端队列（最终选择了
    Redis），但它通常扩展得非常好。
- en: As a start-up, we had to ship some known technical debt in order to make some
    headway—this is something you just have to do, and as long as you know what your
    issues are and when they might surface, it’s not necessarily a bad thing. Python’s
    flexibility in this regard is fantastic; it generally encourages loose coupling
    of components, which means it’s often easy to ship something with a “good enough”
    implementation and then easily refactor a better one in later.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 作为初创企业，我们不得不在前进中留下一些已知的技术债务——这是你必须做的事情，只要你知道自己的问题在哪里，以及何时可能会出现，这并不一定是件坏事。Python
    在这方面的灵活性非常棒；它通常鼓励组件的松耦合，这意味着通常可以轻松地发布一个“足够好”的实现，然后稍后轻松地重构为更好的实现。
- en: Anything critical, such as payment code, had full unit test coverage, but for
    other parts of the site and task queue flow (especially display-related code)
    things were often moving too fast to make unit tests worthwhile (they would be
    too fragile). Instead, we adopted a very agile approach and had a two-minute deploy
    time and excellent error tracking; if a bug made it into live, we could often
    fix it and deploy within five minutes.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 所有关键部分，比如支付代码，都有完整的单元测试覆盖，但对于站点的其他部分和任务队列流程（尤其是与显示相关的代码），事情往往进展得太快，以至于单元测试不值得（它们会太脆弱）。因此，我们采用了非常敏捷的方法，并且拥有两分钟的部署时间和出色的错误跟踪；如果有
    bug 进入生产环境，我们通常可以在五分钟内修复并部署。
- en: Making the Task Queue Performant
  id: totrans-305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提高任务队列的性能
- en: The main issue with a task queue is throughput. If it gets backlogged, the website
    keeps working but starts getting mysteriously outdated—lists don’t update, page
    content is wrong, and emails don’t get sent for hours.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 任务队列的主要问题是吞吐量。如果积压，网站仍然可以工作，但开始出现神秘的过时问题——列表不更新，页面内容错误，电子邮件几个小时内未发送。
- en: Fortunately, though, task queues also encourage a very scalable design; as long
    as your central messaging server (in our case, Redis) can handle the messaging
    overhead of the job requests and responses, for the actual processing you can
    spin up any number of worker daemons to handle the load.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，任务队列还鼓励设计非常可扩展的架构；只要我们的中央消息服务器（在我们的情况下是 Redis）能够处理作业请求和响应的消息开销，实际处理过程中可以启动任意数量的工作守护程序来处理负载。
- en: Reporting, Monitoring, Debugging, and Deployment
  id: totrans-308
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 报告、监控、调试和部署
- en: We had monitoring that kept track of our queue length, and if it started becoming
    long we would just deploy another server with more worker daemons. Celery makes
    this very easy to do. Our deployment system had hooks where we could increase
    the number of worker threads on a box (if our CPU utilization wasn’t optimal)
    and could easily turn a fresh server into a Celery worker within 30 minutes. It’s
    not like website response times going through the floor—if your task queues suddenly
    get a load spike, you have some time to implement a fix and usually it’ll smooth
    over itself, if you’ve left enough spare capacity.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有监控来跟踪我们的队列长度，如果队列开始变长，我们就会部署另一台带有更多工作守护程序的服务器。Celery 让这一切变得非常简单。我们的部署系统有钩子，可以根据需要增加单个服务器上的工作线程数（如果
    CPU 利用率不理想），并且可以在 30 分钟内轻松将新服务器转换为 Celery 工作节点。这不像网站响应时间突然下降一样——如果你的任务队列突然出现负载高峰，你有时间实施修复措施，通常情况下问题会自行平稳过渡，只要你留有足够的备用容量。
- en: Advice to a Fellow Developer
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向同行开发者的建议
- en: My main advice is to shove as much as you can into a task queue (or a similar
    loosely coupled architecture) as soon as possible. It takes some initial engineering
    effort, but as you grow, operations that used to take half a second can grow to
    half a minute, and you’ll be glad they’re not blocking your main rendering thread.
    Once you’ve got there, make sure you keep a close eye on your average queue latency
    (how long it takes a job to go from submission to completion), and make sure there’s
    some spare capacity for when your load increases.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 我的主要建议是尽早将尽可能多的任务放入任务队列（或类似的松耦合架构）中。这需要一些初始的工程努力，但随着业务的增长，原本只需半秒钟的操作可能会增长到半分钟，你会庆幸它们没有阻塞你的主渲染线程。一旦达到这一点，确保密切关注平均队列延迟（任务从提交到完成所需的时间），并确保在负载增加时有一些备用容量。
- en: Finally, be aware that having multiple task queues for different priorities
    of tasks makes sense. Sending email isn’t very high priority; people are used
    to emails taking minutes to arrive. However, if you’re rendering a thumbnail in
    the background and showing a spinner while you do it, you want that job to be
    high priority, as otherwise you’re making the user experience worse. You don’t
    want your 100,000-person mailshot to delay all thumbnailing on your site for the
    next 20 minutes!
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请注意为不同优先级的任务设置多个任务队列是有意义的。发送电子邮件并非很高优先级；人们习惯于等待几分钟才能收到电子邮件。然而，如果在后台渲染缩略图并显示加载指示器的同时，你希望该任务具有较高的优先级，否则会影响用户体验。你不希望你的
    100,000 人邮件群发延迟整个站点的所有缩略图生成 20 分钟！
