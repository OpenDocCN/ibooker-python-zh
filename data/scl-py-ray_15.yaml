- en: Appendix B. Installing and Deploying Ray
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录 B. 安装和部署 Ray
- en: The power of Ray is in its support for various deployment models, ranging from
    a single-node deployment—​allowing you to experiment with Ray locally—​to clusters
    containing thousands of machines. The same code developed on the local Ray installation
    can run on the entire spectrum of Ray’s installations. In this appendix, we will
    show some of the installation options that we evaluated while writing this book.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Ray 的强大之处在于它支持各种部署模型，从单节点部署——允许您在本地进行 Ray 实验——到包含数千台机器的集群。在本附录中，我们将展示编写本书时评估的一些安装选项。
- en: Installing Ray Locally
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在本地安装 Ray
- en: 'The simplest Ray installation is done locally with `pip`. Use the following
    command:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的 Ray 安装是使用 `pip` 在本地进行的。使用以下命令：
- en: '[PRE0]'
  id: totrans-4
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This command installs all the code required to run local Ray programs or launch
    programs on a Ray cluster (see [“Using Ray Clusters”](#sec-using-ray-clusters)).
    The command installs the latest official release. In addition, it is possible
    to install Ray from [daily releases](https://oreil.ly/2VzQD) or a [specific commit](https://oreil.ly/f9k7H).
    It is also possible to install Ray inside the [Conda environment](https://oreil.ly/1TsIZ).
    Finally, you can build Ray from the source by following the instructions in the
    [Ray documentation](https://oreil.ly/rjane).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令安装了运行本地 Ray 程序或在 Ray 集群上启动程序所需的所有代码（参见 [“使用 Ray 集群”](#sec-using-ray-clusters)）。该命令安装了最新的官方发布版本。此外，还可以从
    [每日发布](https://oreil.ly/2VzQD) 或 [特定提交](https://oreil.ly/f9k7H) 安装 Ray。还可以在 [Conda
    环境](https://oreil.ly/1TsIZ) 中安装 Ray。最后，您可以按照 [Ray 文档](https://oreil.ly/rjane)
    中的说明从源代码构建 Ray。
- en: Using Ray Docker Images
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Ray Docker 镜像
- en: 'In addition to natively installing on your local machine, Ray provides an option
    for running the provided [Docker image](https://oreil.ly/zrvoq). The Ray project
    provides a wealth of [Docker images](https://oreil.ly/0qv77) built for various
    versions of Python and hardware options. These images can be used to execute Ray’s
    code by starting a corresponding Ray image:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在本地机器上进行本地安装外，Ray 还提供了通过运行提供的 [Docker 镜像](https://oreil.ly/zrvoq) 的选项。Ray
    项目提供了丰富的 [Docker 镜像](https://oreil.ly/0qv77)，适用于各种 Python 版本和硬件选项。这些镜像可以用来通过启动相应的
    Ray 镜像来执行 Ray 的代码：
- en: '[PRE1]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here `<*shm-size*>` is the memory that Ray uses internally for its object store.
    A good estimate for this value is to use roughly 30% of your available memory;
    `<*image name*>` is the name of the image used.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里 `<*shm-size*>` 是 Ray 内部用于对象存储的内存大小。对于此值的一个良好估计是使用您可用内存的大约 30%；`<*image name*>`
    是所使用的镜像的名称。
- en: Once this command is executed, you will get back a command-line prompt and can
    enter any Ray code.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此命令后，您将收到一个命令行提示符，并可以输入任何 Ray 代码。
- en: Using Ray Clusters
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Ray 集群
- en: Although a local Ray installation is extremely useful for experimenting and
    initial debugging, the real power of Ray is its ability to run and scale on clusters
    of machines.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本地 Ray 安装对于实验和初始调试非常有用，但 Ray 的真正强大之处在于其能够在机器集群上运行和扩展。
- en: Ray *cluster nodes* are logical nodes based on Docker images. Docker images
    provided by the Ray project contain all the code required for running logical
    nodes, but not necessarily all the code required to run user applications. The
    issue here is that the user’s code might need specific Python libraries, which
    are not part of Ray’s Docker images.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Ray *集群节点*是基于 Docker 镜像的逻辑节点。Ray 项目提供的 Docker 镜像包含了运行逻辑节点所需的所有代码，但不一定包含运行用户应用程序所需的所有代码。问题在于用户的代码可能需要特定的
    Python 库，而这些库并不包含在 Ray 的 Docker 镜像中。
- en: To overcome this problem, Ray allows the installation of specific libraries
    to the nodes as part of the cluster installation, which is great for initial testing
    but can significantly impact the node’s creation performance. As a result, in
    production installs, it is typically recommended to use custom images derived
    from Ray-provided ones and add required libraries.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，Ray 允许在集群安装的一部分中向节点安装特定库，这对于初始测试非常有帮助，但可能会显著影响节点的创建性能。因此，在生产安装中，通常建议使用从
    Ray 提供的自定义镜像派生的镜像并添加所需的库。
- en: 'Ray provides two main options for installation: installation directly on the
    hardware nodes or cloud provider’s VMs and installation on Kubernetes. Here we
    will discuss Ray’s installation on cloud providers and Kubernetes. For information
    on Ray’s installation on hardware nodes, refer to the [Ray documentation](https://oreil.ly/3hYV0).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Ray提供了两种主要的安装选项：直接安装在硬件节点或云提供商的VM上，以及在Kubernetes上安装。在这里，我们将讨论Ray在云提供商和Kubernetes上的安装。有关Ray在硬件节点上的安装信息，请参阅[Ray文档](https://oreil.ly/3hYV0)。
- en: The official [documentation](https://oreil.ly/mrThY) describes Ray’s installation
    on several cloud providers, including AWS, Azure, Google Cloud, Alibaba, and custom
    clouds. Here we will discuss installation on AWS (as it is the most popular) and
    IBM Cloud (as one of the coauthors works at IBM, which takes a unique approach).^([1](app02.html#idm45354756650320))
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 官方[文档](https://oreil.ly/mrThY)描述了Ray在包括AWS、Azure、Google Cloud、阿里巴巴和自定义云在内的多个云提供商上的安装。在这里，我们将讨论在AWS上的安装（因为它是最流行的）和IBM
    Cloud上的安装（因为其中一位合著者在IBM工作，采用了独特的方法）。^([1](app02.html#idm45354756650320))
- en: Installing Ray on AWS
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在AWS上安装Ray
- en: AWS cloud installation leverages the Boto3 AWS SDK for Python and requires configuring
    your AWS credentials in the *~/.aws/credentials* file.^([2](app02.html#idm45354756645408))
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: AWS云安装利用了Python的Boto3 AWS SDK，并且需要在*~/.aws/credentials*文件中配置您的AWS凭证。^([2](app02.html#idm45354756645408))
- en: 'Once the credentials are created and Boto3 is installed, you can use the [*ray-aws.yaml*
    file](https://oreil.ly/zkodJ), which was adapted from the [Ray GitHub repository](https://oreil.ly/h0UnW),
    to install Ray on AWS via the following command:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦凭证创建并安装了Boto3，您可以使用从[Ray GitHub存储库](https://oreil.ly/h0UnW)适配的[*ray-aws.yaml*文件](https://oreil.ly/zkodJ)，通过以下命令在AWS上安装Ray：
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This command creates the cluster. It also provides a set of useful commands
    that you can use:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令创建了集群，并提供了一组您可以使用的有用命令：
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that the IP addresses that you’ll see will be different from those shown
    here. When the cluster is created, it uses a firewall that allows only a Secure
    Shell (SSH) connection to the cluster. If you want to access the cluster’s dashboard,
    you need to open port 8265, and for gRPC access, use port 10001\. To do this,
    find your node in the Amazon Elastic Compute Cloud (EC2) dashboard, click the
    Security tab, choose the security group, and modify the inbound rules. [Figure B-1](#fig-appb-1)
    shows a new rule allowing any instance port access from anywhere. For more information
    on inbound rule configuration, refer to the [AWS documentation](https://oreil.ly/MRfib).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您将看到的IP地址与此处显示的不同。在创建集群时，它使用了一个只允许通过Secure Shell（SSH）连接到集群的防火墙。如果您想访问集群的仪表板，您需要打开8265端口；如果需要gRPC访问，请使用10001端口。为此，请在Amazon
    Elastic Compute Cloud（EC2）仪表板中找到您的节点，点击“安全组”选项卡，选择安全组，并修改入站规则。[Figure B-1](#fig-appb-1)显示了一个允许来自任何地方的实例端口访问的新规则。有关入站规则配置的更多信息，请参阅[AWS文档](https://oreil.ly/MRfib)。
- en: '![spwr ab01](assets/spwr_ab01.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![spwr ab01](assets/spwr_ab01.png)'
- en: Figure B-1\. Instances view in the AWS console
  id: totrans-25
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 B-1\. AWS控制台中的实例视图
- en: As requested by your YAML file, you can see only a head, and the worker nodes
    will be created to satisfy the execution requirements of submitted jobs. To verify
    that the cluster is running correctly, you can use the code in [*localPython.py*
    on GitHub](https://oreil.ly/OzOQN), which verifies that it can connect to the
    cluster and its nodes.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 按照您的YAML文件的请求，您只能看到一个头部，并且工作节点将被创建以满足提交作业的执行要求。要验证集群是否正常运行，您可以使用GitHub上[*localPython.py*](https://oreil.ly/OzOQN)中的代码，该代码验证它是否可以连接到集群及其节点。
- en: An alternative approach to using Docker images for installation is [installing
    Ray directly on a VM](https://oreil.ly/k733p). The advantage of this approach
    is the ability to easily add additional software to the VM, which can be useful
    in real life. An obvious use case is managing Python libraries. You can do this
    with Docker-based installation, but you will then need to build Docker images
    for each library configuration. In the VM-based approach, there is no need to
    create and manage Docker images; just do appropriate `pip` installs. Additionally,
    you can install applications on VMs to leverage them in the Ray execution (see
    [“Wrapping Custom Programs with Ray”](ch12.html#sec-wrapping-custom-programs)).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 使用VM直接安装Ray的替代方法是[直接在VM上安装Ray](https://oreil.ly/k733p)。这种方法的优势在于能够轻松地向VM添加附加软件，这在实际中非常有用。一个明显的用例是管理Python库。您可以使用基于Docker的安装来做到这一点，但随后需要为每个库配置构建Docker镜像。在基于VM的方法中，无需创建和管理Docker镜像；只需适当地使用`pip`进行安装即可。此外，您还可以在VM上安装应用程序，以便在Ray执行中利用它们（参见[“使用Ray包装自定义程序”](ch12.html#sec-wrapping-custom-programs)）。
- en: Tip
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Installing Ray on a VM requires a lot of setup commands, and as a result, it
    can take a significant amount of time for the Ray node to start. A recommended
    approach is to start the Ray cluster once, create a new image, and then use this
    image and remove additional setup commands.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在VM上安装Ray需要大量的设置命令，因此Ray节点启动可能需要相当长的时间。推荐的方法是先启动Ray集群一次，创建一个新镜像，然后使用此镜像并移除额外的设置命令。
- en: Installing Ray on IBM Cloud
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在IBM Cloud上安装Ray
- en: 'IBM Cloud installation is based on the [Gen2 connector](https://oreil.ly/tIF6Y)
    that enables the Ray cluster to be deployed on IBM’s Gen2 cloud infrastructure.
    As with Ray on AWS, you’ll start with creating the cluster specification in a
    YAML file. You can use Lithopscloud to do this interactively if you don’t want
    to manually create the YAML file. You install Lithopscloud with `pip` as normal:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: IBM Cloud安装基于[Gen2连接器](https://oreil.ly/tIF6Y)，该连接器使Ray集群可以部署在IBM的Gen2云基础设施上。与在AWS上使用Ray一样，您将首先在YAML文件中创建集群规范。如果您不想手动创建YAML文件，可以交互式地使用Lithopscloud完成此操作。您可以像平常一样使用`pip`安装Lithopscloud：
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: To use Lithopscloud, you first need to either create an [API key](https://oreil.ly/ZO9Nv)
    or reuse the existing one. With your API key, you can run `lithopscloud -o cluster.yaml`
    to generate a *cluster.yaml* file. Once you start Lithopscloud, follow the questions
    to generate a file (you’ll need to use the up and down arrows to make your selections).
    You can find an example of the generated file on [GitHub](https://oreil.ly/rQNOx).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用Lithopscloud，您首先需要创建一个[API密钥](https://oreil.ly/ZO9Nv)或重用现有的密钥。有了您的API密钥，您可以运行
    `lithopscloud -o cluster.yaml` 来生成一个*cluster.yaml*文件。启动Lithopscloud后，按照提示生成文件（您需要使用上下箭头进行选择）。您可以在[GitHub](https://oreil.ly/rQNOx)上找到生成文件的示例。
- en: 'The limitation of the autogenerated file is that it uses the same image type
    for both head and worker nodes, which is not always ideal. You often may want
    to provide different types for these nodes. To do this, you can modify the autogenerated
    [*cluster.yaml* file](https://oreil.ly/LqpIl) as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成文件的限制在于它对头节点和工作节点使用相同的镜像类型，这并不总是理想的。通常情况下，您可能希望为这些节点提供不同的类型。要做到这一点，您可以修改自动生成的[*cluster.yaml*文件](https://oreil.ly/LqpIl)如下：
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here you define two types of nodes: the default head node and default worker
    node (you can define multiple worker node types with a max number of workers per
    time). Therefore, you can now have a relatively small head node (running all the
    time) and much larger worker nodes that will be created just in time.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，您定义了两种类型的节点：默认的头节点和默认的工作节点（您可以定义多个工作节点类型，并设置每次的最大工作节点数）。因此，您现在可以拥有一个相对较小的头节点（始终运行），以及会根据需要创建的更大的工作节点。
- en: Tip
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If you take a look at the generated YAML file, you will notice that it has a
    lot of setup commands, and as a result, it can take a significant amount of time
    for the Ray node to start. A recommended approach is to start the Ray cluster
    once, create a new image, and then use this image and remove the setup commands.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您查看生成的YAML文件，您会注意到它包含许多设置命令，因此Ray节点启动可能需要相当长的时间。推荐的方法是先启动Ray集群一次，创建一个新镜像，然后使用此镜像并移除设置命令。
- en: Once the YAML file is generated, you can install Gen2-connector to be able to
    use it. Run `pip3 install gen2-connector`. You can then create your cluster by
    running `ray up cluster.yaml`.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 生成YAML文件后，您可以安装Gen2连接器以便使用它。运行 `pip3 install gen2-connector`。然后，通过运行 `ray up
    cluster.yaml` 来创建您的集群。
- en: 'Similar to installing Ray on AWS, this installation displays a list of useful
    commands:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于在 AWS 上安装 Ray，此安装显示了一系列有用的命令：
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: To be able to access the cluster, be sure to open the required ports following
    [IBM Cloud documentation](https://oreil.ly/8oTDR) ([Figure B-2](#fig-appB-2)).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问集群，请确保按照[IBM Cloud 文档](https://oreil.ly/8oTDR)（[图 B-2](#fig-appB-2)）开放所需端口。
- en: '![spwr ab02](assets/spwr_ab02.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![spwr ab02](assets/spwr_ab02.png)'
- en: Figure B-2\. IBM Cloud console displaying firewall rules
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 B-2\. IBM Cloud 控制台显示防火墙规则
- en: As requested by your YAML file, you can see only a head; the worker nodes will
    be created to satisfy the execution requirements of submitted jobs. To verify
    that the cluster is running correctly, execute the [*localPython.py* script](https://oreil.ly/rl5SL).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的 YAML 文件的请求，您只能看到一个头部；工作节点将被创建以满足提交作业的执行需求。要验证集群是否正确运行，请执行 [*localPython.py*
    脚本](https://oreil.ly/rl5SL)。
- en: Installing Ray on Kubernetes
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Kubernetes 上安装 Ray
- en: 'When it comes to the actual cluster’s installation on Kubernetes, Ray provides
    two basic mechanisms:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际集群在 Kubernetes 上的安装中，Ray 提供了两种基本机制：
- en: Cluster launcher
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 集群启动器
- en: Similar to installation using VMs, this makes it simple to deploy a Ray cluster
    on any cloud. It will provision a new instance or machine using the cloud provider’s
    SDK, execute shell commands to set up Ray with the provided options, and initialize
    the cluster.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用虚拟机进行安装类似，这使得在任何云上部署 Ray 集群变得简单。它将使用云提供商的 SDK 创建新的实例或机器，执行 shell 命令以使用提供的选项设置
    Ray，并初始化集群。
- en: Ray Kubernetes operator
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Ray Kubernetes 操作器
- en: This facilitates deploying Ray on an existing Kubernetes cluster. The operator
    defines a [custom resource](https://oreil.ly/RTWR9) called a `RayCluster`, which
    describes the desired state of the Ray cluster, and a [custom controller](https://oreil.ly/ADp7y),
    the Ray Operator, which processes RayCluster resources and manages the Ray cluster.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这简化了在现有 Kubernetes 集群上部署 Ray 的过程。操作器定义了一个称为 `RayCluster` 的[自定义资源](https://oreil.ly/RTWR9)，它描述了
    Ray 集群的期望状态，以及一个[自定义控制器](https://oreil.ly/ADp7y)，即 Ray 操作器，它处理 RayCluster 资源并管理
    Ray 集群。
- en: Tip
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: When you install Ray on a Kubernetes cluster by using both the cluster launcher
    and operator, Ray uses Kubernetes capabilities to create a new Ray node in the
    form of Kubernetes pod. Although the Ray autoscaler works the same way, it effectively
    “steals” resources from the Kubernetes cluster. Therefore, your Kubernetes cluster
    has to either be large enough to support all of Ray’s resource requirements or
    provide its own autoscaling mechanism. In addition, because Ray’s nodes are in
    this case implemented as underlying Kubernetes pods, the Kubernetes resource manager
    can delete these pods at any time to obtain additional resources.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当您使用集群启动器和操作器在 Kubernetes 集群上安装 Ray 时，Ray 利用 Kubernetes 的能力创建一个新的 Ray 节点，形式为
    Kubernetes Pod。虽然 Ray 自动扩展器的工作方式相同，但它有效地从 Kubernetes 集群中“窃取”资源。因此，您的 Kubernetes
    集群要么足够大以支持 Ray 的所有资源需求，要么提供自己的自动缩放机制。此外，由于 Ray 的节点在这种情况下是作为底层 Kubernetes Pod 实现的，因此
    Kubernetes 资源管理器可以随时删除这些 Pod 以获取额外的资源。
- en: Installing Ray on a kind Cluster
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 kind 集群上安装 Ray
- en: 'To demonstrate both approaches, let’s start by installing and accessing the
    Ray cluster on a [kind (Kubernetes in Docker) cluster](https://oreil.ly/qvuAi).
    This popular tool runs local Kubernetes clusters by using Docker container “nodes”
    and is often used for local development. To do this, you need to create a cluster
    first by running the following command:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示两种方法，让我们首先在 [kind (Kubernetes in Docker) 集群](https://oreil.ly/qvuAi) 上安装并访问
    Ray 集群。这个流行的工具通过使用 Docker 容器“节点”来运行本地 Kubernetes 集群，并且经常用于本地开发。为此，您需要首先通过运行以下命令来创建一个集群：
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This creates a cluster with a default configuration. To modify the configuration,
    refer to the [configuration documentation](https://oreil.ly/Rvq54). Once the cluster
    is up and running, you can use either `ray up` or the Kubernetes operator to create
    a Ray cluster.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使用默认配置创建一个集群。要修改配置，请参考[配置文档](https://oreil.ly/Rvq54)。一旦集群启动运行，您可以使用 `ray up`
    或 Kubernetes 操作器来创建 Ray 集群。
- en: Using ray up
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 ray up
- en: 'To create a Ray cluster by using `ray up`, you must specify the resource requirements
    in a YAML file, such as [*raycluster.yaml*](https://oreil.ly/YGOp5), which was
    adapted from the Ray Kubernetes autoscaler defaults in the [Ray GitHub repository](https://oreil.ly/m2mm2).
    This file contains all the information required to create the Ray cluster:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 `ray up` 创建 Ray 集群，必须在一个 YAML 文件中指定资源需求，例如[*raycluster.yaml*](https://oreil.ly/YGOp5)，该文件改编自
    [Ray GitHub 仓库](https://oreil.ly/m2mm2) 中的 Ray Kubernetes 自动缩放器默认值。该文件包含创建 Ray
    集群所需的所有信息：
- en: General information about the cluster name and autoscaling parameters.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于集群名称和自动缩放参数的一般信息。
- en: Information about the cluster provider (Kubernetes, in our case), including
    provider-specific information required for the creation of Ray cluster’s nodes.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于集群提供程序（在我们的情况下是 Kubernetes）的信息，包括创建 Ray 集群节点所需的特定于提供程序的信息。
- en: Node-specific information (CPU/memory, etc). This also includes a list of node
    startup commands, including Python libraries required for the installation.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特定于节点的信息（CPU/内存等）。这还包括节点启动命令列表，包括安装所需的 Python 库。
- en: 'With this file in place, a command to create a cluster looks like this:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个文件，创建集群的命令看起来像这样：
- en: '[PRE8]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Once the cluster creation completes, you can see that several pods are running:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 完成集群创建后，您可以看到有几个 pod 在运行：
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As requested by our YAML file, you can see one head and two worker nodes. To
    verify that the cluster is running correctly, you can use the following [job](https://oreil.ly/swESN):'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的 YAML 文件的请求，您可以看到一个 head 和两个 worker 节点。要验证集群是否正常运行，可以使用以下 [作业](https://oreil.ly/swESN)：
- en: '[PRE10]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The execution results in something similar to this:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 执行的结果类似于这样：
- en: '[PRE11]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Once your job is up, you can additionally port-forward the `ray-ray-head` service
    by running the following:^([3](app02.html#idm45354756330752))
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 作业启动后，您还可以通过运行以下命令将 `ray-ray-head` 服务进行端口转发：^([3](app02.html#idm45354756330752))
- en: '[PRE12]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Then, connect to it from your local machine by using the [*localPython.py* testing
    script from the book’s example files](https://oreil.ly/RV8yx). Execution of this
    code produces the same results as shown previously.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用书籍示例文件中的[*localPython.py*测试脚本](https://oreil.ly/RV8yx)从本地机器连接到它。执行此代码会产生与先前显示的相同结果。
- en: 'Additionally, you can port-forward ray service to port 8265 to look at the
    Ray dashboard:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您可以将 ray 服务端口转发到 8265 端口以查看 Ray 仪表板：
- en: '[PRE13]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Once this is done, you can take a look at the Ray dashboard ([Figure B-3](#fig-appB-3)).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，您可以查看 Ray 仪表板（[图 B-3](#fig-appB-3)）。
- en: '![spwr ab03](assets/spwr_ab03.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![spwr ab03](assets/spwr_ab03.png)'
- en: Figure B-3\. Ray dashboard
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 B-3\. Ray 仪表板
- en: You can uninstall the Ray cluster by using the following command:^([4](app02.html#idm45354756317264))
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令可以卸载 Ray 集群：^([4](app02.html#idm45354756317264))
- en: '[PRE14]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Using the Ray Kubernetes Operator
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Ray Kubernetes 操作员
- en: For deployment to the Kubernetes cluster, we can also use the Ray operator,
    which is a recommended approach. To simplify usage of the operator, Ray provides
    a [Helm chart](https://oreil.ly/4jjfI) available as part of the Ray GitHub repository.
    Here, instead of the Helm chart, we are using several YAML files to deploy Ray
    to make installation a bit simpler.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 对于部署到 Kubernetes 集群，我们还可以使用 Ray 操作员，这是一种推荐的方法。为了简化操作员的使用，Ray 提供了作为 Ray GitHub
    仓库的一部分可用的[Helm 图表](https://oreil.ly/4jjfI)。在这里，我们使用多个 YAML 文件来部署 Ray，以使安装变得更加简单，而不是使用
    Helm 图表。
- en: 'Our deployment is split into three files: [*operatorcrd.yaml*](https://oreil.ly/wORyi),
    containing all the commands for CustomResourceDefinition (CRD) creation; [*operator.yaml*](https://oreil.ly/RyjD7),
    containing all the commands for operator creation; and [*rayoperatorcluster.yaml*](https://oreil.ly/Ibqbn),
    containing all the commands for cluster creation. It is assumed in these files
    that the operator is created in the namespace *ray*.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的部署分为三个文件：[*operatorcrd.yaml*](https://oreil.ly/wORyi)，其中包含用于 CustomResourceDefinition（CRD）创建的所有命令；[*operator.yaml*](https://oreil.ly/RyjD7)，其中包含用于操作员创建的所有命令；以及[*rayoperatorcluster.yaml*](https://oreil.ly/Ibqbn)，其中包含用于集群创建的所有命令。这些文件假定操作员是在
    *ray* 命名空间中创建的。
- en: 'To install the operator itself, we need to execute these two commands:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装操作员本身，我们需要执行这两个命令：
- en: '[PRE15]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Once this is done, use the following command to ensure that the operator pod
    is running:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，请使用以下命令确保操作员 pod 正在运行：
- en: '[PRE16]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Once the operator is up and running, you can start the cluster itself by using
    the following command:^([5](app02.html#idm45354756301680))
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦操作员启动并运行，您可以使用以下命令启动集群本身：^([5](app02.html#idm45354756301680))
- en: '[PRE17]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Here the content of *rayoperatorcluster.yaml* is similar to the content of *raycluster.yaml*
    but formatted slightly differently. Once the cluster is up and running, you can
    use the same validation code as described previously for `ray up`.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*rayoperatorcluster.yaml* 的内容与 *raycluster.yaml* 类似，但格式略有不同。一旦集群运行起来，你可以使用与之前描述的
    `ray up` 相同的验证代码。'
- en: Installing Ray on OpenShift
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 OpenShift 上安装 Ray
- en: OpenShift is a type of Kubernetes cluster, so theoretically the Kubernetes operator
    can be used to install Ray on an OpenShift cluster. Unfortunately, this installation
    is a little bit more involved.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift 是一种 Kubernetes 集群类型，因此理论上可以使用 Kubernetes 运算符在 OpenShift 集群上安装 Ray。不过，这种安装过程会稍微复杂一些。
- en: If you have ever used OpenShift, you know that by default all of the pods in
    OpenShift run in [restrictive mode](https://oreil.ly/ZkcDY). This mode denies
    access to all host features and requires pods to be run with a unique identifier
    (UID) and Security-Enhanced Linux (SELinux) context that are allocated to the
    namespace.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾经使用过 OpenShift，你会知道默认情况下，所有 OpenShift 中的 pod 都以 [限制模式](https://oreil.ly/ZkcDY)
    运行。此模式拒绝访问所有主机功能，并要求 pod 使用分配给命名空间的唯一标识符 (UID) 和安全增强型 Linux (SELinux) 上下文来运行。
- en: 'Unfortunately, this does not quite work for the Ray operator, designed to run
    as user 1000\. To enable this, you need to introduce several changes to the files
    that you used for installing on the kind (and any other plain Kubernetes cluster):'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，对于 Ray operator 来说，设计为以用户 1000 运行的情况不太适用。为了启用此功能，你需要对安装在 kind（以及任何其他纯 Kubernetes
    集群）上的文件进行几处更改：
- en: 'Add the `ray-operator-serviceaccount` service account, which is used by the
    operator, to `anyuid` mode. This allows users to run with any nonroot UID:'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加 `ray-operator-serviceaccount` 服务账户，该账户由运算符使用，设置为 `anyuid` 模式。这允许用户使用任何非根
    UID 运行：
- en: '[PRE18]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Modify [*operator.yaml*](https://oreil.ly/eYIht) to ensure that the operator
    pod is running as user 1000.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修改 [*operator.yaml*](https://oreil.ly/eYIht)，确保运算符 pod 以用户 1000 的身份运行。
- en: Additionally, a [testing job](https://oreil.ly/R2r8x) has to be modified slightly
    to run as user 1000\. This requires the creation of a `ray-node-serviceaccount`
    service account used for running a job and adding this service account to `anyuid`
    mode, which allows users to run with any nonroot UID.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，必须稍微修改 [测试作业](https://oreil.ly/R2r8x)，以用户 1000 的身份运行。这需要创建一个 `ray-node-serviceaccount`
    服务账户用于运行作业，并将该服务账户设置为 `anyuid` 模式，允许用户使用任何非根 UID 运行。
- en: Conclusion
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Ray provides a wealth of deployment options. When using Ray to solve a specific
    problem, you will need to decide which option is is most suitable for your specific
    situation.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Ray 提供丰富的部署选项。当使用 Ray 解决特定问题时，你需要决定哪个选项最适合你的具体情况。
- en: '^([1](app02.html#idm45354756650320-marker)) In the interest of transparency:
    Boris currently works at IBM, and Holden used to work at IBM. Holden has also
    worked for Google, Microsoft, and Amazon.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](app02.html#idm45354756650320-marker)) 为了透明起见：Boris 目前在 IBM 工作，Holden 曾在
    IBM 工作过。Holden 也曾在 Google、Microsoft 和 Amazon 工作过。
- en: ^([2](app02.html#idm45354756645408-marker)) See the [“Boto3 Docs 1.24.95” documentation](https://oreil.ly/5A6jE)
    for information on setting up Boto3 configuration.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](app02.html#idm45354756645408-marker)) 参见 [“Boto3 Docs 1.24.95” 文档](https://oreil.ly/5A6jE)
    获取有关设置 Boto3 配置的信息。
- en: ^([3](app02.html#idm45354756330752-marker)) Theoretically, you can also create
    an ingress to connect to the Ray cluster. Unfortunately, in the case of the NGINX
    ingress controller, it will not work. The issue here is that the Ray client is
    using unsecure gRPC, while the NGINX ingress controller supports only secure gRPC
    calls. When using the Ray cluster on a specific cloud, check whether an ingress
    supports unsecure gRPC before exposing Ray’s head service as an ingress.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](app02.html#idm45354756330752-marker)) 理论上，你也可以创建一个入口来连接 Ray 集群。但遗憾的是，在
    NGINX 入口控制器的情况下，它将无法工作。问题在于 Ray 客户端使用的是不安全的 gRPC，而 NGINX 入口控制器仅支持安全的 gRPC 调用。在使用
    Ray 集群时，请检查入口是否支持不安全的 gRPC，在将 Ray 的 head 服务公开为入口之前。
- en: ^([4](app02.html#idm45354756317264-marker)) This command deletes pods, and it
    leaves behind the service created as part of a cluster. You have to manually delete
    a service for a complete cleanup.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](app02.html#idm45354756317264-marker)) 此命令会删除 pod，并且会留下作为集群一部分创建的服务。你必须手动删除服务以进行完全清理。
- en: ^([5](app02.html#idm45354756301680-marker)) Although documentation mentions
    a cluster-wide deploy operator, it works only for a namespace where the operator
    is deployed.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](app02.html#idm45354756301680-marker)) 尽管文档提到了集群范围的部署运算符，但它仅适用于部署运算符的命名空间。
