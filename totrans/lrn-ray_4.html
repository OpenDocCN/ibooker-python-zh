<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 5. Hyperparameter Optimization with Ray Tune" data-type="chapter" epub:type="chapter"><div class="chapter" id="chapter_05">
<h1><span class="label">Chapter 5. </span>Hyperparameter Optimization with Ray Tune</h1>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm44990026016096">
<h5>A Note for Early Release Readers</h5>
<p>With Early Release ebooks, you get books in their earliest form—the author’s raw and unedited content as they write—so you can take advantage of these technologies long before the official release of these titles.</p>
</div></aside>
<p>In the last chapter we’ve seen how to build and run various reinforcement learning experiments.
Running such experiments can be expensive, both in terms of compute resources and the time it takes to run them.
This only gets amplified as you move on to more challenging tasks, since it’s unlikely to just pick an algorithm out of the box and run it to get a good result.
In other words, at some point you’ll need to tune the hyperparameters of your algorithms to get the best results.
As we’ll see in this chapter, tuning machine learning models is hard, but Ray Tune is an excellent choice to help you tackle this task.</p>
<p>Ray Tune is an incredibly powerful tool for hyperparameter optimization.
Not only does it work in a distributed manner by default, such as any other library built on top of Ray, it’s also one of the most feature-rich hyperparameter optimization (HPO) libraries available.
To top this off, Tune integrates with some of the most prominent HPO libraries out there, such as HyperOpt, Optuna, and many more.
This is remarkable, since it makes Tune an ideal candidate for distributed HPO experiments, practically no matter what other libraries you’re coming from or if you start from scratch.</p>
<p>In this chapter we’ll first revisit in a bit more depth why HPO is hard to do, and how you could naively implement it yourself with Ray.
We then teach you the core concepts of Ray Tune and how you can use it to tune the RLlib models we’ve built in the previous chapter.
To wrap things up, we’ll also have a look at how to use Tune for supervised learning tasks, using frameworks like PyTorch and TensorFlow.
Along the way, we demonstrate how Tune integrates with other HPO libraries and introduce you to some of its more advanced features.</p>
<section data-pdf-bookmark="Tuning Hyperparameters" data-type="sect1"><div class="sect1" id="idm44990025984976">
<h1>Tuning Hyperparameters</h1>
<p>Let’s recap the basics of hyperparameter optimization briefly.
If you’re familiar with this topic, you can skip this section, but since we’re also discussing aspects of distributed HPO, you might still benefit from following along.
The <a href="https://github.com/maxpumperla/learning_ray/blob/main/notebooks/ch_05_tune.ipynb">notebook for this chapter</a> can be found in the GitHub repository of this book.</p>
<p>If you recall our first RL experiment introduced in <a data-type="xref" href="ch03.xhtml#chapter_03">Chapter 3</a>, we defined a very basic Q-learning algorithm whose internal <em>state-action values</em> were updated according to an explicit update rule.
After initialization, we never touched these <em>model parameters</em> directly, they were learnt by the algorithm.
By contrast, in setting up the algorithm, we explicitly chose a <code>weight</code> and a <code>discount_factor</code> parameter prior to training.
I didn’t tell you how we chose to set these parameters back then, we simply accepted the fact that they were good enough to crack the problem at hand.
In the same way, in <a data-type="xref" href="ch04.xhtml#chapter_04">Chapter 4</a> we initialized an RLlib algorithm with a <code>config</code> that used a total of four rollout workers for our DQN algorithm by setting <code>num_workers=4</code>.
Parameters like these are called <em>hyperparameters</em>, and finding good choices for them can be crucial for successful experiments.
The field of hyperparameter optimization entirely is devoted to efficiently finding such good choices.</p>
<section data-pdf-bookmark="Building a random search example with Ray" data-type="sect2"><div class="sect2" id="idm44990025977264">
<h2>Building a random search example with Ray</h2>
<p>Hyperparameters like the <code>weight</code> or the  <code>discount_factor</code> of our Q-learning algorithm
are <em>continuous</em> parameters, so we can’t possibly test all  combinations of them.
What’s more, these parameter choices may not be independent of each other.
If we want them to be selected for us, we also need to specify a <em>value range</em> for each of them (both hyperparameters need to be chosen between 0 and 1 in this case).
So, how do we determine good or even optimal hyperparameters?</p>
<p>Let’s take a look at a quick example that implements a naive, yet effective approach to tuning hyperparameters.
This example will also allow us to introduce some terminology that we’ll use later on.
The core idea is that we can attempt to <em>randomly sample</em> hyperparameters, run the algorithm for each sample, and then select the best run based on the results we got.
But to do the theme of this book justice, we don’t just want to run this in a sequential loop, we want to compute our runs in parallel using Ray.</p>
<p>To keep things simple we’ll revisit our simple Q-learning algorithm from <a data-type="xref" href="ch03.xhtml#chapter_03">Chapter 3</a> again.
If you don’t recall the signature of the main training function, we defined it as
<code>train_policy(env, num_episodes=10000, weight=0.1, discount_factor=0.9)</code>.
That means we can tune the <code>weight</code> and <code>discount_factor</code> parameters of our algorithm by passing in different values to the <code>train_policy</code> function and see how the algorithm performs.
To do that, let’s define a so-called <em>search space</em> for our hyperparameters.
For both parameters in question we simply uniformly sample values between 0 and 1,
for a total of 10 choices.
Here’s what that looks like:</p>
<div data-type="example">
<h5><span class="label">Example 5-1. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">random</code>
<code class="n">search_space</code> <code class="o">=</code> <code class="p">[]</code>
<code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">10</code><code class="p">):</code>
    <code class="n">random_choice</code> <code class="o">=</code> <code class="p">{</code>
        <code class="s1">'weight'</code><code class="p">:</code> <code class="n">random</code><code class="o">.</code><code class="n">uniform</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">),</code>
        <code class="s1">'discount_factor'</code><code class="p">:</code> <code class="n">random</code><code class="o">.</code><code class="n">uniform</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>
    <code class="p">}</code>
    <code class="n">search_space</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">random_choice</code><code class="p">)</code></pre></div>
<p>Next, we define an <em>objective function</em>, or simply <em>objective</em>.
The role of an objective function is to evaluate the performance of a given set of hyperparameters for the task we’re interested in.
In our case, we want to train our RL algorithm and evaluate the trained policy.
Recall that in <a data-type="xref" href="ch03.xhtml#chapter_03">Chapter 3</a>&gt; we also defined an <code>evaluate_policy</code> function for precisely this purpose.
The <code>evaluate_policy</code> function was defined to return the average number of steps it took for an agent to reach the goal in the underlying maze environment.
In other words, we want to find a set of hyperparameters that minimizes the result of our objective function.
To parallelize the objective function, we’ll use the <code>ray.remote</code> decorator to make our <code>objective</code> a Ray task.</p>
<div data-type="example">
<h5><span class="label">Example 5-2. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code><code> </code><code class="nn">ray</code><code>
</code><code>
</code><code>
</code><code class="nd">@ray</code><code class="o">.</code><code class="n">remote</code><code>
</code><code class="k">def</code><code> </code><code class="nf">objective</code><code class="p">(</code><code class="n">config</code><code class="p">)</code><code class="p">:</code><code>  </code><a class="co" href="#callout_hyperparameter_optimization_with_ray_tune_CO1-1" id="co_hyperparameter_optimization_with_ray_tune_CO1-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code>
</code><code>    </code><code class="n">environment</code><code> </code><code class="o">=</code><code> </code><code class="n">Environment</code><code class="p">(</code><code class="p">)</code><code>
</code><code>    </code><code class="n">policy</code><code> </code><code class="o">=</code><code> </code><code class="n">train_policy</code><code class="p">(</code><code>  </code><a class="co" href="#callout_hyperparameter_optimization_with_ray_tune_CO1-2" id="co_hyperparameter_optimization_with_ray_tune_CO1-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a><code>
</code><code>        </code><code class="n">environment</code><code class="p">,</code><code> </code><code class="n">weight</code><code class="o">=</code><code class="n">config</code><code class="p">[</code><code class="s2">"</code><code class="s2">weight</code><code class="s2">"</code><code class="p">]</code><code class="p">,</code><code> </code><code class="n">discount_factor</code><code class="o">=</code><code class="n">config</code><code class="p">[</code><code class="s2">"</code><code class="s2">discount_factor</code><code class="s2">"</code><code class="p">]</code><code>
</code><code>    </code><code class="p">)</code><code>
</code><code>    </code><code class="n">score</code><code> </code><code class="o">=</code><code> </code><code class="n">evaluate_policy</code><code class="p">(</code><code class="n">environment</code><code class="p">,</code><code> </code><code class="n">policy</code><code class="p">)</code><code>  </code><a class="co" href="#callout_hyperparameter_optimization_with_ray_tune_CO1-3" id="co_hyperparameter_optimization_with_ray_tune_CO1-3"><img alt="3" height="12" src="assets/3.png" width="12"/></a><code>
</code><code>    </code><code class="k">return</code><code> </code><code class="p">[</code><code class="n">score</code><code class="p">,</code><code> </code><code class="n">config</code><code class="p">]</code><code>  </code><a class="co" href="#callout_hyperparameter_optimization_with_ray_tune_CO1-4" id="co_hyperparameter_optimization_with_ray_tune_CO1-4"><img alt="4" height="12" src="assets/4.png" width="12"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_hyperparameter_optimization_with_ray_tune_CO1-1" id="callout_hyperparameter_optimization_with_ray_tune_CO1-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>We pass in a dictionary with a hyperparameter sample into our objective.</p></dd>
<dt><a class="co" href="#co_hyperparameter_optimization_with_ray_tune_CO1-2" id="callout_hyperparameter_optimization_with_ray_tune_CO1-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a></dt>
<dd><p>Then we train our RL policy using the chosen hyperparameters.</p></dd>
<dt><a class="co" href="#co_hyperparameter_optimization_with_ray_tune_CO1-3" id="callout_hyperparameter_optimization_with_ray_tune_CO1-3"><img alt="3" height="12" src="assets/3.png" width="12"/></a></dt>
<dd><p>Afterwards we can evaluate the policy to retrieve the score we want to minimize.</p></dd>
<dt><a class="co" href="#co_hyperparameter_optimization_with_ray_tune_CO1-4" id="callout_hyperparameter_optimization_with_ray_tune_CO1-4"><img alt="4" height="12" src="assets/4.png" width="12"/></a></dt>
<dd><p>We return both score and hyperparameter choice together for later analysis.</p></dd>
</dl></div>
<p>Finally, we can run the objective function in parallel using Ray,
by iterating over the search space and collecting the results:</p>
<div data-type="example">
<h5><span class="label">Example 5-3. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="n">result_objects</code> <code class="o">=</code> <code class="p">[</code><code class="n">objective</code><code class="o">.</code><code class="n">remote</code><code class="p">(</code><code class="n">choice</code><code class="p">)</code> <code class="k">for</code> <code class="n">choice</code> <code class="ow">in</code> <code class="n">search_space</code><code class="p">]</code>
<code class="n">results</code> <code class="o">=</code> <code class="n">ray</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="n">result_objects</code><code class="p">)</code>

<code class="n">results</code><code class="o">.</code><code class="n">sort</code><code class="p">(</code><code class="n">key</code><code class="o">=</code><code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="n">x</code><code class="p">[</code><code class="mi">0</code><code class="p">])</code>
<code class="nb">print</code><code class="p">(</code><code class="n">results</code><code class="p">[</code><code class="o">-</code><code class="mi">1</code><code class="p">])</code></pre></div>
<p>The actual results of this hyperparameter run are not very interesting, as the problem is so easy to solve (most runs will return the optimum of 8 steps, regardless of the hyperparameters chosen).
But in case I haven’t sold you on Ray’s capabilities yet, what’s more interesting here is how easy it is to parallelize the objective function with Ray.
In fact, I’d like to encourage you to rewrite the above example to simply loop through the search space and call the objective function for each sample, just to confirm how painfully slow such a serial loop can be.</p>
<p>Conceptually, the three steps we took to run the above example are representative of how hyperparameter tuning works in general.
First, you define a search space, then you define an objective function, and finally you run an analysis to find the best hyperparameters.
In HPO it is common to speak of one evaluation of the objective function per hyperparameter sample as a <em>trial</em>, and all trials form the basis for your analysis.
How exactly parameters are sampled from the search space (in our case, randomly) is up to a <em>search algorithm</em> to decide.
In practice, finding good hyperparameters is easier said than done,
so let’s have a closer look at why this problem is so hard.</p>
</div></section>
<section data-pdf-bookmark="Why is HPO hard?" data-type="sect2"><div class="sect2" id="idm44990025976672">
<h2>Why is HPO hard?</h2>
<p>If you zoom out from the above example just enough, you can see that there are several intricacies in making the process of hyperparameter tuning work well.
Here’s a quick overview of the most important ones:</p>
<ul>
<li>
<p>Your search space can be composed of a large number of hyperparameters. These parameters might have different data types and ranges. Some parameters might be correlated, or even depend on others. Sampling good candidates from complex, high-dimensional spaces is a difficult task.</p>
</li>
<li>
<p>Picking parameters at random can work surprisingly well, but it’s not always the best option. In general, you need to test more complex search algorithms to find the best parameters.</p>
</li>
<li>
<p>In particular, even if you parallelize your hyperparameter search like we just did, a single run of your objective function can take a long time to complete. That means you can’t afford to run too many searches overall. For instance, training neural networks can take hours to complete, so your hyperparameter search better be efficient.</p>
</li>
<li>
<p>When distributing search, you need to make sure to have enough compute resources available to run searches over the objective function effectively. For instance, you might need a GPU to compute your objective function fast enough, so all your search runs need to have access to a GPU. Allocating the necessary resources for each trial is critical to speeding up your search.</p>
</li>
<li>
<p>You want to have convenient tooling for your HPO experiments, like stopping bad runs early, saving intermediate results, restarting from previous trials, or pausing and resuming runs, etc.</p>
</li>
</ul>
<p>As a mature, distributed HPO framework, Ray Tune takes addresses all these topics and provides you with a simple interface for running hyperparameter tuning experiments.
Before we look into how Tune works, let’s rewrite our above example to use Tune.</p>
</div></section>
</div></section>
<section data-pdf-bookmark="An introduction to Tune" data-type="sect1"><div class="sect1" id="idm44990025710448">
<h1>An introduction to Tune</h1>
<p>To get your first taste of Tune, porting over our naive Ray Core implementation of random search to Tune is straightforward and follows the same three steps as before.
First, we define a search space, but this time using <code>tune.uniform</code>, instead of the <code>random</code> library:</p>
<div data-type="example">
<h5><span class="label">Example 5-4. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">ray</code> <code class="kn">import</code> <code class="n">tune</code>


<code class="n">search_space</code> <code class="o">=</code> <code class="p">{</code>
    <code class="s2">"weight"</code><code class="p">:</code> <code class="n">tune</code><code class="o">.</code><code class="n">uniform</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">),</code>
    <code class="s2">"discount_factor"</code><code class="p">:</code> <code class="n">tune</code><code class="o">.</code><code class="n">uniform</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">),</code>
<code class="p">}</code></pre></div>
<p>Next, we can define an objective function that almost looks the same as before.
We designed it like that.
The only differences are that this time we return the score as a dictionary, and that we don’t need a <code>ray.remote</code> decorator, as Tune will take care of distributing this objective function for us internally.</p>
<div data-type="example">
<h5><span class="label">Example 5-5. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">tune_objective</code><code class="p">(</code><code class="n">config</code><code class="p">):</code>
    <code class="n">environment</code> <code class="o">=</code> <code class="n">Environment</code><code class="p">()</code>
    <code class="n">policy</code> <code class="o">=</code> <code class="n">train_policy</code><code class="p">(</code>
        <code class="n">environment</code><code class="p">,</code> <code class="n">weight</code><code class="o">=</code><code class="n">config</code><code class="p">[</code><code class="s2">"weight"</code><code class="p">],</code> <code class="n">discount_factor</code><code class="o">=</code><code class="n">config</code><code class="p">[</code><code class="s2">"discount_factor"</code><code class="p">]</code>
    <code class="p">)</code>
    <code class="n">score</code> <code class="o">=</code> <code class="n">evaluate_policy</code><code class="p">(</code><code class="n">environment</code><code class="p">,</code> <code class="n">policy</code><code class="p">)</code>

    <code class="k">return</code> <code class="p">{</code><code class="s2">"score"</code><code class="p">:</code> <code class="n">score</code><code class="p">}</code></pre></div>
<p>With this <code>tune_objective</code> function defined, we can pass is into a <code>tune.run</code> call,
together with the search space we defined.
By default, Tune will run random search for you, but you can also specify other search algorithms, as you will see soon.
Calling <code>tune.run</code> generates random search trials for your objective and returns an <code>analysis</code> object that contains information about the hyperparameter search.
We can get the best hyperparameters found by calling <code>get_best_config</code> and specifying the <code>metric</code> and <code>mode</code> arguments (we want to minimize our score):</p>
<div data-type="example">
<h5><span class="label">Example 5-6. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="n">analysis</code> <code class="o">=</code> <code class="n">tune</code><code class="o">.</code><code class="n">run</code><code class="p">(</code><code class="n">tune_objective</code><code class="p">,</code> <code class="n">config</code><code class="o">=</code><code class="n">search_space</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">analysis</code><code class="o">.</code><code class="n">get_best_config</code><code class="p">(</code><code class="n">metric</code><code class="o">=</code><code class="s2">"score"</code><code class="p">,</code> <code class="n">mode</code><code class="o">=</code><code class="s2">"min"</code><code class="p">))</code></pre></div>
<p>This quick example covers the very basics of Tune, but there’s a lot more to unpack.
The <code>tune.run</code> function is quite powerful and takes a lot of arguments for you to configure your runs.
To understand these different configuration options, we first need to introduce you to the key concepts of Tune.</p>
<section data-pdf-bookmark="How does Tune work?" data-type="sect2"><div class="sect2" id="idm44990025481040">
<h2>How does Tune work?</h2>
<p>To effectively work with Tune, you have to understand a total of six key concepts, four of which you have already used in the last example.
Here’s an informal overview of Ray Tune’s components and how you should think about them:</p>
<ul>
<li>
<p><em>Search spaces</em>: These spaces determine which parameters to select.
Search spaces define the range of values for each parameter, and how they should be sampled.
They are defined as dictionaries and use Tune’s sampling functions to specify valid hyperparameter values.
You have already seen <code>tune.uniform</code>, but <a href="https://docs.ray.io/en/latest/tune/api_docs/search_space.xhtml#tune-sample-docs">there are many more options to choose from</a>.</p>
</li>
<li>
<p><em>Trainables</em>: A <code>Trainable</code> is Tune’s formal representation of an objective you want to “tune”.
Tune has class-based API as well, but we will only use the function-based API in this book.
For us, a <code>Trainable</code> is a function with a single argument, a search space, which reports scores to Tune. The easiest way to do so is by returning a dictionary with the score you’re interested in.</p>
</li>
<li>
<p><em>Trials</em>: By triggering <code>tune.run(...)</code>, Tune will make sure to set up trials and schedule them for execution on your cluster.
A trial contains all necessary information about a single run of your objective, given a set of hyperparameters.</p>
</li>
<li>
<p><em>Analyses</em>: Completing a <code>tune.run</code> call returns an <code>ExperimentAnalysis</code> object, with the results of all trials. You can use this object to drill down into the results of your trials.</p>
</li>
<li>
<p><em>Search Algorithms</em>: Tune supports a large variety of search algorithms, which are at the core of how to tune your hyperparameters.
So far you’ve only implicitly encountered Tune’s default search algorithm, which randomly selects hyperparameters from the search space.</p>
</li>
<li>
<p><em>Schedulers</em>: The last, crucial component of a Tune experiment is that of a <em>scheduler</em>.
Schedulers plan and execute what the search algorithm selects.
By default, Tune schedules trials selected by your search algorithm on a first-in-first-out (FIFO) basis.
In practice, you can think of schedulers as a way to speed up your experiments, for instance by stopping unsuccessful trials early.</p>
</li>
</ul>
<p>Figure <a data-type="xref" href="#fig_tune">Figure 5-1</a> sums up these major components of Tune, and their relationship in one diagram:</p>
<figure><div class="figure" id="fig_tune">
<img alt="Tune" height="470" src="assets/tune_flow.png" width="1247"/>
<h6><span class="label">Figure 5-1. </span>The core components of Ray Tune.</h6>
</div></figure>
<p>Note that internally Tune runs are started on the driver process of your Ray cluster, which spawns several worker processes (using Ray actors) that execute individual trials of your HPO experiment.
Your trainables, defined on the driver, have to be sent to the workers, and trial results need to be communicated to the driver running <code>tune.run(...)</code>.</p>
<p>Search spaces, Trainables, trials, and analyses don’t need much additional explanation, and we’ll see more examples of each of those components in the rest of this chapter.
But search algorithms, or simply <em>searchers</em> for short, and schedulers need a bit more elaboration.</p>
<section data-pdf-bookmark="Search Algorithms" data-type="sect3"><div class="sect3" id="idm44990025462128">
<h3>Search Algorithms</h3>
<p>All advanced search algorithms provided by Tune, and the many third-party HPO libraries it integrates with, fall under the umbrella of <em>Bayesian Optimization</em>.
Unfortunately, going into the details of specific Bayesian search algorithms is far beyond the scope of this book.
The basic idea is that you update your beliefs about which hyperparameter ranges are worth exploring based on the results of your previous trials.
Techniques using this principle make more informed decisions and hence tend to be more efficient than independently sampling parameters (e.g. at random).</p>
<p>Apart from the basic random search we’ve seen already, and <em>grid search</em>, which picks hyperparameters from a predefined “grid” of choices, Tune offers a wide range of Bayesian optimization searchers.
For instance, Tune integrates with the popular HyperOpt and Optuna libraries, and you can use the popular TPE (Tree-structured Parzen Estimator) searcher with Tune through both of these libraries.
Not only that, Tune also integrates with tools such as Ax, BlendSearch, FLAML, Dragonfly, Scikit-Optimize, BayesianOptimization, HpBandSter, Nevergrad, ZOOpt, SigOpt, and HEBO.
If you need to run HPO experiments with any of these tools on a cluster, or want to easily switch between them, Tune is the way to go.</p>
<p>To make things more concrete, let’s rewrite our basic random search Tune example from earlier to use the <code>bayesian-optimization</code> library.
To do so, make sure you install this library in your Python environment first, e.g. with <code>pip install bayesian-optimization</code>.</p>
<div data-type="example">
<h5><span class="label">Example 5-7. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">ray.tune.suggest.bayesopt</code> <code class="kn">import</code> <code class="n">BayesOptSearch</code>


<code class="n">algo</code> <code class="o">=</code> <code class="n">BayesOptSearch</code><code class="p">(</code><code class="n">random_search_steps</code><code class="o">=</code><code class="mi">4</code><code class="p">)</code>

<code class="n">tune</code><code class="o">.</code><code class="n">run</code><code class="p">(</code>
    <code class="n">tune_objective</code><code class="p">,</code>
    <code class="n">config</code><code class="o">=</code><code class="n">search_space</code><code class="p">,</code>
    <code class="n">metric</code><code class="o">=</code><code class="s2">"score"</code><code class="p">,</code>
    <code class="n">mode</code><code class="o">=</code><code class="s2">"min"</code><code class="p">,</code>
    <code class="n">search_alg</code><code class="o">=</code><code class="n">algo</code><code class="p">,</code>
    <code class="n">stop</code><code class="o">=</code><code class="p">{</code><code class="s2">"training_iteration"</code><code class="p">:</code> <code class="mi">10</code><code class="p">},</code>
<code class="p">)</code></pre></div>
<p>Note that we “warm start” our Bayesian optimization with four random steps at the beginning,
and we explicitly <code>stop</code> the trial runs after 10 training iterations.</p>
<p>Note that since we’re not just randomly selecting parameters with <code>BayesOptSearch</code>, the
<code>search_alg</code> we use in our Tune run needs to know which <code>metric</code> to optimize for and whether
to minimize or optimize this metric.
As we’ve argued before, we want to achieve a <code>"min"</code> <code>"score"</code>.</p>
</div></section>
<section data-pdf-bookmark="Schedulers" data-type="sect3"><div class="sect3" id="idm44990025419104">
<h3>Schedulers</h3>
<p>Let’s next discuss how to use <em>trial schedulers</em> in Tune to make your runs more efficient.
We also use this section to introduce a slightly different way to report your metrics to Tune within an objective function.</p>
<p>So, let’s say that instead of computing a score straight-up, like we did in all examples in this chapter so far, we compute an <em>intermediate score</em> in a loop.
This is a situation that often occurs in supervised machine learning scenarios, when training a model for several iterations (we’ll see concrete applications of this later in this chapter).
With good hyperparameter choices selected, this immediate score might stagnate way before the loop in which it is computed.
In other words, if we don’t see enough incremental changes anymore, why not stop the trial early?
This is exactly one of the cases Tune’s schedulers are built for.</p>
<p>Here’s a quick example of such an objective function.
This is a toy example, but it will help us reason about the optimal hyperparameters we want Tune to find much better than if we were discussing a black-box scenario.</p>
<div data-type="example">
<h5><span class="label">Example 5-8. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code><code> </code><code class="nf">objective</code><code class="p">(</code><code class="n">config</code><code class="p">)</code><code class="p">:</code><code>
</code><code>    </code><code class="k">for</code><code> </code><code class="n">step</code><code> </code><code class="ow">in</code><code> </code><code class="nb">range</code><code class="p">(</code><code class="mi">30</code><code class="p">)</code><code class="p">:</code><code>  </code><a class="co" href="#callout_hyperparameter_optimization_with_ray_tune_CO2-1" id="co_hyperparameter_optimization_with_ray_tune_CO2-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code>
</code><code>        </code><code class="n">score</code><code> </code><code class="o">=</code><code> </code><code class="n">config</code><code class="p">[</code><code class="s2">"</code><code class="s2">weight</code><code class="s2">"</code><code class="p">]</code><code> </code><code class="o">*</code><code> </code><code class="p">(</code><code class="n">step</code><code> </code><code class="o">*</code><code class="o">*</code><code> </code><code class="mf">0.5</code><code class="p">)</code><code> </code><code class="o">+</code><code> </code><code class="n">config</code><code class="p">[</code><code class="s2">"</code><code class="s2">bias</code><code class="s2">"</code><code class="p">]</code><code>
</code><code>        </code><code class="n">tune</code><code class="o">.</code><code class="n">report</code><code class="p">(</code><code class="n">score</code><code class="o">=</code><code class="n">score</code><code class="p">)</code><code>  </code><a class="co" href="#callout_hyperparameter_optimization_with_ray_tune_CO2-2" id="co_hyperparameter_optimization_with_ray_tune_CO2-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a><code>
</code><code>
</code><code>
</code><code class="n">search_space</code><code> </code><code class="o">=</code><code> </code><code class="p">{</code><code class="s2">"</code><code class="s2">weight</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="n">tune</code><code class="o">.</code><code class="n">uniform</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code><code> </code><code class="mi">1</code><code class="p">)</code><code class="p">,</code><code> </code><code class="s2">"</code><code class="s2">bias</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="n">tune</code><code class="o">.</code><code class="n">uniform</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code><code> </code><code class="mi">1</code><code class="p">)</code><code class="p">}</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_hyperparameter_optimization_with_ray_tune_CO2-1" id="callout_hyperparameter_optimization_with_ray_tune_CO2-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>Often you may want to compute intermediate scores, e.g. in a “training loop”.</p></dd>
<dt><a class="co" href="#co_hyperparameter_optimization_with_ray_tune_CO2-2" id="callout_hyperparameter_optimization_with_ray_tune_CO2-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a></dt>
<dd><p>You can use <code>tune.report</code> to let Tune know about these intermediate scores.</p></dd>
</dl></div>
<p>The score we want to minimize here is the square root of a positive number times a <code>weight</code>, plus adding a <code>bias</code> term.
It’s clear that both of these hyperparameters need to be as small as possible to minimize the <code>score</code> for any positive <code>x</code>.
Given that the square root function “flattens out”, we might not have to compute all <code>30</code> passes through the loop to find sufficiently good values for our two hyperparameters.
If you imagine that each <code>score</code> computation took an hour, stopping early can be a huge boost to make your experiments run quicker.</p>
<p>Let’s illustrate this idea by using the popular Hyperband algorithm as our trial scheduler.
This scheduler needs to be passed a metric and mode (again, we <code>min</code>-imize our <code>score</code>).
We also make sure to run for 10 samples so as not to stop too prematurely:</p>
<div data-type="example">
<h5><span class="label">Example 5-9. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">ray.tune.schedulers</code> <code class="kn">import</code> <code class="n">HyperBandScheduler</code>


<code class="n">scheduler</code> <code class="o">=</code> <code class="n">HyperBandScheduler</code><code class="p">(</code><code class="n">metric</code><code class="o">=</code><code class="s2">"score"</code><code class="p">,</code> <code class="n">mode</code><code class="o">=</code><code class="s2">"min"</code><code class="p">)</code>


<code class="n">analysis</code> <code class="o">=</code> <code class="n">tune</code><code class="o">.</code><code class="n">run</code><code class="p">(</code>
    <code class="n">objective</code><code class="p">,</code>
    <code class="n">config</code><code class="o">=</code><code class="n">search_space</code><code class="p">,</code>
    <code class="n">scheduler</code><code class="o">=</code><code class="n">scheduler</code><code class="p">,</code>
    <code class="n">num_samples</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code>
<code class="p">)</code>

<code class="nb">print</code><code class="p">(</code><code class="n">analysis</code><code class="o">.</code><code class="n">get_best_config</code><code class="p">(</code><code class="n">metric</code><code class="o">=</code><code class="s2">"score"</code><code class="p">,</code> <code class="n">mode</code><code class="o">=</code><code class="s2">"min"</code><code class="p">))</code></pre></div>
<p>Note that in this case we did not specify a search algorithm, which means that Hyperband will run on parameters selected by random search.
We could also have <em>combined</em> this scheduler with another search algorithm instead.
This would have allowed us to pick better trial hyperparameters and stop bad trials early.
However, note that not every scheduler can be combined with search algorithms.
You’re advised to check <a href="https://docs.ray.io/en/latest/tune/key-concepts.xhtml#schedulers">Tune’s scheduler compatibility matrix</a> for more information.</p>
<p>To wrap this discussion up, apart from Hyperband Tune includes distributed implementations of early stopping algorithms such as the Median Stopping Rule, ASHA, Population Based Training (PBT) and Population Based Bandits (PB2).</p>
</div></section>
</div></section>
<section data-pdf-bookmark="Configuring and running Tune" data-type="sect2"><div class="sect2" id="idm44990025200032">
<h2>Configuring and running Tune</h2>
<p>Before looking into more concrete machine learning examples using Ray Tune, let’s dive into some useful topics that help you get more out of your Tune experiments, such as properly utilizing resources, stopping and resuming trials, adding callbacks to your Tune runs, or defining custom and conditional search spaces.</p>
<section data-pdf-bookmark="Specifying resources" data-type="sect3"><div class="sect3" id="idm44990025198320">
<h3>Specifying resources</h3>
<p>By default, each Tune trial will run on one CPU and leverage as many CPUs as available for concurrent trials.
For instance, if you run Tune on a laptop with 8 CPUs, any of the experiments we computed so far in this chapter will spawn 8 concurrent trials and allocate one CPU each for each trial.
Changing this behaviour can be controlled using the <code>resources_per_trial</code> argument of a Tune run.</p>
<p>What’s interesting is that this does not stop with CPUs, you can also determine the number of GPUs used per trial.
Plus, Tune allows you to use <em>fractional resources</em>, i.e., you can share resources between trials.
So, let’s say that you have a machine with 12 CPUs and two GPUs and you request the following resources for your <code>objective</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">ray</code> <code class="kn">import</code> <code class="n">tune</code>

<code class="n">tune</code><code class="o">.</code><code class="n">run</code><code class="p">(</code><code class="n">objective</code><code class="p">,</code> <code class="n">num_samples</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">resources_per_trial</code><code class="o">=</code><code class="p">{</code><code class="s2">"cpu"</code><code class="p">:</code> <code class="mi">2</code><code class="p">,</code> <code class="s2">"gpu"</code><code class="p">:</code> <code class="mf">0.5</code><code class="p">})</code></pre>
<p>That means Tune can schedule and execute up to four concurrent trials on your machine, as this would max out GPU utilization on this machine (while you’d still have 4 idle CPUs for other tasks).
If you want, you can also specify the amount of <code>"memory"</code> used by a trial by passing the number of bytes into <code>resources_per_trial</code>.
Also note that should you have the need to explicitly <em>restrict</em> the number of concurrent trials, you can do so by passing in the <code>max_concurrent_trials</code> parameter to your <code>tune.run(...)</code>.
In the above example, let’s say you want to always keep one GPU available for other tasks, you can limit the number of concurrent trials to two by setting <code>max_concurrent_trials = 2</code>.</p>
<p>Note that everything we just exemplified for resources on a single machine naturally extends to any Ray cluster and its available resources.
In any case, Ray will always try to schedule the next trials, but will wait and ensure enough resources are available before executing them.</p>
</div></section>
<section data-pdf-bookmark="Callbacks and Metrics" data-type="sect3"><div class="sect3" id="idm44990025140736">
<h3>Callbacks and Metrics</h3>
<p>If you’ve spent some time investigating the outputs of the Tune runs we’ve started in this chapter so far, you’ll have noticed that each trial comes equipped with a lot of information by default, such as the trial ID, its execution date, and much more.
What’s interesting is that Tune not only allows you to customize the metrics you want to report, you can also hook into a <code>tune.run</code> by providing <em>callbacks</em>.
Let’s compute a quick, representative example that does both.</p>
<p>Slightly modifying a previous example, let’s say we want to log a specific message whenever a trial returns a result.
To do so, all you need to do is implement the <code>on_trial_result</code> method on a <code>Callback</code> object from the <code>ray.tune</code> package.
Here’s how that would look like for an objective function that reports a <code>score</code>:</p>
<div data-type="example">
<h5><span class="label">Example 5-10. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">ray</code> <code class="kn">import</code> <code class="n">tune</code>
<code class="kn">from</code> <code class="nn">ray.tune</code> <code class="kn">import</code> <code class="n">Callback</code>
<code class="kn">from</code> <code class="nn">ray.tune.logger</code> <code class="kn">import</code> <code class="n">pretty_print</code>


<code class="k">class</code> <code class="nc">PrintResultCallback</code><code class="p">(</code><code class="n">Callback</code><code class="p">):</code>
    <code class="k">def</code> <code class="nf">on_trial_result</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">iteration</code><code class="p">,</code> <code class="n">trials</code><code class="p">,</code> <code class="n">trial</code><code class="p">,</code> <code class="n">result</code><code class="p">,</code> <code class="o">**</code><code class="n">info</code><code class="p">):</code>
        <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Trial </code><code class="si">{</code><code class="n">trial</code><code class="si">}</code><code class="s2"> in iteration </code><code class="si">{</code><code class="n">iteration</code><code class="si">}</code><code class="s2">, got result: </code><code class="si">{</code><code class="n">result</code><code class="p">[</code><code class="s1">'score'</code><code class="p">]</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>


<code class="k">def</code> <code class="nf">objective</code><code class="p">(</code><code class="n">config</code><code class="p">):</code>
    <code class="k">for</code> <code class="n">step</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">30</code><code class="p">):</code>
        <code class="n">score</code> <code class="o">=</code> <code class="n">config</code><code class="p">[</code><code class="s2">"weight"</code><code class="p">]</code> <code class="o">*</code> <code class="p">(</code><code class="n">step</code> <code class="o">**</code> <code class="mf">0.5</code><code class="p">)</code> <code class="o">+</code> <code class="n">config</code><code class="p">[</code><code class="s2">"bias"</code><code class="p">]</code>
        <code class="n">tune</code><code class="o">.</code><code class="n">report</code><code class="p">(</code><code class="n">score</code><code class="o">=</code><code class="n">score</code><code class="p">,</code> <code class="n">step</code><code class="o">=</code><code class="n">step</code><code class="p">,</code> <code class="n">more_metrics</code><code class="o">=</code><code class="p">{})</code></pre></div>
<p>Note that, apart from the score, we also report <code>step</code> and <code>more_metrics</code> to Tune.
In fact, you could expose any other metric you’d like to track there, and Tune would add it to its trial metrics.
Here’s how you’d run a Tune experiment with our custom callback, and print the custom metrics we just defined:</p>
<div data-type="example">
<h5><span class="label">Example 5-11. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="n">search_space</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"weight"</code><code class="p">:</code> <code class="n">tune</code><code class="o">.</code><code class="n">uniform</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">),</code> <code class="s2">"bias"</code><code class="p">:</code> <code class="n">tune</code><code class="o">.</code><code class="n">uniform</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">)}</code>

<code class="n">analysis</code> <code class="o">=</code> <code class="n">tune</code><code class="o">.</code><code class="n">run</code><code class="p">(</code>
    <code class="n">objective</code><code class="p">,</code>
    <code class="n">config</code><code class="o">=</code><code class="n">search_space</code><code class="p">,</code>
    <code class="n">mode</code><code class="o">=</code><code class="s2">"min"</code><code class="p">,</code>
    <code class="n">metric</code><code class="o">=</code><code class="s2">"score"</code><code class="p">,</code>
    <code class="n">callbacks</code><code class="o">=</code><code class="p">[</code><code class="n">PrintResultCallback</code><code class="p">()])</code>

<code class="n">best</code> <code class="o">=</code> <code class="n">analysis</code><code class="o">.</code><code class="n">best_trial</code>
<code class="nb">print</code><code class="p">(</code><code class="n">pretty_print</code><code class="p">(</code><code class="n">best</code><code class="o">.</code><code class="n">last_result</code><code class="p">))</code></pre></div>
<p>Running this code will result in the following outputs (additional to what you’ll see in any other Tune run).
Note that we need to specify <code>mode</code> and <code>metric</code> explicitly here, so that Tune knows what we mean by <code>best_result</code>.
First, you should see the output of our callback, while the trials are running:</p>
<pre data-type="programlisting">...
Trial objective_85955_00000 in iteration 57, got result: 1.5379782083952644
Trial objective_85955_00000 in iteration 58, got result: 1.5539087627537493
Trial objective_85955_00000 in iteration 59, got result: 1.569535794562848
Trial objective_85955_00000 in iteration 60, got result: 1.5848760187255326
Trial objective_85955_00000 in iteration 61, got result: 1.5999446700996236
...</pre>
<p>Then, at the very end of the program, we print the metrics of the best available trial, which includes the three custom metrics we defined.
The following output omits some default metrics to make it more readable.
We recommend that you run an example like this on your own, in particular to get used to reading the outputs of Tune trials (which can be a bit overwhelming due to their concurrent nature).</p>
<pre data-type="programlisting">Result logdir: /Users/maxpumperla/ray_results/objective_2022-05-23_15-52-01
...
done: true
experiment_id: ea5d89c2018f483183a005a1b5d47302
experiment_tag: 0_bias=0.73356,weight=0.16088
hostname: mac
iterations_since_restore: 30
more_metrics: {}
score: 1.5999446700996236
step: 29
trial_id: '85955_00000'
...</pre>
<p>Note that we used <code>on_trial_result</code> as an example of a method to implement a custom Tune <code>Callback</code>, but you have many other useful options that are all relatively self-explanatory.
It’s not very helpful to list them all here, but some callback methods I find particularly useful are <code>on_trial_start</code>, <code>on_trial_error</code>, <code>on_experiment_end</code> and <code>on_checkpoint</code>.
The latter hints at an important aspect of Tune runs that we’ll discuss next.</p>
</div></section>
<section data-pdf-bookmark="Checkpoints, Stopping, and Resuming" data-type="sect3"><div class="sect3" id="idm44990025140112">
<h3>Checkpoints, Stopping, and Resuming</h3>
<p>The more Tune trials you kick off and the longer they each run individually, especially in a distributed setting, the more you need a mechanism to protect you against failures, to stop a run, or pick a run up again from previous results.
Tune makes this possible by periodically creating <em>checkpoints</em> for you.
The checkpoint cadence is dynamically adjusted by Tune to ensure at least 95% of the time is spent on running trials, and not too many resources are devoted to storing checkpoints.</p>
<p>In the example we just computed, the checkpoint directory, or <code>logdir</code>, used was <code>/Users/maxpumperla/ray_results/objective_2022-05-23_15-52-01</code>.
If you ran this example on your machine, by default its structure would be <code>~/ray_results/&lt;your-objective&gt;_&lt;date&gt;_&lt;time&gt;</code>.
If you know this <code>name</code> of your experiment, you can easily <code>resume</code> it like so:</p>
<div data-type="example">
<h5><span class="label">Example 5-12. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="n">analysis</code> <code class="o">=</code> <code class="n">tune</code><code class="o">.</code><code class="n">run</code><code class="p">(</code>
    <code class="n">objective</code><code class="p">,</code>
    <code class="n">name</code><code class="o">=</code><code class="s2">"/Users/maxpumperla/ray_results/objective_2022-05-23_15-52-01"</code><code class="p">,</code>
    <code class="n">resume</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
    <code class="n">config</code><code class="o">=</code><code class="n">search_space</code><code class="p">)</code></pre></div>
<p>Similarly, you can <em>stop</em> your trials by defining stopping conditions and explicitly passing them to your <code>tune.run</code>.
The easiest option to do that, and we’ve already seen this option before, is by providing a dictionary with a stopping condition.
Here’s how you stop running our <code>objective</code> analysis after reaching a <code>training_iteration</code> count of 10, a built-in metric of all Tune runs:</p>
<div data-type="example">
<h5><span class="label">Example 5-13. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="n">tune</code><code class="o">.</code><code class="n">run</code><code class="p">(</code>
    <code class="n">objective</code><code class="p">,</code>
    <code class="n">config</code><code class="o">=</code><code class="n">search_space</code><code class="p">,</code>
    <code class="n">stop</code><code class="o">=</code><code class="p">{</code><code class="s2">"training_iteration"</code><code class="p">:</code> <code class="mi">10</code><code class="p">})</code></pre></div>
<p>One of the drawbacks of this way of specifying a stopping condition is that it assumes the metric in question is <em>increasing</em>.
For instance, the <code>score</code> we compute starts high and is something we want to minimize.
To formulate a flexible stopping condition for our <code>score</code>, the best way is to provide a stopping function as follows.</p>
<div data-type="example">
<h5><span class="label">Example 5-14. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">stopper</code><code class="p">(</code><code class="n">trial_id</code><code class="p">,</code> <code class="n">result</code><code class="p">):</code>
    <code class="k">return</code> <code class="n">result</code><code class="p">[</code><code class="s2">"score"</code><code class="p">]</code> <code class="o">&lt;</code> <code class="mi">2</code>


<code class="n">tune</code><code class="o">.</code><code class="n">run</code><code class="p">(</code>
    <code class="n">objective</code><code class="p">,</code>
    <code class="n">config</code><code class="o">=</code><code class="n">search_space</code><code class="p">,</code>
    <code class="n">stop</code><code class="o">=</code><code class="n">stopper</code><code class="p">)</code></pre></div>
<p>In situations that require a stopping condition with more context or explicit state, you can also define a custom <code>Stopper</code> class to pass into the <code>stop</code> argument of your Tune run, but we won’t cover this case here.</p>
</div></section>
<section data-pdf-bookmark="Custom and conditional search spaces" data-type="sect3"><div class="sect3" id="idm44990024684592">
<h3>Custom and conditional search spaces</h3>
<p>The last more advanced topic we’re going to cover here is that of complex search spaces.
So far, we’ve only looked at hyperparameters that were independent of each other, but in practice it happens quite often that some depend on others.
Also, while Tune’s built-in search spaces have quite a lot to offer, sometimes you might want to sample parameters from a more exotic distribution or your own modules.</p>
<p>Here’s how you can handle both situations in Tune.
Continuing with our simple <code>objective</code> example, let’s say that  instead of Tune’s <code>tune.uniform</code> you want to use the <code>random.uniform</code> sampler from the <code>numpy</code> package for your <code>weight</code> parameter.
And then your <code>bias</code> parameter should be <code>weight</code> times a standard normal variable.
Using <code>tune.sample_from</code> you can tackle this situation (or more complex and nested ones) as follows:</p>
<div data-type="example">
<h5><span class="label">Example 5-15. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">ray</code> <code class="kn">import</code> <code class="n">tune</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>

<code class="n">search_space</code> <code class="o">=</code> <code class="p">{</code>
    <code class="s2">"weight"</code><code class="p">:</code> <code class="n">tune</code><code class="o">.</code><code class="n">sample_from</code><code class="p">(</code><code class="k">lambda</code> <code class="n">context</code><code class="p">:</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">uniform</code><code class="p">(</code><code class="n">low</code><code class="o">=</code><code class="mf">0.0</code><code class="p">,</code> <code class="n">high</code><code class="o">=</code><code class="mf">1.0</code><code class="p">)),</code>
    <code class="s2">"bias"</code><code class="p">:</code> <code class="n">tune</code><code class="o">.</code><code class="n">sample_from</code><code class="p">(</code><code class="k">lambda</code> <code class="n">context</code><code class="p">:</code> <code class="n">context</code><code class="o">.</code><code class="n">config</code><code class="o">.</code><code class="n">alpha</code> <code class="o">*</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">normal</code><code class="p">())</code>
<code class="p">}</code>

<code class="n">tune</code><code class="o">.</code><code class="n">run</code><code class="p">(</code><code class="n">objective</code><code class="p">,</code> <code class="n">config</code><code class="o">=</code><code class="n">search_space</code><code class="p">)</code></pre></div>
<p>There are many more interesting features to explore in Ray Tune, but let’s switch gears here and look into some machine learning applications using Tune.</p>
</div></section>
</div></section>
</div></section>
<section data-pdf-bookmark="Machine Learning with Tune" data-type="sect1"><div class="sect1" id="idm44990025674544">
<h1>Machine Learning with Tune</h1>
<p>As we’ve seen, Tune is versatile and allows you to tune hyperparameters for any objective you give it.
In particular, you can use it with any machine learning framework you’re interested in.
In this section we’re going to give you two examples to illustrate this.</p>
<p>First, we’re going to use Tune to optimize parameters of an RLlib reinforcement learning experiment, and then we’re tuning a Keras model using Optuna through Tune.</p>
<section data-pdf-bookmark="Using RLlib with Tune" data-type="sect2"><div class="sect2" id="idm44990024568240">
<h2>Using RLlib with Tune</h2>
<p>RLlib and Tune have been designed to work together, so you can quite easily set up an HPO experiment for your existing RLlib code.
In fact, RLlib Trainers can be passed into the first argument of <code>tune.run</code>, as <code>Trainable</code>.
You can choose between the actual Trainer class, like <code>DQNTrainer</code>, or its string representation, like <code>"DQN"</code>.
As Tune <code>metric</code> you can pass any metric tracked by your RLlib experiment, for instance <code>"episode_reward_mean"</code>.
And the <code>config</code> argument to <code>tune.run</code> is just your RLlib Trainer configuration, but you can use the full power of Tune’s search space API to sample hyperparameters like the learning rate or training batch size<sup><a data-type="noteref" href="ch05.xhtml#idm44990024561792" id="idm44990024561792-marker">1</a></sup>.
Here’s a full example of what we just described, running a tuned RLlib experiment on the <code>CartPole-v0</code> gym environment:</p>
<div data-type="example">
<h5><span class="label">Example 5-16. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">ray</code> <code class="kn">import</code> <code class="n">tune</code>

<code class="n">analysis</code> <code class="o">=</code> <code class="n">tune</code><code class="o">.</code><code class="n">run</code><code class="p">(</code>
    <code class="s2">"DQN"</code><code class="p">,</code>
    <code class="n">metric</code><code class="o">=</code><code class="s2">"episode_reward_mean"</code><code class="p">,</code>
    <code class="n">mode</code><code class="o">=</code><code class="s2">"max"</code><code class="p">,</code>
    <code class="n">config</code><code class="o">=</code><code class="p">{</code>
        <code class="s2">"env"</code><code class="p">:</code> <code class="s2">"CartPole-v0"</code><code class="p">,</code>
        <code class="s2">"lr"</code><code class="p">:</code> <code class="n">tune</code><code class="o">.</code><code class="n">uniform</code><code class="p">(</code><code class="mf">1e-5</code><code class="p">,</code> <code class="mf">1e-4</code><code class="p">),</code>
        <code class="s2">"train_batch_size"</code><code class="p">:</code> <code class="n">tune</code><code class="o">.</code><code class="n">choice</code><code class="p">([</code><code class="mi">10000</code><code class="p">,</code> <code class="mi">20000</code><code class="p">,</code> <code class="mi">40000</code><code class="p">]),</code>
    <code class="p">},</code>
<code class="p">)</code></pre></div>
</div></section>
<section data-pdf-bookmark="Tuning Keras Models" data-type="sect2"><div class="sect2" id="idm44990024535616">
<h2>Tuning Keras Models</h2>
<p>To wrap up this chapter, let’s look at a slightly more involved example.
As we mentioned before, this is not primarily a machine learning book, but rather an introduction to Ray and its libraries.
That means that we can neither introduce you to the basics of ML, nor can we spend much time on introducing ML frameworks in detail.
So, in this section we assume familiarity with Keras and its API, and some basic knowledge about supervised learning.
If you do not have these prerequisites, you should still be able to follow along and focus on the Ray Tune specific parts.
You can view the following example as a more realistic scenario of applying Tune to machine learning workloads.</p>
<p>From a bird’s eye view, we’re going to load a common data set, prepare it for an ML task,
define a Tune objective by creating a deep learning model with Keras that reports an accuracy metric to Tune, and use Tune’s HyperOpt integration to define a search algorithm that tunes a set of hyperparameters of our Keras model.
The workflow remains the same - we define an objective, a search space, and then use <code>tune.run</code> with the configuration we want.</p>
<p>To define a data set to train on, let’s write a simple <code>load_data</code> utility function that loads the famous MNIST data that ships with Keras.
MNIST consists of 28 times 28 pixel images of handwritten digits.
We normalize the pixel values to be between 0 and 1, and make the labels for those ten digits <em>categorical variables</em>.
Here’s how you can do this purely with Keras’ built-in functionality (make sure to <code>pip install tensorflow</code> before running this):</p>
<div data-type="example">
<h5><span class="label">Example 5-17. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">tensorflow.keras.datasets</code> <code class="kn">import</code> <code class="n">mnist</code>
<code class="kn">from</code> <code class="nn">tensorflow.keras.utils</code> <code class="kn">import</code> <code class="n">to_categorical</code>


<code class="k">def</code> <code class="nf">load_data</code><code class="p">():</code>
    <code class="p">(</code><code class="n">x_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">),</code> <code class="p">(</code><code class="n">x_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code> <code class="o">=</code> <code class="n">mnist</code><code class="o">.</code><code class="n">load_data</code><code class="p">()</code>
    <code class="n">num_classes</code> <code class="o">=</code> <code class="mi">10</code>
    <code class="n">x_train</code><code class="p">,</code> <code class="n">x_test</code> <code class="o">=</code> <code class="n">x_train</code> <code class="o">/</code> <code class="mf">255.0</code><code class="p">,</code> <code class="n">x_test</code> <code class="o">/</code> <code class="mf">255.0</code>
    <code class="n">y_train</code> <code class="o">=</code> <code class="n">to_categorical</code><code class="p">(</code><code class="n">y_train</code><code class="p">,</code> <code class="n">num_classes</code><code class="p">)</code>
    <code class="n">y_test</code> <code class="o">=</code> <code class="n">to_categorical</code><code class="p">(</code><code class="n">y_test</code><code class="p">,</code> <code class="n">num_classes</code><code class="p">)</code>
    <code class="k">return</code> <code class="p">(</code><code class="n">x_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">),</code> <code class="p">(</code><code class="n">x_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code></pre></div>
<p>Next, we define a Tune <code>objective</code> function, or Trainable, by loading the data we just defined, setting up a sequential Keras model with hyperparameters selected from the <code>config</code> we pass into our <code>objective</code>, and then compile and fit the model.
To define our deep learning model, we first flatten the MNIST input images to vectors and then add two fully-connected layers (called <code>Dense</code> in Keras) and a <code>Dropout</code> layer in between.
The hyperparameters we want to tune are the activation function of the first <code>Dense</code> layer,
the <code>Dropout</code> rate, and the number of “hidden” output units of the first layer.
We could tune any other hyperparameter of this model the same way, this selection is just an example.</p>
<p>We could manually report a metric of interest in the same way we did in other examples in this chapter (e.g. by returning a dictionary in our <code>objective</code> or using <code>tune.report(...)</code>).
But since Tune comes with a proper Keras integration, we can use the so-called <code>TuneReportCallback</code> as a custom Keras callback that we pass into our model’s <code>fit</code> method.
This is what our Keras <code>objective</code> function looks like:</p>
<div data-type="example">
<h5><span class="label">Example 5-18. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">tensorflow.keras.models</code> <code class="kn">import</code> <code class="n">Sequential</code>
<code class="kn">from</code> <code class="nn">tensorflow.keras.layers</code> <code class="kn">import</code> <code class="n">Flatten</code><code class="p">,</code> <code class="n">Dense</code><code class="p">,</code> <code class="n">Dropout</code>
<code class="kn">from</code> <code class="nn">ray.tune.integration.keras</code> <code class="kn">import</code> <code class="n">TuneReportCallback</code>


<code class="k">def</code> <code class="nf">objective</code><code class="p">(</code><code class="n">config</code><code class="p">):</code>
    <code class="p">(</code><code class="n">x_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">),</code> <code class="p">(</code><code class="n">x_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code> <code class="o">=</code> <code class="n">load_data</code><code class="p">()</code>
    <code class="n">model</code> <code class="o">=</code> <code class="n">Sequential</code><code class="p">()</code>
    <code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Flatten</code><code class="p">(</code><code class="n">input_shape</code><code class="o">=</code><code class="p">(</code><code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">)))</code>
    <code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="n">config</code><code class="p">[</code><code class="s2">"hidden"</code><code class="p">],</code> <code class="n">activation</code><code class="o">=</code><code class="n">config</code><code class="p">[</code><code class="s2">"activation"</code><code class="p">]))</code>
    <code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dropout</code><code class="p">(</code><code class="n">config</code><code class="p">[</code><code class="s2">"rate"</code><code class="p">]))</code>
    <code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"softmax"</code><code class="p">))</code>

    <code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code><code class="s2">"categorical_crossentropy"</code><code class="p">,</code> <code class="n">metrics</code><code class="o">=</code><code class="p">[</code><code class="s2">"accuracy"</code><code class="p">])</code>
    <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">x_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">128</code><code class="p">,</code> <code class="n">epochs</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code>
              <code class="n">validation_data</code><code class="o">=</code><code class="p">(</code><code class="n">x_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">),</code>
              <code class="n">callbacks</code><code class="o">=</code><code class="p">[</code><code class="n">TuneReportCallback</code><code class="p">({</code><code class="s2">"mean_accuracy"</code><code class="p">:</code> <code class="s2">"accuracy"</code><code class="p">})])</code></pre></div>
<p>Next, let’s use a custom search algorithm to tune this objective.
Specifically, we’re using the <code>HyperOptSearch</code> algorithm, which gives us access to HyperOpt’s TPE algorithm through Tune.
To use this integration, make sure to install HyperOpt on your machine (for instance with <code>pip install hyperopt==0.2.5</code>).
<code>HyperOptSearch</code> allows us to define a list of promising, initial hyperparameter choices to investigate.
This is entirely optional, but sometimes you might have good guesses to start from.
In our case, we go with a dropout <code>"rate"</code> of 0.2, 128 <code>"hidden"</code> units, and a rectified linear unit (ReLU) <code>"activation"</code> function initially.
Other than that, we can define a search space with <code>tune</code> utility just as we did before.
Finally, we can get an <code>analysis</code> object to determine the best hyperparameters found by passing everything into a <code>tune.run</code> call.</p>
<div data-type="example">
<h5><span class="label">Example 5-19. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">ray</code> <code class="kn">import</code> <code class="n">tune</code>
<code class="kn">from</code> <code class="nn">ray.tune.suggest.hyperopt</code> <code class="kn">import</code> <code class="n">HyperOptSearch</code>


<code class="n">initial_params</code> <code class="o">=</code> <code class="p">[{</code><code class="s2">"rate"</code><code class="p">:</code> <code class="mf">0.2</code><code class="p">,</code> <code class="s2">"hidden"</code><code class="p">:</code> <code class="mi">128</code><code class="p">,</code> <code class="s2">"activation"</code><code class="p">:</code> <code class="s2">"relu"</code><code class="p">}]</code>
<code class="n">algo</code> <code class="o">=</code> <code class="n">HyperOptSearch</code><code class="p">(</code><code class="n">points_to_evaluate</code><code class="o">=</code><code class="n">initial_params</code><code class="p">)</code>

<code class="n">search_space</code> <code class="o">=</code> <code class="p">{</code>
    <code class="s2">"rate"</code><code class="p">:</code> <code class="n">tune</code><code class="o">.</code><code class="n">uniform</code><code class="p">(</code><code class="mf">0.1</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">),</code>
    <code class="s2">"hidden"</code><code class="p">:</code> <code class="n">tune</code><code class="o">.</code><code class="n">randint</code><code class="p">(</code><code class="mi">32</code><code class="p">,</code> <code class="mi">512</code><code class="p">),</code>
    <code class="s2">"activation"</code><code class="p">:</code> <code class="n">tune</code><code class="o">.</code><code class="n">choice</code><code class="p">([</code><code class="s2">"relu"</code><code class="p">,</code> <code class="s2">"tanh"</code><code class="p">])</code>
<code class="p">}</code>


<code class="n">analysis</code> <code class="o">=</code> <code class="n">tune</code><code class="o">.</code><code class="n">run</code><code class="p">(</code>
    <code class="n">objective</code><code class="p">,</code>
    <code class="n">name</code><code class="o">=</code><code class="s2">"keras_hyperopt_exp"</code><code class="p">,</code>
    <code class="n">search_alg</code><code class="o">=</code><code class="n">algo</code><code class="p">,</code>
    <code class="n">metric</code><code class="o">=</code><code class="s2">"mean_accuracy"</code><code class="p">,</code>
    <code class="n">mode</code><code class="o">=</code><code class="s2">"max"</code><code class="p">,</code>
    <code class="n">stop</code><code class="o">=</code><code class="p">{</code><code class="s2">"mean_accuracy"</code><code class="p">:</code> <code class="mf">0.99</code><code class="p">},</code>
    <code class="n">num_samples</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code>
    <code class="n">config</code><code class="o">=</code><code class="n">search_space</code><code class="p">,</code>
<code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Best hyperparameters found were: "</code><code class="p">,</code> <code class="n">analysis</code><code class="o">.</code><code class="n">best_config</code><code class="p">)</code></pre></div>
<p>Note that we’re leveraging the full power of HyperOpt here, without having to learn any specifics of it.
We use Tune as a distributed front-end to another HPO tool, plus leveraging its native integration with Keras.</p>
<p>While we chose a combination of Keras and HyperOpt as example of using Tune with an advanced ML framework and a third-party HPO library, as indicated earlier we could have chosen literally any other machine learning library and practically any other HPO library in popular use today.
If you’re interested in diving deeper into any of the many other integrations Tune has to offer, check out the <a href="https://docs.ray.io/en/latest/tune/examples/index.xhtml">Ray Tune documentation examples</a>.</p>
</div></section>
</div></section>
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm44990024510208">
<h1>Summary</h1>
<p>Tune is arguably one of the most versatile HPO tools you can choose today.
It’s very feature-rich, offering many search algorithms, advanced schedulers, complex search spaces, custom stoppers and many other features that we couldn’t cover in this chapter.
Also, it seamlessly integrates with most notable HPO tools, such as Optuna or HyperOpt, making it easy to either migrate from these tools, or simply leverage their features through Tune.
Since Tune, as part of the Ray ecosystem, is distributed by default, it has an edge over many of its competitors.
You can view Ray Tune as a flexible, distributed HPO framework that <em>extends</em> others that might only work on single machines.
Seen that way, and given that you have a need to scale out your HPO experiments, there’s very little speaking against adopting Tune.</p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="idm44990024561792"><sup><a href="ch05.xhtml#idm44990024561792-marker">1</a></sup> In case you were wondering why the “config” argument in <code>tune.run</code> was not called <code>search_space</code>, the historical reason lies in this interoperability with RLlib <code>config</code> objects.</p></div></div></section></div></body></html>