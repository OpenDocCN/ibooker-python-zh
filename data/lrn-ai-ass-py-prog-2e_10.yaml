- en: 11 Creating an authorship identification program
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11 创建一个作者身份识别程序
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章内容包括
- en: Writing an authorship identification program using top-down design
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自顶向下设计编写一个作者身份识别程序
- en: Learning about refactoring code and why you would do it
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习重构代码以及为什么要进行重构
- en: 'In chapter 7, we learned about problem decomposition and top-down design when
    we wrote our Spelling Suggestions program. Here, we’re going to take top-down
    design to the next level and solve a much larger problem. We’re still doing the
    same thing as in chapter 7: dividing a problem into subproblems, and further dividing
    those subproblems into sub-subproblems as needed. And, just like before, we’re
    looking to design functions with a small number of parameters that return a meaningful
    and useful result to their caller. It’s also a good sign if we’re able to design
    functions that are called by multiple other functions—that helps reduce code repetition!'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在第7章中，我们通过编写拼写建议程序学习了问题分解和自顶向下设计。在这里，我们将自顶向下设计提升到一个新的层次，解决一个更大的问题。我们依然在做与第7章相同的事情：将一个问题分解成子问题，必要时再将这些子问题分解成更小的子问题。而且，就像之前一样，我们希望设计出具有少量参数的函数，并且这些函数能返回一个有意义且对调用者有用的结果。如果我们能设计出被多个其他函数调用的函数，那就更好了——这有助于减少代码重复！
- en: We’re including this chapter because we wanted to provide a more authentic example
    than the Spelling Suggestions problem we solved in chapter 7\. We hope our example
    here is motivating and feels like a real problem that you could imagine yourself
    wanting to solve.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们包括这一章是因为我们想提供一个比第7章中我们解决的拼写建议问题更真实的例子。我们希望这里的例子能够激励你，并让你感觉这是一个你自己也能想象并希望解决的实际问题。
- en: In this chapter, we’re going to write a program that tries to identify the unknown
    author of a mystery book. It’ll be an example of a program that uses artificial
    intelligence (AI) to make a prediction. We couldn’t resist the opportunity to
    include an AI example in a book about programming with AI!
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将编写一个程序，试图识别一本神秘书籍的未知作者。这将是一个使用人工智能（AI）来进行预测的程序示例。我们无法抗拒在一本关于编程与人工智能的书中加入一个AI示例！
- en: 11.1 Authorship identification
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.1 作者身份识别
- en: 'This problem is based on an assignment created by our colleague Michelle Craig
    [1]. Let’s start by taking a look at these two book excerpts:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题基于我们的同事Michelle Craig[1]创建的一个作业。让我们先来看一下这两段书籍摘录：
- en: '*Excerpt 1*—I have not yet described to you the most singular part. About six
    years ago—to be exact, upon the 4^(th) of May 1882—an advertisement appeared in
    the Times asking for the address of Miss Mary Morstan and stating that it would
    be to her advantage to come forward. There was no name or address appended. I
    had at that time just entered the family of Mrs. Cecil Forrester in the capacity
    of governess. By her advice I published my address in the advertisement column.
    The same day there arrived through the post a small card-board box addressed to
    me, which I found to contain a very large and lustrous pearl. No word of writing
    was enclosed. Since then, every year upon the same date there has always appeared
    a similar box, containing a similar pearl, without any clue as to the sender.
    They have been pronounced by an expert to be of a rare variety and of considerable
    value. You can see for yourselves that they are very handsome.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*摘录 1*——我尚未向你们描述最独特的部分。大约六年前——确切地说，是1882年5月4日——《泰晤士报》上刊登了一则广告，寻求玛丽·莫尔斯坦小姐的地址，并表示如果她能够现身，将对她有利。广告上没有附上姓名或地址。那时我刚刚进入塞西尔·福雷斯特夫人家中，担任家庭教师。在她的建议下，我在广告栏中发布了我的地址。就在同一天，一只小纸板盒通过邮寄送到我这里，盒子里装着一颗非常大且光泽明亮的珍珠。盒中没有附带任何文字。自那时以来，每年同一日期，总会出现一个类似的盒子，里面有一颗类似的珍珠，且没有任何关于寄件人的线索。经过专家鉴定，它们属于一种稀有种类，且价值不菲。你们可以自己看，这些珍珠非常美丽。'
- en: '*Excerpt 2*—It was the Dover Road that lay on a Friday night late in November,
    before the first of the persons with whom this history has business. The Dover
    Road lay, as to him, beyond the Dover mail, as it lumbered up Shooter’s Hill.
    He walked up hill in the mire by the side of the mail, as the rest of the passengers
    did; not because they had the least relish for walking exercise, under the circumstances,
    but because the hill, and the harness, and the mud, and the mail, were all so
    heavy, that the horses had three times already come to a stop, besides once drawing
    the coach across the road, with the mutinous intent of taking it back to Blackheath.
    Reins and whip and coachman and guard, however, in combination, had read that
    article of war which forbade a purpose otherwise strongly in favour of the argument,
    that some brute animals are endued with Reason; and the team had capitulated and
    returned to their duty.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*片段2*——这是十一月的一个星期五晚上，道佛路上的景象，正是此历史事件中的第一位与此有关系的人出现之前。对他来说，道佛路延伸至道佛邮车前方，当邮车轰隆轰隆地驶上射手山时，他走在泥泞的山路上，与其他乘客一样；不是因为在这种情况下他们特别喜欢走路，而是因为山路、马具、泥巴和邮车太重，以至于马车已经停了三次，甚至有一次试图把车拉到路边，意图带着车回到布莱克希思。尽管如此，缰绳、鞭子、车夫和警卫的组合，还是遵循了那条禁止某些野兽拥有理性目的的战争条款；于是，马车队终于屈服，恢复了职责。'
- en: Suppose we asked you whether these two excerpts were likely written by the same
    author. One reasonable assumption you might make is that different authors write
    differently, and that these differences would show up in metrics that we could
    calculate about their texts.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们问你，这两个片段是否很可能是同一个作者写的。你可能做出的一个合理假设是，不同的作者写作风格不同，而这些差异会在我们可以计算的文本指标中表现出来。
- en: For example, whoever wrote the first excerpt seems to use quite a few short
    sentences in terms of number of words compared to the second excerpt. We find
    short sentences like “There was no name or address appended” and “No word of writing
    was enclosed” in the first excerpt; those kinds of sentences are absent from the
    second. Similarly, the sentences from the first excerpt seem to be less complex
    than those in the second; look at all of those commas and semicolons in the second
    excerpt.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，无论是谁写的第一个片段，与第二个片段相比，似乎使用了更多的短句。从第一个片段中，我们可以看到像“没有附上姓名或地址”和“没有写的字条”这样的短句；而这些句子在第二个片段中是没有的。同样，第一个片段中的句子似乎比第二个片段的句子更简单；看看第二个片段中那些逗号和分号。
- en: This analysis may lead you to believe that these texts are written by different
    authors, and, indeed, they are. The first is written by Sir Arthur Conan Doyle,
    and the second by Charles Dickens.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分析可能会让你相信这些文本是由不同的作者写的，事实上，它们确实是。第一个片段是由阿瑟·柯南·道尔爵士写的，第二个片段则是由查尔斯·狄更斯写的。
- en: To be fair, we’ve absolutely cherry-picked these two excerpts. Doyle does use
    some long, complex sentences. Dickens does use some short ones. But, on average,
    at least for the two books that we took these excerpts from, Doyle does write
    shorter sentences than Dickens. More generally, if we look at two books written
    by different authors, we might expect to find some quantifiable differences on
    average.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 公平地说，我们完全是精挑细选了这两个片段。道尔确实使用了一些长而复杂的句子。狄更斯确实使用了一些短句。但总的来说，至少对于我们从中提取的两本书而言，道尔写的句子比狄更斯短。更一般来说，如果我们看两本不同作者写的书，我们可能会发现在某些可量化的方面存在平均差异。
- en: Suppose that we have a bunch of books whose authors we know. We have one written
    by Doyle, one written by Dickens, and so on. Then, along comes a mystery book.
    Oh no! We don’t know who wrote it! Is it a lost Sherlock Holmes story from Doyle?
    A lost *Oliver Twist* sequel from Dickens? We want to find out who that unknown
    author is, and to do that, we’ll turn to a basic AI technique.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一堆书籍，且我们知道它们的作者。我们有一本是由道尔写的，一本是由狄更斯写的，等等。然后，一本神秘的书出现了。哦不！我们不知道它是谁写的！它是道尔失落的《福尔摩斯故事》吗？还是狄更斯失落的《雾都孤儿》续集？我们想弄清楚那位未知的作者是谁，为此，我们将使用一种基本的人工智能技术。
- en: Our strategy will be to come up with a *signature* for each author, using one
    of the books we know they wrote. We’ll refer to these signatures as *known signatures*.
    Each of these signatures will capture metrics about the book text, such as the
    average number of words per sentence and the average sentence complexity. Then,
    we’ll come up with a signature for the mystery book with an unknown author. We’ll
    call this the *unknown signature*. We’ll look through all the known signatures,
    comparing each one to our unknown signature. We’ll use whichever is the closest
    as our guess for the unknown author.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的策略是为每位作者想出一个*签名*，使用我们知道他们写的其中一本书。我们将这些签名称为*已知签名*。每个签名将捕捉关于书籍文本的度量，如每句平均单词数和句子的平均复杂度。接着，我们将为那本作者未知的神秘书籍想出一个签名。我们将这个签名称为*未知签名*。然后，我们将查看所有已知签名，将每个与我们的未知签名进行比较。我们将使用与未知签名最接近的那个作为我们对未知作者的猜测。
- en: Of course, we have no idea if the unknown author is really one of the authors
    whose signatures we have. It could be a completely new author, for example. Even
    if the unknown author *is* one of the authors whose signature we have, we still
    might end up guessing wrong. After all, maybe the same author writes books in
    different styles (giving their books very different signatures), or we simply
    fail to capture what is most salient about how each of our authors writes. Indeed,
    we’re not after an industry-strength author identification program in this chapter.
    Still, considering the difficulty of this task, we think it’s impressive how well
    the approach that we’ll show you here works.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们并不清楚未知作者是否真的是我们已有签名的其中一位作者。比如说，可能是一个全新的作者。即使未知作者*确实*是我们已有签名的其中一位作者，我们仍然可能猜错。毕竟，可能同一位作者会以不同的风格写书（这会导致他们的书籍有非常不同的签名），或者我们根本没能捕捉到每位作者写作时最具代表性的特点。事实上，我们在这一章并非要开发一个行业级的作者身份识别程序。尽管如此，考虑到这一任务的难度，我们认为我们将在这里展示的方法取得的效果仍然非常令人印象深刻。
- en: Machine learning
  id: totrans-18
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 机器学习
- en: Authorship identification, as we’re doing here, is a *machine learning*(ML)
    task. ML is a branch of AI designed to help computers “learn” from data in order
    to make predictions. There are various forms of ML; the one we’re using here is
    called supervised learning. In supervised learning, we have access to training
    data, which consists of objects and their known categories (or labels). In our
    case, our objects are book texts, and the category for each book is the author
    who wrote it. We can train (i.e., learn) on the training set by calculating features—average
    number of words per sentence, average sentence complexity, and so on—for each
    book. Later, when we’re provided a book whose author we don’t know, we can use
    what we learned in the training to make our prediction (or guess).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里进行的作者身份识别是一个*机器学习*(ML)任务。机器学习是人工智能的一个分支，旨在帮助计算机“从数据中学习”，从而进行预测。机器学习有多种形式；我们在这里使用的是有监督学习。在有监督学习中，我们可以访问训练数据，这些数据由对象及其已知类别（或标签）组成。在我们的案例中，对象是书籍文本，而每本书的类别是写这本书的作者。我们可以通过计算每本书的特征——如每句平均单词数、句子平均复杂度等——在训练集上进行训练（即学习）。之后，当我们得到一本作者未知的书时，我们可以利用在训练中学到的内容来进行预测（或猜测）。
- en: 11.2 Authorship identification using top-down design
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.2 使用自顶向下设计的作者身份识别
- en: Alright, we want to “write a program to determine the author of a book.” This
    seems like a daunting task, and it would be if we were trying to do this in one
    shot, using a single function. But just like in our Spelling Suggestions example
    in chapter 7, we’re not going to do that. We’re going to systematically break
    this problem down into subproblems that we can solve.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，我们想“编写一个程序来确定一本书的作者”。这看起来是一个艰巨的任务，如果我们尝试一次性完成，使用一个单独的函数来解决，确实会很困难。但就像在第七章的拼写建议例子中一样，我们并不打算这么做。我们将系统地将这个问题分解为可以解决的子问题。
- en: 'In chapter 7, we solved the Spelling Suggestions problem by using the model
    of reading input, processing that input, and producing an output result. We can
    think about our authorship identification program as following that model as well:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在第七章中，我们通过使用读取输入、处理输入并生成输出结果的模型解决了拼写建议问题。我们可以认为我们的作者身份识别程序也遵循了这个模型：
- en: '*Input step* —For the input step, we need to ask the user for the filename
    of the mystery book.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输入步骤* ——对于输入步骤，我们需要询问用户神秘书籍的文件名。'
- en: '*Process step* —For the process step, we need to figure out the signature for
    the mystery book (that’s the unknown signature), as well as the signature for
    each of the books whose author we know (those are the known signatures). Creating
    the signature for each book is commonly called the training phase in ML. We also
    need to compare the unknown signature to each known signature to figure out which
    known signature is closest. These comparisons are the prediction phase in ML.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*过程步骤* — 在过程步骤中，我们需要找出神秘书籍的签名（也就是未知签名），以及我们已知作者的每本书的签名（这些是已知签名）。为每本书创建签名通常称为机器学习中的训练阶段。我们还需要将未知签名与每个已知签名进行比较，以找出哪个已知签名最接近。这些比较是机器学习中的预测阶段。'
- en: '*Output step* —For the output step, we need to report to the user the unknown
    signature that’s closest to the known signature.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输出步骤* — 在输出步骤中，我们需要向用户报告与已知签名最接近的未知签名。'
- en: That is, to solve our overall authorship identification problem, we need to
    solve these three subproblems. We’re starting our top-down design!
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，为了解决我们的整体作者身份识别问题，我们需要解决这三个子问题。我们正在开始顶层设计！
- en: We’ll name our top-level function `make_guess`. In it, we’ll solve each of the
    three subproblems we identified.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将顶层函数命名为`make_guess`。在这个函数中，我们将解决我们识别的三个子问题。
- en: 'For the input step, we’re simply asking the user for a filename. That’s something
    we can do in a small number of lines of code, so we probably don’t need a separate
    function for that. The output step seems similar: assuming that we already know
    which known signature is closest, we can just report that to the user. By contrast,
    the process step looks like a lot of work, and we’ll certainly want to break that
    subproblem down further. That’s what we’ll do next.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于输入步骤，我们只需要询问用户一个文件名。这是我们可以用少量代码完成的事情，因此我们可能不需要为此单独创建一个函数。输出步骤似乎也类似：假设我们已经知道哪个已知签名最接近，我们可以直接向用户报告。相比之下，过程步骤看起来需要做的工作比较多，我们肯定希望进一步拆解这个子问题。这就是我们接下来要做的。
- en: 11.3 Breaking down the process subproblem
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.3 拆解过程子问题
- en: We’ll name our overall process function `process_data`. It will take the mystery
    book filename and the name of a directory of known-author books as parameters
    and return the name of the closest known signature.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将整体过程函数命名为`process_data`。它将接受神秘书籍文件名和已知作者书籍目录名称作为参数，并返回最接近的已知签名的名称。
- en: 'Looking at our description for the process step, it seems that we have three
    subproblems to solve here:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 查看我们对过程步骤的描述，似乎我们有三个子问题需要解决：
- en: '*Figure out the signature for the mystery book.* That’s our unknown signature.
    We’ll name this function `make_signature`.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*找出神秘书籍的签名。* 这是我们的未知签名。我们将这个函数命名为`make_signature`。'
- en: '*Figure out the signature for each of the books whose author we know.* These
    are our known signatures. We’ll name this function `get_all_signatures`.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*找出我们已知作者的每本书的签名。* 这些是我们的已知签名。我们将这个函数命名为`get_all_signatures`。'
- en: '*Compare the unknown signature to each known signature to figure out which
    known signature is closest.* Because close signatures will have small differences,
    we’ll name this function `lowest_score`.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*将未知签名与每个已知签名进行比较，以找出哪个已知签名最接近。* 因为相似的签名差异很小，我们将这个函数命名为`lowest_score`。'
- en: We’ll work on our top-down design for each of these subproblems in turn. Figure
    11.1 shows a diagram of what we have so far.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将依次处理这些子问题的顶层设计。图11.1展示了到目前为止我们所得到的设计图。
- en: '![figure](../Images/11-1.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/11-1.png)'
- en: Figure 11.1 Functions diagram with the three subtasks of `process_data`
  id: totrans-37
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.1 展示了`process_data`的三个子任务的函数图
- en: 11.3.1 Figuring out the signature for the mystery book
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3.1 找出神秘书籍的签名
- en: The function for this task, `make_signature`, will take the text for our book
    as a parameter and return the book’s signature. At this point, we need to decide
    on the features that we’ll use to determine the signature for each book. Let’s
    break this down by thinking back to the previous example passages. We noticed
    there are differences in the authors’ passages based on the complexity and length
    of sentences. You may have also suspected that the authors may differ in the length
    of words used and ways they use words (e.g., some authors may be more repetitive
    than others). As such, we’ll want some features to be based on the structure of
    the author’s sentences, and we’ll want others based on the words that the author
    uses. We’ll look at each of these in detail.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这个任务的函数`make_signature`将以我们书本的文本作为参数，并返回书本的签名。在此时，我们需要决定将用来确定每本书签名的特征。我们可以通过回顾之前的示例段落来拆解这个问题。我们注意到，作者的段落在句子复杂度和长度上有所不同。你可能也已经猜到，作者们在使用的单词长度和单词的使用方式上也会有所不同（例如，有些作者可能比其他作者更加重复）。因此，我们需要一些特征基于作者句子的结构，另一些则基于作者使用的单词。我们将详细探讨这些特征。
- en: Features related to the structure of the author’s sentences
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 与作者句子结构相关的特征
- en: 'In our earlier Doyle versus Dickens example, we talked about using the average
    number of words per sentence as one feature. We can calculate this by dividing
    the total number of words by the total number of sentences. For example, consider
    the following text:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前提到的道尔与狄更斯的例子中，我们讨论了将每句的单词平均数作为一个特征。我们可以通过将单词总数除以句子总数来计算这一点。例如，考虑以下文本：
- en: The same day there arrived through the post a small card-board box addressed
    to me, which I found to contain a very large and lustrous pearl. No word of writing
    was enclosed.
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 就在那天，一只小纸板盒通过邮寄送到我这里，里面有一颗非常大且光亮的珍珠。我没有发现任何写信的字句。
- en: If you count the words and sentences, you should find that there are 32 words
    (card-board counts as one word) and two sentences, so we’ll calculate the average
    words per sentence as 32/2 = 16\. This will be the*average number of words per
    sentence*feature.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你计算单词和句子的数量，你会发现共有32个单词（card-board算作一个单词）和两句话，因此我们将计算平均每句单词数，结果为32/2 = 16。这将是*平均每句单词数*特征。
- en: 'We also noticed that the complexity of sentences may vary between authors (i.e.,
    some authors have sentences with many more commas and semicolons compared to other
    authors), so it makes sense to use that as another feature. More complex sentences
    have more phrases, which are coherent pieces of sentences. Breaking a sentence
    into its component phrases is a tough challenge in its own right, and although
    we could try to do it more accurately, we’ll settle here for a simpler rule of
    thumb. Namely, we’ll say that a phrase is separated from other phrases in the
    sentence by a comma, semicolon, or colon. Looking at the previous text again,
    we find that there are three phrases. The first sentence has two phrases: “The
    same day there arrived through the post a small card-board box addressed to me”
    and “which I found to contain a very large and lustrous pearl.” The second sentence
    has no commas, semicolons, or colons, so it has only one phrase. As there are
    three phrases and two sentences, we’d say that the sentence complexity for this
    text is 3/2 = 1.5\. This will be the *average sentence complexity* feature.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还注意到，句子的复杂度可能因作者而异（即，有些作者的句子中使用更多的逗号和分号），因此将句子复杂度作为一个特征是合理的。更复杂的句子通常包含更多的短语，而短语是句子中有逻辑关系的部分。将一个句子拆解成其组成的短语本身就是一个挑战，尽管我们可以尝试更准确地进行拆解，但我们将在此采用一个更简便的经验法则。也就是说，我们认为，句子中的短语之间是由逗号、分号或冒号分隔的。再看看之前的文本，我们发现共有三个短语。第一句有两个短语：“The
    same day there arrived through the post a small card-board box addressed to me”和“which
    I found to contain a very large and lustrous pearl”。第二句没有逗号、分号或冒号，因此只有一个短语。由于共有三个短语和两句话，我们可以认为这段文本的句子复杂度为3/2
    = 1.5。这将是*平均句子复杂度*特征。
- en: We hope that these sentence-level features intuitively make sense as things
    we could use to differentiate how authors write. Next, let’s start looking at
    the ways that authors may differ in their use of words.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望这些基于句子的特征在直觉上能合理地帮助我们区分作者的写作风格。接下来，让我们开始看看作者在使用单词时可能的不同之处。
- en: Features related to the author’s word selection
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 与作者的单词选择相关的特征
- en: 'You can probably think of your own metrics for word-level features, but we’ll
    use three here that in our experience work well. First, it’s likely that some
    authors use shorter words on average than other authors. To that end, we’ll use
    the average word length, which is just the average number of letters per word.
    Let’s consider this sample text that we created:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经能想到自己的单词级特征指标，但我们在这里使用三个在我们经验中效果良好的指标。首先，有可能某些作者使用的单词比其他作者更短。为此，我们将使用平均单词长度，即每个单词的平均字母数。让我们考虑这段我们创建的示例文本：
- en: A pearl! Pearl! Lustrous pearl! Rare. What a nice find.
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一颗珍珠！珍珠！光辉的珍珠！稀有。多么好的发现。
- en: If you count the letters and words, you should find that there are 41 letters
    and 10 words. (Don’t count punctuation as letters here.) So, we’ll calculate the
    average word length as 41/10 = 4.1\. This will be the *average word length* feature.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你计算字母和单词，你应该会发现总共有41个字母和10个单词。（这里不要将标点符号算作字母。）所以，我们计算平均单词长度为 41/10 = 4.1。这就是
    *平均单词长度* 特征。
- en: 'Second, it may be that some authors use words more repetitively than others.
    To capture this, we’ll take the number of different words that the author uses
    and divide it by the total number of words. For our previous sample text, there
    are only seven different words used: *a*, *pearl*, *lustrous*, *rare*, *what*,
    *nice*, and *find*. There are 10 words in all, so our calculation for this metric
    would be 7/10 = 0.7\. This will be the*different words divided by total words*feature.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，有些作者可能比其他作者使用更多重复的词。为了捕捉这一点，我们将计算作者使用的不同词汇数，并将其除以总词数。以我们之前的示例文本为例，使用了七个不同的单词：*a*、*pearl*、*lustrous*、*rare*、*what*、*nice*
    和 *find*。总共有10个单词，因此我们计算此指标为 7/10 = 0.7。这就是 *不同词汇数除以总词数* 特征。
- en: 'Third, it may be that some authors tend to use many words a single time, whereas
    other authors tend to use words multiple times. To calculate this one, we’ll take
    the number of words used exactly once and divide it by the total number of words.
    For our sample text, there are five words that are used exactly once: *lustrous*,
    *rare*, *what*, *nice*, and *find*. There are 10 words in all, so our calculation
    for this metric would be 5/10 = 0.5\. This will be the *number of words used exactly
    once divided by total words*feature.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，有些作者可能倾向于使用很多一次性单词，而其他作者则倾向于重复使用单词。为了计算这一点，我们将计算那些仅使用一次的单词数，并将其除以总词数。以我们的示例文本为例，有五个单词仅使用一次：*lustrous*、*rare*、*what*、*nice*
    和 *find*。总共有10个单词，因此我们计算此指标为 5/10 = 0.5。这就是 *仅使用一次的单词数除以总词数* 特征。
- en: In all, we have five features that will make up each signature. We’ll need to
    store those numbers together in a single value, so we’ll end up using a list of
    five numbers for each signature.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，我们有五个特征将组成每个签名。我们需要将这些数字存储在一个单一值中，因此每个签名将使用一个包含五个数字的列表。
- en: 'Let’s dig into how we’ll implement each of these features, starting with the
    word-level ones and proceeding to the sentence-level ones. We’ll go in this order:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨如何实现这些特征，从单词级别的特征开始，然后转向句子级别的特征。我们将按以下顺序进行：
- en: Average word length
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均单词长度
- en: Different words divided by total words
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同单词数除以总词数
- en: Words used exactly once divided by total words
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅使用一次的单词数除以总词数
- en: Average number of words per sentence
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个句子的平均单词数
- en: Average sentence complexity
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均句子复杂度
- en: For each one, we’ll ultimately end up writing a function. We have an updated
    diagram with function names for each of these five new functions that will help
    us implement `make_signature` in figure 11.2\. Do we need to further break down
    these problems, or are they OK as is? Let’s see!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每一个子任务，我们最终都将编写一个函数。我们有一个更新的图，列出了这些新函数的函数名，帮助我们在图11.2中实现 `make_signature`。我们是否需要进一步细化这些问题，还是它们就这样可以？让我们看看！
- en: '![figure](../Images/11-2.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/11-2.png)'
- en: Figure 11.2 Functions diagram with the additional five subtasks of `make_signature`
  id: totrans-61
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.2 函数图，展示了 `make_signature` 的附加五个子任务
- en: Average word length
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 平均单词长度
- en: The function for this task, `average_word_length`, will take the text of the
    book as a parameter and return the average word length. We might start solving
    this task by using the split method on the text. As a reminder, the `split` method
    is used to split a string into a list of its pieces. By default, `split` will
    split around spaces. The book text is a string, and if we split around spaces,
    we’ll get its words! That’s exactly what we need here. We can then loop through
    that list of words, counting up the number of letters and number of words.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这个任务的函数`average_word_length`将把书籍的文本作为参数，并返回平均单词长度。我们可以通过对文本使用`split`方法来开始解决这个任务。提醒一下，`split`方法用于将一个字符串分割成其各个部分的列表。默认情况下，`split`会以空格为分隔符。书籍文本是一个字符串，如果我们按空格分割，就能得到它的单词！这正是我们需要的。接着，我们可以遍历这个单词列表，统计字母数和单词数。
- en: That’s a good start, but we need to be a little more careful here because we
    don’t want to end up counting nonletters as letters. For example, “pearl” has
    five letters. But so does “pearl.” or “pearl!!” or “(pearl)”. Aha—this sounds
    like a subtask to us! Namely, we can divide out the subtask of cleaning up a word
    into its own function to be used by `average_word_length`. We’ll call that cleanup
    function `clean_word`.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个不错的开始，但我们需要小心一些，因为我们不想把非字母当作字母来计算。例如，“pearl”有五个字母，但“pearl.”或“pearl!!”或“(pearl)”也有五个字母。哈哈，这听起来像是一个子任务！换句话说，我们可以将清理单词的子任务提取到一个独立的函数中，由`average_word_length`来使用。我们将这个清理函数命名为`clean_word`。
- en: There’s another benefit to having our `clean_word` function, and that’s to help
    us identify when a “word” is actually not a word. For example, suppose one of
    our “words” in the text is . . . . When we pass this to `clean_word`, we’ll get
    an empty string back. That signifies that this in fact isn’t a word at all, so
    we won’t count it as such.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个好处是拥有`clean_word`函数，它帮助我们识别“单词”实际上并不是单词。例如，假设文本中的一个“单词”是....当我们将其传递给`clean_word`时，它会返回一个空字符串。这意味着它实际上不是一个单词，因此我们不会将其计入单词数量。
- en: Different words divided by total words
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 不同的单词除以总单词数
- en: The function for this task, `different_to_total`, will take the text of the
    book as a parameter and will return the number of different words used divided
    by the total number of words.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这个任务的函数`different_to_total`将把书籍的文本作为参数，并返回使用的不同单词数量与总单词数量的比例。
- en: As with `average_word_length`, we need to be careful to count only letters,
    not punctuation. But wait—we just talked about a `clean_word` function that we
    needed for `average_word_length`. We can use that function here as well! In fact,
    we’ll use `clean_ word` in most of our five feature tasks. This is the sign of
    a useful general-purpose function! Our top-down design is going well. We can see
    how the `clean_word` function will be called by both functions in our updated
    function diagram in figure 11.3.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 和`average_word_length`一样，我们需要小心，只计算字母，而不包括标点符号。但是等等——我们刚才谈到过`clean_word`函数，它在`average_word_length`中需要用到。我们也可以在这里使用这个函数！事实上，我们将在五个特性任务的大部分中使用`clean_word`。这就是一个有用的通用函数的标志！我们的自顶向下设计进行得很顺利。我们可以在图11.3中的更新函数图中看到，`clean_word`函数将被两个函数调用。
- en: '![figure](../Images/11-3.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/11-3.png)'
- en: Figure 11.3 Functions diagram with two functions, both using our `clean_word`
    function to help
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.3 函数图，展示了两个函数，它们都使用`clean_word`函数来辅助处理
- en: There’s one extra complication here, though, and it involves words like *pearl*,
    *Pearl*, and *PEARL*. We want to consider those to be the same words, but if we
    simply use string comparisons, they will be treated as different words. One solution
    here is to split this off into another subproblem to convert a string to all lowercase.
    We could also think of this as another part of cleaning up a word, right along
    with removing the punctuation. We’ll go with this second option. What we’ll do,
    then, is make our `clean_word` function not only remove punctuation but also convert
    the word to lowercase.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这里还有一个额外的复杂情况，涉及到像*pearl*、*Pearl*和*PEARL*这样的单词。我们希望将它们视为相同的单词，但如果我们仅仅使用字符串比较，它们会被视为不同的单词。一个解决方案是将其分离为另一个子问题，将字符串转换为全小写字母。我们也可以把这看作是清理单词的另一个部分，就像去除标点符号一样。我们选择第二个选项。那么，我们将做的是让`clean_word`函数不仅去除标点符号，还将单词转换为小写字母。
- en: You might wonder whether we need to split off another subtask here, one that
    determines the number of different words. You could do that, and it wouldn’t be
    a mistake to do so. However, if we persevere without doing that, we’ll see that
    the function remains quite manageable without this additional subtask. Practice
    and experience over time will help you anticipate when a task needs to be further
    broken down.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，是否需要拆分出另一个子任务，来确定不同单词的数量。你可以这么做，且这样做不会错。然而，如果我们不这样做，我们会发现这个函数依然非常可管理，因此我们就不再拆分。随着实践和经验的积累，你会逐渐学会判断哪些任务需要进一步拆解。
- en: Words used exactly once divided by total words
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 仅使用一次的单词数与总单词数的比值
- en: 'The function for this task, `exactly_once_to_total`, will take the text of
    the book as a parameter and will return the number of words used exactly once
    divided by the total number of words. We’re going to need our `clean_word` function
    here as well for reasons similar to why we needed it in the prior two tasks: to
    make sure we’re working only with letters, not punctuation. Again, while we could
    split out a subtask to determine the number of words that are used exactly once,
    we’ll find that it doesn’t take much Python code to do this, so we’ll just leave
    this task alone without splitting it further.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这个任务的函数`exactly_once_to_total`将以书籍的文本作为参数，返回仅使用一次的单词数与总单词数的比值。我们还需要使用`clean_word`函数，原因与前两个任务类似：确保我们只处理字母，而不是标点符号。同样，尽管我们可以拆分一个子任务来确定仅使用一次的单词数，但我们会发现，编写这段Python代码其实并不复杂，因此我们将直接完成这个任务，不再进一步拆分。
- en: Average number of words per sentence
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 每个句子的平均单词数
- en: The function for this task, `average_sentence_length`, will take the text of
    the book as a parameter and will return the average number of words per sentence.
    To split our text into words for the previous three tasks, we can use the string
    split method. How do we split our text into sentences? Is there a string method
    for that?
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这个任务的函数`average_sentence_length`将以书籍的文本作为参数，返回每个句子的平均单词数。为了分割我们文本中的单词，前面三个任务我们使用了字符串的split方法。那么，我们该如何将文本分割成句子呢？有没有字符串方法可以做到这一点？
- en: Unfortunately, there isn’t. For that reason, it will be helpful to split out
    a task to break our text string into sentences. We’ll call the function for that
    subtask `get_ sentences`. The `get_sentences` function will take the text of the
    book as a parameter and will return a list of sentences from the text.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，没有。为此，将有助于拆分一个任务，将我们的文本字符串分割成句子。我们将这个子任务的函数命名为`get_sentences`。`get_sentences`函数将以书籍的文本作为参数，并返回一个由文本中句子组成的列表。
- en: What’s a sentence? We’ll define a sentence as text that is separated by a period
    (.), question mark (?), or exclamation point (!). This rule, while convenient
    and simple, is going to make mistakes. For example, how many sentences are in
    this text?
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是句子？我们将句子定义为由句号（.）、问号（?）或感叹号（!）分隔的文本。这个规则虽然方便且简单，但会犯错误。例如，这段文本有多少个句子？
- en: I had at that time just entered the family of Mrs. Cecil Forrester in the capacity
    of governess.
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当时，我刚刚以家庭教师的身份进入塞西尔·福雷斯特夫人的家庭。
- en: The answer is one. Our program, though, is going to pull out two sentences,
    not one. It’ll get tricked by the word *Mrs.*, which has a period at the end.
    If you continue with authorship identification past this chapter, you could work
    on making your rules more robust or, use sophisticated natural language processing
    (NLP) software to do even better. For our purposes, however, we’ll be content
    with this rule that sometimes gets sentences wrong because most of the time we’ll
    get them right. If we’re only wrong once in a while, the errors won’t have an
    appreciable effect on our metric.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是有的。然而，我们的程序将会提取出两个句子，而不是一个。它会被像*Mrs.*这样的词误导，因为它以一个句号结尾。如果你在这一章后继续做作者身份识别，你可以尝试让规则更健壮，或者使用复杂的自然语言处理（NLP）软件来做得更好。然而，对于我们的目的来说，我们会满足于这个规则，尽管它有时会将句子识别错误，因为大多数时候我们会识别正确。如果我们偶尔出错一次，这些错误对我们的度量标准不会产生显著影响。
- en: Average sentence complexity
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 平均句子复杂度
- en: We’ll name the function for this task `average_sentence_complexity`. It will
    take the text of a sentence as a parameter and return a measure of the sentence
    complexity.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这个任务的函数我们命名为`average_sentence_complexity`。它将以一句话的文本作为参数，并返回句子复杂度的度量。
- en: As we discussed previously, we’re interested in quantifying sentence complexity
    using the number of phrases in a sentence. Much as we used punctuation to separate
    sentences from each other, we’ll use different punctuation to separate phrases
    from each other. Namely, we’ll say that a phrase is separated by a comma (,),
    semicolon (;), or colon (:).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们对使用句子中的短语数量来量化句子复杂性感兴趣。就像我们用标点符号将句子分开一样，我们将使用不同的标点符号来将短语分开。也就是说，我们认为短语是由逗号（,）、分号（;）或冒号（:）分隔的。
- en: It would be nice to have a subtask to break a sentence into its phrases, just
    like we had a subtask to break text into its sentences. Let’s make that happen!
    We’ll call the function for that subtask `get_phrases`. The `get_phrases` function
    will take a sentence of the book as a parameter and return a list of phrases from
    the sentence.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个子任务来将句子分解成短语，像我们曾经有一个子任务来将文本分解成句子一样，那将是很好的。我们来实现这个目标！我们将为这个子任务创建一个名为`get_phrases`的函数。`get_phrases`函数将接受书中的一段句子作为参数，并返回从句子中分离出的短语列表。
- en: Let’s pause for a moment and think about what we’re doing with our `get_sentences`
    and `get_phrases` functions. They’re both quite similar, come to think of it.
    All that distinguishes them is the characters that they use to make the splits.
    `get_sentences` cares about periods, question marks, and exclamation points, whereas
    `get_phrases` cares about commas, semicolons, and colons. We see an opportunity
    for a parent task that would simplify both of these tasks!
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们暂停一下，思考一下我们正在做的`get_sentences`和`get_phrases`函数的工作。想一想，这两个函数其实非常相似。它们唯一的区别在于用来分隔文本的字符。`get_sentences`关心句号、问号和感叹号，而`get_phrases`关心逗号、分号和冒号。我们看到了一个可以简化这两个任务的父任务的机会！
- en: Namely, imagine that we had a `split_string` function that took two parameters,
    the text and a string of separator characters, and it returned a list of pieces
    of text separated by any of the separators. We could then call it with `'.?!'`
    to split into sentences and `',;:'` to split into phrases. That would make both
    `get_sentences` and `get_phrases` easier to implement and reduce code duplication.
    This is a win!
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，假设我们有一个`split_string`函数，它接受两个参数：文本和一个分隔字符的字符串，并返回一个由这些分隔符分隔开的文本片段列表。我们可以用`'.?!'`来分割句子，或者用`,;:`来分割短语。这样，`get_sentences`和`get_phrases`的实现就会更简单，且能减少代码重复。这是一个胜利！
- en: At this point, we’ve fully fleshed out the functions necessary to support the
    higher-level function `make_signature`, as reflected in figure 11.4\. We’ll next
    turn to the `get_all_signatures` function.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经完整地展开了支持`make_signature`高级函数所需的所有函数，这一点在图11.4中有所体现。接下来我们将转向`get_all_signatures`函数。
- en: '![figure](../Images/11-4.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/11-4.png)'
- en: Figure 11.4 Functions diagram with all the supporting functions for the `make_signature`
    function complete
  id: totrans-89
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.4 完整的`make_signature`函数支持函数图
- en: Figuring out each known signature
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 计算每个已知签名
- en: We just worked hard to break down our `make_signature` function into five main
    tasks, one for each feature of our signatures. We designed that function to determine
    the unknown signature—the signature for the mystery text whose author we’re trying
    to identify.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚努力将`make_signature`函数拆解成五个主要任务，每个任务对应签名的一个特征。我们设计这个函数是为了确定未知签名——即我们正在尝试识别作者的神秘文本的签名。
- en: Our next task is to figure out the signature for each of the books for which
    we know the author. In the resources for this book, under the ch11 folder, you’ll
    find a directory called `known_authors`. In there, you’ll find several files,
    each named as an author. Each file contains a book written by that author. For
    example, if you open Arthur_Conan_Doyle.txt, you’ll find the text of the book
    *A Study in Scarlet* by Arthur Conan Doyle. We need to determine the signature
    for each of these files.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的任务是找出每本我们知道作者的书籍的签名。在本书的资源文件夹中的ch11目录下，你会找到一个名为`known_authors`的目录。在这个目录下，你会发现几个文件，每个文件以作者的名字命名。每个文件包含该作者的书籍。例如，如果你打开Arthur_Conan_Doyle.txt，你会看到由亚瑟·柯南·道尔（Arthur
    Conan Doyle）所著的《血字的研究》（*A Study in Scarlet*）的文本。我们需要为这些文件确定签名。
- en: Amazingly, we have far less work to do to solve this problem than it may seem.
    That’s because we can use that same `make_signature` function, the one we designed
    to determine the signature of the mystery book, to also determine the signature
    for any known book!
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，解决这个问题的工作量比看起来要少得多。这是因为我们可以使用相同的`make_signature`函数，那个我们为确定神秘书籍的签名而设计的函数，也能用来确定任何已知书籍的签名！
- en: We’ll name the function for this task `get_all_signatures`. It wouldn’t make
    sense for this function to take the text of one book as a parameter because it’s
    supposed to be able to get the signature for all of our known books. Rather, it
    will take a directory of known books as a parameter. Its behavior will be to loop
    through the files in that directory, calculating the signature for each one.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为这个任务命名函数为`get_all_signatures`。让这个函数接受一本书的文本作为参数是没有意义的，因为它应该能够获取我们所有已知书籍的签名。相反，它将接受一个已知书籍目录作为参数。它的行为是遍历该目录中的文件，为每个文件计算签名。
- en: We need the function to tell us which signature goes with which book. In other
    words, we need it to associate each book with its corresponding signature. This
    kind of association is precisely why Python has dictionaries! We’ll therefore
    have this function return a dictionary, where the keys are names of files, and
    the values are the corresponding signature. Our function diagram didn’t need any
    *new* functions to support the `get_all_signatures` function, so our updated diagram
    in figure 11.5 just shows how `get_all_signatures` calls `make_signature`.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要这个函数告诉我们哪个签名对应哪本书。换句话说，我们需要它将每本书与其对应的签名关联起来。正因为如此，Python才有字典！因此，我们让这个函数返回一个字典，其中键是文件名，值是对应的签名。我们的函数图并不需要任何*新*函数来支持`get_all_signatures`函数，因此我们在图11.5中的更新图只展示了`get_all_signatures`如何调用`make_signature`。
- en: '![figure](../Images/11-5.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/11-5.png)'
- en: Figure 11.5 Functions diagram updated for `get_all_signatures` to call `make_signature`
  id: totrans-97
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.5 函数图已经更新，`get_all_signatures`调用`make_signature`
- en: Finding closest known signature
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 寻找最接近的已知签名
- en: 'Let’s recap what we’ve designed so far:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下我们目前设计的内容：
- en: We’ve designed our `make_signature` function to get us the unknown signature
    for the mystery book.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们设计了`make_signature`函数，以便获取神秘书籍的未知签名。
- en: We’ve designed our `get_all_signatures` function to get us all of our known
    signatures.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们设计了`get_all_signatures`函数，以便获取我们所有已知的签名。
- en: 'Now, we need to design a function that tells us which of those known signatures
    is best; that is, which known signature is closest to our unknown signature. Each
    of our signatures will be a list of five numbers giving the quantity for each
    of our five features. The order of these numbers will be the same order we used
    before: average word length, different words divided by total words, words used
    exactly once divided by total words, average number of words per sentence, and
    average sentence complexity.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要设计一个函数，告诉我们哪个已知签名最好；也就是说，哪个已知签名最接近我们的未知签名。我们每个签名将是一个包含五个数字的列表，表示每个特征的数量。这些数字的顺序将与之前使用的顺序相同：平均单词长度、不同单词占总单词的比例、仅出现一次的单词占总单词的比例、每个句子的平均单词数，以及平均句子复杂度。
- en: Suppose that we have two signatures. The first one is `[4.6,` `0.1,` `0.05,`
    `10,` `2]`which means that the average word length for that book is 4.6, the different
    words divided by total words is 0.1, and so on. The second signature is `[4.3,`
    `0.1,` `0.04,` `16,` `4]`.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有两个签名。第一个签名是`[4.6,` `0.1,` `0.05,` `10,` `2]`，这意味着该书的平均单词长度为4.6，不同单词占总单词的比例为0.1，依此类推。第二个签名是`[4.3,`
    `0.1,` `0.04,` `16,` `4]`。
- en: There are many ways to get an overall score giving the difference between signatures.
    The one we’ll use will give us a difference score for each feature, and then we’ll
    add up those scores to get our overall score.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多种方法可以通过比较签名的差异来得出整体评分。我们将使用的方法是给每个特征一个差异评分，然后将这些评分加起来得到我们的整体评分。
- en: 'Let’s look at the values of each signature for the first feature: 4.6 and 4.3\.
    If we subtract those, we get a difference of 4.6 – 4.3 = 0.3\. We could use 0.3
    as our answer for this feature, but it turns out to work better if we *weight*
    each difference using a different weight. Each weight gives the importance of
    that feature. We’ll use some weights(`[11,` `33,` `50,` `0.4,` `4]`) that in our
    experience have proven to work well. You might wonder where the heck these weights
    come from. But note that there’s no magic about them: in working with our students
    over the years, we’ve just found that these weights seem to work out. This would
    be only a starting point for a stronger authorship identification program. When
    doing this type of research, people routinely *tune* their training, which means
    to adjust weights to obtain stronger results.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下每个签名在第一个特征上的值：4.6和4.3。如果我们相减，得到的差值是4.6 – 4.3 = 0.3。我们可以将0.3作为这个特征的答案，但实际上，如果我们使用不同的权重来对每个差值进行*加权*，效果会更好。每个权重表示该特征的重要性。我们将使用一些权重（`[11,`
    `33,` `50,` `0.4,` `4]`），这些在我们的经验中证明效果良好。你可能会好奇这些权重到底从哪里来。但请注意，它们并没有什么神秘之处：多年来与我们的学生合作，我们发现这些权重似乎有效。这只是一个强大的作者识别程序的起点。在进行这种类型的研究时，人们通常会*调整*他们的训练，也就是调整权重，以获得更强的结果。
- en: When we say that we’re using weights of `[11,` `33,` `50,` `0.4,` `4]`, it means
    that we’ll multiply the difference on the first feature by 11, the difference
    on the second feature by 33, and so on. So, rather than getting a difference of
    0.3 for the first feature, we’ll get 0.3 × 11 = 3.3.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们说我们使用权重`[11,` `33,` `50,` `0.4,` `4]`时，这意味着我们会将第一个特征的差异乘以11，第二个特征的差异乘以33，依此类推。因此，我们不会仅仅得到第一个特征的差值0.3，而是得到0.3
    × 11 = 3.3。
- en: We need to be careful with features like the fourth one, where the difference
    is negative. We don’t want to start with 10 – 16 = –6 because that’s a negative
    number, and that would *undo* some of the positive difference from other features.
    Instead, we need to first make this number positive and then multiply it by its
    weight. Removing the negative sign from a number is known as taking the absolute
    value, and the absolute value is denoted as `abs`. The full calculation for this
    fourth feature, then, is abs(10 – 16) × 0.4 = 2.4.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要小心像第四个特征这样的特征，其中差异是负数。我们不希望从10 – 16 = -6开始，因为这是一个负数，这会*抵消*其他特征的正差异。相反，我们需要先将这个数变为正数，然后再乘以它的权重。去掉数字的负号称为取绝对值，绝对值用`abs`表示。那么，第四个特征的完整计算是abs(10
    – 16) × 0.4 = 2.4。
- en: Table 11.1 gives the calculation for each feature. If we add up all five scores,
    we get an overall score of 14.2.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 表11.1给出了每个特征的计算。如果我们将五个得分加起来，我们得到一个总得分14.2。
- en: Table 11.1 Calculating the difference between two signatures
  id: totrans-109
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表11.1 计算两个签名之间的差异
- en: '| Feature Number | Value of Feature in Signature 1 | Value of Feature in Signature
    2 | Weight of Feature | Contribution of Feature |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 特征编号 | 签名1中该特征的值 | 签名2中该特征的值 | 特征的权重 | 特征的贡献 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 1  | 4.6  | 4.3  | 11  | abs(4.6 – 4.3) × 11 = 3.3  |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 1  | 4.6  | 4.3  | 11  | abs(4.6 – 4.3) × 11 = 3.3  |'
- en: '| 2  | 0.1  | 0.1  | 33  | abs(0.1 – 0.1) × 33 = 0  |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 2  | 0.1  | 0.1  | 33  | abs(0.1 – 0.1) × 33 = 0  |'
- en: '| 3  | 0.05  | 0.04  | 50  | abs(0.05 – 0.04) × 50 = .5  |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 3  | 0.05  | 0.04  | 50  | abs(0.05 – 0.04) × 50 = 0.5  |'
- en: '| 4  | 10  | 16  | 0.4  | abs(10 – 16) × 0.4 = 2.4  |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 4  | 10  | 16  | 0.4  | abs(10 – 16) × 0.4 = 2.4  |'
- en: '| 5  | 2  | 4  | 4  | abs(2 – 4) × 4 = 8  |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 5  | 2  | 4  | 4  | abs(2 – 4) × 4 = 8  |'
- en: '| Sum  |  |  |  | 14.2  |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 总和  |  |  |  | 14.2  |'
- en: 'Remember where we are in the top-down design: we need a function that tells
    us which known signature is best. Well, now we know how to compare two signatures
    and get the score for that comparison. We’ll want to make that comparison between
    the unknown signature and each known signature to determine which known signature
    is best. The lower the score, the closer the signatures; the higher the score,
    the more different the signatures are. As such, we’ll want to ultimately choose
    the signature with the lowest comparison score.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 记住我们在自顶向下设计中的位置：我们需要一个函数来告诉我们哪个已知的签名是最好的。现在，我们知道如何比较两个签名并获取比较得分。我们需要将这种比较应用于未知签名与每个已知签名之间，以确定哪个已知签名是最好的。得分越低，签名越相似；得分越高，签名之间的差异越大。因此，我们最终需要选择得分最低的签名。
- en: 'We’ll name the function for this task `lowest_score`. It will take three parameters:
    a dictionary mapping author names to their known signatures, an unknown signature,
    and a list of weights. The function will return the signature that has the lowest
    comparison score with our unknown signature.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为这个任务命名函数为`lowest_score`。它将接受三个参数：一个字典，将作者名映射到他们已知的签名，一个未知的签名，以及一个权重列表。该函数将返回与我们的未知签名比较得分最低的签名。
- en: Think about the work that this function will need to do. It needs to loop through
    the known signatures. We can do that with a `for` loop—no need for a subtask there.
    It will need to compare the unknown signature to the current known signature.
    Oh! That’s a subtask right there, embodying the scoring mechanism that we outlined
    in table 11.1\. We’ll name the function for that subtask `get_score`. Our `get_score`
    function will take two signatures to compare and the list of weights and return
    the score for the comparison between these two signatures.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 想一想这个函数需要做的工作。它需要遍历已知的签名。我们可以用`for`循环来实现——这里不需要子任务。它将需要将未知签名与当前已知签名进行比较。哦！那就是一个子任务，它体现了我们在表11.1中概述的评分机制。我们将为这个子任务命名函数为`get_score`。我们的`get_score`函数将接受两个签名进行比较，以及权重列表，并返回这两个签名比较的得分。
- en: 11.4 Summary of our top-down design
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.4 我们自上而下设计的总结
- en: We did it! We’ve broken down our original big problem into several smaller problems
    that are amenable to being implemented as a function.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们做到了！我们将原本的大问题分解成了几个较小的问题，这些问题可以实现为一个函数。
- en: Figure 11.6 depicts all the work that we’ve done during the process of decomposing
    the problem. Remember, we started with a `make_guess` function, which will solve
    the overall problem. To help us with `make_guess`, we created a `process_data`
    function that will do some of the work for `make_guess`. To help `process_data`,
    we created three more functions, `make_signature`, `get_all_signatures`, and `lowest_score`,
    that each have their own helper functions, and so forth. Having sketched out the
    functions we’ll need to solve our problem, our next step will be to implement
    them.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.6展示了我们在分解问题过程中所做的所有工作。记住，我们从`make_guess`函数开始，它将解决整体问题。为了帮助我们实现`make_guess`，我们创建了一个`process_data`函数，它将为`make_guess`做一些工作。为了帮助`process_data`，我们又创建了三个函数，`make_signature`、`get_all_signatures`和`lowest_score`，它们各自有自己的辅助函数，等等。在勾画出我们为解决问题所需的函数后，我们的下一步将是实现它们。
- en: '![figure](../Images/11-6.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/11-6.png)'
- en: Figure 11.6 Full functions diagram for `make_guess`
  id: totrans-125
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.6 `make_guess`的完整函数图
- en: 11.5 Implementing our functions
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.5 实现我们的函数
- en: Now we’re ready to ask Copilot to implement each function that we need. We designed
    our functions by starting from the top—the biggest problem—and working down to
    smaller problems. But remember from chapter 7 that this isn’t the order that we
    implement the functions; instead, we implement the functions in the opposite order,
    from bottom to top (or right to left in figure 11.6).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备好让Copilot实现我们所需的每个函数。我们通过从顶部开始——最大的问题——然后向下分解为更小的问题来设计我们的函数。但请记住，在第7章中提到过，这并不是我们实现函数的顺序；相反，我们应该按照相反的顺序实现函数，从底部到顶部（或者在图11.6中从右到左）。
- en: Just as in our example in chapter 7, we’re not going to focus much on testing,
    prompt engineering, debugging, or code reading. We do encourage you to run doctest
    on the docstring tests that we’ve provided, and further encourage you to add additional
    tests for each function.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在第7章中的示例一样，我们不会过多关注测试、提示工程、调试或代码阅读。我们确实鼓励你运行我们提供的文档字符串中的doctest测试，并进一步鼓励你为每个函数添加额外的测试。
- en: 11.5.1 clean_word
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.1 clean_word
- en: We’ll start with our `clean_word` function. As usual, we provide the function
    header (the `def` line) and docstring, and we let Copilot fill in the code. We
    also provide some annotations to briefly illustrate how the code works.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从`clean_word`函数开始。像往常一样，我们提供函数头（`def`行）和文档字符串，让Copilot填充代码。我们还提供了一些注释，简要说明代码的工作原理。
- en: Remember that we want our `clean_word` function to remove punctuation that might
    show up around the word and to convert the word to lowercase. But we don’t want
    to mess with punctuation in the middle of the word, such as the “-” in *card-board*.
    We’ve written the docstring to make clear what we want.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们希望我们的`clean_word`函数去除可能出现在单词周围的标点符号，并将单词转换为小写。但是我们不希望修改单词中间的标点符号，例如*card-board*中的“-”。我们已经写了文档字符串来明确我们想要的效果。
- en: Listing 11.1 Clean words for analysis
  id: totrans-132
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.1 分析的干净单词
- en: '[PRE0]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '#1 Converts the word to lowercase'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 将单词转换为小写'
- en: '#2 Uses the string module to strip punctuation from ends'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 使用 string 模块去除两端的标点符号'
- en: When working on our password functions in chapter 3, we saw Copilot using the
    string module, and we see Copilot doing that again here. We know from our work
    in chapter 3 that this won’t work unless we import string first, so add
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 3 章处理密码功能时，我们看到 Copilot 使用了 string 模块，现在我们再次看到 Copilot 在这里使用它。根据我们在第 3 章的工作，我们知道如果不先导入
    string，这将不起作用，因此请添加
- en: '[PRE1]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: above this function as we’ve done in the following listing.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在以下列表中所做的那样，放置在该函数上方。
- en: Listing 11.2 Clean words for analysis, complete
  id: totrans-139
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.2 用于分析的清理单词，已完成
- en: '[PRE2]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This completes the `clean_word` function, so we can mark this as complete in
    our functions diagram in figure 11.7.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了 `clean_word` 函数，所以我们可以在图 11.7 的函数图表中将其标记为已完成。
- en: '![figure](../Images/11-7.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/11-7.png)'
- en: Figure 11.7 Full functions diagram with `clean_word` now finished
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.7 完整的函数图表，`clean_word` 已完成
- en: 11.5.2 average_word_length
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.2 average_word_length
- en: 'Now let’s tackle the first of our five signature feature functions: `average_word_length`.
    It needs to determine the average number of letters per word, but we don’t want
    to count surrounding punctuation as letters nor include words that don’t have
    any letters. We want to use our `clean_word` function here, as shown in the following
    listing. As always, we’ve written the docstring in a way that we hope directs
    Copilot to make these decisions.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们处理五个标志性特性函数中的第一个：`average_word_length`。它需要确定每个单词的平均字母数，但我们不希望将周围的标点符号算作字母，也不包括没有字母的单词。我们希望在这里使用我们的`clean_word`函数，如以下列表所示。和往常一样，我们已经编写了文档字符串，希望能引导
    Copilot 做出这些决策。
- en: Listing 11.3 Average word length
  id: totrans-146
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.3 平均单词长度
- en: '[PRE3]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '#1 Splits string into its words'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 将字符串分割成单词'
- en: '#2 total will count the total number of letters across all words.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 total 将计算所有单词中字母的总数。'
- en: '#3 count will count the number of words.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 count 将计算单词数。'
- en: '#4 Loops through each word'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 遍历每个单词'
- en: '#5 Copilot calls clean_word for us!'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 Copilot 为我们调用 clean_word！'
- en: '#6 Considers this word only if it isn’t empty'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 仅在单词不为空时考虑此单词'
- en: '#7 Adds number of letters in word'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 添加单词中字母的数量'
- en: '#8 Adds 1 to count this word'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 给这个单词的计数加 1'
- en: '#9 Returns number of letters divided by number of words'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '#9 返回字母数除以单词数'
- en: You’ll notice in the doctest here that we’ve split our string over two lines,
    ending the first line with a \ character. The reason we did this is that the string
    wouldn’t otherwise fit on one line in the book. We also needed to keep the second
    line without any indentation; otherwise, doctest would use that indentation as
    spaces in the string. On your computer, you can type the string on a single line
    and not worry about the \ or lack of indentation.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到在这里的 doctest 中，我们将字符串拆分为两行，第一行以 `\` 字符结尾。我们这么做的原因是，字符串否则无法在书中适应一行。我们还需要保持第二行没有任何缩进；否则，doctest
    会将那个缩进作为字符串中的空格。在你的计算机上，你可以将字符串输入为一行，而无需担心 `\` 或缺少缩进。
- en: We can now mark `average_word_length` as done in our updated figure (figure
    11.8). Although satisfying, marking these off in the figure one by one might be
    a bit too much noise, so we’ll revisit the figure only periodically going forward.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以在更新后的图表（图 11.8）中标记 `average_word_length` 为完成。尽管这样做令人满意，但逐个标记这些功能可能会造成太多干扰，因此我们将仅定期回顾图表。
- en: '![figure](../Images/11-8.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/11-8.png)'
- en: Figure 11.8 Full functions diagram with `average_word_length` now finished
  id: totrans-160
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.8 完整的函数图表，`average_word_length` 已完成
- en: 11.5.3 different_to_total
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.3 different_to_total
- en: This is the second of our signature features. We need this one to calculate
    the number of different words used divided by the total number of words. Again,
    we don’t want surrounding punctuation or empty words.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的第二个标志性特性。我们需要这个功能来计算不同单词的数量除以总单词数。同样，我们不希望包含周围的标点符号或空单词。
- en: Listing 11.4 Different words divided by total number of words
  id: totrans-163
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.4 按总词数划分的不同单词
- en: '[PRE4]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '#1 Splits string into its words'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 将字符串分割成单词'
- en: '#2 total will count the total number of nonempty words.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 total 将计算所有非空单词的总数。'
- en: '#3 A set of the unique words found'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 一组唯一的单词'
- en: '#4 Copilot again calls clean_word for us!'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 Copilot 再次为我们调用 clean_word！'
- en: '#5 A set is like a list but doesn’t accept duplicates.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 集合像列表一样，但不接受重复项。'
- en: '#6 Returns the number of different words divided by total number of words'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 返回不同单词的数量除以总单词数'
- en: This code uses a Python set, rather than a Python list, to store the unique
    words that it finds. A set is similar to a list except that it doesn’t accept
    duplicates, so if you try to add the same word multiple times, it retains only
    one copy of the word. That’s why we can use `len(unique)` in the return statement
    to get the total number of unique words.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码使用 Python 集合，而不是 Python 列表，来存储它找到的唯一单词。集合类似于列表，但它不允许重复，因此如果你尝试多次添加相同的单词，它只会保留一个副本。这就是为什么我们可以在返回语句中使用
    `len(unique)` 来获取唯一单词的总数。
- en: We don’t need to know any more about Python sets to continue here. But if you’re
    interested in learning more, you can ask Copilot for a code explanation and/or
    do a Google search for information on Python sets.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们不需要了解更多关于 Python 集合的知识。但如果你有兴趣了解更多，你可以向 Copilot 请求代码解释，或在 Google 上搜索关于
    Python 集合的信息。
- en: 11.5.4 exactly_once_to_total
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.4 exactly_once_to_total
- en: We’re ready to implement our third signature feature. This one calculates the
    number of words used exactly once divided by the total number of words, as shown
    in the following listing. We provided the prompt to Copilot and received the following
    function.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备实现我们的第三个签名功能。这个功能计算了仅使用一次的单词的数量与总单词数量的比值，如下所示。我们向 Copilot 提供了提示，并收到了以下函数。
- en: 'Listing 11.5 Fraction of words used just once: Try 1'
  id: totrans-175
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.5 仅使用一次的单词的比例：尝试 1
- en: '[PRE5]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '#1 Removing a word from the unique set?'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 从唯一集合中移除单词？'
- en: When we skim that code, it looks like `unique` is a set that keeps track of
    the unique words. At the end, we divide that number of unique words by the total
    number of words.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们快速浏览代码时，看起来 `unique` 是一个集合，用于跟踪唯一的单词。在最后，我们将唯一单词的数量除以总单词的数量。
- en: However, run our test case using doctest, and you’ll see that our code fails
    the test. We need the result to be 0.5, but our code is unfortunately giving us
    0.6\. Our code is wrong. We need to stay vigilant!
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当我们使用 doctest 运行我们的测试用例时，你会看到我们的代码没有通过测试。我们需要的结果是 0.5，但我们的代码不幸给出了 0.6。我们的代码是错误的，我们需要保持警惕！
- en: Let’s review the code to see why it was wrong. Imagine that a word is used three
    times in a text. The first time our code finds it, the code will add it to `unique`.
    The second time, it will *remove* it from `unique`. That’s fine so far because
    the word isn’t being used exactly once anymore, so it shouldn’t be in there. But
    then the third time, it will add that word back to`unique`! So, our code is going
    to count some words as being used exactly once when in fact they were used more
    than once. We looked through the list that appears when you press Ctrl-Enter and
    found this alternate version, which is correct, as shown in the following listing.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下代码，看看为什么它是错误的。假设某个单词在文本中出现了三次。当我们的代码第一次找到这个单词时，它会将其添加到`unique`中。第二次，它会从`unique`中*移除*该单词。到目前为止，这没问题，因为该单词不再是只使用一次，所以它不应该在其中。但第三次，它会将这个单词再次添加到`unique`中！因此，我们的代码会将一些本应计算为只使用一次的单词错误地计算为使用了多次。我们查看了按
    Ctrl-Enter 按下时出现的列表，找到了这个正确的替代版本，如下所示。
- en: 'Listing 11.6 Fraction of words used just once: Try 2'
  id: totrans-181
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.6 仅使用一次的单词的比例：尝试 2
- en: '[PRE6]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#1 Keeps track of all words we’ve seen'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 跟踪我们看到的所有单词'
- en: '#2 Keeps track of words used exactly once—what we ultimately care about'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 跟踪仅使用一次的单词——这是我们最终关心的'
- en: '#3 We’re seeing the word again.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 我们再次看到了这个单词。'
- en: '#4 So the word has to be removed from once.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 所以这个单词必须从仅使用一次的集合中移除。'
- en: '#5 We’ve now seen this word.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 我们现在已经看到了这个单词。'
- en: '#6 So far, the word is used exactly once.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 到目前为止，这个单词只使用了一次。'
- en: '#7 Returns the number of words used exactly once divided by the total number
    of words'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 返回仅使用一次的单词数量与总单词数量的比值'
- en: This code is tricky! To understand it, start by focusing on the `else` code.
    That’s the code that runs the first time we see each word. That word gets added
    to both the `unique` and `once` sets. It’s the `once` set that’s going to keep
    track for us of the words used exactly once.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码很棘手！要理解它，首先要关注 `else` 代码。这是我们第一次看到每个单词时运行的代码。这个单词会同时被添加到 `unique` 和 `once`
    集合中。正是 `once` 集合会帮助我们跟踪仅使用一次的单词。
- en: 'Now imagine that we see a word for a second time. The `if` code is going to
    run when this happens because the word is already in `unique` (we added it there
    the first time we saw this word). Now, because we’ve seen the word more than once,
    we need it gone from the `once` set. That’s exactly what the `if` code does: it
    uses `once.discard(word)` to remove the word from `once`.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们第二次遇到一个单词。由于这个单词已经在 `unique` 中（我们第一次遇到这个单词时将它添加到了 `unique`），`if` 代码会在这种情况下运行。现在，由于我们已经遇到该单词多次，我们需要将它从
    `once` 集中移除。这正是 `if` 代码所做的：它使用 `once.discard(word)` 将该单词从 `once` 中移除。
- en: To summarize, the first time we see a word, it gets added to `once`. When we
    see it again, it gets removed from `once` with no way to ever have that word added
    back to `once`. The `once` set is correctly tracking the words used exactly once.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，第一次遇到一个单词时，它会被添加到 `once` 中。当我们再次遇到它时，它会从 `once` 中移除，并且永远不会再被添加回 `once`。`once`
    集正确地跟踪了仅出现一次的单词。
- en: 11.5.5 split_string
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.5 split_string
- en: We’ve finished our three word-level signature feature functions. Before we can
    move on to our two sentence-level signature feature functions, we need to write
    `get_ sentences`. But to write `get_sentences`, we first need `split_string`,
    which is what we’ll work on now.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了三个基于单词级别的特征函数。在我们能够继续处理两个基于句子级别的特征函数之前，我们需要编写 `get_sentences`。但要编写 `get_sentences`，我们首先需要
    `split_string`，这就是我们现在要处理的内容。
- en: 'Our `split_string` function is supposed to be able to split a string around
    any number of separators. It inherently has nothing to do with sentences or phrases.
    We’ve included one docstring test to highlight this fact: even though we’re going
    to use it to split sentences and phrases, it’s more general than that. Look at
    the following listing.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 `split_string` 函数应该能够根据任意数量的分隔符拆分字符串。它本身与句子或短语无关。我们已经包括了一个文档字符串测试来突出这一点：尽管我们将用它来拆分句子和短语，但它比这更通用。请看下面的列表。
- en: Listing 11.7 Split a string around separators
  id: totrans-196
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.7 根据分隔符拆分字符串
- en: '[PRE7]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '******#1 A better variable name would be all_strings.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '******#1 一个更好的变量名应该是 all_strings。'
- en: '#2 A better variable name would be current_string.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 一个更好的变量名应该是 current_string。'
- en: '#3 Current string ends here.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 当前字符串在此结束。'
- en: '#4 Removes any space from beginning and end of current string'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 移除当前字符串开头和结尾的空格'
- en: '#5 If the current string isn’t empty . . .'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 如果当前字符串不为空 . . .'
- en: '#6 . . . saves this as one of the split strings.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 . . . 将其保存为拆分字符串之一。'
- en: '#7 Clears the current string to get ready for the next one'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 清空当前字符串，为下一个字符串做准备'
- en: '#8 Adds to the current string (don’t split yet)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 添加到当前字符串（暂不拆分）'
- en: '#9 Handles the final split string by adding if not empty******  ******You might
    be curious about the code after the `for` loop and before the `return` statement.
    It seems to be duplicating some of the code from within the `for` loop, so what’s
    it doing there? This code is there because the loop only adds a split string to
    our list of strings when it finds one of the separator characters. If the text
    doesn’t end with a separator character, the loop won’t add the final split string.
    The code below the loop ensures that this final split string isn’t lost.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '#9 处理最终的拆分字符串，若不为空******  ******你可能会对 `for` 循环之后和 `return` 语句之前的代码感到好奇。它似乎在重复
    `for` 循环中的一些代码，那么它在那里做什么呢？这些代码存在是因为循环只有在找到分隔符字符时，才会将拆分字符串添加到我们的字符串列表中。如果文本没有以分隔符字符结尾，循环就不会添加最终的拆分字符串。循环下方的代码确保了这个最终的拆分字符串不会丢失。'
- en: It’s been a little while since we updated our diagram with functions we’ve completed.
    Time for an update! This also serves as a reminder that we’re finishing functions
    from the bottom up (right-to-left in the diagram). As such, figure 11.9 has our
    functions completed so far.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 自从我们更新了函数图后已经有一段时间了，是时候更新了！这也提醒我们，我们是从底层向上（图中的从右到左）完成函数的。因此，图 11.9 展示了我们迄今完成的所有函数。
- en: '![figure](../Images/11-9.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/11-9.png)'
- en: Figure 11.9 Full functions diagram updated with `different_to_total`, `exactly_once_to_total`,
    and `split_string` now finished
  id: totrans-209
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.9 完整的函数图已更新，包含 `different_to_total`、`exactly_once_to_total` 和现已完成的 `split_string`
- en: 11.5.6 get_sentences
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.6 get_sentences
- en: In our top-down design, we kicked most of the work for `get_sentences` off to
    the `split_string` function. Therefore, what we’re hoping for is that Copilot
    implements `get_sentences` by making a suitable call to `split_string`.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的自上而下设计中，我们将大部分 `get_sentences` 的工作委派给了 `split_string` 函数。因此，我们期望的是 Copilot
    通过合适的调用来实现 `get_sentences`，并调用 `split_string`。
- en: We don’t want our sentences to have any space at the beginning or end, and we
    don’t want any empty sentences. We initially had these requirements in the docstring,
    but even without them Copilot correctly calls `split_string`, and that function
    handles these edge cases anyway.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不希望句子开头或结尾有空格，也不希望有空句子。最初我们在文档字符串中有这些要求，但即使没有这些要求，Copilot也正确地调用了`split_string`，该函数本身也能处理这些边缘情况。
- en: Listing 11.8 Return list of sentences in text
  id: totrans-213
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11.8 返回文本中的句子列表
- en: '[PRE8]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '#1 Calls split_string with the sentence separator characters'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 使用句子分隔符字符调用split_string'
- en: 11.5.7 average_sentence_length
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.7 平均句子长度
- en: Now that we have `get_sentences`, we can proceed with `average_sentence_length`,
    our fourth signature feature. This one calculates the average number of words
    per sentence. We provided the prompt to Copilot and received the function shown
    in the following listing.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了`get_sentences`，可以继续实现我们的第四个签名特征`average_sentence_length`。这个函数计算每个句子的平均单词数。我们向Copilot提供了提示，并得到了如下的函数。
- en: Listing 11.9 Average number of words per sentence
  id: totrans-218
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11.9 每个句子的平均单词数
- en: '[PRE9]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '#1 Gets a list of sentences'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 获取句子列表'
- en: '#2 Loops through the sentences'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 遍历句子'
- en: '#3 Breaks current sentence into its words'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 将当前句子分解成单词'
- en: '#4 Loops through sentence’s words'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 遍历句子的单词'
- en: '#5 Returns number of words divided by number of sentences'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 返回单词数除以句子数'
- en: 11.5.8 get_phrases
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.8 get_phrases
- en: Much liked we needed `get_sentences` before we could implement `average_sentence_
    length`, we need `get_phrases` before we can implement `average_sentence_complexity`.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在实现`average_sentence_length`之前需要`get_sentences`一样，我们在实现`average_sentence_complexity`之前也需要`get_phrases`。
- en: As with `get_sentences`, we expect Copilot to call `split_string` to get the
    phrases. That is what it has done for us here, as shown in the following listing.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 与`get_sentences`一样，我们期望Copilot调用`split_string`来获取短语。正如它在这里所做的那样，下面的示例展示了这一过程。
- en: Listing 11.10 Return list of phrases from a sentence
  id: totrans-228
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11.10 返回句子的短语列表
- en: '[PRE10]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '#1 Calls split_string with the phrase separator characters'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 使用短语分隔符字符调用split_string'
- en: 11.5.9 average_sentence_complexity
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.9 平均句子复杂度
- en: With `get_phrases` completed, we can now prompt for an implementation of `average_
    sentence_complexity`. The code is shown in the following listing.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 完成`get_phrases`之后，我们可以提示实现`average_sentence_complexity`。代码如下所示。
- en: Listing 11.11 Average number of phrases per sentence
  id: totrans-233
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11.11 每个句子的平均短语数
- en: '[PRE11]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**#1 We changed a period to a comma to make this 5/4 = 1.25.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '**#1 我们将句号改为逗号，使得这个5/4 = 1.25。'
- en: '#2 Gets a list of sentences'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 获取句子列表'
- en: '#3 Loops through the sentences'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 遍历句子'
- en: '#4 Gets a list of phrases in the current sentence'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 获取当前句子的短语列表'
- en: '#5 Adds the number of phrases in the current sentence'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 添加当前句子的短语数'
- en: '#6 Returns the number of phrases divided by the number of sentences**  **We’re
    really coming along now! We’ve finished all the functions needed to create `make_signature`,
    as shown in figure 11.10.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 返回短语数除以句子数 **我们现在进展顺利！我们已经完成了生成`make_signature`所需的所有函数，如图11.10所示。'
- en: '![figure](../Images/11-10.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/11-10.png)'
- en: Figure 11.10 Full functions diagram updated to show that we’re now ready to
    write `make_signature`
  id: totrans-242
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.10 完整的函数图更新，显示我们现在已经准备好编写`make_signature`
- en: 11.5.10 make_signature
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.10 make_signature
- en: We’ve written nine functions to this point, and while they’re all important,
    we may feel a little unsatisfied right now because we’re not even dealing with
    text signatures yet. We’ve got some functions that clean words, split strings
    in various ways, and calculate individual features of signatures, but no function
    to make a full signature.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经编写了九个函数，虽然它们都很重要，但我们现在可能有些不满足，因为我们甚至还没有处理文本签名。我们有一些函数来清理单词，按不同方式拆分字符串，计算签名的个别特征，但没有一个函数能生成完整的签名。
- en: That changes now because we’re finally ready to implement `make_signature` to
    give us the signature for a text. This function will take the text of a book and
    return a list of five numbers, each of which is the result of calling one of our
    five feature functions.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 现在情况发生了变化，因为我们终于准备好实现`make_signature`来为文本生成签名。此函数将接收一本书的文本，并返回五个数字的列表，每个数字都是调用我们五个特征函数中的一个的结果。
- en: Listing 11.12 Numeric signature for the text
  id: totrans-246
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11.12 文本的数值签名
- en: '[PRE12]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '******#1 Each of our five feature functions is called.******  ******Notice
    that this function can be implemented as nothing more than a call to each of our
    five feature functions. It’s important to pause now to think about just how messy
    this function would have been without having done a solid top-down design first.
    The code for all five of the functions that we’re calling here would have had
    to be in a single function, with all of their own variables and calculations mingled
    together into a real mess. Lucky for us, we’re using top-down design! Our function
    is therefore easier for us to read and easier to convince ourselves that it’s
    doing the right thing.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '******#1 调用了我们的五个特性函数中的每一个。******  ******注意，这个函数可以实现为仅仅调用我们五个特性函数中的每一个。现在暂停一下，思考一下如果没有先进行良好的自顶向下设计，这个函数会有多乱。我们在这里调用的所有五个函数的代码本应在一个单独的函数中，而且它们各自的变量和计算会混合成一团糟。幸运的是，我们使用了自顶向下设计！因此，我们的函数更易于阅读，也更容易让我们确信它做的事情是对的。'
- en: 11.5.11 get_all_signatures
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.11 get_all_signatures
- en: Our `process_data` function has three subtasks for us to implement. We just
    finished with the first one (`make_signature`), so now we’ll move on to its second
    subtask, which is our `get_all_signatures` function.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 `process_data` 函数有三个子任务需要我们实现。我们刚刚完成了第一个子任务（`make_signature`），现在我们将继续第二个子任务，即我们的
    `get_all_signatures` 函数。
- en: From now on, we’ll assume that your working directory has your code and that
    it also has the subdirectory of books that we’ve provided. We need this function
    to return the signature for each file in our directory of known authors. We’re
    hoping for Copilot to call `make_signature` here to make this function far simpler
    than it otherwise would be.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在开始，我们假设你的工作目录中有你的代码，并且它还包含我们提供的书籍子目录。我们需要这个函数返回我们已知作者目录中每个文件的签名。我们希望 Copilot
    在这里调用 `make_signature`，使这个函数比它本来会更简单。
- en: Copilot did do that for us, but the code we got still had two problems. Our
    initial code is shown in the following listing.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: Copilot 确实为我们做了这个，但我们得到的代码仍然有两个问题。我们最初的代码如下面的列表所示。
- en: 'Listing 11.13 Obtain all signatures from known authors: Try 1'
  id: totrans-253
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.13 从已知作者处获取所有签名：尝试 1
- en: '[PRE13]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '**#1 Our dictionary, initially empty, maps filenames to signatures.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '**#1 我们的字典，最初为空，映射文件名到签名。'
- en: '#2 Loops through each file in the known authors directory'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 遍历已知作者目录中的每个文件'
- en: '#3 Opens the current file'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 打开当前文件'
- en: '#4 Reads all text from the file'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 读取文件中的所有文本'
- en: '#5 Makes the signature for text and stores it in the dictionary**  **Try running
    this function from the Python prompt as'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 为文本生成签名并将其存储在字典中 ** ** 尝试从 Python 提示符运行此函数'
- en: '[PRE14]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'and you’ll get the following error:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你会得到以下错误：
- en: '[PRE15]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The error is telling us that the function is trying to use a module named os,
    but we don’t have this module available. This module is built-in to Python, and
    we know what to do in this case: import it! That is, we need to add'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 错误告诉我们该函数尝试使用一个名为 os 的模块，但我们没有这个模块可用。这个模块是 Python 内置的，我们知道该怎么办：导入它！也就是说，我们需要添加
- en: '[PRE16]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'above this function. After that, we still get an error:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个函数上方。之后，我们仍然会得到一个错误：
- en: '[PRE17]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: You might be wondering what a `UnicodeDecodeError` is. You could google it or
    ask ChatGPT if you’re interested in a technical explanation. What we need to know
    is that each file that we open is encoded in a specific way, and Python has chosen
    the wrong encoding to try to read this file.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道什么是 `UnicodeDecodeError`。如果你对技术解释感兴趣，可以去谷歌搜索或者问 ChatGPT。我们需要知道的是，我们打开的每个文件都是以特定的方式编码的，而
    Python 选择了错误的编码方式来读取这个文件。
- en: We can, however, direct Copilot to fix it by adding a comment near the top of
    our function. (When you encounter errors like these, you can try placing a comment
    directly above the erroneous code that was generated. Then, once you delete the
    incorrect code, Copilot can often generate new code that is correct.) Once we
    do that, all is well, as shown in the following listing.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们可以通过在函数顶部附近添加注释来指导 Copilot 修复它。 （当你遇到类似的错误时，可以尝试在生成的错误代码上方直接添加注释。然后，一旦删除了错误代码，Copilot
    通常可以生成正确的代码。）一旦我们这样做，一切都会顺利，如下所示。
- en: 'Listing 11.14 Obtain all signatures from known authors: Try 2'
  id: totrans-269
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.14 从已知作者处获取所有签名：尝试 2
- en: '[PRE18]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '**#1 This prompt tells Copilot to fix the error we saw previously.**  **Now,
    if you run this function, you should see a dictionary of authors and their signatures,
    like this:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '**#1 此提示告诉 Copilot 修复我们之前看到的错误。** **现在，如果你运行这个函数，你应该会看到一个包含作者及其签名的字典，像这样：'
- en: '[PRE19]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: For simplicity, we haven’t added a test in the docstring for this function.
    If we did, however, we would create a fake, small book, along the lines of what
    we did in our second example in chapter 6\. We’d like to proceed here with our
    overall purpose of function decomposition, though, so we’ll leave that exercise
    to you if you’d like to pursue that. As shown in figure 11.11, we’ve gotten two
    `process_data` subtasks out of the way. Let’s keep going!
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简便起见，我们没有在此函数的文档字符串中添加测试。不过，如果我们添加了测试，我们会创建一个假的小书，类似于我们在第六章第二个示例中做的那样。尽管如此，我们还是希望继续推进函数分解的总体目标，因此如果你想继续深入，可以自己完成这个练习。如图
    11.11 所示，我们已经完成了两个 `process_data` 子任务。让我们继续前进！
- en: '![figure](../Images/11-11.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/11-11.png)'
- en: Figure 11.11 Full functions diagram updated to show that `make_signature` and
    `get_all_signatures` are finished
  id: totrans-275
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.11 完整的功能图，已更新以显示 `make_signature` 和 `get_all_signatures` 已完成
- en: 11.5.12 get_score
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.12 get_score
- en: Let’s implement `get_score`, where we need to encode the way that we compare
    signatures. Remember the whole thing where we find the difference on each feature,
    multiply it by a weight, and then add everything together into an overall score?
    That’s what we want `get_score` to do.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现 `get_score`，在这里我们需要编码比较签名的方式。记住，之前我们讨论过，找到每个特征上的差异，乘以权重，然后将所有差异加起来得出总分？这正是我们希望
    `get_score` 实现的功能。
- en: 'It would be a challenge to explain this formula in the docstring. And we’re
    not even sure that it should go there: a docstring is supposed to explain how
    someone can use your function, not how it works internally. And, arguably, users
    of our function won’t care about this specific formula anyway. What we can do
    is use a general docstring, without our specific formula, and see what Copilot
    does with it. Here we go in the following listing.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在文档字符串中解释这个公式将是一个挑战。我们甚至不确定它是否应该放在那里：文档字符串应该解释如何使用你的函数，而不是它的内部工作原理。而且，可以说，我们函数的使用者反而不会在乎这个具体的公式。我们可以做的是使用一个通用的文档字符串，不涉及具体公式，看看
    Copilot 如何处理它。接下来我们在下面的列表中演示。
- en: Listing 11.15 Compare two signatures
  id: totrans-279
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.15 比较两个签名
- en: '[PRE20]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '**#1 These weights, [11, 33, 50, 0.4, 4], worked well for us.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '**#1 这些权重，[11, 33, 50, 0.4, 4]，对我们非常有效。'
- en: '#2 Loops through each signature index'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 遍历每个签名索引'
- en: '#3 Adds the weighted difference to score**  **Copilot has implemented exactly
    the formula that we wanted. Now, before we start thinking that Copilot mind-melded
    us or anything like that, remember that the formula we’ve used here is a very
    common metric for comparing signatures. Many students and other programmers over
    the years have implemented authorship identification using this very formula.
    Copilot is just giving that back to us because it occurs so often in its training
    data. If Copilot happened to give us a different formula, we could have tried
    to describe what we want in a comment or, failing that, changed the code ourselves
    to get what we want.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 将加权差异添加到得分** **Copilot 完全实现了我们想要的公式。现在，在我们开始认为 Copilot 能读取我们的心思或类似的事情之前，请记住，我们使用的这个公式是一个非常常见的用于比较签名的度量标准。多年来，许多学生和程序员都用这个公式实现了作者身份识别。Copilot
    之所以给我们返回这个公式，是因为它在其训练数据中出现得非常频繁。如果 Copilot 给出了一个不同的公式，我们本可以尝试在注释中描述我们的需求，或者如果那样做无效，我们自己更改代码来实现所需功能。'
- en: 11.5.13 lowest_score
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.13 lowest_score
- en: Our `lowest_score` function will finally wrap up everything we need to implement
    `process_data`. The `get_score` function that we just implemented gives us the
    score between any two signatures. Our `lowest_score` function is going to call
    `get_score` once for each known signature to compare the unknown signature to
    each known signature. It will then return the known signature that has the lowest
    score with the unknown signature, as shown in the following listing.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 `lowest_score` 函数最终将完成我们需要实现 `process_data` 的所有工作。我们刚刚实现的 `get_score` 函数给出了任意两个签名之间的得分。我们的
    `lowest_score` 函数将会调用 `get_score`，对每个已知签名与未知签名进行比较。然后它将返回与未知签名得分最低的已知签名，如以下列表所示。
- en: Listing 11.16 Closest known signature
  id: totrans-286
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.16 最接近的已知签名
- en: '[PRE21]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '****#1 Using variables in the doctest to make the test itself easier to read'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '****#1 使用变量在 doctest 中使测试本身更容易阅读'
- en: '#2 This line is easier to read because we’re using our variables.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 这一行更容易阅读，因为我们使用了变量。'
- en: '#3 Loops through each author name'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 遍历每个作者名字'
- en: '#4 Gets a score for comparing this known signature to the unknown signature'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 获取将此已知签名与未知签名比较的得分'
- en: '#5 If this is the first comparison or we’ve found a lower score . . .'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 如果这是第一次比较，或者我们找到更低的分数……'
- en: '#6 . . . this stores both the best key and score for that key.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 . . . 这会存储最佳键及其对应的分数。'
- en: '#7 lowest[0] is the best key.****  ****The first parameter, `signatures_dict`,
    is a dictionary that maps names of authors to their known signatures. That will
    ultimately come from the `get_all_signatures` function. The second parameter,
    `unknown_signature`, will ultimately come from calling `make_signature` on the
    mystery book. The third parameter, `weights`, will be hard-coded by us when we
    call this function.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 lowest[0] 是最佳键。****  ****第一个参数`signatures_dict`是一个字典，将作者的名字映射到他们已知的签名。这最终会通过`get_all_signatures`函数得到。第二个参数`unknown_signature`将最终来自调用`make_signature`生成的神秘书籍签名。第三个参数`weights`将在我们调用这个函数时由我们硬编码。'
- en: 11.5.14 process_data
  id: totrans-295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.14 process_data
- en: Only two functions to go! One of them is `process_data`—it feels like it took
    us forever, but we’re finally ready for it.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 只剩下两个函数了！其中之一是`process_data`—感觉我们花了很久时间，但终于准备好了。
- en: 'Our `process_data` function is going to take two parameters in the following
    listing: the filename of a mystery book and the directory of known-author books.
    It will return the author that we think wrote the mystery book.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`process_data`函数将在以下代码中接收两个参数：神秘书籍的文件名和已知作者书籍的目录。它将返回我们认为写这本神秘书籍的作者。
- en: Listing 11.17 Signature closest to the mystery author
  id: totrans-298
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例11.17 最接近神秘作者的签名
- en: '[PRE22]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '#1 Gets all the known signatures'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 获取所有已知的签名'
- en: '#2 Copilot uses our prior work to get the encoding right this time.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 Copilot利用我们的先前工作来正确获取编码。'
- en: '#3 Reads text of the mystery book'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 读取神秘书籍的文本'
- en: '#4 Gets the unknown signature'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 获取未知的签名'
- en: '#5 Returns the signature with the lowest comparison score'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 返回具有最低比较分数的签名'
- en: Again, notice how much we’re relying on our earlier functions. This massively
    useful `process_data` function is now really nothing more than a carefully sequenced
    list of function calls.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 再次注意我们多么依赖于早期的函数。这个极其有用的`process_data`函数现在实际上只是一个经过精心排序的函数调用列表。
- en: In the book resources for this chapter, we’ve included a few unknown author
    files, for example, unknown1.txt and unknown2.txt. Those should be in your current
    working directory along with your code (and the subdirectory of known author files).
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章节的书籍资源中，我们包含了几个未知作者的文件，例如unknown1.txt和unknown2.txt。这些文件应该与您的代码（以及已知作者文件的子目录）一起放在当前工作目录中。
- en: 'Let’s call `process_data` to guess who wrote `''unknown1.txt''`:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们调用`process_data`来猜测是谁写了`'unknown1.txt'`：
- en: '[PRE23]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Our program guesses that Arthur Conan Doyle wrote unknown1.txt. And if you peek
    at the text of unknown1.txt by opening the file, you’ll see that our guess is
    right. The book is called *The Sign of the Four*, which is a well-known Arthur
    Conan Doyle book.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的程序猜测是亚瑟·柯南·道尔写了unknown1.txt。如果你通过打开文件查看unknown1.txt的内容，你会发现我们的猜测是正确的。这本书叫做《四签名》，是一本著名的亚瑟·柯南·道尔作品。
- en: 11.5.15 make_guess
  id: totrans-310
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.15 make_guess
- en: To guess the author of a book, we currently need to type the Python code to
    run `process_data`. That’s not very friendly to users; it would be nice if we
    could run the program and have it ask us which mystery book file we want to work
    with.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，为了猜测一本书的作者，我们需要输入Python代码来运行`process_data`。这对用户来说并不友好；如果我们能运行程序并让它询问我们要使用哪个神秘书籍文件，那就太好了。
- en: We’ll put that finishing touch on our program by implementing `make_guess`,
    our top-most function! This function will ask the user for a filename of a mystery
    book, get the best guess using `process_data`, and tell the user about that guess,
    as shown in the following listing.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过实现`make_guess`这个最顶层函数，为我们的程序画上完美的句号！这个函数将会要求用户输入一个神秘书籍的文件名，使用`process_data`获取最佳猜测，并告知用户这一猜测，具体实现请参见以下代码。
- en: Listing 11.18 Interacts with user and guesses text’s author
  id: totrans-313
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例11.18 与用户互动并猜测文本的作者
- en: '[PRE24]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '#1 Asks the user for the filename of the mystery book'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 向用户询问神秘书籍的文件名'
- en: '#2 Calls process_data to do all the work and report our guess'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 调用`process_data`完成所有工作并报告我们的猜测'
- en: This completes all the functions from our diagram! Figure 11.12 shows that we’ve
    checked off every function from the bottom to the very top of our diagram.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了我们图表中的所有函数！图11.12展示了我们已经从底部到最顶部检查完了每个函数。
- en: '![figure](../Images/11-12.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/11-12.png)'
- en: Figure 11.12 All the required functions for `make_guess` are now complete!
  id: totrans-319
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.12 `make_guess`所需的所有函数现在都已完成！
- en: 'If you have all of our code in your Python file, you’ll be able to run it to
    guess the author of a mystery book after you add the following line of code at
    the bottom of that file:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将我们所有的代码放入你的Python文件中，并在该文件的底部添加以下代码行，你就能运行程序来猜测一本神秘书籍的作者：
- en: '[PRE25]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'For example, here’s what happens when we run our program and type `unknown1.txt`
    as the unknown book:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，下面是当我们运行程序并输入`unknown1.txt`作为未知书籍时发生的情况：
- en: '[PRE26]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: It correctly tells us that unknown1.txt is written by Arthur Conan Doyle! Try
    running it for each of the other unknown book files that we’ve provided. How many
    of those does it guess correctly? Which ones does it get wrong?
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 它正确地告诉我们，`unknown1.txt`是由阿瑟·柯南·道尔所写！尝试对我们提供的每个其他未知书籍文件进行运行。它能正确猜出多少本？又错猜了哪些？
- en: Congratulations! You’ve completed your first real-world top-down design. And
    look at what we’ve managed to accomplish—an authorship identification program
    that any beginning programmer should be proud of. Your program uses AI to learn
    how individual authors write (do they use shorter or longer words on average,
    shorter or longer sentences on average, etc.?) by using the text of books in its
    training data. It then applies that learning to make a prediction on a mystery
    book by determining which author the mystery book most closely emulates—very cool!
    We managed to solve a very difficult problem, and we did it by breaking down the
    problem and letting Copilot write the code for each of the subproblems.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你！你已经完成了你的第一个真实世界的自顶向下设计。看看我们完成了什么——一个任何初学者程序员都应该为之自豪的作者识别程序。你的程序使用人工智能学习各个作者的写作风格（他们平均使用短词还是长词，平均使用短句还是长句，等等），通过使用其训练数据中的书籍文本。然后，它应用这些学习，预测一本神秘书籍的作者，通过确定哪位作者的风格最接近这本神秘书籍——非常酷！我们成功地解决了一个非常困难的问题，并通过将问题分解并让Copilot为每个子问题编写代码来实现它。
- en: 11.6 Going further
  id: totrans-326
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.6 进一步发展
- en: After people do a top-down design, they often see opportunities to refactor
    their code, which means making the code cleaner or better organized without changing
    its behavior. It’s possible to refactor our program in several ways. For example,
    you might notice that many of our signature feature functions split the string
    into words and then ignore empty words. This task (returning a list of nonempty
    words from a string) could be split off into its own subtask function, which would
    further simplify any function that calls it.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行自顶向下设计之后，人们通常会发现重构代码的机会，这意味着在不改变代码行为的情况下，使代码更清晰或更有条理。我们可以通过多种方式重构我们的程序。例如，你可能会注意到，我们的许多标志性特征函数将字符串拆分成单词，然后忽略空白单词。这个任务（从字符串中返回一个非空单词的列表）可以被拆分成一个独立的子任务函数，这样任何调用它的函数都会更加简洁。
- en: We might also decide that weights should be passed to `process_data`, rather
    than hard-coding the weights in that function. The weights would then be hard-coded
    in `make_guess`, moving the decision higher in the function hierarchy and therefore
    making it easier to find and change if needed.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可能决定将权重传递给`process_data`，而不是在该函数中硬编码权重。然后，权重将在`make_guess`中硬编码，这将使决策在函数层次结构中更高，从而使得需要时更容易找到并更改。
- en: It’s also possible to improve the program in terms of its features or efficiency.
    For features, right now, our program simply prints its best guess for the mystery
    book author. But we don’t know anything about that guess. Was there a second author
    that was very close to the one that was guessed? If so, we might want to know
    that. More generally, we might want to know the top few guesses rather than just
    the top guess. That way, we have useful information about who the author might
    be even if the top guess happens to be wrong. These are additional features that
    we could add to our program.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以在功能或效率方面改进程序。在功能方面，现在我们的程序只是打印出它对神秘书籍作者的最佳猜测。但我们对这个猜测一无所知。是否有第二个作者的猜测与我们猜测的作者非常接近？如果有，我们可能会想知道。更一般来说，我们可能希望知道前几名的猜测，而不仅仅是最顶端的猜测。这样，即使最顶端的猜测错误，我们也能获得关于作者是谁的有用信息。这些是我们可以添加到程序中的附加功能。
- en: For efficiency, let’s think about that `get_all_signatures` function again.
    That function does a lot of work! If we have five books in our known directory,
    then it will read each of the five files and calculate each signature. Big deal,
    right? It’s only five files, and computers are really fast. But imagine if we
    had 100 files or 10,000 files. It may be acceptable to do all that work as a one-time-only
    thing, but that’s not what our program does. In fact, every time we run the program
    to get a guess for the author of a mystery book, it runs that `get_all_signatures`
    function, which means re-creating those signatures every single time. That’s a
    huge amount of wasted effort; it would be nice if we could just store those signatures
    somewhere, never having to calculate them again. Indeed, if we were to redesign
    the code for efficiency, a first step would be to ensure that the signature for
    a known text is only computed once and reused thereafter.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高效率，让我们再想一想那个`get_all_signatures`函数。那个函数做了很多工作！如果我们在已知目录中有五本书，那么它将读取每个文件并计算每个签名。没什么大不了的，对吧？只有五个文件，计算机处理起来很快。但试想一下，如果我们有100个文件或1万个文件呢？将这些工作当作一次性任务来做可能是可以接受的，但这不是我们程序的做法。事实上，每次我们运行程序来猜测神秘书籍的作者时，它都会运行那个`get_all_signatures`函数，这意味着每次都需要重新创建那些签名。这是一个巨大的浪费；如果我们能将那些签名存储在某个地方，以后再也不需要计算它们，那就好了。的确，如果我们要重新设计代码以提高效率，第一步就是确保已知文本的签名只计算一次，并且以后复用。
- en: That’s exactly what tools like Copilot do! OpenAI trained GitHub Copilot just
    once on a huge corpus of code. That took thousands or millions of computer hours.
    But now that the training is done, it can keep writing code for us without having
    to train from scratch every time. The idea of doing the training once and then
    using that training for many subsequent predictions is a common paradigm throughout
    all of ML.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是像 Copilot 这样的工具所做的！OpenAI 只在大量代码语料库上训练了 GitHub Copilot 一次。那花费了成千上万小时的计算时间。但现在训练完成后，它就能不断为我们写代码，而无需每次从头开始训练。一次训练，之后多次预测的这个理念，在所有的机器学习中都是一个常见的范式。
- en: 11.7 Exercises
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.7 练习
- en: Which of the following isn’t a step in the AI-based authorship identification
    process described in this chapter?
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下哪一项不是本章中描述的基于 AI 的作者身份识别过程中的一步？
- en: Calculating the average word length of the mystery book
  id: totrans-334
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算神秘书籍的平均单词长度
- en: Comparing the mystery book’s signature to known signatures
  id: totrans-335
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较神秘书籍的签名与已知签名
- en: Asking the user for the filename of the mystery book
  id: totrans-336
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请求用户提供神秘书籍的文件名
- en: Finding the total number of pages in the mystery book
  id: totrans-337
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算神秘书籍的总页数
- en: 'Build a classifier that can distinguish between spam and non-spam (ham) emails
    based on email content. Use features like word frequency, presence of certain
    keywords, and email length. Here are the steps you’ll need to take:'
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个能够根据邮件内容区分垃圾邮件和非垃圾邮件（正常邮件）的分类器。使用特征如单词频率、某些关键词的出现和邮件长度。以下是你需要采取的步骤：
- en: Collect a dataset of spam and non-spam emails. You can find publicly available
    datasets online, such as the Enron spam dataset.
  id: totrans-339
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集垃圾邮件和非垃圾邮件的数据集。你可以在线找到公开的数据集，例如 Enron 垃圾邮件数据集。
- en: Preprocess the emails (remove stop words, punctuation, etc.).
  id: totrans-340
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对邮件进行预处理（去除停用词、标点符号等）。
- en: Extract features (e.g., word counts, presence of certain words).
  id: totrans-341
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取特征（例如，单词计数、某些单词的出现）。
- en: Train a classifier using our labeled data (supervised learning). A simple and
    effective choice for the classifier is the Naïve Bayes classifier (feel free to
    use a Python library to help you).
  id: totrans-342
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用我们标记的数据训练分类器（监督学习）。对于分类器，一个简单有效的选择是朴素贝叶斯分类器（可以使用Python库来帮助实现）。
- en: Test the classifier with a separate set of emails to check its accuracy.
  id: totrans-343
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用一个单独的邮件集来测试分类器，检查其准确性。
- en: 'In this exercise, you’ll create a simple text generation program using n-grams.
    N-grams are contiguous sequences of *n* items from a given sample of text or speech.
    You’ll use these n-grams to generate new text that mimics the style of the input
    text. The key idea is to build a model that is trained to know which words commonly
    follow other words (i.e., “cat eats” makes sense, “tissue eats” does not) and
    then, among the possible choices, randomly select the next one. Feel free to look
    up n-grams for more information. Here are the steps you’ll need to take:'
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个练习中，你将创建一个简单的文本生成程序，使用n-gram。N-gram是给定文本或语音样本中的连续序列的*n*项。你将使用这些n-gram来生成模仿输入文本风格的新文本。关键思想是构建一个模型，训练它知道哪些单词通常跟在其他单词后面（例如，“猫吃”是合理的，“纸巾吃”不合理），然后在可能的选择中随机选择下一个单词。如有需要，可以查阅n-gram获取更多信息。以下是你需要采取的步骤：
- en: 'Choose input text that you can load into Python. You can use something like:
    “Pride and Prejudice” by Jane Austen.'
  id: totrans-345
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择可以加载到Python中的输入文本。你可以使用像是：“傲慢与偏见”（简·奥斯汀著）这样的作品。
- en: Preprocess the text by converting it to lowercase and removing punctuation.
  id: totrans-346
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预处理文本，将其转换为小写并去除标点符号。
- en: Create n-grams from the input text. An n-gram is a contiguous sequence of *n*
    items from a given text. For simplicity, we’ll use bigrams (*n* = 2) in this example.
  id: totrans-347
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从输入文本创建n-gram。一个n-gram是给定文本中的连续序列的*n*项。在这个例子中，我们将使用二元组（*n* = 2）。
- en: Use the generated n-grams to produce new text. Start with a random n-gram, and
    keep adding new words based on the n-gram model until the desired length is reached.
  id: totrans-348
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用生成的n-gram来产生新文本。从一个随机的n-gram开始，并根据n-gram模型不断添加新单词，直到达到所需的长度。
- en: Summary
  id: totrans-349
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: Top-down design becomes more and more critical as the complexity of our programs
    increase.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着我们程序复杂度的增加，自顶向下的设计变得越来越关键。
- en: Author identification is the process of guessing the author of a mystery book.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作者识别是猜测神秘书籍作者的过程。
- en: We can use features about words (e.g., average word length) and sentences (e.g.,
    average number of words per sentence) to characterize how each known author writes.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用关于单词的特征（例如，平均单词长度）和句子的特征（例如，每个句子的平均单词数）来表征每位已知作者的写作风格。
- en: Machine learning is an important area of computer science that investigates
    how machines can learn from data and make predictions.
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习是计算机科学中一个重要的领域，它研究机器如何从数据中学习并进行预测。
- en: In supervised learning, we have some training data in the form of objects (e.g.,
    books) and their categories (who wrote each book). We can learn from that data
    to make predictions about new objects.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在监督学习中，我们有一些训练数据，以对象（例如书籍）和它们的类别（谁写了每本书）的形式存在。我们可以从这些数据中学习，以便对新对象进行预测。
- en: A signature consists of a list of features, one signature per object.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个签名由特征列表组成，每个对象一个签名。
- en: Refactoring code means to improve the design of the code (e.g., by reducing
    code repetition).************************
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重构代码意味着改进代码的设计（例如，通过减少代码重复）。
