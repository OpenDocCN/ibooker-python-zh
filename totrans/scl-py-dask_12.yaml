- en: 'Chapter 12\. Productionizing Dask: Notebooks, Deployment, Tuning, and Monitoring'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 12 章。将 Dask 投入生产：笔记本、部署、调整和监控
- en: We have bundled most of the things we believe are going to be critical for you
    to move from your laptop into production in this chapter. Notebooks and deployments
    go together, as Dask’s notebook interface greatly simplifies many aspects of using
    its distributed deployments. While you don’t need to use notebooks to access Dask,
    and in many cases [notebooks have serious drawbacks](https://oreil.ly/oi09S),
    for interactive use cases it’s often hard to beat the trade-offs. Interactive/exploratory
    work has a way of becoming permanent mission-critical workflows, and we cover
    the steps necessary to turn exploratory work into production deployments.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将捆绑我们认为对您从笔记本电脑转入生产环境至关重要的大部分内容。笔记本和部署是相关联的，因为 Dask 的笔记本界面极大简化了使用其分布式部署的许多方面。虽然您不必使用笔记本来访问
    Dask，在许多情况下[笔记本存在严重缺点](https://oreil.ly/oi09S)，但对于交互式用例，很难击败这种权衡。交互式/探索性工作往往会成为永久的关键工作流程，我们将介绍将探索性工作转变为生产部署所需的步骤。
- en: You can deploy Dask in many fashions, from running it on top of other distributed
    compute engines like Ray to deploying it on YARN or a raw collection of machines.
    Once you’ve got your Dask job deployed, you’ll likely need to tune it so you don’t
    use your company’s entire AWS budget on one job. And then, finally, before you
    can walk away from a job, you’ll need to set up monitoring—so you know when it’s
    broken.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以以多种方式部署 Dask，从在其他分布式计算引擎（如 Ray）上运行到部署在 YARN 或原始机器集合上。一旦部署了您的 Dask 作业，您可能需要调整它，以避免将公司的整个
    AWS 预算用于一个作业。最后，在离开一个作业之前，您需要设置监控——这样您就会知道它何时出现故障。
- en: Note
  id: totrans-3
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: If you’re just here to learn how to use Dask with notebooks, feel free to skip
    ahead to that section. If you want to learn more about deploying Dask, congratulations
    and condolences on exceeding the scale you can handle on a single computer.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您只是想学习如何在笔记本中使用 Dask，可以直接跳到该部分。如果您想了解更多关于部署 Dask 的信息，祝贺您并对超出单台计算机处理能力的规模感到遗憾。
- en: In this chapter, we will cover some (but not all) of the deployment options
    for Dask and their trade-offs. You will learn how to integrate notebooks into
    the most common deployment environments. You’ll see how to use these notebooks
    to track the progress of your Dask tasks and access the Dask UI when running remotely.
    We will finish by covering some options for deploying your scheduled tasks, so
    you can take a vacation without lining up someone to press run on your notebook
    every day.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍一些（但不是全部）Dask 的部署选项及其权衡。您将学习如何将笔记本集成到最常见的部署环境中。您将看到如何使用这些笔记本来跟踪您的
    Dask 任务的进度，并在远程运行时访问 Dask UI。最后，我们将介绍一些部署您计划任务的选项，这样您就可以放心度假，而不必每天找人按下笔记本的运行按钮。
- en: Note
  id: totrans-6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: This chapter covers Dask’s distributed deployments, but if your Dask program
    is happy in local mode, don’t feel the need to deploy a cluster just for the sake
    of it.^([1](ch12.xhtml#id853))
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了 Dask 的分布式部署，但如果您的 Dask 程序在本地模式下运行良好，不必为了部署集群而感到需要。^([1](ch12.xhtml#id853))
- en: Factors to Consider in a Deployment Option
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在部署选项中考虑的因素
- en: When you are choosing how to deploy Dask, there are many different factors to
    consider, but often the biggest one is what tools your organization is already
    using. Most of the deployment options map to different types of cluster managers
    (CMs). CMs manage sets of computers and provide some isolation between users and
    jobs. Isolation can be incredibly important—for example, if one user eats all
    of the candy (or CPU), then another user won’t have any candy. Most cluster managers
    provide CPU and memory isolation, and some also isolate other resources (like
    disks and GPUs). Most clouds (AWS, GCP, etc.) offer both Kubernetes and YARN cluster
    managers, which can dynamically scale the number of nodes up and down. Dask does
    not need a CM to run, but without one, auto-scaling and other important features
    are not available.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择如何部署 Dask 时，有许多不同的因素需要考虑，但通常最重要的因素是您的组织已经在使用哪些工具。大多数部署选项都与不同类型的集群管理器（CMs）相关联。CMs
    管理一组计算机，并在用户和作业之间提供一些隔离。隔离可能非常重要——例如，如果一个用户吃掉了所有的糖果（或者 CPU），那么另一个用户就没有糖果了。大多数集群管理器提供
    CPU 和内存隔离，有些还隔离其他资源（如磁盘和 GPU）。大多数云平台（AWS、GCP 等）都提供 Kubernetes 和 YARN 集群管理器，可以动态调整节点的数量。Dask
    不需要 CM 即可运行，但如果没有 CM，将无法使用自动扩展和其他重要功能。
- en: When choosing a deployment mechanism, with or without a CM, some important factors
    to consider are the ability to scale up and down, multi-tenancy, dependency management,
    and whether the deployment method supports heterogeneous workers.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择部署机制时，无论是否使用配置管理器（CM），需要考虑的一些重要因素包括扩展能力、多租户、依赖管理，以及部署方法是否支持异构工作节点。
- en: The ability to scale up and down (or *dynamic scale*) is important in many situations,
    as computers cost money. Heterogeneous or mixed worker types are important for
    workloads that take advantage of accelerators (like GPUs), so that non-accelerated
    work can be scheduled on less-expensive nodes. Support for heterogeneous workers
    goes well with dynamic scaling, as the workers can be replaced.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，扩展能力（或*动态扩展*）非常重要，因为计算机是需要花钱的。对于利用加速器（如GPU）的工作负载来说，异构或混合工作节点类型非常重要，这样非加速工作可以被调度到成本较低的节点上。支持异构工作节点与动态扩展很搭配，因为工作节点可以被替换。
- en: Multi-tenancy can reduce wasted compute resources for systems that cannot scale
    up and down.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 多租户可以减少不能扩展的系统中浪费的计算资源。
- en: Dependency management allows you to control, at runtime or in advance, what
    software is on the workers. This is critical in Dask; if the workers and the client
    do not have the same libraries, your code may not function. Additionally, some
    libraries can be slow to install at runtime, so the ability to pre-install or
    share an environment can be beneficial for some use cases, especially those in
    deep learning.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 依赖管理允许您在运行时或预先控制工作节点上的软件，这在Dask中非常关键；如果工作节点和客户端没有相同的库，您的代码可能无法正常运行。此外，有些库在运行时安装可能很慢，因此能够预先安装或共享环境对某些用例尤其有益，特别是在深度学习领域。
- en: '[Table 12-1](#table_deployment_options_ch12) compares some of the deployment
    options in Dask.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[表12-1](#table_deployment_options_ch12) 比较了一些Dask的部署选项。'
- en: Table 12-1\. Deployment option comparisons
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 表12-1\. 部署选项比较
- en: '| Deployment method | Dynamic scale | Recommended use case^([a](ch12.xhtml#id863))
    | Dependency management | Notebook deployed inside^([b](ch12.xhtml#id864)) | Mixed
    worker types |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 部署方法 | 动态扩展 | 推荐用例^([a](ch12.xhtml#id863)) | 依赖管理 | 在笔记本内部部署^([b](ch12.xhtml#id864))
    | 混合工作节点类型 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| localhost | No | Testing, solo dev, GPU-only acceleration | Yes (runtime
    or pre-install) | Yes | No |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| localhost | 否 | 测试，独立开发，仅GPU加速 | 运行时或预安装 | 是 | 否 |'
- en: '| ssh | No | Solo lab, testing, but generally not recommended (k8s instead)
    | Runtime only | Yes | Yes (manual) |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| ssh | 否 | 单独实验室，测试，但通常不推荐（使用k8s替代） | 仅运行时 | 是 | 是（手动） |'
- en: '| Slurm + GW | Yes | Existing high-performance computing/Slurm environments
    | Yes (runtime or pre-install) | Separate project | Varies |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| Slurm + GW | 是 | 现有的高性能计算/Slurm环境 | 是（运行时或预安装） | 单独的项目 | 各异 |'
- en: '| Dask “Cloud” | Yes | Not recommended; use Dask + K8s or YARN on cloud provider
    | Runtime only | Medium effort^([c](ch12.xhtml#id865)) | No |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| Dask “Cloud” | 是 | 不推荐；在云提供商上使用Dask + K8s或YARN | 仅运行时 | 中等难度^([c](ch12.xhtml#id865))
    | 否 |'
- en: '| Dask + K8s | Yes | Cloud environments, existing K8s deployments | Runtime
    or pre-install (but more effort) | Separate project, medium effort | Yes |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| Dask + K8s | 是 | 云环境，现有的K8s部署 | 运行时或预安装（但需要更多工作） | 单独的项目，中等难度 | 是 |'
- en: '| Dask + YARN | Yes | Existing big data deployments | Runtime or pre-install
    (but more effort) | Separate project that has not been updated since 2019 | Yes
    |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| Dask + YARN | 是 | 现有的大数据部署 | 运行时或预安装（但需要更多工作） | 自2019年以来未更新的单独项目 | 是 |'
- en: '| Dask + Ray + [CM] | Depends on CM | Existing Ray deployments, multi-tool
    (TF, etc.), or actor systems | Depends on CM (always at least runtime) | Depends
    on CM | Yes |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| Dask + Ray + [CM] | 取决于CM | 现有的Ray部署，多工具（TF等），或者actor系统 | 取决于CM（至少总是运行时）
    | 取决于CM | 是 |'
- en: '| Coiled | yes | New cloud deployments | Yes, including magic “auto-sync” |
    No | Yes |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| Coiled | 是 | 新的云部署 | 是，包括魔术“自动同步” | 否 | 是 |'
- en: '| ^([a](ch12.xhtml#id863-marker)) This is largely based on our experiences
    and may be biased toward large companies and academic environments. Please feel
    free to make your own call.^([b](ch12.xhtml#id864-marker)) There are [some workarounds](https://oreil.ly/TqhBb).^([c](ch12.xhtml#id865-marker))
    Some large commodity cloud providers were easier than others. Mika’s own experience
    ranks Google Cloud as easiest, Amazon as moderate, and Azure as hardest to work
    with. Google Cloud has a good working guide on using Dask with RAPIDS NVIDIA architecture
    and a well-documented workflow. Amazon Web Services similarly has good documentation
    on running Dask workers on multiple Amazon Elastic Compute Cloud (EC2) instances
    and a guide on attaching S3 buckets. Azure needed some work to make worker provisioning
    work well, mostly due to its environment and user provisioning workflow being
    a bit different from that of AWS or GCP. |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| ^([a](ch12.xhtml#id863-marker)) 这主要基于我们的经验，可能偏向于大公司和学术环境。请随意做出您自己的决定。^([b](ch12.xhtml#id864-marker))
    有[一些解决方案](https://oreil.ly/TqhBb)。^([c](ch12.xhtml#id865-marker)) 有些大型通用云提供商比其他更容易。Mika
    自己的经验认为，Google Cloud 最容易，Amazon 居中，Azure 最难处理。Google Cloud 在使用 Dask 与 RAPIDS NVIDIA
    架构和工作流程方面有良好的工作指南。同样，Amazon Web Services 在多个 Amazon Elastic Compute Cloud (EC2)
    实例上运行 Dask workers 和挂载 S3 存储桶的文档都很好。Azure 需要做一些工作才能使 worker 配置工作良好，主要是由于其环境和用户配置工作流程与
    AWS 或 GCP 有所不同。 |'
- en: Building Dask on a Kubernetes Deployment
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Kubernetes 部署 Dask
- en: There are two main ways to deploy Dask on Kubernetes:^([2](ch12.xhtml#id866))
    KubeCluster and HelmCluster. Helm is a popular tool for managing deployments on
    Kubernetes, with the deployments being specified in Helm charts. Since Helm is
    the newer recommended way of managing deployments on Kubernetes, we will cover
    that one here.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种主要的方式可以在 Kubernetes 上部署 Dask:^([2](ch12.xhtml#id866)) KubeCluster 和 HelmCluster。Helm
    是管理 Kubernetes 上部署的流行工具，部署在 Helm 图表中指定。由于 Helm 是管理 Kubernetes 上部署的新推荐方式，我们将在这里涵盖这一点。
- en: The [Helm documentation](https://oreil.ly/EBzBm) offers an excellent starting
    point on the different ways to install Helm, but for those in a hurry, `curl https://raw.githubuser​con⁠tent.com/helm/helm/main/scripts/get-helm-3
    | bash` will do it for you.^([3](ch12.xhtml#id873))
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[Helm 文档](https://oreil.ly/EBzBm) 提供了关于不同安装 Helm 方式的优秀起始点，但是对于那些着急的人，`curl
    https://raw.githubuser​con⁠tent.com/helm/helm/main/scripts/get-helm-3 | bash`
    就可以搞定了。^([3](ch12.xhtml#id873))'
- en: Note
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The Dask on Kubernetes Helm chart deploys what is called an *operator*. Currently,
    installing operators requires the ability to install Custom Resource Definitions
    (CRDs) and may require administrator privileges. If you can’t get the permissions
    (or someone with the permissions), you can still use the [“vanilla” or “classic”
    deployment mode](https://oreil.ly/-MLRk).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 在 Kubernetes 上的 Helm 图表部署了所谓的*operator*。当前，安装 operators 需要安装自定义资源定义（CRDs）的能力，并且可能需要管理员权限。如果你无法获取权限（或者有权限的人），你仍然可以使用[“vanilla”
    or “classic” deployment mode](https://oreil.ly/-MLRk)。
- en: Since GPU resources are costly, it is typical to want to allocate only as many
    of them as needed. Some cluster manager interfaces, including Dask’s Kubernetes
    plug-in, allow you to configure multiple types of workers so that Dask will allocate
    GPU workers only when needed. On our Kubernetes cluster, we deploy the Dask operator
    as shown in [Example 12-1](#deploy_dask_operator_helm).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 GPU 资源昂贵，通常希望只分配所需数量的资源。一些集群管理器接口，包括 Dask 的 Kubernetes 插件，允许您配置多种类型的 workers，以便
    Dask 只在需要时分配 GPU workers。在我们的 Kubernetes 集群上，我们部署 Dask operator 如 [Example 12-1](#deploy_dask_operator_helm)
    所示。
- en: Example 12-1\. Deploying the Dask operator with Helm
  id: totrans-33
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 12-1\. 使用 Helm 部署 Dask operator
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now you can use the Dask operator either by creating YAML files (likely not
    your favorite) or with the `KubeCluster` API, as shown in [Example 12-2](#helm_ex),
    where we create a cluster and then add additional worker types, allowing Dask
    to create two different kinds of workers.^([4](ch12.xhtml#id876))
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以通过创建 YAML 文件（可能不是你最喜欢的方式）或者使用`KubeCluster` API 来使用 Dask operator，如 [Example 12-2](#helm_ex)
    所示，在这里我们创建一个集群，然后添加额外的 worker 类型，允许 Dask 创建两种不同类型的 workers。^([4](ch12.xhtml#id876))
- en: Example 12-2\. Using the Dask operator
  id: totrans-36
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 12-2\. 使用 Dask operator
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In 2020 Dask added a [DaskHub Helm chart](https://oreil.ly/otOQX), which combines
    the deployment of JupyterHub with the Dask Gateway.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 2020 年，Dask 添加了一个[DaskHub Helm 图表](https://oreil.ly/otOQX)，它将 JupyterHub 的部署与
    Dask Gateway 结合在一起。
- en: Dask on Ray
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dask on Ray
- en: Deploying Dask on Ray is a little bit different than all of the other options,
    in that it changes not only how Dask workers and tasks are scheduled but also
    [how Dask objects are stored](https://oreil.ly/PIwuX). This can reduce the number
    of copies of the same object that need to be stored, allowing you to use your
    cluster memory more efficiently.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 将Dask部署到Ray上与所有其他选项略有不同，因为它不仅改变了Dask工作节点和任务的调度方式，还改变了[Dask对象的存储方式](https://oreil.ly/PIwuX)。这可以减少需要存储的同一对象的副本数量，从而更有效地利用集群内存。
- en: If you have a Ray deployment available to you, enabling Dask can be incredibly
    straightforward, as shown in [Example 12-3](#dask_on_ray).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经有一个可用的Ray部署，启用Dask可能会非常简单，就像在[示例 12-3](#dask_on_ray)中所示的那样。
- en: Example 12-3\. Running Dask on Ray
  id: totrans-42
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 12-3\. 在Ray上运行Dask
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: However, if you don’t have an existing Ray cluster, you will still need to deploy
    Ray somewhere with the same considerations as Dask. Deploying Ray is beyond the
    scope of this book. Ray’s [production guide](https://oreil.ly/PNb0_) has details
    for deploying on Ray, as does *Scaling Python with Ray*.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果您没有现有的Ray集群，您仍然需要在某处部署Ray，并考虑与Dask相同的考虑因素。部署Ray超出了本书的范围。Ray的[生产指南](https://oreil.ly/PNb0_)以及*Scaling
    Python with Ray*中有有关在Ray上部署的详细信息。
- en: Dask on YARN
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在YARN上的Dask
- en: 'YARN is a popular cluster manager from the big data world that is available
    in open source as well as commercial on-premises (e.g., Cloudera) and cloud (e.g.,
    Elastic Map Reduce). There are two ways to run Dask on a YARN cluster: one is
    with Dask-Yarn, and the other is with Dask-Gateway. While the two methods are
    similar, Dask-Gateway is potentially a bit more involved, as it adds a centrally
    managed server that runs to manage Dask clusters, but it has more fine-grained
    security and administrative controls.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: YARN是来自大数据领域的流行集群管理器，它在开源和商业的本地（例如Cloudera）和云（例如Elastic Map Reduce）环境中都有提供。在YARN集群上运行Dask有两种方式：一种是使用Dask-Yarn，另一种是使用Dask-Gateway。尽管这两种方法相似，但Dask-Gateway可能需要更多的操作，因为它添加了一个集中管理的服务器来管理Dask集群，但它具有更精细的安全性和管理控制。
- en: Depending on the cluster, your workers might be more transient than other types,
    and their IP address might not be static when they get spun up again. You should
    ensure worker/scheduler service discovery methods are put in place for your own
    cluster setup. It could be as simple as a shared file that they read from, or
    a more resilient broker. If no additional arguments are given, Dask workers would
    use the `DASK_SCHEDULER_ADDRESS` environment variable to connect.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 根据集群的不同，您的工作节点可能比其他类型的工作节点更加短暂，并且它们的IP地址在重新启动时可能不是静态的。您应确保为自己的集群设置工作节点/调度器服务发现方法。可以简单地使用一个共享文件让它们读取，或者使用更可靠的代理。如果没有提供额外的参数，Dask工作节点将使用`DASK_SCHEDULER_ADDRESS`环境变量进行连接。
- en: '[Example 12-4](#ex_yarn_deployment_ch12_1685536104476) expands on [Example 9-1](ch09.xhtml#ex_yarn_deployment_ch09_1685536092648)
    with a custom conda environment and logging framework.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 12-4](#ex_yarn_deployment_ch12_1685536104476)在一个自定义的conda环境和日志框架中扩展了[示例 9-1](ch09.xhtml#ex_yarn_deployment_ch09_1685536092648)。'
- en: Example 12-4\. Deploying Dask on YARN with a custom conda environment
  id: totrans-49
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 12-4\. 使用自定义conda环境在YARN上部署Dask
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Alternatively, you can run the cluster with the CLI interface that Dask-Yarn
    exposes. You would first deploy YARN in the shell script of your choosing; the
    shell script then invokes the Python file you want to run. Within the Python file,
    you reference the deployed YARN cluster, as shown in [Example 12-5](#ex_yarn_deployment_CLI_tuning).
    This can be an easier way to chain your jobs and inspect and pull logs. Note that
    the CLI is supported only in Python versions above 2.7.6.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以使用Dask-Yarn公开的CLI界面运行集群。您首先会在您选择的Shell脚本中部署YARN；然后，Shell脚本会调用您要运行的Python文件。在Python文件中，您引用部署的YARN集群，如[示例 12-5](#ex_yarn_deployment_CLI_tuning)所示。这可以是一个更简单的方式来链接您的作业并检查和获取日志。请注意，CLI仅在Python版本高于2.7.6时受支持。
- en: Example 12-5\. Deploying Dask on YARN with CLI interface
  id: totrans-52
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 12-5\. 使用CLI界面在YARN上部署Dask
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Dask on High-Performance Computing
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在高性能计算中的Dask
- en: Dask has gained a big academic and scientific user base. This comes in part
    from how you can use existing high-performance computing (HPC) clusters with Dask
    to make readily available scalable scientific computing without rewriting all
    of your code.^([5](ch12.xhtml#id890))
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Dask已经获得了大量的学术和科学用户群体。这在一定程度上归功于使用现有的高性能计算（HPC）集群与Dask一起，可以轻松实现可扩展的科学计算，而无需重写所有代码。^([5](ch12.xhtml#id890))
- en: You can turn your HPC account into a high-performance Dask environment that
    you can connect to from Jupyter on your local machine. Dask uses its Dask-jobqueue
    library to support many HPC cluster types, including HTCondor, LSF, Moab, OAR,
    PBS, SGE, TORQUE, DRMAA, and Slurm. A separate library, Dask-MPI, supports MPI
    clusters. In [Example 9-2](ch09.xhtml#ex_slurm_deployment_ch09_1685536141262),
    we showed a sample of how to use Dask on Slurm, and in the following section,
    we’ll build on that example.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将您的 HPC 帐户转换为高性能 Dask 环境，从而可以在本地机器上的 Jupyter 中连接到它。Dask 使用其 Dask-jobqueue
    库来支持许多 HPC 集群类型，包括 HTCondor、LSF、Moab、OAR、PBS、SGE、TORQUE、DRMAA 和 Slurm。另一个库 Dask-MPI
    支持 MPI 集群。在 [示例 9-2](ch09.xhtml#ex_slurm_deployment_ch09_1685536141262) 中，我们展示了如何在
    Slurm 上使用 Dask 的示例，并在接下来的部分中，我们将进一步扩展该示例。
- en: Setting Up Dask in a Remote Cluster
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在远程集群中设置 Dask
- en: The first step for using Dask on your cluster is to set up your own Python and
    iPython environments in the cluster. The exact way to do that will vary by your
    cluster’s admin’s preferences. Generally, users often use [virtualenv](https://oreil.ly/mLAOj)
    or [miniconda](https://oreil.ly/zXsTP) to install related libraries on a user
    level. Min⁠i­conda makes it easier to use not just your own libraries but your
    own version of Python. Once that is done, ensure your Python command points to
    the Python binary in your user space by running `which python` or installing and
    importing a library not available in system Python.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在集群上使用 Dask 的第一步是在集群中设置自己的 Python 和 iPython 环境。确切的操作方法会因集群管理员的偏好而异。一般来说，用户通常使用
    [virtualenv](https://oreil.ly/mLAOj) 或 [miniconda](https://oreil.ly/zXsTP) 在用户级别安装相关库。Min⁠i­conda
    不仅可以更轻松地使用您自己的库，还可以使用您自己的 Python 版本。完成此操作后，请确保您的 Python 命令指向用户空间中的 Python 二进制文件，方法是运行`which
    python`或安装和导入系统 Python 中不可用的库。
- en: The Dask-jobqueue library converts your Dask settings and configurations into
    a job script that is submitted to the HPC cluster. The following example starts
    a cluster with Slurm workers, and the semantics are similar for other HPC API.
    Dask-MPI uses a slightly different pattern, so be sure to refer to its documentation
    for details. `job_directives_skip` is an optional parameter, used to ignore errors
    in cases where auto-generated job script inserts some commands that your particular
    cluster does not recognize. `job_script_prologue` is also an optional parameter
    that specifies shell commands to be run at each worker spawn. This is a good place
    to ensure proper Python environments are set up, or a cluster-specific setup script.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Dask-jobqueue 库将您的 Dask 设置和配置转换为一个作业脚本，该脚本将提交到 HPC 集群。以下示例启动了一个包含 Slurm 工作节点的集群，对其他
    HPC API，语义类似。Dask-MPI 使用稍有不同的模式，因此请务必参考其文档获取详细信息。`job_directives_skip` 是一个可选参数，用于忽略自动生成的作业脚本插入您的特定集群不识别的命令的错误。`job_script_prologue`
    也是一个可选参数，指定在每次工作节点生成时运行的 shell 命令。这是确保设置适当的 Python 环境或特定集群设置脚本的好地方。
- en: Tip
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Make sure HPC cluster specs for worker memory and cores are correctly matched
    in `resource_spec` arguments, which are given to your HPC system itself to request
    the workers. The former is used for the Dask scheduler to set its internals; the
    latter is for you to request the resource within the HPC.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 确保工作节点的内存和核心的 HPC 集群规格在`resource_spec`参数中正确匹配，这些参数将传递给您的 HPC 系统本身来请求工作节点。前者用于
    Dask 调度器设置其内部；后者用于您在 HPC 内部请求资源。
- en: HPC systems often leverage high-performance network interface on top of a standard
    Ethernet network, and this has become a crucial way to speed up data movement.
    You can pass an optional interface parameter, as shown in [Example 12-6](#ex_deploy_SLURM_by_hand),
    to instruct Dask to use the higher-bandwidth network. If you are not sure which
    interfaces are available, type in `ifconfig` on your terminal, and it will show
    infiniband, often `ib0`, as one of the available network interfaces.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: HPC 系统通常利用高性能网络接口，这是在标准以太网网络之上加快数据移动的关键方法。您可以通过将可选的接口参数传递给 Dask（如在 [示例 12-6](#ex_deploy_SLURM_by_hand)
    中所示），以指示其使用更高带宽的网络。如果不确定哪些接口可用，请在终端上输入`ifconfig`，它将显示 Infiniband，通常为 `ib0`，作为可用网络接口之一。
- en: Finally, the cores and memory description are per-worker resources, and `n_workers`
    specifies how many jobs you want to queue initially. You can scale and add more
    workers after the fact, as we do in [Example 12-6](#ex_deploy_SLURM_by_hand),
    with the `cluster.scale()` command.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，核心和内存描述是每个工作节点资源，`n_workers` 指定您想要最初排队的作业数量。您可以像在 [示例 12-6](#ex_deploy_SLURM_by_hand)
    中那样，在事后扩展和添加更多工作节点，使用 `cluster.scale()` 命令。
- en: Tip
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Some HPC systems use GB when they mean 1,024-based units. Dask-jobqueue sticks
    with the proper notation of GiB. 1 GB is 1,000³ bytes, and 1 GiB is 1,024³ bytes.
    Academic settings often use binary measurements, while commercial settings usually
    opt for SI units, hence the discrepancy.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一些HPC系统在使用GB时实际上是指1024为基础的单位。Dask-jobqueue坚持使用GiB的正确符号。1 GB等于1000³字节，而1 GiB等于1024³字节。学术设置通常使用二进制测量单位，而商业设置通常选择SI单位，因此存在差异。
- en: Before running Dask in a new environment, you should check the job script that
    is auto-generated by Dask-jobqueue for unsupported commands. While Dask’s jobqueue
    library tries to work with many HPC systems, it may not have all of the quirks
    of your institution’s setup. If you are familiar with the cluster capabilities,
    you may find unsupported commands by calling `print(cluster.job_script())`. You
    can also try to run a small version of your job first with a limited number of
    workers and see where they fail. If you find any issues with the script, you should
    use the `job_directives_skip` parameter to skip the unsupported components, as
    outlined in [Example 12-6](#ex_deploy_SLURM_by_hand).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在新环境中运行Dask之前，您应该检查由Dask-jobqueue自动生成的作业脚本，以查找不受支持的命令。虽然Dask的作业队列库尝试与许多HPC系统兼容，但可能不具备您机构设置的所有特殊性。如果您熟悉集群的能力，可以通过调用`print(cluster.job_script())`来查找不受支持的命令。您还可以尝试先运行一个小版本的作业，使用有限数量的工作节点，看看它们在哪里失败。如果发现脚本存在任何问题，应使用`job_directives_skip`参数跳过不受支持的组件，如[示例 12-6](#ex_deploy_SLURM_by_hand)所述。
- en: Example 12-6\. Deploying Dask on an HPC cluster by hand
  id: totrans-67
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 12-6\. 手动在HPC集群上部署Dask
- en: '[PRE5]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In [Example 12-7](#ex_slurm_deployment_tuning) we tie together many of the concepts
    we’ve introduced. Here we run a delayed execution over some asynchronous task
    using Dask delayed, which is deployed on a Slurm cluster. The example also combines
    several logging strategies we’ve mentioned, such as revealing the underlying deployment
    HPC job script, as well as providing a progress bar for users to track in a notebook
    or in their CLI of choice.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例 12-7](#ex_slurm_deployment_tuning)中，我们整合了许多我们介绍的概念。在这里，我们使用Dask delayed执行了一些异步任务，该任务部署在一个Slurm集群上。该示例还结合了我们提到的几种日志记录策略，例如显示底层部署的HPC作业脚本，并为用户在笔记本或所选择的CLI中提供进度条以跟踪进度。
- en: Example 12-7\. Deploying Dask using jobqueue over Slurm with Dask futures
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 12-7\. 使用Dask futures在Slurm上通过jobqueue部署Dask
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Tip
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Always ensure your walltime requests don’t run afoul of HPC resource managers.
    For example, Slurm has a backfill scheduler that applies its own logic, and if
    you request too long a walltime, your request for compute resources can get stuck
    in the queue, unable to spin up on time. In such a case, the Dask client may error
    out with a non-descriptive message, such as “Failed to start worker process. Restarting.”
    At the time of writing, there aren’t many ways to surface specific deployment
    issues without some logging code from the user end.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 始终确保您的walltime请求不会违反HPC资源管理器的规则。例如，Slurm有一个后台填充调度程序，应用其自己的逻辑，如果您请求的walltime过长，则可能会导致您的计算资源请求在队列中被卡住，无法按时启动。在这种情况下，Dask客户端可能会因为“Failed
    to start worker process. Restarting.”等非描述性消息而报错。在撰写本文时，尚没有太多方法可以从用户端的日志代码中突显特定的部署问题。
- en: On the more advanced end, you can control your cluster configuration by updating
    the Dask-jobqueue YAML file, which is generated on first run and stored at */.config/dask/jobqueue.yaml*.
    The jobqueue configuration file contains, commented out, default configurations
    for many different types of clusters. To get started editing the file, uncomment
    the cluster type you are using (e.g., Slurm), and then you can change the values
    to meet your specific needs. The jobqueue configuration file allows you to configure
    additional parameters not available through the Python constructor.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在更高级的情况下，您可以通过更新在第一次运行时生成并存储在*/.config/dask/jobqueue.yaml*路径下的Dask-jobqueue
    YAML文件来控制集群配置。作业队列配置文件包含了许多不同类型集群的默认配置，这些配置被注释掉了。要开始编辑该文件，取消注释您正在使用的集群类型（例如，Slurm），然后您可以更改值以满足您的特定需求。作业队列配置文件允许您配置通过Python构造函数无法访问的附加参数。
- en: If Dask starts to run out of memory, it will by default start to write data
    out to disk (called spill-to-disk). This is normally great, since we tend to have
    more disk than memory, and while it is slower, it’s not that much slower. However,
    on HPC environments, the default location that Dask may write to could be a network
    storage drive, which will be as slow as transferring the data on the network.
    You should make sure Dask writes to local storage. You can ask your cluster administrator
    for the local scratch directory or use `df -h` to see where different storage
    devices are mapped. If you don’t have local storage available, or if it’s too
    small, you can also turn off spill-to-disk. Both disabling and changing the location
    of spill-to-disk on clusters can be configured in the *~/.config/dask/distributed.yaml*
    file (also created on first run).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 Dask 开始内存不足，它将默认开始将数据写入磁盘（称为溢写到磁盘）。这通常很好，因为我们通常有更多的磁盘空间而不是内存，尽管它较慢，但速度并不会慢太多。但是，在
    HPC 环境中，Dask 可能写入的默认位置可能是网络存储驱动器，这将像在网络上传输数据一样慢。您应确保 Dask 写入本地存储。您可以向集群管理员询问本地临时目录，或者使用
    `df -h` 查看不同存储设备映射到哪里。如果没有可用的本地存储，或者存储太小，还可以关闭溢写到磁盘功能。在集群上配置禁用和更改溢写到磁盘的位置可以在 *~/.config/dask/distributed.yaml*
    文件中进行（首次运行时也会创建此文件）。
- en: Tip
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: '*Adaptive scaling* is a great way to scale your job up and down while your
    application is running, especially on busy shared machines like HPC systems. However,
    each HPC system is unique, and sometimes the way Dask-jobqueue handles adaptive
    scaling creates issues. We have encountered such a problem when running Dask adaptive
    scaling on Slurm using jobqueue but with some effort were able to configure it
    properly.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '*自适应缩放* 是在应用程序运行时调整作业大小的好方法，特别是在忙碌的共享机器（如 HPC 系统）上。然而，每个 HPC 系统都是独特的，有时 Dask-jobqueue
    处理自适应缩放方式可能会出现问题。我们在使用 jobqueue 在 Slurm 上运行 Dask 自适应缩放时遇到了这样的问题，但通过一些努力，我们能够正确配置它。'
- en: Dask also uses files for locking, which can cause issues when using a shared
    network drive, as is common in HPC clusters. If there are multiple workers working
    simultaneously, it uses a locking mechanism, which excludes another process from
    accessing this file, to orchestrate itself. Some issues on HPC can come down to
    an incomplete locking transaction, or an inability to write a file on disk due
    to administrative restrictions. Worker configs can be toggled to disable this
    behavior.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 也使用文件进行锁定，在使用共享网络驱动器时可能会出现问题，这在 HPC 集群中很常见。如果有多个工作进程同时运行，它会使用锁定机制，该机制会排除其他进程访问此文件，以协调自身。在
    HPC 上的一些问题可能归结为锁定事务不完整，或者由于管理限制而无法在磁盘上写入文件。可以切换工作进程配置以禁用此行为。
- en: Tip
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Cluster parameters, such as memory allocation and number of jobs, workers, processes,
    and CPUs per task, are quite sensitive to user input and can be difficult to grasp
    at the beginning. For example, if you launch an HPC cluster with multiple processes,
    each process will take a fraction of total allocated memory. Ten processes with
    30 GB of memory would mean each process gets 3 GB of memory. If your workflow
    at peak has more than 95% of the process memory (2.85 GB in our example), your
    process will be paused and even terminated early due to memory overflow risks,
    potentially resulting in a failed task. For more on memory management, refer to
    [“Worker Memory Management”](#worker_memory_management_ch12_1687869893068).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 集群参数，如内存分配和作业数、工作进程、进程和任务每个任务的 CPU 数，对用户输入非常敏感，初学者可能难以理解。例如，如果使用多个进程启动 HPC 集群，则每个进程将占用总分配内存的一部分。10
    个进程配备 30 GB 内存意味着每个进程获取 3 GB 内存。如果您的工作流在峰值时占用了超过 95% 的进程内存（例如我们的示例中的 2.85 GB），您的进程将因内存溢出风险而被暂停甚至提前终止，可能导致任务失败。有关内存管理的更多信息，请参阅
    [“工作进程内存管理”](#worker_memory_management_ch12_1687869893068)。
- en: For HPC users, most processes you launch will have a limited amount of walltime
    that the job is allowed to stay on. You can stagger the creation of workers in
    such a way that you will have at least one worker running at all times, creating
    an infinite worker loop. Alternatively, you can also stagger the creation and
    end of the workers, so that you avoid all of the workers ending simultaneously.
    [Example 12-8](#ex_hpc_infinite_workers) shows you how.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 HPC 用户，大多数启动的进程都将有一个有限的墙上时间，该作业允许保持运行。您可以以一种方式交错地创建工作进程，以便始终至少有一个工作进程在运行，从而创建一个无限工作循环。或者，您还可以交错地创建和结束工作进程，以避免所有工作进程同时结束。[示例 12-8](#ex_hpc_infinite_workers)
    展示了如何做到这一点。
- en: Example 12-8\. Dask worker management through adaptive scaling
  id: totrans-82
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 12-8\. 自适应缩放管理 Dask 工作节点
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Tip
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Different workers can have different startup times, as well as contain different
    amounts of data, which impacts the cost of fault recovery.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的工作节点启动时间可能不同，并且包含的数据量也可能不同，这会影响故障恢复的成本。
- en: While Dask has good tools to monitor its own behavior, sometimes the integration
    between Dask and your HPC cluster (or other cluster) can break. If you suspect
    jobqueue isn’t sending the right worker commands for your particular cluster,
    you can inspect the */.config/dask/jobqueue.yaml* file directly or dynamically
    at runtime or in your Jupyter notebook by running `config.get('jobqueue.yaml')`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Dask 有良好的工具来监控其自身的行为，但有时 Dask 与您的 HPC 集群（或其他集群）之间的集成可能会中断。如果您怀疑 jobqueue
    没有为特定集群发送正确的工作节点命令，您可以直接检查或在运行时动态检查 `/.config/dask/jobqueue.yaml` 文件，或者在 Jupyter
    笔记本中运行 `config.get('jobqueue.yaml')`。
- en: Connecting a Local Machine to an HPC Cluster
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将本地机器连接到 HPC 集群
- en: Part of running Dask remotely is being able to connect to the server to run
    your tasks. If you want to connect your client to a remote cluster, run Jupyter
    remotely, or just access the UI on a cluster, you’ll need to be able to connect
    to some ports on the remote machine.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 远程运行 Dask 的一部分是能够连接到服务器以运行您的任务。如果您希望将客户端连接到远程集群，远程运行 Jupyter，或者只是在集群上访问 UI，则需要能够连接到远程机器上的一些端口。
- en: Warning
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Another option is to have Dask bind to a public IP address, but without careful
    firewall rules, this means that anyone can access your Dask cluster, which is
    likely not your intention.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择是让 Dask 绑定到公共 IP 地址，但是如果没有仔细配置防火墙规则，这意味着任何人都可以访问您的 Dask 集群，这可能不是您的意图。
- en: In HPC environments you often already connect using SSH, so using SSH port forwarding
    is often the easiest way to connect. SSH port forwarding allows you to map a port
    on another computer to one on your local computer.^([6](ch12.xhtml#id906)) The
    default Dask monitoring port is 8787, but if that port is busy (or you configure
    a different one), Dask may bind to a different port. The Dask server prints out
    which ports it is bound to at start time. To forward port 8787 on a remote machine
    to the same local port, you could run `ssh -L localhost:8787:my-awesome-hpc-node.hpc.fake:8787`.
    You can use the same techniques (but with different port numbers) for a remote
    JupyterLab, or to connect a Dask client to a remote scheduler.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在 HPC 环境中，通常已经使用 SSH 进行连接，因此使用 SSH 端口转发通常是最简便的连接方式。SSH 端口转发允许您将另一台计算机上的端口映射到本地计算机上的一个端口。^([6](ch12.xhtml#id906))
    默认的 Dask 监控端口是 8787，但如果该端口已被占用（或者您配置了不同的端口），Dask 可能会绑定到其他端口。Dask 服务器在启动时会打印绑定的端口信息。要将远程机器上的
    8787 端口转发到本地相同的端口，您可以运行 `ssh -L localhost:8787:my-awesome-hpc-node.hpc.fake:8787`。您可以使用相同的技术（但使用不同的端口号）连接远程
    JupyterLab，或者将 Dask 客户端连接到远程调度程序。
- en: Tip
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If you want to leave a process running remotely (like JupyterLab), the screen
    command can be a great way of having a process last beyond a single session.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望远程保持运行某个进程（如 JupyterLab），`screen` 命令是让进程持续超出单个会话的好方法。
- en: With the immense popularity of notebooks, some HPC clusters have special tools
    to make it even easier to launch Jupyter notebooks. We recommend looking for your
    cluster administrator’s documentation on how to launch Jupyter notebooks, as you
    may accidentally create security issues if you don’t do it correctly.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 随着笔记本的广泛流行，一些 HPC 集群提供特殊工具，使启动 Jupyter 笔记本更加简便。我们建议查阅您集群管理员的文档，了解如何正确启动 Jupyter
    笔记本，否则可能会导致安全问题。
- en: Dask JupyterLab Extension and Magics
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dask JupyterLab 扩展和魔法命令
- en: You can run Dask in Jupyter like any other library, but Dask’s JupyterLab extensions
    make it easier to understand the status of your Dask job while it’s running.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以像运行其他库一样在 Jupyter 中运行 Dask，但是使用 Dask 的 JupyterLab 扩展可以更轻松地了解您的 Dask 作业在运行时的状态。
- en: Installing JupyterLab Extensions
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装 JupyterLab 扩展
- en: Dask’s lab extensions require `nodejs`, which can be installed with `conda install
    -c conda-forge nodejs`. If you are not using conda, it can also be installed with
    `brew install node` on Apple or `sudo apt install nodejs` on Ubuntu.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 的 lab 扩展需要安装 `nodejs`，可以通过 `conda install -c conda-forge nodejs` 安装。如果您没有使用
    conda，也可以在苹果上使用 `brew install node` 或者在 Ubuntu 上使用 `sudo apt install nodejs` 安装。
- en: Dask’s lab extensions package is available as `dask-labextension`.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 的 lab 扩展包名为 `dask-labextension`。
- en: Once you’ve installed the lab extension, it will show up with the Dask logo
    on the left side, as shown in [Figure 12-1](#dask_instance_on_jupyterlab_ch12_1685474991278).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完实验室扩展后，它将显示带有Dask标志的左侧，如[图 12-1](#dask_instance_on_jupyterlab_ch12_1685474991278)所示。
- en: '![spwd 1201](Images/spwd_1201.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![spwd 1201](Images/spwd_1201.png)'
- en: Figure 12-1\. A successfully deployed Dask instance on JupyterLab ([digital,
    color version](https://oreil.ly/TlOSc))
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-1\. 在JupyterLab上成功部署的Dask实例（[数字，彩色版本](https://oreil.ly/TlOSc))
- en: Launching Clusters
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启动集群
- en: From there you can launch your cluster. By default, the extension launches a
    local cluster, but you can configure it to use different deployment options, including
    Kubernetes, by editing *~/.config/dask*.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 从那里，您可以启动您的集群。默认情况下，该扩展程序启动一个本地集群，但您可以通过编辑*~/.config/dask*来配置它以使用不同的部署选项，包括Kubernetes。
- en: UI
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用户界面
- en: If you are using Dask’s JupyterLab extension (see [Figure 12-2](#jupyter-lab-extension_ch12_1685475054811)),
    it provides a link to the cluster UI as well as the ability to drag individual
    components into the Jupyter interface.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在使用Dask的JupyterLab扩展（参见[图 12-2](#jupyter-lab-extension_ch12_1685475054811)），它提供了一个到集群UI的链接，以及将单个组件拖放到Jupyter界面中的功能。
- en: '![spwd 1202](Images/spwd_1202.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![spwd 1202](Images/spwd_1202.png)'
- en: Figure 12-2\. Dask web UI inside JupyterHub using the JupyterLab extension ([digital,
    color version](https://oreil.ly/5UOHI))
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-2\. 在JupyterHub内使用JupyterLab扩展显示的Dask Web UI（[数字，彩色版本](https://oreil.ly/5UOHI)）
- en: The JupyterLab extension links to the Dask web UI, and you can also get a link
    through the cluster’s `repr`. If the cluster link does not work/is not accessible,
    you can try installing the `jupyter-server-proxy` extension so you can use the
    notebook host as a [jump host](https://oreil.ly/0eIhP).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: JupyterLab扩展程序链接到Dask Web UI，您还可以通过集群的`repr`获取链接。如果集群链接不起作用/无法访问，您可以尝试安装`jupyter-server-proxy`扩展程序，以便将笔记本主机用作[跳转主机](https://oreil.ly/0eIhP)。
- en: Watching Progress
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 观察进度
- en: Dask jobs tend to take a long time to run; otherwise we would not be putting
    in the effort to parallelize them. You can use Dask’s `progress` function from
    `dask​.dis⁠tributed` to track your futures’ progress inside your notebook itself
    (see [Figure 12-3](#dask_progress_monitoring_ch12_1685475108548)).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Dask作业通常需要很长时间才能运行；否则我们不会努力并行化它们。您可以使用Dask的`dask.distributed`中的`progress`函数来跟踪您笔记本中的未来进度（参见[图 12-3](#dask_progress_monitoring_ch12_1685475108548)）。
- en: '![spwd 1203](Images/spwd_1203.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![spwd 1203](Images/spwd_1203.png)'
- en: Figure 12-3\. Real-time Dask progress monitoring in JupyterHub ([digital, color
    version](https://oreil.ly/amaBq))
  id: totrans-113
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-3\. 在JupyterHub中实时监控Dask进度（[数字，彩色版本](https://oreil.ly/amaBq)）
- en: Understanding Dask Performance
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解Dask性能
- en: Tuning your Dask program involves understanding the intersection of many components.
    You will need to understand your code’s behavior and how it interacts with the
    data given and the machines. You can use Dask metrics to gain insight on much
    of this, but, especially if you did not create it, it’s important to look at the
    program as well.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 调整您的Dask程序涉及理解多个组件的交集。您需要了解您的代码行为以及其与给定数据和机器的交互方式。您可以使用Dask指标来深入了解其中的许多内容，但特别是如果不是您创建的代码，查看程序本身也很重要。
- en: Metrics in Distributed Computing
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分布式计算中的指标
- en: Distributed computing requires constantly making decisions and weighing the
    optimization of the cost and benefits of distributing workload against locally
    running the work. Most of that low-level decision making is delegated to the internals
    of Dask. The user should still monitor the runtime characteristics and make modifications
    to the code and configurations if needed.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式计算需要不断做出决策，并权衡分发工作负载的优化成本和收益。大部分低级别的决策都委托给Dask的内部。用户仍应监控运行时特性，并根据需要修改代码和配置。
- en: Dask automatically tracks relevant compute and runtime metrics. You can use
    this to help you decide how to store your data, as well as inform where to focus
    your time on optimizing your code.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Dask会自动跟踪相关的计算和运行时指标。您可以利用这一点来帮助决定如何存储数据，以及在优化代码时应该关注哪些方面。
- en: Of course, the cost of computation is more than just the compute time. Users
    should also consider the time spent transferring data over network, the memory
    footprint within the workers, GPU/CPU utilization rate, and disk I/O costs. These
    in turn let you understand the higher-level insights of data movement and computation
    flow, such as how much of the memory in a worker is used up in storing previous
    computation that hasn’t been passed on to the next computation, or what particular
    function is taking up the most amount of time. Monitoring these can help tune
    your cluster and code, but it also can help identify emergent computation patterns
    or logical bottlenecks that you can change.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，计算成本不仅仅是计算时间。用户还应考虑通过网络传输数据的时间，工作节点内存占用情况，GPU/CPU 利用率以及磁盘 I/O 成本。这些因素帮助理解数据移动和计算流的更高层次洞见，比如工作节点中有多少内存用于存储尚未传递给下一个计算的先前计算，或者哪些特定函数占用了大部分时间。监控这些可以帮助优化集群和代码，同时还可以帮助识别可能出现的计算模式或逻辑瓶颈，从而进行调整。
- en: Dask’s dashboard provides a lot of statistics and graphs to answer these questions.
    The dashboard is a web page tied to your Dask cluster at runtime. You can access
    it through your local machine or on the remote machine that it is running in,
    through methods we discussed earlier in this chapter. Here, we will cover a few
    of the ways you can get insights from the performance metrics and tune Dask accordingly
    to achieve better results.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 的仪表板提供了大量统计数据和图表来回答这些问题。该仪表板是一个与您的 Dask 集群在运行时绑定的网页。您可以通过本地机器或运行它的远程机器访问它，方法我们在本章前面已经讨论过。在这里，我们将覆盖一些从性能指标中获取洞见并据此调整
    Dask 以获得更好结果的方法。
- en: The Dask Dashboard
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dask 仪表板
- en: Dask’s dashboard contains many different pages, each of which can help with
    understanding different parts of your program.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 的仪表板包含许多不同页面，每个页面可以帮助理解程序的不同部分。
- en: Task stream
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 任务流
- en: 'The Task Stream dashboard gives a high-level view of each worker and its behavior.
    Exact methods invoked are color-coded, and you can inspect them by zooming in
    and out. Each row represents one worker. The custom-colored bars are user-induced
    tasks, and there are four preset colors to indicate common worker tasks: data
    transfer between workers, disk read and writes, serialization and deserialization
    times, and failed tasks. [Figure 12-4](#task_stream_ch12_1685475169009) shows
    a compute workload that is distributed over 10 workers and is well balanced, with
    no one worker finishing late, evenly distributed compute time, and minimal network
    IO overhead.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 任务流仪表板提供了每个工作节点及其行为的高级视图。精确调用的方法以颜色代码显示，并可以通过缩放来检查它们。每行代表一个工作节点。自定义颜色的条形图是用户生成的任务，有四种预设颜色表示常见的工作节点任务：工作节点之间的数据传输、磁盘读写、序列化和反序列化时间以及失败的任务。[图 12-4](#task_stream_ch12_1685475169009)
    展示了分布在 10 个工作节点上的计算工作负载，平衡良好，没有一个工作节点完成较晚，计算时间均匀分布，并且最小化了网络 IO 开销。
- en: '![spwd 1204](Images/spwd_1204.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![spwd 1204](Images/spwd_1204.png)'
- en: Figure 12-4\. A task stream with well-balanced workers ([digital, color version](https://oreil.ly/VbpGF))
  id: totrans-126
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-4\. 带有良好平衡工作节点的任务流（[数字版，彩色](https://oreil.ly/VbpGF)）
- en: On the other hand, [Figure 12-5](#task_stream_with_too_many_ch12_1685475203241)
    shows a situation in which compute is uneven. You can see that there is a lot
    of whitespace between computation, meaning the workers are blocked and are not
    actually computing during that time. Additionally, you see some workers start
    earlier and others finish later, hinting that there are some issues with distributing
    the job. This could be due to the inherent dependency of your code or suboptimal
    tuning. Changing the DataFrame or array chunk sizes might make these less fragmented.
    You do see that when the job starts on each worker, they take roughly the same
    amount of work, meaning the work itself is still balanced fairly well and distributing
    the workload is giving you good returns. This is a fairly contrived illustrative
    example, so this task only took a few seconds, but the same idea applies to longer
    and bulkier workloads as well.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，[图 12-5](#task_stream_with_too_many_ch12_1685475203241) 展示了计算不均匀的情况。你可以看到计算之间有很多空白，这意味着工作人员被阻塞，并且在此期间实际上没有计算。此外，您可以看到一些工作人员开始较早，而其他人结束较晚，暗示代码分发中存在问题。这可能是由于代码本身的依赖性或子优化调整不当所致。改变
    DataFrame 或数组块的大小可能会减少这些碎片化。您可以看到，每个工作人员启动工作时，他们处理的工作量大致相同，这意味着工作本身仍然相当平衡，并且分配工作负载带来了良好的回报。这是一个相当虚构的例子，因此此任务仅花了几秒钟，但相同的想法也适用于更长和更笨重的工作负载。
- en: '![spwd 1205](Images/spwd_1205.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![spwd 1205](Images/spwd_1205.png)'
- en: Figure 12-5\. A task stream with too many small data chunks ([digital, color
    version](https://oreil.ly/oI2ab))
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-5\. 任务流中有太多小数据块（[数字，彩色版](https://oreil.ly/oI2ab)）
- en: Memory
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内存
- en: You can monitor the memory usage, sometimes referred to as *memory pressure*,^([7](ch12.xhtml#id927))
    of each worker on the Bytes Stored portion (see [Figure 12-6](#memory_usage_by_worker_ch12_1685475242277)).
    These are by default color-coded to signify memory pressure within limits, approaching
    limit, and spilled to disk. Even if memory usage is within the limits, as it increases
    beyond 60% to 70%, you are likely to encounter performance slowdowns. Since memory
    usage is rising, internals of Python and Dask are going to run costlier garbage
    collection and memory optimization tasks in the background to keep it from rising.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以监视内存使用情况，有时称为*内存压力*，^([7](ch12.xhtml#id927)) 每个工作人员在“存储字节”部分的使用情况（参见[图 12-6](#memory_usage_by_worker_ch12_1685475242277)）。这些默认情况下颜色编码，表示在限制内存压力、接近限制和溢出到磁盘。即使内存使用在限制内，当其超过60%至70%时，可能会遇到性能减慢。由于内存使用正在上升，Python
    和 Dask 的内部将运行更昂贵的垃圾收集和内存优化任务，以防止其上升。
- en: '![spwd 1206](Images/spwd_1206.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![spwd 1206](Images/spwd_1206.png)'
- en: Figure 12-6\. Memory usage by worker in the monitoring UI ([digital, color version](https://oreil.ly/9GfNp))
  id: totrans-133
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-6\. 监视 UI 中每个工作人员的内存使用情况（[数字，彩色版](https://oreil.ly/9GfNp)）
- en: Task progress
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 任务进度
- en: You can see the aggregated view of task completion in the progress bar in [Figure 12-7](#progress_monitoring_by_tasks_ch12_1685475294066).
    The order of execution is from top to bottom, although that does not always mean
    it’s completely sequential. The colors of the bars are particularly information-rich
    for tuning. The solid gray on the far right for `sum()` and `random_sample()`
    in [Figure 12-7](#progress_monitoring_by_tasks_ch12_1685475294066) means tasks
    that are ready to run, with dependent data ready but not yet assigned to a worker.
    The bold non-gray colors mean tasks are finished, with result data that is waiting
    for the next sequence of tasks to take it. The fainter non-gray color blocks signify
    tasks that are done, with result data handed off and purged from memory. Your
    goal is to keep the solid-color blocks within a manageable size, to make sure
    you utilize most of your allocated memory.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过进度条看到任务完成的汇总视图，参见[图 12-7](#progress_monitoring_by_tasks_ch12_1685475294066)。执行顺序是从上到下，虽然这并不总是完全顺序的。条的颜色对调整特别信息丰富。在[图 12-7](#progress_monitoring_by_tasks_ch12_1685475294066)中，`sum()`
    和 `random_sample()` 的实心灰色表示任务准备运行，依赖数据已准备好但尚未分配给工作人员。加粗的非灰色条表示任务已完成，结果数据等待下一个任务序列处理。较淡的非灰色块表示任务已完成，结果数据已移交并从内存中清除。您的目标是保持实心色块的可管理大小，以确保充分利用分配的大部分内存。
- en: '![spwd 1207](Images/spwd_1207.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![spwd 1207](Images/spwd_1207.png)'
- en: Figure 12-7\. Progress monitoring by task, summed over all workers ([digital,
    color version](https://oreil.ly/v8XoP))
  id: totrans-137
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-7\. 按任务监视的进度，所有工作人员汇总（[数字，彩色版](https://oreil.ly/v8XoP)）
- en: Task graph
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 任务图
- en: Similar information is available on Task Graph (see [Figure 12-8](#task_graph_ch12_1685475338667)),
    from the view of individual tasks. You may be familiar with these types of MapReduce-like
    directed acyclic graphs. Order of computation is shown from left to right, with
    your tasks originating from many parallel workloads, distributed among your workers,
    and ending up with an outcome that is distributed among 10 workers, in this case.
    This graph is also an accurate low-level depiction of task dependencies. The color
    coding also highlights where in the computational life cycle each work and data
    is currently sitting in. By looking at this, you can get a sense of which tasks
    are bottlenecks and thus are potentially good places to start optimizing your
    code.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的信息也可以在任务图上找到（参见 [图 12-8](#task_graph_ch12_1685475338667)），从单个任务的视角来看。您可能熟悉这些类似
    MapReduce 的有向无环图。计算顺序从左到右显示，您的任务来源于许多并行工作负载，分布在工作人员之间，并以此种方式结束，最终得到由 10 个工作人员分布的结果。该图还准确地描述了任务依赖关系的低级视图。颜色编码还突出显示了计算生命周期中当前每个工作和数据所处的位置。通过查看这些信息，您可以了解哪些任务是瓶颈，因此可能是优化代码的良好起点。
- en: '![spwd 1208](Images/spwd_1208.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![spwd 1208](Images/spwd_1208.png)'
- en: Figure 12-8\. A task graph showing the color-coded status of each task and its
    preceding and succeeding tasks ([digital, color version](https://oreil.ly/RSBiv))
  id: totrans-141
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-8\. 显示每个任务的颜色编码状态及其前后任务的任务图（[数字，彩色版本](https://oreil.ly/RSBiv)）
- en: The Workers tab allows you to see real-time CPU, memory, and disk IO, among
    other things (see [Figure 12-9](#worker_monitoring_tab_ch12_1685475400589)). Monitoring
    this tab can be useful if you suspect that your worker is running out of memory
    or disk space. Some of the remedies for that can include allocating more memory
    to the workers or choosing a different chunking size or method for the data.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 工作人员标签页允许您实时查看 CPU、内存和磁盘 IO 等情况（参见 [图 12-9](#worker_monitoring_tab_ch12_1685475400589)）。如果您怀疑您的工作人员内存不足或磁盘空间不足，监视此标签页可能会很有用。解决这些问题的一些方法可以包括为工作人员分配更多内存或选择不同的数据分块大小或方法。
- en: '[Figure 12-10](#worker_event_monitoring_tab_ch12_1685475374935) shows worker
    event monitoring. Dask’s distributed scheduler runs on a loop called event loop,
    which manages all tasks that are to be scheduled and the workers, managing the
    execution, communication, and status of the computation. The `event_loop_interval`
    metric is a measure of average time between the iterations of this loop for each
    worker. A shorter time means it took less time for the scheduler to do its management
    tasks for this worker. If this goes higher, this could mean a number of things,
    including suboptimal network configuration, resource contention, or high communication
    overhead. If this remains high, you might want to look into whether you have enough
    resources for the compute, and either allocate larger resources per worker or
    rechunk the data into smaller portions.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 12-10](#worker_event_monitoring_tab_ch12_1685475374935) 显示了工作事件监视。Dask 的分布式调度器在称为事件循环的循环上运行，该循环管理要安排的所有任务以及管理执行、通信和计算状态的工作人员。`event_loop_interval`
    指标衡量了每个工作人员的此循环迭代之间的平均时间。较短的时间意味着调度器在为该工作人员执行其管理任务时花费的时间较少。如果此时间增加，可能意味着诸如网络配置不佳、资源争用或高通信开销等问题。如果保持较高，您可能需要查看是否为计算分配了足够的资源，并且可以为每个工作人员分配更大的资源或重新对数据进行分块。'
- en: '![spwd 1209](Images/spwd_1209.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![spwd 1209](Images/spwd_1209.png)'
- en: Figure 12-9\. Worker monitoring for a Dask cluster with 10 workers ([digital,
    color version](https://oreil.ly/QlDQE))
  id: totrans-145
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-9\. 具有 10 个工作人员的 Dask 集群的工作人员监视（[数字，彩色版本](https://oreil.ly/QlDQE)）
- en: '![spwd 1210](Images/spwd_1210.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![spwd 1210](Images/spwd_1210.png)'
- en: Figure 12-10\. Worker event monitoring for a Dask cluster ([digital, color version](https://oreil.ly/vU6lG))
  id: totrans-147
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-10\. Dask 集群的工作事件监视（[数字，彩色版本](https://oreil.ly/vU6lG)）
- en: The System tab allows you to track the CPU, memory, network bandwidth, and file
    descriptors. CPU and memory are easy to understand. An HPC user would be keen
    to track network bandwidth if their job requires heavy amounts of data to be moved
    around. File descriptors here track the number of input and output resources the
    system has open at the same time. This includes actual files open for read/write,
    but also network sockets that communicate between machines. There is a limit to
    how many of these descriptors a system can have open at the same time, so a very
    complicated job or a leaky workload that opens a lot of connections, gets stuck,
    and does not close can create trouble. Similar to leaky memory, this can lead
    to performance issues as time goes on.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 系统标签允许您跟踪 CPU、内存、网络带宽和文件描述符的使用情况。CPU 和内存易于理解。如果作业需要大量数据传输，那么 HPC 用户会特别关注网络带宽。这里的文件描述符跟踪系统同时打开的输入和输出资源数量。这包括实际打开的读/写文件，以及在机器之间通信的网络套接字。系统同时可以打开的描述符数量有限，因此一个非常复杂的作业或者开启了许多连接但未关闭的漏洞工作负载可能会造成问题。类似于内存泄漏，这会随着时间的推移导致性能问题。
- en: The Profile tab allows you to see the amount of time spent on executing code,
    down to the exact function call, on an aggregate level. This can be helpful in
    identifying tasks that create a bottleneck. [Figure 12-11](#task_duration_histogram_ch12_1685475458634)
    shows a task duration histogram, which shows a fine-grained view of each task
    and all the subroutines needed to call for that task, and their runtime. This
    can help quickly identify a task that is consistently longer than others.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Profile 标签允许您查看执行代码所花费的时间，可以精确到每次函数调用的细节，以聚合级别显示。这有助于识别造成瓶颈的任务。[Figure 12-11](#task_duration_histogram_ch12_1685475458634)
    显示了一个任务持续时间直方图，展示了每个任务及其所有调用的子例程的细粒度视图，以及它们的运行时间。这有助于快速识别比其他任务持续时间更长的任务。
- en: '![spwd 1211](Images/spwd_1211.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![spwd 1211](Images/spwd_1211.png)'
- en: Figure 12-11\. Task duration histogram for a Dask job ([digital, color version](https://oreil.ly/dIq2o))
  id: totrans-151
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-11\. Dask 作业的任务持续时间直方图（[数字，彩色版本](https://oreil.ly/dIq2o)）
- en: Tip
  id: totrans-152
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: You can change the logging intervals with the `distributed​.cli⁠ent.scheduler-info-interval`
    argument within Dask client configurations.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过 Dask 客户端配置中的 `distributed​.cli⁠ent.scheduler-info-interval` 参数更改日志记录间隔。
- en: Saving and Sharing Dask Metrics/Performance Logs
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保存和共享 Dask 指标/性能日志
- en: You can monitor Dask in real time with the dashboard, but the dashboard will
    disappear once you close out your cluster. You can save the HTML page, as well
    as export the metrics as a DataFrame, and write out custom code for metrics (see
    [Example 12-9](#ex_generate_performance_report)).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过仪表板实时监控 Dask，但一旦关闭集群，仪表板将消失。您可以保存 HTML 页面，导出指标为 DataFrame，并编写用于指标的自定义代码（参见
    [Example 12-9](#ex_generate_performance_report)）。
- en: Example 12-9\. Generating and saving the Dask dashboard to file
  id: totrans-156
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 12-9\. 生成并保存 Dask 仪表板至文件
- en: '[PRE8]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: You can generate a performance report manually for any block of computation,
    without having to save the entire runtime report, with the code in [Example 12-9](#ex_generate_performance_report).
    Any computation that you pass within `performance_report(“filename”)` will be
    saved under that file. Note that under the hood, this requires Bokeh to be installed.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以为任何计算块手动生成性能报告，而无需保存整个运行时报告，只需使用 [Example 12-9](#ex_generate_performance_report)
    中的代码执行 `performance_report("filename")`。请注意，在幕后，这需要安装 Bokeh。
- en: For much more heavy-duty usage, you can use Dask with Prometheus, the popular
    Python metrics and alerting tool. This requires you to have Prometheus deployed.
    Then through Prometheus, you can hook up other tools, like Grafana for visualization
    or PagerDuty for alerts.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更加重型的使用，您可以结合流行的 Python 指标和警报工具 Prometheus 使用 Dask。这需要您已部署 Prometheus。然后通过
    Prometheus，您可以连接其他工具，例如用于可视化的 Grafana 或用于警报的 PagerDuty。
- en: Dask’s distributed scheduler provides the metrics info as a task stream object
    without using the UI itself. You can access the information in the Task Stream
    UI tab from Dask directly, down to the level of lines of code that you want this
    to be profiled over. [Example 12-10](#ex_get_task_stream) demonstrates how to
    use a task stream and then extract some statistics out of it into a small pandas
    DataFrame for further analysis and sharing.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 的分布式调度器提供了作为任务流对象的度量信息，而无需使用 UI 本身。您可以直接从 Dask 的任务流 UI 标签中访问信息，以及您希望对其进行性能分析的代码行级别。[示例 12-10](#ex_get_task_stream)展示了如何使用任务流，并将一些统计信息提取到一个小的
    pandas DataFrame 中，以供进一步分析和分享。
- en: Example 12-10\. Generating and computing Dask’s runtime statistics with task
    stream
  id: totrans-161
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 12-10\. 使用任务流生成和计算 Dask 运行时统计
- en: '[PRE9]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Advanced Diagnostics
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高级诊断
- en: You can insert custom metrics using the `dask.distributed.diagnostics` class.
    One of the functions here is a `MemorySampler` context manager. When you run your
    Dask code within `ms.sample()`, it records a detailed memory usage on cluster.
    [Example 12-11](#ex_memory_sampler), while contrived, shows how you would run
    the same compute over two different cluster configurations and then plot to compare
    the two different environment configurations.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`dask.distributed.diagnostics`类插入自定义度量。其中一个函数是`MemorySampler`上下文管理器。当您在`ms.sample()`中运行您的
    Dask 代码时，它会记录集群上的详细内存使用情况。[示例 12-11](#ex_memory_sampler)虽然是人为的，但展示了如何在两种不同的集群配置上运行相同的计算，然后绘制以比较两个不同的环境配置。
- en: Example 12-11\. Inserting a memory sampler for your code
  id: totrans-165
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 12-11\. 为您的代码插入内存采样器
- en: '[PRE10]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Scaling and Debugging Best Practices
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展和调试最佳实践
- en: Here, we discuss some of the commonly identified issues and overlooked considerations
    when running your code in distributed cluster settings.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们讨论了在分布式集群设置中运行代码时常见的问题和被忽视的考虑因素。
- en: Manual Scaling
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 手动扩展
- en: If your cluster manager supports it, you can scale up and down the number of
    workers by calling `scale` with the desired number of workers. You can also tell
    the Dask scheduler to wait until the requested number of workers are allocated
    and then proceed with computation with the `client.wait_for_workers(n_workers)`
    command. This can be useful for training certain ML models.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的集群管理器支持，您可以通过调用`scale`并设置所需的工作节点数来进行工作节点的动态扩展和缩减。您还可以告知 Dask 调度器等待直到请求的工作节点数分配完成，然后再使用`client.wait_for_workers(n_workers)`命令进行计算。这在训练某些机器学习模型时非常有用。
- en: Adaptive/Auto-scaling
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自适应/自动扩展
- en: We briefly touched on adaptive scaling in previous chapters. You can enable
    auto/adaptive scaling on your cluster by calling `adapt()` on the Dask client.
    The scheduler analyzes the computation and invokes the `scale` command to add
    or remove workers. The Dask cluster types—KubeCluster, PBSCluster, LocalClusters,
    etc.—are the cluster classes that handle actual requests and scaling up and down
    of the workers. If you see issues with adaptive scaling, ensure that your Dask
    is correctly asking for resources from the cluster manager. Of course, for auto-scaling
    in Dask to work, you have to be able to scale your own resource allocations within
    the cluster that you are running your job on, be it HPC, managed cloud, etc. We
    already introduced adaptive scaling in [Example 12-11](#ex_memory_sampler); refer
    to that example for code snippets.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前几章节简要介绍了自适应扩展。您可以通过在 Dask 客户端上调用`adapt()`来启用集群的自动/自适应扩展。调度器会分析计算并调用`scale`命令来增加或减少工作节点。Dask
    集群类型——KubeCluster、PBSCluster、LocalClusters 等——是处理实际请求以及工作节点的动态扩展和缩减的集群类。如果在自适应扩展中遇到问题，请确保您的
    Dask 正确地向集群管理器请求资源。当然，要使 Dask 中的自动扩展生效，您必须能够在运行作业的集群内部自行扩展资源分配，无论是 HPC、托管云等。我们在[示例 12-11](#ex_memory_sampler)中已经介绍了自适应扩展；请参考该示例获取代码片段。
- en: Persist and Delete Costly Data
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 持久化和删除成本高昂的数据
- en: Some intermediate results can be used further down in the code execution, but
    not immediately after. In these cases, Dask might delete the data, not realizing
    it will need it further down, and you will end up needing another round of costly
    computation. If you identify this pattern, you can use the `.persist()` command.
    With this command, you should also use Python’s built-in `del` command to ensure
    that the data is removed when it’s no longer needed.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 一些中间结果可以在代码执行的后续阶段使用，但不能立即使用。在这些情况下，Dask 可能会删除数据，而不会意识到在稍后会再次需要它，从而需要进行另一轮昂贵的计算。如果识别出这种模式，可以使用`.persist()`命令。使用此命令时，还应使用
    Python 的内置`del`命令，以确保数据在不再需要时被删除。
- en: Dask Nanny
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dask Nanny
- en: Dask Nanny is a process that manages workers. Its job is to prevent workers
    from exceeding its resource limits, leading to an unrecoverable machine state.
    It constantly monitors CPU and memory usage for the worker and triggers memory
    clean-up and compaction. If the worker reaches a bad state, it will automatically
    restart the worker and try to recover the previous state.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Dask Nanny 是一个管理工作进程的进程。它的工作是防止工作进程超出其资源限制，导致机器状态无法恢复。它不断监视工作进程的 CPU 和内存使用情况，并触发内存清理和压缩。如果工作进程达到糟糕的状态，它会自动重新启动工作进程，并尝试恢复先前的状态。
- en: Complications may arise if a worker that contains a computationally expensive
    and large chunk of data is lost for some reason. The nanny will restart the worker
    and try to redo the work that led to that point. During that, other workers will
    also hold on to data that they were working on, leading to a spike in memory usage.
    The strategies to remedy this will vary, from disabling the nanny to modifying
    chunking sizes, worker size, and so on. If this happens often, you should consider
    persisting, or writing that data to disk.^([8](ch12.xhtml#id954))
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如果某个工作进程因某种原因丢失，其中包含计算密集和大数据块，可能会出现问题。Nanny 将重新启动工作进程，并尝试重新执行导致问题的工作。在此期间，其他工作进程也将保留它们正在处理的数据，导致内存使用量激增。解决此类问题的策略各不相同，可以禁用
    Nanny，修改块大小、工作进程大小等。如果此类情况经常发生，应考虑持久化或将数据写入磁盘。^([8](ch12.xhtml#id954))
- en: If you see error messages such as “Worker exceeded 95% memory budget. Restarting,”
    nanny is likely where it came from. It’s the class responsible for starting workers,
    monitoring, terminating, and restarting the workers. This memory fraction, as
    well as spillover location, can be set in the *distributed.yaml* configuration
    file. HPC users can turn the nanny’s memory monitoring off if the system itself
    has its own memory management strategies. If the system also restarts killed jobs,
    you could turn off the nanny with the `--no-nanny` option.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如果看到诸如“工作进程超过 95% 内存预算。正在重启”之类的错误消息，则很可能是 Nanny 引起的。它是负责启动、监视、终止和重新启动工作进程的类。这种内存分数以及溢出位置可以在
    *distributed.yaml* 配置文件中设置。如果 HPC 用户的系统本身具有自己的内存管理策略，则可以关闭 Nanny 的内存监控。如果系统还重新启动被终止的作业，则可以使用
    `--no-nanny` 选项关闭 Nanny。
- en: Worker Memory Management
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作进程内存管理
- en: By default, when the worker’s memory is around 60% full, it starts sending some
    data to disk. At over 80%, it stops allocating new data. At 95%, the worker is
    terminated preemptively, in order to avoid running out of memory. This means after
    your worker’s memory is more than 60% full, there will be performance degradations,
    and it’s usually best practice to keep memory pressure lower.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，当工作进程的内存使用达到大约 60% 时，它开始将一些数据发送到磁盘。超过 80% 时，停止分配新数据。达到 95% 时，工作进程会预防性地终止，以避免内存耗尽。这意味着在工作进程的内存使用超过
    60% 后，性能会下降，通常最好保持内存压力较低。
- en: Advanced users can use Active Memory Manager, a daemon that optimizes memory
    usage of workers on a holistic view of the cluster. You give this manager a particular
    goal to optimize for, such as reducing replication of the same data within the
    cluster, or `retire_workers`, a special advanced use case where you do memory
    transfer from one worker to another when the worker is being retired, or other
    custom policies. In some cases, Active Memory Manager has been shown to decrease
    memory usage up to 20% for the same task.^([9](ch12.xhtml#id959))
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 高级用户可以使用 Active Memory Manager，这是一个守护进程，从整体视角优化集群工作进程的内存使用。您可以为此管理器设定特定的优化目标，例如减少集群内相同数据的复制，或者在工作进程退休时进行内存转移，或其他自定义策略。在某些情况下，Active
    Memory Manager 已被证明能够减少相同任务的内存使用高达 20%。^([9](ch12.xhtml#id959))
- en: Cluster Sizing
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集群规模
- en: 'Auto/adaptive scaling takes care of the question of “how many” workers but
    not of “how big” each worker should be. That said, here are some general rules
    of thumb:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 自动/自适应缩放解决了“有多少”工作进程的问题，但没有解决每个工作进程“有多大”的问题。尽管如此，以下是一些经验法则：
- en: Use smaller worker size when debugging, unless you expect that the bug is due
    to the large number of workers given.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在调试时使用较小的工作进程大小，除非你预期 bug 是由于大量工作进程导致的。
- en: Size up worker memory allocation with the input data size and number of workers
    you are using.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据输入数据大小和使用的工作进程数量调整工作进程内存分配。
- en: The number of chunks in the data should roughly match the number of workers.
    Fewer workers than chunks will lead to some chunks not being worked on until the
    first round of computation is over, leading to a larger memory footprint of intermediate
    data. Conversely, having more workers than the number of chunks will result in
    idling workers.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据中的块数应大致匹配工作人员的数量。工作人员少于块数将导致一些块在第一轮计算结束之前未被处理，从而导致中间数据的内存占用量较大。相反，工作人员多于块数将导致空闲的工作人员。
- en: If you have the option of having higher worker count and smaller individual
    worker memory (versus smaller worker count and larger individual worker memory),
    analyze your data’s chunk sizes. That chunk must fit in one worker with some room
    for computation, setting the minimum memory needed for your worker.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你可以选择高工作人数和较小的单个工作内存（与较少工作人数和更大的单个工作内存相比），分析你的数据块大小。这些块必须适合一个工作人员进行一些计算，并设置工作人员所需的最小内存。
- en: Fine-tuning your machine sizes can become a never-ending exercise, so it’s important
    to know what “good enough” is for your purposes.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 调整你的机器大小可能成为一个永无止境的练习，所以了解对你的目的来说什么是“足够好”的很重要。
- en: Chunking, Revisited
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 块划分再探讨
- en: We have briefly covered chunks and chunk sizes in earlier chapters. Now we expand
    on this to cluster scale. Chunk size and worker sizes are integral to how Dask
    functions, as it uses a block-level view of computation and data within its task
    graph–based execution model. It’s the essential parameter that determines how
    the distributed computation will work. While using Dask and other distributed
    systems, we find this is one of the essential ideas to keep in mind as we turn
    the knobs and dials of such large machines.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前简要讨论了块和块大小，现在我们将此扩展到集群规模。块大小和工作人员大小对于Dask的功能至关重要，因为它使用任务图执行模型中的块级视图来进行计算和数据。这是决定分布式计算如何工作的重要参数。在使用Dask和其他分布式系统时，我们发现这是在调整这些大型机器的旋钮和控制器时要记住的重要理念之一。
- en: For any given worker hardware configuration and computation you are doing, there
    will be a sweet spot for the chunk sizes, and the user’s job is to set this size.
    Finding the exact number might not be useful, but finding roughly what type of
    configuration is likely to give you the best result can make a huge difference.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 对于正在进行的任何给定的工作人员硬件配置和计算，都会有一个适合块大小的最佳点，用户的任务是设置这个大小。找到确切的数字可能没有用，但大致找到可能会给你带来最佳结果的配置类型可以产生巨大的差异。
- en: The key idea of chunking is to load balance computation and storage, at a cost
    of overhead of communications. On one end of the extreme, you have single-machine
    data engineering, with your data in one pandas DataFrame, or a single Dask DataFrame
    with no partitioning. There is not much communication cost, as all communication
    happens between your RAM and your GPU or CPU, with data moving through a single
    computer’s motherboard. As your data size grows, this monolithic block will not
    work, and you run into out-of-memory errors, losing all your previous in-memory
    computation. Hence, you would use a distributed system like Dask.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 块划分的关键思想是在计算和存储之间实现负载平衡，但会增加通信的开销。在一个极端，你有单机数据工程，数据在一个pandas DataFrame中，或者一个没有分区的单个Dask
    DataFrame中。通信成本不高，因为所有通信发生在RAM和GPU或CPU之间，数据通过单台计算机的主板移动。随着数据大小的增长，这种单体块将无法工作，你会遇到内存不足的错误，失去所有之前在内存中的计算。因此，你会使用像Dask这样的分布式系统。
- en: On the other extreme, a very fragmented dataset with small chunk sizes over
    multiple machines connected over Ethernet cables will be slower to work together
    as the communication overhead grows, and may even overrun the scheduler’s capacity
    to handle communications, gathering, and coordinating. Maintaining a happy balance
    between the two extremes, and knowing which problem requires which tools, is an
    essential job of modern distributed data engineering.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一个极端，一个非常分散的数据集，使用多台机器通过以太网连接，将在通信开销增加时更慢地共同工作，甚至可能超出调度程序处理通信、收集和协调的能力。在现代分布式数据工程中，保持两个极端之间的良好平衡，并了解哪个问题需要哪些工具，是一项重要工作。
- en: Avoid Rechunking
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 避免重新划分块
- en: When pipelining multiple data streams into a job, you might have two datasets,
    with two different chunk sizes, even if their data dimensions match. In runtime,
    Dask will have to rechunk one of the datasets to match the chunk sizes of the
    other. Doing so in runtime can get costly and memory inefficient. If this is spotted,
    you can consider having a separate job that does the rechunking before ingesting
    into your job.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在将多个数据流管道化到作业中时，你可能会有两个数据集，其数据维度匹配，但具有不同的块大小。在运行时，Dask 将不得不重新对一个数据集进行重新分块，以匹配另一个数据集的块大小。如果发现了这种情况，可以考虑在作业进入之前执行单独的重新分块作业。
- en: Scheduled Jobs
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计划任务
- en: There are many different systems to make your jobs run on a schedule. This schedule
    can range from being periodic and time-based to being triggered by upstream events
    (like data becoming available). Popular tools to schedule jobs include Apache
    Airflow, Flyte, Argo, GitHub Actions, and Kubeflow.^([10](ch12.xhtml#id969)) Airflow
    and Flyte have built-in support for Dask, which can simplify running your scheduled
    task, so we think that they are both excellent options for scheduled Dask jobs.
    The built-in operators make it easier to track failure, which is important, as
    taking actions on stale data can be as bad as taking actions on wrong data.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多不同的系统可以让你的作业按计划运行。这些计划可以是周期性的和基于时间的，也可以是由上游事件触发的（比如数据变为可用）。流行的调度作业工具包括 Apache
    Airflow、Flyte、Argo、GitHub Actions 和 Kubeflow。^([10](ch12.xhtml#id969)) Airflow
    和 Flyte 内置支持 Dask，可以简化运行计划任务，因此我们认为它们都是优秀的 Dask 计划任务选项。内置操作符使得跟踪失败更加容易，这很重要，因为对陈旧数据采取行动和对错误数据采取行动一样糟糕。
- en: We also often see people use Unix crontabs and schtasks, but we advise against
    that, as they run on a single machine and require substantial additional work.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也经常看到人们使用 Unix crontab 和 schtasks，但我们建议不要这样做，因为它们只在单台机器上运行，并且需要大量的额外工作。
- en: Tip
  id: totrans-199
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: For scheduled jobs on Kubernetes, you can also have your scheduler create a
    [DaskJob resource](https://oreil.ly/Uw5Rp), which will run your Dask program inside
    the cluster.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Kubernetes 上的计划任务，你还可以让调度器创建一个 [DaskJob 资源](https://oreil.ly/Uw5Rp)，这将在集群内运行你的
    Dask 程序。
- en: In [Appendix A](app01.xhtml#appA), you will learn details about testing and
    validation, which are especially important for scheduled and automated jobs, where
    there is no time for manual checking.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [附录 A](app01.xhtml#appA) 中，你将了解有关测试和验证的详细信息，这对于计划和自动化作业尤为重要，因为没有时间进行手动检查。
- en: Deployment Monitoring
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署监控
- en: Like many other distributed libraries, Dask provides logs, and you can configure
    Dask logs to be sent to a storage system. The method will vary by the deployment
    environment and if you are using Jupyter.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 像许多其他分布式库一样，Dask 提供了日志记录功能，你可以配置 Dask 日志记录发送到存储系统。具体方法会因部署环境以及是否使用 Jupyter 而有所不同。
- en: One generic way you can get the worker and scheduler logs is through `get_worker_logs()`
    and `get_scheduler_logs()` on the Dask client. You can specify specific topics
    and log to/read from just the relevant topics. Refer to [Example 9-6](ch09.xhtml#ex_basic_logging_ch09_1685536244456)
    for more information.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过 Dask 客户端的 `get_worker_logs()` 和 `get_scheduler_logs()` 方法通用地获取工作器和调度器的日志。你可以指定特定的主题来记录或读取相关主题的日志。更多信息请参考
    [示例 9-6](ch09.xhtml#ex_basic_logging_ch09_1685536244456)。
- en: You are not limited to logging strings, and you can instead log structured events.
    This can be especially useful for performance analysis or for anything where the
    log messages might be visualized rather than looked at individually by a human.
    In [Example 12-12](#structured-logging-on-workers_ch12_1685536295970), we do this
    with a distributed `softmax` function, and log the events and retrieve them on
    the client.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 你不仅限于记录字符串，还可以记录结构化事件。这在性能分析或者日志消息可能被可视化而不是人工逐个查看的情况下特别有用。在 [示例 12-12](#structured-logging-on-workers_ch12_1685536295970)
    中，我们通过分布式`softmax`函数实现了这一点，并记录了事件，并在客户端检索它们。
- en: Example 12-12\. Structured logging on workers
  id: totrans-206
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 12-12\. 工作器上的结构化日志记录
- en: '[PRE11]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Conclusion
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this chapter, you’ve learned the various deployment options for Dask distributed,
    from commodity cloud to HPC infrastructures. You’ve also learned Jupyter magics
    to simplify getting access to information with remote deployments. In our experience,
    Dask on Kubernetes and Dask on Ray on Kubernetes offer the flexibility we need.
    Your own decision about how to deploy Dask may be different, especially if you
    are working in a larger institution with existing cluster deployments. Most of
    the deployment options are covered in detail in the [“Deploy Dask Clusters” guide](https://oreil.ly/j8ueU),
    with the notable exception of Dask on Ray, which is covered in the [Ray documentation](https://oreil.ly/hN3Tk).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你已经学到了Dask分布式的各种部署选项，从大众化云到HPC基础设施。你还学到了简化远程部署获取信息的Jupyter魔术。根据我们的经验，Dask在Kubernetes上和Dask在Ray上的Kubernetes提供了我们需要的灵活性。你自己关于如何部署Dask的决定可能会有所不同，特别是如果你在一个拥有现有集群部署的较大机构中工作。大部分部署选项都在[“部署Dask集群”指南](https://oreil.ly/j8ueU)中有详细介绍，但Dask在Ray上的情况则在[Ray文档](https://oreil.ly/hN3Tk)中有介绍。
- en: You’ve also learned about runtime considerations and metrics to track when running
    a distributed work, and the various tools in Dask’s dashboard to accomplish that,
    expanded with more advanced user-defined metrics generation. Using these metrics,
    you’ve learned the conceptual basis for tuning your Dask Distributed clusters,
    troubleshooting, and how this relates to the fundamental design principle of Dask
    and distributed computing.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 你还学到了运行时考虑因素以及运行分布式工作时要跟踪的度量标准，以及Dask仪表板中的各种工具，用于生成更高级的用户定义的度量标准。通过使用这些度量标准，你学到了调整Dask分布式集群、故障排除的概念基础，以及这与Dask和分布式计算的基本设计原则的关系。
- en: ^([1](ch12.xhtml#id853-marker)) We don’t (currently) work for cloud providers,
    so if your workload fits on your laptop, more power to you. Just remember to use
    source control. If possible, though, putting it on a server can be a useful exercise
    for capturing the dependencies and ensuring your production environment can survive
    the loss of a laptop.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch12.xhtml#id853-marker)) 我们目前并不为云提供商工作，所以如果你的工作负载适合在笔记本电脑上运行，那就更好了。只是记得使用源代码控制。但是，如果可能的话，将其放在服务器上可能是一个有用的练习，以捕获依赖关系并确保您的生产环境可以承受丢失笔记本电脑的情况。
- en: ^([2](ch12.xhtml#id866-marker)) PEP20’s view on the obvious way to do things
    remains a suggestion more observed in the breach.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch12.xhtml#id866-marker)) PEP20对于显而易见的做事方式的看法仍然更多地是一种建议，而不是普遍遵守的规范。
- en: ^([3](ch12.xhtml#id873-marker)) Note this installs Helm 3.X. As with Python
    3, Helm 3 has a [large number of breaking changes over Helm 2](https://oreil.ly/vR5BM),
    so when you’re reading documentation (or installing packages), make sure it’s
    referencing the current major versions.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch12.xhtml#id873-marker)) 请注意，这会安装Helm 3.X。与Python 3一样，Helm 3与Helm 2相比有[大量的破坏性变化](https://oreil.ly/vR5BM)，所以当您阅读文档（或安装软件包）时，请确保它引用的是当前的主要版本。
- en: ^([4](ch12.xhtml#id876-marker)) Mixed worker types; see [“Worker Resources”](https://oreil.ly/O52ib)
    in the Dask documentation and the blog article [“How to Run Different Worker Types
    with the Dask Helm Chart”](https://oreil.ly/mJR6b).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch12.xhtml#id876-marker)) 混合的工作类型；参见[Dask文档中的“Worker Resources”](https://oreil.ly/O52ib)以及博客文章[“如何使用Dask
    Helm图运行不同的工作类型”](https://oreil.ly/mJR6b)。
- en: ^([5](ch12.xhtml#id890-marker)) In some ways, HPC and cluster managers are different
    names for the same thing, with cluster managers coming out of industry and HPC
    out of research. HPC clusters tend to have and use a shared network storage that
    is not as common in industry.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch12.xhtml#id890-marker)) 在某些方面，HPC和集群管理器是同一个东西的不同名称，其中集群管理器来自于工业，而HPC来自于研究。HPC集群倾向于拥有并使用不太常见于工业的共享网络存储。
- en: ^([6](ch12.xhtml#id906-marker)) You can also run an SSH socks proxy, which makes
    it easy to access other servers inside the HPC cluster, but which also requires
    changing your browser configuration (and does not work for the Dask client).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch12.xhtml#id906-marker)) 你还可以运行SSH socks代理，这样就可以轻松访问HPC集群内的其他服务器，但这也需要更改您的浏览器配置（并且对于Dask客户端不起作用）。
- en: ^([7](ch12.xhtml#id927-marker)) You can think of the memory as a balloon that
    we fill up, and as we get higher pressure it’s more likely to have issues. It’s
    a bit of a stretch as a metaphor, we admit.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch12.xhtml#id927-marker)) 你可以把内存想象成一个我们填充的气球，随着压力的增加，出现问题的可能性也越大。我们承认这个比喻有点牵强。
- en: ^([8](ch12.xhtml#id954-marker)) There is a knob you can use to control how fast
    precedent tasks complete. Sometimes running all the easy tasks too fast might
    cause you to end up with a lot of intermediate data that is piled up for the later
    step to go through, leading to undesirable high memory saturation. Look at the
    documentation related to `distributed​.sched⁠uler.worker-saturation` for more
    information.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch12.xhtml#id954-marker)) 有一个旋钮可以控制前置任务完成的速度。有时候运行所有简单的任务太快可能会导致大量中间数据堆积，后续步骤处理时可能会出现不良的内存饱和现象。查看与
    `distributed​.sched⁠uler.worker-saturation` 相关的文档，以获取更多信息。
- en: ^([9](ch12.xhtml#id959-marker)) You can find out more about it in Dask’s [documentation](https://oreil.ly/MhMHf).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch12.xhtml#id959-marker)) 您可以在 Dask 的[文档](https://oreil.ly/MhMHf)中找到更多信息。
- en: ^([10](ch12.xhtml#id969-marker)) Holden is a co-author of [*Kubeflow for Machine
    Learning*](https://learning.oreilly.com/library/view/kubeflow-for-machine/9781492050117/)
    (O’Reilly), so she is biased here.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch12.xhtml#id969-marker)) Holden 是 [*Kubeflow for Machine Learning*](https://learning.oreilly.com/library/view/kubeflow-for-machine/9781492050117/)（O’Reilly）的合著者，所以她在这里有偏见。
