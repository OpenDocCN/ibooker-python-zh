- en: Chapter 8\. Asynchronous I/O
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 8 章 异步 I/O
- en: So far we have focused on speeding up code by increasing the number of compute
    cycles that a program can complete in a given time. However, in the days of big
    data, getting the relevant data to your code can be the bottleneck, as opposed
    to the actual code itself. When this is the case, your program is called *I/O
    bound*; in other words, the speed is bounded by the efficiency of the input/output.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经集中精力通过增加程序在给定时间内完成的计算周期数来加速代码。然而，在大数据时代，将相关数据传递给您的代码可能成为瓶颈，而不是代码本身。当这种情况发生时，你的程序被称为*I/O
    绑定*；换句话说，速度受到输入/输出效率的限制。
- en: I/O can be quite burdensome to the flow of a program. Every time your code reads
    from a file or writes to a network socket, it must pause to contact the kernel,
    request that the actual read happens, and then wait for it to complete. This is
    because it is not your program but the kernel that does the actual read operation,
    since the kernel is responsible for managing any interaction with hardware. The
    additional layer may not seem like the end of the world, especially once you realize
    that a similar operation happens every time memory is allocated; however, if we
    look back at [Figure 1-3](ch01_split_000.xhtml#FIG-performant-connection-speed),
    we see that most of the I/O operations we perform are on devices that are orders
    of magnitude slower than the CPU. So even if the communication with the kernel
    is fast, we’ll be waiting quite some time for the kernel to get the result from
    the device and return it to us.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: I/O 可以对程序的流程造成相当大的负担。每当你的代码从文件中读取或向网络套接字写入时，它都必须暂停联系内核，请求实际执行读取操作，然后等待其完成。这是因为实际的读取操作不是由你的程序完成的，而是由内核完成的，因为内核负责管理与硬件的任何交互。这个额外的层次可能看起来并不像世界末日一样糟糕，尤其是当你意识到类似的操作每次分配内存时也会发生；然而，如果我们回顾一下[图 1-3](ch01_split_000.xhtml#FIG-performant-connection-speed)，我们会发现我们执行的大多数
    I/O 操作都是在比 CPU 慢几个数量级的设备上进行的。因此，即使与内核的通信很快，我们也将花费相当长的时间等待内核从设备获取结果并将其返回给我们。
- en: For example, in the time it takes to write to a network socket, an operation
    that typically takes about 1 millisecond, we could have completed 2,400,000 instructions
    on a 2.4 GHz computer. Worst of all, our program is halted for much of this 1
    millisecond of time—our execution is paused, and then we wait for a signal that
    the write operation has completed. This time spent in a paused state is called
    *I/O wait*.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在写入网络套接字所需的时间内，通常需要约 1 毫秒，我们可以在一台 2.4 GHz 计算机上完成 2,400,000 条指令。最糟糕的是，我们的程序在这
    1 毫秒时间内大部分时间都被暂停了——我们的执行被暂停，然后我们等待一个信号表明写入操作已完成。这段时间在暂停状态中度过称为*I/O 等待*。
- en: Asynchronous I/O helps us utilize this wasted time by allowing us to perform
    other operations while we are in the I/O wait state. For example, in [Figure 8-1](#conn_serial_vs_concurrent)
    we see a depiction of a program that must run three tasks, all of which have periods
    of I/O wait within them. If we run them serially, we suffer the I/O wait penalty
    three times. However, if we run these tasks concurrently, we can essentially hide
    the wait time by running another task in the meantime. It is important to note
    that this is all still happening on a single thread and still uses only one CPU
    at a time!
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 异步 I/O 帮助我们利用这段浪费的时间，允许我们在 I/O 等待状态时执行其他操作。例如，在[图 8-1](#conn_serial_vs_concurrent)中，我们看到一个程序的示例，必须运行三个任务，其中所有任务都有
    I/O 等待期。如果我们串行运行它们，我们将遭受三次 I/O 等待的惩罚。然而，如果我们并行运行这些任务，我们可以通过在此期间运行另一个任务来实际隐藏等待时间。重要的是要注意，所有这些仍然发生在单个线程上，并且仍然一次只使用一个
    CPU！
- en: This is possible because while a program is in I/O wait, the kernel is simply
    waiting for whatever device we’ve requested to read from (hard drive, network
    adapter, GPU, etc.) to send a signal that the requested data is ready. Instead
    of waiting, we can create a mechanism (the event loop) so that we can dispatch
    requests for data, continue performing compute operations, and be notified when
    the data is ready to be read. This is in stark contrast to the multiprocessing/multithreading
    ([Chapter 9](ch09_split_000.xhtml#multiprocessing)) paradigm, where a new process
    is launched that does experience I/O wait but uses the multi-tasking nature of
    modern CPUs to allow the main process to continue. However, the two mechanisms
    are often used in tandem, where we launch multiple processes, each of which is
    efficient at asynchronous I/O, in order to fully take advantage of our computer’s
    resources.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方式的实现是因为当程序处于 I/O 等待时，内核只是等待我们请求的设备（硬盘、网络适配器、GPU 等）发出信号，表示请求的数据已经准备好。我们可以创建一个机制（即事件循环）来分派数据请求，继续执行计算操作，并在数据准备好读取时得到通知，而不是等待。这与多进程/多线程（[第 9 章](ch09_split_000.xhtml#multiprocessing)）范式形成了鲜明对比，后者启动新进程以进行
    I/O 等待，但利用现代 CPU 的多任务性质允许主进程继续。然而，这两种机制通常同时使用，其中我们启动多个进程，每个进程在异步 I/O 方面都很有效，以充分利用计算机的资源。
- en: Note
  id: totrans-6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Since concurrent programs run on a single thread, they are generally easier
    to write and manage than standard multithreaded programs. All concurrent functions
    share the same memory space, so sharing data between them works in the normal
    ways you would expect. However, you still need to be careful about race conditions
    since you can’t be sure which lines of code get run when.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 由于并发程序在单线程上运行，通常比标准多线程程序更容易编写和管理。所有并发函数共享同一个内存空间，因此在它们之间共享数据的方式与您预期的正常方式相同。然而，您仍然需要注意竞态条件，因为不能确定代码的哪些行在何时运行。
- en: By modeling a program in this event-driven way, we are able to take advantage
    of I/O wait to perform more operations on a single thread than would otherwise
    be possible.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以事件驱动的方式对程序建模，我们能够利用 I/O 等待，在单线程上执行比以往更多的操作。
- en: '![hpp2 0801](Images/hpp2_0801.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![hpp2 0801](Images/hpp2_0801.png)'
- en: Figure 8-1\. Comparison of serial and concurrent programs
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-1\. 串行和并发程序的比较
- en: Introduction to Asynchronous Programming
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 异步编程简介
- en: Typically, when a program enters I/O wait, the execution is paused so that the
    kernel can perform the low-level operations associated with the I/O request (this
    is called a *context switch*), and it is not resumed until the I/O operation is
    completed. Context switching is quite a heavy operation. It requires us to save
    the state of our program (losing any sort of caching we had at the CPU level)
    and give up the use of the CPU. Later, when we are allowed to run again, we must
    spend time reinitializing our program on the motherboard and getting ready to
    resume (of course, all this happens behind the scenes).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当程序进入 I/O 等待时，执行会暂停，以便内核执行与 I/O 请求相关的低级操作（这称为*上下文切换*），直到 I/O 操作完成。上下文切换是一种相当重的操作。它要求我们保存程序的状态（丢失
    CPU 级别的任何缓存），并放弃使用 CPU。稍后，当我们被允许再次运行时，我们必须花时间在主板上重新初始化我们的程序，并准备恢复（当然，所有这些都是在幕后进行的）。
- en: With concurrency, on the other hand, we typically have an *event loop* running
    that manages what gets to run in our program, and when. In essence, an event loop
    is simply a list of functions that need to be run. The function at the top of
    the list gets run, then the next, etc. [Example 8-1](#conn_toy_eventloop) shows
    a simple example of an event loop.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 与并发相比，我们通常会有一个*事件循环*在运行，负责管理程序中需要运行的内容以及运行的时间。本质上，事件循环就是一系列需要运行的函数。列表顶部的函数被运行，然后是下一个，以此类推。[示例 8-1](#conn_toy_eventloop)展示了一个简单的事件循环示例。
- en: Example 8-1\. Toy event loop
  id: totrans-14
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 8-1\. 玩具事件循环
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This may not seem like a big change; however, we can couple event loops with
    asynchronous (async) I/O operations for massive gains when performing I/O tasks.
    In this example, the call `eventloop.put(do_world)` approximates an asynchronous
    call to the `do_world` function. This operation is called `nonblocking`, meaning
    it will return immediately but guarantee that `do_world` is called at some point
    later. Similarly, if this were a network write with an async function, it will
    return right away even though the write has not happened yet. When the write has
    completed, an event fires so our program knows about it.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能看起来不是一个很大的改变；然而，当我们将事件循环与异步（async）I/O操作结合使用时，可以在执行I/O任务时获得显著的性能提升。在这个例子中，调用
    `eventloop.put(do_world)` 大致相当于对 `do_world` 函数进行异步调用。这个操作被称为`非阻塞`，意味着它会立即返回，但保证稍后某个时刻调用
    `do_world`。类似地，如果这是一个带有异步功能的网络写入，它将立即返回，即使写入尚未完成。当写入完成时，一个事件触发，这样我们的程序就知道了。
- en: Putting these two concepts together, we can have a program that, when an I/O
    operation is requested, runs other functions while waiting for the original I/O
    operation to complete. This essentially allows us to still do meaningful calculations
    when we otherwise would have been in I/O wait.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 将这两个概念结合起来，我们可以编写一个程序，当请求I/O操作时，运行其他函数，同时等待原始I/O操作完成。这本质上允许我们在本来会处于I/O等待状态时仍然进行有意义的计算。
- en: Note
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Switching from function to function does have a cost. The kernel must take the
    time to set up the function to be called in memory, and the state of our caches
    won’t be as predictable. It is because of this that concurrency gives the best
    results when your program has a lot of I/O wait—the cost associated with switching
    can be much less than what is gained by making use of I/O wait time.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 切换从一个函数到另一个函数确实是有成本的。内核必须花费时间在内存中设置要调用的函数，并且我们的缓存状态不会那么可预测。正是因为这个原因，并发在程序有大量I/O等待时提供了最佳结果——与通过利用I/O等待时间获得的收益相比，切换的成本可能要少得多。
- en: 'Generally, programming using event loops can take two forms: callbacks or futures.
    In the callback paradigm, functions are called with an argument that is generally
    called the *callback*. Instead of the function returning its value, it calls the
    callback function with the value instead. This sets up long chains of functions
    that are called, with each function getting the result of the previous function
    in the chain (these chains are sometimes referred to as “callback hell”). [Example 8-2](#example8-2)
    is a simple example of the callback paradigm.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，使用事件循环进行编程可以采用两种形式：回调或期约。在回调范式中，函数被调用并带有一个通常称为*回调*的参数。函数不返回其值，而是调用回调函数并传递该值。这样设置了一长串被调用的函数链，每个函数都得到前一个函数链中的结果（这些链有时被称为“回调地狱”）。[示例 8-2](#example8-2)
    是回调范式的一个简单示例。
- en: Example 8-2\. Example with callbacks
  id: totrans-21
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 8-2\. 使用回调的示例
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[![1](Images/1.png)](#co_asynchronous_i_o_CO1-1)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_asynchronous_i_o_CO1-1)'
- en: '`save_result_to_db` is an asynchronous function; it will return immediately,
    and the function will end and allow other code to run. However, once the data
    is ready, `print_response` will be called.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '`save_result_to_db` 是一个异步函数；它将立即返回，并允许其他代码运行。然而，一旦数据准备好，将调用 `print_response`。'
- en: Before Python 3.4, the callback paradigm was quite popular. However, the `asyncio`
    standard library module and PEP 492 made the future’s mechanism native to Python.
    This was done by creating a standard API for dealing with asynchronous I/O and
    the new `await` and `async` keywords, which define an asynchronous function and
    a way to wait for a result.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python 3.4之前，回调范式非常流行。然而，`asyncio`标准库模块和PEP 492使期约机制成为Python的本地特性。通过创建处理异步I/O的标准API以及新的`await`和`async`关键字，定义了异步函数和等待结果的方式。
- en: 'In this paradigm, an asynchronous function returns a `Future` object, which
    is a promise of a future result. Because of this, if we want the result at some
    point we must wait for the future that is returned by this sort of asynchronous
    function to complete and be filled with the value we desire (either by doing an
    `await` on it or by running a function that explicitly waits for a value to be
    ready). However, this also means that the result can be available in the callers
    context, while in the callback paradigm the result is available only in the callback
    function. While waiting for the `Future` object to be filled with the data we
    requested, we can do other calculations. If we couple this with the concept of
    generators—functions that can be paused and whose execution can later be resumed—we
    can write asynchronous code that looks very close to serial code in form:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种范式中，异步函数返回一个 `Future` 对象，这是一个未来结果的承诺。因此，如果我们希望在某个时刻获取结果，我们必须等待由这种类型的异步函数返回的未来完成并填充我们期望的值（通过对其进行
    `await` 或运行显式等待值的函数）。然而，这也意味着结果可以在调用者的上下文中可用，而在回调范式中，结果仅在回调函数中可用。在等待 `Future`
    对象填充我们请求的数据时，我们可以进行其他计算。如果将这与生成器的概念相结合——可以暂停并稍后恢复执行的函数——我们可以编写看起来非常接近串行代码形式的异步代码：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[![1](Images/1.png)](#co_asynchronous_i_o_CO2-1)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_asynchronous_i_o_CO2-1)'
- en: In this case, `save_result_to_db` returns a `Future` type. By `await`ing it,
    we ensure that `save_value` gets paused until the value is ready and then resumes
    and completes its operations.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，`save_result_to_db` 返回一个 `Future` 类型。通过 `await` 它，我们确保 `save_value` 在值准备好之前暂停，然后恢复并完成其操作。
- en: It’s important to realize that the `Future` object returned by `save_result_to_db`
    holds the *promise* of a `Future` result and doesn’t hold the result itself or
    even call any of the `save_result_to_db` code. In fact, if we simply did `db_response_future
    = save_result_to_db(result)`, the statement would complete immediately and we
    could do other things with the `Future` object. For example, we could collect
    a list of futures and wait for all of them at the same time.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要意识到，由 `save_result_to_db` 返回的 `Future` 对象保持了未来结果的承诺，并不持有结果本身或调用任何 `save_result_to_db`
    代码。事实上，如果我们简单地执行 `db_response_future = save_result_to_db(result)`，该语句会立即完成，并且我们可以对
    `Future` 对象执行其他操作。例如，我们可以收集一个未来对象的列表，并同时等待它们的完成。
- en: How Does async/await Work?
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: async/await 是如何工作的？
- en: An `async` function (defined with `async def`) is called a *coroutine*. In Python,
    coroutines are implemented with the same philosophies as generators. This is convenient
    because generators already have the machinery to pause their execution and resume
    later. Using this paradigm, an `await` statement is similar in function to a `yield`
    statement; the execution of the current function gets paused while other code
    is run. Once the `await` or `yield` resolves with data, the function is resumed.
    So in the preceding example, our `save_result_to_db` will return a `Future` object,
    and the `await` statement pauses the function until that `Future` contains a result.
    The event loop is responsible for scheduling the resumption of `save_value` after
    the `Future` is ready to return a result.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 `async` 函数（使用 `async def` 定义）称为协程。在Python中，协程的实现与生成器具有相同的哲学。这很方便，因为生成器已经有了暂停执行和稍后恢复的机制。使用这种范式，`await`
    语句在功能上类似于 `yield` 语句；当前函数的执行在运行其他代码时暂停。一旦 `await` 或 `yield` 解析出数据，函数就会恢复执行。因此，在前面的例子中，我们的
    `save_result_to_db` 将返回一个 `Future` 对象，而 `await` 语句会暂停函数，直到 `Future` 包含一个结果。事件循环负责安排在
    `Future` 准备好返回结果后恢复 `save_value` 的执行。
- en: For Python 2.7 implementations of future-based concurrency, things can get a
    bit strange when we’re trying to use coroutines as actual functions. Remember
    that generators cannot return values, so libraries deal with this issue in various
    ways. The in Python 3.4, new machinery has been introduced in order to easily
    create coroutines and have them still return values. However, many asynchronous
    libraries that have been around since Python 2.7 have legacy code meant to deal
    with this awkward transition (in particular, `tornado`’s `gen` module).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于 Python 2.7 实现的基于未来的并发，当我们尝试将协程用作实际函数时，事情可能会变得有些奇怪。请记住，生成器无法返回值，因此库以各种方式处理此问题。在
    Python 3.4 中引入了新的机制，以便轻松创建协程并使它们仍然返回值。然而，许多自 Python 2.7 以来存在的异步库具有处理这种尴尬转换的遗留代码（特别是
    `tornado` 的 `gen` 模块）。
- en: It is critical to realize our reliance on an event loop when running concurrent
    code. In general, this leads to most fully concurrent code’s main code entry point
    consisting mainly of setting up and starting the event loop. However, this assumes
    that your entire program is concurrent. In other cases, a set of futures is created
    within the program, and then a temporary event loop is started simply to manage
    the existing futures, before the event loop exits and the code can resume normally.
    This is generally done with either the `loop.run_until_complete(coro)` or `loop.run_forever()`
    method from the `asyncio.loop` module. However, asyncio also provides a convenience
    function (`asyncio.run(coro)`) to simplify this process.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行并发代码时，意识到我们依赖于事件循环是至关重要的。一般来说，这导致大多数完全并发的代码的主要代码入口主要是设置和启动事件循环。然而，这假设整个程序都是并发的。在其他情况下，程序内部会创建一组futures，然后简单地启动一个临时事件循环来管理现有的futures，然后事件循环退出，代码可以正常恢复。这通常使用`asyncio.loop`模块中的`loop.run_until_complete(coro)`或`loop.run_forever()`方法来完成。然而，`asyncio`还提供了一个便利函数（`asyncio.run(coro)`）来简化这个过程。
- en: In this chapter we will analyze a web crawler that fetches data from an HTTP
    server that has latency built into it. This represents the general response-time
    latency that will occur whenever dealing with I/O. We will first create a serial
    crawler that looks at the naive Python solution to this problem. Then we will
    build up to a full `aiohttp` solution by iterating through `gevent` and then `tornado`.
    Finally, we will look at combining async I/O tasks with CPU tasks in order to
    effectively hide any time spent doing I/O.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将分析一个从具有内置延迟的HTTP服务器获取数据的网络爬虫。这代表了在处理I/O时通常会发生的响应时间延迟。我们首先会创建一个串行爬虫，查看这个问题的简单Python解决方案。然后，我们将通过迭代`gevent`和`torando`逐步构建出一个完整的`aiohttp`解决方案。最后，我们将探讨如何将异步I/O任务与CPU任务结合起来，以有效地隐藏任何花费在I/O上的时间。
- en: Note
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The web server we implemented can support multiple connections at a time. This
    will be true for most services that you will be performing I/O with—most databases
    can support multiple requests at a time, and most web servers support 10,000+
    simultaneous connections. However, when interacting with a service that cannot
    handle multiple connections at a time, we will always have the same performance
    as the serial case.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现的Web服务器可以同时支持多个连接。对于大多数需要进行I/O操作的服务来说，这通常是真实的情况——大多数数据库可以支持同时进行多个请求，大多数Web服务器支持10,000个以上的同时连接。然而，当与无法处理多个连接的服务进行交互时，我们的性能将始终与串行情况相同。
- en: Serial Crawler
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 串行爬虫
- en: For the control in our experiment with concurrency, we will write a serial web
    scraper that takes a list of URLs, fetches them, and sums the total length of
    the content from the pages. We will use a custom HTTP server that takes two parameters,
    `name` and `delay`. The `delay` field will tell the server how long, in milliseconds,
    to pause before responding. The `name` field is for logging purposes.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的并发实验控制中，我们将编写一个串行网络爬虫，接收一个URL列表，获取页面内容并计算总长度。我们将使用一个自定义的HTTP服务器，它接收两个参数，`name`和`delay`。`delay`字段告诉服务器在响应之前暂停的时间长度（以毫秒为单位）。`name`字段用于记录日志。
- en: By controlling the `delay` parameter, we can simulate the time it takes a server
    to respond to our query. In the real world, this could correspond to a slow web
    server, a strenuous database call, or any I/O call that takes a long time to perform.
    For the serial case, this leads to more time that our program is stuck in I/O
    wait, but in the concurrent examples later on, it will provide an opportunity
    to spend the I/O wait time doing other tasks.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 通过控制`delay`参数，我们可以模拟服务器响应查询的时间。在现实世界中，这可能对应于一个响应缓慢的Web服务器、繁重的数据库调用，或者任何执行时间长的I/O调用。对于串行情况，这会导致程序在I/O等待中耗费更多时间，但在后面的并发示例中，这将提供一个机会来利用I/O等待时间来执行其他任务。
- en: In [Example 8-3](#conn_serial_http), we chose to use the `requests` module to
    perform the HTTP call. We made this choice because of the simplicity of the module.
    We use HTTP in general for this section because it is a simple example of I/O
    and can be performed easily. In general, any call to a HTTP library can be replaced
    with any I/O.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例 8-3](#conn_serial_http)中，我们选择使用`requests`模块执行HTTP调用。我们之所以选择这个模块，是因为它的简单性。我们在本节中通常使用HTTP，因为它是I/O的一个简单示例，可以轻松执行。一般来说，可以用任何I/O替换对HTTP库的任何调用。
- en: Example 8-3\. Serial HTTP scraper
  id: totrans-42
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 8-3\. 串行HTTP抓取器
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: When running this code, an interesting metric to look at is the start and stop
    time of each request as seen by the HTTP server. This tells us how efficient our
    code was during I/O wait—since our task is to launch HTTP requests and then sum
    the number of characters that were returned, we should be able to launch more
    HTTP requests, and process any responses, while waiting for other requests to
    complete.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码时，一个有趣的度量标准是查看每个请求在 HTTP 服务器中的开始和结束时间。这告诉我们我们的代码在 I/O 等待期间的效率有多高——因为我们的任务是发起
    HTTP 请求并汇总返回的字符数，我们应该能够在等待其他请求完成时发起更多的 HTTP 请求并处理任何响应。
- en: We can see in [Figure 8-2](#conn_serial_request_time) that, as expected, there
    is no interleaving of our requests. We do one request at a time and wait for the
    previous request to happen before we move to the next request. In fact, the total
    runtime of the serial process makes perfect sense knowing this. Since each request
    takes 0.1 seconds (because of our `delay` parameter) and we are doing 500 requests,
    we expect this runtime to be about 50 seconds.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在 [图 8-2](#conn_serial_request_time) 中看到，正如预期的那样，我们的请求没有交错。我们一次只执行一个请求，并在移动到下一个请求之前等待前一个请求完成。实际上，串行进程的总运行时间在这种情况下是完全合理的。由于每个请求都需要
    0.1 秒（因为我们的 `delay` 参数），而我们要执行 500 个请求，我们预计运行时间大约为 50 秒。
- en: '![Request times for serial scraper](Images/hpp2_0802.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![串行爬虫请求时间](Images/hpp2_0802.png)'
- en: Figure 8-2\. Request time for [Example 8-3](#conn_serial_http)
  id: totrans-47
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-2\. [示例 8-3](#conn_serial_http) 请求时间
- en: Gevent
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Gevent
- en: One of the simplest asynchronous libraries is `gevent`. It follows the paradigm
    of having asynchronous functions return futures, which means most of the logic
    in your code can stay the same. In addition, `gevent` monkey patches the standard
    I/O functions to be asynchronous, so most of the time you can simply use the standard
    I/O packages and benefit from asynchronous behavior.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的异步库之一是 `gevent`。它遵循异步函数返回未来任务的范例，这意味着您的代码中的大部分逻辑可以保持不变。此外，`gevent` 修补了标准
    I/O 函数以支持异步，因此大多数情况下，您可以简单地使用标准 I/O 包并从异步行为中受益。
- en: Gevent provides two mechanisms to enable asynchronous programming—as mentioned
    before, it patches the standard library with asynchronous I/O functions, and it
    has a `Greenlets` object that can be used for concurrent execution. A *greenlet*
    is a type of coroutine and can be thought of as a thread (see [Chapter 9](ch09_split_000.xhtml#multiprocessing)
    for a discussion of threads); however, all greenlets run on the same physical
    thread. Instead of using multiple CPUs to run all the greenlets, we have an event
    loop on a single CPU that is able to switch between them during I/O wait. For
    the most part, `gevent` tries to make the handling of the event loop as transparent
    as possible through the use of `wait` functions. The `wait` function will start
    an event loop and run it as long as is needed for all greenlets to finish. Because
    of this, most of your `gevent` code will run serially; then at some point you
    will set up many greenlets to do a concurrent task and start the event loop with
    the `wait` function. While the `wait` function is executing, all of the concurrent
    tasks you have queued up will run until completion (or some stopping condition),
    and then your code will go back to being serial again.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Gevent 提供了两种机制来实现异步编程——如前所述，它通过异步 I/O 函数修补了标准库，并且它有一个 `Greenlets` 对象可用于并发执行。*Greenlet*
    是一种协程类型，可以看作是线程（请参阅 [第 9 章](ch09_split_000.xhtml#multiprocessing) 中有关线程的讨论）；然而，所有的
    Greenlet 都在同一个物理线程上运行。我们有一个事件循环在单个 CPU 上，在 I/O 等待期间能够在它们之间切换。大部分时间，`gevent` 试图通过使用
    `wait` 函数尽可能透明地处理事件循环。`wait` 函数将启动一个事件循环，并运行它直到所有的 Greenlet 完成为止。因此，大部分 `gevent`
    代码将会串行运行；然后在某个时刻，您会设置许多 Greenlet 来执行并发任务，并使用 `wait` 函数启动事件循环。在 `wait` 函数执行期间，您排队的所有并发任务都将运行直到完成（或某些停止条件），然后您的代码将恢复为串行。
- en: The futures are created with `gevent.spawn`, which takes a function and the
    arguments to that function and launches a greenlet that is responsible for running
    that function. The greenlet can be thought of as a future since, once the function
    we specified completes, its value will be contained within the greenlet’s `value`
    field.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 未来任务是通过 `gevent.spawn` 创建的，该函数接受一个函数及其参数，并启动一个负责运行该函数的绿色线程（greenlet）。绿色线程可以被视为一个未来任务，因为一旦我们指定的函数完成，其值将包含在绿色线程的
    `value` 字段中。
- en: This patching of Python standard modules can make it harder to control the subtleties
    of what is going on. For example, one thing we want to ensure when doing async
    I/O is that we don’t open too many files or connections at one time. If we do,
    we can overload the remote server or slow down our process by having to context-switch
    between too many operations.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这种对Python标准模块的修补可能会使控制正在进行的细微变化变得更加困难。例如，在进行异步I/O时，我们要确保不要同时打开太多文件或连接。如果这样做，我们可能会使远程服务器过载，或者通过不得不在太多操作之间进行上下文切换来减慢我们的进程。
- en: To limit the number of open files manually, we use a semaphore to only do HTTP
    requests from one hundred greenlets at a time. A semaphore works by making sure
    that only a certain number of coroutines can enter the context block at a time.
    As a result, we launch all the greenlets that we need to fetch the URLs right
    away; however, only one hundred of them can make HTTP calls at a time. Semaphores
    are one type of locking mechanism used a lot in various parallel code flows. By
    restricting the progression of your code based on various rules, locks can help
    you make sure that the various components of your program don’t interfere with
    one another.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了手动限制打开文件的数量，我们使用信号量一次只允许100个绿色线程进行HTTP请求。信号量通过确保只有一定数量的协程可以同时进入上下文块来工作。因此，我们立即启动所有需要获取URL的绿色线程；但是每次只有100个线程可以进行HTTP调用。信号量是各种并行代码流中经常使用的一种锁定机制类型。通过基于各种规则限制代码的执行顺序，锁定可以帮助您确保程序的各个组件不会互相干扰。
- en: Now that we have all the futures set up and have put in a locking mechanism
    to control the flow of the greenlets, we can wait until we start having results
    by using the `gevent.iwait` function, which will take a sequence of futures and
    iterate over the ready items. Conversely, we could have used `gevent.wait`, which
    would block execution of our program until all requests are done.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经设置好了所有的未来并且放入了一个锁机制来控制绿色线程的流程，我们可以使用`gevent.iwait`函数等待，该函数将获取一个准备好的项目序列并迭代它们。相反，我们也可以使用`gevent.wait`，它将阻塞程序的执行，直到所有请求都完成。
- en: We go through the trouble of grouping our requests with the semaphore instead
    of sending them all at once because overloading the event loop can cause performance
    decreases (and this is true for all asynchronous programming). In addition, the
    server we are communicating with will have a limit to the number of concurrent
    requests it can respond to at the same time.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们费力地使用信号量来分组我们的请求，而不是一次性发送它们，因为过载事件循环可能会导致性能下降（对于所有异步编程都是如此）。此外，我们与之通信的服务器将限制同时响应的并发请求数量。
- en: From experimentation (shown in [Figure 8-3](#conn_num_concurrent_requests)),
    we generally see that one hundred or so open connections at a time is optimal
    for requests with a reply time of about 50 milliseconds. If we were to use fewer
    connections, we would still have wasted time during I/O wait. With more, we are
    switching contexts too often in the event loop and adding unnecessary overhead
    to our program. We can see this effect come into play with four hundred concurrent
    requests for 50-millisecond requests. That being said, this value of one hundred
    depends on many things—the computer the code is being run on, the implementation
    of the event loop, the properties of the remote host, the expected time to respond
    of the remote server, and so on. We recommend doing some experimentation before
    settling on a choice.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 通过实验（见图 8-3](#conn_num_concurrent_requests))，我们通常看到一次大约100个开放连接对于约50毫秒响应时间的请求是最佳的。如果我们使用更少的连接，我们仍然会在I/O等待期间浪费时间。而使用更多连接时，我们在事件循环中频繁切换上下文，并给程序增加了不必要的开销。我们可以看到，对于50毫秒请求，400个并发请求的情况下，这种效果就显现出来了。话虽如此，这个100的值取决于许多因素——计算机运行代码的机器、事件循环的实现、远程主机的属性、远程服务器的预期响应时间等。我们建议在做出选择之前进行一些实验。
- en: '![Experimenting with different numbers of concurrent requests for various request
    times](Images/hpp2_0803.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![对不同请求时间进行实验](Images/hpp2_0803.png)'
- en: Figure 8-3\. Experimenting with different numbers of concurrent requests for
    various request times
  id: totrans-58
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-3\. 对不同数量的并发请求进行实验，针对不同的请求时间。
- en: In [Example 8-4](#conn_grequest_http), we implement the `gevent` scraper by
    using a semaphore to ensure only 100 requests at a time.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例 8-4](#conn_grequest_http)，我们通过使用信号量来实现`gevent`爬虫，以确保一次只有100个请求。
- en: Example 8-4\. `gevent` HTTP scraper
  id: totrans-60
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 8-4\. `gevent` HTTP爬虫
- en: '[PRE4]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[![1](Images/1.png)](#co_asynchronous_i_o_CO3-2)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_asynchronous_i_o_CO3-2)'
- en: Here we generate a semaphore that lets `chunk_size` downloads happen.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们生成一个信号量，允许`chunk_size`个下载同时进行。
- en: '[![2](Images/2.png)](#co_asynchronous_i_o_CO3-1)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_asynchronous_i_o_CO3-1)'
- en: By using the semaphore as a context manager, we ensure that only `chunk_size`
    greenlets can run the body of the context at one time.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用信号量作为上下文管理器，我们确保一次只能运行`chunk_size`个绿色线程。
- en: '[![3](Images/3.png)](#co_asynchronous_i_o_CO3-3)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_asynchronous_i_o_CO3-3)'
- en: We can queue up as many greenlets as we need, knowing that none of them will
    run until we start an event loop with `wait` or `iwait`.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以排队尽可能多的绿色线程，知道没有一个会在我们用`wait`或`iwait`启动事件循环之前运行。
- en: '[![4](Images/4.png)](#co_asynchronous_i_o_CO3-4)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_asynchronous_i_o_CO3-4)'
- en: '`response_futures` now holds a generator over completed futures, all of which
    have our desired data in the `.value` property.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`response_futures`现在持有已完成的期货生成器，所有这些期货都在`.value`属性中包含了我们需要的数据。'
- en: An important thing to note is that we have used `gevent` to make our I/O requests
    asynchronous, but we are not doing any non-I/O computations while in I/O wait.
    However, in [Figure 8-4](#conn_gevent_request_time) we can see the massive speedup
    we get (see [Table 8-1](#conn_runtime_comparison)). By launching more requests
    while waiting for previous requests to finish, we are able to achieve a 90× speedup!
    We can explicitly see that requests are being sent out before previous requests
    finish through the stacked horizontal lines representing the requests. This is
    in sharp contrast to the case of the serial crawler ([Figure 8-2](#conn_serial_request_time)),
    where a line starts only when the previous line finishes.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的事情需要注意的是，我们使用了`gevent`来使我们的I/O请求异步化，但在I/O等待期间我们不进行任何非I/O计算。然而，在[图8-4](#conn_gevent_request_time)中，我们可以看到我们得到的大幅加速（见[表8-1](#conn_runtime_comparison)）。通过在等待前一个请求完成时发起更多请求，我们能够实现90倍的加速！我们可以明确地看到在代表请求的堆叠水平线上一次性发出请求，之前的请求完成之前。这与串行爬虫的情况形成鲜明对比（参见[图8-2](#conn_serial_request_time)），在那里一条线仅在前一条线结束时开始。
- en: 'Furthermore, we can see more interesting effects reflected in the shape of
    the `gevent` request timeline in [Figure 8-4](#conn_gevent_request_time). For
    example, at around the 100th request, we see a pause where new requests are not
    launched. This is because it is the first time that our semaphore is hit, and
    we are able to lock the semaphore before any previous requests finish. After this,
    the semaphore goes into an equilibrium: it locks just as another request finishes
    and unlocks it.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以在[图8-4](#conn_gevent_request_time)中看到更多有趣的效果，反映在`gevent`请求时间线的形状上。例如，在大约第100次请求时，我们看到了一个暂停，此时没有启动新的请求。这是因为这是第一次我们的信号量被触发，并且我们能够在任何之前的请求完成之前锁定信号量。此后，信号量进入平衡状态：在另一个请求完成时刚好锁定和解锁它。
- en: '![Request times for gevent scraper. The red line highlights the 100th request
    where we can see a pause before subsequent requests are issued.](Images/hpp2_0804.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![gevent爬虫请求时间。红线标示第100次请求，我们可以看到后续请求之前的暂停。](Images/hpp2_0804.png)'
- en: Figure 8-4\. Request times for `gevent` scraper—the red line highlights the
    100th request, where we can see a pause before subsequent requests are issued
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 8-4\. `gevent`爬虫的请求时间——红线标示第100次请求，我们可以看到后续请求之前的暂停。
- en: tornado
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 龙卷风
- en: Another frequently used package for asynchronous I/O in Python is `tornado`,
    originally developed by Facebook primarily for HTTP clients and servers. This
    framework has been around since Python 3.5, when `async/await` was introduced,
    and originally used a system of callbacks to organize asynchronous calls. Recently,
    however, the maintainers of the project have chosen to embrace the use of coroutines
    and in general have been critical in the architecture of the `asyncio` module.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个经常在Python中用于异步I/O的包是`tornado`，最初由Facebook开发，主要用于HTTP客户端和服务器。该框架自Python 3.5引入`async/await`以来就存在，并最初使用回调系统组织异步调用。然而，最近，项目的维护者选择采用协程，并在`asyncio`模块的架构中起到了重要作用。
- en: Currently, `tornado` can be used either by using `async`/`await` syntax, as
    is standard in Python, or by using Python’s `tornado.gen` module. This module
    was provided as a precursor to the native coroutines in Python. It did so by providing
    a decorator to turn a method into a coroutine (i.e., a way to get the same result
    as defining a function with `async def`) and various utilities to manage the runtime
    of coroutines. Currently this decorator approach is necessary only if you intend
    on providing support to Python versions older than 3.5.^([1](ch08.xhtml#idm46122412293032))
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，`tornado` 可以通过使用 `async`/`await` 语法（这是 Python 中的标准）或使用 Python 的 `tornado.gen`
    模块来使用。这个模块作为 Python 中原生协程的前身提供。它通过提供一个装饰器将方法转换为协程（即，获得与使用 `async def` 定义函数相同结果的方法）以及各种实用工具来管理协程的运行时来实现。当前，只有在您打算支持早于
    3.5 版本的 Python 时，才需要使用这种装饰器方法。^([1](ch08.xhtml#idm46122412293032))
- en: Tip
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: When using `tornado`, make sure to have `pycurl` installed. It is an optional
    backend for tornado but performs better, especially with DNS requests, than the
    default backend.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `tornado` 时，请确保已安装 `pycurl`。它是 tornado 的可选后端，但性能更好，特别是在 DNS 请求方面，优于默认后端。
- en: In [Example 8-5](#conn_tornado_http), we implement the same web crawler as we
    did for `gevent`, but we use the `tornado` I/O loop (its version of an event loop)
    and HTTP client. This saves us the trouble of having to batch our requests and
    deal with other, more low-level aspects of our code.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [示例 8-5](#conn_tornado_http) 中，我们实现了与 `gevent` 相同的网络爬虫，但是我们使用了 `tornado` 的
    I/O 循环（它的版本是事件循环）和 HTTP 客户端。这使我们不必批量处理请求和处理代码的其他更底层的方面。
- en: Example 8-5\. `tornado` HTTP scraper
  id: totrans-80
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 8-5。`tornado` HTTP 爬虫
- en: '[PRE5]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[![1](Images/1.png)](#co_asynchronous_i_o_CO4-1)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_asynchronous_i_o_CO4-1)'
- en: We can configure our HTTP client and pick what backend library we wish to use
    and how many requests we would like to batch together. Tornado defaults to a max
    of 10 concurrent requests.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以配置我们的 HTTP 客户端，并选择我们希望使用的后端库以及我们希望将多少个请求一起批处理。Tornado 默认最多同时进行 10 个并发请求。
- en: '[![2](Images/2.png)](#co_asynchronous_i_o_CO4-2)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_asynchronous_i_o_CO4-2)'
- en: We generate many `Future` objects to queue the task of fetching the URL contents.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生成许多 `Future` 对象来排队获取 URL 内容的任务。
- en: '[![3](Images/3.png)](#co_asynchronous_i_o_CO4-3)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_asynchronous_i_o_CO4-3)'
- en: This will run all of the coroutines that are queued in the `tasks` list and
    yield them as they complete.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这将运行在 `tasks` 列表中排队的所有协程，并在它们完成时将它们作为结果返回。
- en: '[![4](Images/4.png)](#co_asynchronous_i_o_CO4-4)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_asynchronous_i_o_CO4-4)'
- en: Since the coroutine is already completed, the `await` statement here returns
    immediately with the result of the earliest completed task.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 由于协程已经完成，因此此处的 `await` 语句立即返回最早完成的任务的结果。
- en: '[![5](Images/5.png)](#co_asynchronous_i_o_CO4-5)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_asynchronous_i_o_CO4-5)'
- en: '`ioloop.run_sync` will start the `IOLoop` just for the duration of the runtime
    of the specified function. `ioloop.start()`, on the other hand, starts an `IOLoop`
    that must be terminated manually.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`ioloop.run_sync` 将启动 `IOLoop`，并在指定函数的运行时段内持续运行。另一方面，`ioloop.start()` 启动一个必须手动终止的
    `IOLoop`。'
- en: An important difference between the `tornado` code in [Example 8-5](#conn_tornado_http)
    and the `gevent` code in [Example 8-4](#conn_grequest_http) is when the event
    loop runs. For `gevent`, the event loop is running only while the `iwait` function
    is running. On the other hand, in `tornado` the event loop is running the entire
    time and controls the complete execution flow of the program, not just the asynchronous
    I/O parts.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [示例 8-5](#conn_tornado_http) 中的 `tornado` 代码与 [示例 8-4](#conn_grequest_http)
    中的 `gevent` 代码之间的一个重要区别是事件循环的运行方式。对于 `gevent`，事件循环仅在 `iwait` 函数运行时才会运行。另一方面，在
    `tornado` 中，事件循环始终运行，并控制程序的完整执行流程，而不仅仅是异步 I/O 部分。
- en: This makes `tornado` ideal for any application that is mostly I/O-bound and
    where most, if not all, of the application should be asynchronous. This is where
    `tornado` makes its biggest claim to fame, as a performant web server. In fact,
    Micha has on many occasions written `tornado`-backed databases and data structures
    that require a lot of I/O.^([2](ch08.xhtml#idm46122411776344))
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得 `tornado` 成为大多数 I/O 密集型应用和大多数（如果不是全部）应用都应该是异步的理想选择。这是 `tornado` 最为人所知的地方，作为一款高性能的
    Web 服务器。事实上，Micha 在许多情况下都是用 `tornado` 支持的数据库和数据结构来进行大量 I/O。^([2](ch08.xhtml#idm46122411776344))
- en: On the other hand, since `gevent` makes no requirements of your program as a
    whole, it is an ideal solution for mainly CPU-based problems that sometimes involve
    heavy I/O—for example, a program that does a lot of computations over a dataset
    and then must send the results back to the database for storage. This becomes
    even simpler with the fact that most databases have simple HTTP APIs, which means
    you can even use `grequests`.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，由于 `gevent` 对整个程序没有任何要求，因此它是主要用于基于 CPU 的问题的理想解决方案，有时涉及大量 I/O，例如对数据集进行大量计算，然后必须将结果发送回数据库进行存储。由于大多数数据库都具有简单的
    HTTP API，因此甚至可以使用 `grequests` 进行简化。
- en: Another interesting difference between `gevent` and `tornado` is the way the
    internals change the request call graphs. Compare [Figure 8-5](#conn_tornado_request_time)
    with [Figure 8-4](#conn_gevent_request_time). For the `gevent` call graph, we
    see a very uniform call graph in which new requests are issued the second a slot
    in the semaphore opens up. On the other hand, the call graph for tornado is very
    stop-and-go. This means that the internal mechanism limiting the number of open
    connects is not reacting fast enough to request finishing. These areas of the
    call graph in which the line seems to be thinner/thicker than usual represent
    times when the event loop isn’t doing its job optimally—times when we are either
    underutilizing or overutilizing our resources.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的区别在于 `gevent` 和 `tornado` 在内部更改请求调用图的方式。将 [图 8-5](#conn_tornado_request_time)
    与 [图 8-4](#conn_gevent_request_time) 进行比较。对于 `gevent` 的调用图，我们看到一个非常均匀的调用图，即当信号量中的插槽打开时，新请求会立即发出。另一方面，tornado
    的调用图则非常起伏不定。这意味着限制打开连接数的内部机制未能及时响应请求完成。调用图中那些看起来比平时更细或更粗的区域表示事件循环未能有效地执行其工作的时段——即我们要么未充分利用资源，要么过度利用资源。
- en: Note
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: For all libraries that use `asyncio` to run the event loop, we can actually
    change the backend library that is being used. For example, the [`uvloop`](https://oreil.ly/Qvgq6)
    project supplies a drop-in replacement to `asyncio`’s event loop that claims massive
    speedups. These speedups are mainly seen server-side; in the client-side examples
    outlined in this chapter, they provide only a small performance boost. However,
    since it takes only two extra lines of code to use this event loop, there aren’t
    many reasons not to use it!
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有使用 `asyncio` 运行事件循环的库，我们实际上可以更改正在使用的后端库。例如，[`uvloop`](https://oreil.ly/Qvgq6)
    项目提供了一个替换 `asyncio` 事件循环的即插即用解决方案，声称大幅提升速度。这些速度提升主要在服务器端可见；在本章概述的客户端示例中，它们只提供了小幅性能提升。然而，由于只需额外两行代码即可使用此事件循环，几乎没有理由不使用它！
- en: 'We can start to understand this slowdown in light of the lesson we’ve been
    learning over and over again: that generalized code is useful because it solves
    all problems well but no individual problem perfectly. The mechanism to limit
    one hundred ongoing connections is fantastic when working with a large web app
    or a code base that may make HTTP requests in many different places. One simple
    configuration guarantees that overall we won’t have more than the defined connections
    opened. However, in our situation we can benefit from being very specific as to
    how this is handled (as we did in the `gevent` example).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以开始理解这种减速的原因，考虑到我们一遍又一遍地学到的教训：通用代码之所以有用，是因为它能很好地解决所有问题，但没有完美解决任何一个单独的问题。当处理大型
    Web 应用程序或代码库中可能在许多不同位置进行 HTTP 请求时，限制一百个正在进行的连接的机制非常有用。一种简单的配置保证总体上我们不会打开超过定义的连接数。然而，在我们的情况下，我们可以从处理方式非常具体的好处中受益（就像我们在
    `gevent` 示例中所做的那样）。
- en: '![Request times for tornado scraper](Images/hpp2_0805.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![tornado 爬虫请求时间](Images/hpp2_0805.png)'
- en: Figure 8-5\. Chronology of HTTP requests for [Example 8-5](#conn_tornado_http)
  id: totrans-100
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-5\. [示例 8-5](#conn_tornado_http) 的 HTTP 请求时间的时间轴
- en: aiohttp
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: aiohttp
- en: In response to the popularity of using async functionality to deal with heavy
    I/O systems, Python 3.4+ introduced a revamping of the old `asyncio` standard
    library module. At the time, however, this module was quite low level, providing
    all of the low-level mechanisms for third-party libraries to create easy-to-use
    asynchronous libraries. `aiohttp` arose as the first popular library built entirely
    on the new `asyncio` library. It provides both HTTP client and server functionality
    and uses a similar API to those familiar with `tornado`. The entire project, [`aio-libs`](https://oreil.ly/c0dgk),
    provides native asynchronous libraries for a wide variety of uses. In [Example 8-6](#conn_asyncio_http),
    we show how to implement the `asyncio` scraper using `aiohttp`.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 针对使用异步功能处理重型IO系统的普及，Python 3.4+引入了对旧的`asyncio`标准库模块的改进。然而，这个模块当时相当低级，提供了所有用于第三方库创建易于使用的异步库的底层机制。`aiohttp`作为第一个完全基于新`asyncio`库构建的流行库应运而生。它提供HTTP客户端和服务器功能，并使用与熟悉`tornado`的人类似的API。整个项目[`aio-libs`](https://oreil.ly/c0dgk)提供了广泛用途的原生异步库。在[示例 8-6](#conn_asyncio_http)中，我们展示了如何使用`aiohttp`实现`asyncio`爬虫。
- en: Example 8-6\. `asyncio` HTTP scraper
  id: totrans-103
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 8-6\. `asyncio` HTTP爬虫
- en: '[PRE6]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[![1](Images/1.png)](#co_asynchronous_i_o_CO5-1)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_asynchronous_i_o_CO5-1)'
- en: As in the `gevent` example, we must use a semaphore to limit the number of requests.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 与`gevent`示例中一样，我们必须使用信号量来限制请求的数量。
- en: '[![2](Images/2.png)](#co_asynchronous_i_o_CO5-2)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_asynchronous_i_o_CO5-2)'
- en: We return a new coroutine that will asynchronously download files and respect
    the locking of the semaphore.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们返回一个新的协程，将异步下载文件并尊重信号量的锁定。
- en: '[![3](Images/3.png)](#co_asynchronous_i_o_CO5-3)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_asynchronous_i_o_CO5-3)'
- en: The `http_client` function returns futures. To keep track of progress, we save
    the futures into a list.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`http_client`返回futures。为了跟踪进度，我们将futures保存到列表中。
- en: '[![4](Images/4.png)](#co_asynchronous_i_o_CO5-4)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_asynchronous_i_o_CO5-4)'
- en: As with `gevent`, we can wait for futures to become ready and iterate over them.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 与`gevent`一样，我们可以等待futures变为就绪并对其进行迭代。
- en: One immediate reaction to this code is the number of `async with`, `async def`,
    and `await` calls. In the definition for `http_get`, we use the async context
    manager to get access to shared resources in a concurrent-friendly way. That is
    to say, by using `async with`, we allow other coroutines to run while waiting
    to acquire the resources we are requesting. As a result, sharing things such as
    open semaphore slots or already opened connections to our host can be done more
    efficiently than we experienced with `tornado`.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 对这段代码的一个直接反应是`async with`、`async def`和`await`调用的数量。在`http_get`的定义中，我们使用异步上下文管理器以并发友好的方式访问共享资源。也就是说，通过使用`async
    with`，我们允许其他协程在等待获取我们请求的资源时运行。因此，可以更有效地共享诸如开放的信号量插槽或已经打开的连接到我们主机的东西，比我们在使用`tornado`时经历的更有效。
- en: In fact, the call graph in [Figure 8-6](#conn_asyncio_request_time) shows a
    smooth transition similar to that of `gevent` in [Figure 8-4](#conn_gevent_request_time).
    Furthermore, the `asyncio` code runs slightly faster than the `gevent` code overall
    (1.10 seconds versus 1.14 seconds—see [Table 8-1](#conn_runtime_comparison)),
    even though the time for each request is slightly longer. This can be explained
    only by a faster resumption of coroutines paused by the semaphore or waiting for
    the HTTP client.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，[图 8-6](#conn_asyncio_request_time)中的调用图显示了与[图 8-4](#conn_gevent_request_time)中的`gevent`类似的平滑过渡。此外，总体上，`asyncio`的代码运行速度略快于`gevent`（1.10秒对比1.14秒—参见[表 8-1](#conn_runtime_comparison)），尽管每个请求的时间稍长。这只能通过信号量暂停的协程或等待HTTP客户端的更快恢复来解释。
- en: '![Request times for AsyncIO scraper](Images/hpp2_0806.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![异步IO爬虫的请求时间](Images/hpp2_0806.png)'
- en: Figure 8-6\. Chronology of HTTP requests for [Example 8-6](#conn_asyncio_http)
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-6\. [示例 8-6](#conn_asyncio_http)的HTTP请求的年表
- en: The code sample also shows a big difference between using `aiohttp` and using
    `tornado` in that with `aiohttp`, we are very much in control of the event loop
    and the various subtleties of the request we are making. For example, we manually
    acquire the client session, which is responsible for caching open connections,
    and we manually read from the connection. If we wanted, we could change what time
    of connection caching is happening or decide to only write to the server and never
    read its response.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码示例还展示了使用`aiohttp`和使用`tornado`之间的巨大区别，因为使用`aiohttp`时，我们对事件循环以及我们正在进行的请求的各种微妙之处有很大的控制。例如，我们手动获取客户端会话，负责缓存打开的连接，以及手动从连接中读取。如果我们愿意，我们可以改变连接缓存的时间或者决定仅向服务器写入而不读取其响应。
- en: While this control may be a bit of overkill for such a simple example, in real-world
    applications we can use this to really tune the performance of our applications.
    Tasks can easily be added to the event loop without waiting for their response,
    and we can easily add time-outs to tasks so that their runtime is limited; we
    can even add functions that automatically get triggered when a task is completed.
    This allows us to create complicated runtime patterns that optimally utilize the
    time we gain by being able to run code during I/O wait. In particular, when we
    are running a web service (such as an API that may need to perform computational
    tasks for each request), this control can allow us to write “defensive” code that
    knows how to concede runtime to other tasks if a new request comes in. We will
    discuss this aspect more in [“Full Async”](#full-async).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管对于这样一个简单的示例来说，这种控制可能有点过度，但在实际应用中，我们可以使用它来真正调整我们应用程序的性能。任务可以轻松地添加到事件循环中而无需等待其响应，并且我们可以轻松地为任务添加超时，使其运行时间受限；我们甚至可以添加在任务完成时自动触发的函数。这使我们能够创建复杂的运行时模式，以最优化地利用通过能够在
    I/O 等待期间运行代码而获得的时间。特别是，当我们运行一个 Web 服务时（例如，一个可能需要为每个请求执行计算任务的 API），这种控制可以使我们编写“防御性”代码，知道如何在新请求到达时将运行时间让步给其他任务。我们将在
    [“完全异步”](#full-async) 中更多地讨论这个方面。
- en: Table 8-1\. Comparison of total runtime for crawlers
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8-1\. 爬虫的总运行时间比较
- en: '|  | serial | gevent | tornado | aiohttp |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  | 序号 | gevent | tornado | aiohttp |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Runtime (s) | 102.684 | 1.142 | 1.171 | 1.101 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 运行时间（秒） | 102.684 | 1.142 | 1.171 | 1.101 |'
- en: Shared CPU–I/O Workload
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 共享 CPU-I/O 负载
- en: 'To make the preceding examples more concrete, we will create another toy problem
    in which we have a CPU-bound problem that needs to communicate frequently with
    a database to save results. The CPU workload can be anything; in this case, we
    are taking the `bcrypt` hash of a random string with larger and larger workload
    factors to increase the amount of CPU-bound work (see [Table 8-2](#time-per-difficulty)
    to understand how the “difficulty” parameter affects runtime). This problem is
    representative of any sort of problem in which your program has heavy calculations
    to do, and the results of those calculations must be stored into a database, potentially
    incurring a heavy I/O penalty. The only restrictions we are putting on our database
    are as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使前述示例更具体化，我们将创建另一个玩具问题，在这个问题中，我们有一个需要频繁与数据库通信以保存结果的 CPU 绑定问题。CPU 负载可以是任何东西；在这种情况下，我们正在使用较大和较大的工作负载因子对随机字符串进行`bcrypt`哈希以增加
    CPU 绑定工作的量（请参见[表 8-2](#time-per-difficulty) 以了解“难度”参数如何影响运行时间）。这个问题代表了任何需要程序进行大量计算，并且这些计算的结果必须存储到数据库中的问题，可能会导致严重的
    I/O 惩罚。我们对数据库的唯一限制如下：
- en: It has an HTTP API so we can use code like that in the earlier examples.^([3](ch08.xhtml#idm46122411089528))
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它有一个 HTTP API，因此我们可以使用早期示例中的代码。^([3](ch08.xhtml#idm46122411089528))
- en: Response times are on the order of 100 milliseconds.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 响应时间在 100 毫秒的数量级上。
- en: The database can satisfy many requests at a time.^([4](ch08.xhtml#idm46122411087272))
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据库可以同时满足许多请求。^([4](ch08.xhtml#idm46122411087272))
- en: The response time of this “database” was chosen to be higher than usual in order
    to exaggerate the turning point in the problem, where the time to do one of the
    CPU tasks is longer than one of the I/O tasks. For a database that is being used
    only to store simple values, a response time greater than 10 milliseconds should
    be considered slow!
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这个“数据库”的响应时间被选择得比通常要高，以夸大问题的转折点，即执行 CPU 任务中的一个所需的时间长于执行 I/O 任务中的一个。对于只用于存储简单值的数据库，响应时间大于
    10 毫秒应被认为是慢的！
- en: Table 8-2\. Time to calculate a single hash
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8-2\. 计算单个哈希的时间
- en: '| Difficulty parameter | 8 | 10 | 11 | 12 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 难度参数 | 8 | 10 | 11 | 12 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Seconds per iteration | 0.0156 | 0.0623 | 0.1244 | 0.2487 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 每次迭代秒数 | 0.0156 | 0.0623 | 0.1244 | 0.2487 |'
- en: Serial
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 串行
- en: 'We start with some simple code that calculates the `bcrypt` hash of a string
    and makes a request to the database’s HTTP API every time a result is calculated:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一些简单的代码开始，计算字符串的 `bcrypt` 哈希，并在计算结果时每次向数据库的 HTTP API 发送请求：
- en: '[PRE7]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[![1](Images/1.png)](#co_asynchronous_i_o_CO6-1)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_asynchronous_i_o_CO6-1)'
- en: We generate a random 10-character byte array.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生成一个随机的10字符字节数组。
- en: '[![2](Images/2.png)](#co_asynchronous_i_o_CO6-2)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_asynchronous_i_o_CO6-2)'
- en: The `difficulty` parameter sets how hard it is to generate the password by increasing
    the CPU and memory requirements of the hashing algorithm.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`difficulty` 参数设置了生成密码的难度，通过增加哈希算法的 CPU 和内存需求来实现。'
- en: Just as in our serial example ([Example 8-3](#conn_serial_http)), the request
    times for each database save (100 milliseconds) do not stack, and we must pay
    this penalty for each result. As a result, iterating six hundred times with a
    task difficulty of 8 takes 71 seconds. We know, however, that because of the way
    our serial requests work, we are spending 40 seconds at minimum doing I/O! 56%
    of our program’s runtime is being spent doing I/O and, moreover, just sitting
    around in “I/O wait,” when it could be doing something else!
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们的串行示例中一样（[示例 8-3](#conn_serial_http)），每个数据库保存的请求时间（100 毫秒）不会叠加，我们必须为每个结果支付这个惩罚。因此，以
    8 的任务难度进行六百次迭代需要 71 秒。然而，由于我们串行请求的方式，我们至少要花费 40 秒在 I/O 上！我们程序运行时的 56% 时间都在做 I/O，并且仅仅是在“I/O
    等待”时，而它本可以做一些其他事情！
- en: Of course, as the CPU problem takes more and more time, the relative slowdown
    of doing this serial I/O decreases. This is simply because the cost of having
    a 100-millisecond pause after each task pales in comparison to the long amount
    of time needed to do this computation (as we can see in [Figure 8-7](#serial_code_CPU)).
    This fact highlights how important it is to understand your workload before considering
    which optimizations to make. If you have a CPU task that takes hours and an I/O
    task that takes only seconds, work done to speed up the I/O task will not bring
    the huge speedups you may be looking for!
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，随着 CPU 问题所需时间越来越长，做这种串行 I/O 的相对减速也会减少。这只是因为在每个任务后都暂停 100 毫秒的成本，相比于完成这个计算所需的长时间来说微不足道（正如我们在
    [图 8-7](#serial_code_CPU) 中所见）。这一事实突显了在考虑进行哪些优化之前了解工作负载的重要性。如果您有一个需要几小时的 CPU 任务和仅需要几秒钟的
    I/O 任务，那么加速 I/O 任务所带来的巨大提速可能并不会达到您所期望的效果！
- en: '![Comparison of the serial code to the CPU task with no I/O](Images/hpp2_08in01.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![串行代码与无 I/O 的 CPU 任务比较](Images/hpp2_08in01.png)'
- en: Figure 8-7\. Comparison of the serial code to the CPU task with no I/O
  id: totrans-143
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-7\. 串行代码与无 I/O 的 CPU 任务比较
- en: Batched Results
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批处理结果
- en: 'Instead of immediately going to a full asynchronous solution, let’s try an
    intermediate solution. If we don’t need to know the results in our database right
    away, we can batch up the results and send them to the database asynchronously.
    To do this, we create an object, `AsyncBatcher`, that will take care of queuing
    results to be sent to the database in small asynchronous bursts. This will still
    pause the program and put it into I/O wait with no CPU tasks; however, during
    this time we can issue many concurrent requests instead of issuing them one at
    a time:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是立即转向完全的异步解决方案，让我们尝试一个中间解决方案。如果我们不需要立即知道数据库中的结果，我们可以批量处理结果，并以小的异步突发方式将它们发送到数据库。为此，我们创建一个
    `AsyncBatcher` 对象，负责将结果排队，以便在没有 CPU 任务的 I/O 等待期间发送它们。在这段时间内，我们可以发出许多并发请求，而不是逐个发出它们：
- en: '[PRE8]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[![1](Images/1.png)](#co_asynchronous_i_o_CO7-1)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_asynchronous_i_o_CO7-1)'
- en: We are able to start up an event loop just to run a single asynchronous function.
    The event loop will run until the asynchronous function is complete, and then
    the code will resume as normal.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能够启动一个事件循环来运行单个异步函数。事件循环将一直运行，直到异步函数完成，然后代码将恢复正常运行。
- en: '[![2](Images/2.png)](#co_asynchronous_i_o_CO7-2)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_asynchronous_i_o_CO7-2)'
- en: This function is nearly identical to that of [Example 8-6](#conn_asyncio_http).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数与 [示例 8-6](#conn_asyncio_http) 几乎相同。
- en: 'Now we can proceed almost in the same way as we did before. The main difference
    is that we add our results to our `AsyncBatcher` and let it take care of when
    to send the requests. Note that we chose to make this object into a context manager
    so that once we are done batching, the final `flush()` gets called. If we didn’t
    do this, there would be a chance that we still have some results queued that didn’t
    trigger a flush:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们几乎可以和以前做法一样继续。主要区别在于我们将结果添加到我们的`AsyncBatcher`中，并让它负责何时发送请求。请注意，我们选择将此对象作为上下文管理器，以便一旦我们完成批处理，最终的`flush()`将被调用。如果我们没有这样做，可能会出现一些结果仍在排队等待触发刷新的情况：
- en: '[PRE9]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[![1](Images/1.png)](#co_asynchronous_i_o_CO8-1)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_asynchronous_i_o_CO8-1)'
- en: We choose to batch at 100 requests, for reasons similar to those illustrated
    in [Figure 8-3](#conn_num_concurrent_requests).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择以100个请求为一批处理的原因，类似于[图 8-3](#conn_num_concurrent_requests)中所示的情况。
- en: With this change, we are able to bring our runtime for a difficulty of 8 down
    to 10.21 seconds. This represents a 6.95× speedup without our having to do much
    work. In a constrained environment such as a real-time data pipeline, this extra
    speed could mean the difference between a system being able to keep up with demand
    and that system falling behind (in which case a queue will be required; you’ll
    learn about these in [Chapter 10](ch10.xhtml#clustering)).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这一变更，我们将难度为8的运行时间缩短到了10.21秒。这代表了6.95倍的加速，而我们几乎没有做什么额外的工作。在像实时数据管道这样的受限环境中，这种额外的速度可能意味着系统能否跟得上需求的差异，并且这种情况下可能需要一个队列；在[第10章](ch10.xhtml#clustering)中会学习到这些内容。
- en: To understand what is happening in this timing, let’s consider the variables
    that could affect the timings of this batched method. If our database had infinite
    throughput (i.e., if we could send an infinite number of requests at the same
    time without penalty), we could take advantage of the fact that we get only the
    100-millisecond penalty when our `AsyncBatcher` is full and does a flush. In this
    case, we’d get the best performance by just saving all of our requests to the
    database and doing them all at once when the calculation was finished.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解此批处理方法的时间计时，请考虑可能影响批处理方法时间的变量。如果我们的数据库吞吐量无限（即，我们可以同时发送无限数量的请求而没有惩罚），我们可以利用我们的`AsyncBatcher`满时执行刷新时只有100毫秒的惩罚。在这种情况下，我们可以在计算完成时一次性将所有请求保存到数据库并执行它们，从而获得最佳性能。
- en: However, in the real world, our databases have a maximum throughput that limits
    the number of concurrent requests they can process. In this case, our server is
    limited at 100 requests a second, which means we must flush our batcher every
    one hundred results and take the 100-millisecond penalty then. This is because
    the batcher still pauses the execution of the program, just as the serial code
    did; however, in that paused time it performs many requests instead of just one.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在现实世界中，我们的数据库有最大的吞吐量限制，限制了它们可以处理的并发请求数量。在这种情况下，我们的服务器每秒限制在100个请求，这意味着我们必须每100个结果刷新我们的批处理器，并在那时进行100毫秒的惩罚。这是因为批处理器仍然会暂停程序的执行，就像串行代码一样；但在这个暂停的时间内，它执行了许多请求而不是只有一个。
- en: If we tried to save all our results to the end and then issued them all at once,
    the server would only *process* one hundred at a time, and we’d have an extra
    penalty in terms of the overhead to making all those requests at the same time
    in addition to overloading our database, which can cause all sorts of unpredictable
    slowdowns.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们试图将所有结果保存到最后然后一次性发出它们，服务器一次只会处理一百个，而且我们会因为同时发出所有这些请求而额外增加开销，这会导致数据库超载，可能会导致各种不可预测的减速。
- en: On the other hand, if our server had terrible throughput and could deal with
    only one request at a time, we may as well run our code in serial! Even if we
    kept our batching at one hundred results per batch, when we actually go to make
    the requests, only one would get responded to at a time, effectively invalidating
    any batching we made.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果我们的服务器吞吐量非常差，一次只能处理一个请求，我们可能还是会串行运行我们的代码！即使我们将我们的批处理保持在每批100个结果，当我们实际去发起请求时，每次只会有一个请求被响应，有效地使我们所做的任何批处理无效。
- en: This mechanism of batching results, also known as *pipelining*, can help tremendously
    when trying to lower the burden of an I/O task (as seen in [Figure 8-8](#batching_results_vs_no_IO)).
    It offers a good compromise between the speeds of asynchronous I/O and the ease
    of writing serial programs. However, a determination of how much to pipeline at
    a time is very case-dependent and requires some profiling and tuning to get the
    best performance.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这种批处理结果的机制，也被称为*流水线处理*，在试图降低I/O任务负担时非常有帮助（如[图8-8](#batching_results_vs_no_IO)所示）。它在异步I/O速度和编写串行程序的便利性之间提供了很好的折衷方案。然而，确定一次性流水线处理多少内容非常依赖于具体情况，并且需要一些性能分析和调整才能获得最佳性能。
- en: '![Comparison of batching requests versus not doing any I/O](Images/hpp2_08in02.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![批处理请求与不进行任何I/O的比较](Images/hpp2_08in02.png)'
- en: Figure 8-8\. Comparison of batching requests versus not doing any I/O
  id: totrans-162
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-8\. 批处理请求与不进行任何I/O的比较
- en: Full Async
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 完全异步
- en: In some cases, we may need to implement a full asynchronous solution. This may
    happen if the CPU task is part of a larger I/O-bound program, such as an HTTP
    server. Imagine that you have an API service that, in response to some of its
    endpoints, has to perform heavy computational tasks. We still want the API to
    be able to handle concurrent requests and be performant in its tasks, but we also
    want the CPU task to run quickly.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，我们可能需要实现一个完全异步的解决方案。如果CPU任务是较大的I/O绑定程序的一部分，例如HTTP服务器，则可能会发生这种情况。想象一下，你有一个API服务，对于其中一些端点的响应，必须执行繁重的计算任务。我们仍然希望API能够处理并发请求，并在其任务中表现良好，但我们也希望CPU任务能够快速运行。
- en: The implementation of this solution in [Example 8-7](#async_CPU_workload) uses
    code very similar to that of [Example 8-6](#conn_asyncio_http).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例 8-7](#async_CPU_workload)中实现此解决方案的代码与[示例 8-6](#conn_asyncio_http)的代码非常相似。
- en: Example 8-7\. Async CPU workload
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例8-7\. 异步CPU负载
- en: '[PRE10]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[![1](Images/1.png)](#co_asynchronous_i_o_CO9-1)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_asynchronous_i_o_CO9-1)'
- en: Instead of `await`ing our database save immediately, we queue it into the event
    loop using `asyncio.create_task` and keep track of it so we can ensure that the
    task has completed before the end of the function.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会立即`await`数据库保存，而是使用`asyncio.create_task`将其排入事件循环，并跟踪它，以确保任务在函数结束前已完成。
- en: '[![2](Images/2.png)](#co_asynchronous_i_o_CO9-2)'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_asynchronous_i_o_CO9-2)'
- en: This is arguably the most important line in the function. Here, we pause the
    main function to allow the event loop to take care of any pending tasks. Without
    this, none of our queued tasks would run until the end of the function.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是函数中最重要的一行。在这里，我们暂停主函数，以便事件循环处理任何未完成的任务。如果没有这个，我们排队的任务将直到函数结束才会运行。
- en: '[![3](Images/3.png)](#co_asynchronous_i_o_CO9-3)'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_asynchronous_i_o_CO9-3)'
- en: Here we wait for any tasks that haven’t completed yet. If we had not done the
    `asyncio.sleep` in the `for` loop, all the saves would happen here!
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们等待任何尚未完成的任务。如果我们在`for`循环中没有执行`asyncio.sleep`，那么所有的保存操作将在这里发生！
- en: Before we go into the performance characteristics of this code, we should first
    talk about the importance of the `asyncio.sleep(0)` statement. It may seem strange
    to be sleeping for zero seconds, but this statement is a way to force the function
    to defer execution to the event loop and allow other tasks to run. In general
    in asynchronous code, this deferring happens every time an `await` statement is
    run. Since we generally don’t `await` in CPU-bound code, it’s important to force
    this deferment, or else no other task will run until the CPU-bound code is complete.
    In this case, if we didn’t have the sleep statement, all the HTTP requests would
    be paused until the `asyncio.wait` statement, and then all the requests would
    be issued at once, which is definitely not what we want!
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论此代码的性能特征之前，我们应该先谈谈`asyncio.sleep(0)`语句的重要性。让函数睡眠零秒可能看起来很奇怪，但这是一种将函数推迟到事件循环并允许其他任务运行的方法。在异步代码中，每次运行`await`语句时都会发生这种推迟。由于我们通常不会在CPU绑定的代码中`await`，所以强制进行这种推迟非常重要，否则在CPU绑定的代码完成之前不会运行任何其他任务。在这种情况下，如果没有睡眠语句，所有的HTTP请求将会暂停，直到`asyncio.wait`语句，然后所有的请求将会立即发出，这绝对不是我们想要的！
- en: One nice thing about having this control is that we can choose the best times
    to defer back to the event loop. There are many considerations when doing this.
    Since the run state of the program changes when we defer, we don’t want to do
    it in the middle of a calculation and potentially change our CPU cache. In addition,
    deferring to the event loop has an overhead cost, so we don’t want to do it too
    frequently. However, while we are bound up doing the CPU task, we cannot do any
    I/O tasks. So if our full application is an API, no requests can be handled during
    the CPU time!
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这种控制权，我们可以选择最佳时间推迟回到事件循环。在这样做时有很多考虑因素。由于程序的运行状态在推迟时发生变化，我们不希望在计算过程中进行推迟，可能会改变我们的CPU缓存。此外，推迟到事件循环会产生额外的开销，所以我们不希望太频繁地这样做。然而，在我们忙于CPU任务时，我们无法执行任何I/O任务。因此，如果我们的整个应用程序是一个API，那么在CPU时间内不能处理任何请求！
- en: Our general rule of thumb is to try to issue an `asyncio.sleep(0)` at any loop
    that we expect to iterate every 50 to 100 milliseconds or so. Some applications
    use `time.perf_counter` and allow the CPU task to have a specific amount of runtime
    before forcing a sleep. For a situation such as this, though, since we have control
    of the number of CPU and I/O tasks, we just need to make sure that the time between
    sleeps coincides with the time needed for pending I/O tasks to complete.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的一般经验法则是尝试在期望每50到100毫秒迭代一次的任何循环中发出`asyncio.sleep(0)`。有些应用程序使用`time.perf_counter`，允许CPU任务在强制休眠之前具有特定的运行时间。对于这种情况，由于我们可以控制CPU和I/O任务的数量，我们只需确保休眠之间的时间与待处理的I/O任务完成所需的时间相符即可。
- en: One major performance benefit to the full asynchronous solution is that we can
    perform all of our I/O *while* we are doing our CPU work, effectively hiding it
    from our total runtime (as we can see from the overlapping lines in [Figure 8-9](#graph_aiohttp_solution)).
    While it will never be completely hidden because of the overhead costs of the
    event loop, we can get very close. In fact, for a difficulty of 8 with 600 iterations,
    our code runs 7.3× faster than the serial code and performs its total I/O workload
    2× faster than the batched code (and this benefit over the batched code would
    only get better as we do more iterations, since the batched code loses time versus
    the asynchronous code every time it has to pause the CPU task to flush a batch).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 完全异步解决方案的一个主要性能优势是，在执行CPU工作的同时，我们可以执行所有的I/O操作，有效地隐藏它们不计入总运行时间（正如我们从[图8-9](#graph_aiohttp_solution)中重叠的线条中可以看到的）。虽然由于事件循环的开销成本，它永远不会完全隐藏，但我们可以非常接近。事实上，对于难度为8的600次迭代，我们的代码运行速度比串行代码快7.3倍，并且执行其总I/O工作负载比批处理代码快2倍（而且这种优势随着迭代次数的增加而增加，因为批处理代码每次需要暂停CPU任务来刷新批次时都会浪费时间）。
- en: '![Call graph for 25 difficulty-8 CPU tasks using the aiohttp solution. The
    red lines represents time working on a CPU task while blue lines represent time
    sending a result to the server](Images/hpp2_08in03.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![使用`aiohttp`解决方案进行25个难度为8的CPU任务的调用图。红线代表工作在CPU任务上的时间，蓝线代表发送结果到服务器的时间](Images/hpp2_08in03.png)'
- en: Figure 8-9\. Call graph for 25 difficulty-8 CPU tasks using the `aiohttp` solution—the
    red lines represent time working on a CPU task, while blue lines represent time
    sending a result to the server
  id: totrans-179
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-9\. 使用`aiohttp`解决方案进行25个难度为8的CPU任务的调用图—红线代表工作在CPU任务上的时间，蓝线代表发送结果到服务器的时间。
- en: In the call timeline, we can really see what is going on. What we’ve done is
    to mark the beginning and end of each CPU and I/O task for a short run of 25 CPU
    tasks with difficulty 8\. The first several I/O tasks are the slowest, taking
    a while to make the initial connection to our server. Because of our use of `aiohttp`’s
    `ClientSession`, these connections are cached, and all subsequent connections
    to the same server are much faster.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用时间线中，我们可以真正看到发生了什么。我们所做的是标记了难度为8的25个CPU任务的每个CPU和I/O任务的开始和结束。前几个I/O任务是最慢的，花费了一些时间来建立与我们服务器的初始连接。由于我们使用`aiohttp`的`ClientSession`，这些连接被缓存，后续对同一服务器的所有连接都要快得多。
- en: After this, if we just focus on the blue lines, they seem to happen very regularly
    without much of a pause between CPU tasks. Indeed, we don’t see the 100-millisecond
    delay from the HTTP request between tasks. Instead, we see the HTTP request being
    issued quickly at the end of each CPU task and later being marked as completed
    at the end of another CPU task.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 此后，如果我们只关注蓝线，它们似乎在 CPU 任务之间非常规律地发生，几乎没有暂停。事实上，我们在任务之间并没有看到来自 HTTP 请求的 100 毫秒延迟。相反，我们看到
    HTTP 请求在每个 CPU 任务的末尾快速发出，并在另一个 CPU 任务的末尾被标记为完成。
- en: We do see, though, that each individual I/O task takes longer than the 100-millisecond
    response time from the server. This longer wait time is given by the frequency
    of our `asyncio.sleep(0)` statements (since each CPU task has one `await`, while
    each I/O task has three) and the way the event loop decides which tasks come next.
    For the I/O task, this extra wait time is OK because it doesn’t interrupt the
    CPU task at hand. In fact, at the end of the run we can see the I/O runtimes shorten
    until the final I/O task is run. This final blue line is triggered by the `asyncio.wait`
    statement and runs incredibly quickly since it is the only remaining task and
    never needs to switch to other tasks.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，我们确实看到每个单独的 I/O 任务所需的时间比服务器的 100 毫秒响应时间长。这种较长的等待时间是由我们的 `asyncio.sleep(0)`
    语句的频率决定的（因为每个 CPU 任务有一个 `await`，而每个 I/O 任务有三个），以及事件循环决定下一个任务的方式。对于 I/O 任务来说，这种额外的等待时间是可以接受的，因为它不会中断手头的
    CPU 任务。事实上，在运行结束时，我们可以看到 I/O 运行时间缩短，直到最后一个 I/O 任务运行。这最后的蓝线是由 `asyncio.wait` 语句触发的，并且因为它是唯一剩下的任务，且永远不需要切换到其他任务，所以运行非常快速。
- en: In Figures [8-10](#conn_workload_all) and [8-11](#conn_workload_async), we can
    see a summary of how these changes affect the runtime of our code for different
    workloads. The speedup in the async code over the serial code is significant,
    although we are still a ways away from the speeds achieved in the raw CPU problem.
    For this to be completely remedied, we would need to use modules like `multiprocessing`
    to have a completely separate process that can deal with the I/O burden of our
    program without slowing down the CPU portion of the problem.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 [8-10](#conn_workload_all) 和 [8-11](#conn_workload_async) 中，我们可以看到这些变化如何影响不同工作负载下我们代码的运行时间总结。异步代码在串行代码上的加速效果显著，尽管我们离原始
    CPU 问题的速度还有一段距离。要完全解决这个问题，我们需要使用诸如 `multiprocessing` 这样的模块，以便有一个完全独立的进程来处理我们程序的
    I/O 负担，而不会减慢 CPU 部分的问题。
- en: '![Processing time difference between serial I/O, batched async I/O, full async
    I/O, and a control case where I/O is completely disabled](Images/hpp2_0807.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![串行 I/O、批量异步 I/O、完全异步 I/O 和完全禁用 I/O 之间的处理时间差异](Images/hpp2_0807.png)'
- en: Figure 8-10\. Processing time difference between serial I/O, batched async I/O,
    full async I/O, and a control case where I/O is completely disabled
  id: totrans-185
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-10\. 串行 I/O、批量异步 I/O、完全异步 I/O 和完全禁用 I/O 之间的处理时间差异
- en: '![Processing time difference between batched async, full async I/O, and I/O
    disabled](Images/hpp2_0808.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![批量异步、完全异步 I/O 和禁用 I/O 之间的处理时间差异](Images/hpp2_0808.png)'
- en: Figure 8-11\. Processing time difference between batched async, full async I/O,
    and I/O disabled
  id: totrans-187
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-11\. 批量异步、完全异步 I/O 和禁用 I/O 之间的处理时间差异
- en: Wrap-Up
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结语
- en: When solving problems in real-world and production systems, it is often necessary
    to communicate with an outside source. This outside source could be a database
    running on another server, another worker computer, or a data service that is
    providing the raw data that must be processed. Whenever this is the case, your
    problem can quickly become I/O-bound, meaning that most of the runtime is dominated
    by dealing with input/output.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在解决实际和生产系统中的问题时，通常需要与外部源通信。这个外部源可以是运行在另一台服务器上的数据库，另一个工作计算机，或者是提供必须处理的原始数据的数据服务。在这种情况下，你的问题很快可能会成为
    I/O 绑定，这意味着大部分运行时间都受输入/输出处理的影响。
- en: Concurrency helps with I/O-bound problems by allowing you to interleave computation
    with potentially multiple I/O operations. This allows you to exploit the fundamental
    difference between I/O and CPU operations in order to speed up overall runtime.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 并发通过允许您将计算与可能的多个 I/O 操作交错，有助于处理 I/O 绑定的问题。这使您能够利用 I/O 和 CPU 操作之间的基本差异，以加快总体运行时间。
- en: As we saw, `gevent` provides the highest-level interface for asynchronous I/O.
    On the other hand, `tornado` and `aiohttp` allow full control of an asynchronous
    I/O stack. In addition to the various levels of abstraction, every library uses
    a different paradigm for its syntax. However, `asyncio` is the binding glue for
    the asynchronous solutions and provides the fundamental mechanisms to control
    them all.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，`gevent` 提供了最高级别的异步I/O接口。另一方面，`tornado` 和 `aiohttp` 允许完全控制异步I/O堆栈。除了各种抽象级别外，每个库还使用不同的范式来表示其语法。然而，`asyncio`是异步解决方案的绑定胶水，并提供了控制它们所有的基本机制。
- en: We also saw how to mesh CPU and I/O tasks together and how to consider the various
    performance characteristics of each to come up with a good solution to the problem.
    While it may be appealing to go to a full asynchronous code immediately, sometimes
    intermediate solutions work almost as well without having quite the engineering
    burden.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还看到了如何将CPU和I/O任务合并在一起，以及如何考虑每个任务的各种性能特征，以便提出解决问题的好方法。虽然立即转向完全异步代码可能很吸引人，但有时中间解决方案几乎可以达到同样的效果，而不需要太多的工程负担。
- en: In the next chapter, we will take this concept of interleaving computation from
    I/O-bound problems and apply it to CPU-bound problems. With this new ability,
    we will be able to perform not only multiple I/O operations at once but also many
    computational operations. This capability will allow us to start to make fully
    scalable programs where we can achieve more speed by simply adding more computer
    resources that can each handle a chunk of the problem.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将把这种从I/O绑定问题中交错计算的概念应用到CPU绑定问题上。有了这种新能力，我们不仅可以同时执行多个I/O操作，还可以执行许多计算操作。这种能力将使我们能够开始制作完全可扩展的程序，通过简单地添加更多的计算资源，每个资源都可以处理问题的一部分，从而实现更快的速度。
- en: ^([1](ch08.xhtml#idm46122412293032-marker)) Which we’re sure you’re not doing!
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch08.xhtml#idm46122412293032-marker)) 我们确信你没有这样做！
- en: ^([2](ch08.xhtml#idm46122411776344-marker)) For example, [`fuggetaboutit`](http://bit.ly/fuggetaboutit)
    is a special type of probabilistic data structure (see [“Probabilistic Data Structures”](ch11_split_001.xhtml#less_ram_prob_ds))
    that uses the `tornado IOLoop` to schedule time-based tasks.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch08.xhtml#idm46122411776344-marker)) 例如，[`fuggetaboutit`](http://bit.ly/fuggetaboutit)
    是一种特殊类型的概率数据结构（参见[“概率数据结构”](ch11_split_001.xhtml#less_ram_prob_ds)），它使用`tornado
    IOLoop`来安排基于时间的任务。
- en: ^([3](ch08.xhtml#idm46122411089528-marker)) This is not necessary; it just serves
    to simplify our code.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch08.xhtml#idm46122411089528-marker)) 这不是必需的；它只是为了简化我们的代码。
- en: ^([4](ch08.xhtml#idm46122411087272-marker)) This is true for all distributed
    databases and other popular databases, such as Postgres, MongoDB, and so on.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch08.xhtml#idm46122411087272-marker)) 这对于所有分布式数据库和其他流行的数据库（例如Postgres，MongoDB等）都是正确的。
