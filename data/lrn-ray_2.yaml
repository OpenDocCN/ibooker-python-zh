- en: Chapter 3\. Building Your First Distributed Application
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三章。构建你的第一个分布式应用程序
- en: Now that you’ve seen the basics of the Ray API in action, let’s build something
    more realistic with it. By the end of this comparatively short chapter, you will
    have built a reinforcement learning (RL) problem from scratch, implemented your
    first algorithm to tackle it, and used Ray tasks and actors to parallelize this
    solution to a local cluster — all in less than 250 lines of code.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经看到了 Ray API 的基础知识，让我们用它构建一个更现实的东西。通过这个相对较短的章节结束时，你将已经从零开始构建了一个强化学习（RL）问题，实现了第一个解决方案，并使用了
    Ray 任务和执行者将此解决方案并行化到一个本地集群中——所有这些都不到 250 行代码。
- en: This chapter is designed to work for readers who don’t have any experience with
    reinforcement learning. We’ll work on a straightforward problem and develop the
    necessary skills to tackle it hands-on. Since chapter [Chapter 4](ch04.xhtml#chapter_04)
    is devoted entirely to this topic, we’ll skip all advanced RL topics and language
    and just focus on the problem at hand. But even if you’re a quite advanced RL
    user, you’ll likely benefit from implementing a classical algorithm in a distributed
    setting.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章旨在适用于没有任何强化学习经验的读者。我们将解决一个简单的问题，并培养必要的技能来实际应对它。由于[第四章](ch04.xhtml#chapter_04)完全致力于这个主题，我们将跳过所有高级的强化学习主题和术语，只专注于手头的问题。但即使你是一个相当高级的强化学习用户，你也很可能会受益于在分布式环境中实现一个经典算法。
- en: This is the last chapter working *only* with Ray Core. I hope you learn to appreciate
    how powerful and flexible it is, and how quickly you can implement distributed
    experiments, that would otherwise take considerable efforts to scale.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这是与 Ray Core *仅仅* 一起工作的最后一章了。我希望你学会欣赏它的强大和灵活性，以及你可以多快地实现分布式实验，否则需要相当大的努力来扩展。
- en: Setting Up A Simple Maze Problem
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置一个简单的迷宫问题
- en: As with the chapters before, I encourage you to code this chapter with me and
    build this application together as we go. In case you don’t want to do that, you
    can also simply follow [the notebook for this chapter](https://github.com/maxpumperla/learning_ray/blob/main/notebooks/ch_03_core_app.ipynb).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的章节一样，我鼓励你与我一起编写本章的代码，并在我们进行时共同构建此应用程序。如果你不想这样做，你也可以简单地跟随[本章的笔记本](https://github.com/maxpumperla/learning_ray/blob/main/notebooks/ch_03_core_app.ipynb)。
- en: 'To give you an idea, the app we’re building is structured as follows:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 为了给你一个概念，我们正在构建的应用程序结构如下：
- en: You implement a simple 2D-maze game in which a single player can move around
    in the four major directions.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你实现了一个简单的二维迷宫游戏，其中一个单个玩家可以在四个主要方向上移动。
- en: You initialize the maze as a `5x5` grid to which the player is confined.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将迷宫初始化为一个`5x5`的网格，玩家被限制在其中。
- en: One of the `25` grid cells is the “goal” that a player called the “seeker” must
    reach.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`25`个网格单元中的一个是“目标”，玩家称为“追寻者”必须到达的目标。'
- en: Instead of hard-coding a solution, you will employ a reinforcement learning
    algorithm, so that the seeker learns to find the goal.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你不会硬编码一个解决方案，而是会使用一个强化学习算法，让追寻者学会找到目标。
- en: This is done by repeatedly running simulations of the maze, rewarding the seeker
    for finding the goal and smartly keeping track of which decisions of the seeker
    worked and which didn’t.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是通过反复运行迷宫的模拟来完成的，奖励追寻者找到目标并聪明地跟踪追寻者的哪些决策有效，哪些不行。
- en: As running simulations can be parallelized and our RL algorithm can also be
    trained in parallel, we utilize the Ray API to parallelize the whole process.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于模拟可以并行化，并且我们的 RL 算法也可以并行训练，我们利用 Ray API 来并行化整个过程。
- en: We’re not quite ready to deploy this application on an actual Ray cluster comprised
    of multiple nodes just yet, so for now we’ll continue to work with local clusters.
    If you’re interested in infrastructure topics and want to learn how to set up
    Ray clusters, jump ahead to [Link to Come], and to see a fully deployed Ray application
    you can go to [Link to Come].
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有准备好将此应用程序部署到由多个节点组成的实际 Ray 集群上，所以现在我们将继续使用本地集群。如果你对基础设施主题感兴趣，并想学习如何设置 Ray
    集群，请跳到[链接将到来的地方]，要看到一个完全部署的 Ray 应用程序，你可以转到[链接将到来的地方]。
- en: Let’s start by implementing the 2D maze we just sketched. The idea is to implement
    a simple grid in Python that spans a 5x5 grid starting at `(0, 0)` and ending
    at `(4, 4)` and properly define how a player can move around the grid. To do this,
    we first need an abstraction for moving in the four cardinal directions. These
    four actions, namely moving up, down, left, and right, can be encoded in Python
    as a class we call `Discrete`. The abstraction of moving in several discrete actions
    is so useful that we’ll generalize it to `n` directions, instead of just four.
    In case you’re worried, this is not premature - we’ll actually need a general
    `Discrete` class in a moment.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从实现刚刚草绘的二维迷宫开始。我们的想法是在Python中实现一个简单的网格，该网格从`(0, 0)`开始，到`(4, 4)`结束，并正确定义玩家如何在网格中移动。为此，我们首先需要一个用于沿四个基本方向移动的抽象。这四个动作，即向上、向下、向左和向右，可以在Python中被编码为我们称之为`Discrete`的类。移动多个离散动作的抽象是如此有用，以至于我们将其泛化到`n`个方向，而不仅仅是四个。如果你担心，这并不是过早
    - 我们实际上在一会儿需要一个通用的`Discrete`类。
- en: Example 3-1\.
  id: totrans-15
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 例子 3-1\.
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[![1](assets/1.png)](#co_building_your_first_distributed_application_CO1-1)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_building_your_first_distributed_application_CO1-1)'
- en: A discrete action can be uniformly sampled between `0` and `n-1`.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一个离散动作可以在`0`到`n-1`之间均匀抽样。
- en: '[![2](assets/2.png)](#co_building_your_first_distributed_application_CO1-2)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_building_your_first_distributed_application_CO1-2)'
- en: For instance, a `Discrete(4)` sample will give you `0`, `1`, `2`, or `3`.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，从`Discrete(4)`中抽样将给出`0`、`1`、`2`或`3`。
- en: Sampling from a `Discrete(4)` like in this example will randomly return `0`,
    `1`, `2`, or `3`. How we interpret these numbers is up to us, so let’s say we
    go for “down”, “left”, “right”, and “up” in that order.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 从`Discrete(4)`中进行抽样，例如在这个例子中会随机返回`0`、`1`、`2`或`3`。我们如何解释这些数字由我们决定，所以假设我们选择“向下”、“向左”、“向右”和“向上”的顺序。
- en: Now that we know how to encode moving around the maze, let’s code the maze itself,
    including the `goal` cell and the position of the `seeker` player that tries to
    find the goal. To this end we’re going to implement a Python class called `Environment`.
    It’s called that, because the maze is the environment in which the player “lives”.
    To make matters easy, we’ll always put the `seeker` at `(0, 0)` and the `goal`
    at `(4, 4)`. To make the `seeker` move and find its goal, we initialize the `Environment`
    with an `action_space` of `Discrete(4)`.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何编码在迷宫中移动了，让我们编写迷宫本身的代码，包括`goal`单元格和试图找到目标的`seeker`玩家的位置。为此，我们将实现一个名为`Environment`的Python类。它被称为这个名字，因为迷宫是玩家“生活”的环境。为了简化问题，我们将`seeker`始终放在`(0,
    0)`，将`goal`放在`(4, 4)`。为了使`seeker`移动并找到其目标，我们将用`Discrete(4)`初始化`Environment`的`action_space`。
- en: 'There is one last bit of information we need to set up for our maze environment,
    namely an encoding of the `seeker` position. The reason for that is that we’re
    going to implement an algorithm later that keeps track of which actions led to
    good results for which seeker positions. By encoding the seeker position as a
    `Discrete(5*5)`, it becomes a single number that’s much easier to work with. In
    RL lingo it is common to call the information of the game that is accessible to
    the player an *observation*. So, in analogy to the actions we can carry out for
    our `seeker`, we can also define an `observation_space` for it. Here’s the implementation
    of what we’ve just discussed:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的迷宫环境中还有一点信息需要设置，那就是`seeker`位置的编码。原因是我们将来要实现一个算法，用于跟踪哪些动作对应于哪些`seeker`位置带来了良好的结果。将`seeker`位置编码为`Discrete(5*5)`，这样可以变成一个更易处理的单一数字。在强化学习术语中，把玩家可以访问的游戏信息称为*observation*。所以，类比我们为`seeker`定义动作空间一样，我们也可以为其定义一个`observation_space`。这里是我们刚刚讨论过的实现：
- en: Example 3-2\.
  id: totrans-24
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 例子 3-2\.
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[![1](assets/1.png)](#co_building_your_first_distributed_application_CO2-1)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_building_your_first_distributed_application_CO2-1)'
- en: The `seeker` gets initialized in the top left, the `goal` in the bottom right
    of the maze.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`seeker`初始化在迷宫的左上角，`goal`初始化在右下角。'
- en: '[![2](assets/2.png)](#co_building_your_first_distributed_application_CO2-2)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_building_your_first_distributed_application_CO2-2)'
- en: Our `seeker` can move down, left, up and right.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`seeker`可以向下、向左、向上和向右移动。
- en: '[![3](assets/3.png)](#co_building_your_first_distributed_application_CO2-3)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_building_your_first_distributed_application_CO2-3)'
- en: And it can be in a total of `25` states, one for each position on the grid.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 并且它可以处于总共`25`种状态中，每个状态代表网格上的一个位置。
- en: Note that we defined an `info` variable as well, which can be used to print
    information about the current state of the maze, for instance for debugging purposes.
    To play an actual game of find-the-goal from the perspective of the seeker, we
    have to define a few helper methods. Clearly, the game should be considered “done”
    when the seeker finds the goal. Also, we should reward the seeker for finding
    the goal. And when the game is over, we should be able to reset it to its initial
    state, to play again. To round things off, we also define a `get_observation`
    method that returns the encoded `seeker` position. Continuing our implementation
    of the `Environment` class, this translates into the following four methods.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们还定义了一个`info`变量，它可以用于打印迷宫当前状态的信息，例如用于调试目的。要从寻求者的角度玩找到目标的实际游戏，我们必须定义几个辅助方法。显然，当寻求者找到目标时，游戏应该被认为是“完成”的。此外，我们应该奖励寻求者找到目标。当游戏结束时，我们应该能够将其重置为初始状态，以便再次游戏。最后，我们还定义了一个`get_observation`方法，返回编码的`seeker`位置。继续实现`Environment`类，这转化为以下四种方法。
- en: Example 3-3\.
  id: totrans-33
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-3\.
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[![1](assets/1.png)](#co_building_your_first_distributed_application_CO3-1)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_building_your_first_distributed_application_CO3-1)'
- en: To play a new game, we’ll have to `reset` the grid to its original state.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始一个新游戏，我们必须将网格`reset`为其原始状态。
- en: '[![2](assets/2.png)](#co_building_your_first_distributed_application_CO3-2)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_building_your_first_distributed_application_CO3-2)'
- en: Converting the seeker tuple to a value from the environment’s `observation_space`.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 将寻求者元组转换为环境的`observation_space`中的值。
- en: '[![3](assets/3.png)](#co_building_your_first_distributed_application_CO3-3)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_building_your_first_distributed_application_CO3-3)'
- en: The seeker is only rewarded when reaching the goal.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 只有当寻求者到达目标时才会获得奖励。
- en: '[![4](assets/4.png)](#co_building_your_first_distributed_application_CO3-4)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_building_your_first_distributed_application_CO3-4)'
- en: If the seeker is at the goal, the game is over.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果寻求者在目标处，则游戏结束。
- en: 'The last essential method to implement is the `step` method. Imagine you’re
    playing our maze game and decide to go right as your next move. The `step` method
    will take this action (namely `3`, the encoding of “right”) and apply it to the
    internal state of the game. To reflect what changed, the `step` method will then
    return the seeker’s observations, its reward, whether the game is over, and the
    `info` value of the game. Here’s how the `step` method works:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最后要实现的关键方法是`step`方法。想象一下你在玩迷宫游戏，并决定向右移动作为你的下一步。`step`方法将采取这个动作（即`3`，表示“右”）并将其应用到游戏的内部状态。为了反映发生了什么变化，`step`方法将返回寻求者的观察结果、其奖励、游戏是否结束以及游戏的`info`值。这就是`step`方法的工作原理：
- en: Example 3-4\.
  id: totrans-44
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-4\.
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[![1](assets/1.png)](#co_building_your_first_distributed_application_CO4-1)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_building_your_first_distributed_application_CO4-1)'
- en: After taking a step in the specified direction, we return observation, reward,
    whether we’re done, and any additional information we might find useful.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在指定方向迈出一步后，我们返回观察结果、奖励、游戏是否结束以及可能有用的任何其他信息。
- en: I said the `step` method was the last essential method, but we actually want
    to define one more helper method that’s extremely helpful to visualize the game
    and help us understand it. This `render` method will print the current state of
    the game to the command line.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我说`step`方法是最后一个必要的方法，但实际上我们想定义一个更有帮助的辅助方法来可视化游戏并帮助我们理解它。这个`render`方法将把游戏的当前状态打印到命令行。
- en: Example 3-5\.
  id: totrans-49
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-5\.
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[![1](assets/1.png)](#co_building_your_first_distributed_application_CO5-1)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_building_your_first_distributed_application_CO5-1)'
- en: First we clear the screen.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们清除屏幕。
- en: '[![2](assets/2.png)](#co_building_your_first_distributed_application_CO5-2)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_building_your_first_distributed_application_CO5-2)'
- en: Then we draw the grid and mark the goal as `G` and the seeker as `S` on it.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们绘制网格，在上面标记目标为`G`，标记寻求者为`S`。
- en: '[![3](assets/3.png)](#co_building_your_first_distributed_application_CO5-3)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_building_your_first_distributed_application_CO5-3)'
- en: The grid then gets rendered by printing it to your screen.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，通过将其打印到您的屏幕上来渲染网格。
- en: Great, now we have completed the implementation of our `Environment` class that’s
    defining our 2D-maze game. We can `step` through this game, know when it’s `done`
    and `reset` it again. The player of the game, the `seeker`, can also observe its
    environment and gets rewarded for finding the goal.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了，现在我们完成了定义我们的2D迷宫游戏的`Environment`类的实现。我们可以通过这个游戏进行`step`，知道什么时候`done`并再次`reset`它。游戏的玩家，即寻求者，还可以观察其环境，并为找到目标而获得奖励。
- en: 'Let’s use this implementation to play a game of find-the-goal for a seeker
    that simply takes random actions. This can be done by creating a new `Environment`,
    sampling and applying actions to it, and rendering the environment until the game
    is over:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用这个实现来玩一个寻找目标的游戏，寻找者只需随机采取行动。这可以通过创建一个新的`Environment`，对其进行采样并应用行动，然后渲染环境直到游戏结束来完成：
- en: Example 3-6\.
  id: totrans-59
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例3-6。
- en: '[PRE5]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[![1](assets/1.png)](#co_building_your_first_distributed_application_CO6-1)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_building_your_first_distributed_application_CO6-1)'
- en: We can test our environment by applying sampled actions until we’re done.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过应用采样的行动来测试我们的环境，直到我们完成为止。
- en: '[![2](assets/2.png)](#co_building_your_first_distributed_application_CO6-2)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_building_your_first_distributed_application_CO6-2)'
- en: To visualize the environment we render it after waiting for a tenth of a second
    (otherwise the code runs too fast to follow).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化环境，我们在等待十分之一秒后渲染它（否则代码运行得太快了，无法跟上）。
- en: If you run this on your computer, eventually you’ll see that the game is over
    and the seeker has found the goal. It might take a while if you’re unlucky.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在你的电脑上运行这个程序，最终你会看到游戏结束了，寻找者找到了目标。如果你不幸的话，可能会花一些时间。
- en: 'In case you’re objecting that this is an extremely simple problem, and to solve
    it all you have to do is take at total of 8 steps, namely going right and down
    four times each in arbitrary order, I’m not arguing with you. The point is that
    we want to tackle this problem using machine learning, so that we can take on
    much harder problems later. Specifically, we want to implement an algorithm that
    figures out on its own how to play the game, merely by playing the game repeatedly:
    observing what’s happening, deciding what to do next, and getting rewarded for
    your actions.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你认为这是一个非常简单的问题，并且解决它只需走8步，即随意向右和向下各四次，我不会和你争辩。关键是，我们想要使用机器学习来解决这个问题，这样我们以后就可以解决更困难的问题。具体来说，我们想要实现一个算法，通过反复玩游戏：观察发生了什么，决定下一步要做什么，并为你的行为获得奖励，从而自行解决如何玩游戏。
- en: 'If you want to, now is a good time to make the game more complex yourself.
    As long as you do not change the interface we defined for the `Environment` class,
    you could modify this game in many ways. Here are a few suggestions:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你愿意的话，现在是时候让游戏变得更加复杂了。只要你不改变我们为`Environment`类定义的接口，你可以以许多方式修改这个游戏。以下是一些建议：
- en: Make it a 10x10 grid or randomize the initial position of the seeker.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将其设为10x10网格或随机化寻找者的初始位置。
- en: Make the outer walls of the grid dangerous. Whenever you try to touch them,
    you’ll incur a reward of -100, i.e a steep penalty.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让网格的外围墙变得危险。每当你试图接触它们时，你都会受到-100的奖励，即严重惩罚。
- en: Introduce obstacles in the grid that the seeker cannot pass through.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在网格中引入障碍物，寻找者无法通过。
- en: If you’re feeling really adventurous, you could also randomize the goal position.
    This requires extra care, as currently the seeker has no information about the
    goal position in terms of the `get_observation` method. Maybe come back to tackling
    this last exercise after you’ve finished reading this chapter.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你感觉非常有冒险精神，你也可以随机化目标位置。这需要额外的注意，因为目前寻找者在`get_observation`方法中对目标位置没有信息。也许在阅读完本章之后再回来解决这个最后的练习。
- en: Building a Simulation
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 建立一个模拟
- en: With the `Environment` class implemented, what does it take to tackle the problem
    of “teaching” the seeker to play the game well? How can it find the goal consistently
    in the minimum number of 8 steps necessary? We’ve equipped the maze environment
    with reward information, so that the seeker can use this signal to learn to play
    the game. In reinforcement learning, you play games repeatedly and learn from
    the experience you made in the process. The player of the game is often referred
    to as *agent* that takes *actions* in the environment, observes its *state* and
    receives a *reward*.^([1](ch03.xhtml#idm44990031607120)) The better an agent learns,
    the better it becomes at interpreting the current game state (observations) and
    finding actions that lead to more rewarding outcomes.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 有了`Environment`类的实现，解决“教导”寻找者如何玩得好这个问题需要什么？它如何能够在最少的8步中一直找到目标？我们已经为迷宫环境配备了奖励信息，这样寻找者就可以使用这个信号来学习玩游戏。在强化学习中，你会反复玩游戏，并从你在这个过程中获得的经验中学习。游戏的玩家通常被称为*代理*，它在环境中采取*行动*，观察其*状态*并获得*奖励*。^([1](ch03.xhtml#idm44990031607120))代理学得越好，它就越能够解释当前游戏状态（观察）并找到导致更有益的结果的行动。
- en: Regardless of the RL algorithm you want to use (in case you know any), you need
    to have a way of simulating the game repeatedly, to collect experience data. For
    this reason we’re going to implement a simple `Simulation` class in just a bit.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您想使用什么样的强化学习算法（如果您了解任何算法），您都需要一种重复模拟游戏以收集经验数据的方法。因此，我们将很快实现一个简单的`Simulation`类。
- en: The other useful abstraction we need to proceed is that of a `Policy`, a way
    of specifying actions. Right now the only thing we can do to play the game is
    sampling random actions for our seeker. What a `Policy` allows us to do is to
    get better actions for the current state of the game. In fact, we define a `Policy`
    to be a class with a `get_action` method that takes a game state and returns an
    action.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要继续进行的另一个有用的抽象是`Policy`，一种指定行动的方式。目前，我们可以为追逐者随机抽取行动来玩游戏。`Policy`允许我们为当前游戏状态获取更好的行动。实际上，我们将`Policy`定义为一个具有`get_action`方法的类，该方法接受游戏状态并返回一个动作。
- en: Remember that in our game the seeker has a total of `25` possible states on
    the grid, and can carry out `4` actions. A simple idea would be to look at pairs
    of states and actions and assign a high value to a pair if carrying out this action
    in this state will lead to a high reward, and a low value otherwise. For instance,
    from your intuition of the game it should be clear that going down or right is
    always a good idea, whereas going left or up is not. Then, create a `25x4` lookup
    table of all possible state-action pairs and store it in our `Policy`. Then we
    could simply ask our policy to return the highest value of any action given a
    state. Of course, implementing an algorithm that finds good values for these state-action
    pairs is the challenging part. Let’s implement this idea of a `Policy` in first
    and worry about a suitable algorithm later.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在我们的游戏中，追逐者在网格上有`25`种可能的状态，并且可以执行`4`个动作。一个简单的想法是观察状态和动作的配对，并为配对分配高值，如果在这个状态下执行此动作将导致高奖励，否则分配低值。例如，从您对游戏的直觉来看，向下或向右移动总是一个好主意，而向左或向上移动则不是。然后，创建一个`25x4`的查找表，包含所有可能的状态-动作配对，并将其存储在我们的`Policy`中。然后我们只需请求我们的策略在给定状态时返回任何动作的最高值。当然，实现一个为这些状态-动作配对找到好值的算法是具有挑战性的部分。让我们首先实现这个`Policy`的概念，然后再考虑适合的算法。
- en: Example 3-7\.
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-7\.
- en: '[PRE6]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[![1](assets/1.png)](#co_building_your_first_distributed_application_CO7-1)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_building_your_first_distributed_application_CO7-1)'
- en: We define a nested list of values for each state-action pair, initialized to
    zero.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为每个状态-动作对定义了一个值的嵌套列表，初始值为零。
- en: '[![2](assets/2.png)](#co_building_your_first_distributed_application_CO7-2)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_building_your_first_distributed_application_CO7-2)'
- en: On demand, we can `explore` random actions so that we don’t get stuck in suboptimal
    behavior.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 根据需求，我们可以`explore`随机行动，以避免陷入次优行为。
- en: '[![3](assets/3.png)](#co_building_your_first_distributed_application_CO7-3)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_building_your_first_distributed_application_CO7-3)'
- en: Sometimes we might want to randomly explore actions in the game, which is why
    we introduce an `explore` parameter to the `get_action` method. By default, this
    happens 10% of the time.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候我们可能希望在游戏中随机探索行动，这就是为什么我们在`get_action`方法中引入了一个`explore`参数。默认情况下，这种情况发生的概率为10%。
- en: We return the action with the highest value in the lookup table, given the current
    state.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们返回查找表中值最高的动作，给定当前状态。
- en: I’ve snuck in a little implementation detail into the `Policy` definition that
    might be a bit confusing. The `get_action` method has an `explore` parameter.
    The reason for this is that if you learn an extremely poor policy, e.g. one that
    always wants you to move left, you have no chance of ever finding better solutions.
    In other words, sometimes you need to explore new ways, and not “exploit” your
    current understanding of the game. As indicated before, we haven’t discussed how
    to learn to improve the values in the `state_action_table` of our policy. For
    now, just keep in mind that the policy gives us the actions we want to follow
    when simulating the maze game.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我在`Policy`定义中添加了一个小的实现细节，可能有点令人困惑。`get_action`方法有一个`explore`参数。这样做的原因是，如果您学习了一个极度不好的策略，例如总是想向左移动的策略，您将永远没有机会找到更好的解决方案。换句话说，有时您需要探索新的方法，而不是“利用”您对游戏的当前理解。如前所述，我们还没有讨论如何学习改善我们策略的`state_action_table`中的值。现在，只需记住策略在模拟迷宫游戏时给我们所需的行动即可。
- en: 'Moving on to the `Simulation` class we spoke about earlier, a simulation should
    take an `Environment` and compute actions of a given `Policy` until the goal is
    reached and the game ends. The data we observe when “rolling out” a full game
    like this is what we call the *experience* we gained. Accordingly, our `Simulation`
    class has a `rollout` method that computes `experiences` for a full game and returns
    them. Here’s what the implementation of the `Simulation` class looks like:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 继续之前提到的`Simulation`类，一个模拟应当接受一个`Environment`并计算给定`Policy`的动作，直至达到目标并结束游戏。当我们“执行”这样的完整游戏时观察到的数据，就是我们所说的*经验*。因此，我们的`Simulation`类具有一个`rollout`方法，用于计算一个完整游戏的`经验`并返回它们。这里是`Simulation`类的实现：
- en: Example 3-8\.
  id: totrans-88
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-8\.
- en: '[PRE7]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[![1](assets/1.png)](#co_building_your_first_distributed_application_CO8-1)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_building_your_first_distributed_application_CO8-1)'
- en: We compute a game “roll-out” by following the actions of a `policy`, and we
    can optionally render the simulation.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过遵循`policy`的动作计算游戏的“roll-out”，可以选择渲染模拟。
- en: '[![2](assets/2.png)](#co_building_your_first_distributed_application_CO8-2)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_building_your_first_distributed_application_CO8-2)'
- en: To be sure, we reset the environment before each rollout.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保，我们在每次`rollout`之前重置环境。
- en: '[![3](assets/3.png)](#co_building_your_first_distributed_application_CO8-3)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_building_your_first_distributed_application_CO8-3)'
- en: The passed in `policy` drives the actions we take. The `explore` and `epsilon`
    parameters are passed through.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 传入的`policy`驱动我们采取的动作。`explore`和`epsilon`参数被传递。
- en: '[![4](assets/4.png)](#co_building_your_first_distributed_application_CO8-4)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_building_your_first_distributed_application_CO8-4)'
- en: We step through the environment by applying the policy’s `action`.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过应用策略的`action`在环境中进行步进。
- en: '[![5](assets/5.png)](#co_building_your_first_distributed_application_CO8-5)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_building_your_first_distributed_application_CO8-5)'
- en: We define an experience as a `(state, action, reward, next_state)` quadruple.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义经验为一个`(state, action, reward, next_state)`四元组。
- en: '[![6](assets/6.png)](#co_building_your_first_distributed_application_CO8-6)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](assets/6.png)](#co_building_your_first_distributed_application_CO8-6)'
- en: Optionally render the environment at each step.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个步骤可选择渲染环境。
- en: 'Note that each entry of the `experiences` we collect in a `rollout` consists
    of four values: the current state, the action taken, the reward received, and
    the next state. The algorithm we’re going to implement in a moment will use these
    experiences to learn from them. Other algorithms might use other experience values,
    but those are the ones we need to proceed.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们在`rollout`中收集的每个`experiences`条目包含四个值：当前状态、采取的动作、接收到的奖励和下一个状态。我们即将实现的算法将利用这些经验来学习。其他算法可能使用其他的经验值，但这些是我们继续所需的。
- en: 'Now we have a policy that hasn’t learned anything just yet, but we can already
    test its interface to see if it works. Let’s try it out by initializing a `Simulation`
    object, calling its `rollout` method on a not-so-smart `Policy`, and then printing
    the `state_action_table` of it:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个尚未学到任何东西的策略，但我们已经可以测试其接口是否工作。让我们通过初始化一个`Simulation`对象，调用其在一个不那么聪明的`Policy`上的`rollout`方法，并打印其`state_action_table`来试试看：
- en: Example 3-9\.
  id: totrans-104
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-9\.
- en: '[PRE8]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[![1](assets/1.png)](#co_building_your_first_distributed_application_CO9-1)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_building_your_first_distributed_application_CO9-1)'
- en: We roll-out one full game with an “untrained” policy that we render.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一个“未经训练”的策略进行一个完整游戏的“roll-out”，并进行渲染。
- en: '[![2](assets/2.png)](#co_building_your_first_distributed_application_CO9-2)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_building_your_first_distributed_application_CO9-2)'
- en: The state-action values are currently all zero.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 当前所有状态-动作值均为零。
- en: If you feel like we haven’t made much progress since the last section, I can
    promise you that things will come together in the next one. The prep work of setting
    up a `Simulation` and a `Policy` were necessary to frame the problem correctly.
    Now the only thing that’s left is to devise a smart way to update the internal
    state of the `Policy` based on the experiences we’ve collected, so that it actually
    learns to play the maze game.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你觉得自上一节以来我们没有取得太多进展，我可以向你保证，接下来的部分将会有所进展。设置`Simulation`和`Policy`的准备工作是为了正确地构架问题。现在唯一剩下的就是设计一种智能的方式，根据我们收集到的经验更新`Policy`的内部状态，使其真正学会玩迷宫游戏。
- en: Training a Reinforcement Learning Model
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练强化学习模型
- en: Imagine we have a set of experiences that we’ve collected from a couple of games.
    What would be a smart way to update the values in the `state_action_table` of
    our `Policy`? Here’s one idea. Let’s say you’re sitting at position `(3,5)`, and
    you’ve decided to go right, which puts you at `(4,5)`, just one step away from
    the goal. Clearly you could then just go right and collect a reward of `1` in
    this scenario. That must mean the current state you’re in combined with an action
    of going “right” should have a high value. In other words, the value of this particular
    state-action pair should be high. In contrast, moving left in the same situation
    does not lead to anything, and the corresponding state-action pair should have
    a low value.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一组经验数据，这些数据是从几场游戏中收集到的。更新我们`Policy`的`state_action_table`中的值的聪明方法是什么？这里有一个想法。假设你处于位置`(3,5)`，然后决定向右移动，将你移到`(4,5)`，距离目标只有一步之遥。显然，在这种情况下，你可以向右移动并收到奖励`1`。这必须意味着你当前的状态结合向右行动应该有一个很高的值。换句话说，这个特定的状态-动作对的值应该很高。相反，在相同的情况下向左移动并没有带来任何进展，因此相应的状态-动作对应该有一个低值。
- en: More generally, let’s say you were in a given `state`, you’ve decided to take
    an `action`, leading to a `reward`, and you’re then in `next_state`. Remember
    that this is how we defined an experience. With our `policy.state_action_table`
    we can peek a little ahead and see if we can expect to gain anything from actions
    taken from `next_state`. That is, we can compute
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般地说，假设你处于给定的`state`，然后决定采取一个`action`，导致一个`reward`，然后进入`next_state`。请记住，这就是我们定义的一次经验。通过我们的`policy.state_action_table`，我们可以稍微展望一下，并查看从`next_state`采取行动是否能带来任何好处。也就是说，我们可以计算
- en: '[PRE9]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'How should we compare the knowledge of this value to the current state-action
    value, which is `value = policy.state_action_table[state][action]`? There are
    many ways to go about this, but we clearly can’t completely discard the current
    `value` and put too much trust in `next_max`. After all, this is just a single
    piece of experience we’re using here. So as a first approximation, why don’t we
    simply compute a weighted sum of the old and the expected value and go with `new_value
    = 0.9 * value + 0.1 * next_max`? Here, the values `0.9` and `0.1` have been chosen
    somewhat arbitrarily, the only important piece is that the first value is high
    enough to reflect our preference to keep the old value, and that both weights
    sum to `1`. That formula is a good starting point, but the problem is that we’re
    not at all factoring in the crucial information that we’re getting from the `reward`.
    In fact, we should put more trust in the current `reward` value than in the projected
    `next_max` value, so it’s a good idea to discount the latter a little, let’s say
    by 10%. Updating the state-action value would then look like this:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如何将这个值的知识与当前的状态-动作值进行比较，即`value = policy.state_action_table[state][action]`？有许多方法可以解决这个问题，但显然我们不能完全丢弃当前的`value`，而过于信任`next_max`。毕竟，这只是我们在这里使用的单一经验。因此，作为第一个近似值，为什么我们不简单地计算旧值和期望值的加权和，并选择`new_value
    = 0.9 * value + 0.1 * next_max`？在这里，`0.9`和`0.1`这些值被任意地选择，唯一重要的是第一个值足够高以反映我们保留旧值的偏好，并且这两个权重的总和为`1`。这个公式是一个很好的起点，但问题在于我们根本没有考虑到从`reward`中获取的关键信息。事实上，我们应该更加信任当前的`reward`值而不是预测的`next_max`值，因此最好稍微打折后者，比如说10%。更新状态-动作值将如下所示：
- en: '[PRE10]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Depending on your level of experience with this kind of reasoning, the last
    few paragraphs might be a lot to digest. The good thing is that, if you’ve understood
    the explanations up to this point, the remainder of this chapter will likely come
    easy to you. Mathematically, this was the last (and only) hard part of this example.
    If you’ve worked with RL before, you will have noticed by now that this is an
    implementation of the so-called Q-Learning algorithm. It’s called that, because
    the state-action table can be described as a function `Q(state, action)` that
    returns values for these pairs.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你对这种推理方式的经验水平，最后几段可能需要一些时间消化。好消息是，如果你已经理解到这一点，这一章剩下的部分可能会很容易理解。从数学上讲，这是这个示例中唯一的（也是最后的）难点。如果你以前接触过强化学习，你现在可能已经注意到这是所谓的Q学习算法的实现方式。它被称为这样，因为状态-动作表可以描述为一个函数`Q(state,
    action)`，它返回这些对的值。
- en: 'We’re almost there, so let’s formalize this procedure by implementing an `update_policy`
    function for a policy and collected experiences:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经接近了，现在让我们通过为一个策略和收集到的经验实现一个`update_policy`函数来正式化这个过程：
- en: Example 3-10\.
  id: totrans-119
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-10。
- en: '[PRE11]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[![1](assets/1.png)](#co_building_your_first_distributed_application_CO10-1)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_building_your_first_distributed_application_CO10-1)'
- en: We loop through all experiences in order.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按顺序遍历所有经验。
- en: '[![2](assets/2.png)](#co_building_your_first_distributed_application_CO10-2)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_building_your_first_distributed_application_CO10-2)'
- en: Then we choose the maximum value among all possible actions in the next state.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们在下一个状态中选择所有可能动作中的最大值。
- en: '[![3](assets/3.png)](#co_building_your_first_distributed_application_CO10-3)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_building_your_first_distributed_application_CO10-3)'
- en: We then extract the current state-action value.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 然后提取当前状态动作值。
- en: '[![4](assets/4.png)](#co_building_your_first_distributed_application_CO10-4)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_building_your_first_distributed_application_CO10-4)'
- en: The new value is the weighted sum of the old value and the expected value, which
    is the sum of the current reward and the discounted `next_max`.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 新值是旧值和期望值的加权和，期望值是当前奖励和折扣`next_max`的总和。
- en: '[![5](assets/5.png)](#co_building_your_first_distributed_application_CO10-5)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_building_your_first_distributed_application_CO10-5)'
- en: After updating, we set the new `state_action_table` value.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 更新后，我们设置新的`state_action_table`值。
- en: 'Having this function in place now makes it really simple to train a policy
    to make better decisions. We can use the following procedure:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有了这个函数，训练策略做出更好的决策就变得非常简单了。我们可以使用以下过程：
- en: Initialize a policy and a simulation.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始化一个策略和一个模拟器。
- en: Run the simulation many times, let’s say for a total of `10000` runs.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行模拟多次，比如总共运行`10000`次。
- en: For each game, first collect the experiences by running a `rollout`.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每场游戏，首先通过运行`rollout`收集经验。
- en: Then update the policy by calling `update_policy` on the collected experiences.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后通过调用`update_policy`更新收集到的经验来更新策略。
- en: That’s it! The following `train_policy` function implements the above procedure
    straight up.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！以下`train_policy`函数直接实现了上述过程。
- en: Example 3-11\.
  id: totrans-137
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-11\.
- en: '[PRE12]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[![1](assets/1.png)](#co_building_your_first_distributed_application_CO11-1)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_building_your_first_distributed_application_CO11-1)'
- en: Collect experiences for each game.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 收集每场游戏的经验。
- en: '[![2](assets/2.png)](#co_building_your_first_distributed_application_CO11-2)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_building_your_first_distributed_application_CO11-2)'
- en: Update our policy with those experiences.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些经验更新我们的策略。
- en: '[![3](assets/3.png)](#co_building_your_first_distributed_application_CO11-3)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_building_your_first_distributed_application_CO11-3)'
- en: Finally, train and return a policy for our `enviroment` from before.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，从之前的`enviroment`中训练并返回一个策略。
- en: Note that the high-brow way of speaking of a full play-through of the maze game
    is an *episode* in the RL literature. That’s why we call the argument `num_episodes`
    in the `train_policy` function, rather than `num_games`.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在强化学习文献中，高级说法是迷宫游戏的完整游戏称为一个*情节*。这就是为什么我们在`train_policy`函数中称之为`num_episodes`而不是`num_games`的原因。
- en: 'Now that we have a trained policy, let’s see how well it performs. We’ve run
    random policies twice before in this chapter, just to get an idea of how well
    they work for the maze problem. But let’s now properly evaluate our trained policy
    on several games and see how it does on average. Specifically, we’ll run our simulation
    for a couple of episodes and count how many steps it took per episode to reach
    the goal. So, let’s implement an `evaluate_policy` function that does precisely
    that:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个训练过的策略，让我们看看它的表现如何。在本章中，我们之前随机运行了两次策略，只是为了了解它们在迷宫问题上的表现。但现在让我们在几个游戏上适当评估我们训练过的策略，看看它的平均表现如何。具体来说，我们将运行我们的模拟几个情节，并计算每个情节需要多少步才能达到目标。因此，让我们实现一个`evaluate_policy`函数，正好做到这一点：
- en: Example 3-12\.
  id: totrans-147
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-12\.
- en: '[PRE13]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[![1](assets/1.png)](#co_building_your_first_distributed_application_CO12-1)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_building_your_first_distributed_application_CO12-1)'
- en: This time we set `explore` to `False` to fully exploit the learnings of the
    trained policy.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这次我们将`explore`设置为`False`，以充分利用训练过的策略的学习。
- en: '[![2](assets/2.png)](#co_building_your_first_distributed_application_CO12-2)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_building_your_first_distributed_application_CO12-2)'
- en: The length of the `experiences` is the number of steps we took to finish the
    game.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '`experiences`的长度是我们完成游戏所需的步数。'
- en: 'Apart from seeing the trained policy crush the maze problem ten times in a
    row, as we hoped it would, you should also see the following prompt:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 除了看到训练过的策略连续十次击败迷宫问题，正如我们希望的那样，您还应该看到以下提示：
- en: '[PRE14]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In other words, the trained policy is able to find optimal solutions for the
    maze game. That means you’ve successfully implemented your first RL algorithm
    from scratch!
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，训练后的策略能够为迷宫游戏找到最优解。这意味着你成功地从头开始实现了你的第一个强化学习算法！
- en: With the understanding you’ve built up by now, do you think placing the `seeker`
    into randomized starting positions and then running this evaluation function would
    still work? Why don’t you go ahead and make the changes necessary for that?
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 基于你目前积累的理解，你认为将 `seeker` 放入随机起始位置，然后运行这个评估函数仍然有效吗？为什么不继续做必要的更改？
- en: Another interesting question to ask yourself is what assumptions went into the
    algorithm we used. For instance, it’s clearly a prerequisite for the algorithm
    that all state-action pairs can be tabulated. Do you think this would still work
    well if we had millions of states and thousands of actions?
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的问题是问问自己，我们使用的算法中有什么假设？例如，该算法明确要求所有状态-动作对都可以被制表。如果我们有数百万个状态和数千个动作，你认为它仍然会表现良好吗？
- en: Building a Distributed Ray App
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建一个分布式 Ray 应用程序。
- en: 'Let’s take a step back here. If you’re an RL expert, you’ll know what we’ve
    been doing the whole time. If you’re completely new to RL, you might just be a
    little overwhelmed. If you’re somewhere in between, you hopefully like the example
    but might be wondering how what we’ve done so far relates to Ray. That’s a great
    question. As you’ll see shortly, all we need to make the above RL experiment a
    distributed Ray app is writing three short code snippets. This is what we’re going
    to do:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在这里退一步。如果你是一个强化学习专家，你会知道我们一直在做什么。如果你完全是强化学习的新手，你可能会感到有些不知所措。如果你处于中间某个位置，你可能会喜欢这个例子，但可能会想知道我们到目前为止所做的与
    Ray 有什么关系。这是一个很好的问题。很快你就会看到，为了使上述强化学习实验成为一个分布式 Ray 应用程序，我们只需要编写三个简短的代码片段。这就是我们要做的：
- en: We create a Ray task that can initialize a `Policy` remotely.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们创建了一个 Ray 任务，可以远程初始化一个 `Policy`。
- en: Then we make the `Simulation` a Ray actor in just a few lines of code.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们只需几行代码将 `Simulation` 设为 Ray actor。
- en: After that we wrap the `update_policy` function in a Ray task.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们将 `update_policy` 函数封装在一个 Ray 任务中。
- en: Finally, we define a parallel version of `train_policy` that’s structurally
    identical to its original version.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们定义了一个结构与原始版本完全相同的并行版本的 `train_policy`。
- en: 'Let’s tackle the first two steps of this plan by implementing a `create_policy`
    task and a Ray actor called `SimulationActor`:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过实现一个 `create_policy` 任务和一个名为 `SimulationActor` 的 Ray actor 来解决这个计划的前两个步骤：
- en: Example 3-13\.
  id: totrans-165
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-13。
- en: '[PRE15]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[![1](assets/1.png)](#co_building_your_first_distributed_application_CO13-1)'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_building_your_first_distributed_application_CO13-1)'
- en: After initializing it, we put our `environment` into the Ray object store.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化后，我们将我们的 `environment` 放入 Ray 对象存储。
- en: '[![2](assets/2.png)](#co_building_your_first_distributed_application_CO13-2)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_building_your_first_distributed_application_CO13-2)'
- en: This remote task returns a new `Policy` object.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这个远程任务返回一个新的 `Policy` 对象。
- en: '[![3](assets/3.png)](#co_building_your_first_distributed_application_CO13-3)'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_building_your_first_distributed_application_CO13-3)'
- en: This Ray actor wraps our `Simulation` class in a straightforward way.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 Ray actor 以一种简单直接的方式封装了我们的 `Simulation` 类。
- en: With the foundations on Ray Core you’ve developed in chapter [Chapter 2](ch02.xhtml#chapter_02)
    you should have no problems reading this code. It might take some getting used
    to writing it yourself, but conceptually you should be on top of this example.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在你在第 [第二章](ch02.xhtml#chapter_02) 中建立的 Ray Core 基础上，阅读这段代码应该不成问题。也许自己编写它可能需要一些适应时间，但在概念上，你应该掌握这个例子的要领。
- en: 'Moving on, let’s define a distributed `update_policy_task` Ray task and then
    wrap everything (two tasks and one actor) in a `train_policy_parallel` function
    that distributes this RL workload on your local Ray cluster:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们定义一个分布式的 `update_policy_task` Ray 任务，然后将所有内容（两个任务和一个 actor）封装在一个名为 `train_policy_parallel`
    的函数中，在你的本地 Ray 集群上分发这个强化学习工作负载：
- en: Example 3-14\.
  id: totrans-175
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-14。
- en: '[PRE16]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[![1](assets/1.png)](#co_building_your_first_distributed_application_CO14-1)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_building_your_first_distributed_application_CO14-1)'
- en: This task defers to the original `update_policy` function by passing a reference
    to a policy and experiences retrieved from the object store.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这个任务通过向 `update_policy` 函数传递从对象存储中检索到的策略和经验的引用，委托给原始函数。
- en: '[![2](assets/2.png)](#co_building_your_first_distributed_application_CO14-2)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_building_your_first_distributed_application_CO14-2)'
- en: To train in parallel, we first create a policy remotely, which returns a reference
    we call `policy`.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 为了并行训练，我们首先远程创建一个策略，返回一个我们称之为 `policy` 的引用。
- en: '[![3](assets/3.png)](#co_building_your_first_distributed_application_CO14-3)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_building_your_first_distributed_application_CO14-3)'
- en: Instead of one simulation, we create four simulation actors.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不是创建一个模拟，而是创建四个模拟执行者。
- en: '[![4](assets/4.png)](#co_building_your_first_distributed_application_CO14-4)'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_building_your_first_distributed_application_CO14-4)'
- en: Experiences now get collected from remote roll-outs on simulation actors.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们从远程模拟执行者收集经验。
- en: '[![5](assets/5.png)](#co_building_your_first_distributed_application_CO14-5)'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_building_your_first_distributed_application_CO14-5)'
- en: Then we can update our policy remotely. Note that `experiences` is a nested
    list of experiences.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以远程更新我们的策略。注意`experiences`是经验的嵌套列表。
- en: '[![6](assets/6.png)](#co_building_your_first_distributed_application_CO14-6)'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](assets/6.png)](#co_building_your_first_distributed_application_CO14-6)'
- en: Finally, we return the trained policy by retrieving it from the object store
    again.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们再次从对象存储中检索训练好的策略。
- en: This allows us to take the last step and run the training procedure in parallel
    and then evaluate the resulting as before.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得我们能够采取最后一步，以与之前相同的方式并行运行训练过程，然后评估结果。
- en: Example 3-15\.
  id: totrans-190
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-15。
- en: '[PRE17]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The result of those two lines is the same as before, when we ran the serial
    version of the RL training for the maze. I hope you appreciate how `train_policy_parallel`
    has the exact same high-level structure as `train_policy`. It’s a good exercise
    to compare the two line-by-line. Essentially, all it took to parallelize the training
    process was to use the `ray.remote` decorator three times in a suitable way. Of
    course, you need some experience to get this right. But notice how little time
    we spent on thinking about distributed computing, and how much time we could spend
    on the actual application code. We didn’t need to adopt an entirely new programming
    paradigm and could simply approach the problem in the most natural way. Ultimately,
    that’s what you want — and Ray is great at giving you this kind of flexibility.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这两行的结果与我们在迷宫RL训练的串行版本运行时相同。希望您能欣赏`train_policy_parallel`与`train_policy`具有完全相同的高级结构。逐行比较这两者是一个很好的练习。基本上，将训练过程并行化所需的全部工作就是以适当的方式三次使用`ray.remote`装饰器。当然，您需要一些经验才能做到这一点。但请注意，我们几乎没有花费时间思考分布式计算，而是将更多时间花在实际的应用代码上。我们无需采用全新的编程范式，只需以最自然的方式解决问题。最终，这才是您想要的——而Ray非常擅长提供这种灵活性。
- en: 'To wrap things up, let’s have a quick look at the task execution graph of the
    Ray application that we’ve just built. To recap, what we did was:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们快速查看我们刚刚构建的Ray应用程序的任务执行图。总结一下，我们所做的是：
- en: The `train_policy_parallel` function creates several `SimulationActor` actors
    and a policy with `create_policy`
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_policy_parallel`函数创建了多个`SimulationActor`执行者和一个`create_policy`策略。'
- en: The simulation actors create roll-outs with the policy and thereby collect experiences
    that `update_policy_task` uses to update the policy.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模拟执行者使用策略创建回合，并收集用于更新策略的经验，`update_policy_task`利用这些经验来更新策略。
- en: This works, because of the way updating the policy is designed. It doesn’t matter
    if the experiences were collected by one or multiple simulations.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是可行的，因为策略更新的方式设计得很好。不管经验是由一个模拟还是多个模拟收集的，都没有关系。
- en: The rolling out and updating continues until we reached the number of episodes
    we wante to train for, then the final `trained_policy` is returned.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行回合和更新操作，直到达到我们希望训练的轮数，然后返回最终的`trained_policy`。
- en: 'Figure [Figure 3-1](#fig_ray_train_policy) summarizes this task graph in a
    compact way:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图[图 3-1](#fig_ray_train_policy)以一种紧凑的方式总结了此任务图：
- en: '![Ray Training](assets/train_policy.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![Ray 训练](assets/train_policy.png)'
- en: Figure 3-1\. Parallel training of a reinforcement learning policy with Ray
  id: totrans-200
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-1。使用Ray并行训练强化学习策略。
- en: An interesting side note about the running example of this chapter is that it’s
    an implementation of the pseudo-code example used to illustrate the flexibility
    of Ray in [the initial paper](https://arxiv.org/abs/1712.05889) by its creators.
    That paper has a figure similar to [Figure 3-1](#fig_ray_train_policy) and is
    worth reading for context.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的运行示例有一个有趣的旁注，即它是[其创作者在初始论文](https://arxiv.org/abs/1712.05889)中用来展示Ray灵活性的伪代码实现。该论文有一张类似于[图
    3-1](#fig_ray_train_policy)的图表，值得阅读以获取背景知识。
- en: Recapping RL Terminology
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回顾强化学习术语。
- en: Before we wrap up this chapter, let’s discuss the concepts we’ve encountered
    in the maze example in a broader context. Doing so will prepare you for more complex
    RL settings in the next chapter and show you where we simplified things a little
    for the running example of this chapter.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们结束本章之前，让我们在更广泛的背景下讨论我们在迷宫示例中遇到的概念。这样做将为您准备好在下一章中更复杂的强化学习设置，并向您展示我们为本章的运行示例简化了哪些内容。
- en: Every RL problem starts with the formulation of an *environment*, which describes
    the dynamics of the “game” you want to play. The environment hosts a player or
    *agent* which interacts with its environment through a simple interface. The agent
    can request information from the environment, namely its current *state* within
    the environment, the *reward* it has received in this state, and whether the game
    is *done* or not. In observing states and rewards, the agent can learn to make
    decisions based on the information it receives. Specifically, the agent will emit
    an *action* that can be executed by the environment by taking the next `step`.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 每个强化学习问题都从制定*环境*的形式开始，该环境描述了您想要进行“游戏”的动态。环境托管一个玩家或*代理*，通过简单的接口与其环境进行交互。代理可以请求来自环境的信息，即其在环境中的当前*状态*、在此状态下收到的*奖励*以及游戏是否*完成*。通过观察状态和奖励，代理可以学习基于其接收到的信息做出决策。具体而言，代理将发出一个*动作*，环境可以通过采取下一步来执行该动作。
- en: The mechanism used by an agent to produce actions for a given state is called
    a *policy*, and we will sometimes say that the agent follows a given policy. Given
    a policy, we can simulate or *roll-out* a few steps or an entire game using said
    policy. During a roll-out we can collect *experiences*, which we collect information
    about the current state and reward, the next action and the resulting state. An
    entire sequence of steps from start to finish is referred to as an *episode*,
    and the environment can be `reset` to its initial state to start a new episode.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 代理用于在给定状态下生成动作的机制称为*策略*，有时我们会说代理遵循特定策略。给定一个策略，我们可以使用该策略模拟或*展开*几步或整个游戏。在展开过程中，我们可以收集*经验*，这些经验包括当前状态和奖励、下一步动作以及结果状态的信息。从开始到结束的整个步骤序列称为*回合*，环境可以通过将其重置到初始状态来开始新的回合。
- en: The policy we used in this chapter was based on the simple idea of tabulating
    *state-action values* (also called *Q-values*), and the algorithm used to update
    the policy from the experiences collected during roll-outs is called *Q-learning*.
    More generally, you can consider the state-action table we implemented as the
    *model* used by the policy. In the next chapter you will see examples of more
    complex models, such as a neural network to learn state-action values. The policy
    can decide to *exploit* what it has learnt about the environment by choosing the
    best available value of its model, or *explore* the environment by choosing a
    random action.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中使用的策略基于简单的*状态-动作值*（也称为*Q值*）制表的理念，并且用于根据回合收集的经验更新策略的算法称为*Q学习*。更普遍地说，您可以将我们实施的状态-动作表视为策略使用的*模型*。在下一章中，您将看到更复杂的模型示例，例如神经网络用于学习状态-动作值。策略可以决定*利用*它对环境学到的内容，选择其模型的最佳可用值，或者通过选择随机动作*探索*环境。
- en: Many of the basic concepts introduced here hold for any RL problem, but we’ve
    made a few simplifying assumptions. For instance, there could be *multiple agents*
    acting in the environment (imagine having multiple seekers competing for reaching
    the goal first), and we’ll look into so-called multi-agent environments and multi-agent
    RL and in the next chapter. Also, we assumed that the *action space* of an agent
    was *discrete*, meaning that the agent could only take a fixed set of actions.
    You can, of course, also have *continuous* action spaces, and the pendulum example
    from [Chapter 1](ch01.xhtml#chapter_01) is one example of this. Especially when
    you have multiple agents, action spaces can be more complicated, and you might
    need tuples of actions or even nest them accordingly. The *observation space*
    we’ve considered for the maze game was also quite simple, and was modeled as a
    discrete set of states. You can easily imagine that complex agents like robots
    interacting with their environments might work with image or video data as observations,
    which would require a more complex observation space, too.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这里介绍的许多基本概念适用于任何RL问题，但我们做了一些简化的假设。例如，环境中可能存在*多个代理*（想象一下有多个寻找者竞争首先达到目标的情况），我们将在下一章中研究所谓的多代理环境和多代理RL。此外，我们假设代理的*action
    space*是*discrete*的，这意味着代理只能采取固定的一组行动。当然，您也可以有*continuous*的动作空间，第一章中的摆锤示例就是其中的一个例子。特别是当您有多个代理时，动作空间可能会更复杂，您可能需要动作元组，甚至根据情况进行嵌套。我们为迷宫游戏考虑的*observation
    space*也相当简单，被建模为一组离散状态。您可以很容易想象，像机器人这样与其环境互动的复杂代理可能会使用图像或视频数据作为观测值，这也需要一个更复杂的观测空间。
- en: Another crucial assumption we made is that the environment is *deterministic*,
    meaning that when our agent chose to take an action, the resulting state would
    always reflect that choice. In general environments this is not the case, and
    there can be elements of randomness at play in the environment. For instance,
    we could have implemented a coin flip in the maze game and whenever tails came
    up, the agent would get pushed in a random direction. In that scenario, we couldn’t
    have planned ahead like we did in this chapter, as actions would not deterministically
    lead to the same next state every time. To reflect this probabilistic behavior,
    in general we have to account for *state transition probabilities* in our RL experiments.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所做的另一个关键假设是环境是*deterministic*的，这意味着当我们的代理选择采取行动时，结果的状态总是反映出那个选择。在一般环境中，情况并非如此，环境中可能存在随机因素。例如，在迷宫游戏中，我们可以实现一个硬币翻转，每当出现反面时，代理就会被随机推向一个方向。在这种情况下，我们不能像本章中那样事先规划，因为行动不会确定地导致每次都是相同的下一个状态。为了反映这种概率行为，在一般情况下，我们必须考虑我们的RL实验中的*state
    transition probabilities*。
- en: And the last simplifying assumption I’d like to talk about here is that we’ve
    been treating the environment and its dynamics as a game that can be perfectly
    simulated. But the fact is that there are physical systems that can’t be faithfully
    simulated. In that case you might still interact with this physical environment
    through an interface like the one we defined in our `Environment` class, but there
    would be some communication overhead involved. In practice, I find that *reasoning*
    about RL problems as if they were games takes very little away from the experience.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我想在这里谈论的最后一个简化假设是，我们一直把环境及其动态视为可以完美模拟的游戏。但事实是，有些物理系统无法忠实地模拟。在这种情况下，您可能仍然可以通过像我们在我们的`Environment`类中定义的接口与这个物理环境交互，但会涉及一些通信开销。实际上，我发现将RL问题视为游戏来推理几乎没有损害体验的感觉。
- en: Summary
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: To recap, we’ve implemented a simple maze problem in plain Python and then solved
    the task of finding the goal in that maze using a straightforward reinforcement
    learning algorithm. We then took this solution and ported it to a distributed
    Ray application in roughly 25 lines of code. We did so without having to plan
    how to work with Ray — we simply used the Ray API to parallelize our Python code.
    This example shows how Ray gets out of your way and lets you focus on your application
    code. It also demonstrates how custom workloads that use advanced techniques like
    RL can be efficiently implemented and distributed with Ray.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们在纯Python中实现了一个简单的迷宫问题，然后使用简单的强化学习算法解决了在该迷宫中找到目标的任务。然后，我们将这个解决方案移植到一个大致只有25行代码的分布式Ray应用程序中。在这个过程中，我们无需规划如何使用Ray，只需简单地使用Ray
    API来并行化我们的Python代码。这个例子展示了Ray如何让你专注于应用程序代码，而不必考虑与Ray的交互。它还展示了如何高效地实现和分布使用RL等高级技术的定制工作负载。
- en: In the next chapter, you’ll build on what you’ve learned here and see how easy
    it is to solve our maze problem directly with the higher-level Ray RLlib library.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将基于所学内容继续构建，并看到使用更高级别的Ray RLlib库直接解决迷宫问题是多么容易。
- en: ^([1](ch03.xhtml#idm44990031607120-marker)) As we’ll see in chapter [Chapter 4](ch04.xhtml#chapter_04),
    you can run RL on multi-player games, too. Making the maze environment a so-called
    multi-agent environment, in which multiple seekers compete for the goal, is an
    interesting exercise.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch03.xhtml#idm44990031607120-marker)) 正如我们将在[第四章](ch04.xhtml#chapter_04)中看到的那样，你也可以在多人游戏中运行强化学习。将迷宫环境变成所谓的多智能体环境，其中多个搜索者竞争目标，是一个有趣的练习。
