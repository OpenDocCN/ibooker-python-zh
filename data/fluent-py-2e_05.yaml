- en: Chapter 4\. Unicode Text Versus Bytes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章。Unicode文本与字节
- en: Humans use text. Computers speak bytes.
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 人类使用文本。计算机使用字节。
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Esther Nam and Travis Fischer, “Character Encoding and Unicode in Python”^([1](ch04.html#idm46582490670096))
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Esther Nam和Travis Fischer，“Python中的字符编码和Unicode”^([1](ch04.html#idm46582490670096))
- en: Python 3 introduced a sharp distinction between strings of human text and sequences
    of raw bytes. Implicit conversion of byte sequences to Unicode text is a thing
    of the past. This chapter deals with Unicode strings, binary sequences, and the
    encodings used to convert between them.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Python 3引入了人类文本字符串和原始字节序列之间的明显区别。将字节序列隐式转换为Unicode文本已经成为过去。本章涉及Unicode字符串、二进制序列以及用于在它们之间转换的编码。
- en: Depending on the kind of work you do with Python, you may think that understanding
    Unicode is not important. That’s unlikely, but anyway there is no escaping the
    `str` versus `byte` divide. As a bonus, you’ll find that the specialized binary
    sequence types provide features that the “all-purpose” Python 2 `str` type did
    not have.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您在Python中的工作类型，您可能认为理解Unicode并不重要。这不太可能，但无论如何，无法避免`str`与`byte`之间的分歧。作为奖励，您会发现专门的二进制序列类型提供了Python
    2通用`str`类型没有的功能。
- en: 'In this chapter, we will visit the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主题：
- en: Characters, code points, and byte representations
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字符、代码点和字节表示
- en: 'Unique features of binary sequences: `bytes`, `bytearray`, and `memoryview`'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二进制序列的独特特性：`bytes`、`bytearray`和`memoryview`
- en: Encodings for full Unicode and legacy character sets
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完整Unicode和传统字符集的编码
- en: Avoiding and dealing with encoding errors
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免和处理编码错误
- en: Best practices when handling text files
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理文本文件时的最佳实践
- en: The default encoding trap and standard I/O issues
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认编码陷阱和标准I/O问题
- en: Safe Unicode text comparisons with normalization
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用规范化进行安全的Unicode文本比较
- en: Utility functions for normalization, case folding, and brute-force diacritic
    removal
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于规范化、大小写折叠和强制去除变音符号的实用函数
- en: Proper sorting of Unicode text with `locale` and the *pyuca* library
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`locale`和*pyuca*库正确对Unicode文本进行排序
- en: Character metadata in the Unicode database
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Unicode数据库中的字符元数据
- en: Dual-mode APIs that handle `str` and `bytes`
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理`str`和`bytes`的双模式API
- en: What’s New in This Chapter
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本章新内容
- en: Support for Unicode in Python 3 has been comprehensive and stable, so the most
    notable addition is [“Finding Characters by Name”](#finding_chars_sec), describing
    a utility for searching the Unicode database—a great way to find circled digits
    and smiling cats from the command line.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Python 3中对Unicode的支持是全面且稳定的，因此最值得注意的新增内容是[“按名称查找字符”](#finding_chars_sec)，描述了一种用于搜索Unicode数据库的实用程序——这是从命令行查找带圈数字和微笑猫的好方法。
- en: One minor change worth mentioning is the Unicode support on Windows, which is
    better and simpler since Python 3.6, as we’ll see in [“Beware of Encoding Defaults”](#encoding_defaults).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的一项较小更改是关于Windows上的Unicode支持，自Python 3.6以来更好且更简单，我们将在[“注意编码默认值”](#encoding_defaults)中看到。
- en: Let’s start with the not-so-new, but fundamental concepts of characters, code
    points, and bytes.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从不那么新颖但基础的概念开始，即字符、代码点和字节。
- en: Note
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: For the second edition, I expanded the section about the `struct` module and
    published it online at [“Parsing binary records with struct”](https://fpy.li/4-3),
    in the [*fluentpython.com*](http://fluentpython.com) companion website.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第二版，我扩展了关于`struct`模块的部分，并在[*fluentpython.com*](http://fluentpython.com)的伴随网站上发布了在线版本[“使用struct解析二进制记录”](https://fpy.li/4-3)。
- en: There you will also find [“Building Multi-character Emojis”](https://fpy.li/4-4),
    describing how to make country flags, rainbow flags, people with different skin
    tones, and diverse family icons by combining Unicode characters.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在那里，您还会发现[“构建多字符表情符号”](https://fpy.li/4-4)，描述如何通过组合Unicode字符制作国旗、彩虹旗、不同肤色的人以及多样化家庭图标。
- en: Character Issues
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 字符问题
- en: 'The concept of “string” is simple enough: a string is a sequence of characters.
    The problem lies in the definition of “character.”'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: “字符串”的概念足够简单：字符串是字符序列。问题在于“字符”的定义。
- en: In 2021, the best definition of “character” we have is a Unicode character.
    Accordingly, the items we get out of a Python 3 `str` are Unicode characters,
    just like the items of a `unicode` object in Python 2—and not the raw bytes we
    got from a Python 2 `str`.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在2021年，我们对“字符”的最佳定义是Unicode字符。因此，我们从Python 3的`str`中获取的项目是Unicode字符，就像在Python
    2中的`unicode`对象中获取的项目一样——而不是从Python 2的`str`中获取的原始字节。
- en: 'The Unicode standard explicitly separates the identity of characters from specific
    byte representations:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Unicode标准明确将字符的身份与特定字节表示分开：
- en: The identity of a character—its *code point*—is a number from 0 to 1,114,111
    (base 10), shown in the Unicode standard as 4 to 6 hex digits with a “U+” prefix,
    from U+0000 to U+10FFFF. For example, the code point for the letter A is U+0041,
    the Euro sign is U+20AC, and the musical symbol G clef is assigned to code point
    U+1D11E. About 13% of the valid code points have characters assigned to them in
    Unicode 13.0.0, the standard used in Python 3.10.0b4.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '字符的身份——其*代码点*——是从0到1,114,111（十进制）的数字，在Unicode标准中显示为带有“U+”前缀的4到6位十六进制数字，从U+0000到U+10FFFF。例如，字母A的代码点是U+0041，欧元符号是U+20AC，音乐符号G谱号分配给代码点U+1D11E。在Unicode
    13.0.0中，约13%的有效代码点有字符分配给它们，这是Python 3.10.0b4中使用的标准。 '
- en: 'The actual bytes that represent a character depend on the *encoding* in use.
    An encoding is an algorithm that converts code points to byte sequences and vice
    versa. The code point for the letter A (U+0041) is encoded as the single byte
    `\x41` in the UTF-8 encoding, or as the bytes `\x41\x00` in UTF-16LE encoding.
    As another example, UTF-8 requires three bytes—`\xe2\x82\xac`—to encode the Euro
    sign (U+20AC), but in UTF-16LE the same code point is encoded as two bytes: `\xac\x20`.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表示字符的实际字节取决于正在使用的*编码*。编码是一种将代码点转换为字节序列及其反向转换的算法。字母A（U+0041）的代码点在UTF-8编码中被编码为单个字节`\x41`，在UTF-16LE编码中被编码为两个字节`\x41\x00`。另一个例子，UTF-8需要三个字节—`\xe2\x82\xac`—来编码欧元符号（U+20AC），但在UTF-16LE中，相同的代码点被编码为两个字节：`\xac\x20`。
- en: Converting from code points to bytes is *encoding*; converting from bytes to
    code points is *decoding*. See [Example 4-1](#ex_encode_decode).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 从代码点转换为字节是*编码*；从字节转换为代码点是*解码*。参见 [示例 4-1](#ex_encode_decode)。
- en: Example 4-1\. Encoding and decoding
  id: totrans-32
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-1\. 编码和解码
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[![1](assets/1.png)](#co_unicode_text_versus_bytes_CO1-1)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_unicode_text_versus_bytes_CO1-1)'
- en: The `str` `'café'` has four Unicode characters.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '`str` `''café''` 有四个 Unicode 字符。'
- en: '[![2](assets/2.png)](#co_unicode_text_versus_bytes_CO1-2)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_unicode_text_versus_bytes_CO1-2)'
- en: Encode `str` to `bytes` using UTF-8 encoding.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 UTF-8 编码将 `str` 编码为 `bytes`。
- en: '[![3](assets/3.png)](#co_unicode_text_versus_bytes_CO1-3)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_unicode_text_versus_bytes_CO1-3)'
- en: '`bytes` literals have a `b` prefix.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`bytes` 字面量有一个 `b` 前缀。'
- en: '[![4](assets/4.png)](#co_unicode_text_versus_bytes_CO1-4)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_unicode_text_versus_bytes_CO1-4)'
- en: '`bytes` `b` has five bytes (the code point for “é” is encoded as two bytes
    in UTF-8).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`bytes` `b` 有五个字节（“é”的代码点在 UTF-8 中编码为两个字节）。'
- en: '[![5](assets/5.png)](#co_unicode_text_versus_bytes_CO1-5)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_unicode_text_versus_bytes_CO1-5)'
- en: Decode `bytes` to `str` using UTF-8 encoding.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 UTF-8 编码将 `bytes` 解码为 `str`。
- en: Tip
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If you need a memory aid to help distinguish `.decode()` from `.encode()`, convince
    yourself that byte sequences can be cryptic machine core dumps, while Unicode
    `str` objects are “human” text. Therefore, it makes sense that we *decode* `bytes`
    to `str` to get human-readable text, and we *encode* `str` to `bytes` for storage
    or transmission.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要一个记忆辅助来帮助区分 `.decode()` 和 `.encode()`，说服自己字节序列可以是晦涩的机器核心转储，而 Unicode `str`
    对象是“人类”文本。因此，将 `bytes` *解码* 为 `str` 以获取可读文本是有意义的，而将 `str` *编���* 为 `bytes` 用于存储或传输也是有意义的。
- en: Although the Python 3 `str` is pretty much the Python 2 `unicode` type with
    a new name, the Python 3 `bytes` is not simply the old `str` renamed, and there
    is also the closely related `bytearray` type. So it is worthwhile to take a look
    at the binary sequence types before advancing to encoding/decoding issues.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Python 3 的 `str` 在很大程度上就是 Python 2 的 `unicode` 类型换了个新名字，但 Python 3 的 `bytes`
    并不仅仅是旧的 `str` 更名，还有与之密切相关的 `bytearray` 类型。因此，在进入编码/解码问题之前，值得看一看二进制序列类型。
- en: Byte Essentials
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 字节要点
- en: 'The new binary sequence types are unlike the Python 2 `str` in many regards.
    The first thing to know is that there are two basic built-in types for binary
    sequences: the immutable `bytes` type introduced in Python 3 and the mutable `bytearray`,
    added way back in Python 2.6.^([2](ch04.html#idm46582490431168)) The Python documentation
    sometimes uses the generic term “byte string” to refer to both `bytes` and `bytearray`.
    I avoid that confusing term.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 新的二进制序列类型在许多方面与 Python 2 的 `str` 不同。首先要知道的是，有两种基本的内置二进制序列类型：Python 3 中引入的不可变
    `bytes` 类型和早在 Python 2.6 中添加的可变 `bytearray`。^([2](ch04.html#idm46582490431168))
    Python 文档有时使用通用术语“字节字符串”来指代 `bytes` 和 `bytearray`。我避免使用这个令人困惑的术语。
- en: Each item in `bytes` or `bytearray` is an integer from 0 to 255, and not a one-character
    string like in the Python 2 `str`. However, a slice of a binary sequence always
    produces a binary sequence of the same type—including slices of length 1\. See
    [Example 4-2](#ex_bytes_bytearray).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`bytes` 或 `bytearray` 中的每个项都是从 0 到 255 的整数，而不是像 Python 2 的 `str` 中的单个字符字符串。然而，二进制序列的切片始终产生相同类型的二进制序列，包括长度为
    1 的切片。参见 [示例 4-2](#ex_bytes_bytearray)。'
- en: Example 4-2\. A five-byte sequence as `bytes` and as `bytearray`
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-2\. 作为 `bytes` 和 `bytearray` 的五字节序列
- en: '[PRE1]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[![1](assets/1.png)](#co_unicode_text_versus_bytes_CO2-1)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_unicode_text_versus_bytes_CO2-1)'
- en: '`bytes` can be built from a `str`, given an encoding.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 可以从 `str` 构建 `bytes`，并给定一个编码。
- en: '[![2](assets/2.png)](#co_unicode_text_versus_bytes_CO2-2)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_unicode_text_versus_bytes_CO2-2)'
- en: Each item is an integer in `range(256)`.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 每个项都是 `range(256)` 中的整数。
- en: '[![3](assets/3.png)](#co_unicode_text_versus_bytes_CO2-3)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_unicode_text_versus_bytes_CO2-3)'
- en: Slices of `bytes` are also `bytes`—even slices of a single byte.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`bytes` 的切片也是 `bytes` ——即使是单个字节的切片。'
- en: '[![4](assets/4.png)](#co_unicode_text_versus_bytes_CO2-4)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_unicode_text_versus_bytes_CO2-4)'
- en: 'There is no literal syntax for `bytearray`: they are shown as `bytearray()`
    with a `bytes` literal as argument.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '`bytearray` 没有字面量语法：它们显示为带有 `bytes` 字面量作为参数的 `bytearray()`。'
- en: '[![5](assets/5.png)](#co_unicode_text_versus_bytes_CO2-5)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_unicode_text_versus_bytes_CO2-5)'
- en: A slice of `bytearray` is also a `bytearray`.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`bytearray` 的切片也是 `bytearray`。'
- en: Warning
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: The fact that `my_bytes[0]` retrieves an `int` but `my_bytes[:1]` returns a
    `bytes` sequence of length 1 is only surprising because we are used to Python’s
    `str` type, where `s[0] == s[:1]`. For all other sequence types in Python, 1 item
    is not the same as a slice of length 1.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`my_bytes[0]` 检索一个 `int`，但 `my_bytes[:1]` 返回长度为 1 的 `bytes` 序列，这只是因为我们习惯于 Python
    的 `str` 类型，其中 `s[0] == s[:1]`。对于 Python 中的所有其他序列类型，1 项不等于长度为 1 的切片。'
- en: 'Although binary sequences are really sequences of integers, their literal notation
    reflects the fact that ASCII text is often embedded in them. Therefore, four different
    displays are used, depending on each byte value:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管二进制序列实际上是整数序列，但它们的字面值表示反映了 ASCII 文本经常嵌入其中的事实。因此，根据每个字节值的不同，使用四种不同的显示方式：
- en: For bytes with decimal codes 32 to 126—from space to `~` (tilde)—the ASCII character
    itself is used.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于十进制代码为 32 到 126 的字节——从空格到 `~`（波浪号）——使用 ASCII 字符本身。
- en: For bytes corresponding to tab, newline, carriage return, and `\`, the escape
    sequences `\t`, `\n`, `\r`, and `\\` are used.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于制表符、换行符、回车符和 `\` 对应的字节，使用转义序列 `\t`、`\n`、`\r` 和 `\\`。
- en: If both string delimiters `'` and `"` appear in the byte sequence, the whole
    sequence is delimited by `'`, and any `'` inside are escaped as `\'`.^([3](ch04.html#idm46582490297184))
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果字节序列中同时出现字符串定界符 `'` 和 `"`，则整个序列由 `'` 定界，并且任何 `'` 都会被转义为 `\'`。^([3](ch04.html#idm46582490297184))
- en: For other byte values, a hexadecimal escape sequence is used (e.g., `\x00` is
    the null byte).
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于其他字节值，使用十六进制转义序列（例如，`\x00` 是空字节）。
- en: 'That is why in [Example 4-2](#ex_bytes_bytearray) you see `b''caf\xc3\xa9''`:
    the first three bytes `b''caf''` are in the printable ASCII range, the last two
    are not.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么在 [示例 4-2](#ex_bytes_bytearray) 中你会看到 `b'caf\xc3\xa9'`：前三个字节 `b'caf'`
    在可打印的 ASCII 范围内，而最后两个不在范围内。
- en: Both `bytes` and `bytearray` support every `str` method except those that do
    formatting (`format`, `format_map`) and those that depend on Unicode data, including
    `casefold`, `isdecimal`, `isidentifier`, `isnumeric`, `isprintable`, and `encode`.
    This means that you can use familiar string methods like `endswith`, `replace`,
    `strip`, `translate`, `upper`, and dozens of others with binary sequences—only
    using `bytes` and not `str` arguments. In addition, the regular expression functions
    in the `re` module also work on binary sequences, if the regex is compiled from
    a binary sequence instead of a `str`. Since Python 3.5, the `%` operator works
    with binary sequences again.^([4](ch04.html#idm46582490281616))
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`bytes`和`bytearray`都支持除了依赖于Unicode数据的格式化方法（`format`，`format_map`）和那些依赖于Unicode数据的方法（包括`casefold`，`isdecimal`，`isidentifier`，`isnumeric`，`isprintable`和`encode`）之外的所有`str`方法。这意味着您可以使用熟悉的字符串方法，如`endswith`，`replace`，`strip`，`translate`，`upper`等，与二进制序列一起使用——只使用`bytes`而不是`str`参数。此外，如果正则表达式是从二进制序列而不是`str`编译而成，则`re`模块中的正则表达式函数也适用于二进制序列。自Python
    3.5以来，`%`运算符再次适用于二进制序列。^([4](ch04.html#idm46582490281616))'
- en: 'Binary sequences have a class method that `str` doesn’t have, called `fromhex`,
    which builds a binary sequence by parsing pairs of hex digits optionally separated
    by spaces:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 二进制序列有一个`str`没有的类方法，称为`fromhex`，它通过解析以空格分隔的十六进制数字对构建二进制序列：
- en: '[PRE2]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The other ways of building `bytes` or `bytearray` instances are calling their
    constructors with:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 构建`bytes`或`bytearray`实例的其他方法是使用它们的构造函数，并提供：
- en: A `str` and an `encoding` keyword argument
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个`str`和一个`encoding`关键字参数
- en: An iterable providing items with values from 0 to 255
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个可提供值从0到255的项目的可迭代对象
- en: An object that implements the buffer protocol (e.g., `bytes`, `bytearray`, `memoryview`,
    `array.array`) that copies the bytes from the source object to the newly created
    binary sequence
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个实现缓冲区协议的对象（例如，`bytes`，`bytearray`，`memoryview`，`array.array`），它将源对象的字节复制到新创建的二进制序列中
- en: Warning
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Until Python 3.5, it was also possible to call `bytes` or `bytearray` with a
    single integer to create a binary sequence of that size initialized with null
    bytes. This signature was deprecated in Python 3.5 and removed in Python 3.6.
    See [PEP 467—Minor API improvements for binary sequences](https://fpy.li/pep467).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 直到Python 3.5，还可以使用单个整数调用`bytes`或`bytearray`来创建一个以空字节初始化的该大小的二进制序列。这个签名在Python
    3.5中被弃用，并在Python 3.6中被移除。请参阅[PEP 467—二进制序列的次要API改进](https://fpy.li/pep467)。
- en: Building a binary sequence from a buffer-like object is a low-level operation
    that may involve type casting. See a demonstration in [Example 4-3](#ex_buffer_demo).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 从类似缓冲区的对象构建二进制序列是一个涉及类型转换的低级操作。在[示例4-3](#ex_buffer_demo)中看到演示。
- en: Example 4-3\. Initializing bytes from the raw data of an array
  id: totrans-80
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例4-3。从数组的原始数据初始化字节
- en: '[PRE3]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[![1](assets/1.png)](#co_unicode_text_versus_bytes_CO3-1)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_unicode_text_versus_bytes_CO3-1)'
- en: Typecode `'h'` creates an `array` of short integers (16 bits).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 类型码`'h'`创建一个短整数（16位）的`array`。
- en: '[![2](assets/2.png)](#co_unicode_text_versus_bytes_CO3-2)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_unicode_text_versus_bytes_CO3-2)'
- en: '`octets` holds a copy of the bytes that make up `numbers`.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`octets`保存构成`numbers`的字节的副本。'
- en: '[![3](assets/3.png)](#co_unicode_text_versus_bytes_CO3-3)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_unicode_text_versus_bytes_CO3-3)'
- en: These are the 10 bytes that represent the 5 short integers.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这是代表5个短整数的10个字节。
- en: Creating a `bytes` or `bytearray` object from any buffer-like source will always
    copy the bytes. In contrast, `memoryview` objects let you share memory between
    binary data structures, as we saw in [“Memory Views”](ch02.html#memoryview_sec).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 从任何类似缓冲区的源创建`bytes`或`bytearray`对象将始终复制字节。相反，`memoryview`对象允许您在二进制数据结构之间共享内存，正如我们在[“内存视图”](ch02.html#memoryview_sec)中看到的那样。
- en: After this basic exploration of binary sequence types in Python, let’s see how
    they are converted to/from strings.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这对Python中二进制序列类型的基本探索之后，让我们看看它们如何转换为/从字符串。
- en: Basic Encoders/Decoders
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本编码器/解码器
- en: The Python distribution bundles more than 100 *codecs* (encoder/decoders) for
    text to byte conversion and vice versa. Each codec has a name, like `'utf_8'`,
    and often aliases, such as `'utf8'`, `'utf-8'`, and `'U8'`, which you can use
    as the `encoding` argument in functions like `open()`, `str.encode()`, `bytes.decode()`,
    and so on. [Example 4-4](#ex_codecs) shows the same text encoded as three different
    byte sequences.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Python发行版捆绑了100多个*编解码器*（编码器/解码器），用于文本到字节的转换以及反之。每个编解码器都有一个名称，如`'utf_8'`，通常还有别名，如`'utf8'`，`'utf-8'`和`'U8'`，您可以将其用作函数中的`encoding`参数，如`open()`，`str.encode()`，`bytes.decode()`等。[示例4-4](#ex_codecs)展示了相同文本编码为三种不同的字节序列。
- en: Example 4-4\. The string “El Niño” encoded with three codecs producing very
    different byte sequences
  id: totrans-92
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例4-4。使用三种编解码器对字符串“El Niño”进行编码，生成非常不同的字节序列
- en: '[PRE4]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[Figure 4-1](#encodings_demo_fig) demonstrates a variety of codecs generating
    bytes from characters like the letter “A” through the G-clef musical symbol. Note
    that the last three encodings are variable-length, multibyte encodings.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-1](#encodings_demo_fig)展示了各种编解码器从字符（如字母“A”到G大调音符）生成字节的情况。请注意，最后三种编码是可变长度的多字节编码。'
- en: '![Encodings demonstration table](assets/flpy_0401.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![编码演示表](assets/flpy_0401.png)'
- en: Figure 4-1\. Twelve characters, their code points, and their byte representation
    (in hex) in 7 different encodings (asterisks indicate that the character cannot
    be represented in that encoding).
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-1。十二个字符，它们的代码点以及它们在7种不同编码中的字节表示（星号表示该字符无法在该编码中表示）。
- en: All those asterisks in [Figure 4-1](#encodings_demo_fig) make clear that some
    encodings, like ASCII and even the multibyte GB2312, cannot represent every Unicode
    character. The UTF encodings, however, are designed to handle every Unicode code
    point.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-1](#encodings_demo_fig)中所有那些星号清楚地表明，一些编码，如ASCII甚至多字节GB2312，无法表示每个Unicode字符。然而，UTF编码被设计用于处理每个Unicode代码点。'
- en: 'The encodings shown in [Figure 4-1](#encodings_demo_fig) were chosen as a representative
    sample:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图4-1](#encodings_demo_fig)中显示的编码被选为代表性样本：
- en: '`latin1` a.k.a. `iso8859_1`'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '`latin1`又称`iso8859_1`'
- en: Important because it is the basis for other encodings, such as `cp1252` and
    Unicode itself (note how the `latin1` byte values appear in the `cp1252` bytes
    and even in the code points).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 重要，因为它是其他编码的基础，例如`cp1252`和 Unicode 本身（注意`latin1`字节值如何出现在`cp1252`字节和代码点中）。
- en: '`cp1252`'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`cp1252`'
- en: A useful `latin1` superset created by Microsoft, adding useful symbols like
    curly quotes and € (euro); some Windows apps call it “ANSI,” but it was never
    a real ANSI standard.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 由 Microsoft 创建的有用的`latin1`超集，添加了诸如弯引号和€（欧元）等有用符号；一些 Windows 应用程序称其为“ANSI”，但它从未是真正的
    ANSI 标准。
- en: '`cp437`'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`cp437`'
- en: The original character set of the IBM PC, with box drawing characters. Incompatible
    with `latin1`, which appeared later.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: IBM PC 的原始字符集，带有绘制框线字符。与`latin1`不兼容，后者出现得更晚。
- en: '`gb2312`'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '`gb2312`'
- en: Legacy standard to encode the simplified Chinese ideographs used in mainland
    China; one of several widely deployed multibyte encodings for Asian languages.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 用于编码中国大陆使用的简体中文汉字的传统标准；亚洲语言的几种广泛部署的多字节编码之一。
- en: '`utf-8`'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`utf-8`'
- en: 'The most common 8-bit encoding on the web, by far, as of July 2021, [“W³Techs:
    Usage statistics of character encodings for websites”](https://fpy.li/4-5) claims
    that 97% of sites use UTF-8, up from 81.4% when I wrote this paragraph in the
    first edition of this book in September 2014.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 截至 2021 年 7 月，网络上最常见的 8 位编码远远是 UTF-8，[“W³Techs：网站字符编码使用统计”](https://fpy.li/4-5)声称
    97% 的网站使用 UTF-8，这比我在 2014 年 9 月第一版书中写这段话时的 81.4% 要高。
- en: '`utf-16le`'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`utf-16le`'
- en: One form of the UTF 16-bit encoding scheme; all UTF-16 encodings support code
    points beyond U+FFFF through escape sequences called “surrogate pairs.”
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: UTF 16 位编码方案的一种形式；所有 UTF-16 编码通过称为“代理对”的转义序列支持 U+FFFF 之上的代码点。
- en: Warning
  id: totrans-111
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: UTF-16 superseded the original 16-bit Unicode 1.0 encoding—UCS-2—way back in
    1996. UCS-2 is still used in many systems despite being deprecated since the last
    century because it only supports code points up to U+FFFF. As of 2021, more than
    57% of the allocated code points are above U+FFFF, including the all-important
    emojis.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: UTF-16 在 1996 年取代了原始的 16 位 Unicode 1.0 编码——UCS-2。尽管 UCS-2 自上个世纪以来已被弃用，但仍在许多系统中使用，因为它仅支持到
    U+FFFF 的代码点。截至 2021 年，超过 57% 的分配代码点在 U+FFFF 以上，包括所有重要的表情符号。
- en: With this overview of common encodings now complete, we move to handling issues
    in encoding and decoding operations.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在完成了对常见编码的概述，我们将转向处理编码和解码操作中的问题。
- en: Understanding Encode/Decode Problems
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解编码/解码问题
- en: 'Although there is a generic `UnicodeError` exception, the error reported by
    Python is usually more specific: either a `UnicodeEncodeError` (when converting
    `str` to binary sequences) or a `UnicodeDecodeError` (when reading binary sequences
    into `str`). Loading Python modules may also raise `SyntaxError` when the source
    encoding is unexpected. We’ll show how to handle all of these errors in the next
    sections.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在一个通用的`UnicodeError`异常，Python 报告的错误通常更具体：要么是`UnicodeEncodeError`（将`str`转换为二进制序列时），要么是`UnicodeDecodeError`（将二进制序列读入`str`时）。加载
    Python 模块时，如果源编码意外，则还可能引发`SyntaxError`。我们将在接下来的部分展示如何处理所有这些错误。
- en: Tip
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: The first thing to note when you get a Unicode error is the exact type of the
    exception. Is it a `UnicodeEncodeError`, a `UnicodeDecodeError`, or some other
    error (e.g., `SyntaxError`) that mentions an encoding problem? To solve the problem,
    you have to understand it first.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 当遇到 Unicode 错误时，首先要注意异常的确切类型。它是`UnicodeEncodeError`、`UnicodeDecodeError`，还是提到编码问题的其他错误（例如`SyntaxError`）？要解决问题，首先必须理解它。
- en: Coping with UnicodeEncodeError
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理 UnicodeEncodeError
- en: Most non-UTF codecs handle only a small subset of the Unicode characters. When
    converting text to bytes, if a character is not defined in the target encoding,
    `UnicodeEncodeError` will be raised, unless special handling is provided by passing
    an `errors` argument to the encoding method or function. The behavior of the error
    handlers is shown in [Example 4-5](#ex_encoding).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数非 UTF 编解码器仅处理 Unicode 字符的一小部分。将文本转换为字节时，如果目标编码中未定义字符，则会引发`UnicodeEncodeError`，除非通过向编码方法或函数传递`errors`参数提供了特殊处理。错误处理程序的行为显示在[示例
    4-5](#ex_encoding)中。
- en: 'Example 4-5\. Encoding to bytes: success and error handling'
  id: totrans-120
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-5\. 编码为字节：成功和错误处理
- en: '[PRE5]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[![1](assets/1.png)](#co_unicode_text_versus_bytes_CO4-1)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_unicode_text_versus_bytes_CO4-1)'
- en: The UTF encodings handle any `str`.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: UTF 编码处理任何`str`。
- en: '[![2](assets/2.png)](#co_unicode_text_versus_bytes_CO4-2)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_unicode_text_versus_bytes_CO4-2)'
- en: '`iso8859_1` also works for the `''São Paulo''` string.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`iso8859_1`也适用于`''São Paulo''`字符串。'
- en: '[![3](assets/3.png)](#co_unicode_text_versus_bytes_CO4-3)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_unicode_text_versus_bytes_CO4-3)'
- en: '`cp437` can’t encode the `''ã''` (“a” with tilde). The default error handler—`''strict''`—raises
    `UnicodeEncodeError`.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '`cp437` 无法编码`''ã''`（带有波浪符号的“a”）。默认错误处理程序`''strict''`会引发`UnicodeEncodeError`。'
- en: '[![4](assets/4.png)](#co_unicode_text_versus_bytes_CO4-4)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_unicode_text_versus_bytes_CO4-4)'
- en: The `error='ignore'` handler skips characters that cannot be encoded; this is
    usually a very bad idea, leading to silent data loss.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`error=''ignore''`处理程序跳过无法编码的字符；这通常是一个非常糟糕的主意，会导致数据悄悄丢失。'
- en: '[![5](assets/5.png)](#co_unicode_text_versus_bytes_CO4-5)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_unicode_text_versus_bytes_CO4-5)'
- en: When encoding, `error='replace'` substitutes unencodable characters with `'?'`;
    data is also lost, but users will get a clue that something is amiss.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在编码时，`error='replace'`用`'?'`替换无法编码的字符；数据也会丢失，但用户会得到提示有问题的线索。
- en: '[![6](assets/6.png)](#co_unicode_text_versus_bytes_CO4-6)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](assets/6.png)](#co_unicode_text_versus_bytes_CO4-6)'
- en: '`''xmlcharrefreplace''` replaces unencodable characters with an XML entity.
    If you can’t use UTF, and you can’t afford to lose data, this is the only option.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`''xmlcharrefreplace''`用 XML 实体替换无法编码的字符。如果不能使用 UTF，也不能承受数据丢失，这是唯一的选择。'
- en: Note
  id: totrans-134
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The `codecs` error handling is extensible. You may register extra strings for
    the `errors` argument by passing a name and an error handling function to the
    `codecs.register_error` function. See [the `codecs.register_error` documentation](https://fpy.li/4-6).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '`codecs`错误处理是可扩展的。您可以通过向`codecs.register_error`函数传递名称和错误处理函数来为`errors`参数注册额外的字符串。请参阅[the
    `codecs.register_error` documentation](https://fpy.li/4-6)。'
- en: ASCII is a common subset to all the encodings that I know about, therefore encoding
    should always work if the text is made exclusively of ASCII characters. Python
    3.7 added a new boolean method [`str.isascii()`](https://fpy.li/4-7) to check
    whether your Unicode text is 100% pure ASCII. If it is, you should be able to
    encode it to bytes in any encoding without raising `UnicodeEncodeError`.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ASCII 是我所知的所有编码的一个常见子集，因此如果文本完全由 ASCII 字符组成，编码应该总是有效的。Python 3.7 添加了一个新的布尔方法[`str.isascii()`](https://fpy.li/4-7)来检查您的
    Unicode 文本是否是 100% 纯 ASCII。如果是，您应该能够在任何编码中将其编码为字节，而不会引发`UnicodeEncodeError`。
- en: Coping with UnicodeDecodeError
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理 UnicodeDecodeError
- en: Not every byte holds a valid ASCII character, and not every byte sequence is
    valid UTF-8 or UTF-16; therefore, when you assume one of these encodings while
    converting a binary sequence to text, you will get a `UnicodeDecodeError` if unexpected
    bytes are found.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 并非每个字节都包含有效的 ASCII 字符，并非每个字节序列都是有效的 UTF-8 或 UTF-16；因此，当您在将二进制序列转换为文本时假定其中一个编码时，如果发现意外字节，则会收到`UnicodeDecodeError`。
- en: On the other hand, many legacy 8-bit encodings like `'cp1252'`, `'iso8859_1'`,
    and `'koi8_r'` are able to decode any stream of bytes, including random noise,
    without reporting errors. Therefore, if your program assumes the wrong 8-bit encoding,
    it will silently decode garbage.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，许多传统的 8 位编码，如`'cp1252'`、`'iso8859_1'`和`'koi8_r'`，能够解码任何字节流，包括随机噪音，而不报告错误。因此，如果您的程序假定了错误的
    8 位编码，它将悄悄地解码垃圾数据。
- en: Tip
  id: totrans-140
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Garbled characters are known as gremlins or mojibake (文字化け—Japanese for “transformed
    text”).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 乱码字符被称为 gremlins 或 mojibake（文字化け—日语中的“转换文本”）。
- en: '[Example 4-6](#ex_decoding) illustrates how using the wrong codec may produce
    gremlins or a `UnicodeDecodeError`.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[Example 4-6](#ex_decoding) 说明了使用错误的编解码器可能会产生乱码或`UnicodeDecodeError`。'
- en: 'Example 4-6\. Decoding from `str` to bytes: success and error handling'
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-6\. 从`str`解码为字节：成功和错误处理
- en: '[PRE6]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[![1](assets/1.png)](#co_unicode_text_versus_bytes_CO5-1)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_unicode_text_versus_bytes_CO5-1)'
- en: The word “Montréal” encoded as `latin1`; `'\xe9'` is the byte for “é”.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 编码为`latin1`的单词“Montréal”；`'\xe9'`是“é”的字节。
- en: '[![2](assets/2.png)](#co_unicode_text_versus_bytes_CO5-2)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_unicode_text_versus_bytes_CO5-2)'
- en: Decoding with Windows 1252 works because it is a superset of `latin1`.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Windows 1252 解码有效，因为它是`latin1`的超集。
- en: '[![3](assets/3.png)](#co_unicode_text_versus_bytes_CO5-3)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_unicode_text_versus_bytes_CO5-3)'
- en: ISO-8859-7 is intended for Greek, so the `'\xe9'` byte is misinterpreted, and
    no error is issued.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ISO-8859-7 用于希腊语，因此`'\xe9'`字节被错误解释，不会发出错误。
- en: '[![4](assets/4.png)](#co_unicode_text_versus_bytes_CO5-4)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_unicode_text_versus_bytes_CO5-4)'
- en: KOI8-R is for Russian. Now `'\xe9'` stands for the Cyrillic letter “И”.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: KOI8-R 用于俄语。现在`'\xe9'`代表西里尔字母“И”。
- en: '[![5](assets/5.png)](#co_unicode_text_versus_bytes_CO5-5)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_unicode_text_versus_bytes_CO5-5)'
- en: The `'utf_8'` codec detects that `octets` is not valid UTF-8, and raises `UnicodeDecodeError`.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '`''utf_8''`编解码器检测到`octets`不是有效的 UTF-8，并引发`UnicodeDecodeError`。'
- en: '[![6](assets/6.png)](#co_unicode_text_versus_bytes_CO5-6)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](assets/6.png)](#co_unicode_text_versus_bytes_CO5-6)'
- en: Using `'replace'` error handling, the `\xe9` is replaced by “�” (code point
    U+FFFD), the official Unicode `REPLACEMENT CHARACTER` intended to represent unknown
    characters.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ��用`'replace'`错误处理，`\xe9`会被“�”（代码点 U+FFFD）替换，这是官方的 Unicode `REPLACEMENT CHARACTER`，用于表示未知字符。
- en: SyntaxError When Loading Modules with Unexpected Encoding
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载具有意外编码的模块时出现 SyntaxError
- en: 'UTF-8 is the default source encoding for Python 3, just as ASCII was the default
    for Python 2. If you load a *.py* module containing non-UTF-8 data and no encoding
    declaration, you get a message like this:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: UTF-8 是 Python 3 的默认源编码，就像 ASCII 是 Python 2 的默认编码一样。如果加载包含非 UTF-8 数据且没有编码声明的
    *.py* 模块，则会收到如下消息：
- en: '[PRE7]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Because UTF-8 is widely deployed in GNU/Linux and macOS systems, a likely scenario
    is opening a *.py* file created on Windows with `cp1252`. Note that this error
    happens even in Python for Windows, because the default encoding for Python 3
    source is UTF-8 across all platforms.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 UTF-8 在 GNU/Linux 和 macOS 系统中被广泛部署，一个可能的情况是在 Windows 上用`cp1252`打开一个 *.py*
    文件。请注意，即使在 Windows 的 Python 中，这种错误也会发生，因为 Python 3 源代码在所有平台上的默认编码都是 UTF-8。
- en: To fix this problem, add a magic `coding` comment at the top of the file, as
    shown in [Example 4-7](#ex_ola_mundo).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 要解决这个问题，在文件顶部添加一个魔术`coding`注释，如[Example 4-7](#ex_ola_mundo)所示。
- en: 'Example 4-7\. *ola.py*: “Hello, World!” in Portuguese'
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-7\. *ola.py*：葡萄牙语中的“Hello, World!”
- en: '[PRE8]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Tip
  id: totrans-164
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Now that Python 3 source code is no longer limited to ASCII and defaults to
    the excellent UTF-8 encoding, the best “fix” for source code in legacy encodings
    like `'cp1252'` is to convert them to UTF-8 already, and not bother with the `coding`
    comments. If your editor does not support UTF-8, it’s time to switch.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在 Python 3 源代码不再限于 ASCII，并且默认使用优秀的 UTF-8 编码，因此对于像`'cp1252'`这样的遗留编码的源代码，最好的“修复”方法是将它们转换为
    UTF-8，并且不再使用`coding`注释。如果您的编辑器不支持 UTF-8，那么是时候换一个了。
- en: Suppose you have a text file, be it source code or poetry, but you don’t know
    its encoding. How do you detect the actual encoding? Answers in the next section.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您有一个文本文件，无论是源代码还是诗歌，但您不知道其编码。如何检测实际的编码？答案在下一节中。
- en: How to Discover the Encoding of a Byte Sequence
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何发现字节序列的编码
- en: 'How do you find the encoding of a byte sequence? Short answer: you can’t. You
    must be told.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如何找到字节序列的编码？简短回答：你无法。你必须被告知。
- en: Some communication protocols and file formats, like HTTP and XML, contain headers
    that explicitly tell us how the content is encoded. You can be sure that some
    byte streams are not ASCII because they contain byte values over 127, and the
    way UTF-8 and UTF-16 are built also limits the possible byte sequences.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 一些通信协议和文件格式，比如HTTP和XML，包含明确告诉我们内容如何编码的头部。你可以确定一些字节流不是ASCII，因为它们包含超过127的字节值，而UTF-8和UTF-16的构建方式也限制了可能的字节序列。
- en: However, considering that human languages also have their rules and restrictions,
    once you assume that a stream of bytes is human *plain text*, it may be possible
    to sniff out its encoding using heuristics and statistics. For example, if `b'\x00'`
    bytes are common, it is probably a 16- or 32-bit encoding, and not an 8-bit scheme,
    because null characters in plain text are bugs. When the byte sequence `b'\x20\x00'`
    appears often, it is more likely to be the space character (U+0020) in a UTF-16LE
    encoding, rather than the obscure U+2000 `EN QUAD` character—whatever that is.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，考虑到人类语言也有其规则和限制，一旦假定一系列字节是人类*纯文本*，可能可以通过启发式和统计方法来嗅探其编码。例如，如果`b'\x00'`字节很常见，那么它可能是16位或32位编码，而不是8位方案，因为纯文本中的空字符是错误的。当字节序列`b'\x20\x00'`经常出现时，更可能是UTF-16LE编码中的空格字符（U+0020），而不是晦涩的U+2000
    `EN QUAD`字符—不管那是什么。
- en: 'That is how the package [“Chardet—The Universal Character Encoding Detector”](https://fpy.li/4-8)
    works to guess one of more than 30 supported encodings. *Chardet* is a Python
    library that you can use in your programs, but also includes a command-line utility,
    `chardetect`. Here is what it reports on the source file for this chapter:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是包[“Chardet—通用字符编码检测器”](https://fpy.li/4-8)是如何工作的，猜测其中的一个支持的30多种编码。*Chardet*是一个你可以在程序中使用的Python库，但也包括一个命令行实用程序，`chardetect`。这是它在本章源文件上报告的内容：
- en: '[PRE9]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Although binary sequences of encoded text usually don’t carry explicit hints
    of their encoding, the UTF formats may prepend a byte order mark to the textual
    content. That is explained next.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管编码文本的二进制序列通常不包含其编码的明确提示，但UTF格式可能在文本内容前面添加字节顺序标记。接下来将对此进行解释。
- en: 'BOM: A Useful Gremlin'
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BOM：一个有用的小精灵
- en: 'In [Example 4-4](#ex_codecs), you may have noticed a couple of extra bytes
    at the beginning of a UTF-16 encoded sequence. Here they are again:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例 4-4](#ex_codecs)中，你可能已经注意到UTF-16编码序列开头有一对额外的字节。这里再次展示：
- en: '[PRE10]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The bytes are `b'\xff\xfe'`. That is a *BOM*—byte-order mark—denoting the “little-endian”
    byte ordering of the Intel CPU where the encoding was performed.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这些字节是`b'\xff\xfe'`。这是一个*BOM*—字节顺序标记—表示进行编码的Intel CPU的“小端”字节顺序。
- en: 'On a little-endian machine, for each code point the least significant byte
    comes first: the letter `''E''`, code point U+0045 (decimal 69), is encoded in
    byte offsets 2 and 3 as `69` and `0`:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在小端机器上，对于每个代码点，最低有效字节先出现：字母`'E'`，代码点U+0045（十进制69），在字节偏移2和3中编码为`69`和`0`：
- en: '[PRE11]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: On a big-endian CPU, the encoding would be reversed; `'E'` would be encoded
    as `0` and `69`.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在大端CPU上，编码会被颠倒；`'E'`会被编码为`0`和`69`。
- en: To avoid confusion, the UTF-16 encoding prepends the text to be encoded with
    the special invisible character `ZERO WIDTH NO-BREAK SPACE` (U+FEFF). On a little-endian
    system, that is encoded as `b'\xff\xfe'` (decimal 255, 254). Because, by design,
    there is no U+FFFE character in Unicode, the byte sequence `b'\xff\xfe'` must
    mean the `ZERO WIDTH NO-BREAK SPACE` on a little-endian encoding, so the codec
    knows which byte ordering to use.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免混淆，UTF-16编码在要编码的文本前面加上特殊的不可见字符`零宽不换行空格`（U+FEFF）。在小端系统上，它被编码为`b'\xff\xfe'`（十进制255，254）。因为按设计，Unicode中没有U+FFFE字符，字节序列`b'\xff\xfe'`必须表示小端编码中的`零宽不换行空格`，所以编解码器知道要使用哪种字节顺序。
- en: 'There is a variant of UTF-16—UTF-16LE—that is explicitly little-endian, and
    another one explicitly big-endian, UTF-16BE. If you use them, a BOM is not generated:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种UTF-16的变体——UTF-16LE，明确是小端的，另一种是明确是大端的，UTF-16BE。如果使用它们，就不会生成BOM：
- en: '[PRE12]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: If present, the BOM is supposed to be filtered by the UTF-16 codec, so that
    you only get the actual text contents of the file without the leading `ZERO WIDTH
    NO-BREAK SPACE`. The Unicode standard says that if a file is UTF-16 and has no
    BOM, it should be assumed to be UTF-16BE (big-endian). However, the Intel x86
    architecture is little-endian, so there is plenty of little-endian UTF-16 with
    no BOM in the wild.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 如果存在BOM，应该由UTF-16编解码器过滤，这样你只会得到文件的实际文本内容，而不包括前导的`零宽不换行空格`。Unicode标准规定，如果一个文件是UTF-16且没有BOM，应该假定为UTF-16BE（大端）。然而，Intel
    x86架构是小端的，因此在实际中有很多没有BOM的小端UTF-16。
- en: This whole issue of endianness only affects encodings that use words of more
    than one byte, like UTF-16 and UTF-32. One big advantage of UTF-8 is that it produces
    the same byte sequence regardless of machine endianness, so no BOM is needed.
    Nevertheless, some Windows applications (notably Notepad) add the BOM to UTF-8
    files anyway—and Excel depends on the BOM to detect a UTF-8 file, otherwise it
    assumes the content is encoded with a Windows code page. This UTF-8 encoding with
    BOM is called UTF-8-SIG in Python’s codec registry. The character U+FEFF encoded
    in UTF-8-SIG is the three-byte sequence `b'\xef\xbb\xbf'`. So if a file starts
    with those three bytes, it is likely to be a UTF-8 file with a BOM.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这整个字节序问题只影响使用多字节的编码，比如UTF-16和UTF-32。UTF-8的一个重要优势是，无论机器的字节序如何，它都会产生相同的字节序列，因此不需要BOM。然而，一些Windows应用程序（特别是记事本）仍然会向UTF-8文件添加BOM—Excel依赖BOM来检测UTF-8文件，否则它会假定内容是用Windows代码页编码的。Python的编解码器注册表中称带有BOM的UTF-8编码为UTF-8-SIG。UTF-8-SIG中编码的字符U+FEFF是三字节序列`b'\xef\xbb\xbf'`。因此，如果一个文件以这三个字节开头，很可能是带有BOM的UTF-8文件。
- en: Caleb’s Tip about UTF-8-SIG
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Caleb关于UTF-8-SIG的提示
- en: 'Caleb Hattingh—one of the tech reviewers—suggests always using the UTF-8-SIG
    codec when reading UTF-8 files. This is harmless because UTF-8-SIG reads files
    with or without a BOM correctly, and does not return the BOM itself. When writing,
    I recommend using UTF-8 for general interoperability. For example, Python scripts
    can be made executable in Unix systems if they start with the comment: `#!/usr/bin/env
    python3`. The first two bytes of the file must be `b''#!''` for that to work,
    but the BOM breaks that convention. If you have a specific requirement to export
    data to apps that need the BOM, use UTF-8-SIG but be aware that Python’s [codecs
    documentation](https://fpy.li/4-9) says: “In UTF-8, the use of the BOM is discouraged
    and should generally be avoided.”'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 技术审查员之一 Caleb Hattingh 建议在读取 UTF-8 文件时始终使用 UTF-8-SIG 编解码器。这是无害的，因为 UTF-8-SIG
    可以正确读取带或不带 BOM 的文件，并且不会返回 BOM 本身。在写入时，我建议为了一般的互操作性使用 UTF-8。例如，如果 Python 脚本以 `#!/usr/bin/env
    python3` 开头，可以在 Unix 系统中使其可执行。文件的前两个字节必须是 `b'#!'` 才能正常工作，但 BOM 打破了这个约定。如果有特定要求需要将数据导出到需要
    BOM 的应用程序中，请使用 UTF-8-SIG，但请注意 Python 的 [编解码器文档](https://fpy.li/4-9) 表示：“在 UTF-8
    中，不鼓励使用 BOM，通常应避免使用。”
- en: We now move on to handling text files in Python 3.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们转向在 Python 3 中处理文本文件。
- en: Handling Text Files
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理文本文件
- en: The best practice for handling text I/O is the “Unicode sandwich” ([Figure 4-2](#unicode_sandwich_fig)).^([5](ch04.html#idm46582487659424))
    This means that `bytes` should be decoded to `str` as early as possible on input
    (e.g., when opening a file for reading). The “filling” of the sandwich is the
    business logic of your program, where text handling is done exclusively on `str`
    objects. You should never be encoding or decoding in the middle of other processing.
    On output, the `str` are encoded to `bytes` as late as possible. Most web frameworks
    work like that, and we rarely touch `bytes` when using them. In Django, for example,
    your views should output Unicode `str`; Django itself takes care of encoding the
    response to `bytes`, using UTF-8 by default.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 处理文本 I/O 的最佳实践是“Unicode 三明治”（[图 4-2](#unicode_sandwich_fig)）。^([5](ch04.html#idm46582487659424))
    这意味着 `bytes` 应尽早解码为 `str`（例如，在打开文件进行读取时）。三明治的“馅料”是程序的业务逻辑，在这里文本处理完全在 `str` 对象上进行。您永远不应该在其他处理过程中进行编码或解码。在输出时，`str`
    应尽可能晚地编码为 `bytes`。大多数 Web 框架都是这样工作的，当使用它们时我们很少接触 `bytes`。例如，在 Django 中，您的视图应输出
    Unicode `str`；Django 本身负责将响应编码为 `bytes`，默认使用 UTF-8。
- en: Python 3 makes it easier to follow the advice of the Unicode sandwich, because
    the `open()` built-in does the necessary decoding when reading and encoding when
    writing files in text mode, so all you get from `my_file.read()` and pass to `my_file.write(text)`
    are `str` objects.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: Python 3 更容易遵循 Unicode 三明治的建议，因为内置的 `open()` 在读取和写入文本模式文件��进行必要的解码和编码，因此从 `my_file.read()`
    获取的内容并传递给 `my_file.write(text)` 的都是 `str` 对象。
- en: Therefore, using text files is apparently simple. But if you rely on default
    encodings, you will get bitten.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用文本文件似乎很简单。但是，如果依赖默认编码，您将受到影响。
- en: '![Unicode sandwich diagram](assets/flpy_0402.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![Unicode 三明治图示](assets/flpy_0402.png)'
- en: 'Figure 4-2\. Unicode sandwich: current best practice for text processing.'
  id: totrans-194
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-2\. Unicode 三明治：文本处理的当前最佳实践。
- en: Consider the console session in [Example 4-8](#ex_cafe_file1). Can you spot
    the bug?
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑 [示例 4-8](#ex_cafe_file1) 中的控制台会话。您能发现 bug 吗？
- en: Example 4-8\. A platform encoding issue (if you try this on your machine, you
    may or may not see the problem)
  id: totrans-196
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-8\. 平台编码问题（如果您在自己的计算机上尝试此操作，可能会看到问题，也可能不会）
- en: '[PRE13]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The bug: I specified UTF-8 encoding when writing the file but failed to do
    so when reading it, so Python assumed Windows default file encoding—code page
    1252—and the trailing bytes in the file were decoded as characters `''Ã©''` instead
    of `''é''`.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: Bug：我在写入文件时指定了 UTF-8 编码，但在读取文件时未这样做，因此 Python 假定 Windows 默认文件编码为代码页 1252，并且文件中的尾随字节被解码为字符
    `'Ã©'` 而不是 `'é'`。
- en: I ran [Example 4-8](#ex_cafe_file1) on Python 3.8.1, 64 bits, on Windows 10
    (build 18363). The same statements running on recent GNU/Linux or macOS work perfectly
    well because their default encoding is UTF-8, giving the false impression that
    everything is fine. If the encoding argument was omitted when opening the file
    to write, the locale default encoding would be used, and we’d read the file correctly
    using the same encoding. But then this script would generate files with different
    byte contents depending on the platform or even depending on locale settings in
    the same platform, creating compatibility problems.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我在 Windows 10（版本 18363）上运行了 Python 3.8.1 64 位上的 [示例 4-8](#ex_cafe_file1)。在最近的
    GNU/Linux 或 macOS 上运行相同的语句完全正常，因为它们的默认编码是 UTF-8，给人一种一切正常的假象。如果在打开文件进行写入时省略了编码参数，将使用区域设置的默认编码，我们将使用相同的编码正确读取文件。但是，这个脚本将根据平台或甚至相同平台中的区域设置生成具有不同字节内容的文件，从而创建兼容性问题。
- en: Tip
  id: totrans-200
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Code that has to run on multiple machines or on multiple occasions should never
    depend on encoding defaults. Always pass an explicit `encoding=` argument when
    opening text files, because the default may change from one machine to the next,
    or from one day to the next.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 必须在多台机器上运行或在多个场合上运行的代码绝不能依赖于编码默认值。在打开文本文件时始终传递显式的 `encoding=` 参数，因为默认值可能会从一台机器变为另一台机器，或者从一天变为另一天。
- en: A curious detail in [Example 4-8](#ex_cafe_file1) is that the `write` function
    in the first statement reports that four characters were written, but in the next
    line five characters are read. [Example 4-9](#ex_cafe_file2) is an extended version
    of [Example 4-8](#ex_cafe_file1), explaining that and other details.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 4-8](#ex_cafe_file1) 中一个有趣的细节是，第一条语句中的 `write` 函数报告写入了四个字符，但在下一行读取了五个字符。[示例
    4-9](#ex_cafe_file2) 是 [示例 4-8](#ex_cafe_file1) 的扩展版本，解释了这个问题和其他细节。'
- en: Example 4-9\. Closer inspection of [Example 4-8](#ex_cafe_file1) running on
    Windows reveals the bug and how to fix it
  id: totrans-203
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-9\. 仔细检查在 Windows 上运行的 [示例 4-8](#ex_cafe_file1) 中的 bug 以及如何修复它
- en: '[PRE14]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[![1](assets/1.png)](#co_unicode_text_versus_bytes_CO6-1)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_unicode_text_versus_bytes_CO6-1)'
- en: By default, `open` uses text mode and returns a `TextIOWrapper` object with
    a specific encoding.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`open`使用文本模式并返回一个具有特定编码的`TextIOWrapper`对象。
- en: '[![2](assets/2.png)](#co_unicode_text_versus_bytes_CO6-2)'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_unicode_text_versus_bytes_CO6-2)'
- en: The `write` method on a `TextIOWrapper` returns the number of Unicode characters
    written.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '`TextIOWrapper`上的`write`方法返回写入的Unicode字符数。'
- en: '[![3](assets/3.png)](#co_unicode_text_versus_bytes_CO6-3)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_unicode_text_versus_bytes_CO6-3)'
- en: '`os.stat` says the file has 5 bytes; UTF-8 encodes `''é''` as 2 bytes, 0xc3
    and 0xa9.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '`os.stat`显示文件有5个字节；UTF-8将`''é''`编码为2个字节，0xc3和0xa9。'
- en: '[![4](assets/4.png)](#co_unicode_text_versus_bytes_CO6-4)'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_unicode_text_versus_bytes_CO6-4)'
- en: Opening a text file with no explicit encoding returns a `TextIOWrapper` with
    the encoding set to a default from the locale.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 打开一个没有明确编码的文本文件会返回一个`TextIOWrapper`，其编码设置为来自区域设置的默认值。
- en: '[![5](assets/5.png)](#co_unicode_text_versus_bytes_CO6-5)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_unicode_text_versus_bytes_CO6-5)'
- en: 'A `TextIOWrapper` object has an encoding attribute that you can inspect: `cp1252`
    in this case.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '`TextIOWrapper`对象有一个编码属性，可以进行检查：在这种情况下是`cp1252`。'
- en: '[![6](assets/6.png)](#co_unicode_text_versus_bytes_CO6-6)'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](assets/6.png)](#co_unicode_text_versus_bytes_CO6-6)'
- en: In the Windows `cp1252` encoding, the byte 0xc3 is an “Ã” (A with tilde), and
    0xa9 is the copyright sign.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在Windows的`cp1252`编码中，字节0xc3是“Ã”（带波浪符的A），0xa9���版权符号。
- en: '[![7](assets/7.png)](#co_unicode_text_versus_bytes_CO6-7)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '[![7](assets/7.png)](#co_unicode_text_versus_bytes_CO6-7)'
- en: Opening the same file with the correct encoding.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 使用正确的编码打开相同的文件。
- en: '[![8](assets/8.png)](#co_unicode_text_versus_bytes_CO6-8)'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '[![8](assets/8.png)](#co_unicode_text_versus_bytes_CO6-8)'
- en: 'The expected result: the same four Unicode characters for `''café''`.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 预期结果：对于`'café'`相同的四个Unicode字符。
- en: '[![9](assets/9.png)](#co_unicode_text_versus_bytes_CO6-9)'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '[![9](assets/9.png)](#co_unicode_text_versus_bytes_CO6-9)'
- en: The `'rb'` flag opens a file for reading in binary mode.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '`''rb''`标志以二进制模式打开文件进行读取。'
- en: '[![10](assets/10.png)](#co_unicode_text_versus_bytes_CO6-10)'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '[![10](assets/10.png)](#co_unicode_text_versus_bytes_CO6-10)'
- en: The returned object is a `BufferedReader` and not a `TextIOWrapper`.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的对象是`BufferedReader`而不是`TextIOWrapper`。
- en: '[![11](assets/11.png)](#co_unicode_text_versus_bytes_CO6-11)'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '[![11](assets/11.png)](#co_unicode_text_versus_bytes_CO6-11)'
- en: Reading that returns bytes, as expected.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 读取返回的是字节，符合预期。
- en: Tip
  id: totrans-227
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Do not open text files in binary mode unless you need to analyze the file contents
    to determine the encoding—even then, you should be using Chardet instead of reinventing
    the wheel (see [“How to Discover the Encoding of a Byte Sequence”](#discover_encoding)).
    Ordinary code should only use binary mode to open binary files, like raster images.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 除非需要分析文件内容以确定编码，否则不要以二进制模式打开文本文件——即使这样，你应该使用Chardet而不是重复造轮子（参见[“如何发现字节序列的编码”](#discover_encoding)）。普通代码应该只使用二进制模式打开二进制文件，如光栅图像。
- en: The problem in [Example 4-9](#ex_cafe_file2) has to do with relying on a default
    setting while opening a text file. There are several sources for such defaults,
    as the next section shows.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '[Example 4-9](#ex_cafe_file2)中的问题涉及依赖默认设置打开文本文件。如下一节所示，有几个来源可以提供这些默认值。'
- en: Beware of Encoding Defaults
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 警惕编码默认值
- en: Several settings affect the encoding defaults for I/O in Python. See the *default_encodings.py*
    script in [Example 4-10](#ex_default_encodings).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 几个设置影响Python中I/O的编码默认值。查看[Example 4-10](#ex_default_encodings)中的*default_encodings.py*脚本。
- en: Example 4-10\. Exploring encoding defaults
  id: totrans-232
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 4-10\. 探索编码默认值
- en: '[PRE15]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output of [Example 4-10](#ex_default_encodings) on GNU/Linux (Ubuntu 14.04
    to 19.10) and macOS (10.9 to 10.14) is identical, showing that `UTF-8` is used
    everywhere in these systems:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '[Example 4-10](#ex_default_encodings)在GNU/Linux（Ubuntu 14.04至19.10）和macOS（10.9至10.14）上的输出是相同的，显示`UTF-8`在这些系统中随处可用：'
- en: '[PRE16]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: On Windows, however, the output is [Example 4-11](#ex_default_encodings_ps).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在Windows上，输出是[Example 4-11](#ex_default_encodings_ps)。
- en: Example 4-11\. Default encodings on Windows 10 PowerShell (output is the same
    on cmd.exe)
  id: totrans-237
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 4-11\. Windows 10 PowerShell上的默认编码（在cmd.exe上输出相同）
- en: '[PRE17]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[![1](assets/1.png)](#co_unicode_text_versus_bytes_CO7-1)'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_unicode_text_versus_bytes_CO7-1)'
- en: '`chcp` shows the active code page for the console: `437`.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '`chcp`显示控制台的活动代码页为`437`。'
- en: '[![2](assets/2.png)](#co_unicode_text_versus_bytes_CO7-2)'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_unicode_text_versus_bytes_CO7-2)'
- en: Running *default_encodings.py* with output to console.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 运行*default_encodings.py*并输出到控制台。
- en: '[![3](assets/3.png)](#co_unicode_text_versus_bytes_CO7-3)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_unicode_text_versus_bytes_CO7-3)'
- en: '`locale.getpreferredencoding()` is the most important setting.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '`locale.getpreferredencoding()`是最重要的设置。'
- en: '[![4](assets/4.png)](#co_unicode_text_versus_bytes_CO7-4)'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_unicode_text_versus_bytes_CO7-4)'
- en: Text files use `locale.getpreferredencoding()` by default.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 文本文件默认使用`locale.getpreferredencoding()`。
- en: '[![5](assets/5.png)](#co_unicode_text_versus_bytes_CO7-5)'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_unicode_text_versus_bytes_CO7-5)'
- en: The output is going to the console, so `sys.stdout.isatty()` is `True`.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将发送到控制台，因此`sys.stdout.isatty()`为`True`。
- en: '[![6](assets/6.png)](#co_unicode_text_versus_bytes_CO7-6)'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](assets/6.png)](#co_unicode_text_versus_bytes_CO7-6)'
- en: Now, `sys.stdout.encoding` is not the same as the console code page reported
    by `chcp`!
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`sys.stdout.encoding`与`chcp`报告的控制台代码页不同！
- en: 'Unicode support in Windows itself, and in Python for Windows, got better since
    I wrote the first edition of this book. [Example 4-11](#ex_default_encodings_ps)
    used to report four different encodings in Python 3.4 on Windows 7. The encodings
    for `stdout`, `stdin`, and `stderr` used to be the same as the active code page
    reported by the `chcp` command, but now they’re all `utf-8` thanks to [PEP 528—Change
    Windows console encoding to UTF-8](https://fpy.li/pep528) implemented in Python
    3.6, and Unicode support in PowerShell in *cmd.exe* (since Windows 1809 from October
    2018).^([6](ch04.html#idm46582487064752)) It’s weird that `chcp` and `sys.stdout.encoding`
    say different things when `stdout` is writing to the console, but it’s great that
    now we can print Unicode strings without encoding errors on Windows—unless the
    user redirects output to a file, as we’ll soon see. That does not mean all your
    favorite emojis will appear in the console: that also depends on the font the
    console is using.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: Windows本身以及Python针对Windows的Unicode支持在我写这本书的第一版之后变得更好了。[示例 4-11](#ex_default_encodings_ps)曾经在Windows
    7上的Python 3.4中报告了四种不同的编码。`stdout`、`stdin`和`stderr`的编码曾经与`chcp`命令报告的活动代码页相同，但现在由于Python
    3.6中实现的[PEP 528—将Windows控制台编码更改为UTF-8](https://fpy.li/pep528)，以及*cmd.exe*中的PowerShell中的Unicode支持（自2018年10月的Windows
    1809起）。^([6](ch04.html#idm46582487064752)) 当`stdout`写入控制台时，`chcp`和`sys.stdout.encoding`说不同的事情是很奇怪的，但现在我们可以在Windows上打印Unicode字符串而不会出现编码错误——除非用户将输出重定向到文件，正如我们很快将看到的。这并不意味着所有你喜欢的表情符号都会出现在控制台中：这也取决于控制台使用���字体。
- en: Another change was [PEP 529—Change Windows filesystem encoding to UTF-8](https://fpy.li/pep529),
    also implemented in Python 3.6, which changed the filesystem encoding (used to
    represent names of directories and files) from Microsoft’s proprietary MBCS to
    UTF-8.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个变化是[PEP 529—将Windows文件系统编码更改为UTF-8](https://fpy.li/pep529)，也在Python 3.6中实现，将文件系统编码（用于表示目录和文件名称）从微软专有的MBCS更改为UTF-8。
- en: 'However, if the output of [Example 4-10](#ex_default_encodings) is redirected
    to a file, like this:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果[示例 4-10](#ex_default_encodings)的输出被重定向到文件，就像这样：
- en: '[PRE18]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: then, the value of `sys.stdout.isatty()` becomes `False`, and `sys.stdout.encoding`
    is set by `locale.getpreferredencoding()`, `'cp1252'` in that machine—but `sys.stdin.encoding`
    and `sys.stderr.encoding` remain `utf-8`.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，`sys.stdout.isatty()`的值变为`False`，`sys.stdout.encoding`由`locale.getpreferredencoding()`设置，在该机器上为`'cp1252'`—但`sys.stdin.encoding`和`sys.stderr.encoding`仍然为`utf-8`。
- en: Tip
  id: totrans-256
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'In [Example 4-12](#ex_stdout_check) I use the `''\N{}''` escape for Unicode
    literals, where we write the official name of the character inside the `\N{}`.
    It’s rather verbose, but explicit and safe: Python raises `SyntaxError` if the
    name doesn’t exist—much better than writing a hex number that could be wrong,
    but you’ll only find out much later. You’d probably want to write a comment explaining
    the character codes anyway, so the verbosity of `\N{}` is easy to accept.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例 4-12](#ex_stdout_check)中，我使用`'\N{}'`转义来表示Unicode文字，其中我们在`\N{}`内写入字符的官方名称。这样做相当冗长，但明确且安全：如果名称不存在，Python会引发`SyntaxError`——比起写一个可能错误的十六进制数，这样做要好得多，但你只能在很久以后才会发现。你可能想要写一个解释字符代码的注释，所以`\N{}`的冗长是容易接受的。
- en: This means that a script like [Example 4-12](#ex_stdout_check) works when printing
    to the console, but may break when output is redirected to a file.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着像[示例 4-12](#ex_stdout_check)这样的脚本在打印到控制台时可以正常工作，但在输出被重定向到文件时可能会出现问题。
- en: Example 4-12\. stdout_check.py
  id: totrans-259
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-12\. stdout_check.py
- en: '[PRE19]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[Example 4-12](#ex_stdout_check) displays the result of `sys.stdout.isatty()`,
    the value of `sys.​stdout.encoding`, and these three characters:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 4-12](#ex_stdout_check)显示了`sys.stdout.isatty()`的结果，`sys.​stdout.encoding`的值，以及这三个字符：'
- en: '`''…''` `HORIZONTAL ELLIPSIS`—exists in CP 1252 but not in CP 437.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''…''` `HORIZONTAL ELLIPSIS`—存在于CP 1252中，但不存在于CP 437中。'
- en: '`''∞''` `INFINITY`—exists in CP 437 but not in CP 1252.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''∞''` `INFINITY`—存在于CP 437中，但不存在于CP 1252中。'
- en: '`''㊷''` `CIRCLED NUMBER FORTY TWO`—doesn’t exist in CP 1252 or CP 437.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''㊷''` `CIRCLED NUMBER FORTY TWO`—在CP 1252或CP 437中不存在。'
- en: When I run *stdout_check.py* on PowerShell or *cmd.exe*, it works as captured
    in [Figure 4-3](#fig_stdout_check).
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 当我在PowerShell或*cmd.exe*上运行*stdout_check.py*时，它的运行情况如[图 4-3](#fig_stdout_check)所示。
- en: '![Screen capture of `stdout_check.py` on PowerShell](assets/flpy_0403.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![在PowerShell上的`stdout_check.py`的屏幕截图](assets/flpy_0403.png)'
- en: Figure 4-3\. Running *stdout_check.py* on PowerShell.
  id: totrans-267
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-3\. 在PowerShell上运行*stdout_check.py*。
- en: Despite `chcp` reporting the active code as 437, `sys.stdout.encoding` is UTF-8,
    so the `HORIZONTAL ELLIPSIS` and `INFINITY` both output correctly. The `CIRCLED
    NUMBER FORTY TWO` is replaced by a rectangle, but no error is raised. Presumably
    it is recognized as a valid character, but the console font doesn’t have the glyph
    to display it.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管`chcp`报告活动代码为437，但`sys.stdout.encoding`为UTF-8，因此`HORIZONTAL ELLIPSIS`和`INFINITY`都能正确输出。`CIRCLED
    NUMBER FORTY TWO`被一个矩形替换，但不会引发错误。可能它被识别为有效字符，但控制台字体没有显示它的字形。
- en: However, when I redirect the output of *stdout_check.py* to a file, I get [Figure 4-4](#fig_stdout_check_redir).
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当我将*stdout_check.py*的输出重定向到文件时，我得到了[图 4-4](#fig_stdout_check_redir)。
- en: '![Screen capture of `stdout_check.py` on PowerShell, redirecting output](assets/flpy_0404.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![在PowerShell上的`stdout_check.py`的屏幕截图，重定向输出](assets/flpy_0404.png)'
- en: Figure 4-4\. Running *stdout_check.py* on PowerShell, redirecting output.
  id: totrans-271
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-4\. 在PowerShell上运行*stdout_check.py*，重定向输出。
- en: The first problem demonstrated by [Figure 4-4](#fig_stdout_check_redir) is the
    `UnicodeEncodeError` mentioning character `'\u221e'`, because `sys.stdout.encoding`
    is `'cp1252'`—a code page that doesn’t have the `INFINITY` character.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-4](#fig_stdout_check_redir)展示的第一个问题是`UnicodeEncodeError`，提到字符`''\u221e''`，因为`sys.stdout.encoding`是`''cp1252''`—一个不包含`INFINITY`字符的代码页。'
- en: Reading *out.txt* with the `type` command—or a Windows editor like VS Code or
    Sublime Text—shows that instead of HORIZONTAL ELLIPSIS, I got `'à'` (`LATIN SMALL
    LETTER A WITH GRAVE`). As it turns out, the byte value 0x85 in CP 1252 means `'…'`,
    but in CP 437 the same byte value represents `'à'`. So it seems the active code
    page does matter, not in a sensible or useful way, but as partial explanation
    of a bad Unicode experience.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`type`命令读取*out.txt*，或者使用 Windows 编辑器如 VS Code 或 Sublime Text，显示的不是水平省略号，而是`'à'`（带重音的拉丁小写字母
    A）。事实证明，在 CP 1252 中，字节值 0x85 表示`'…'`，但在 CP 437 中，相同的字节值代表`'à'`。因此，似乎活动代码页确实很重要，但并不是以明智或有用的方式，而是作为糟糕的
    Unicode 经历的部分解释。
- en: Note
  id: totrans-274
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: I used a laptop configured for the US market, running Windows 10 OEM to run
    these experiments. Windows versions localized for other countries may have different
    encoding configurations. For example, in Brazil the Windows console uses code
    page 850 by default—not 437.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用配置为美国市场的笔记本电脑，运行 Windows 10 OEM 来运行这些实验。为其他国家本地化的 Windows 版本可能具有不同的编码配置。例如，在巴西，Windows
    控制台默认使用代码页 850，而不是 437。
- en: 'To wrap up this maddening issue of default encodings, let’s give a final look
    at the different encodings in [Example 4-11](#ex_default_encodings_ps):'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 为了总结这个令人疯狂的默认编码问题，让我们最后看一下[示例 4-11](#ex_default_encodings_ps)中的不同编码：
- en: If you omit the `encoding` argument when opening a file, the default is given
    by `locale.getpreferredencoding()` (`'cp1252'` in [Example 4-11](#ex_default_encodings_ps)).
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果在打开文件时省略`encoding`参数，则默认值由`locale.getpreferredencoding()`给出（在[示例 4-11](#ex_default_encodings_ps)中为`'cp1252'`）。
- en: The encoding of `sys.stdout|stdin|stderr` used to be set by the [`PYTHONIOENCODING`](https://fpy.li/4-12)
    environment variable before Python 3.6—now that variable is ignored, unless [`PYTHONLEGACYWINDOWSSTDIO`](https://fpy.li/4-13)
    is set to a nonempty string. Otherwise, the encoding for standard I/O is UTF-8
    for interactive I/O, or defined by `locale.getpreferredencoding()` if the output/input
    is redirected to/from a file.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Python 3.6 之前，`sys.stdout|stdin|stderr`的编码是由[`PYTHONIOENCODING`](https://fpy.li/4-12)环境变量设置的，现在该变量被忽略，除非[`PYTHONLEGACYWINDOWSSTDIO`](https://fpy.li/4-13)设置为非空字符串。否则，标准
    I/O 的编码对于交互式 I/O 是 UTF-8，或者如果输出/输入被重定向到/从文件，则由`locale.getpreferredencoding()`定义。
- en: '`sys.getdefaultencoding()` is used internally by Python in implicit conversions
    of binary data to/from `str`. Changing this setting is not supported.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sys.getdefaultencoding()`在 Python 中用于二进制数据与`str`之间的隐式转换。不支持更改此设置。'
- en: '`sys.getfilesystemencoding()` is used to encode/decode filenames (not file
    contents). It is used when `open()` gets a `str` argument for the filename; if
    the filename is given as a `bytes` argument, it is passed unchanged to the OS
    API.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sys.getfilesystemencoding()`用于对文件名进行编码/解码（而不是文件内容）。当`open()`以`str`参数作为文件名时使用它；如果文件名以`bytes`参数给出，则不做更改地���递给操作系统
    API。'
- en: Note
  id: totrans-281
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: On GNU/Linux and macOS, all of these encodings are set to UTF-8 by default,
    and have been for several years, so I/O handles all Unicode characters. On Windows,
    not only are different encodings used in the same system, but they are usually
    code pages like `'cp850'` or `'cp1252'` that support only ASCII, with 127 additional
    characters that are not the same from one encoding to the other. Therefore, Windows
    users are far more likely to face encoding errors unless they are extra careful.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GNU/Linux 和 macOS 上，默认情况下，所有这些编码都设置为 UTF-8，已经有好几年了，因此 I/O 处理所有 Unicode 字符。在
    Windows 上，不仅在同一系统中使用不同的编码，而且通常是像`'cp850'`或`'cp1252'`这样只支持 ASCII 的代码页，还有 127 个额外字符，这些字符在不同编码之间并不相同。因此，Windows
    用户更有可能遇到编码错误，除非他们特别小心。
- en: 'To summarize, the most important encoding setting is that returned by `locale.getpreferredencoding()`:
    it is the default for opening text files and for `sys.stdout/stdin/stderr` when
    they are redirected to files. However, the [documentation](https://fpy.li/4-14)
    reads (in part):'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，最重要的编码设置是由`locale.getpreferredencoding()`返回的：它是打开文本文件和当`sys.stdout/stdin/stderr`被重定向到文件时的默认值。然而，[文档](https://fpy.li/4-14)部分内容如下：
- en: '`locale.getpreferredencoding(do_setlocale=True)`'
  id: totrans-284
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`locale.getpreferredencoding(do_setlocale=True)`'
- en: ''
  id: totrans-285
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Return the encoding used for text data, according to user preferences. User
    preferences are expressed differently on different systems, and might not be available
    programmatically on some systems, so this function only returns a guess. […]
  id: totrans-286
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 根据用户偏好返回用于文本数据的编码。用户偏好在不同系统上表达方式不同，有些系统可能无法以编程方式获取，因此此函数只返回一个猜测。[…]
- en: 'Therefore, the best advice about encoding defaults is: do not rely on them.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，关于编码默认值的最佳建议是：不要依赖于它们。
- en: 'You will avoid a lot of pain if you follow the advice of the Unicode sandwich
    and always are explicit about the encodings in your programs. Unfortunately, Unicode
    is painful even if you get your `bytes` correctly converted to `str`. The next
    two sections cover subjects that are simple in ASCII-land, but get quite complex
    on planet Unicode: text normalization (i.e., converting text to a uniform representation
    for comparisons) and sorting.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您遵循 Unicode 三明治的建议并始终明确指定程序中的编码，您将避免很多痛苦。不幸的是，即使您将您的`bytes`正确转换为`str`，Unicode
    也是令人头痛的。接下来的两节涵盖了在 ASCII 领域简单的主题，在 Unicode 行星上变得非常复杂的文本规范化（即将文本转换为用于比较的统一表示）和排序。
- en: Normalizing Unicode for Reliable Comparisons
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为了可靠比较而规范化 Unicode
- en: 'String comparisons are complicated by the fact that Unicode has combining characters:
    diacritics and other marks that attach to the preceding character, appearing as
    one when printed.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 字符串比较变得复杂的原因在于 Unicode 具有组合字符：附加到前一个字符的变音符号和其他标记，在打印时会显示为一个字符。
- en: 'For example, the word “café” may be composed in two ways, using four or five
    code points, but the result looks exactly the same:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，单词“café”可以用四个或五个代码点组成，但结果看起来完全相同：
- en: '[PRE20]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Placing `COMBINING ACUTE ACCENT` (U+0301) after “e” renders “é”. In the Unicode
    standard, sequences like `'é'` and `'e\u0301'` are called “canonical equivalents,”
    and applications are supposed to treat them as the same. But Python sees two different
    sequences of code points, and considers them not equal.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在“e”后面放置`COMBINING ACUTE ACCENT`（U+0301）会呈现“é”。在Unicode标准中，像`'é'`和`'e\u0301'`这样的序列被称为“规范等价物”，应用程序应将它们视为相同。但是Python看到两个不同的代码点序列，并认为它们不相等。
- en: 'The solution is `unicodedata.normalize()`. The first argument to that function
    is one of four strings: `''NFC''`, `''NFD''`, `''NFKC''`, and `''NFKD''`. Let’s
    start with the first two.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案是`unicodedata.normalize()`。该函数的第一个参数是四个字符串之一：`'NFC'`，`'NFD'`，`'NFKC'`和`'NFKD'`。让我们从前两个开始。
- en: 'Normalization Form C (NFC) composes the code points to produce the shortest
    equivalent string, while NFD decomposes, expanding composed characters into base
    characters and separate combining characters. Both of these normalizations make
    comparisons work as expected, as the next example shows:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 规范化形式C（NFC）将代码点组合以生成最短等效字符串，而NFD将分解，将组合字符扩展为基本字符和单独的组合字符。这两种规范化使比较按预期工作，如下一个示例所示：
- en: '[PRE21]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Keyboard drivers usually generate composed characters, so text typed by users
    will be in NFC by default. However, to be safe, it may be good to normalize strings
    with `normalize(''NFC'', user_text)` before saving. NFC is also the normalization
    form recommended by the W3C in [“Character Model for the World Wide Web: String
    Matching and Searching”](https://fpy.li/4-15).'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 键盘驱动程序通常生成组合字符，因此用户输入的文本默认情况下将是NFC。但是，为了安全起见，在保存之前最好使用`normalize('NFC', user_text)`对字符串进行规范化。NFC也是W3C在[“全球网络字符模型：字符串匹配和搜索”](https://fpy.li/4-15)中推荐的规范化形式。
- en: 'Some single characters are normalized by NFC into another single character.
    The symbol for the ohm (Ω) unit of electrical resistance is normalized to the
    Greek uppercase omega. They are visually identical, but they compare as unequal,
    so it is essential to normalize to avoid surprises:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 一些单个字符被NFC规范化为另一个单个字符。电阻单位欧姆（Ω）的符号被规范化为希腊大写omega。它们在视觉上是相同的，但它们比较不相等，因此规范化是必不可少的，以避免意外：
- en: '[PRE22]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The other two normalization forms are NFKC and NFKD, where the letter K stands
    for “compatibility.” These are stronger forms of normalization, affecting the
    so-called “compatibility characters.” Although one goal of Unicode is to have
    a single “canonical” code point for each character, some characters appear more
    than once for compatibility with preexisting standards. For example, the `MICRO
    SIGN`, `µ` (`U+00B5`), was added to Unicode to support round-trip conversion to
    `latin1`, which includes it, even though the same character is part of the Greek
    alphabet with code point `U+03BC` (`GREEK SMALL LETTER MU`). So, the micro sign
    is considered a “compatibility character.”
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 另外两种规范化形式是NFKC和NFKD，其中字母K代表“兼容性”。这些是更强的规范化形式，影响所谓的“兼容性字符”。尽管Unicode的一个目标是为每个字符有一个单��的“规范”代码点，但一些字符出现多次是为了与现有标准兼容。例如，`MICRO
    SIGN`，`µ`（`U+00B5`），被添加到Unicode以支持与包括它在内的`latin1`的往返转换，即使相同的字符是希腊字母表的一部分，具有代码点`U+03BC`（`GREEK
    SMALL LETTER MU`）。因此，微符号被视为“兼容性字符”。
- en: In the NFKC and NFKD forms, each compatibility character is replaced by a “compatibility
    decomposition” of one or more characters that are considered a “preferred” representation,
    even if there is some formatting loss—ideally, the formatting should be the responsibility
    of external markup, not part of Unicode. To exemplify, the compatibility decomposition
    of the one-half fraction `'½'` (`U+00BD`) is the sequence of three characters
    `'1/2'`, and the compatibility decomposition of the micro sign `'µ'` (`U+00B5`)
    is the lowercase mu `'μ'` (`U+03BC`).^([7](ch04.html#idm46582486563520))
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在NFKC和NFKD形式中，每个兼容字符都被一个或多个字符的“兼容分解”替换，这些字符被认为是“首选”表示，即使存在一些格式损失——理想情况下，格式应该由外部标记负责，而不是Unicode的一部分。举例来说，一个半分数`'½'`（`U+00BD`）的兼容分解是三个字符的序列`'1/2'`，而微符号`'µ'`（`U+00B5`）的兼容分解是小写的希腊字母mu`'μ'`（`U+03BC`）。^([7](ch04.html#idm46582486563520))
- en: 'Here is how the NFKC works in practice:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是NFKC在实践中的工作方式：
- en: '[PRE23]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Although `'1⁄2'` is a reasonable substitute for `'½'`, and the micro sign is
    really a lowercase Greek mu, converting `'4²'` to `'42'` changes the meaning.
    An application could store `'4²'` as `'4<sup>2</sup>'`, but the `normalize` function
    knows nothing about formatting. Therefore, NFKC or NFKD may lose or distort information,
    but they can produce convenient intermediate representations for searching and
    indexing.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管`'1⁄2'`是`'½'`的一个合理替代品，而微符号实际上是一个小写希腊字母mu，但将`'4²'`转换为`'42'`会改变含义。一个应用程序可以将`'4²'`存储为`'4<sup>2</sup>'`，但`normalize`函数对格式一无所知。因此，NFKC或NFKD可能会丢失或扭曲信息，但它们可以生成方便的中间表示形式用于搜索和索引。
- en: Unfortunately, with Unicode everything is always more complicated than it first
    seems. For the `VULGAR FRACTION ONE HALF`, the NFKC normalization produced 1 and
    2 joined by `FRACTION SLASH`, instead of `SOLIDUS`, a.k.a. “slash”—the familiar
    character with ASCII code decimal 47. Therefore, searching for the three-character
    ASCII sequence `'1/2'` would not find the normalized Unicode sequence.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，对于Unicode来说，一切总是比起初看起来更加复杂。对于`VULGAR FRACTION ONE HALF`，NFKC规范化产生了用`FRACTION
    SLASH`连接的1和2，而不是`SOLIDUS`，即“斜杠”—ASCII代码十进制47的熟悉字符。因此，搜索三字符ASCII序列`'1/2'`将找不到规范化的Unicode序列。
- en: Warning
  id: totrans-306
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: NFKC and NFKD normalization cause data loss and should be applied only in special
    cases like search and indexing, and not for permanent storage of text.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: NFKC和NFKD规范会导致数据丢失，应仅在特殊情况下如搜索和索引中应用，而不是用于文本的永久存储。
- en: 'When preparing text for searching or indexing, another operation is useful:
    case folding, our next subject.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 当准备文本进行搜索或索引时，另一个有用的操作是大小写折叠，我们的下一个主题。
- en: Case Folding
  id: totrans-309
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大小写折叠
- en: Case folding is essentially converting all text to lowercase, with some additional
    transformations. It is supported by the `str.casefold()` method.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 大小写折叠基本上是将所有文本转换为小写，还有一些额外的转换。它由`str.casefold()`方法支持。
- en: 'For any string `s` containing only `latin1` characters, `s.casefold()` produces
    the same result as `s.lower()`, with only two exceptions—the micro sign `''µ''`
    is changed to the Greek lowercase mu (which looks the same in most fonts) and
    the German Eszett or “sharp s” (ß) becomes “ss”:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 对于只包含 `latin1` 字符的任何字符串 `s`，`s.casefold()` 产生与 `s.lower()` 相同的结果，只有两个例外——微符号
    `'µ'` 被更改为希腊小写 mu（在大多数字体中看起来相同），德语 Eszett 或 “sharp s”（ß）变为 “ss”：
- en: '[PRE24]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: There are nearly 300 code points for which `str.casefold()` and `str.lower()`
    return different results.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 有将近 300 个代码点，`str.casefold()` 和 `str.lower()` 返回不同的结果。
- en: As usual with anything related to Unicode, case folding is a hard issue with
    plenty of linguistic special cases, but the Python core team made an effort to
    provide a solution that hopefully works for most users.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 和 Unicode 相关的任何事物一样，大小写折叠是一个困难的问题，有很多语言特殊情况，但 Python 核心团队努力提供了一个解决方案，希望能适用于大多数用户。
- en: In the next couple of sections, we’ll put our normalization knowledge to use
    developing utility functions.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将利用我们的规范化知识开发实用函数。
- en: Utility Functions for Normalized Text Matching
  id: totrans-316
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于规范化文本匹配的实用函数
- en: As we’ve seen, NFC and NFD are safe to use and allow sensible comparisons between
    Unicode strings. NFC is the best normalized form for most applications. `str.casefold()`
    is the way to go for case-insensitive comparisons.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，NFC 和 NFD 是安全的，并允许在 Unicode 字符串之间进行明智的比较。对于大多数应用程序，NFC 是最佳的规范化形式。`str.casefold()`
    是进行不区分大小写比较的方法。
- en: If you work with text in many languages, a pair of functions like `nfc_equal`
    and `fold_equal` in [Example 4-13](#ex_normeq) are useful additions to your toolbox.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用多种语言的文本，像 [示例 4-13](#ex_normeq) 中的 `nfc_equal` 和 `fold_equal` 这样的一对函数对您的工具箱是有用的补充。
- en: 'Example 4-13\. normeq.py: normalized Unicode string comparison'
  id: totrans-319
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '示例 4-13\. normeq.py: 规范化的 Unicode 字符串比较'
- en: '[PRE25]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Beyond Unicode normalization and case folding—which are both part of the Unicode
    standard—sometimes it makes sense to apply deeper transformations, like changing
    `'café'` into `'cafe'`. We’ll see when and how in the next section.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 超出 Unicode 标准中的规范化和大小写折叠之外，有时候进行更深层次的转换是有意义的，比如将 `'café'` 改为 `'cafe'`。我们将在下一节看到何时以及如何进行。
- en: 'Extreme “Normalization”: Taking Out Diacritics'
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 极端的“规范化”：去除变音符号
- en: 'The Google Search secret sauce involves many tricks, but one of them apparently
    is ignoring diacritics (e.g., accents, cedillas, etc.), at least in some contexts.
    Removing diacritics is not a proper form of normalization because it often changes
    the meaning of words and may produce false positives when searching. But it helps
    coping with some facts of life: people sometimes are lazy or ignorant about the
    correct use of diacritics, and spelling rules change over time, meaning that accents
    come and go in living languages.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌搜索��秘密酱包含许多技巧，但其中一个显然是忽略变音符号（例如，重音符号、锐音符等），至少在某些情况下是这样。去除变音符号并不是一种适当的规范化形式，因为它经常改变单词的含义，并且在搜索时可能产生误报。但它有助于应对生活中的一些事实：人们有时懒惰或无知于正确使用变音符号，拼写规则随时间变化，这意味着重音符号在活语言中来来去去。
- en: 'Outside of searching, getting rid of diacritics also makes for more readable
    URLs, at least in Latin-based languages. Take a look at the URL for the Wikipedia
    article about the city of São Paulo:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 除了搜索之外，去除变音符号还可以使 URL 更易读，至少在基于拉丁语言的语言中是这样。看看关于圣保罗市的维基百科文章的 URL：
- en: '[PRE26]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The `%C3%A3` part is the URL-escaped, UTF-8 rendering of the single letter
    “ã” (“a” with tilde). The following is much easier to recognize, even if it is
    not the right spelling:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '`%C3%A3` 部分是 URL 转义的，UTF-8 渲染的单个字母 “ã”（带有波浪符的 “a”）。即使拼写不正确，以下内容也更容易识别：'
- en: '[PRE27]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: To remove all diacritics from a `str`, you can use a function like [Example 4-14](#ex_shave_marks).
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 要从 `str` 中移除所有变音符号，可以使用类似 [示例 4-14](#ex_shave_marks) 的函数。
- en: 'Example 4-14\. simplify.py: function to remove all combining marks'
  id: totrans-329
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '示例 4-14\. simplify.py: 用于移除所有组合标记的函数'
- en: '[PRE28]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[![1](assets/1.png)](#co_unicode_text_versus_bytes_CO8-1)'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_unicode_text_versus_bytes_CO8-1)'
- en: Decompose all characters into base characters and combining marks.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有字符分解为基本字符和组合标记。
- en: '[![2](assets/2.png)](#co_unicode_text_versus_bytes_CO8-2)'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_unicode_text_versus_bytes_CO8-2)'
- en: Filter out all combining marks.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤掉所有组合标记。
- en: '[![3](assets/3.png)](#co_unicode_text_versus_bytes_CO8-3)'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_unicode_text_versus_bytes_CO8-3)'
- en: Recompose all characters.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 重新组合所有字符。
- en: '[Example 4-15](#ex_shave_marks_demo) shows a couple of uses of `shave_marks`.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 4-15](#ex_shave_marks_demo) 展示了几种使用 `shave_marks` 的方法。'
- en: Example 4-15\. Two examples using `shave_marks` from [Example 4-14](#ex_shave_marks)
  id: totrans-338
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-15\. 使用 `shave_marks` 的两个示例，来自 [示例 4-14](#ex_shave_marks)
- en: '[PRE29]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[![1](assets/1.png)](#co_unicode_text_versus_bytes_CO9-1)'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_unicode_text_versus_bytes_CO9-1)'
- en: Only the letters “è”, “ç”, and “í” were replaced.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 仅字母 “è”、“ç” 和 “í” 被替换。
- en: '[![2](assets/2.png)](#co_unicode_text_versus_bytes_CO9-2)'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_unicode_text_versus_bytes_CO9-2)'
- en: Both “έ” and “é” were replaced.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: “έ” 和 “é” 都被替换了。
- en: The function `shave_marks` from [Example 4-14](#ex_shave_marks) works all right,
    but maybe it goes too far. Often the reason to remove diacritics is to change
    Latin text to pure ASCII, but `shave_marks` also changes non-Latin characters—like
    Greek letters—which will never become ASCII just by losing their accents. So it
    makes sense to analyze each base character and to remove attached marks only if
    the base character is a letter from the Latin alphabet. This is what [Example 4-16](#ex_shave_marks_latin)
    does.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 [示例 4-14](#ex_shave_marks) 的函数 `shave_marks` 运行良好，但也许它做得太过了。通常移除变音符号的原因是将拉丁文本更改为纯
    ASCII，但 `shave_marks` 也会改变非拉丁字符，比如希腊字母，这些字母仅仅通过失去重音就不会变成 ASCII。因此，有必要分析每个基本字符，并仅在基本字符是拉丁字母时才移除附加标记。这就是
    [示例 4-16](#ex_shave_marks_latin) 的作用。
- en: Example 4-16\. Function to remove combining marks from Latin characters (import
    statements are omitted as this is part of the simplify.py module from [Example 4-14](#ex_shave_marks))
  id: totrans-345
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-16\. 从拉丁字符中移除组合标记的函数（省略了导入语句，因为这是来自 [示例 4-14](#ex_shave_marks) 的 simplify.py
    模块的一部分）
- en: '[PRE30]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[![1](assets/1.png)](#co_unicode_text_versus_bytes_CO10-1)'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_unicode_text_versus_bytes_CO10-1)'
- en: Decompose all characters into base characters and combining marks.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有字符分解为基本字符和组合标记。
- en: '[![2](assets/2.png)](#co_unicode_text_versus_bytes_CO10-2)'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_unicode_text_versus_bytes_CO10-2)'
- en: Skip over combining marks when base character is Latin.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 当基本字符为拉丁字符时，跳过组合标记。
- en: '[![3](assets/3.png)](#co_unicode_text_versus_bytes_CO10-3)'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_unicode_text_versus_bytes_CO10-3)'
- en: Otherwise, keep current character.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，保留当前字符。
- en: '[![4](assets/4.png)](#co_unicode_text_versus_bytes_CO10-4)'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_unicode_text_versus_bytes_CO10-4)'
- en: Detect new base character and determine if it’s Latin.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 检测新的基本字符，并确定它是否为拉丁字符。
- en: '[![5](assets/5.png)](#co_unicode_text_versus_bytes_CO10-5)'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_unicode_text_versus_bytes_CO10-5)'
- en: Recompose all characters.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 重新组合所有字符。
- en: An even more radical step would be to replace common symbols in Western texts
    (e.g., curly quotes, em dashes, bullets, etc.) into `ASCII` equivalents. This
    is what the function `asciize` does in [Example 4-17](#ex_asciize).
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 更激进的一步是将西方文本中的常见符号（例如，卷曲引号、破折号、项目符号等）替换为`ASCII`等效符号。这就是[示例 4-17](#ex_asciize)中的`asciize`函数所做的。
- en: Example 4-17\. Transform some Western typographical symbols into ASCII (this
    snippet is also part of simplify.py from [Example 4-14](#ex_shave_marks))
  id: totrans-358
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-17\. 将一些西方排版符号转换为ASCII（此片段也是[示例 4-14](#ex_shave_marks)中`simplify.py`的一部分）
- en: '[PRE31]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[![1](assets/1.png)](#co_unicode_text_versus_bytes_CO11-1)'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_unicode_text_versus_bytes_CO11-1)'
- en: Build mapping table for char-to-char replacement.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 为字符替换构建映射表。
- en: '[![2](assets/2.png)](#co_unicode_text_versus_bytes_CO11-2)'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_unicode_text_versus_bytes_CO11-2)'
- en: Build mapping table for char-to-string replacement.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 为字符到字符串替换构建映射表。
- en: '[![3](assets/3.png)](#co_unicode_text_versus_bytes_CO11-3)'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_unicode_text_versus_bytes_CO11-3)'
- en: Merge mapping tables.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 合并映射表。
- en: '[![4](assets/4.png)](#co_unicode_text_versus_bytes_CO11-4)'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_unicode_text_versus_bytes_CO11-4)'
- en: '`dewinize` does not affect `ASCII` or `latin1` text, only the Microsoft additions
    to `latin1` in `cp1252`.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '`dewinize`不影响`ASCII`或`latin1`文本，只影响`cp1252`中的Microsoft附加内容。'
- en: '[![5](assets/5.png)](#co_unicode_text_versus_bytes_CO11-5)'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_unicode_text_versus_bytes_CO11-5)'
- en: Apply `dewinize` and remove diacritical marks.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 应用`dewinize`并移除变音符号。
- en: '[![6](assets/6.png)](#co_unicode_text_versus_bytes_CO11-6)'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](assets/6.png)](#co_unicode_text_versus_bytes_CO11-6)'
- en: Replace the Eszett with “ss” (we are not using case fold here because we want
    to preserve the case).
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 用“ss”替换Eszett（我们这里不使用大小写折叠，因为我们想保留大小写）。
- en: '[![7](assets/7.png)](#co_unicode_text_versus_bytes_CO11-7)'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '[![7](assets/7.png)](#co_unicode_text_versus_bytes_CO11-7)'
- en: Apply NFKC normalization to compose characters with their compatibility code
    points.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 对具有其兼容性代码点的字符进行NFKC规范化以组合字符。
- en: '[Example 4-18](#ex_asciize_demo) shows `asciize` in use.'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 4-18](#ex_asciize_demo)展示了`asciize`的使用。'
- en: Example 4-18\. Two examples using `asciize` from [Example 4-17](#ex_asciize)
  id: totrans-375
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-18\. 使用[示例 4-17](#ex_asciize)中的`asciize`的两个示例
- en: '[PRE32]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[![1](assets/1.png)](#co_unicode_text_versus_bytes_CO12-1)'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_unicode_text_versus_bytes_CO12-1)'
- en: '`dewinize` replaces curly quotes, bullets, and ™ (trademark symbol).'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '`dewinize`替换卷曲引号、项目符号和™（商标符号）。'
- en: '[![2](assets/2.png)](#co_unicode_text_versus_bytes_CO12-2)'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_unicode_text_versus_bytes_CO12-2)'
- en: '`asciize` applies `dewinize`, drops diacritics, and replaces the `''ß''`.'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '`asciize`应用`dewinize`，删除变音符号，并替换`''ß''`。'
- en: Warning
  id: totrans-381
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Different languages have their own rules for removing diacritics. For example,
    Germans change the `'ü'` into `'ue'`. Our `asciize` function is not as refined,
    so it may or not be suitable for your language. It works acceptably for Portuguese,
    though.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 不同语言有自己的去除变音符号的规则。例如，德语将`'ü'`改为`'ue'`。我们的`asciize`函数不够精细，因此可能不适合您的语言。但对葡萄牙语来说，它的效果还可以接受。
- en: To summarize, the functions in *simplify.py* go way beyond standard normalization
    and perform deep surgery on the text, with a good chance of changing its meaning.
    Only you can decide whether to go so far, knowing the target language, your users,
    and how the transformed text will be used.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，在`simplify.py`中的函数远远超出了标准规范化，并对文本进行了深度处理，有可能改变其含义。只有您可以决定是否走得这么远，了解目标语言、您的用户以及转换后的文本将如何使用。
- en: This wraps up our discussion of normalizing Unicode text.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对规范化Unicode文本的讨论。
- en: Now let’s sort out Unicode sorting.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来解决Unicode排序问题。
- en: Sorting Unicode Text
  id: totrans-386
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对Unicode文本进行排序
- en: Python sorts sequences of any type by comparing the items in each sequence one
    by one. For strings, this means comparing the code points. Unfortunately, this
    produces unacceptable results for anyone who uses non-ASCII characters.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: Python通过逐个比较每个序列中的项目来对任何类型的序列进行排序。对于字符串，这意味着比较代码点。不幸的是，这对于使用非ASCII字符的人来说产生了无法接受的结果。
- en: 'Consider sorting a list of fruits grown in Brazil:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑对在巴西种植的水果列表进行排序：
- en: '[PRE33]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Sorting rules vary for different locales, but in Portuguese and many languages
    that use the Latin alphabet, accents and cedillas rarely make a difference when
    sorting.^([8](ch04.html#idm46582485433232)) So “cajá” is sorted as “caja,” and
    must come before “caju.”
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 不同区域设置的排序规则不同，但在葡萄牙语和许多使用拉丁字母表的语言中，重音符号和塞迪利亚很少在排序时产生差异。^([8](ch04.html#idm46582485433232))
    因此，“cajá”被排序为“caja”，并且必须位于“caju”之前。
- en: 'The sorted `fruits` list should be:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 排序后的`fruits`列表应为：
- en: '[PRE34]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The standard way to sort non-ASCII text in Python is to use the `locale.strxfrm`
    function which, according to the [`locale` module docs](https://fpy.li/4-16),
    “transforms a string to one that can be used in locale-aware comparisons.”
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中对非ASCII文本进行排序的标准方法是使用`locale.strxfrm`函数，根据[`locale`模块文档](https://fpy.li/4-16)，“将一个字符串转换为可用于区域设置感知比较的字符串”。
- en: To enable `locale.strxfrm`, you must first set a suitable locale for your application,
    and pray that the OS supports it. The sequence of commands in [Example 4-19](#ex_locale_sort)
    may work for you.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用`locale.strxfrm`，您必须首先为您的应用程序设置一个合适的区域设置，并祈祷操作系统支持它。[示例 4-19](#ex_locale_sort)中的命令序列可能适用于您。
- en: 'Example 4-19\. *locale_sort.py*: using the `locale.strxfrm` function as the
    sort key'
  id: totrans-395
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-19\. *locale_sort.py*：使用`locale.strxfrm`函数作为排序键
- en: '[PRE35]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Running [Example 4-19](#ex_locale_sort) on GNU/Linux (Ubuntu 19.10) with the
    `pt_BR.UTF-8` locale installed, I get the correct result:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GNU/Linux（Ubuntu 19.10）上运行 [示例 4-19](#ex_locale_sort)，安装了 `pt_BR.UTF-8` 区域设置，我得到了正确的结果：
- en: '[PRE36]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: So you need to call `setlocale(LC_COLLATE, «your_locale»)` before using `locale.strxfrm`
    as the key when sorting.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在排序时需要在使用 `locale.strxfrm` 作为键之前调用 `setlocale(LC_COLLATE, «your_locale»)`。
- en: 'There are some caveats, though:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，还有一些注意事项：
- en: Because locale settings are global, calling `setlocale` in a library is not
    recommended. Your application or framework should set the locale when the process
    starts, and should not change it afterward.
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因为区域设置是全局的，不建议在库中调用 `setlocale`。您的应用程序或框架应该在进程启动时设置区域设置，并且不应该在之后更改它。
- en: 'The locale must be installed on the OS, otherwise `setlocale` raises a `locale.Error:
    unsupported locale setting` exception.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '操作系统必须安装区域设置，否则 `setlocale` 会引发 `locale.Error: unsupported locale setting`
    异常。'
- en: You must know how to spell the locale name.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您必须知道如何拼写区域设置名称。
- en: The locale must be correctly implemented by the makers of the OS. I was successful
    on Ubuntu 19.10, but not on macOS 10.14\. On macOS, the call `setlocale(LC_COLLATE,
    'pt_BR.UTF-8')` returns the string `'pt_BR.UTF-8'` with no complaints. But `sorted(fruits,
    key=locale.strxfrm)` produced the same incorrect result as `sorted(fruits)` did.
    I also tried the `fr_FR`, `es_ES`, and `de_DE` locales on macOS, but `locale.strxfrm`
    never did its job.^([9](ch04.html#idm46582485249344))
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区域设置必须由操作系统的制造商正确实现。我在 Ubuntu 19.10 上成功了，但在 macOS 10.14 上没有成功。在 macOS 上，调用 `setlocale(LC_COLLATE,
    'pt_BR.UTF-8')` 返回字符串 `'pt_BR.UTF-8'` 而没有任何投诉。但 `sorted(fruits, key=locale.strxfrm)`
    产生了与 `sorted(fruits)` 相同的不正确结果。我还在 macOS 上尝试了 `fr_FR`、`es_ES` 和 `de_DE` 区域设置，但
    `locale.strxfrm` 从未起作用。^([9](ch04.html#idm46582485249344))
- en: So the standard library solution to internationalized sorting works, but seems
    to be well supported only on GNU/Linux (perhaps also on Windows, if you are an
    expert). Even then, it depends on locale settings, creating deployment headaches.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，标准库提供的国际化排序解决方案有效，但似乎只在 GNU/Linux 上得到很好的支持（也许在 Windows 上也是如此，如果您是专家的话）。即使在那里，它也依赖于区域设置，会带来部署上的麻烦。
- en: 'Fortunately, there is a simpler solution: the *pyuca* library, available on
    *PyPI*.'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有一个更简单的解决方案：*pyuca* 库，可以在 *PyPI* 上找到。
- en: Sorting with the Unicode Collation Algorithm
  id: totrans-407
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Unicode Collation Algorithm 进行排序
- en: James Tauber, prolific Django contributor, must have felt the pain and created
    [*pyuca*](https://fpy.li/4-17), a pure-Python implementation of the Unicode Collation
    Algorithm (UCA). [Example 4-20](#ex_pyuca_sort) shows how easy it is to use.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: James Tauber，多产的 Django 贡献者，一定感受到了痛苦，并创建了 [*pyuca*](https://fpy.li/4-17)，这是
    Unicode Collation Algorithm（UCA）的纯 Python 实现。[示例 4-20](#ex_pyuca_sort) 展示了它的易用性。
- en: Example 4-20\. Using the `pyuca.Collator.sort_key` method
  id: totrans-409
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-20。使用 `pyuca.Collator.sort_key` ���法
- en: '[PRE37]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: This is simple and works on GNU/Linux, macOS, and Windows, at least with my
    small sample.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法简单易行，在 GNU/Linux、macOS 和 Windows 上都可以运行，至少在我的小样本中是这样的。
- en: '`pyuca` does not take the locale into account. If you need to customize the
    sorting, you can provide the path to a custom collation table to the `Collator()`
    constructor. Out of the box, it uses [*allkeys.txt*](https://fpy.li/4-18), which
    is bundled with the project. That’s just a copy of the [Default Unicode Collation
    Element Table from *Unicode.org*](https://fpy.li/4-19).'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '`pyuca` 不考虑区域设置。如果需要自定义排序，可以向 `Collator()` 构造函数提供自定义排序表的路径。默认情况下，它使用 [*allkeys.txt*](https://fpy.li/4-18)，这是项目捆绑的。这只是
    [来自 *Unicode.org* 的默认 Unicode Collation Element Table 的副本](https://fpy.li/4-19)。'
- en: 'PyICU: Miro’s Recommendation for Unicode Sorting'
  id: totrans-413
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyICU：Miro 的 Unicode 排序推荐
- en: (Tech reviewer Miroslav Šedivý is a polyglot and an expert on Unicode. This
    is what he wrote about *pyuca*.)
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: （技术审阅员 Miroslav Šedivý 是一位多语言使用者，也是 Unicode 方面的专家。这是他对 *pyuca* 的评价。）
- en: '*pyuca* has one sorting algorithm that does not respect the sorting order in
    individual languages. For instance, Ä in German is between A and B, while in Swedish
    it comes after Z. Have a look at [PyICU](https://fpy.li/4-20) that works like
    locale without changing the locale of the process. It is also needed if you want
    to change the case of iİ/ıI in Turkish. PyICU includes an extension that must
    be compiled, so it may be harder to install in some systems than *pyuca*, which
    is just Python.'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '*pyuca* 有一个排序算法，不考虑各个语言中的排序顺序。例如，在德语中 Ä 在 A 和 B 之间，而在瑞典语中它在 Z 之后。看看 [PyICU](https://fpy.li/4-20)，它像区域设置一样工作，而不会更改进程的区域设置。如果您想要在土耳其语中更改
    iİ/ıI 的大小写，也需要它。PyICU 包含一个必须编译的扩展，因此在某些系统中安装可能比只是 Python 的 *pyuca* 更困难。'
- en: By the way, that collation table is one of the many data files that comprise
    the Unicode database, our next subject.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一句，那个排序表是组成 Unicode 数据库的许多数据文件之一，我们下一个主题。
- en: The Unicode Database
  id: totrans-417
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Unicode 数据库
- en: The Unicode standard provides an entire database—in the form of several structured
    text files—that includes not only the table mapping code points to character names,
    but also metadata about the individual characters and how they are related. For
    example, the Unicode database records whether a character is printable, is a letter,
    is a decimal digit, or is some other numeric symbol. That’s how the `str` methods
    `isalpha`, `isprintable`, `isdecimal`, and `isnumeric` work. `str.casefold` also
    uses information from a Unicode table.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: Unicode 标准提供了一个完整的数据库，以几个结构化的文本文件的形式存在，其中不仅包括将代码点映射到字符名称的表，还包括有关各个字符及其相关性的元数据。例如，Unicode
    数据库记录了字符是否可打印、是否为字母、是否为十进制数字，或者是否为其他数字符号。这就是 `str` 方法 `isalpha`、`isprintable`、`isdecimal`
    和 `isnumeric` 的工作原理。`str.casefold` 也使用了来自 Unicode 表的信息。
- en: Note
  id: totrans-419
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'The `unicodedata.category(char)` function returns the two-letter category of
    `char` from the Unicode database. The higher-level `str` methods are easier to
    use. For example, [`label.isalpha()`](https://fpy.li/4-21) returns `True` if every
    character in `label` belongs to one of these categories: `Lm`, `Lt`, `Lu`, `Ll`,
    or `Lo`. To learn what those codes mean, see [“General Category”](https://fpy.li/4-22)
    in the English Wikipedia’s [“Unicode character property” article](https://fpy.li/4-23).'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '`unicodedata.category(char)` 函数从 Unicode 数据库返回 `char` 的两个字母类别。更高级别的 `str` 方法更容易使用。例如，[`label.isalpha()`](https://fpy.li/4-21)
    如果 `label` 中的每个字符属于以下类别之一，则返回 `True`：`Lm`、`Lt`、`Lu`、`Ll` 或 `Lo`。要了解这些代码的含义，请参阅英文维基百科的
    [“Unicode 字符属性”文章](https://fpy.li/4-23) 中的 [“通用类别”](https://fpy.li/4-22)。 '
- en: Finding Characters by Name
  id: totrans-421
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 按名称查找字符
- en: The `unicodedata` module has functions to retrieve character metadata, including
    `unicodedata.name()`, which returns a character’s official name in the standard.
    [Figure 4-5](#unicodedata_name_fig) demonstrates that function.^([10](ch04.html#idm46582457454960))
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '`unicodedata` 模块包括检索字符元数据的函数，包括 `unicodedata.name()`，它返回标准中字符的官方名称。[图 4-5](#unicodedata_name_fig)
    展示了该函数的使用。^([10](ch04.html#idm46582457454960))'
- en: '![Exploring unicodedata.name in the Python console](assets/flpy_0405.png)'
  id: totrans-423
  prefs: []
  type: TYPE_IMG
  zh: '![在 Python 控制台中探索 unicodedata.name](assets/flpy_0405.png)'
- en: Figure 4-5\. Exploring `unicodedata.name()` in the Python console.
  id: totrans-424
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-5\. 在 Python 控制台中探索 `unicodedata.name()`。
- en: You can use the `name()` function to build apps that let users search for characters
    by name. [Figure 4-6](#cf_demo_fig) demonstrates the *cf.py* command-line script
    that takes one or more words as arguments, and lists the characters that have
    those words in their official Unicode names. The full source code for *cf.py*
    is in [Example 4-21](#ex_cfpy).
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 `name()` 函数构建应用程序，让用户可以按名称搜索字符。[图 4-6](#cf_demo_fig) 展示了 *cf.py* 命令行脚本，它接受一个或多个单词作为参数，并列出具有这些单词在官方
    Unicode 名称中的字符。*cf.py* 的完整源代码在 [示例 4-21](#ex_cfpy) 中。
- en: '![Using cf.py to find smiling cats.](assets/flpy_0406.png)'
  id: totrans-426
  prefs: []
  type: TYPE_IMG
  zh: '![使用 cf.py 查找微笑的猫。](assets/flpy_0406.png)'
- en: Figure 4-6\. Using *cf.py* to find smiling cats.
  id: totrans-427
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-6\. 使用 *cf.py* 查找微笑的猫。
- en: Warning
  id: totrans-428
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Emoji support varies widely across operating systems and apps. In recent years
    the macOS terminal offers the best support for emojis, followed by modern GNU/Linux
    graphic terminals. Windows *cmd.exe* and PowerShell now support Unicode output,
    but as I write this section in January 2020, they still don’t display emojis—at
    least not “out of the box.” Tech reviewer Leonardo Rochael told me about a new,
    open source [Windows Terminal by Microsoft](https://fpy.li/4-24), which may have
    better Unicode support than the older Microsoft consoles. I did not have time
    to try it.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 表情符号在各种操作系统和应用程序中的支持差异很大。近年来，macOS 终端提供了最好的表情符号支持，其次是现代 GNU/Linux 图形终端。Windows
    *cmd.exe* 和 PowerShell 现在支持 Unicode 输出，但截至我在 2020 年 1 月撰写本节时，它们仍然不显示表情符号——至少不是“开箱即用”。技术评论员莱昂纳多·罗查尔告诉我有一个新的、由微软推出的开源
    [Windows 终端](https://fpy.li/4-24)，它可能比旧的微软控制台具有更好的 Unicode 支持。我还没有时间尝试。
- en: In [Example 4-21](#ex_cfpy), note the `if` statement in the `find` function
    using the `.issubset()` method to quickly test whether all the words in the `query`
    set appear in the list of words built from the character’s name. Thanks to Python’s
    rich set API, we don’t need a nested `for` loop and another `if` to implement
    this check.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [示例 4-21](#ex_cfpy) 中，请注意 `find` 函数中的 `if` 语句，使用 `.issubset()` 方法快速测试 `query`
    集合中的所有单词是否出现在从字符名称构建的单词列表中。由于 Python 丰富的集合 API，我们不需要嵌套的 `for` 循环和另一个 `if` 来实现此检查。
- en: 'Example 4-21\. cf.py: the character finder utility'
  id: totrans-431
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-21\. cf.py：字符查找实用程序
- en: '[PRE38]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[![1](assets/1.png)](#co_unicode_text_versus_bytes_CO13-1)'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_unicode_text_versus_bytes_CO13-1)'
- en: Set defaults for the range of code points to search.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 设置搜索的代码点范围的默认值。
- en: '[![2](assets/2.png)](#co_unicode_text_versus_bytes_CO13-2)'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_unicode_text_versus_bytes_CO13-2)'
- en: '`find` accepts `query_words` and optional keyword-only arguments to limit the
    range of the search, to facilitate testing.'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '`find` 接受 `query_words` 和可选的关键字参数来限制搜索范围，以便进行测试。'
- en: '[![3](assets/3.png)](#co_unicode_text_versus_bytes_CO13-3)'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_unicode_text_versus_bytes_CO13-3)'
- en: Convert `query_words` into a set of uppercased strings.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `query_words` 转换为大写字符串集合。
- en: '[![4](assets/4.png)](#co_unicode_text_versus_bytes_CO13-4)'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_unicode_text_versus_bytes_CO13-4)'
- en: Get the Unicode character for `code`.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 获取 `code` 的 Unicode 字符。
- en: '[![5](assets/5.png)](#co_unicode_text_versus_bytes_CO13-5)'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_unicode_text_versus_bytes_CO13-5)'
- en: Get the name of the character, or `None` if the code point is unassigned.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 获取字符的名称，如果代码点未分配，则返回 `None`。
- en: '[![6](assets/6.png)](#co_unicode_text_versus_bytes_CO13-6)'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](assets/6.png)](#co_unicode_text_versus_bytes_CO13-6)'
- en: If there is a name, split it into a list of words, then check that the `query`
    set is a subset of that list.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有名称，将其拆分为单词列表，然后检查 `query` 集合是否是该列表的子集。
- en: '[![7](assets/7.png)](#co_unicode_text_versus_bytes_CO13-7)'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '[![7](assets/7.png)](#co_unicode_text_versus_bytes_CO13-7)'
- en: Print out line with code point in `U+9999` format, the character, and its name.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 打印出以 `U+9999` 格式的代码点、字符和其名称的行。
- en: The `unicodedata` module has other interesting functions. Next, we’ll see a
    few that are related to getting information from characters that have numeric
    meaning.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '`unicodedata` 模块还有其他有趣的函数。接下来，我们将看到一些与获取具有数字含义的字符信息相关的函数。'
- en: Numeric Meaning of Characters
  id: totrans-448
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 字符的数字含义
- en: The `unicodedata` module includes functions to check whether a Unicode character
    represents a number and, if so, its numeric value for humans—as opposed to its
    code point number. [Example 4-22](#ex_numerics_demo) shows the use of `unicodedata.name()`
    and `unicodedata.numeric()`, along with the `.isdecimal()` and `.isnumeric()`
    methods of `str`.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: '`unicodedata` 模块包括函数，用于检查 Unicode 字符是否表示数字，如果是，则返回其人类的数值，而不是其代码点数。[示例 4-22](#ex_numerics_demo)
    展示了 `unicodedata.name()` 和 `unicodedata.numeric()` 的使用，以及 `str` 的 `.isdecimal()`
    和 `.isnumeric()` 方法。'
- en: Example 4-22\. Demo of Unicode database numerical character metadata (callouts
    describe each column in the output)
  id: totrans-450
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-22\. Unicode 数据库数字字符元数据演示（标注描述输出中的每列）
- en: '[PRE39]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[![1](assets/1.png)](#co_unicode_text_versus_bytes_CO14-1)'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_unicode_text_versus_bytes_CO14-1)'
- en: Code point in `U+0000` format.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 以`U+0000`格式的代码点。
- en: '[![2](assets/2.png)](#co_unicode_text_versus_bytes_CO14-2)'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_unicode_text_versus_bytes_CO14-2)'
- en: Character centralized in a `str` of length 6.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 字符在长度为6的`str`中居中。
- en: '[![3](assets/3.png)](#co_unicode_text_versus_bytes_CO14-3)'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_unicode_text_versus_bytes_CO14-3)'
- en: Show `re_dig` if character matches the `r'\d'` regex.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 如果字符匹配`r'\d'`正则表达式，则显示`re_dig`。
- en: '[![4](assets/4.png)](#co_unicode_text_versus_bytes_CO14-4)'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_unicode_text_versus_bytes_CO14-4)'
- en: Show `isdig` if `char.isdigit()` is `True`.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`char.isdigit()`为`True`，则显示`isdig`。
- en: '[![5](assets/5.png)](#co_unicode_text_versus_bytes_CO14-5)'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_unicode_text_versus_bytes_CO14-5)'
- en: Show `isnum` if `char.isnumeric()` is `True`.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`char.isnumeric()`为`True`，则显示`isnum`。
- en: '[![6](assets/6.png)](#co_unicode_text_versus_bytes_CO14-6)'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](assets/6.png)](#co_unicode_text_versus_bytes_CO14-6)'
- en: Numeric value formatted with width 5 and 2 decimal places.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 数值格式化为宽度为5和2位小数。
- en: '[![7](assets/7.png)](#co_unicode_text_versus_bytes_CO14-7)'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: '[![7](assets/7.png)](#co_unicode_text_versus_bytes_CO14-7)'
- en: Unicode character name.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: Unicode字符名称。
- en: Running [Example 4-22](#ex_numerics_demo) gives you [Figure 4-7](#numerics_demo_fig),
    if your terminal font has all those glyphs.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 运行[示例 4-22](#ex_numerics_demo)会给你[图 4-7](#numerics_demo_fig)，如果你的终端字体有所有这些字形。
- en: '![Numeric characters screenshot](assets/flpy_0407.png)'
  id: totrans-467
  prefs: []
  type: TYPE_IMG
  zh: '![数字字符截图](assets/flpy_0407.png)'
- en: Figure 4-7\. macOS terminal showing numeric characters and metadata about them;
    `re_dig` means the character matches the regular expression `r'\d'`.
  id: totrans-468
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-7\. macOS终端显示数字字符及其元数据；`re_dig`表示字符匹配正则表达式`r'\d'`。
- en: The sixth column of [Figure 4-7](#numerics_demo_fig) is the result of calling
    `unicodedata.numeric(char)` on the character. It shows that Unicode knows the
    numeric value of symbols that represent numbers. So if you want to create a spreadsheet
    application that supports Tamil digits or Roman numerals, go for it!
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-7](#numerics_demo_fig)的第六列是在字符上调用`unicodedata.numeric(char)`的结果。它显示Unicode知道代表数字的符号的数值。因此，如果你想创建支持泰米尔数字或罗马数字的电子表格应用程序，就去做吧！'
- en: '[Figure 4-7](#numerics_demo_fig) shows that the regular expression `r''\d''`
    matches the digit “1” and the Devanagari digit 3, but not some other characters
    that are considered digits by the `isdigit` function. The `re` module is not as
    savvy about Unicode as it could be. The new `regex` module available on PyPI was
    designed to eventually replace `re` and provides better Unicode support.^([11](ch04.html#idm46582456966128))
    We’ll come back to the `re` module in the next section.'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-7](#numerics_demo_fig)显示正则表达式`r''\d''`匹配数字“1”和梵文数字3，但不匹配一些其他被`isdigit`函数视为数字的字符。`re`模块对Unicode的了解不如它本应该的那样深入。PyPI上提供的新`regex`模块旨在最终取代`re`，并提供更好的Unicode支持。^([11](ch04.html#idm46582456966128))我们将在下一节回到`re`模块。'
- en: Throughout this chapter we’ve used several `unicodedata` functions, but there
    are many more we did not cover. See the standard library documentation for the
    [`unicodedata` module](https://fpy.li/4-25).
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: '在本章中，我们使用了几个`unicodedata`函数，但还有许多我们没有涉及的函数。查看标准库文档中的[`unicodedata`模块](https://fpy.li/4-25)。 '
- en: Next we’ll take a quick look at dual-mode APIs offering functions that accept
    `str` or `bytes` arguments with special handling depending on the type.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将快速查看双模式API，提供接受`str`或`bytes`参数的函数，并根据类型进行特殊处理。
- en: Dual-Mode str and bytes APIs
  id: totrans-473
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 双模式str和bytes API
- en: Python’s standard library has functions that accept `str` or `bytes` arguments
    and behave differently depending on the type. Some examples can be found in the
    `re` and `os` modules.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: Python标准库有接受`str`或`bytes`参数并根据类型表现不同的函数。一些示例可以在`re`和`os`模块中找到。
- en: str Versus bytes in Regular Expressions
  id: totrans-475
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则表达式中的str与bytes
- en: If you build a regular expression with `bytes`, patterns such as `\d` and `\w`
    only match ASCII characters; in contrast, if these patterns are given as `str`,
    they match Unicode digits or letters beyond ASCII. [Example 4-23](#ex_re_demo)
    and [Figure 4-8](#fig_re_demo) compare how letters, ASCII digits, superscripts,
    and Tamil digits are matched by `str` and `bytes` patterns.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 如果用`bytes`构建正则表达式，模式如`\d`和`\w`只匹配ASCII字符；相反，如果这些模式给定为`str`，它们将匹配ASCII之外的Unicode数字或字母。[示例 4-23](#ex_re_demo)和[图 4-8](#fig_re_demo)比较了`str`和`bytes`模式如何匹配字母、ASCII数字、上标和泰米尔数字。
- en: 'Example 4-23\. ramanujan.py: compare behavior of simple `str` and `bytes` regular
    expressions'
  id: totrans-477
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例4-23\. ramanujan.py：比较简单`str`和`bytes`正则表达式的行为
- en: '[PRE40]'
  id: totrans-478
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[![1](assets/1.png)](#co_unicode_text_versus_bytes_CO15-1)'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_unicode_text_versus_bytes_CO15-1)'
- en: The first two regular expressions are of the `str` type.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个正则表达式是`str`类型。
- en: '[![2](assets/2.png)](#co_unicode_text_versus_bytes_CO15-2)'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_unicode_text_versus_bytes_CO15-2)'
- en: The last two are of the `bytes` type.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 最后两个是`bytes`类型。
- en: '[![3](assets/3.png)](#co_unicode_text_versus_bytes_CO15-3)'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_unicode_text_versus_bytes_CO15-3)'
- en: Unicode text to search, containing the Tamil digits for `1729` (the logical
    line continues until the right parenthesis token).
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: Unicode文本搜索，包含泰米尔数字`1729`（逻辑行一直延续到右括号标记）。
- en: '[![4](assets/4.png)](#co_unicode_text_versus_bytes_CO15-4)'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_unicode_text_versus_bytes_CO15-4)'
- en: This string is joined to the previous one at compile time (see [“2.4.2\. String
    literal concatenation”](https://fpy.li/4-26) in *The Python Language Reference*).
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 此字符串在编译时与前一个字符串连接（参见[“2.4.2\. 字符串文字连接”](https://fpy.li/4-26)中的*Python语言参考*）。
- en: '[![5](assets/5.png)](#co_unicode_text_versus_bytes_CO15-5)'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_unicode_text_versus_bytes_CO15-5)'
- en: A `bytes` string is needed to search with the `bytes` regular expressions.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 需要使用`bytes`正则表达式来搜索`bytes`字符串。
- en: '[![6](assets/6.png)](#co_unicode_text_versus_bytes_CO15-6)'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: '![6](assets/6.png)](#co_unicode_text_versus_bytes_CO15-6)'
- en: The `str` pattern `r'\d+'` matches the Tamil and ASCII digits.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: '`str`模式`r''\d+''`匹配泰米尔和ASCII数字。'
- en: '[![7](assets/7.png)](#co_unicode_text_versus_bytes_CO15-7)'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: '[![7](assets/7.png)](#co_unicode_text_versus_bytes_CO15-7)'
- en: The `bytes` pattern `rb'\d+'` matches only the ASCII bytes for digits.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: '`bytes`模式`rb''\d+''`仅匹配数字的ASCII字节。'
- en: '[![8](assets/8.png)](#co_unicode_text_versus_bytes_CO15-8)'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: '[![8](assets/8.png)](#co_unicode_text_versus_bytes_CO15-8)'
- en: The `str` pattern `r'\w+'` matches the letters, superscripts, Tamil, and ASCII
    digits.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: '`str`模式`r''\w+''`匹配字母、上标、泰米尔语和 ASCII 数字。'
- en: '[![9](assets/9.png)](#co_unicode_text_versus_bytes_CO15-9)'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: '[![9](assets/9.png)](#co_unicode_text_versus_bytes_CO15-9)'
- en: The `bytes` pattern `rb'\w+'` matches only the ASCII bytes for letters and digits.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: '`bytes`模式`rb''\w+''`仅匹配字母和数字的 ASCII 字节。'
- en: '![Output of ramanujan.py](assets/flpy_0408.png)'
  id: totrans-497
  prefs: []
  type: TYPE_IMG
  zh: '![ramanujan.py 的输出](assets/flpy_0408.png)'
- en: Figure 4-8\. Screenshot of running ramanujan.py from [Example 4-23](#ex_re_demo).
  id: totrans-498
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-8。从[示例 4-23](#ex_re_demo)运行 ramanujan.py 的屏幕截图。
- en: '[Example 4-23](#ex_re_demo) is a trivial example to make one point: you can
    use regular expressions on `str` and `bytes`, but in the second case, bytes outside
    the ASCII range are treated as nondigits and nonword characters.'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 4-23](#ex_re_demo)是一个简单的例子，用来说明一个观点：你可以在`str`和`bytes`上使用正则表达式，但在第二种情况下，ASCII
    范围之外的字节被视为非数字和非单词字符。'
- en: For `str` regular expressions, there is a `re.ASCII` flag that makes `\w`, `\W`,
    `\b`, `\B`, `\d`, `\D`, `\s`, and `\S` perform ASCII-only matching. See the [documentation
    of the `re` module](https://fpy.li/4-27) for full details.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`str`正则表达式，有一个`re.ASCII`标志，使得`\w`、`\W`、`\b`、`\B`、`\d`、`\D`、`\s`和`\S`只执行 ASCII
    匹配。详细信息请参阅[re 模块的文档](https://fpy.li/4-27)。
- en: Another important dual-mode module is `os`.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的双模块是`os`。
- en: str Versus bytes in os Functions
  id: totrans-502
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '`os`函数中的 str 与 bytes'
- en: The GNU/Linux kernel is not Unicode savvy, so in the real world you may find
    filenames made of byte sequences that are not valid in any sensible encoding scheme,
    and cannot be decoded to `str`. File servers with clients using a variety of OSes
    are particularly prone to this problem.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: GNU/Linux 内核不支持 Unicode，因此在现实世界中，您可能会发现由字节序列组成的文件名，这些文件名在任何明智的编码方案中都无效，并且无法解码为`str`。使用各种操作系统的客户端的文件服务器特别容易出现这个问题。
- en: In order to work around this issue, all `os` module functions that accept filenames
    or pathnames take arguments as `str` or `bytes`. If one such function is called
    with a `str` argument, the argument will be automatically converted using the
    codec named by `sys.getfilesystemencoding()`, and the OS response will be decoded
    with the same codec. This is almost always what you want, in keeping with the
    Unicode sandwich best practice.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，所有接受文件名或��径名的`os`模块函数都以`str`或`bytes`形式接受参数。如果调用这样的函数时使用`str`参数，参数将自动使用`sys.getfilesystemencoding()`命名的编解码器进行转换，并且
    OS 响应将使用相同的编解码器进行解码。这几乎总是您想要的，符合 Unicode 三明治最佳实践。
- en: But if you must deal with (and perhaps fix) filenames that cannot be handled
    in that way, you can pass `bytes` arguments to the `os` functions to get `bytes`
    return values. This feature lets you deal with any file or pathname, no matter
    how many gremlins you may find. See [Example 4-24](#ex_listdir1).
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果您必须处理（或者可能修复）无法以这种方式处理的文件名，您可以将`bytes`参数传递给`os`函数以获得`bytes`返回值。这个功能让您可以处理任何文件或路径名，无论您可能遇到多少小精灵。请参阅[示例
    4-24](#ex_listdir1)。
- en: Example 4-24\. `listdir` with `str` and `bytes` arguments and results
  id: totrans-506
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-24。`listdir`使用`str`和`bytes`参数和结果
- en: '[PRE41]'
  id: totrans-507
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[![1](assets/1.png)](#co_unicode_text_versus_bytes_CO16-1)'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_unicode_text_versus_bytes_CO16-1)'
- en: The second filename is “digits-of-π.txt” (with the Greek letter pi).
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个文件名是“digits-of-π.txt”（带有希腊字母π）。
- en: '[![2](assets/2.png)](#co_unicode_text_versus_bytes_CO16-2)'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_unicode_text_versus_bytes_CO16-2)'
- en: 'Given a `byte` argument, `listdir` returns filenames as bytes: `b''\xcf\x80''`
    is the UTF-8 encoding of the Greek letter pi.'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个`byte`参数，`listdir`以字节形式返回文件名：`b'\xcf\x80'`是希腊字母π的 UTF-8 编码。
- en: To help with manual handling of `str` or `bytes` sequences that are filenames
    or pathnames, the `os` module provides special encoding and decoding functions
    `os.fsencode(name_or_path)` and `os.fsdecode(name_or_path)`. Both of these functions
    accept an argument of type `str`, `bytes`, or an object implementing the `os.PathLike`
    interface since Python 3.6.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助处理作为文件名或路径名的`str`或`bytes`序列，`os`模块提供了特殊的编码和解码函数`os.fsencode(name_or_path)`和`os.fsdecode(name_or_path)`。自
    Python 3.6 起，这两个函数都接受`str`、`bytes`或实现`os.PathLike`接口的对象作为参数。
- en: Unicode is a deep rabbit hole. Time to wrap up our exploration of `str` and
    `bytes`.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: Unicode 是一个深奥的领域。是时候结束我们对`str`和`bytes`的探索了。
- en: Chapter Summary
  id: totrans-514
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 章节总结
- en: We started the chapter by dismissing the notion that `1 character == 1 byte`.
    As the world adopts Unicode, we need to keep the concept of text strings separated
    from the binary sequences that represent them in files, and Python 3 enforces
    this separation.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章开始时否定了`1 个字符 == 1 个字节`的概念。随着世界采用 Unicode，我们需要将文本字符串的概念与文件中表示它们的二进制序列分开，而
    Python 3 强制执行这种分离。
- en: After a brief overview of the binary sequence data types—`bytes`, `bytearray`,
    and `memoryview`—we jumped into encoding and decoding, with a sampling of important
    codecs, followed by approaches to prevent or deal with the infamous `UnicodeEncodeError`,
    `UnicodeDecodeError`, and the `SyntaxError` caused by wrong encoding in Python
    source files.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 在简要概述二进制序列数据类型——`bytes`、`bytearray`和`memoryview`后，我们开始了编码和解码，列举了一些重要的编解码器，然后介绍了如何防止或处理由
    Python 源文件中错误编码引起的臭名昭著的`UnicodeEncodeError`、`UnicodeDecodeError`和`SyntaxError`。
- en: 'We then considered the theory and practice of encoding detection in the absence
    of metadata: in theory, it can’t be done, but in practice the Chardet package
    pulls it off pretty well for a number of popular encodings. Byte order marks were
    then presented as the only encoding hint commonly found in UTF-16 and UTF-32 files—sometimes
    in UTF-8 files as well.'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有元数据的情况下考虑编码检测的理论和实践：理论上是不可能的，但实际上 Chardet 软件包对一些流行的编码做得相当不错。然后介绍了字节顺序标记作为
    UTF-16 和 UTF-32 文件中唯一常见的编码提示，有时也会在 UTF-8 文件中找到。
- en: 'In the next section, we demonstrated opening text files, an easy task except
    for one pitfall: the `encoding=` keyword argument is not mandatory when you open
    a text file, but it should be. If you fail to specify the encoding, you end up
    with a program that manages to generate “plain text” that is incompatible across
    platforms, due to conflicting default encodings. We then exposed the different
    encoding settings that Python uses as defaults and how to detect them. A sad realization
    for Windows users is that these settings often have distinct values within the
    same machine, and the values are mutually incompatible; GNU/Linux and macOS users,
    in contrast, live in a happier place where UTF-8 is the default pretty much everywhere.'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们演示了如何打开文本文件，这是一个简单的任务，除了一个陷阱：当你打开文本文件时，`encoding=` 关键字参数不是强制的，但应该是。如果你未指定编码，你最终会得到一个在不同平台上不兼容的“纯文本”生成程序，这是由于冲突的默认编码。然后我们揭示了
    Python 使用的不同编码设置作为默认值以及如何检测它们。对于 Windows 用户来说，一个令人沮丧的认识是这些设置在同一台机器内往往具有不同的值，并且这些值是相互不兼容的；相比之下，GNU/Linux
    和 macOS 用户生活在一个更幸福的地方，UTF-8 几乎是默认编码。
- en: 'Unicode provides multiple ways of representing some characters, so normalizing
    is a prerequisite for text matching. In addition to explaining normalization and
    case folding, we presented some utility functions that you may adapt to your needs,
    including drastic transformations like removing all accents. We then saw how to
    sort Unicode text correctly by leveraging the standard `locale` module—with some
    caveats—and an alternative that does not depend on tricky locale configurations:
    the external *pyuca* package.'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: Unicode 提供了多种表示某些字符的方式，因此规范化是文本匹配的先决条件。除了解释规范化和大小写折叠外，我们还提供了一些实用函数，您可以根据自己的需求进行调整，包括像删除所有重音这样的彻底转换。然后我们看到如何通过利用标准的
    `locale` 模块正确对 Unicode 文本进行排序——带有一些注意事项——以及一个不依赖于棘���的 locale 配置的替代方案：外部的 *pyuca*
    包。
- en: We leveraged the Unicode database to program a command-line utility to search
    for characters by name—in 28 lines of code, thanks to the power of Python. We
    glanced at other Unicode metadata, and had a brief overview of dual-mode APIs
    where some functions can be called with `str` or `bytes` arguments, producing
    different results.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用 Unicode 数据库编写了一个命令行实用程序，通过名称搜索字符——感谢 Python 的强大功能，只需 28 行代码。我们还简要介绍了其他
    Unicode 元数据，并对一些双模式 API 进行了概述，其中一些函数可以使用 `str` 或 `bytes` 参数调用，产生不同的结果。
- en: Further Reading
  id: totrans-521
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: Ned Batchelder’s 2012 PyCon US talk [“Pragmatic Unicode, or, How Do I Stop the
    Pain?”](https://fpy.li/4-28) was outstanding. Ned is so professional that he provides
    a full transcript of the talk along with the slides and video.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: Ned Batchelder 在 2012 年 PyCon US 的演讲[“实用 Unicode，或者，我如何停止痛苦？”](https://fpy.li/4-28)非常出色。Ned
    是如此专业，他提供了演讲的完整文本以及幻灯片和视频。
- en: '“Character encoding and Unicode in Python: How to (╯°□°)╯︵ ┻━┻ with dignity”
    ([slides](https://fpy.li/4-1), [video](https://fpy.li/4-2)) was the excellent
    PyCon 2014 talk by Esther Nam and Travis Fischer, where I found this chapter’s
    pithy epigraph: “Humans use text. Computers speak bytes.”'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: “Python 中的字符编码和 Unicode：如何（╯°□°）╯︵ ┻━┻ 有尊严地处理”（[幻灯片](https://fpy.li/4-1)，[视频](https://fpy.li/4-2)）是
    Esther Nam 和 Travis Fischer 在 PyCon 2014 上的出色演讲，我在这个章节中找到了这个简洁的题记：“人类使用文本。计算机使用字节。”
- en: 'Lennart Regebro—one of the technical reviewers for the first edition of this
    book—shares his “Useful Mental Model of Unicode (UMMU)” in the short post [“Unconfusing
    Unicode: What Is Unicode?”](https://fpy.li/4-31). Unicode is a complex standard,
    so Lennart’s UMMU is a really useful starting point.'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: Lennart Regebro——本书第一版的技术审查者之一——在短文[“澄清 Unicode：什么是 Unicode？”](https://fpy.li/4-31)中分享了他的“Unicode
    有用的心智模型（UMMU）”。Unicode 是一个复杂的标准，所以 Lennart 的 UMMU 是一个非常有用的起点。
- en: The official [“Unicode HOWTO”](https://fpy.li/4-32) in the Python docs approaches
    the subject from several different angles, from a good historic intro, to syntax
    details, codecs, regular expressions, filenames, and best practices for Unicode-aware
    I/O (i.e., the Unicode sandwich), with plenty of additional reference links from
    each section. [Chapter 4, “Strings”](https://fpy.li/4-33), of Mark Pilgrim’s awesome
    book [*Dive into Python 3*](https://fpy.li/4-34) (Apress) also provides a very
    good intro to Unicode support in Python 3\. In the same book, [Chapter 15](https://fpy.li/4-35)
    describes how the Chardet library was ported from Python 2 to Python 3, a valuable
    case study given that the switch from the old `str` to the new `bytes` is the
    cause of most migration pains, and that is a central concern in a library designed
    to detect encodings.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: Python 文档中官方的[“Unicode HOWTO”](https://fpy.li/4-32)从多个不同角度探讨了这个主题，从一个很好的历史介绍，到语法细节，编解码器，正则表达式，文件名，以及
    Unicode-aware I/O 的最佳实践（即 Unicode 三明治），每个部分都有大量额外的参考链接。Mark Pilgrim 的精彩书籍[*Dive
    into Python 3*](https://fpy.li/4-34)（Apress）的[第 4 章，“字符串”](https://fpy.li/4-33)也提供了
    Python 3 中 Unicode 支持的很好介绍。在同一本书中，[第 15 章](https://fpy.li/4-35)描述了 Chardet 库是如何从
    Python 2 移植到 Python 3 的，这是一个有价值的案例研究，因为从旧的 `str` 到新的 `bytes` 的转换是大多数迁移痛点的原因，这也是一个设计用于检测编码的库的核心关注点。
- en: 'If you know Python 2 but are new to Python 3, Guido van Rossum’s [“What’s New
    in Python 3.0”](https://fpy.li/4-36) has 15 bullet points that summarize what
    changed, with lots of links. Guido starts with the blunt statement: “Everything
    you thought you knew about binary data and Unicode has changed.” Armin Ronacher’s
    blog post [“The Updated Guide to Unicode on Python”](https://fpy.li/4-37) is deep
    and highlights some of the pitfalls of Unicode in Python 3 (Armin is not a big
    fan of Python 3).'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你了解 Python 2 但是对 Python 3 感到陌生，Guido van Rossum 的[“Python 3.0 有什么新特性”](https://fpy.li/4-36)列出了
    15 个要点，总结了发生的变化，并提供了许多链接。Guido 以直率的话语开始：“你所知道的关于二进制数据和 Unicode 的一切都发生了变化。”Armin
    Ronacher 的博客文章[“Python 中 Unicode 的更新指南”](https://fpy.li/4-37)深入探讨了 Python 3 中
    Unicode 的一些陷阱（Armin 不是 Python 3 的铁粉）。
- en: Chapter 2, “Strings and Text,” of the [*Python Cookbook*, 3rd ed.](https://fpy.li/pycook3)
    (O’Reilly), by David Beazley and Brian K. Jones, has several recipes dealing with
    Unicode normalization, sanitizing text, and performing text-oriented operations
    on byte sequences. Chapter 5 covers files and I/O, and it includes “Recipe 5.17\.
    Writing Bytes to a Text File,” showing that underlying any text file there is
    always a binary stream that may be accessed directly when needed. Later in the
    cookbook, the `struct` module is put to use in “Recipe 6.11\. Reading and Writing
    Binary Arrays of Structures.”
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 第三版的[*Python Cookbook*](https://fpy.li/pycook3)（O’Reilly）中的第二章“字符串和文本”，由大卫·比兹利和布莱恩·K·琼斯编写，包含了几个处理Unicode标准化、文本清理以及在字节序列上执行面向文本操作的示例。第五章涵盖了文件和I/O，并包括“第5.17节
    写入字节到文本文件”，展示了在任何文本文件下始终存在一个可以在需要时直接访问的二进制流。在后续的食谱中，`struct`模块被用于“第6.11节 读取和写入二进制结构数组”。
- en: 'Nick Coghlan’s “Python Notes” blog has two posts very relevant to this chapter:
    [“Python 3 and ASCII Compatible Binary Protocols”](https://fpy.li/4-38) and [“Processing
    Text Files in Python 3”](https://fpy.li/4-39). Highly recommended.'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 尼克·科格兰的“Python笔记”博客有两篇与本章非常相关的文章：[“Python 3和ASCII兼容的二进制协议”](https://fpy.li/4-38)和[“在Python
    3中处理文本文件”](https://fpy.li/4-39)。强烈推荐。
- en: A list of encodings supported by Python is available at [“Standard Encodings”](https://fpy.li/4-40)
    in the `codecs` module documentation. If you need to get that list programmatically,
    see how it’s done in the [*/Tools/unicode/listcodecs.py*](https://fpy.li/4-41)
    script that comes with the CPython source code.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: Python支持的编码列表可在`codecs`模块文档中的[“标准编码”](https://fpy.li/4-40)中找到。如果需要以编程方式获取该列表，请查看随CPython源代码提供的[*/Tools/unicode/listcodecs.py*](https://fpy.li/4-41)脚本。
- en: The books *[Unicode Explained](https://fpy.li/4-42)* by Jukka K. Korpela (O’Reilly)
    and [*Unicode Demystified*](https://fpy.li/4-43) by Richard Gillam (Addison-Wesley)
    are not Python-specific but were very helpful as I studied Unicode concepts. [*Programming
    with Unicode*](https://fpy.li/4-44) by Victor Stinner is a free, self-published
    book (Creative Commons BY-SA) covering Unicode in general, as well as tools and
    APIs in the context of the main operating systems and a few programming languages,
    including Python.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 书籍[*Unicode Explained*](https://fpy.li/4-42)由尤卡·K·科尔佩拉（O’Reilly）和[*Unicode Demystified*](https://fpy.li/4-43)由理查德·吉拉姆（Addison-Wesley）撰写，虽然不是针对Python的，但在我学习Unicode概念时非常有帮助。[*Programming
    with Unicode*](https://fpy.li/4-44)由维克多·斯汀纳自由出版，涵盖了Unicode的一般概念，以及主要操作系统和几种编程语言的工具和API。
- en: 'The W3C pages [“Case Folding: An Introduction”](https://fpy.li/4-45) and [“Character
    Model for the World Wide Web: String Matching”](https://fpy.li/4-15) cover normalization
    concepts, with the former being a gentle introduction and the latter a working
    group note written in dry standard-speak—the same tone of the [“Unicode Standard
    Annex #15—Unicode Normalization Forms”](https://fpy.li/4-47). The [“Frequently
    Asked Questions, Normalization”](https://fpy.li/4-48) section from [*Unicode.org*](https://fpy.li/4-49)
    is more readable, as is the [“NFC FAQ”](https://fpy.li/4-50) by Mark Davis—author
    of several Unicode algorithms and president of the Unicode Consortium at the time
    of this writing.'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: W3C页面[“大小写折叠：简介”](https://fpy.li/4-45)和[“全球网络字符模型：字符串匹配”](https://fpy.li/4-15)涵盖了标准化概念，前者是一个简单���介绍，后者是一个以干燥标准语言撰写的工作组说明书—与[“Unicode标准附录＃15—Unicode标准化形式”](https://fpy.li/4-47)相同的语调。[*Unicode.org*](https://fpy.li/4-49)的[“常见问题，标准化”](https://fpy.li/4-48)部分更易读，马克·戴维斯的[“NFC
    FAQ”](https://fpy.li/4-50)也是如此—他是几个Unicode算法的作者，也是本文撰写时Unicode联盟的主席。
- en: In 2016, the Museum of Modern Art (MoMA) in New York added to its collection
    [the original emoji](https://fpy.li/4-51), the 176 emojis designed by Shigetaka
    Kurita in 1999 for NTT DOCOMO—the Japanese mobile carrier. Going further back
    in history, [*Emojipedia*](https://fpy.li/4-52) published [“Correcting the Record
    on the First Emoji Set”](https://fpy.li/4-53), crediting Japan’s SoftBank for
    the earliest known emoji set, deployed in cell phones in 1997. SoftBank’s set
    is the source of 90 emojis now in Unicode, including U+1F4A9 (`PILE OF POO`).
    Matthew Rothenberg’s [*emojitracker.com*](https://fpy.li/4-54) is a live dashboard
    showing counts of emoji usage on Twitter, updated in real time. As I write this,
    `FACE WITH TEARS OF JOY` (U+1F602) is the most popular emoji on Twitter, with
    more than 3,313,667,315 recorded occurrences.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 2016年，纽约现代艺术博物馆（MoMA）将1999年由NTT DOCOMO的栗田茂高设计的原始表情符号[加入了其收藏品](https://fpy.li/4-51)。回顾历史，[*Emojipedia*](https://fpy.li/4-52)发表了[“关于第一个表情符号集的纠正”](https://fpy.li/4-53)，将日本的SoftBank归功于已知最早的表情符号集，于1997年在手机上部署。SoftBank的集合是Unicode中90个表情符号的来源，包括U+1F4A9（`PILE
    OF POO`）。马修·罗森伯格的[*emojitracker.com*](https://fpy.li/4-54)是一个实时更新的Twitter表情符号使用计数仪表板。在我写这篇文章时，`FACE
    WITH TEARS OF JOY`（U+1F602）是Twitter上最受欢迎的表情符号，记录的出现次数超过3,313,667,315次。
- en: ^([1](ch04.html#idm46582490670096-marker)) Slide 12 of PyCon 2014 talk “Character
    Encoding and Unicode in Python” ([slides](https://fpy.li/4-1), [video](https://fpy.li/4-2)).
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch04.html#idm46582490670096-marker)) PyCon 2014演讲“Python中的字符编码和Unicode”（[幻灯片](https://fpy.li/4-1)，[视频](https://fpy.li/4-2)）的第12张幻灯片。
- en: ^([2](ch04.html#idm46582490431168-marker)) Python 2.6 and 2.7 also had `bytes`,
    but it was just an alias to the `str` type.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch04.html#idm46582490431168-marker)) Python 2.6和2.7也有`bytes`，但它只是`str`类型的别名。
- en: '^([3](ch04.html#idm46582490297184-marker)) Trivia: the ASCII “single quote”
    character that Python uses by default as the string delimiter is actually named
    APOSTROPHE in the Unicode standard. The real single quotes are asymmetric: left
    is U+2018 and right is U+2019.'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch04.html#idm46582490297184-marker)) 小知识：Python默认使用的ASCII“单引号”字符实际上在Unicode标准中被命名为APOSTROPHE。真正的单引号是不对称的：左边是U+2018，右边是U+2019。
- en: ^([4](ch04.html#idm46582490281616-marker)) It did not work in Python 3.0 to
    3.4, causing much pain to developers dealing with binary data. The reversal is
    documented in [PEP 461—Adding % formatting to bytes and bytearray](https://fpy.li/pep461).
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch04.html#idm46582490281616-marker)) 在Python 3.0到3.4中不起作用，给处理二进制数据的开发人员带来了很多痛苦。这种逆转在[PEP
    461—为bytes和bytearray添加%格式化](https://fpy.li/pep461)中有记录。
- en: ^([5](ch04.html#idm46582487659424-marker)) I first saw the term “Unicode sandwich”
    in Ned Batchelder’s excellent [“Pragmatic Unicode” talk](https://fpy.li/4-10)
    at US PyCon 2012.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch04.html#idm46582487659424-marker)) 我第一次看到“Unicode 三明治”这个术语是在Ned Batchelder在2012年美国PyCon大会上的优秀[“务实的Unicode”演讲](https://fpy.li/4-10)中。
- en: '^([6](ch04.html#idm46582487064752-marker)) Source: [“Windows Command-Line:
    Unicode and UTF-8 Output Text Buffer”](https://fpy.li/4-11).'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch04.html#idm46582487064752-marker)) 来源：[“Windows命令行：Unicode和UTF-8输出文本缓冲区”](https://fpy.li/4-11)。
- en: ^([7](ch04.html#idm46582486563520-marker)) Curiously, the micro sign is considered
    a “compatibility character,” but the ohm symbol is not. The end result is that
    NFC doesn’t touch the micro sign but changes the ohm symbol to capital omega,
    while NFKC and NFKD change both the ohm and the micro into Greek characters.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch04.html#idm46582486563520-marker)) 有趣的是，微符号被认为是一个“兼容字符”，但欧姆符号不是。最终结果是NFC不会触及微符号，但会将欧姆符号更改为大写希腊字母omega，而NFKC和NFKD会将欧姆符号和微符号都更改为希腊字符。
- en: ^([8](ch04.html#idm46582485433232-marker)) Diacritics affect sorting only in
    the rare case when they are the only difference between two words—in that case,
    the word with a diacritic is sorted after the plain word.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch04.html#idm46582485433232-marker)) 重音符号只在两个单词之间唯一的区别是它们时才会影响排序—在这种情况下，带有重音符号的单词会在普通单词之后排序。
- en: '^([9](ch04.html#idm46582485249344-marker)) Again, I could not find a solution,
    but did find other people reporting the same problem. Alex Martelli, one of the
    tech reviewers, had no problem using `setlocale` and `locale.strxfrm` on his Macintosh
    with macOS 10.9\. In summary: your mileage may vary.'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch04.html#idm46582485249344-marker)) 再次，我找不到解决方案，但发现其他人报告了相同的问题。其中一位技术审阅者Alex
    Martelli在他的Macintosh上使用`setlocale`和`locale.strxfrm`没有问题，他的macOS版本是10.9。总结：结果可能有所不同。
- en: ^([10](ch04.html#idm46582457454960-marker)) That’s an image—not a code listing—because
    emojis are not well supported by O’Reilly’s digital publishing toolchain as I
    write this.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch04.html#idm46582457454960-marker)) 那是一张图片—而不是代码清单—因为在我写这篇文章时，O’Reilly的数字出版工具链对表情符号的支持不佳。
- en: ^([11](ch04.html#idm46582456966128-marker)) Although it was not better than
    `re` at identifying digits in this particular sample.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch04.html#idm46582456966128-marker)) 尽管在这个特定样本中，它并不比`re`更擅长识别数字。
