["```py\npip install ray[\"serve\"]==1.12.0 transformers requests\n```", "```py\nfrom ray import serve\n\nfrom starlette.requests import Request\nfrom transformers import pipeline\n\n@serve.deployment\nclass SentimentAnalysis:\n    def __init__(self):\n        self._classifier = pipeline(\"sentiment-analysis\")\n\n    def __call__(self, request: Request) -> str:\n        input_text = request.query_params[\"input_text\"]\n        return self._classifier(input_text)[0][\"label\"]\n```", "```py\nbasic_deployment = SentimentAnalysis.bind()\n```", "```py\nserve run app:basic_deployment\n```", "```py\nimport requests\nprint(requests.get(\"http://localhost:8000/\", params={\"input_text\": \"Hello friend!\"}).json())\n```", "```py\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n@serve.deployment\n@serve.ingress(app)\nclass SentimentAnalysis:\n    def __init__(self):\n        self._classifier = pipeline(\"sentiment-analysis\")\n\n    @app.get(\"/\")\n    def classify(self, input_text: str) -> str:\n        return self._classifier(input_text)[0][\"label\"]\n\nfastapi_deployment = SentimentAnalysis.bind()\n```", "```py\napp = FastAPI()\n\n@serve.deployment(num_replicas=2, ray_actor_options={\"num_cpus\": 2})\n@serve.ingress(app)\nclass SentimentAnalysis:\n    def __init__(self):\n        self._classifier = pipeline(\"sentiment-analysis\")\n\n    @app.get(\"/\")\n    def classify(self, input_text: str) -> str:\n        import os\n        print(\"from process:\", os.getpid())\n        return self._classifier(input_text)[0][\"label\"]\n\nscaled_deployment = SentimentAnalysis.bind()\n```", "```py\nfrom typing import List\n\napp = FastAPI()\n\n@serve.deployment\n@serve.ingress(app)\nclass SentimentAnalysis:\n    def __init__(self):\n        self._classifier = pipeline(\"sentiment-analysis\")\n\n    @serve.batch(max_batch_size=10, batch_wait_timeout_s=0.1)\n    async def classify_batched(self, batched_inputs: List[str]) -> List[str]:\n        print(\"Got batch size:\", len(batched_inputs))\n        results = self._classifier(batched_inputs)\n        return [result[\"label\"] for result in results]\n\n    @app.get(\"/\")\n    async def classify(self, input_text: str) -> str:\n        return await self.classify_batched(input_text)\n\nbatched_deployment = SentimentAnalysis.bind()\n```", "```py\nfrom app import batched_deployment\n\n# Get a handle to the deployment so we can send requests in parallel.\nhandle = serve.run(batched_deployment)\nray.get([handle.classify.remote(\"sample text\") for _ in range(10)])\n```", "```py\n@serve.deployment\nclass DownstreamModel:\n    def __call__(self, inp: str):\n        return \"Hi from downstream model!\"\n\n@serve.deployment\nclass Driver:\n    def __init__(self, downstream):\n        self._d = downstream\n\n    async def __call__(self, *args) -> str:\n        return await self._d.remote()\n\ndownstream = DownstreamModel.bind()\ndriver = Driver.bind(downstream)\n```", "```py\n@serve.deployment\nclass DownstreamModel:\n    def __init__(self, my_val: str):\n        self._my_val = my_val\n\n    def __call__(self, inp: str):\n        return inp + \"|\" + self._my_val\n\n@serve.deployment\nclass PipelineDriver:\n    def __init__(self, model1, model2):\n        self._m1 = model1\n        self._m2 = model2\n\n    async def __call__(self, *args) -> str:\n        intermediate = self._m1.remote(\"input\")\n        final = self._m2.remote(intermediate)\n        return await final\n\nm1 = DownstreamModel.bind(\"val1\")\nm2 = DownstreamModel.bind(\"val2\")\npipeline_driver = PipelineDriver.bind(m1, m2)\n```", "```py\n@serve.deployment\nclass DownstreamModel:\n    def __init__(self, my_val: str):\n        self._my_val = my_val\n\n    def __call__(self):\n        return self._my_val\n\n@serve.deployment\nclass BroadcastDriver:\n    def __init__(self, model1, model2):\n        self._m1 = model1\n        self._m2 = model2\n\n    async def __call__(self, *args) -> str:\n        output1, output2 = self._m1.remote(), self._m2.remote()\n        return [await output1, await output2]\n\nm1 = DownstreamModel.bind(\"val1\")\nm2 = DownstreamModel.bind(\"val2\")\nbroadcast_driver = BroadcastDriver.bind(m1, m2)\n```", "```py\n@serve.deployment\nclass DownstreamModel:\n    def __init__(self, my_val: str):\n        self._my_val = my_val\n\n    def __call__(self):\n        return self._my_val\n\n@serve.deployment\nclass ConditionalDriver:\n    def __init__(self, model1, model2):\n        self._m1 = model1\n        self._m2 = model2\n\n    async def __call__(self, *args) -> str:\n        import random\n        if random.random() > 0.5:\n            return await self._m1.remote()\n        else:\n            return await self._m2.remote()\n\nm1 = DownstreamModel.bind(\"val1\")\nm2 = DownstreamModel.bind(\"val2\")\nconditional_driver = ConditionalDriver.bind(m1, m2)\n```", "```py\npip install ray[\"serve\"]==1.12.0 transformers requests wikipedia\n```", "```py\nfrom typing import Optional\n\nimport wikipedia\n\ndef fetch_wikipedia_page(search_term: str) -> Optional[str]:\n    results = wikipedia.search(search_term)\n    # If no results, return to caller.\n    if len(results) == 0:\n        return None\n\n    # Get the page for the top result.\n    return wikipedia.page(results[0]).content\n```", "```py\nfrom transformers import pipeline\n\n@serve.deployment\nclass SentimentAnalysis:\n    def __init__(self):\n        self._classifier = pipeline(\"sentiment-analysis\")\n\n    @serve.batch(max_batch_size=10, batch_wait_timeout_s=0.1)\n    async def is_positive_batched(self, inputs: List[str]) -> List[bool]:\n        results = self._classifier(inputs, truncation=True)\n        return [result[\"label\"] == \"POSITIVE\" for result in results]\n\n    async def __call__(self, input_text: str) -> bool:\n        return await self.is_positive_batched(input_text)\n```", "```py\n@serve.deployment(num_replicas=2)\nclass Summarizer:\n    def __init__(self, max_length: Optional[int] = None):\n        self._summarizer = pipeline(\"summarization\")\n        self._max_length = max_length\n\n    def __call__(self, input_text: str) -> str:\n        result = self._summarizer(\n            input_text, max_length=self._max_length, truncation=True)\n        return result[0][\"summary_text\"]\n```", "```py\n@serve.deployment\nclass EntityRecognition:\n    def __init__(self, threshold: float = 0.90, max_entities: int = 10):\n        self._entity_recognition = pipeline(\"ner\")\n        self._threshold = threshold\n        self._max_entities = max_entities\n\n    def __call__(self, input_text: str) -> List[str]:\n        final_results = []\n        for result in self._entity_recognition(input_text):\n            if result[\"score\"] > self._threshold:\n                final_results.append(result[\"word\"])\n            if len(final_results) == self._max_entities:\n                break\n\n        return final_results\n```", "```py\nfrom pydantic import BaseModel\n\nclass Response(BaseModel):\n    success: bool\n    message: str = \"\"\n    summary: str = \"\"\n    named_entities: List[str] = []\n```", "```py\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n@serve.deployment\n@serve.ingress(app)\nclass NLPPipelineDriver:\n    def __init__(self, sentiment_analysis, summarizer, entity_recognition):\n        self._sentiment_analysis = sentiment_analysis\n        self._summarizer = summarizer\n        self._entity_recognition = entity_recognition\n\n    @app.get(\"/\", response_model=Response)\n    async def summarize_article(self, search_term: str) -> Response:\n        # Fetch the top page content for the search term if found.\n        page_content = fetch_wikipedia_page(search_term)\n        if page_content is None:\n            return Response(success=False, message=\"No pages found.\")\n\n        # Conditionally continue based on the sentiment analysis.\n        is_positive = await self._sentiment_analysis.remote(page_content)\n        if not is_positive:\n            return Response(success=False, message=\"Only positivitiy allowed!\")\n\n        # Query the summarizer and named entity recognition models in parallel.\n        summary_result = self._summarizer.remote(page_content)\n        entities_result = self._entity_recognition.remote(page_content)\n        return Response(\n            success=True,\n            summary=await summary_result,\n            named_entities=await entities_result\n        )\n```", "```py\nsentiment_analysis = SentimentAnalysis.bind()\nsummarizer = Summarizer.bind()\nentity_recognition = EntityRecognition.bind(threshold=0.95, max_entities=5)\nnlp_pipeline_driver = NLPPipelineDriver.bind(\n    sentiment_analysis, summarizer, entity_recognition)\n# end::final_driver[]\n```", "```py\nserve run ch_08_model_serving:nlp_pipeline_driver\n```", "```py\nprint(requests.get(\"http://localhost:8000/\", params={\"search_term\": \"rayserve\"}).text)\n'{\"success\":false,\"message\":\"No pages found.\",\"summary\":\"\",\"named_entities\":[]}'\n```", "```py\nprint(requests.get(\"http://localhost:8000/\", params={\"search_term\": \"war\"}).text)\n'{\"success\":false,\"message\":\"Only positivitiy allowed!\",\"summary\":\"\",\"named_entities\":[]}'\n```", "```py\nprint(requests.get(\"http://localhost:8000/\", params={\"search_term\": \"physicist\"}).text)\n'{\"success\":true,\"message\":\"\",\"summary\":\" Physics is the natural science that studies matter, its fundamental constituents, its motion and behavior through space and time, and the related entities of energy and force . During the Scientific Revolution in the 17th century these natural sciences emerged as unique research endeavors in their own right . Physics intersects with many interdisciplinary areas of research, such as biophysics and quantum chemistry .\",\"named_entities\":[\"Scientific\",\"Revolution\",\"Ancient\",\"Greek\",\"Egyptians\"]}'\n```"]