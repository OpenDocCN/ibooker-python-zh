<html><head></head><body><section class="pagenumrestart" data-pdf-bookmark="Chapter 1. What Is Ray, and Where Does It Fit?" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch01">
<h1><span class="label">Chapter 1. </span>What Is Ray, and Where Does It Fit?</h1>


<p><em>Ray</em> is <a data-primary="Ray" data-secondary="reasons for using" data-type="indexterm" id="ray-reasons"/>primarily a Python tool for fast and simple distributed computing. Ray was created by the <a href="https://oreil.ly/aGtp8">RISELab</a> at the University of California, Berkeley. An earlier iteration of this lab created the initial software that eventually became Apache Spark. Researchers from the RISELab started the company Anyscale to continue developing and to offer products and services around Ray.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>You can also use Ray from Java. Like many Python applications, under the hood Ray uses a lot of C++ and some Fortran. Ray streaming also has some Java components.</p>
</div>

<p>The goal of Ray is to solve a wider variety of problems than its predecessors, supporting various scalable programing models that range from actors to machine learning (ML) to data parallelism. Its remote function and actor models make it a truly general-purpose development environment instead of big data only.</p>

<p>Ray automatically scales compute resources as needed, allowing you to focus on your code instead of managing servers. In addition to traditional horizontal scaling (e.g., adding more machines), Ray can schedule tasks to take advantage of different machine sizes and accelerators like graphics processing units (GPUs).</p>

<p>Since the introduction of Amazon Web Services (AWS) Lambda, interest in <em>serverless computing</em> has<a data-primary="serverless computing" data-type="indexterm" id="idm45354787186016"/> exploded. In this cloud computing model, the cloud provider allocates machine resources on demand, taking care of the servers on behalf of its customers. Ray provides a <a href="https://oreil.ly/BfxqQ">great foundation for general-purpose serverless platforms</a> by providing the following features:</p>

<ul>
<li>
<p>It hides servers. Ray autoscaling transparently manages servers based on the application requirements.</p>
</li>
<li>
<p>By supporting actors, Ray implements not only a stateless programming model (typical for the majority of serverless implementations) but also a stateful one.</p>
</li>
<li>
<p>It allows you to specify resources, including hardware accelerators required for the execution of your serverless functions.</p>
</li>
<li>
<p>It supports direct communications between your tasks, thus providing support for not only simple functions but also complex distributed applications.</p>
</li>
</ul>

<p>Ray provides a wealth of libraries that simplify the creation of applications that can fully take advantage of Ray’s serverless capabilities. Normally, you would need different tools for everything, from data processing to workflow management. By using a single tool for a larger portion of your application, you simplify not only development but also your operation management.</p>

<p>In this chapter, we’ll look at where Ray fits in the ecosystem and help you decide whether it’s a good fit for your project.</p>






<section data-pdf-bookmark="Why Do You Need Ray?" data-type="sect1"><div class="sect1" id="idm45354787366480">
<h1>Why Do You Need Ray?</h1>

<p>We often need something like Ray when our problems get too big to handle in a single process. Depending on how large our problems get, this can mean scaling from multicore all the way through multicomputer, all of which Ray supports. If you find yourself wondering how you can handle next month’s growth in users, data, or complexity, our hope is you will take a look at Ray. Ray exists because scaling software is hard, and it tends to be the kind of problem that gets harder rather than simpler with time.</p>

<p>Ray can scale not only to multiple computers but also without you having to directly manage servers. Computer scientist Leslie Lamport <a href="https://oreil.ly/QHxmt">has said</a>, “A distributed system is one in which the failure of a computer you didn’t even know existed can render your own computer unusable.” While this kind of failure is still possible, Ray is able to automatically recover from many types of failures.</p>

<p>Ray runs cleanly on your laptop as well as at scale with the same APIs. This provides a simple starting option for using Ray that does not require you to go to the cloud to start experimenting. Once you feel comfortable with the APIs and application structure, you can simply move your code to the cloud for better scalability without needing to modify your code. This fills the needs that exist between a distributed system and a single-threaded application. Ray is able to manage multiple threads and GPUs with the same abstractions it uses for distributed<a data-primary="Ray" data-secondary="reasons for using" data-startref="ray-reasons" data-type="indexterm" id="idm45354787363056"/> computing.</p>
</div></section>






<section data-pdf-bookmark="Where Can You Run Ray?" data-type="sect1"><div class="sect1" id="idm45354787361424">
<h1>Where Can You Run Ray?</h1>

<p>Ray<a data-primary="Ray" data-secondary="environments for" data-type="indexterm" id="ray-environments"/> can be deployed in a variety of environments, ranging from your laptop to the cloud, to cluster managers like Kubernetes or Yarn, to six Raspberry Pis hidden under your desk.<sup><a data-type="noteref" href="ch01.html#idm45354786473328" id="idm45354786473328-marker">1</a></sup> In local mode, getting started can be as simple as a <code>pip install</code> and a call to <code>ray.init</code>. Much of modern Ray will automatically initialize a context if one is not present, allowing you to skip even this part.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45354786471648">
<h5>Ray Cluster</h5>
<p>A <a data-primary="Ray" data-secondary="clusters" data-tertiary="explained" data-type="indexterm" id="ray-cluster-explain"/><a data-primary="clusters" data-seealso="Kafka" data-type="indexterm" id="idm45354786468192"/><a data-primary="clusters" data-secondary="explained" data-type="indexterm" id="cluster-explain"/>Ray <em>cluster</em> consists of a <em>head node</em> and a set of <em>worker nodes</em>, as<a data-primary="head nodes" data-type="indexterm" id="head-node"/><a data-primary="worker nodes" data-type="indexterm" id="worker-node"/> shown in 
<span class="keep-together"><a data-type="xref" href="#fig-ray-cluster-arch">Figure 1-1</a></span>.</p>

<figure><div class="figure" id="fig-ray-cluster-arch">
<img alt="spwr 0101" src="assets/spwr_0101.png"/>
<h6><span class="label">Figure 1-1. </span>Ray cluster architecture</h6>
</div></figure>

<p>As you can see, a head node, in addition to supporting all the functionality of the worker node, has two additional components:</p>
<dl>
<dt>Global control store (GCS)</dt>
<dd>
<p>Contains<a data-primary="GCS (global control store)" data-type="indexterm" id="idm45354787627920"/> cluster-wide information including <a href="https://oreil.ly/BBG0o">object tables, task tables, function tables, and event logs</a>. The content of this store is used for the web UI, error diagnostics, debugging, and profiling tools.</p>
</dd>
<dt>Autoscaler</dt>
<dd>
<p>Launches and <a data-primary="autoscaler" data-type="indexterm" id="idm45354787726624"/>terminates worker nodes to ensure that workloads have sufficient resources to run while minimizing idle resources.</p>
</dd>
</dl>

<p>The head node is effectively a master (singleton) that manages a complete cluster (via the autoscaler). Unfortunately, a head node is also a single point of failure. If you lose a head node, you will use the cluster and need to re-create it. Moreover, if you lose a head node, existing worker nodes can become orphans and will have to be removed manually.</p>

<p>Each Ray node contains a <em>Raylet</em>, <a data-primary="Raylets" data-type="indexterm" id="idm45354787724112"/>which consists of two main components:</p>
<dl>
<dt>Object store</dt>
<dd>
<p>All of <a data-primary="object stores" data-type="indexterm" id="idm45354787721728"/>the object stores are connected together, and you can think of this collection as somewhat similar to <a href="https://oreil.ly/wtNI5">Memcached</a>, a distributed cache.</p>
</dd>
<dt>Scheduler</dt>
<dd>
<p>Each Ray node provides a local <a data-primary="schedulers" data-type="indexterm" id="idm45354787718704"/>scheduler that can communicate with other nodes, thus creating a unified distributed scheduler for the cluster.</p>
</dd>
</dl>

<p>When we are talking about nodes in a Ray cluster, we are not talking about physical machines but rather about logical nodes based on Docker images. As a result, when mapping to physical machines, a given physical node can run one or more logical <a data-primary="Ray" data-secondary="clusters" data-startref="ray-cluster-explain" data-tertiary="explained" data-type="indexterm" id="idm45354787717104"/><a data-primary="clusters" data-secondary="explained" data-startref="cluster-explain" data-type="indexterm" id="idm45354787358544"/><a data-primary="head nodes" data-startref="head-node" data-type="indexterm" id="idm45354787357328"/><a data-primary="worker nodes" data-startref="worker-node" data-type="indexterm" id="idm45354787356384"/>nodes.</p>
</div></aside>

<p>The<a data-primary="ray up command" data-type="indexterm" id="idm45354787354896"/><a data-primary="Ray" data-secondary="clusters" data-tertiary="creating" data-type="indexterm" id="idm45354787354160"/><a data-primary="clusters" data-secondary="creating" data-type="indexterm" id="idm45354787352944"/> <code>ray up</code> command, which is included as part of Ray, allows you to create clusters and will do the following:</p>

<ul>
<li>
<p>Provision a new instance/machine (if running on the cloud or cluster manager) by using the provider’s software development kit (SDK) or access machines (if running directly on physical machines)</p>
</li>
<li>
<p>Execute shell commands to set up Ray with the desired options</p>
</li>
<li>
<p>Run any custom, user-defined setup commands (for example, setting environment variables and installing packages)</p>
</li>
<li>
<p>Initialize the Ray cluster</p>
</li>
<li>
<p>Deploy an autoscaler if required</p>
</li>
</ul>

<p>In addition to <code>ray up</code>, if running on Kubernetes, you can use the Ray Kubernetes operator. Although <code>ray up</code> and the Kubernetes operator are preferred ways of creating Ray clusters, you can manually set up the Ray cluster if you have a set of existing machines—​either physical or virtual machines (VMs).</p>

<p>Depending on the deployment option, the same Ray code will work, with large variances in speed. This can get more complicated when you need specific libraries or hardware for code, for example. We’ll look more at running Ray in local mode in the next chapter, and if you want to scale even more, we cover deploying to the cloud and resource managers<a data-primary="Ray" data-secondary="environments for" data-startref="ray-environments" data-type="indexterm" id="idm45354784172336"/> in <a data-type="xref" href="app02.html#appB">Appendix B</a>.</p>
</div></section>






<section data-pdf-bookmark="Running Your Code with Ray" data-type="sect1"><div class="sect1" id="idm45354784170000">
<h1>Running Your Code with Ray</h1>

<p>Ray is more<a data-primary="Ray" data-secondary="clusters" data-tertiary="connecting to" data-type="indexterm" id="idm45354784168704"/><a data-primary="clusters" data-secondary="connecting to" data-type="indexterm" id="idm45354784167424"/><a data-primary="connecting to clusters" data-type="indexterm" id="idm45354784166480"/> than just a library you import; it is also a cluster management tool. In addition to importing the library, you need to <em>connect</em> to a Ray cluster. You have three options for connecting your code to a Ray cluster:</p>
<dl class="left-align margin-2-7">
<dt>Calling <code>ray.init</code> with no arguments</dt>
<dd>
<p>This launches an embedded, single-node Ray instance that is immediately available to the application.</p>
</dd>
<dt>Using the <a href="https://oreil.ly/7your">Ray Client <code>ray.init("ray://<em>&lt;head_node_host&gt;</em>:10001")</code></a></dt>
<dd>
<p>By default, each Ray cluster launches with a Ray client server running on the head node that can receive remote client connections. Note, however, that when the client is located remotely, some operations run directly from the client may be slower because of wide area network (WAN) latencies. Ray is not resilient to network failures between the head node and the client.</p>
</dd>
<dt>Using the Ray command-line API</dt>
<dd>
<p>You can use the <code>ray submit</code> command to execute Python scripts on clusters. This will copy the designated file onto the head node cluster and execute it with the given arguments. If you are passing the parameters, your code should use the Python <code>sys</code> module that provides access to any command-line arguments via <code>sys.argv</code>. This removes the potential networking point of failure when using the Ray Client.</p>
</dd>
</dl>
</div></section>






<section data-pdf-bookmark="Where Does It Fit in the Ecosystem?" data-type="sect1"><div class="sect1" id="idm45354787195760">
<h1>Where Does It Fit in the Ecosystem?</h1>

<p>Ray<a data-primary="Ray" data-secondary="in ecosystem" data-secondary-sortas="ecosystem" data-type="indexterm" id="ray-ecosystem"/><a data-primary="ecosystem, Ray in" data-type="indexterm" id="ecosystem-ray"/><a data-primary="problem spaces, Ray in" data-type="indexterm" id="problem-spaces-ray"/> sits at a unique intersection of problem spaces.</p>

<p>The first problem that Ray solves is scaling your Python code by managing resources, whether they are servers, threads, or GPUs. Ray’s core building blocks are a scheduler, distributed data storage, and an actor system. The powerful scheduler that Ray uses is general purpose enough to implement simple workflows, in addition to handling traditional problems of scale. Ray’s actor system gives you a simple way of handling resilient distributed execution state. Ray is therefore able to act as<a data-primary="reactive system, Ray as" data-type="indexterm" id="idm45354787547664"/><a data-primary="Ray" data-secondary="as reactive system" data-secondary-sortas="reactive system" data-type="indexterm" id="idm45354787546960"/> a <em>reactive system</em>, whereby its multiple components can react to their surroundings.</p>

<p>In addition to the scalable building blocks, Ray has higher-level libraries such as Serve, Datasets, Tune, RLlib, Train, and Workflows that exist in the ML problem space. These are designed to be used by folks with more of a data science background than necessarily a distributed systems background.</p>

<p>Overall, the Ray ecosystem is presented in <a data-type="xref" href="#figure-ray-ecosystem">Figure 1-2</a>.</p>

<figure><div class="figure" id="figure-ray-ecosystem">
<img alt="spwr 0102" src="assets/spwr_0102.png"/>
<h6><span class="label">Figure 1-2. </span>The Ray ecosystem</h6>
</div></figure>

<p>Let’s take a look at some of these problem spaces and see how Ray fits in and compares with existing tools. The following list, adapted from the Ray team’s <a href="https://oreil.ly/VJFlK">“Ray 1.x Architecture” documentation</a>, compares Ray to several related system categories:</p>
<dl class="pagebreak-after-3">
<dt>Cluster orchestrators</dt>
<dd>
<p>Cluster <a data-primary="clusters" data-secondary="cluster orchestrators" data-type="indexterm" id="idm45354787537744"/>orchestrators like <a href="https://oreil.ly/OpVAA">Kubernetes</a>, <a href="https://oreil.ly/GAn27">Slurm</a>, and Yarn schedule containers. Ray can leverage these for allocating cluster nodes.</p>
</dd>
<dt>Parallelization frameworks</dt>
<dd>
<p>Compared <a data-primary="parallelization frameworks" data-type="indexterm" id="idm45354787598368"/>to Python parallelization frameworks such as <a href="https://oreil.ly/kij8j">multiprocessing</a> or <a href="https://oreil.ly/xwEYN">Celery</a>, Ray offers a more general, higher-performance API. In addition, Ray’s distributed objects support data sharing across parallel executors.</p>
</dd>
<dt>Data processing frameworks</dt>
<dd>
<p>Ray’s <a data-primary="data processing frameworks" data-type="indexterm" id="idm45354787594368"/>lower-level APIs are more flexible and better suited for a “distributed glue” framework than existing data processing frameworks such as <a href="https://oreil.ly/7DSc3">Spark</a>, <a href="https://oreil.ly/ejcw4">Mars</a>, or <a href="https://oreil.ly/Ol4SQ">Dask</a>. Although Ray has no inherent understanding of data schemas, relational tables, or streaming dataflow, it supports running many of these data processing frameworks—​for example, <a href="https://oreil.ly/SZKkm">Modin</a>, <a href="https://oreil.ly/9O2RK">Dask on Ray</a>, <a href="https://oreil.ly/GjawU">Mars on Ray</a>, and <a href="https://oreil.ly/cSE1V">Spark on Ray (RayDP)</a>.</p>
</dd>
<dt>Actor frameworks</dt>
<dd>
<p>Unlike <a data-primary="actor frameworks" data-type="indexterm" id="idm45354787587280"/>specialized actor frameworks such as <a href="https://oreil.ly/uRUun">Erlang</a>, <a href="https://oreil.ly/Y4O4S">Akka</a>, and <a href="https://oreil.ly/7WOhb">Orleans</a>, Ray integrates the actor framework directly into programming languages. In addition, Ray’s distributed objects support data sharing across actors.</p>
</dd>
<dt>Workflows</dt>
<dd>
<p>When <a data-primary="workflows" data-type="indexterm" id="idm45354784221648"/>most people talk about workflows, they talk about UI or script-driven low-code development. While this approach might be useful for nontechnical users, it frequently brings more pain than value to software engineers. Ray uses programmatic workflow implementation, similar to <a href="https://oreil.ly/w6pjl">Cadence</a>. This implementation combines the flexibility of Ray’s dynamic task graphs with strong durability guarantees. Ray Workflows offers subsecond overhead for task launch and supports workflows with hundreds of thousands of steps. It also takes advantage of the Ray object store to pass distributed datasets between steps.</p>
</dd>
<dt>HPC systems</dt>
<dd>
<p>Unlike Ray, <a data-primary="HPC (high-performance computing) systems" data-type="indexterm" id="idm45354784218816"/><a data-primary="high-performance computing (HPC) systems" data-type="indexterm" id="idm45354784218048"/>which exposes task and actor APIs, a majority of high-performance computing (HPC) systems expose lower-level messaging APIs, providing a greater application flexibility. Additionally, many of the HPC implementations offer optimized collective communication primitives. Ray provides a collective communication library that implements many of these<a data-primary="Ray" data-secondary="in ecosystem" data-secondary-sortas="ecosystem" data-startref="ray-ecosystem" data-type="indexterm" id="idm45354784217216"/><a data-primary="ecosystem, Ray in" data-startref="ecosystem-ray" data-type="indexterm" id="idm45354784215728"/><a data-primary="problem spaces, Ray in" data-startref="problem-spaces-ray" data-type="indexterm" id="idm45354784214784"/> functionalities.</p>
</dd>
</dl>








<section data-pdf-bookmark="Big Data / Scalable DataFrames" data-type="sect2"><div class="sect2" id="idm45354784213328">
<h2>Big Data / Scalable DataFrames</h2>

<p>Ray offers <a data-primary="DataFrames" data-type="indexterm" id="idm45354784211792"/><a data-primary="big data" data-type="indexterm" id="idm45354784211056"/><a data-primary="ray.data.Dataset" data-type="indexterm" id="idm45354784210384"/>a few APIs for scalable DataFrames, a cornerstone of the big data ecosystem. Ray builds on top of the Apache Arrow project to provide a (limited) distributed DataFrame API called <code>ray.data.Dataset</code>. This is largely intended for the simplest of transformations and reading from cloud or distributed storage. Beyond that, Ray also provides support for a more pandas-like experience through Dask on Ray, which leverages the Dask interface on top of Ray.</p>

<p>We cover scalable DataFrames in <a data-type="xref" href="ch09.html#ch09">Chapter 9</a>.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>In addition to the libraries noted previously, you may find references <a data-primary="Mars on Ray" data-type="indexterm" id="idm45354787401424"/><a data-primary="pandas" data-secondary="support for" data-type="indexterm" id="idm45354787400640"/>to Mars on Ray or Ray’s (deprecated) built-in pandas support. These libraries do not support distributed mode, so they can limit your scalability. This is a rapidly evolving area and something to keep your eye on in the future.</p>
</div>
<aside class="pagebreak-before less_space" data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45354787399312">
<h5>Ray and Spark</h5>
<p>It is tempting<a data-primary="Ray" data-secondary="Spark versus" data-type="indexterm" id="idm45354787397440"/><a data-primary="Spark" data-secondary="Ray versus" data-type="indexterm" id="idm45354787396432"/><a data-primary="Apache Spark" data-secondary="Ray versus" data-type="indexterm" id="idm45354787395488"/> to compare Ray with Apache Spark, and in some abstract ways, they are similar. From a user’s point of view, Spark is ideal for data-intensive tasks, and Ray is better suited to compute-intensive tasks.</p>

<p>Ray has a lower task overhead and support for distributed state, making it especially appealing for ML tasks. Ray’s lower-level APIs make it a more appealing platform to build tools on top of.</p>

<p>Spark has more data tools but depends on centralized scheduling and state management. This centralization makes implementing reinforcement learning (RL) and recursive algorithms a challenge. For analytical use cases, especially in existing big data deployments, Spark may be a better choice.</p>

<p>Ray and Spark are complementary and can be used together. A common pattern is data processing with Spark and then ML with Ray. In fact, the RayDP library provides a way to use Spark DataFrames inside Ray.</p>
</div></aside>
</div></section>








<section data-pdf-bookmark="Machine Learning" data-type="sect2"><div class="sect2" id="idm45354787393072">
<h2>Machine Learning</h2>

<p>Ray has<a data-primary="Ray" data-secondary="machine learning in" data-type="indexterm" id="idm45354787391632"/><a data-primary="machine learning (ML)" data-type="indexterm" id="idm45354787390736"/><a data-primary="ML (machine learning)" data-type="indexterm" id="idm45354787390064"/> multiple ML libraries, and for the most part, they serve to delegate much of the fancy parts of ML to existing tools like PyTorch, scikit-learn, and TensorFlow while using Ray’s distributed computing facilities to scale. <em>Ray Tune</em> <a data-primary="Ray Tune" data-type="indexterm" id="idm45354787388880"/><a data-primary="Ray Train" data-type="indexterm" id="idm45354787388144"/><a data-primary="RLlib" data-type="indexterm" id="idm45354787387472"/>implements hyperparameter tuning, using Ray’s ability to train many local Python-based models in parallel across a distributed set of machines. <em>Ray Train</em> implements distributed training with PyTorch or TensorFlow. Ray’s <em>RLlib</em> interface offers reinforcement learning with core algorithms.</p>

<p>Part of what allows Ray to stand out from pure data-parallel systems for ML is its actor model, which allows easier tracking of state (including parameters) and inter-worker communication. You can use this model to implement your own custom algorithms that are not a part of Ray Core.</p>

<p>We cover ML in more detail in <a data-type="xref" href="ch10.html#ch10">Chapter 10</a>.</p>
</div></section>








<section data-pdf-bookmark="Workflow Scheduling" data-type="sect2"><div class="sect2" id="idm45354787425760">
<h2>Workflow Scheduling</h2>

<p>Workflow scheduling is<a data-primary="Ray" data-secondary="workflow scheduling in" data-type="indexterm" id="idm45354787424192"/><a data-primary="workflows" data-secondary="scheduling" data-type="indexterm" id="idm45354787423184"/><a data-primary="schedulers" data-type="indexterm" id="idm45354787422240"/> one of these areas which, at first glance, can seem really simple. A workflow is “just” a graph of work that needs to be done. However, all programs can be expressed as “just” a graph of work that needs to be done. New in 2.0, Ray has a Workflows library to simplify expressing both traditional business logic workflows and large-scale (e.g., ML training) workflows.</p>

<p>Ray is unique in workflow scheduling because it allows tasks to schedule other tasks without having to call back to a central node. This allows for greater flexibility and throughput.</p>

<p>If you find Ray’s workflow engine too low-level, you can use Ray to run Apache Airflow. Airflow is <a data-primary="Apache Airflow" data-type="indexterm" id="idm45354787420528"/><a data-primary="Airflow" data-type="indexterm" id="idm45354787419824"/>one of the more popular workflow scheduling engines in the big data space. The <a href="https://oreil.ly/sxMC8">Apache Airflow Provider for Ray</a> lets you use your Ray cluster as a worker pool for Airflow.</p>

<p>We cover workflow scheduling in <a data-type="xref" href="ch08.html#ch08">Chapter 8</a>.</p>
</div></section>








<section data-pdf-bookmark="Streaming" data-type="sect2"><div class="sect2" id="idm45354787416848">
<h2>Streaming</h2>

<p>Streaming <a data-primary="Ray" data-secondary="streaming in" data-type="indexterm" id="idm45354787415632"/><a data-primary="streaming applications" data-type="indexterm" id="idm45354787414624"/>is generally considered to be processing “real-time-ish” data, or data “as-it-arrives-ish.” Streaming adds another layer of complexity, especially the closer to real time you try to get, as not all of your data will always arrive in order or on time. Ray offers standard streaming primitives and can use Kafka as a streaming data source and sink. Ray uses its actor model APIs to interact with streaming data.</p>

<p>Ray streaming, like many streaming systems bolted on batch systems, has some interesting quirks. Ray streaming, notably, implements more of its logic in Java, unlike the rest of Ray. This can make debugging streaming applications more challenging than other components in Ray.</p>

<p>We cover how to build streaming applications with Ray in <a data-type="xref" href="ch06.html#ch06">Chapter 6</a>.</p>
</div></section>








<section data-pdf-bookmark="Interactive" data-type="sect2"><div class="sect2" id="idm45354787411856">
<h2>Interactive</h2>

<p>Not <a data-primary="Ray" data-secondary="interactivity with" data-type="indexterm" id="idm45354787410528"/><a data-primary="interactivity" data-type="indexterm" id="idm45354787409520"/>all “real-time-ish” applications are necessarily streaming applications. A common example is interactively exploring a dataset. Similarly, interacting with user input (e.g., serving models) can be considered an interactive rather than a batch process, but it is handled separately from the streaming libraries with<a data-primary="Ray Serve" data-type="indexterm" id="idm45354787408720"/> Ray Serve.</p>
</div></section>
</div></section>






<section data-pdf-bookmark="What Ray Is Not" data-type="sect1"><div class="sect1" id="idm45354787407536">
<h1>What Ray Is Not</h1>

<p>While Ray<a data-primary="Ray" data-secondary="what it is not" data-type="indexterm" id="idm45354787406208"/> is a general-purpose distributed system, it’s important to note there are some things Ray is not (at least, not without your expending substantial effort):</p>

<ul>
<li>
<p>Structured Query Language (SQL) or an analytics engine</p>
</li>
<li>
<p>A data storage system</p>
</li>
<li>
<p>Suitable for running nuclear reactors</p>
</li>
<li>
<p>Fully language independent</p>
</li>
</ul>

<p class="pagebreak-before">Ray can be used to do a bit of all of these, but you’re likely better off using more specialized tooling. For example, while Ray does have a key/value store, it isn’t designed to survive the loss of the leader node. This doesn’t mean that if you find yourself working on a problem that needs a bit of SQL, or some non-Python libraries, Ray cannot meet your needs—​you just may need to bring in additional tools.</p>
</div></section>






<section data-pdf-bookmark="Conclusion" data-type="sect1"><div class="sect1" id="idm45354787577232">
<h1>Conclusion</h1>

<p>Ray has the potential to greatly simplify your development and operational overhead for medium- to large-scale problems. It achieves this by offering a unified API across a variety of traditionally separate problems while providing serverless scalability. If you have problems spanning the domains that Ray serves, or just are tired of the operational overhead of managing your own clusters, we hope you’ll join us on the adventure of learning Ray.</p>

<p>In the next chapter, we’ll show you how to get Ray installed in local mode on your machine. We’ll also look at a few Hello Worlds from some of the ecosystems that Ray supports (including actors and big data).</p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="idm45354786473328"><sup><a href="ch01.html#idm45354786473328-marker">1</a></sup> ARM support, including for Raspberry PIs, requires manual building for now.</p></div></div></section></body></html>