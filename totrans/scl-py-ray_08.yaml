- en: Chapter 7\. Implementing Microservices
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 7 章 实现微服务
- en: Initially, Ray was created as a framework for implementing [reinforcement learning](https://oreil.ly/xRIXk)
    but gradually morphed into a full-fledged serverless platform. Similarly, initially
    introduced as a [better way to serve ML models](https://oreil.ly/tFneS), [Ray
    Serve](https://oreil.ly/970jH) has recently evolved into a full-fledged microservices
    framework. In this chapter, you will learn how to use Ray Serve for implementing
    a general-purpose microservice framework and how to use this framework for model
    serving.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，Ray 被创建为一个实现 [强化学习](https://oreil.ly/xRIXk) 的框架，但逐渐演变成了一个完整的无服务器平台。同样地，最初作为一种
    [更好的机器学习模型服务方式](https://oreil.ly/tFneS) 被引入的 [Ray Serve](https://oreil.ly/970jH)
    最近演变成了一个完整的微服务框架。在本章中，你将学习如何使用 Ray Serve 来实现一个通用的微服务框架，以及如何使用该框架进行模型服务。
- en: Complete code of all examples used in this chapter can be found in the folder
    [*/ray_examples/serving*](https://oreil.ly/truUQ) in the book’s GitHub repo.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中使用的所有示例的完整代码可以在书籍的 GitHub 存储库的文件夹 [*/ray_examples/serving*](https://oreil.ly/truUQ)
    中找到。
- en: Understanding Microservice Architecture in Ray
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Ray 中理解微服务架构
- en: 'Ray microservice architecture (Ray Serve) is implemented on top of Ray by leveraging
    [Ray actors](https://oreil.ly/12pG5). Three kinds of actors are created to make
    up a Serve instance:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Ray 微服务架构（Ray Serve）是基于 [Ray actors](https://oreil.ly/12pG5) 来实现的，通过利用 Ray 实现。有三种类型的
    actors 被创建来组成一个 Serve 实例：
- en: Controller
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器
- en: A global actor unique to each Serve instance that manages the control plane.
    It is responsible for creating, updating, and destroying other actors. All of
    the Serve API calls (e.g., creating or getting a deployment) use the controller
    for their execution.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Serve 实例中的一个全局 actor，负责管理控制平面。它负责创建、更新和销毁其他 actors。所有 Serve API 调用（例如，创建或获取一个部署）都使用控制器进行执行。
- en: Router
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 路由器
- en: There is one router per node. Each router is a [Uvicorn HTTP server](https://oreil.ly/IexLX)
    that accepts incoming requests, forwards them to replicas, and responds after
    they are completed.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 每个节点有一个路由器。每个路由器是一个 [Uvicorn HTTP 服务器](https://oreil.ly/IexLX)，它接受传入的请求，将它们转发给副本，然后在它们完成后响应。
- en: Worker replica
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Worker 副本
- en: Worker replicas execute the user-defined code in response to a request. Each
    replica processes individual requests from the routers.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Worker 副本根据请求执行用户定义的代码。每个副本从路由器处理单独的请求。
- en: User-defined code is implemented using a Ray [deployment](https://oreil.ly/6UnLe),
    an extension of a Ray actor with additional features. We will start by examining
    the deployment itself.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 用户定义的代码使用 Ray [deployment](https://oreil.ly/6UnLe) 实现，这是 Ray actor 的一个扩展，具有额外的功能。我们将从检查部署本身开始。
- en: Deployment
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署
- en: The central concept in Ray Serve is the deployment, defining business logic
    that will handle incoming requests and the way this logic is exposed over HTTP
    or in Python. Let’s start with a simple deployment implementing a temperature
    controller ([Example 7-1](#tc_control_basic)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Ray Serve 中的核心概念是部署，定义将处理传入请求的业务逻辑以及这个逻辑在 HTTP 或 Python 中的暴露方式。让我们从一个简单的部署开始，实现一个温度控制器（[示例
    7-1](#tc_control_basic)）。
- en: Example 7-1\. [Temperature controller deployment](https://oreil.ly/8pbXQ)
  id: totrans-14
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-1\. [温度控制器部署](https://oreil.ly/8pbXQ)
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The implementation is decorated by an `@serve.deployment` annotation, telling
    Ray that this is a deployment. This deployment implements a single method, `call`,
    which has a special meaning in deployment: it is invoked via HTTP. It is a class
    method taking a [`starlette` request](https://oreil.ly/F76fs), which provides
    a convenient interface for the incoming HTTP request. In the case of the temperature
    controller, the request contains two parameters: the temperature and the conversion
    type.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 实现是通过 `@serve.deployment` 注解进行装饰的，告诉 Ray 这是一个部署。这个部署实现了一个名为 `call` 的单一方法，这个方法在部署中具有特殊的意义：它通过
    HTTP 被调用。它是一个类方法，接受一个 [`starlette` 请求](https://oreil.ly/F76fs)，它为传入的 HTTP 请求提供了一个方便的接口。在温度控制器的情况下，请求包含两个参数：温度和转换类型。
- en: Once the deployment is defined, you need to deploy it using `Converter.deploy`,
    similar to `.remote` when deploying an actor. You can then immediately access
    it via an HTTP interface ([Example 7-2](#call_http)).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 定义部署后，你需要使用 `Converter.deploy` 进行部署，类似于在部署 actor 时使用 `.remote`。然后你可以立即通过 HTTP
    接口访问它（[示例 7-2](#call_http)）。
- en: Example 7-2\. [Accessing converter over HTTP](https://oreil.ly/8pbXQ)
  id: totrans-18
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-2\. [通过 HTTP 访问转换器](https://oreil.ly/8pbXQ)
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note here that we are using URL parameters (query strings) to specify parameters.
    Also, because the services are exposed externally via HTTP, the requester can
    run anywhere, including in code that is running outside Ray.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '[Example 7-3](#output_ex1) shows the results of this invocation.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-3\. Results of HTTP invocations of deployment
  id: totrans-22
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In addition to being able to invoke a deployment over HTTP, you can invoke it
    directly using Python. To do this, you need to get a *handle* to the deployment
    and then use it for invocation, as shown in [Example 7-4](#direct_invoke).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-4\. [Invoking a deployment via a handle](https://oreil.ly/8pbXQ)
  id: totrans-25
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that in this code, we are manually creating `starlette` requests by specifying
    the request type and a query string.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Once executed, this code returns the same results as in [Example 7-3](#output_ex1).
    This example uses the same `call` method for both HTTP and Python requests. Although
    this works, the [best practice](https://oreil.ly/LqkPK) is to implement additional
    methods for Python invocation to avoid the usage of `Request` objects in the Python
    invocation. In our example, we can extend our initial deployment in [Example 7-1](#tc_control_basic)
    with additional methods for Python invocations in [Example 7-5](#tc_control_ext).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-5\. [Implementing additional methods for Python invocation](https://oreil.ly/8pbXQ)
  id: totrans-29
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: With these additional methods in place, Python invocations can be significantly
    simplified ([Example 7-6](#simpler_call)).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-6\. [Using additional methods for handle-based invocation](https://oreil.ly/8pbXQ)
  id: totrans-32
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Unlike [Example 7-4](#direct_invoke), which uses the default `call` method,
    these invocation methods are explicitly specified (instead of putting the request
    type in the request itself, the request type here is implicit—​it’s a method name).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: 'Ray offers synchronous and asynchronous handles. A *sync* flag, `Deployment.get​_han⁠dle(…​,
    sync=True|False)`, can be used to specify a handle type:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: The default handle is synchronous. In this case, calling `handle.remote` returns
    a Ray `ObjectRef`.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To create an asynchronous handle, set `sync=False`. As its name indicates, async
    handle invocation is asynchronous, and you will have to use `await` to get a Ray
    `ObjectRef`. To use `await`, you have to run `deployment.get_handle` and `handle.remote`
    in the Python `asyncio` event loop.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will demonstrate the use of async handles later in this chapter.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Finally, deployments can be updated by simply modifying the code or configuration
    options and calling `deploy` again. In addition to HTTP and direct Python invocation,
    described here, you can use the Python APIs to invoke deployment with Kafka (see
    [Chapter 6](ch06.html#ch06) for the Kafka integration approach).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know the basics of deployment, let’s take a look at additional
    capabilities available for deployments.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Additional Deployment Capabilities
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Additional deployment capabilities are provided in three ways:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Adding parameters to annotations
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向注释添加参数
- en: Using FastAPI HTTP deployments
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 FastAPI 进行 HTTP 部署
- en: Using deployment composition
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用部署组合
- en: Of course, you can combine all three to achieve your goals. Let’s take a close
    look at the options provided by each approach.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，您可以结合这三种方法来实现您的目标。让我们仔细看看每种方法提供的选项。
- en: Adding parameters to annotations
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 向注释添加参数
- en: The `@serve.deployment` annotation can take several [parameters](https://oreil.ly/mcUML).
    The most widely used is the number of replicas and resource requirements.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`@serve.deployment` 注释可以接受几个 [参数](https://oreil.ly/mcUML)。其中最常用的是副本数和资源需求。'
- en: Improving scalability with resource replicas
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用资源副本提高可扩展性
- en: By default, `deployment.deploy` creates a single instance of a deployment. By
    specifying the number of replicas in `@serve.deployment`, you can scale out a
    deployment to many processes. When the requests are sent to such a replicated
    deployment, Ray uses round-robin scheduling to invoke individual replicas. You
    can modify [Example 7-1](#tc_control_basic) to add multiple replicas and IDs for
    individual instances ([Example 7-7](#tc_control_scaled)).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`deployment.deploy` 创建部署的单个实例。通过在 `@serve.deployment` 中指定副本数，您可以将部署扩展到多个进程。当请求发送到这样一个复制的部署时，Ray
    使用轮询调度来调用各个副本。您可以修改 [示例 7-1](#tc_control_basic) 来添加多个副本和每个实例的 ID（[示例 7-7](#tc_control_scaled)）。
- en: Example 7-7\. [Scaled deployment](https://oreil.ly/zImkU)
  id: totrans-51
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-7\. [扩展部署](https://oreil.ly/zImkU)
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now the usage of either HTTP or handle-based invocation produces the result
    in [Example 7-8](#invoke_scaled_result).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在使用 HTTP 或基于句柄的调用会在 [示例 7-8](#invoke_scaled_result) 中生成结果。
- en: Example 7-8\. Invoking scaled deployment
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-8\. 调用扩展部署
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Looking at this result, you can see that every request is processed by a different
    deployment instance (a different ID).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 查看此结果，您可以看到每个请求都由不同的部署实例（不同的 ID）处理。
- en: This is manual scaling of deployment. What about autoscaling? Similar to the
    autoscaling of Kafka listeners (discussed in [Chapter 6](ch06.html#ch06)), Ray’s
    approach to autoscaling is different from the one taken by Kubernetes natively—​see,
    for example, [Knative](https://oreil.ly/2Tj0l). Instead of creating a new instance,
    Ray’s autoscaling approach is to create more Ray nodes and redistribute deployments
    appropriately.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这是部署的手动扩展。那么自动扩展呢？与 Kafka 监听器的自动扩展类似（见 [第 6 章](ch06.html#ch06) 讨论），Ray 的自动扩展方法与
    Kubernetes 本地方法不同—​例如，请参见 [Knative](https://oreil.ly/2Tj0l)。Ray 的自动扩展方法不是创建新实例，而是创建更多
    Ray 节点，并适当重新分配部署。
- en: 'If your deployments begin to exceed about three thousand requests per second,
    you should also scale the HTTP ingress to Ray. By default, the ingress HTTP server
    is started on only the head node, but you can also start an HTTP server on every
    node by using `serve.start(http_options=\{"location": "EveryNode"})`. If you scale
    the number of HTTP ingresses, you will also need to deploy a load balancer, available
    from your cloud provider or installed locally.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '如果您的部署开始超过每秒大约三千个请求，您也应该将 HTTP 入口扩展到 Ray。默认情况下，入口 HTTP 服务器仅在主节点上启动，但您也可以使用
    `serve.start(http_options=\{"location": "EveryNode"})` 在每个节点上启动一个 HTTP 服务器。如果您扩展了
    HTTP 入口的数量，您还需要部署一个负载均衡器，可以从您的云提供商获取或在本地安装。'
- en: Resource requirements for deployments
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部署的资源需求
- en: 'You can request specific resource requirements in `@serve.deployment`. For
    example, two CPUs and one GPU would be indicated as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 `@serve.deployment` 中请求特定的资源需求。例如，两个 CPU 和一个 GPU 可以如下表示：
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Another useful parameter of `@serve.deployment` is `route_prefix`. As you can
    see from [Example 7-2](#call_http), the default prefix is the name of the Python
    class used in this deployment. Using `route_prefix`, for example, allows you to
    explicitly specify a prefix used by HTTP requests:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`@serve.deployment` 的另一个有用参数是 `route_prefix`。如您在 [示例 7-2](#call_http) 中所见，默认前缀是用于此部署的
    Python 类的名称。例如，使用 `route_prefix` 允许您显式指定 HTTP 请求使用的前缀：'
- en: '[PRE9]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: For descriptions of additional configuration parameters, refer to the [“Ray
    Core API” documentation](https://oreil.ly/3NWgQ).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 有关附加配置参数的描述，请参阅 [“Ray 核心 API” 文档](https://oreil.ly/3NWgQ)。
- en: Implementing request routing with FastAPI
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 FastAPI 实现请求路由
- en: Although the initial example of a temperature converter deployment in [Example 7-1](#tc_control_basic)
    works fine, it is not convenient to use. You need to specify the transformation
    type with every request. A better approach is to have two separate endpoints (URLs)
    for the API—​one for Celsius-to-Fahrenheit transformation and one for Fahrenheit-to-Celsius
    transformation. You can achieve this by leveraging [Serve integration](https://oreil.ly/ue4Mz)
    with [FastAPI](https://oreil.ly/h8QKh). With this, you can rewrite [Example 7-1](#tc_control_basic)
    as shown in [Example 7-9](#tc_control_http).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在[示例 7-1](#tc_control_basic)中初始的温度转换器部署工作正常，但使用起来并不方便。您需要在每个请求中指定转换类型。更好的方法是为API拥有两个单独的端点（URL）——一个用于摄氏度转华氏度的转换，另一个用于华氏度转摄氏度的转换。您可以利用[FastAPI](https://oreil.ly/h8QKh)与[Serve集成](https://oreil.ly/ue4Mz)来实现这一点。通过这种方式，您可以像在[示例 7-9](#tc_control_http)中所示重写[示例 7-1](#tc_control_basic)。
- en: Example 7-9\. [Implementing multiple HTTP APIs in a deployment](https://oreil.ly/OuurE)
  id: totrans-67
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-9\. [在部署中实现多个HTTP API](https://oreil.ly/OuurE)
- en: '[PRE10]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note that here, we have introduced two HTTP-accessible APIs with two different
    URLs (effectively converting the second query string parameter to a set of URLs)—one
    per conversion type. (We also leverage the `route_prefix` parameter described
    previously.) This can simplify HTTP access; compare [Example 7-10](#call_multi)
    to the original in [Example 7-2](#call_http).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这里，我们引入了两个具有不同URL的HTTP可访问API（实际上将第二个查询字符串参数转换为一组URL），每种转换类型一个。这可以简化HTTP访问；将[示例 7-10](#call_multi)与[示例 7-2](#call_http)进行比较。
- en: Example 7-10\. [Invoking deployment with multiple HTTP endpoints](https://oreil.ly/OuurE)
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-10\. [使用多个HTTP端点调用部署](https://oreil.ly/OuurE)
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Additional features provided through FastAPI implementation include variable
    routes, automatic type validation, dependency injection (e.g., for database connections),
    [security support](https://oreil.ly/atwGv), and more. Refer to the [FastAPI documentation](https://oreil.ly/3pw0k)
    on how to use these features.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 通过FastAPI实现提供的附加功能包括可变路由、自动类型验证、依赖注入（例如数据库连接）、[安全支持](https://oreil.ly/atwGv)等。请参阅[FastAPI文档](https://oreil.ly/3pw0k)了解如何使用这些功能。
- en: Deployment Composition
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署组合
- en: Deployments can be built as a composition of other deployments. This allows
    for building powerful deployment pipelines.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 部署可以构建为其他部署的组合。这允许构建强大的部署流水线。
- en: 'Let’s take a look at the specific example: [canary deployment](https://oreil.ly/YT27x).
    In this deployment strategy, you deploy a new version of your code or model in
    a limited fashion to see how it behaves. You can easily build this type of deployment
    by using deployment composition. We will start by defining and deploying two simple
    deployments in [Example 7-11](#versioned_deploy).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个具体的例子：[金丝雀部署](https://oreil.ly/YT27x)。在这种部署策略中，您会以有限的方式部署您的代码或模型的新版本，以查看其行为如何。您可以通过使用部署组合轻松构建这种类型的部署。我们将从定义并部署两个简单的部署开始，在[示例 7-11](#versioned_deploy)中。
- en: Example 7-11\. [Two basic deployments](https://oreil.ly/EaD6e)
  id: totrans-76
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-11\. [两个基本部署](https://oreil.ly/EaD6e)
- en: '[PRE12]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'These deployments take any data and return a string: `"result": "version1"`
    for deployment 1 and `"result": “version2"` for deployment 2\. You can combine
    these two deployments by implementing a canary deployment ([Example 7-12](#canary_deploy)).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '这些部署接受任何数据并返回一个字符串："result": "version1"用于部署1和"result": “version2"用于部署2。您可以通过实现金丝雀部署（[示例 7-12](#canary_deploy)）将这两个部署组合起来。'
- en: Example 7-12\. [Canary deployment](https://oreil.ly/EaD6e)
  id: totrans-79
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-12\. [金丝雀部署](https://oreil.ly/EaD6e)
- en: '[PRE13]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This deployment illustrates several points. First, it demonstrates a constructor
    with parameters, which is useful for deployment, allowing a single definition
    to be deployed with different parameters. Second, we define the `call` function
    as `async`, to process queries concurrently. The implementation of the `call`
    function is simple: generate a new random number and, depending on its value and
    a value of `canary_percent`, you will invoke either the version 1 or version 2
    deployment.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 此部署演示了几个要点。首先，它展示了具有参数的构造函数，这对部署非常有用，允许单个定义使用不同的参数进行部署。其次，我们将`call`函数定义为`async`，以便并发处理查询。`call`函数的实现很简单：生成一个新的随机数，并根据其值和`canary_percent`的值调用版本1或版本2的部署。
- en: Once the `Canary` class is deployed (by using `Canary.deploy(.3)`, you can invoke
    it using HTTP. The result of invoking canary deployment 10 times is shown in [Example 7-13](#res_canary).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦通过`Canary.deploy(.3)`部署了`Canary`类，你可以使用 HTTP 调用它。调用 canary 部署 10 次的结果显示在[示例 7-13](#res_canary)中。
- en: Example 7-13\. Results of the canary deployment invocation
  id: totrans-83
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-13\. Canary 部署调用的结果
- en: '[PRE14]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As you can see here, the canary model works fairly well and does exactly what
    you need. Now that you know how to build and use Ray-based microservices, let’s
    see how you can use them for model serving.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在这里看到的，Canary 模型运行得相当好，并且完全符合你的需求。现在你知道如何构建和使用基于 Ray 的微服务之后，让我们看看如何将它们用于模型服务。
- en: Using Ray Serve for Model Serving
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Ray Serve 进行模型服务
- en: In a nutshell, serving a model is no different from invoking any other microservice
    (we will talk about specific model-serving requirements later in this chapter).
    As long as you can get an ML-generated model in some shape or form compatible
    with Ray’s runtime—e.g., in [pickle format](https://oreil.ly/DWVE7), straight
    Python code, or binary format along with a Python library for its processing—you
    can use this model to process inference requests. Let’s start with a simple example
    of model serving.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 简言之，服务模型与调用任何其他微服务没有什么不同（我们将在本章后面讨论特定的模型服务需求）。只要你能以某种形式获取与 Ray 运行时兼容的 ML 生成模型—例如，以[pickle
    格式](https://oreil.ly/DWVE7)，纯 Python 代码或二进制格式以及其处理的 Python 库—你就可以使用这个模型处理推断请求。让我们从一个简单的模型服务示例开始。
- en: Simple Model Service Example
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单的模型服务示例
- en: 'One popular model-learning application is predicting the quality of red wine,
    based on the [Kaggle Red Wine Quality dataset](https://oreil.ly/yvPRp). Numerous
    blog posts use this dataset to build ML implementations of wine quality—​for example,
    see articles by [Mayur Badole](https://oreil.ly/9yLZ9) and [Dexter Nguyen](https://oreil.ly/JWwgO).
    For our example, we have built several classification models for the Red Wine
    Quality dataset, based on Terence Shin’s [“Predicting Wine Quality with Several
    Classification Techniques”](https://oreil.ly/M6lc4); the actual code can be found
    in the book’s [GitHub repo](https://oreil.ly/ChZtD). The code uses several techniques
    for building a classification model of the red wine quality, including the following:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 一个流行的模型学习应用是基于[Kaggle 红葡萄酒质量数据集](https://oreil.ly/yvPRp)预测红酒的质量。许多博客文章使用这个数据集来构建红酒质量的机器学习实现—例如，参见[Mayur
    Badole](https://oreil.ly/9yLZ9)和[Dexter Nguyen](https://oreil.ly/JWwgO)的文章。在我们的例子中，我们基于
    Terence Shin 的[“使用多种分类技术预测葡萄酒质量”](https://oreil.ly/M6lc4)建立了几个红葡萄酒质量分类模型；实际代码可以在书的[GitHub
    仓库](https://oreil.ly/ChZtD)找到。该代码使用多种技术来构建红葡萄酒质量的分类模型，包括以下内容：
- en: '[Decision trees](https://oreil.ly/Qnx8W)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[决策树](https://oreil.ly/Qnx8W)'
- en: '[Random forest](https://oreil.ly/yXqZz)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[随机森林](https://oreil.ly/yXqZz)'
- en: '[AdaBoost](https://oreil.ly/gerOD)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[AdaBoost](https://oreil.ly/gerOD)'
- en: '[Gradient boost](https://oreil.ly/bTNNZ)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[梯度提升](https://oreil.ly/bTNNZ)'
- en: '[XGBoost](https://oreil.ly/csAzq)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[XGBoost](https://oreil.ly/csAzq)'
- en: All implementations leverage the scikit-learn Python library, which allows you
    to generate a model and export it using pickle. When validating the models, we
    saw the best results from the random forest, gradient boost, and XGBoost classifications,
    so we saved only these models locally—​generated models are available in the book’s
    [GitHub repo](https://oreil.ly/VY9NE). With the models in place, you can use a
    simple deployment that allows serving the red wine quality model using random
    forest classification ([Example 7-14](#rf_serve)).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的实现都利用了 scikit-learn Python 库，允许你生成模型并使用 pickle 导出。在验证模型时，我们从随机森林、梯度提升和 XGBoost
    分类中看到了最佳结果，所以我们只保存了这些模型—生成的模型可以在书的[GitHub 仓库](https://oreil.ly/VY9NE)找到。有了这些模型，你可以使用一个简单的部署来服务红葡萄酒质量模型，使用随机森林分类（[示例 7-14](#rf_serve)）。
- en: Example 7-14\. [Implementing model serving using random forest classification](https://oreil.ly/52qR4)
  id: totrans-96
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-14\. [使用随机森林分类实现模型服务](https://oreil.ly/52qR4)
- en: '[PRE15]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This deployment has three methods:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这个部署有三个方法：
- en: The constructor
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 构造函数
- en: Loads a model and stores it locally. We are using model location as a parameter
    so we can redeploy this deployment when a model changes.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 加载模型并在本地存储。我们使用模型位置作为参数，这样当模型变化时可以重新部署这个部署。
- en: '`call`'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`call`'
- en: Invoked by HTTP requests, this method retrieves the features (as a dictionary)
    and invokes the `serve` method for the actual processing. By defining it as async,
    it can process multiple requests simultaneously.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 通过HTTP请求调用，此方法检索特征（作为字典）并调用`serve`方法进行实际处理。通过定义为异步，它可以同时处理多个请求。
- en: '`serve`'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`serve`'
- en: Can be used to invoke deployment via a handle. It converts the incoming dictionary
    into a vector and calls the underlying model for inference.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 可用于通过句柄调用部署。它将传入的字典转换为向量，并调用底层模型进行推断。
- en: Once the implementation is deployed, it can be used for model serving. If invoked
    via HTTP, it takes a JSON string as a payload; for direct invocation, the request
    is in the form of a dictionary. Implementations for [XGBoost](https://oreil.ly/Kc2oH)
    and [gradient boost](https://oreil.ly/UbrD7) look pretty much the same, with the
    exception that a generated model in these cases takes a two-dimensional array
    instead of a vector, so you need to do this transformation before invoking the
    model.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦实施部署，它可以用于模型服务。如果通过HTTP调用，它将以JSON字符串形式接收负载；对于直接调用，请求是一个字典形式。对于[XGBoost](https://oreil.ly/Kc2oH)和[gradient
    boost](https://oreil.ly/UbrD7)的实现看起来几乎相同，唯一的区别是在这些情况下生成的模型需要一个二维数组而不是向量，因此您需要在调用模型之前进行此转换。
- en: Additionally, you can take a look at Ray’s documentation for [serving other
    types of models](https://oreil.ly/qouwp), including TensorFlow and PyTorch.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您还可以查看Ray的文档，用于[serving other types of models](https://oreil.ly/qouwp)，包括TensorFlow和PyTorch。
- en: Now that you know how to build a simple model-serving implementation, the question
    is whether Ray-based microservices are a good platform for model serving.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您知道如何构建一个简单的模型服务实现，问题是基于Ray的微服务是否是模型服务的良好平台。
- en: Considerations for Model-Serving Implementations
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型服务实现的考虑事项
- en: 'When it comes to model serving, a few specific requirements are important.
    A good definition of requirements specific to model serving can be found in [*Kubeflow
    for Machine Learning*](https://oreil.ly/sikPG) by Trevor Grant et al. (O’Reilly).
    These requirements are as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型服务方面，有几个具体要求非常重要。可以在《[*Kubeflow for Machine Learning*](https://oreil.ly/sikPG)》中找到关于模型服务特定要求的良好定义，作者是Trevor
    Grant等人（O’Reilly）。这些要求如下：
- en: 'The implementation has to be flexible. It should allow for your training to
    be implementation agnostic (i.e., TensorFlow versus PyTorch, versus scikit-learn).
    For an inference service invocation, it should not matter if the underlying model
    was trained using PyTorch, scikit-learn, or TensorFlow: the service interface
    should be shared so that the user’s API remains consistent.'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现必须灵活。它应该允许您的训练是实现无关的（即TensorFlow与PyTorch与scikit-learn）。对于推断服务调用，不应关心底层模型是使用PyTorch、scikit-learn还是TensorFlow训练的：服务接口应该是共享的，以便用户的API保持一致。
- en: It is sometimes advantageous to be able to batch requests in a variety of settings
    in order to realize better throughput. The implementation should make it simple
    to support batching of model-serving requests.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了实现更好的吞吐量，有时可以在各种设置中对请求进行批处理。实现应该简化支持模型服务请求的批处理。
- en: The implementation should provide the ability to leverage hardware optimizers
    that match the needs of the algorithm. Sometimes in the evaluation phase, you
    would benefit from hardware optimizers like GPUs to infer the models.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现应该提供利用与算法需求匹配的硬件优化器的能力。在评估阶段，有时您会受益于像GPU这样的硬件优化器来推断模型。
- en: The implementation should be able to seamlessly include additional components
    of an [inference graph](https://oreil.ly/9Gr2X). An inference graph could comprise
    feature transformers, predictors, explainers, and drift detectors.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现应能够无缝地包含推断图的其他组件。推断图可以包括特征转换器、预测器、解释器和漂移检测器。
- en: Implementation should allow scaling of serving instances, both explicitly and
    using autoscalers, regardless of the underlying hardware.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现应该允许服务实例的扩展，无论底层硬件如何，都可以明确地使用自动缩放器。
- en: It should be possible to expose model-serving functionality via different protocols
    including HTTP and Kafka.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应该能够通过包括HTTP和Kafka在内的不同协议公开模型服务功能。
- en: ML models traditionally do not extrapolate well outside the training data distribution.
    As a result, if data drift occurs, the model performance can deteriorate, and
    it should be retrained and redeployed. Implementation should support an easy redeployment
    of models.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 传统的 ML 模型在训练数据分布之外通常表现不佳。因此，如果发生数据漂移，模型性能可能会下降，需要重新训练和重新部署。实现应支持模型的轻松重新部署。
- en: Flexible deployment strategy implementations (including canary deployment, blue-green
    deployments, and A/B testing) are required to ensure that new versions of models
    will not behave worse than the existing ones.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 灵活的部署策略实现（包括金丝雀部署、蓝绿部署和A/B测试）是必需的，以确保新版本的模型不会表现比现有版本更差。
- en: 'Let’s see how these requirements are satisfied by Ray’s microservice framework:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 Ray 微服务框架是如何满足这些需求的：
- en: Ray’s deployment cleanly separates deployment APIs from model APIs. Thus, Ray
    “standardizes” deployment APIs and provides support for converting incoming data
    to the format required for the model. See [Example 7-14](#rf_serve).
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ray 的部署清晰地将部署 API 与模型 API 分开。因此，Ray “标准化”了部署 API，并提供了将传入数据转换为模型所需格式的支持。参见 [示例 7-14](#rf_serve)。
- en: Ray’s deployment makes it easy to implement request batching. Refer to the Ray
    [“Batching Tutorial” guide](https://oreil.ly/K3up4) for details on how to implement
    and deploy a Ray Serve deployment that accepts batches, configure the batch size,
    and query the model in Python.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ray 的部署使得实现请求批处理变得简单。有关如何实现和部署接受批处理、配置批处理大小并在 Python 中查询模型的详细信息，请参阅 Ray [“批处理教程”指南](https://oreil.ly/K3up4)。
- en: As described earlier in this chapter, deployments support configurations that
    allow specifying hardware resources (CPU/GPU) required for its execution.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如本章前文所述，部署支持配置，允许指定执行所需的硬件资源（CPU/GPU）。
- en: Deployment composition, described earlier in this chapter, allows for easy creation
    of the model-serving graphs, mixing and matching plain Python code and existing
    deployments. We will present an additional example of deployment compositions
    later in this chapter.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如本章前文所述，部署组合允许轻松创建模型服务图，混合和匹配纯 Python 代码和现有部署。我们将在本章后面再介绍一个部署组合的示例。
- en: As described earlier in this chapter, deployments support setting the number
    of replicas, thus easily scaling deployments. Coupled with Ray’s autoscaling and
    the ability to define the number of HTTP servers, the microservice framework allows
    for very efficient scaling of model serving.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如本章前文所述，部署支持设置副本数量，因此可以轻松扩展部署。结合 Ray 的自动缩放功能和定义 HTTP 服务器数量的能力，微服务框架允许非常高效地扩展模型服务。
- en: As we’ve described, deployments can be exposed via HTTP or straight Python.
    The latter option allows for integration with any required transport.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如前文所述，部署可以通过 HTTP 或纯 Python 公开。后一选项允许与任何所需传输进行集成。
- en: As described earlier in this chapter, a simple redeployment of deployment allows
    you to update models without restarting the Ray cluster and interrupting applications
    that are leveraging model serving.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如本章前文所述，部署的简单重新部署允许更新模型而无需重新启动 Ray 集群和中断正在利用模型服务的应用程序。
- en: As shown in [Example 7-12](#canary_deploy), using deployment composition allows
    for easy implementation of any deployment strategy.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如 [示例 7-12](#canary_deploy) 所示，使用部署组合可轻松实现任何部署策略。
- en: As we have shown here, the Ray microservice framework is a solid foundation
    for model serving that satisfies all of the main requirements for model serving.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在这里展示的，Ray 微服务框架是满足模型服务的所有主要需求的坚实基础。
- en: The last thing that you are going to learn in this chapter is the implementation
    of one of the advanced model-serving techniques—[speculative model serving](https://oreil.ly/KH8EZ)—using
    the Ray microservices framework.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 本章最后要学习的是一种高级模型服务技术的实现——[推测性模型服务](https://oreil.ly/KH8EZ)，使用 Ray 微服务框架。
- en: Speculative Model Serving Using the Ray Microservice Framework
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Ray 微服务框架进行推测性模型服务
- en: 'Speculative model serving is an application of [speculative execution](https://oreil.ly/RRvzK).
    In this optimization technique, a computer system performs a task that may not
    be needed. The work is done before knowing whether it is actually required. This
    allows getting results up front, so if they are actually needed, they will be
    available with no delay. Speculative execution is important in model serving because
    it provides the following features for machine-serving applications:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 推测模型服务是 [推测执行](https://oreil.ly/RRvzK) 的一种应用。在这种优化技术中，计算机系统执行可能不需要的任务。在确切知道是否真正需要之前，工作已经完成。这样可以提前获取结果，因此如果确实需要，将无延迟可用。在模型服务中，推测执行很重要，因为它为机器服务应用程序提供以下功能：
- en: Guaranteed execution time
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 保证执行时间
- en: Assuming that you have several models, with the fastest providing a fixed execution
    time, it is possible to provide a model-serving implementation with a fixed upper
    limit on execution time, as long as that time is larger than the execution time
    of the simplest model.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您有多个模型，并且最快的提供固定的执行时间，那么可以提供一个模型服务实现，其执行时间上限是固定的，只要该时间大于最简单模型的执行时间。
- en: Consensus-based model serving
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 基于共识的模型服务
- en: Assuming that you have several models, you can implement model serving in such
    a way that prediction is the one returned by the majority of the models.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您有多个模型，您可以实现模型服务，使得预测结果是多数模型返回的结果。
- en: Quality-based model serving
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 基于质量的模型服务
- en: Assuming that you have a metric allowing you to evaluate the quality of model-serving
    results, this approach allows you to pick the result with the best quality.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您有一个评估模型服务结果质量的度量标准，这种方法允许您选择质量最佳的结果。
- en: Here you will learn how to implement consensus-based model serving using Ray’s
    microservice framework.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，您将学习如何使用 Ray 的微服务框架实现基于共识的模型服务。
- en: 'You learned earlier in this chapter how to implement quality scoring of red
    wine using three models: random forest, gradient boost, and XGBoost. Now let’s
    try to produce an implementation that returns a result on which at least two models
    agree. The basic implementation is shown in [Example 7-15](#spec_serve).'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的早些时候，您学习了如何使用三个模型（随机森林、梯度提升和 XGBoost）实现红葡萄酒的质量评分。现在让我们尝试生成一个实现，返回至少两个模型同意的结果。基本实现如
    [示例 7-15](#spec_serve) 所示。
- en: Example 7-15\. [Consensus-based model serving](https://oreil.ly/2RyYu)
  id: totrans-139
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-15\. [基于共识的模型服务](https://oreil.ly/2RyYu)
- en: '[PRE16]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The constructor of this deployment creates handles for all of your deployments
    implementing individual models. Note that here we are creating async handles that
    allow parallel execution of each deployment.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 此部署的构造函数为所有实现个别模型的部署创建句柄。请注意，这里我们创建的是异步句柄，允许并行执行每个部署。
- en: The `call` method gets the payload and starts executing all three models in
    parallel and then waits for all to complete—see [“Waiting in asyncio”](https://oreil.ly/pKovE)
    by Hynek Schlawack for information on using `asyncio` for the execution of many
    coroutines and running them concurrently. Once you have all the results, you implement
    the consensus calculations and return the result (along with methods that voted
    for it).^([1](ch07.html#idm45354770971824))
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`call` 方法获取有效载荷并开始并行执行所有三个模型，然后等待所有模型执行完毕。有关使用 `asyncio` 在执行多个协程和并行运行它们时的信息，请参见
    Hynek Schlawack 的 [“在 asyncio 中等待”](https://oreil.ly/pKovE)。一旦您获得所有结果，就实施共识计算并返回结果（以及投票支持它的方法）^([1](ch07.html#idm45354770971824))。'
- en: Conclusion
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this chapter, you learned Ray’s implementation of the microservice framework
    and how this framework can be used by model serving. We started by describing
    a basic microservices deployment and extensions allowing for better control, scale,
    and extending of the deployment’s execution. We then showed an example of how
    this framework can be used to implement model serving, analyzed typical model-serving
    requirements, and showed how they can be satisfied by Ray. Finally, you learned
    how to implement an advanced model-serving example—consensus-based model serving—allowing
    you to improve the quality of individual model-serving methods. The article [“Building
    Highly Available and Scalable Online Applications on Ray at Ant Group”](https://oreil.ly/y9BuV)
    by Tengwei Cai et al. shows how to bring together the basic building blocks described
    here into more complex implementations.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了 Ray 实现的微服务框架以及这个框架如何被模型服务所使用。我们从描述基本的微服务部署开始，并介绍了一些扩展，允许更好地控制、扩展和扩展部署的执行。然后，我们展示了一个示例，说明了这个框架如何被用来实现模型服务，分析了典型的模型服务需求，并展示了
    Ray 如何满足这些需求。最后，你学习了如何实现一个高级的模型服务示例——基于共识的模型服务——使你能够提高个别模型服务方法的质量。文章["在蚂蚁集团的 Ray
    上构建高可用和可扩展的在线应用"](https://oreil.ly/y9BuV)由蔡腾伟等人展示了如何将这里描述的基本构建块汇集到更复杂的实现中。
- en: In the next chapter, you will learn about workflow implementation in Ray and
    how to use workflows to automate your application execution.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将学习有关在 Ray 中实现工作流以及如何使用工作流来自动化你的应用程序执行的内容。
- en: ^([1](ch07.html#idm45354770971824-marker)) You can also implement different
    policies for waiting for the model’s execution. You could, for example, use at
    least one model’s result via `asyncio.wait(tasks). return_when=asyncio.FIRST_COMPLETED)`
    or just wait for a given time interval by using `asyncio.wait(tasks, interval)`.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch07.html#idm45354770971824-marker)) 你还可以实现不同的策略来等待模型的执行。例如，你可以通过`asyncio.wait(tasks).
    return_when=asyncio.FIRST_COMPLETED)`至少使用一个模型的结果，或者仅通过`asyncio.wait(tasks, interval)`等待一段给定的时间间隔。
