- en: Chapter 5\. Iterators and Generators
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第五章. 迭代器和生成器
- en: When many people with experience in another language start learning Python,
    they are taken aback by the difference in `for` loop notation. That is to say,
    instead of writing
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当许多有其他语言经验的人开始学习 Python 时，他们对 `for` 循环符号的差异感到惊讶。也就是说，不是写成
- en: '[PRE0]'
  id: totrans-2
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'they are introduced to a new function called `range`:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 他们被介绍了一个名为 `range` 的新函数：
- en: '[PRE1]'
  id: totrans-4
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'It seems that in the Python code sample we are calling a function, `range`,
    which creates all of the data we need for the `for` loop to continue. Intuitively,
    this can be quite a time-consuming process—if we are trying to loop over the numbers
    1 through 100,000,000, then we need to spend a lot of time creating that array!
    However, this is where *generators* come into play: they essentially allow us
    to lazily evaluate these sorts of functions so we can have the code-readability
    of these special-purpose functions without the performance impacts.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 代码示例中，似乎我们正在调用一个名为 `range` 的函数，它创建了 `for` 循环所需的所有数据。直观地说，这可能是一个耗时的过程——如果我们尝试循环遍历
    1 到 100,000,000 的数字，那么我们需要花费大量时间创建该数组！然而，这就是 *生成器* 发挥作用的地方：它们基本上允许我们惰性评估这些函数，因此我们可以使用这些特殊用途函数而不会影响性能。
- en: 'To understand this concept, let’s implement a function that calculates several
    Fibonacci numbers both by filling a list and by using a generator:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解这个概念，让我们实现一个函数，它通过填充列表和使用生成器来计算多个 Fibonacci 数字：
- en: '[PRE2]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[![1](Images/1.png)](#co_iterators_and_generators_CO1-1)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_iterators_and_generators_CO1-1)'
- en: This function will `yield` many values instead of returning one value. This
    turns this regular-looking function into a generator that can be polled repeatedly
    for the next available value.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数将会 `yield` 多个值而不是返回一个值。这将使得这个看起来很普通的函数成为一个可以重复轮询下一个可用值的生成器。
- en: The first thing to note is that the `fibonacci_list` implementation must create
    and store the list of all the relevant Fibonacci numbers. So if we want to have
    10,000 numbers of the sequence, the function will do 10,000 appends to the `numbers`
    list (which, as we discussed in [Chapter 3](ch03.xhtml#chapter-lists-tuples),
    has overhead associated with it) and then return it.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要注意的是 `fibonacci_list` 实现必须创建并存储所有相关的 Fibonacci 数字列表。因此，如果我们想要获取序列的 10,000
    个数字，函数将执行 10,000 次向 `numbers` 列表追加（正如我们在 [第三章](ch03.xhtml#chapter-lists-tuples)
    中讨论过的，这会带来额外的开销），然后返回它。
- en: On the other hand, the generator is able to “return” many values. Every time
    the code gets to the `yield`, the function emits its value, and when another value
    is requested, the function resumes running (maintaining its previous state) and
    emits the new value. When the function reaches its end, a `StopIteration` exception
    is thrown, indicating that the given generator has no more values. As a result,
    even though both functions must, in the end, do the same number of calculations,
    the `fibonacci_list` version of the preceding loop uses 10,000× more memory (or
    `num_items` times more memory).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，生成器能够“返回”许多值。每次代码执行到 `yield` 时，函数都会发出其值，当请求另一个值时，函数恢复运行（保持其先前状态）并发出新值。当函数达到结尾时，会抛出
    `StopIteration` 异常，表示给定的生成器没有更多的值。因此，尽管这两个函数最终必须执行相同数量的计算，但前面循环的 `fibonacci_list`
    版本使用的内存是后者的 10,000 倍（或 `num_items` 倍）。
- en: With this code in mind, we can decompose the `for` loops that use our implementations
    of `fibonacci_list` and `fibonacci_gen`. In Python, `for` loops require that the
    object we are looping over supports iteration. This means that we must be able
    to create an iterator out of the object we want to loop over. To create an iterator
    from almost any object, we can use Python’s built-in `iter` function. This function,
    for lists, tuples, dictionaries, and sets, returns an iterator over the items
    or keys in the object. For more complex objects, `iter` returns the result of
    the `__iter__` property of the object. Since `fibonacci_gen` already returns an
    iterator, calling `iter` on it is a trivial operation, and it returns the original
    object (so `type(fibonacci_gen(10)) == type(iter(fibonacci_gen(10)))`). However,
    since `fibonacci_list` returns a list, we must create a new object, a list iterator,
    that will iterate over all values in the list. In general, once an iterator is
    created, we call the `next()` function with it, retrieving new values until a
    `StopIteration` exception is thrown. This gives us a good deconstructed view of
    `for` loops, as illustrated in [Example 5-1](#iter_py_for).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这段代码，我们可以分解使用我们的`fibonacci_list`和`fibonacci_gen`实现的`for`循环。在Python中，`for`循环要求我们要循环遍历的对象支持迭代。这意味着我们必须能够从我们想要遍历的对象中创建一个迭代器。为了从几乎任何对象创建迭代器，我们可以使用Python内置的`iter`函数。对于列表、元组、字典和集合，这个函数返回对象中的项或键的迭代器。对于更复杂的对象，`iter`返回对象的`__iter__`属性的结果。由于`fibonacci_gen`已经返回一个迭代器，在其上调用`iter`是一个微不足道的操作，它返回原始对象（因此`type(fibonacci_gen(10))
    == type(iter(fibonacci_gen(10)))`）。然而，由于`fibonacci_list`返回一个列表，我们必须创建一个新对象，即列表迭代器，它将迭代列表中的所有值。一般来说，一旦创建了迭代器，我们就可以用它调用`next()`函数，获取新的值，直到抛出`StopIteration`异常。这为我们提供了`for`循环的一个良好分解视图，如[示例 5-1](#iter_py_for)所示。
- en: Example 5-1\. Python `for` loop deconstructed
  id: totrans-13
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-1\. Python `for`循环分解
- en: '[PRE3]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The `for` loop code shows that we are doing extra work calling `iter` when using
    `fibonacci_list` instead of `fibonacci_gen`. When using `fibonacci_gen`, we create
    a generator that is trivially transformed into an iterator (since it is already
    an iterator!); however, for `fibonacci_list` we need to allocate a new list and
    precompute its values, and then we still must create an iterator.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '`for`循环代码显示，在使用`fibonacci_list`时，我们调用`iter`会多做一些工作，而使用`fibonacci_gen`时，我们创建一个生成器，该生成器轻松转换为迭代器（因为它本身就是迭代器！）；然而，对于`fibonacci_list`，我们需要分配一个新列表并预先计算其值，然后仍然必须创建一个迭代器。'
- en: 'More importantly, precomputing the `fibonacci_list` list requires allocating
    enough space for the full dataset and setting each element to the correct value,
    even though we always require only one value at a time. This also makes the list
    allocation useless. In fact, it may even make the loop unrunnable, because it
    may be trying to allocate more memory than is available (`fibonacci_list(100_000_000)`
    would create a list 3.1 GB large!). By timing the results, we can see this very
    explicitly:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 更重要的是，预计算`fibonacci_list`列表需要分配足够的空间来存储完整的数据集，并将每个元素设置为正确的值，即使我们始终一次只需一个值。这也使得列表分配变得无用。事实上，这可能使循环无法运行，因为它可能尝试分配比可用内存更多的内存（`fibonacci_list(100_000_000)`将创建一个3.1
    GB大的列表！）。通过时间结果的比较，我们可以非常明确地看到这一点：
- en: '[PRE4]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As we can see, the generator version is over twice as fast and requires no measurable
    memory as compared to the `fibonacci_list`’s 441 MB. It may seem at this point
    that you should use generators everywhere in place of creating lists, but that
    would create many complications.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，生成器版本的速度是列表版本的两倍多，并且不需要测量的内存，而`fibonacci_list`则需要441 MB。此时可能会认为应该在任何地方使用生成器代替创建列表，但这会带来许多复杂性。
- en: What if, for example, you needed to reference the list of Fibonacci numbers
    multiple times? In this case, `fibonacci_list` would provide a precomputed list
    of these digits, while `fibonacci_gen` would have to recompute them over and over
    again. In general, changing to using generators instead of precomputed arrays
    requires algorithmic changes that are sometimes not so easy to understand.^([1](ch05.xhtml#idm46122423820792))
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 比如说，如果你需要多次引用斐波那契数列，`fibonacci_list`会提供一个预先计算好的数字列表，而`fibonacci_gen`则需要一遍又一遍地重新计算。一般来说，改用生成器而非预先计算的数组需要进行有时并不容易理解的算法改动。^([1](ch05.xhtml#idm46122423820792))
- en: Note
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: An important choice that must be made when architecting your code is whether
    you are going to optimize CPU speed or memory efficiency. In some cases, using
    extra memory so that you have values precalculated and ready for future reference
    will save in overall speed. Other times, memory may be so constrained that the
    only solution is to recalculate values as opposed to saving them in memory. Every
    problem has its own considerations for this CPU/memory trade-off.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计代码架构时必须做出的一个重要选择是，您是否要优化CPU速度还是内存效率。在某些情况下，使用额外的内存来预先计算值并为将来的引用做好准备会节省整体速度。在其他情况下，内存可能受限，唯一的解决方案是重新计算值，而不是将它们保存在内存中。每个问题都有其对于CPU/内存权衡的考虑因素。
- en: 'One simple example of this that is often seen in source code is using a generator
    to create a sequence of numbers, only to use list comprehension to calculate the
    length of the result:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这种的一个简单示例经常在源代码中看到，就是使用一个发生器来创建一个数字序列，然后使用列表推导来计算结果的长度：
- en: '[PRE5]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: While we are still using `fibonacci_gen` to generate the Fibonacci sequence
    as a generator, we are then saving all values divisible by 3 into an array, only
    to take the length of that array and then throw away the data. In the process,
    we’re consuming 86 MB of data for no reason.^([2](ch05.xhtml#idm46122423727656))
    In fact, if we were doing this for a long enough Fibonacci sequence, the preceding
    code wouldn’t be able to run because of memory issues, even though the calculation
    itself is quite simple!
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们仍在使用`fibonacci_gen`生成斐波那契数列作为一个生成器，但我们随后将所有能被3整除的值保存到一个数组中，然后只取该数组的长度，然后丢弃数据。在这个过程中，我们无缘无故地消耗了86
    MB的数据。[^2] 事实上，如果我们对足够长的斐波那契数列执行此操作，由于内存问题，上述代码甚至无法运行，即使计算本身非常简单！
- en: Recall that we can create a list comprehension using a statement of the form
    [*`<value>`* `for` *`<item>`* `in` *`<sequence>`* `if` *`<condition>`*`]`. This
    will create a list of all the *`<value>`* items. Alternatively, we can use similar
    syntax to create a generator of the *`<value>`* items instead of a list with `(`*`<value>`*
    `for` *`<item>`* `in` *`<sequence>`* `if` *`<condition>`*`)`.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们可以使用形式为[*`<value>`* `for` *`<item>`* `in` *`<sequence>`* `if` *`<condition>`*`]`的语句创建一个列表推导。这将创建一个所有*`<value>`*项的列表。或者，我们可以使用类似的语法来创建一个生成器的*`<value>`*项，而不是一个带有`(`*`<value>`*
    `for` *`<item>`* `in` *`<sequence>`* `if` *`<condition>`*`)`的列表。
- en: 'Using this subtle difference between list comprehension and generator comprehension,
    we can optimize the preceding code for `divisible_by_three`. However, generators
    do not have a `length` property. As a result, we will have to be a bit clever:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 利用列表推导和生成器推导之间的这种细微差别，我们可以优化上述的`divisible_by_three`代码。然而，生成器没有`length`属性。因此，我们将不得不聪明地处理：
- en: '[PRE6]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Here, we have a generator that emits a value of `1` whenever it encounters a
    number divisible by 3, and nothing otherwise. By summing all elements in this
    generator, we are essentially doing the same as the list comprehension version
    and consuming no significant memory.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们有一个发生器，每当遇到能被3整除的数字时就发出`1`的值，否则不发出任何值。通过对这个发生器中的所有元素求和，我们本质上与列表推导版本做的事情相同，并且不消耗任何显著的内存。
- en: Note
  id: totrans-29
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Many of Python’s built-in functions that operate on sequences are generators
    themselves (albeit sometimes a special type of generator). For example, `range`
    returns a generator of values as opposed to the actual list of numbers within
    the specified range. Similarly, `map`, `zip`, `filter`, `reversed`, and `enumerate`
    all perform the calculation as needed and don’t store the full result. This means
    that the operation `zip(range(100_000), range(100_000))` will always have only
    two numbers in memory in order to return its corresponding values, instead of
    precalculating the result for the entire range beforehand.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Python中许多内置函数都是它们自己的生成器（尽管有时是一种特殊类型的生成器）。例如，`range`返回一个值的生成器，而不是指定范围内实际数字的列表。类似地，`map`、`zip`、`filter`、`reversed`和`enumerate`都根据需要执行计算，并不存储完整的结果。这意味着操作`zip(range(100_000),
    range(100_000))`将始终在内存中只有两个数字，以便返回其对应的值，而不是预先计算整个范围的结果。
- en: The performance of the two versions of this code is almost equivalent for these
    smaller sequence lengths, but the memory impact of the generator version is far
    less than that of the list comprehension. Furthermore, we transform the list version
    into a generator, because all that matters for each element of the list is its
    current value—either the number is divisible by 3 or it is not; it doesn’t matter
    where its placement is in the list of numbers or what the previous/next values
    are. More complex functions can also be transformed into generators, but depending
    on their reliance on state, this can become a difficult thing to do.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码的两个版本在较小的序列长度上的性能几乎相当，但是生成器版本的内存影响远远小于列表推导的影响。此外，我们将列表版本转换为生成器，因为列表中每个元素的重要性都在于其当前值——无论数字是否能被3整除，都没有关系；它的位置在数字列表中或前/后值是什么并不重要。更复杂的函数也可以转换为生成器，但是取决于它们对状态的依赖程度，这可能变得难以做到。
- en: Iterators for Infinite Series
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无限系列的迭代器
- en: Instead of calculating a known number of Fibonacci numbers, what if we instead
    attempted to calculate all of them?
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不计算已知数量的斐波那契数，而是尝试计算所有的呢？
- en: '[PRE7]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In this code we are doing something that wouldn’t be possible with the previous
    `fibonacci_list` code: we are encapsulating an infinite series of numbers into
    a function. This allows us to take as many values as we’d like from this stream
    and terminate when our code thinks it has had enough.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们正在做一些以前的`fibonacci_list`代码无法做到的事情：我们正在将无限系列的数字封装到一个函数中。这允许我们从此流中取出尽可能多的值，并在我们的代码认为已经足够时终止。
- en: 'One reason generators aren’t used as much as they could be is that a lot of
    the logic within them can be encapsulated in your logic code. Generators are really
    a way of organizing your code and having smarter loops. For example, we could
    answer the question “How many Fibonacci numbers below 5,000 are odd?” in multiple
    ways:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器没有被充分利用的一个原因是其中很多逻辑可以封装在您的逻辑代码中。生成器实际上是一种组织代码并拥有更智能的循环的方式。例如，我们可以用多种方式回答问题“5000以下有多少个斐波那契数是奇数？”：
- en: '[PRE8]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: All of these methods have similar runtime properties (as measured by their memory
    footprint and runtime performance), but the `fibonacci_transform` function benefits
    from several things. First, it is much more verbose than `fibonacci_succinct`,
    which means it will be easy for another developer to debug and understand. The
    latter mainly stands as a warning for the next section, where we cover some common
    workflows using `itertools`—while the module greatly simplifies many simple actions
    with iterators, it can also quickly make Python code very un-Pythonic. Conversely,
    `fibonacci_naive` is doing multiple things at a time, which hides the actual calculation
    it is doing! While it is obvious in the generator function that we are iterating
    over the Fibonacci numbers, we are not overencumbered by the actual calculation.
    Last, `fibonacci_transform` is more generalizable. This function could be renamed
    `num_odd_under_5000` and take in the generator by argument, and thus work over
    any series.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些方法在运行时特性上都相似（由内存占用和运行时性能测量），但是`fibonacci_transform`函数受益于几个方面。首先，它比`fibonacci_succinct`更冗长，这意味着另一个开发者可以更容易地进行调试和理解。后者主要是为了下一节，我们将在其中涵盖一些使用`itertools`的常见工作流程——虽然该模块可以大大简化许多迭代器的简单操作，但它也可能迅速使Python代码变得不太Pythonic。相反，`fibonacci_naive`一次做多件事情，这隐藏了它实际正在执行的计算！虽然在生成器函数中很明显我们正在迭代斐波那契数，但我们并没有因实际计算而过于繁重。最后，`fibonacci_transform`更具一般性。该函数可以重命名为`num_odd_under_5000`，并通过参数接受生成器，因此可以处理任何系列。
- en: 'One additional benefit of the `fibonacci_transform` and `fibonacci_succinct`
    functions is that they support the notion that in computation there are two phases:
    generating data and transforming data. These functions are very clearly performing
    a transformation on data, while the `fibonacci` function generates it. This demarcation
    adds extra clarity and functionality: we can move a transformative function to
    work on a new set of data, or perform multiple transformations on existing data.
    This paradigm has always been important when creating complex programs; however,
    generators facilitate this clearly by making generators responsible for creating
    the data and normal functions responsible for acting on the generated data.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`fibonacci_transform` 和 `fibonacci_succinct` 函数的另一个好处是它们支持计算中有两个阶段的概念：生成数据和转换数据。这些函数明显地对数据执行转换，而
    `fibonacci` 函数则生成数据。这种划分增加了额外的清晰度和功能：我们可以将一个转换函数移动到新的数据集上，或者在现有数据上执行多次转换。在创建复杂程序时，这种范式一直很重要；然而，生成器通过使生成器负责创建数据，普通函数负责对生成的数据进行操作，从而清晰地促进了这一点。'
- en: Lazy Generator Evaluation
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 惰性生成器评估
- en: As touched on previously, the way we get the memory benefits with a generator
    is by dealing only with the current values of interest. At any point in our calculation
    with a generator, we have only the current value and cannot reference any other
    items in the sequence (algorithms that perform this way are generally called *single
    pass* or *online*). This can sometimes make generators more difficult to use,
    but many modules and functions can help.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面提到的，我们通过生成器获得记忆优势的方式是仅处理当前感兴趣的值。在使用生成器进行计算时，我们始终只有当前值，并且不能引用序列中的任何其他项（按这种方式执行的算法通常称为*单通道*或*在线*）。这有时会使得生成器更难使用，但许多模块和函数可以帮助。
- en: 'The main library of interest is `itertools`, in the standard library. It supplies
    many other useful functions, including these:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 主要感兴趣的库是标准库中的`itertools`。它提供许多其他有用的函数，包括这些：
- en: '`islice`'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '`islice`'
- en: Allows slicing a potentially infinite generator
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 允许切片潜在无限生成器
- en: '`chain`'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`chain`'
- en: Chains together multiple generators
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 将多个生成器链接在一起
- en: '`takewhile`'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`takewhile`'
- en: Adds a condition that will end a generator
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 添加一个将结束生成器的条件
- en: '`cycle`'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`cycle`'
- en: Makes a finite generator infinite by constantly repeating it
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 通过不断重复使有限生成器变为无限
- en: Let’s build up an example of using generators to analyze a large dataset. Let’s
    say we’ve had an analysis routine going over temporal data, one piece of data
    per second, for the last 20 years—that’s 631,152,000 data points! The data is
    stored in a file, one second per line, and we cannot load the entire dataset into
    memory. As a result, if we wanted to do some simple anomaly detection, we’d have
    to use generators to save memory!
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举一个使用生成器分析大型数据集的例子。假设我们有一个分析例程，对过去20年的时间数据进行分析，每秒一条数据，总共有631,152,000个数据点！数据存储在文件中，每行一秒，我们无法将整个数据集加载到内存中。因此，如果我们想进行一些简单的异常检测，我们必须使用生成器来节省内存！
- en: 'The problem will be: Given a datafile of the form “timestamp, value,” find
    days whose values differ from normal distribution. We start by writing the code
    that will read the file, line by line, and output each line’s value as a Python
    object. We will also create a `read_fake_data` generator to generate fake data
    that we can test our algorithms with. For this function we still take the argument
    `filename`, so as to have the same function signature as `read_data`; however,
    we will simply disregard it. These two functions, shown in [Example 5-2](#iter_read_data),
    are indeed lazily evaluated—we read the next line in the file, or generate new
    fake data, only when the `next()` function is called.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 问题将是：给定一个形式为“时间戳，值”的数据文件，找出值与正态分布不同的日期。我们首先编写代码，逐行读取文件，并将每行的值输出为Python对象。我们还将创建一个`read_fake_data`生成器，生成我们可以用来测试算法的假数据。对于这个函数，我们仍然接受`filename`参数，以保持与`read_data`相同的函数签名；但是，我们将简单地忽略它。这两个函数，如[示例
    5-2](#iter_read_data)所示，确实是惰性评估的——我们只有在调用`next()`函数时才会读取文件中的下一行，或生成新的假数据。
- en: Example 5-2\. Lazily reading data
  id: totrans-53
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-2\. 惰性读取数据
- en: '[PRE9]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now we’d like to create a function that outputs groups of data that occur in
    the same day. For this, we can use the `groupby` function in `itertools` ([Example 5-3](#iter_day_grouper)).
    This function works by taking in a sequence of items and a key used to group these
    items. The output is a generator that produces tuples whose items are the key
    for the group and a generator for the items in the group. As our key function,
    we will output the calendar day that the data was recorded. This “key” function
    could be anything—we could group our data by hour, by year, or by some property
    in the actual value. The only limitation is that groups will be formed only for
    data that is sequential. So if we had the input `A A A A B B A A` and had `groupby`
    group by the letter, we would get three groups: `(A, [A, A, A, A])`, `(B, [B,
    B])`, and `(A, [A, A])`.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们想创建一个函数，输出出现在同一天的数据组。为此，我们可以使用`itertools`中的`groupby`函数（[示例 5-3](#iter_day_grouper)）。该函数通过接受一个项目序列和一个用于分组这些项目的键来工作。输出是一个生成器，产生元组，其项目是组的键和组中项目的生成器。作为我们的键函数，我们将输出数据记录的日历日期。这个“键”函数可以是任何东西——我们可以按小时、按年或按实际值中的某个属性对数据进行分组。唯一的限制是只有顺序数据才会形成组。因此，如果我们有输入`A
    A A A B B A A`，并且`groupby`按字母分组，我们将得到三组：`(A, [A, A, A, A])`，`(B, [B, B])`和`(A,
    [A, A])`。
- en: Example 5-3\. Grouping our data
  id: totrans-56
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-3\. 数据分组
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now to do the actual anomaly detection. We do this in [Example 5-4](#iter_anomaly)
    by creating a function that, given one group of data, returns whether it follows
    the normal distribution (using `scipy.stats.normaltest`). We can use this check
    with `itertools.filterfalse` to filter down the full dataset only to inputs that
    *don’t* pass the test. These inputs are what we consider to be anomalous.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在进行实际的异常检测。我们在[示例 5-4](#iter_anomaly)中通过创建一个函数来完成这个任务，该函数在给定一个数据组时返回其是否符合正态分布（使用`scipy.stats.normaltest`）。我们可以使用`itertools.filterfalse`来仅过滤掉完整数据集中不通过测试的输入。这些输入被认为是异常的。
- en: Note
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'In [Example 5-3](#iter_day_grouper), we cast `data_group` into a list, even
    though it is provided to us as an iterator. This is because the `normaltest` function
    requires an array-like object. We could, however, write our own `normaltest` function
    that is “one-pass” and could operate on a single view of the data. This could
    be done without too much trouble by using [Welford’s online averaging algorithm](https://oreil.ly/p2g8Q)
    to calculate the skew and kurtosis of the numbers. This would save us even more
    memory by always storing only a single value of the dataset in memory at once
    instead of storing a full day at a time. However, performance time regressions
    and development time should be taken into consideration: is storing one day of
    data in memory at a time sufficient for this problem, or does it need to be further
    optimized?'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例 5-3](#iter_day_grouper)中，我们将`data_group`转换为列表，即使它是以迭代器形式提供给我们的。这是因为`normaltest`函数需要一个类似数组的对象。然而，我们可以编写自己的“一次通过”`normaltest`函数，它可以在数据的单个视图上操作。通过使用[Welford的在线平均算法](https://oreil.ly/p2g8Q)计算数字的偏度和峰度，我们可以轻松实现这一点。这将通过始终仅在内存中存储数据集的单个值而不是整天的方式来节省更多内存。然而，性能时间回归和开发时间应该考虑进去：将一天的数据存储在内存中是否足以解决这个问题，或者是否需要进一步优化？
- en: Example 5-4\. Generator-based anomaly detection
  id: totrans-61
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-4\. 基于生成器的异常检测
- en: '[PRE11]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Finally, we can put together the chain of generators to get the days that had
    anomalous data ([Example 5-5](#iter_chaining_generators)).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以将生成器链结合起来以获取具有异常数据的日期（[示例 5-5](#iter_chaining_generators)）。
- en: Example 5-5\. Chaining together our generators
  id: totrans-64
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-5\. 连接我们的生成器
- en: '[PRE12]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This method allows us to get the list of days that are anomalous without having
    to load the entire dataset. Only enough data is read to generate the first five
    anomalies. Additionally, the `anomaly_generator` object can be read further to
    continue retrieving anomalous data This is called *lazy evaluation*—only the calculations
    that are explicitly requested are performed, which can drastically reduce overall
    runtime if there is an early termination condition.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法使我们能够获取异常日的列表，而无需加载整个数据集。只读取足够的数据以生成前五个异常。此外，`anomaly_generator`对象可以进一步阅读以继续检索异常数据。这被称为*惰性评估*——只有明确请求的计算才会执行，这可以显著减少总体运行时间，如果存在早期终止条件的话。
- en: 'Another nicety about organizing analysis this way is it allows us to do more
    expansive calculations easily, without having to rework large parts of the code.
    For example, if we want to have a moving window of one day instead of chunking
    up by days, we can replace the `groupby_day` in [Example 5-3](#iter_day_grouper)
    with something like this:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有关以这种方式组织分析的好处是，它使我们能够更轻松地进行更加广泛的计算，而无需重做大部分代码。例如，如果我们想要一个移动窗口为一天而不是按天分块，我们可以在
    [Example 5-3](#iter_day_grouper) 中用类似以下方式替换 `groupby_day`：
- en: '[PRE14]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In this version, we also see very explicitly the memory guarantee of this and
    the previous method—it will store only the window’s worth of data as state (in
    both cases, one day, or 3,600 data points). Note that the first item retrieved
    by the `for` loop is the `window_size`-th value. This is because `data` is an
    iterator, and in the previous line we consumed the first `window_size` values.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在此版本中，我们还可以非常明确地看到这种方法及其前一种方法的内存保证——它只会将窗口大小的数据作为状态存储（在两种情况下均为一天或3,600个数据点）。请注意，`for`
    循环检索的第一项是第 `window_size` 个值。这是因为 `data` 是一个迭代器，在前一行我们消耗了前 `window_size` 个值。
- en: 'A final note: in the `groupby_window` function, we are constantly creating
    new tuples, filling them with data, and yielding them to the caller. We can greatly
    optimize this by using the `deque` object in the `collections` module. This object
    gives us `O(1)` appends and removals to and from the beginning or end of a list
    (while normal lists are `O(1)` for appends or removals to/from the end of the
    list and `O(n)` for the same operations at the beginning of the list). Using the
    `deque` object, we can `append` the new data to the right (or end) of the list
    and use `deque.popleft()` to delete data from the left (or beginning) of the list
    without having to allocate more space or perform long `O(n)` operations. However,
    we would have to work on the `deque` object in-place and destroy previous views
    to the rolling window (see [“Memory Allocations and In-Place Operations”](ch06_split_000.xhtml#SEC-numpy-inplace)
    for more about in-place operations). The only way around this would be to copy
    the data into a tuple before yielding it back to the caller, which gets rid of
    any benefit of the change!'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 最后说明：在 `groupby_window` 函数中，我们不断地创建新的元组，将它们填充数据，并将它们 `yield` 给调用者。我们可以通过使用 `collections`
    模块中的 `deque` 对象来大大优化此过程。该对象提供了 `O(1)` 的向右（或末尾）附加和删除操作（而普通列表对于向末尾附加或删除操作是 `O(1)`，对于向列表开头相同操作是
    `O(n)`）。使用 `deque` 对象，我们可以将新数据追加到列表的右侧（或末尾），并使用 `deque.popleft()` 从左侧（或开头）删除数据，而无需分配更多空间或执行长达
    `O(n)` 的操作。然而，我们必须就地使用 `deque` 对象并销毁先前的滚动窗口视图（参见 [“内存分配和就地操作”](ch06_split_000.xhtml#SEC-numpy-inplace)
    了解有关就地操作的更多信息）。唯一的解决方法是在将数据复制到元组之前，将其销毁并返回给调用者，这将消除任何更改的好处！
- en: Wrap-Up
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: By formulating our anomaly-finding algorithm with iterators, we can process
    much more data than could fit into memory. What’s more, we can do it faster than
    if we had used lists, since we avoid all the costly `append` operations.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用迭代器来制定我们的异常查找算法，我们可以处理比内存容量更大得多的数据。更重要的是，我们可以比使用列表时更快地执行操作，因为我们避免了所有昂贵的
    `append` 操作。
- en: Since iterators are a primitive type in Python, this should always be a go-to
    method for trying to reduce the memory footprint of an application. The benefits
    are that results are lazily evaluated, so you process only the data you need,
    and memory is saved since we don’t store previous results unless explicitly required
    to. In [Chapter 11](ch11_split_000.xhtml#chapter-lessram), we will talk about
    other methods that can be used for more specific problems and introduce some new
    ways of looking at problems when RAM is an issue.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 由于迭代器在Python中是一种原始类型，这应该始终是尝试减少应用程序内存占用的方法。其好处在于结果是惰性评估的，因此您只处理所需的数据，并且节省内存，因为我们不存储以前的结果，除非显式需要。在
    [Chapter 11](ch11_split_000.xhtml#chapter-lessram) 中，我们将讨论其他可以用于更具体问题的方法，并介绍一些在RAM成为问题时看待问题的新方法。
- en: Another benefit of solving problems using iterators is that it prepares your
    code to be used on multiple CPUs or multiple computers, as we will see in Chapters
    [9](ch09_split_000.xhtml#multiprocessing) and [10](ch10.xhtml#clustering). As
    we discussed in [“Iterators for Infinite Series”](#iterators_inf), when working
    with iterators, you must always think about the various states that are necessary
    for your algorithm to work. Once you figure out how to package the state necessary
    for the algorithm to run, it doesn’t matter where it runs. We can see this sort
    of paradigm, for example, with the `multiprocessing` and `ipython` modules, both
    of which use a `map`-like function to launch parallel tasks.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 使用迭代器解决问题的另一个好处是，它能够使你的代码准备好在多个 CPU 或多台计算机上使用，正如我们将在第 [9](ch09_split_000.xhtml#multiprocessing)
    章和第 [10](ch10.xhtml#clustering) 章中看到的那样。正如我们在[“无限级数的迭代器”](#iterators_inf)中讨论的那样，在使用迭代器时，你必须始终考虑算法运行所必需的各种状态。一旦你弄清楚了如何打包算法运行所需的状态，它在哪里运行就不重要了。我们可以在`multiprocessing`和`ipython`模块中看到这种范式，它们都使用类似于
    `map` 的函数来启动并行任务。
- en: ^([1](ch05.xhtml#idm46122423820792-marker)) In general, algorithms that are
    *online* or *single pass* are a great fit for generators. However, when making
    the switch you have to ensure your algorithm can still function without being
    able to reference the data more than once.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch05.xhtml#idm46122423820792-marker)) 一般来说，*在线* 或 *单遍* 算法非常适合使用生成器。然而，在切换时，你必须确保你的算法仍然可以在没有能够多次引用数据的情况下运行。
- en: ^([2](ch05.xhtml#idm46122423727656-marker)) Calculated with `%memit len([n for
    n in fibonacci_gen(100_000) if n % 3 == 0])`.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch05.xhtml#idm46122423727656-marker)) 通过 `%memit len([n for n in fibonacci_gen(100_000)
    if n % 3 == 0])` 计算。
