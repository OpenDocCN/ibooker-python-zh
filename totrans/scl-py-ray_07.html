<html><head></head><body><section data-pdf-bookmark="Chapter 6. Implementing Streaming Applications" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch06">
<h1><span class="label">Chapter 6. </span>Implementing Streaming Applications</h1>


<p>So far in the book, we have been using Ray to implement serverless batch applications. In this case, data is collected, or provided from the user, and then used for calculations. Another important group of use cases are the situations requiring you to process data in real time. We use the overloaded term <em>real</em> time to mean processing the data as it arrives within some latency constraints. This type of data processing is called <em>streaming</em>.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45354775840832">
<h5>Streaming Applications</h5>
<p>In this chapter, we describe a fairly simple<a data-primary="windowing" data-type="indexterm" id="idm45354775839472"/><a data-primary="streaming SQL" data-type="indexterm" id="idm45354775838768"/> streaming implementation. We do not cover <a href="https://oreil.ly/IK6NX">windowing</a> or <a href="https://oreil.ly/VGbnC">streaming SQL</a>, as neither is currently implemented in Ray. If you need windowing or streaming SQL, you can integrate Ray with an additional streaming engine—​for example, <a href="https://oreil.ly/SmOeb">Apache Flink</a> using Kafka.</p>
</div></aside>

<p>In this book, we <a data-primary="streaming applications" data-secondary="defined" data-type="indexterm" id="idm45354775835328"/><a data-primary="applications" data-secondary="streaming" data-see="streaming applications" data-type="indexterm" id="idm45354775834320"/>define <em>streaming</em> as taking action on a series of data close to the time that the data is created.</p>

<p>Some <a data-primary="streaming applications" data-secondary="use cases" data-type="indexterm" id="streaming-usecase"/><a data-primary="use cases for streaming applications" data-type="indexterm" id="usecase-streaming"/>common streaming <a href="https://oreil.ly/QQnmm">use cases</a> include the following:</p>
<dl>
<dt>Log analysis</dt>
<dd>
<p>A way of gaining insights into the state of your hardware and software. It is typically implemented as a distributed processing of streams of logs as they are being produced.</p>
</dd>
<dt>Fraud detection</dt>
<dd>
<p>The monitoring of financial transactions and watching for anomalies that signal fraud in real time and stopping fraudulent transactions.</p>
</dd>
<dt>Cybersecurity</dt>
<dd>
<p>The monitoring of interactions with the system to detect anomalies, allowing the identification of security issues in real time to isolate threats.</p>
</dd>
<dt>Streaming logistics</dt>
<dd>
<p>The monitoring of cars, trucks, fleets, and shipments in real time, to optimize routing.</p>
</dd>
<dt>IoT data processing</dt>
<dd>
<p>An example is collecting data about an engine to gain insights that can detect a faulty situation before becoming a major problem.</p>
</dd>
<dt>Recommendation engines</dt>
<dd>
<p>Used to understand user interests based on online behavior for serving ads, recommending products and <a data-primary="streaming applications" data-secondary="use cases" data-startref="streaming-usecase" data-type="indexterm" id="idm45354776044704"/><a data-primary="use cases for streaming applications" data-startref="usecase-streaming" data-type="indexterm" id="idm45354776043456"/>services, etc.</p>
</dd>
</dl>

<p>When it comes to implementing streaming applications in Ray, you currently have two main options:</p>

<ul>
<li>
<p>Ray’s ecosystem provides a lot of underlying components, described in the previous chapters, that can be used for custom implementations of streaming applications.</p>
</li>
<li>
<p>External libraries and tools can be used with Ray to implement streaming.</p>
</li>
</ul>

<p>Ray is not built as a streaming system. It is an ecosystem that enables companies to build streaming systems on these lower-level primitives. You can find several stories of users from big and small companies building streaming applications on top of Ray.</p>

<p>With that being said, building a small streaming application on Ray will give you a perfect example of how to think about Ray application and how to use Ray effectively, and will allow you to understand the basics of streaming applications and how Ray’s capabilities can be leveraged for its implementation. Even if you decide to use external libraries, this material will help you make better decisions on whether and how to use these libraries.</p>

<p>One of the most popular approaches for implementing streaming applications is using <a href="https://oreil.ly/kMiQC">Apache Kafka</a> to connect data producers with consumers implementing data processing. Before delving into Ray’s streaming implementation, let’s start with a quick introduction to Kafka.</p>






<section data-pdf-bookmark="Apache Kafka" data-type="sect1"><div class="sect1" id="idm45354776037680">
<h1>Apache Kafka</h1>

<p>Here we describe only features of Kafka that are relevant for our discussion. For in-depth information, refer to the <a href="https://oreil.ly/E9Inp">Kafka documentation</a>.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45354776035056">
<h5>Getting Started with Kafka</h5>
<p>If you<a data-primary="streaming applications" data-secondary="Apache Kafka" data-tertiary="installing" data-type="indexterm" id="idm45354776033376"/><a data-primary="Apache Kafka" data-secondary="installing" data-type="indexterm" id="idm45354776032096"/><a data-primary="installing" data-secondary="Apache Kafka" data-type="indexterm" id="idm45354776031152"/><a data-primary="Kafka" data-secondary="installing" data-type="indexterm" id="idm45354776030208"/> want to experiment with Kafka, you can run it locally or on the cloud. For a local Kafka installation, refer to its <a href="https://oreil.ly/kgkld">“Quickstart” instructions</a>. Additionally, for a Mac installation, you can use <a href="https://oreil.ly/Z07S6">Homebrew</a>, following the <a href="https://oreil.ly/Lw4Mw">“Install Kafka” GitHub gist</a>. Alternatively, you can use Kafka on the cloud—for example, leveraging the <a href="https://oreil.ly/wkY9U">Confluent platform</a> or any other Kafka installation provided by your favorite cloud provider. Finally, if you are working on Kubernetes, <a href="https://oreil.ly/xoohU">Strimzi</a> can be a good choice for Kafka installation.</p>
</div></aside>








<section data-pdf-bookmark="Basic Kafka Concepts" data-type="sect2"><div class="sect2" id="idm45354776025376">
<h2>Basic Kafka Concepts</h2>

<p>Although<a data-primary="streaming applications" data-secondary="Apache Kafka" data-tertiary="explained" data-type="indexterm" id="streaming-kafka-explain"/><a data-primary="Apache Kafka" data-secondary="explained" data-type="indexterm" id="apache-kafka-explain"/><a data-primary="Kafka" data-secondary="explained" data-type="indexterm" id="kafka-explain"/> many people consider Kafka to be a type of messaging system—similar to, for example, <a href="https://oreil.ly/UD8ov">RabbitMQ</a>—it is a very different thing. Kafka is a <a href="https://oreil.ly/zXwQs">distributed log</a> that stores records sequentially (see 
<span class="keep-together"><a data-type="xref" href="#Distributed-log">Figure 6-1</a></span>).<sup><a data-type="noteref" href="ch06.html#idm45354776016592" id="idm45354776016592-marker">1</a></sup></p>

<figure><div class="figure" id="Distributed-log">
<img alt="spwr 0601" src="assets/spwr_0601.png"/>
<h6><span class="label">Figure 6-1. </span>Distributed log</h6>
</div></figure>

<p>Kafka records<a data-primary="messaging systems versus distributed log systems" data-type="indexterm" id="messaging-log-versus"/><a data-primary="distributed log systems versus messaging systems" data-type="indexterm" id="log-messaging-versus"/><a data-primary="key/value pairs, in Kafka" data-type="indexterm" id="idm45354775804464"/> are key/value pairs. (Both the key and value are optional, and an empty value can be used to tombstone an existing value.) Both keys and values are represented in Kafka as byte arrays and are opaque to Kafka itself. Producers always write to the end of the log, while consumers can choose the position (offset) where they want to read from.</p>

<p class="pagebreak-after">The main differences between log-oriented systems like Kafka and messaging systems like RabbitMQ are as follows:</p>

<ul class="less_space">
<li>
<p>Messages in queue systems are ephemeral; they are kept in the system only until they are delivered. Messages in log-based systems, on the other hand, are persistent. As a result, you can replay messages in a log-based system, which is impossible in traditional messaging.<sup><a data-type="noteref" href="ch06.html#idm45354775801840" id="idm45354775801840-marker">2</a></sup></p>
</li>
<li>
<p>While traditional message brokers manage consumers and their offsets, in log systems consumers are responsible for managing their offsets. This allows a log-based system to support significantly more consumers.</p>
</li>
</ul>

<p>Similar to messaging systems, <a data-primary="messaging systems versus distributed log systems" data-startref="messaging-log-versus" data-type="indexterm" id="idm45354775799120"/><a data-primary="distributed log systems versus messaging systems" data-startref="log-messaging-versus" data-type="indexterm" id="idm45354775798176"/><a data-primary="topics in Kafka" data-type="indexterm" id="idm45354775797264"/>Kafka organizes data into <em>topics</em>. Unlike messaging systems, topics in Kafka are purely logical constructs, composed of multiple partitions (<a data-type="xref" href="#Anatomy-of-topic">Figure 6-2</a>).</p>

<figure><div class="figure" id="Anatomy-of-topic">
<img alt="spwr 0602" src="assets/spwr_0602.png"/>
<h6><span class="label">Figure 6-2. </span>Anatomy of a topic</h6>
</div></figure>

<p>Data in a partition is sequential and can be replicated across multiple brokers. Partitioning is a vital scalability mechanism, allowing individual consumers to read dedicated partitions in parallel and allowing Kafka to store the partitions separately.</p>

<p>When writing to topics, Kafka supports two main partitioning mechanisms during the write operation: if a key is not defined, it uses round-robin partitioning, distributing the topic’s messages equally across partitions; if the key is defined, the partition to write to is determined by the key. By default, Kafka uses key hashing for 
<span class="keep-together">partioning.</span> You can also implement custom partitioning mechanisms with Kafka. Message 
<span class="keep-together">ordering</span> happens only within a partition, so any messages to be processed in order must be in the same partition.</p>

<p>You deploy Kafka in the form of a cluster composed of multiple (1 to <em>n</em>) brokers (servers) to maintain load balancing.<sup><a data-type="noteref" href="ch06.html#idm45354775789952" id="idm45354775789952-marker">3</a></sup> Depending on the configured replication factor, each partition can exist on one or more brokers, and this can improve Kafka’s throughput. Kafka clients can connect to any broker, and the broker routes the requests transparently to one of the correct brokers.</p>

<p>To understand how applications scale with Kafka, you need to understand how Kafka’s consumer groups<a data-primary="consumer groups in Kafka" data-type="indexterm" id="idm45354775787920"/> work (<a data-type="xref" href="#Kafka-consumer-group">Figure 6-3</a>).</p>

<figure><div class="figure" id="Kafka-consumer-group">
<img alt="spwr 0603" src="assets/spwr_0603.png"/>
<h6><span class="label">Figure 6-3. </span>Kafka consumer group</h6>
</div></figure>

<p>You can assign consumers that read from the same set of topics to a <em>consumer group</em>. Kafka then gives each consumer in the group a subset of the partitions.</p>

<p>For example, if you have a topic with 10 partitions and a single consumer in a group, this consumer will read all of the topics’ partitions. With the same topic, if you instead have 5 consumers in the group, each consumer will read two partitions from the topic. If you have 11 consumers, 10 of them will each read a single partition, and the 11th one will not read any data.</p>

<p>As you can see, the two main factors in how much you can scale your Kafka reading is the number of partitions and the number of consumers in your consumer group. Adding more consumers to a consumer group is easier than adding new partitions, so overprovisioning the number of partitions is a<a data-primary="streaming applications" data-secondary="Apache Kafka" data-startref="streaming-kafka-explain" data-tertiary="explained" data-type="indexterm" id="idm45354775782656"/><a data-primary="Apache Kafka" data-secondary="explained" data-startref="apache-kafka-explain" data-type="indexterm" id="idm45354775781136"/><a data-primary="Kafka" data-secondary="explained" data-startref="kafka-explain" data-type="indexterm" id="idm45354775779920"/> best practice.</p>
</div></section>








<section data-pdf-bookmark="Kafka APIs" data-type="sect2"><div class="sect2" id="idm45354776024752">
<h2>Kafka APIs</h2>

<p>As<a data-primary="streaming applications" data-secondary="Apache Kafka" data-tertiary="APIs" data-type="indexterm" id="streaming-kafka-api"/><a data-primary="Apache Kafka" data-secondary="APIs" data-type="indexterm" id="apache-kafka-api"/><a data-primary="Kafka" data-secondary="APIs" data-type="indexterm" id="kafka-api"/> defined in the <a href="https://oreil.ly/1Edbr">Kafka documentation</a>, Kafka has five core API groups:</p>
<dl>
<dt>Producer API</dt>
<dd>
<p>Allows applications to send streams of data to topics in the Kafka cluster</p>
</dd>
<dt>Consumer API</dt>
<dd>
<p>Allows applications to read streams of data from topics in the Kafka cluster</p>
</dd>
<dt>AdminClient API</dt>
<dd>
<p>Allows managing and inspecting topics, brokers, and other Kafka objects</p>
</dd>
<dt>Streams API</dt>
<dd>
<p>Allows transforming streams of data from input topics to output topics</p>
</dd>
<dt>Connect API</dt>
<dd>
<p>Allows implementing connectors that continually pull from a source system or application into Kafka or push from Kafka into a sink system or application</p>
</dd>
</dl>

<p>These APIs are implemented in multiple <a href="https://oreil.ly/gPVs8">languages</a>, including Java, C/C++, Go, C#, and Python. We will be using Kafka’s <a href="https://oreil.ly/c7g3l">Python APIs</a> for integration with Ray, implementing the first three APIs groups, which is sufficient for our purposes. For a simple example of using Python Kafka APIs, see this book’s <a href="https://oreil.ly/0VJ3D">GitHub repo</a>.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45354775762432">
<h5>Kafka Message Format</h5>
<p>Kafka messages <a data-primary="streaming applications" data-secondary="Apache Kafka" data-tertiary="message format" data-type="indexterm" id="idm45354775761072"/><a data-primary="Apache Kafka" data-secondary="message format" data-type="indexterm" id="idm45354775759408"/><a data-primary="Kafka" data-secondary="message format" data-type="indexterm" id="idm45354775758464"/><a data-primary="message format in Kafka" data-type="indexterm" id="idm45354775757520"/><a data-primary="marshaling in Kafka" data-type="indexterm" id="idm45354775756848"/>are byte arrays, so we need to serialize our messages (called <em>marshaling</em> in Kafka) using, for example, <a href="https://oreil.ly/UxEdi">Apache Avro</a>, <a href="https://oreil.ly/mu3Ud">Google Protocol Buffers</a>, <a href="https://oreil.ly/U6nCq">JSON</a>, or <a href="https://oreil.ly/eTzXL">Python pickling</a>. The Python Kafka GitHub repository provides a handful of <a href="https://oreil.ly/Q11qg">examples</a> of using encoding with Kafka. To simplify code examples, use JSON throughout, but make sure that you pick up an appropriate marshaling for your implementation. When deciding on the format, you need to consider its performance (remember we marshal/unmarshal every message), size (smaller messages are written/read faster), message extensibility (implementation behavior when a message is changed by, for example, adding or removing a field), and language interoperability. Simon Aubury presents a good overview of <a href="https://oreil.ly/aEN91">marshaling methods</a>.</p>
</div></aside>

<p>Unlike other messaging systems, Kafka does not guarantee nonduplicate messages. Instead, each Kafka consumer is responsible for ensuring that messages are processed only once.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>If you are interested in learning more, the Confluent <a href="https://oreil.ly/QOfMd">“Kafka Python Client” documentation</a> has more information on commit options and their implications on delivery guarantees. By default, the Python client uses automatic commit, which is what we use in our examples. For real-life implementation, consider delivery guarantees (exactly once, at least once, etc.) that you need to provide and use an appropriate commit <a data-primary="streaming applications" data-secondary="Apache Kafka" data-startref="streaming-kafka-api" data-tertiary="APIs" data-type="indexterm" id="idm45354775749200"/><a data-primary="Apache Kafka" data-secondary="APIs" data-startref="apache-kafka-api" data-type="indexterm" id="idm45354775747680"/><a data-primary="Kafka" data-secondary="APIs" data-startref="kafka-api" data-type="indexterm" id="idm45354775746464"/>approach.</p>
</div>
</div></section>
</div></section>






<section data-pdf-bookmark="Using Kafka with Ray" data-type="sect1"><div class="sect1" id="idm45354775744736">
<h1>Using Kafka with Ray</h1>

<p>Now <a data-primary="streaming applications" data-secondary="Apache Kafka" data-tertiary="implementation with Ray" data-type="indexterm" id="streaming-kafka-rayimplement"/><a data-primary="Apache Kafka" data-secondary="implementation with Ray" data-type="indexterm" id="apache-kafka-rayimplement"/><a data-primary="Kafka" data-secondary="implementation with Ray" data-type="indexterm" id="kafka-rayimplement"/>that you know about Kafka and its basic APIs, let’s take a look at options for integrating Kafka with Ray. We will implement both the Kafka consumer and producer as Ray actors.<sup><a data-type="noteref" href="ch06.html#idm45354775738704" id="idm45354775738704-marker">4</a></sup> You can benefit from using Ray actors with Kafka for these reasons:</p>

<ul>
<li>
<p>Kafka consumers run in an infinite loop, waiting for new records to arrive, and need to keep track of messages consumed. Being a stateful service, the Ray actor provides an ideal paradigm for implementing a Kafka consumer.</p>
</li>
<li>
<p>By putting your Kafka producer in an actor, you can write records to any Kafka topic asynchronously without having to create separate producers.</p>
</li>
</ul>

<p>A simple implementation of a Kafka producer actor <a data-primary="remote actors" data-secondary="Kafka producers as" data-type="indexterm" id="remote-actor-kafka-produce"/>looks like <a data-type="xref" href="#Kafka-producer-actor">Example 6-1</a>.</p>
<div data-type="example" id="Kafka-producer-actor">
<h5><span class="label">Example 6-1. </span><a href="https://oreil.ly/5Ycum">Kafka producer actor</a></h5>

<pre data-code-language="python" data-type="programlisting"><code class="nd">@ray</code><code class="o">.</code><code class="n">remote</code>
<code class="k">class</code> <code class="nc">KafkaProducer</code><code class="p">:</code>
   <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">broker</code><code class="p">:</code> <code class="nb">str</code> <code class="o">=</code> <code class="s1">'localhost:9092'</code><code class="p">):</code>
       <code class="kn">from</code> <code class="nn">confluent_kafka</code> <code class="kn">import</code> <code class="n">Producer</code>
       <code class="n">conf</code> <code class="o">=</code> <code class="p">{</code><code class="s1">'bootstrap.servers'</code><code class="p">:</code> <code class="n">broker</code><code class="p">}</code>
       <code class="bp">self</code><code class="o">.</code><code class="n">producer</code> <code class="o">=</code> <code class="n">Producer</code><code class="p">(</code><code class="o">**</code><code class="n">conf</code><code class="p">)</code>

   <code class="k">def</code> <code class="nf">produce</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">data</code><code class="p">:</code> <code class="nb">dict</code><code class="p">,</code> <code class="n">key</code><code class="p">:</code> <code class="nb">str</code> <code class="o">=</code> <code class="kc">None</code><code class="p">,</code> <code class="n">topic</code><code class="p">:</code> <code class="nb">str</code> <code class="o">=</code> <code class="s1">'test'</code><code class="p">):</code>

       <code class="k">def</code> <code class="nf">delivery_callback</code><code class="p">(</code><code class="n">err</code><code class="p">,</code> <code class="n">msg</code><code class="p">):</code>
           <code class="k">if</code> <code class="n">err</code><code class="p">:</code>
               <code class="nb">print</code><code class="p">(</code><code class="s1">'Message failed delivery: '</code><code class="p">,</code> <code class="n">err</code><code class="p">)</code>
           <code class="k">else</code><code class="p">:</code>
               <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Message delivered to topic </code><code class="si">{</code><code class="n">msg</code><code class="o">.</code><code class="n">topic</code><code class="p">()</code><code class="si">}</code><code class="s2"> "</code> <code class="o">+</code>
               <code class="sa">f</code><code class="s2">"partition </code><code class="si">{</code><code class="n">msg</code><code class="o">.</code><code class="n">partition</code><code class="p">()</code><code class="si">}</code><code class="s2"> offset </code><code class="si">{</code><code class="n">msg</code><code class="o">.</code><code class="n">offset</code><code class="p">()</code><code class="si">}</code><code class="s2">')</code>

       <code class="n">binary_key</code> <code class="o">=</code> <code class="kc">None</code>
       <code class="k">if</code> <code class="n">key</code> <code class="ow">is</code> <code class="ow">not</code> <code class="kc">None</code><code class="p">:</code>
           <code class="n">binary_key</code> <code class="o">=</code> <code class="n">key</code><code class="o">.</code><code class="n">encode</code><code class="p">(</code><code class="s1">'UTF8'</code><code class="p">)</code>
       <code class="bp">self</code><code class="o">.</code><code class="n">producer</code><code class="o">.</code><code class="n">produce</code><code class="p">(</code><code class="n">topic</code><code class="o">=</code><code class="n">topic</code><code class="p">,</code> <code class="n">value</code><code class="o">=</code><code class="n">json</code><code class="o">.</code><code class="n">dumps</code><code class="p">(</code><code class="n">data</code><code class="p">)</code><code class="o">.</code><code class="n">encode</code><code class="p">(</code><code class="s1">'UTF8'</code><code class="p">),</code>
       <code class="n">key</code><code class="o">=</code><code class="n">binary_key</code><code class="p">,</code> <code class="n">callback</code><code class="o">=</code><code class="n">delivery_callback</code><code class="p">)</code>
       <code class="bp">self</code><code class="o">.</code><code class="n">producer</code><code class="o">.</code><code class="n">poll</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>

   <code class="k">def</code> <code class="nf">destroy</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
       <code class="bp">self</code><code class="o">.</code><code class="n">producer</code><code class="o">.</code><code class="n">flush</code><code class="p">(</code><code class="mi">30</code><code class="p">)</code></pre></div>

<p>The actor implementation in this example includes the following methods:</p>
<dl>
<dt>The constructor</dt>
<dd>
<p>This method initializes the Kafka producer based on the location of the Kafka cluster.</p>
</dd>
<dt><code>produce</code></dt>
<dd>
<p>This is the method you will call to send data. It takes data to write to Kafka (as a Python dictionary), an optional key (as a string), and the Kafka topic to write to. Here we chose to use a dictionary for the data as it is a fairly generic way to represent data and can be easily marshaled/unmarshaled to JSON. For debugging, we added an internal <code>delivery_callback</code> method that prints out when a message is written or an error has occurred.</p>
</dd>
<dt><code>destroy</code></dt>
<dd>
<p>Ray calls this method before exiting the application. Our <code>destroy</code> method waits for up to 30 seconds for any outstanding messages to be delivered and for delivery report callbacks to be<a data-primary="remote actors" data-secondary="Kafka producers as" data-startref="remote-actor-kafka-produce" data-type="indexterm" id="idm45354775632656"/> triggered.</p>
</dd>
</dl>

<p><a data-type="xref" href="#Kafka-consumer-actor">Example 6-2</a> shows <a data-primary="remote actors" data-secondary="Kafka consumers as" data-type="indexterm" id="remote-actor-kafka-comsume"/>a simple implementation of a Kafka consumer actor.</p>
<div data-type="example" id="Kafka-consumer-actor">
<h5><span class="label">Example 6-2. </span><a href="https://oreil.ly/5Ycum">Kafka consumer actor</a></h5>

<pre data-code-language="python" data-type="programlisting"><code class="nd">@ray</code><code class="o">.</code><code class="n">remote</code>
<code class="k">class</code> <code class="nc">KafkaConsumer</code><code class="p">:</code>
   <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">callback</code><code class="p">,</code> <code class="n">group</code><code class="p">:</code> <code class="nb">str</code> <code class="o">=</code> <code class="s1">'ray'</code><code class="p">,</code> <code class="n">broker</code><code class="p">:</code> <code class="nb">str</code> <code class="o">=</code> <code class="s1">'localhost:9092'</code><code class="p">,</code>
       <code class="n">topic</code><code class="p">:</code> <code class="nb">str</code> <code class="o">=</code> <code class="s1">'test'</code><code class="p">,</code> <code class="n">restart</code><code class="p">:</code> <code class="nb">str</code> <code class="o">=</code> <code class="s1">'latest'</code><code class="p">):</code>
       <code class="kn">from</code> <code class="nn">confluent_kafka</code> <code class="kn">import</code> <code class="n">Consumer</code>
       <code class="kn">from</code> <code class="nn">uuid</code> <code class="kn">import</code> <code class="n">uuid4</code>
       <code class="c1"># Configuration</code>
       <code class="n">consumer_conf</code> <code class="o">=</code> <code class="p">{</code><code class="s1">'bootstrap.servers'</code><code class="p">:</code> <code class="n">broker</code><code class="p">,</code>   <code class="c1"># Bootstrap server</code>
                <code class="s1">'group.id'</code><code class="p">:</code> <code class="n">group</code><code class="p">,</code>                      <code class="c1"># Group ID</code>
                <code class="s1">'session.timeout.ms'</code><code class="p">:</code> <code class="mi">6000</code><code class="p">,</code>      <code class="c1"># Session tmout</code>
                <code class="s1">'auto.offset.reset'</code><code class="p">:</code> <code class="n">restart</code><code class="p">}</code>         <code class="c1"># Restart</code>

       <code class="c1"># Create Consumer instance</code>
       <code class="bp">self</code><code class="o">.</code><code class="n">consumer</code> <code class="o">=</code> <code class="n">Consumer</code><code class="p">(</code><code class="n">consumer_conf</code><code class="p">)</code>
       <code class="bp">self</code><code class="o">.</code><code class="n">topic</code> <code class="o">=</code> <code class="n">topic</code>
       <code class="bp">self</code><code class="o">.</code><code class="n">id</code> <code class="o">=</code> <code class="nb">str</code><code class="p">(</code><code class="n">uuid4</code><code class="p">())</code>
       <code class="bp">self</code><code class="o">.</code><code class="n">callback</code> <code class="o">=</code> <code class="n">callback</code>

   <code class="k">def</code> <code class="nf">start</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
       <code class="bp">self</code><code class="o">.</code><code class="n">run</code> <code class="o">=</code> <code class="kc">True</code>
       <code class="k">def</code> <code class="nf">print_assignment</code><code class="p">(</code><code class="n">consumer</code><code class="p">,</code> <code class="n">partitions</code><code class="p">):</code>
           <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Consumer </code><code class="si">{</code><code class="bp">self</code><code class="o">.</code><code class="n">id</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>
           <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Assignment: </code><code class="si">{</code><code class="n">partitions</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>

       <code class="c1"># Subscribe to topics</code>
       <code class="bp">self</code><code class="o">.</code><code class="n">consumer</code><code class="o">.</code><code class="n">subscribe</code><code class="p">([</code><code class="bp">self</code><code class="o">.</code><code class="n">topic</code><code class="p">],</code> <code class="n">on_assign</code> <code class="o">=</code> <code class="n">print_assignment</code><code class="p">)</code>
       <code class="k">while</code> <code class="bp">self</code><code class="o">.</code><code class="n">run</code><code class="p">:</code>
           <code class="n">msg</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">consumer</code><code class="o">.</code><code class="n">poll</code><code class="p">(</code><code class="n">timeout</code><code class="o">=</code><code class="mf">1.0</code><code class="p">)</code>
           <code class="k">if</code> <code class="n">msg</code> <code class="ow">is</code> <code class="kc">None</code><code class="p">:</code>
               <code class="k">continue</code>
           <code class="k">if</code> <code class="n">msg</code><code class="o">.</code><code class="n">error</code><code class="p">():</code>
               <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Consumer error: </code><code class="si">{</code><code class="n">msg</code><code class="o">.</code><code class="n">error</code><code class="p">()</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>
           <code class="k">else</code><code class="p">:</code>
               <code class="c1"># Proper message</code>
               <code class="bp">self</code><code class="o">.</code><code class="n">callback</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">id</code><code class="p">,</code> <code class="n">msg</code><code class="p">)</code>
   <code class="k">def</code> <code class="nf">stop</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
       <code class="bp">self</code><code class="o">.</code><code class="n">run</code> <code class="o">=</code> <code class="kc">False</code>

   <code class="k">def</code> <code class="nf">destroy</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
       <code class="bp">self</code><code class="o">.</code><code class="n">consumer</code><code class="o">.</code><code class="n">close</code><code class="p">()</code></pre></div>

<p>The consumer actor in this example has the following methods:</p>
<dl>
<dt>The constructor</dt>
<dd>
<p>Initializes the Kafka consumer. Here we have more parameters compared to a producer. In addition to the broker location, you need to specify the following:</p>

<ul>
<li>
<p>Topic name</p>
</li>
<li>
<p>Consumer group name (for parallel runs)</p>
</li>
<li>
<p>Restart, which configures how the client behaves when starting with no offset or if the current offset does not exist anymore on the server<sup><a data-type="noteref" href="ch06.html#idm45354775188896" id="idm45354775188896-marker">5</a></sup></p>
</li>
<li>
<p>Callback, which is a pointer to the customer’s function that is used to process a message</p>
</li>
</ul>
</dd>
<dt><code>start</code></dt>
<dd>
<p>Runs an infinite loop polling for records. In our example, new records are just printed. For debugging, we also print the consumer’s assignment (which partitions it is consuming).</p>
</dd>
<dt><code>stop</code></dt>
<dd>
<p>Updates the class property that stops the infinite loop.</p>
</dd>
<dt><code>destroy</code></dt>
<dd>
<p>Called by Ray before exiting the application to terminate<a data-primary="remote actors" data-secondary="Kafka consumers as" data-startref="remote-actor-kafka-comsume" data-type="indexterm" id="idm45354775181200"/> the consumers.</p>
</dd>
</dl>

<p>In addition <a data-primary="topics in Kafka" data-type="indexterm" id="idm45354775153728"/>to these two actors, we also need to set up the Kafka topics. While Kafka auto-creates new topics as they are used, the default parameters for the number of partitions and <a href="https://oreil.ly/ew9Oc">replication factor</a> may not match your needs. We create the topic with our preferred settings in <a data-type="xref" href="#Topics-set-up-function">Example 6-3</a>.</p>
<div data-type="example" id="Topics-set-up-function">
<h5><span class="label">Example 6-3. </span><a href="https://oreil.ly/cKafn">Topics setup function</a></h5>

<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">setup_topics</code><code class="p">(</code><code class="n">broker</code><code class="p">:</code> <code class="nb">str</code> <code class="o">=</code> <code class="s1">'localhost:9092'</code><code class="p">,</code> <code class="n">topics</code><code class="p">:</code> <code class="p">[]</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'test'</code><code class="p">],</code>
                <code class="n">partitions</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">10</code><code class="p">,</code> <code class="n">replication</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">1</code><code class="p">):</code>
   <code class="c1"># Re-create topic</code>
   <code class="c1"># Wait for operation completion method</code>
   <code class="k">def</code> <code class="nf">wait_for_operation_completion</code><code class="p">(</code><code class="n">futures</code><code class="p">:</code> <code class="nb">dict</code><code class="p">,</code> <code class="n">success</code><code class="p">:</code> <code class="nb">str</code><code class="p">,</code> <code class="n">failure</code><code class="p">:</code> <code class="nb">str</code><code class="p">):</code>
       <code class="k">for</code> <code class="n">topic</code><code class="p">,</code> <code class="n">f</code> <code class="ow">in</code> <code class="n">futures</code><code class="o">.</code><code class="n">items</code><code class="p">():</code>
           <code class="k">try</code><code class="p">:</code>
               <code class="n">f</code><code class="o">.</code><code class="n">result</code><code class="p">()</code>  <code class="c1"># The result itself is None</code>
               <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Topic </code><code class="si">{</code><code class="n">topic</code><code class="si">}</code><code class="s2"> </code><code class="si">{</code><code class="n">success</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>
           <code class="k">except</code> <code class="ne">Exception</code> <code class="k">as</code> <code class="n">e</code><code class="p">:</code>
               <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"</code><code class="si">{</code><code class="n">failure</code><code class="si">}</code><code class="s2"> </code><code class="si">{</code><code class="n">topic</code><code class="si">}</code><code class="s2"> error </code><code class="si">{</code><code class="n">e</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>

   <code class="n">admin</code> <code class="o">=</code> <code class="n">AdminClient</code><code class="p">({</code><code class="s1">'bootstrap.servers'</code><code class="p">:</code> <code class="n">broker</code><code class="p">})</code>

   <code class="c1"># Delete topics</code>
   <code class="n">fs</code> <code class="o">=</code> <code class="n">admin</code><code class="o">.</code><code class="n">delete_topics</code><code class="p">(</code><code class="n">topics</code><code class="p">)</code>

   <code class="c1"># Wait for each operation to finish.</code>
   <code class="n">wait_for_operation_completion</code><code class="p">(</code><code class="n">fs</code><code class="p">,</code> <code class="s2">" is deleted"</code><code class="p">,</code> <code class="s2">"Failed to delete topic "</code><code class="p">)</code>

   <code class="c1"># Wait to make sure topic is deleted</code>
   <code class="n">sleep</code><code class="p">(</code><code class="mi">3</code><code class="p">)</code>
   <code class="c1"># Call create_topics to asynchronously create topics.</code>
   <code class="n">new_topics</code> <code class="o">=</code> <code class="p">[</code><code class="n">NewTopic</code><code class="p">(</code><code class="n">topic</code><code class="p">,</code> <code class="n">num_partitions</code><code class="o">=</code><code class="n">partitions</code><code class="p">,</code>
                          <code class="n">replication_factor</code><code class="o">=</code><code class="n">replication</code><code class="p">)</code> <code class="k">for</code> <code class="n">topic</code> <code class="ow">in</code> <code class="n">topics</code><code class="p">]</code>
   <code class="n">fs</code> <code class="o">=</code> <code class="n">admin</code><code class="o">.</code><code class="n">create_topics</code><code class="p">(</code><code class="n">new_topics</code><code class="p">)</code>

   <code class="c1"># Wait for each operation to finish.</code>
   <code class="n">wait_for_operation_completion</code><code class="p">(</code><code class="n">fs</code><code class="p">,</code> <code class="s2">" is created"</code><code class="p">,</code> <code class="s2">"Failed to create topic "</code><code class="p">)</code></pre></div>

<p>Because the topics may already exist, the code first deletes them. Once the deletion is completed, the code waits a short time to make sure that deletion took place on the cluster and then re-creates topics with the target number of partitions and replication factor.</p>

<p>With these three components in place, you can now create a Ray application to publish and read from Kafka. You can run this application either locally or on a cluster. The Ray application itself looks like <a data-type="xref" href="#bringing-it-all-together">Example 6-4</a>.</p>
<div data-type="example" id="bringing-it-all-together">
<h5><span class="label">Example 6-4. </span><a href="https://oreil.ly/97nue">Bringing it all together</a></h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># Simple callback function to print topics</code>
<code class="k">def</code> <code class="nf">print_message</code><code class="p">(</code><code class="n">consumer_id</code><code class="p">:</code> <code class="nb">str</code><code class="p">,</code> <code class="n">msg</code><code class="p">):</code>
    <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Consumer </code><code class="si">{</code><code class="n">consumer_id</code><code class="si">}</code><code class="s2"> new message: topic=</code><code class="si">{</code><code class="n">msg</code><code class="o">.</code><code class="n">topic</code><code class="p">()</code><code class="si">}</code><code class="s2">  "</code>
          <code class="sa">f</code><code class="s2">"partition= </code><code class="si">{</code><code class="n">msg</code><code class="o">.</code><code class="n">partition</code><code class="p">()</code><code class="si">}</code><code class="s2">  offset=</code><code class="si">{</code><code class="n">msg</code><code class="o">.</code><code class="n">offset</code><code class="p">()</code><code class="si">}</code><code class="s2"> "</code>
          <code class="sa">f</code><code class="s2">"key=</code><code class="si">{</code><code class="n">msg</code><code class="o">.</code><code class="n">key</code><code class="p">()</code><code class="o">.</code><code class="n">decode</code><code class="p">(</code><code class="s1">'UTF8'</code><code class="p">)</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>
    <code class="nb">print</code><code class="p">(</code><code class="n">json</code><code class="o">.</code><code class="n">loads</code><code class="p">(</code><code class="n">msg</code><code class="o">.</code><code class="n">value</code><code class="p">()</code><code class="o">.</code><code class="n">decode</code><code class="p">(</code><code class="s1">'UTF8'</code><code class="p">)))</code>
<code class="c1"># Set up topics</code>
<code class="n">setup_topics</code><code class="p">()</code>
<code class="c1"># Set up random number generator</code>
<code class="n">seed</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code>
<code class="c1"># Start Ray</code>
<code class="n">ray</code><code class="o">.</code><code class="n">init</code><code class="p">()</code>
<code class="c1"># Start consumers and producers</code>
<code class="n">n_consumers</code> <code class="o">=</code> <code class="mi">1</code>     <code class="c1"># Number of consumers</code>
<code class="n">consumers</code> <code class="o">=</code> <code class="p">[</code><code class="n">KafkaConsumer</code><code class="o">.</code><code class="n">remote</code><code class="p">(</code><code class="n">print_message</code><code class="p">)</code> <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">n_consumers</code><code class="p">)]</code>
<code class="n">producer</code> <code class="o">=</code> <code class="n">KafkaProducer</code><code class="o">.</code><code class="n">remote</code><code class="p">()</code>
<code class="n">refs</code> <code class="o">=</code> <code class="p">[</code><code class="n">c</code><code class="o">.</code><code class="n">start</code><code class="o">.</code><code class="n">remote</code><code class="p">()</code> <code class="k">for</code> <code class="n">c</code> <code class="ow">in</code> <code class="n">consumers</code><code class="p">]</code>
<code class="c1"># Publish messages</code>
<code class="n">user_name</code> <code class="o">=</code> <code class="s1">'john'</code>
<code class="n">user_favorite_color</code> <code class="o">=</code> <code class="s1">'blue'</code>
<code class="c1"># Loop forever publishing messages to Kafka</code>
<code class="k">try</code><code class="p">:</code>
   <code class="k">while</code> <code class="kc">True</code><code class="p">:</code>
       <code class="n">user</code> <code class="o">=</code> <code class="p">{</code>
           <code class="s1">'name'</code><code class="p">:</code> <code class="n">user_name</code><code class="p">,</code>
           <code class="s1">'favorite_color'</code><code class="p">:</code> <code class="n">user_favorite_color</code><code class="p">,</code>
           <code class="s1">'favorite_number'</code><code class="p">:</code> <code class="n">randint</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1000</code><code class="p">)</code>
       <code class="p">}</code>
       <code class="n">producer</code><code class="o">.</code><code class="n">produce</code><code class="o">.</code><code class="n">remote</code><code class="p">(</code><code class="n">user</code><code class="p">,</code> <code class="nb">str</code><code class="p">(</code><code class="n">randint</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">100</code><code class="p">)))</code>
       <code class="n">sleep</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># End gracefully</code>
<code class="k">except</code> <code class="ne">KeyboardInterrupt</code><code class="p">:</code>
   <code class="k">for</code> <code class="n">c</code> <code class="ow">in</code> <code class="n">consumers</code><code class="p">:</code>
       <code class="n">c</code><code class="o">.</code><code class="n">stop</code><code class="o">.</code><code class="n">remote</code><code class="p">()</code>
<code class="k">finally</code><code class="p">:</code>
   <code class="k">for</code> <code class="n">c</code> <code class="ow">in</code> <code class="n">consumers</code><code class="p">:</code>
       <code class="n">c</code><code class="o">.</code><code class="n">destroy</code><code class="o">.</code><code class="n">remote</code><code class="p">()</code>
   <code class="n">producer</code><code class="o">.</code><code class="n">destroy</code><code class="o">.</code><code class="n">remote</code><code class="p">()</code>
   <code class="n">ray</code><code class="o">.</code><code class="n">kill</code><code class="p">(</code><code class="n">producer</code><code class="p">)</code></pre></div>

<p>This code does the following:</p>
<ol>
<li>
<p>Defines a simple callback function for the Kafka consumer that just prints the message.</p>
</li>
<li>
<p>Initializes Ray.</p>
</li>
<li>
<p>Creates required topics.</p>
</li>
<li>
<p>Starts both producer and consumers (the code allows us to specify the number of consumers we want to use).</p>
</li>
<li>
<p>Calls the <code>start</code> method on all created consumers.</p>
</li>
<li>
<p>Once all consumers are created, the producer starts sending Kafka requests every second.</p>
</li>

</ol>

<p>Additionally, the code implements graceful termination, ensuring that all resources are cleaned up, once the job is interrupted.</p>

<p>Once the code runs, it produces the output shown in <a data-type="xref" href="#execution-results-for-a-single-consumer">Example 6-5</a>.</p>
<div data-type="example" id="execution-results-for-a-single-consumer">
<h5><span class="label">Example 6-5. </span>Execution results for a single consumer</h5>

<pre data-type="programlisting">Topic  test  is deleted
Topic  test  is created
2021-08-23 17:00:57,951	INFO services.py:1264 -- View the Ray dashboard at http://...
(pid=19981) Consumer  04c698a5-db3a-4da9-86df-cd7d6fb7dc6d
(pid=19981) Assignment: [TopicPartition{topic=test,partition=0,offset=-1001,error=...
…………………………………………………………………………………………..
(pid=19981) Consumer  ... new message: topic= test  partition= 8  offset= 0  key= 57
(pid=19981) {'name': 'john', 'favorite_color': 'blue', 'favorite_number': 779}
(pid=19981) Consumer  ... new message: topic= test  partition= 2  offset= 0  key= 63
(pid=19981) {'name': 'john', 'favorite_color': 'blue', 'favorite_number': 120}
(pid=19981) Consumer  ... new message: topic= test  partition= 8  offset= 1  key= 83
(pid=19981) {'name': 'john', 'favorite_color': 'blue', 'favorite_number': 483}
(pid=19977) Message delivered to topic  test  partition  8  offset 0
(pid=19977) Message delivered to topic  test  partition  2  offset 0
(pid=19977) Message delivered to topic  test  partition  8  offset 1
(pid=19981) Consumer  ... new message: topic= test  partition= 8  offset= 2  key= 100
(pid=19981) {'name': 'john', 'favorite_color': 'blue', 'favorite_number': 388}
(pid=19981) Consumer  ... new message: topic= test  partition= 5  offset= 0  key= 12
(pid=19981) {'name': 'john', 'favorite_color': 'blue', 'favorite_number': 214}
(pid=19977) Message delivered to topic  test  partition  8  offset 2
(pid=19981) Consumer  ... new message: topic= test  partition= 1  offset= 0  key= 3
(pid=19981) {'name': 'john', 'favorite_color': 'blue', 'favorite_number': 499}
(pid=19977) Message delivered to topic  test  partition  5  offset 0
(pid=19981) Consumer  ... new message: topic= test  partition= 6  offset= 0  key= 49
(pid=19981) {'name': 'john', 'favorite_color': 'blue', 'favorite_number': 914}
(pid=19977) Message delivered to topic  test  partition  1  offset 0
(pid=19977) Message delivered to topic  test  partition  6  offset 0
(pid=19981) Consumer  ... new message: topic= test  partition= 8  offset= 3  key= 77
…………………………...</pre></div>

<p>As you can see from the results, the execution does the following:</p>
<ol>
<li>
<p>Deletes and re-creates the topic <code>test</code>.</p>
</li>
<li>
<p>Creates a consumer listening to all the partitions of a topic (we are running a single consumer here).</p>
</li>
<li>
<p>Processes messages. Note here that the producer’s messages are delivered to different partitions but are always received and processed by a single consumer<a data-primary="streaming applications" data-secondary="Apache Kafka" data-startref="streaming-kafka-rayimplement" data-tertiary="implementation with Ray" data-type="indexterm" id="idm45354774601440"/><a data-primary="Apache Kafka" data-secondary="implementation with Ray" data-startref="apache-kafka-rayimplement" data-type="indexterm" id="idm45354774600112"/><a data-primary="Kafka" data-secondary="implementation with Ray" data-startref="kafka-rayimplement" data-type="indexterm" id="idm45354774599024"/>.</p>
</li>

</ol>
</div></section>






<section data-pdf-bookmark="Scaling Our Implementation" data-type="sect1"><div class="sect1" id="idm45354775743824">
<h1>Scaling Our Implementation</h1>

<p>Now that<a data-primary="streaming applications" data-secondary="Apache Kafka" data-tertiary="scaling implementation" data-type="indexterm" id="streaming-kafka-scale"/><a data-primary="Apache Kafka" data-secondary="scaling implementation" data-type="indexterm" id="apache-kafka-scale"/><a data-primary="Kafka" data-secondary="scaling implementation" data-type="indexterm" id="kafka-scale"/><a data-primary="scaling" data-secondary="streaming applications" data-type="indexterm" id="scale-stream-apps"/> everything is working, let’s see how to scale our implementation. As discussed earlier in this chapter, the basic approach to scale an application that reads from Kafka is to increase the number of Kafka consumers (assuming that the topic has enough partitions). Luckily, the code (<a data-type="xref" href="#bringing-it-all-together">Example 6-4</a>) already supports this, so we can easily increase the number of consumers by setting <code>n_consumer=5</code>. Once this update is done, rerunning the code will produce the output in <a data-type="xref" href="#execution-results-for-five-consumers">Example 6-6</a>.</p>
<div data-type="example" id="execution-results-for-five-consumers">
<h5><span class="label">Example 6-6. </span>Execution results for five consumers</h5>

<pre data-type="programlisting">Topic  test  is deleted
Topic  test  is created
2021-08-23 17:15:12,353	INFO services.py:1264 -- View the Ray dashboard at http://...
(pid=20100) Message delivered to topic  test  partition  8  offset 0
(pid=20100) Message delivered to topic  test  partition  2  offset 0
(pid=20103) Consumer  9e2773d4-f006-4d4d-aac3-fe75ed27f44b
(pid=20103) Assignment: [TopicPartition{topic=test,partition=0,offset=-1001,error=...
(pid=20107) Consumer  bdedddd9-db16-4c24-a7ef-338e91b4e100
(pid=20107) Assignment: [TopicPartition{topic=test,partition=4,offset=-1001,error=...
(pid=20101) Consumer  d76b7fad-0b98-4e03-92e3-510aac2fcb11
(pid=20101) Assignment: [TopicPartition{topic=test,partition=6,offset=-1001,error=...
(pid=20106) Consumer  e3d181af-d095-4b7f-b3d6-830299c207a8
……………………………………………………………………………………..
(pid=20100) Message delivered to topic  test  partition  8  offset 1
(pid=20104) Consumer ... new message: topic= test  partition= 8  offset= 2  key= 100
(pid=20104) {'name': 'john', 'favorite_color': 'blue', 'favorite_number': 388}
(pid=20100) Message delivered to topic  test  partition  8  offset 2
(pid=20107) Consumer ... new message: topic= test  partition= 5  offset= 0  key= 12
(pid=20107) {'name': 'john', 'favorite_color': 'blue', 'favorite_number': 214}
(pid=20100) Message delivered to topic  test  partition  5  offset 0
(pid=20103) Consumer ... new message: topic= test  partition= 1  offset= 0  key= 3
(pid=20103) {'name': 'john', 'favorite_color': 'blue', 'favorite_number': 499}
(pid=20100) Message delivered to topic  test  partition  1  offset 0
(pid=20101) Consumer ... new message: topic= test  partition= 6  offset= 0  key= 49
(pid=20101) {'name': 'john', 'favorite_color': 'blue', 'favorite_number': 914}
(pid=20100) Message delivered to topic  test  partition  6  offset 0
(pid=20104) Consumer ... new message: topic= test  partition= 8  offset= 3  key= 77
(pid=20104) {'name': 'john', 'favorite_color': 'blue', 'favorite_number': 443}
(pid=20100) Message delivered to topic  test  partition  8  offset 3
(pid=20103) Consumer ... new message: topic= test  partition= 1  offset= 1  key= 98
(pid=20103) {'name': 'john', 'favorite_color': 'blue', 'favorite_number': 780}
……………………………………………………….</pre></div>

<p>Here, unlike <a data-type="xref" href="#execution-results-for-a-single-consumer">Example 6-5</a>, each of the five Kafka consumers starts listening on 2 partitions (remember, our topic uses 10 partitions). You can also see that as 
<span class="keep-together">messages</span> are delivered on different partitions, they are being processed by different consumer instances. So we can scale our Kafka applications manually, but what about 
<span class="keep-together">autoscaling?</span></p>

<p>Unlike native Kubernetes autoscalers—for example, <a href="https://oreil.ly/zJ3yw">KEDA</a>, which scales consumers based on the <a href="https://oreil.ly/u7uEE">queue depth</a>—Ray uses a <a href="https://oreil.ly/4cgo2">different approach</a>. Instead of bringing up and down Kafka consumers, Ray uses a fixed number of consumers and spreads them across nodes (adding nodes if required). This gives better performance for each consumer but still runs into issues when there are not enough partitions.</p>

<p>Now that you know how to integrate Ray with Kafka, let’s discuss how to use this technique for building streaming <a data-primary="streaming applications" data-secondary="Apache Kafka" data-startref="streaming-kafka-scale" data-tertiary="scaling implementation" data-type="indexterm" id="idm45354774578944"/><a data-primary="Apache Kafka" data-secondary="scaling implementation" data-startref="apache-kafka-scale" data-type="indexterm" id="idm45354774577424"/><a data-primary="Kafka" data-secondary="scaling implementation" data-startref="kafka-scale" data-type="indexterm" id="idm45354774576208"/><a data-primary="scaling" data-secondary="streaming applications" data-startref="scale-stream-apps" data-type="indexterm" id="idm45354774574992"/>applications.</p>
</div></section>






<section data-pdf-bookmark="Building Stream-Processing Applications with Ray" data-type="sect1"><div class="sect1" id="idm45354774573392">
<h1>Building Stream-Processing Applications with Ray</h1>

<p>There are two important classes of stream processing:</p>
<dl>
<dt>Stateless stream processing</dt>
<dd>
<p>Each event <a data-primary="streaming applications" data-secondary="stateless stream processing" data-type="indexterm" id="idm45354774570016"/><a data-primary="stateless stream processing" data-type="indexterm" id="idm45354774568944"/>is handled completely independently from any previous events or mutable shared state. Given an event, the stream processor will treat it exactly the same way every time, no matter what data arrived beforehand or the state of the execution.</p>
</dd>
<dt>Stateful stream processing</dt>
<dd>
<p>A state is <a data-primary="streaming applications" data-secondary="stateful stream processing" data-type="indexterm" id="streaming-stateful"/><a data-primary="stateful stream processing" data-type="indexterm" id="stateful-stream"/>shared among events and can influence the way current events are processed. The state, in this case, can be a result of previous events or produced by an external system, controlling stream processing.</p>
</dd>
</dl>

<p>Stateless stream processing implementations are typically simple and straightforward. They require an extension of the <code>start</code> method of the Kafka consumer (<a data-type="xref" href="#Kafka-consumer-actor">Example 6-2</a>) to implement any required transformation of the incoming messages. The result of these transformations can be sent either to different Kafka topics or to any other part of the code. <a href="https://oreil.ly/S51JF">“Serverless Kafka Stream Processing with Ray”</a> by Javier Redondo describes an example stateless streaming application.</p>

<p>Implementing stateful stream processing is typically more involved. Let’s take a look at options for implementing stateful stream processing based on
<a href="https://oreil.ly/AliOW">dynamically controlled streams</a>.</p>

<p>Our sample implementation uses a heater controller example with the following characteristics:<sup><a data-type="noteref" href="ch06.html#idm45354774560064" id="idm45354774560064-marker">6</a></sup></p>

<ul>
<li>
<p>A message producer provides a constant stream of temperature measurements from the sensor.</p>
</li>
<li>
<p>The thermostat settings are defined as the desired temperature Td and ∆t.</p>
</li>
<li>
<p>The thermostat settings can arrive at any point.</p>
</li>
<li>
<p>When the temperature falls below Td – ∆t, an implementation sends a signal to the heater to start.</p>
</li>
<li>
<p>When the temperature goes above Td + ∆t, a signal is sent to the heater to stop.</p>
</li>
<li>
<p>A very simple heater model is used here, where temperature increases by 1 degree every <em>N</em> (configurable) minutes when the heater is on, and decreases by 1 degree every <em>M</em> (configurable) minutes when it is off.</p>
</li>
</ul>

<p>The following are simplifications that we made to the original example:</p>

<ul>
<li>
<p>Instead of using Protobuf marshaling, we are using JSON marshaling (the same as in the previous examples), which allows us to marshal/unmarshal Python dictionary messages generically.</p>
</li>
<li>
<p>To simplify our implementation, instead of using two queues as in the original sample, we are using a single queue containing both control and sensor messages, discriminating between the two as we receive them. Although it works in our toy example, it might not be a good solution in a real-life implementation with a large volume of messages, because it can slow down sensor message processing.</p>
</li>
</ul>

<p>With these simplifications in place, we will now demonstrate two approaches to implement stateful stream processing with Ray: a key-based approach and a key-independent<a data-primary="streaming applications" data-secondary="stateful stream processing" data-startref="streaming-stateful" data-type="indexterm" id="idm45354774547872"/><a data-primary="stateful stream processing" data-startref="stateful-stream" data-type="indexterm" id="idm45354774546560"/> one.</p>








<section data-pdf-bookmark="Key-Based Approach" data-type="sect2"><div class="sect2" id="idm45354774545344">
<h2>Key-Based Approach</h2>

<p>Many <a data-primary="streaming applications" data-secondary="stateful stream processing" data-tertiary="key-based approach" data-type="indexterm" id="streaming-stateful-keybased"/><a data-primary="stateful stream processing" data-secondary="key-based approach" data-type="indexterm" id="stateful-stream-keybased"/><a data-primary="Apache Kafka" data-secondary="stateful stream processing" data-tertiary="key-based approach" data-type="indexterm" id="apache-kafka-stateful-keybased"/><a data-primary="Kafka" data-secondary="stateful stream processing" data-tertiary="key-based approach" data-type="indexterm" id="kafka-stateful-keybased"/>stateful streaming applications rely on Kafka message keys. Remember that Kafka partitioning uses a key hash to determine which partition a message is written to. This means that Kafka guarantees that all messages with the same key are always picked up by the same consumer. In this case, it is possible to implement stateful stream processing locally on the Kafka consumer that receives them. Because the consumer is implemented as a Ray actor, Ray keeps track of the data inside the actor.<sup><a data-type="noteref" href="ch06.html#idm45354774537376" id="idm45354774537376-marker">7</a></sup></p>

<p>For this implementation, we created a small heater simulator program that you can find in the <a href="https://oreil.ly/A6iTb">accompanying GitHub project</a> that publishes and gets data based on the heater ID.<sup><a data-type="noteref" href="ch06.html#idm45354774533936" id="idm45354774533936-marker">8</a></sup> With this in place, you can implement the temperature controller as in <a data-type="xref" href="#implementation-of-temperature-controller">Example 6-7</a>.</p>
<div data-type="example" id="implementation-of-temperature-controller">
<h5><span class="label">Example 6-7. </span><a href="https://oreil.ly/VoVAk">Implementation of temperature controller</a></h5>

<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">enum</code> <code class="kn">import</code> <code class="n">Enum</code>
<code class="k">class</code> <code class="nc">Action</code><code class="p">(</code><code class="n">Enum</code><code class="p">):</code>
    <code class="n">NONE</code> <code class="o">=</code> <code class="o">-</code><code class="mi">1</code>
    <code class="n">OFF</code> <code class="o">=</code> <code class="mi">0</code>
    <code class="n">ON</code> <code class="o">=</code> <code class="mi">1</code>


<code class="k">class</code> <code class="nc">BaseTemperatureController</code><code class="p">:</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="nb">id</code><code class="p">:</code> <code class="nb">str</code><code class="p">):</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">current_setting</code> <code class="o">=</code> <code class="kc">None</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">previous_command</code> <code class="o">=</code> <code class="o">-</code><code class="mi">1</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">id</code> <code class="o">=</code> <code class="nb">id</code>

    <code class="c1"># Process new message</code>
    <code class="k">def</code> <code class="nf">process_new_message</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">message</code><code class="p">:</code> <code class="nb">dict</code><code class="p">):</code>
        <code class="k">if</code> <code class="s1">'measurement'</code> <code class="ow">in</code> <code class="n">message</code><code class="p">:</code>    <code class="c1"># Measurement request</code>
            <code class="bp">self</code><code class="o">.</code><code class="n">process_sensor_data</code><code class="p">(</code><code class="n">message</code><code class="p">)</code>
        <code class="k">else</code><code class="p">:</code>                           <code class="c1"># Temp set request</code>
            <code class="bp">self</code><code class="o">.</code><code class="n">set_temperature</code><code class="p">(</code><code class="n">message</code><code class="p">)</code>

    <code class="c1"># Set new temperature</code>
    <code class="k">def</code> <code class="nf">set_temperature</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">setting</code><code class="p">:</code> <code class="nb">dict</code><code class="p">):</code>
        <code class="n">desired</code> <code class="o">=</code> <code class="n">setting</code><code class="p">[</code><code class="s1">'temperature'</code><code class="p">]</code>
        <code class="n">updelta</code> <code class="o">=</code> <code class="n">setting</code><code class="p">[</code><code class="s1">'up_delta'</code><code class="p">]</code>
        <code class="n">downdelta</code> <code class="o">=</code> <code class="n">setting</code><code class="p">[</code><code class="s1">'down_delta'</code><code class="p">]</code>
        <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Controller </code><code class="si">{</code><code class="bp">self</code><code class="o">.</code><code class="n">id</code><code class="si">}</code><code class="s1"> new temperature setting </code><code class="si">{</code><code class="n">desired</code><code class="si">}</code><code class="s1"> up '</code>
              <code class="sa">f</code><code class="s1">'delta </code><code class="si">{</code><code class="n">updelta</code><code class="si">}</code><code class="s1"> down delta </code><code class="si">{</code><code class="n">downdelta</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">current_setting</code> <code class="o">=</code> <code class="n">desired</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">up_delta</code> <code class="o">=</code> <code class="n">updelta</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">down_delta</code> <code class="o">=</code> <code class="n">down_delta</code>

    <code class="c1"># Process new measurements</code>
    <code class="k">def</code> <code class="nf">process_sensor_data</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">sensor</code><code class="p">:</code> <code class="nb">dict</code><code class="p">)</code> <code class="o">-&gt;</code><code class="nb">bool</code><code class="p">:</code>
        <code class="c1"># Desired temperature is set, otherwise ignore</code>
        <code class="k">if</code> <code class="bp">self</code><code class="o">.</code><code class="n">current_setting</code> <code class="ow">is</code> <code class="ow">not</code> <code class="kc">None</code><code class="p">:</code>           
            <code class="c1"># Calculate desired action</code>
            <code class="n">measurement</code> <code class="o">=</code> <code class="n">sensor</code><code class="p">[</code><code class="s1">'measurement'</code><code class="p">]</code>
            <code class="n">action</code> <code class="o">=</code> <code class="n">Action</code><code class="o">.</code><code class="n">NONE</code>
            <code class="k">if</code> <code class="n">measurement</code> <code class="o">&gt;</code> <code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">current_setting</code> <code class="o">+</code> <code class="bp">self</code><code class="o">.</code><code class="n">up_delta</code><code class="p">):</code>
                <code class="n">action</code> <code class="o">=</code> <code class="n">Action</code><code class="o">.</code><code class="n">ON</code>
            <code class="k">if</code> <code class="n">measurement</code> <code class="o">&lt;</code> <code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">current_setting</code> <code class="o">-</code> <code class="bp">self</code><code class="o">.</code><code class="n">down_delta</code><code class="p">):</code>
                <code class="n">action</code> <code class="o">=</code> <code class="n">Action</code><code class="o">.</code><code class="n">OFF</code>
            <code class="c1"># New action</code>
            <code class="k">if</code> <code class="n">action</code> <code class="o">!=</code> <code class="n">Action</code><code class="o">.</code><code class="n">NONE</code> <code class="ow">and</code> <code class="bp">self</code><code class="o">.</code><code class="n">previous_command</code> <code class="o">!=</code> <code class="n">action</code><code class="p">:</code>  
                <code class="bp">self</code><code class="o">.</code><code class="n">previous_command</code> <code class="o">=</code> <code class="n">action</code>
                <code class="c1"># Publish new action to kafka</code>
                <code class="k">return</code> <code class="kc">True</code>
            <code class="k">else</code><code class="p">:</code>
                <code class="k">return</code> <code class="kc">False</code>
        <code class="k">else</code><code class="p">:</code>
            <code class="k">return</code> <code class="kc">False</code></pre></div>

<p>The implementation is a Python class with the following methods:</p>
<dl>
<dt>The constructor, taking a Kafka producer actor (<a data-type="xref" href="#Kafka-producer-actor">Example 6-1</a>)</dt>
<dd>
<p>Used by this class to write control data to Kafka and an ID of this temperature controller (which is the same as heater device ID).</p>
</dd>
<dt><code>process_new_message</code></dt>
<dd>
<p>Receives messages and, depending on their content, calls either <code>set_temperature</code> or <code>process_sensordata</code>.</p>
</dd>
<dt><code>set_temperature</code></dt>
<dd>
<p>Processes a new set temperature method from the thermostat. This message contains the new desired temperature along with additional heater-specific parameters (temperature intervals where controls are ignored).</p>
</dd>
<dt><code>process_sensordata</code></dt>
<dd>
<p>Handles the temperature control. If the desired temperature is set, this method compares the current temperature with the desired one and calculates the desired control (heater on/off). To avoid resending the same control over and over again, this method additionally compares the calculated control value with the current (cached) and submits a new control value only if it has changed.</p>
</dd>
</dl>

<p>Because Kafka calculates partitions based on the key hash, the same partition can serve many keys. To manage multiple keys per partition, we introduced a 
<span class="keep-together"><code>TemperatureControllerManager</code></span> class whose purpose is to manage individual temperature controllers (<a data-type="xref" href="#implementation-of-temperature-controller-manager">Example 6-8</a>).</p>
<div class="example-margin-7" data-type="example" id="implementation-of-temperature-controller-manager">
<h5><span class="label">Example 6-8. </span><a href="https://oreil.ly/sQbyW">Implementation of temperature controller manager</a></h5>

<pre data-code-language="python" data-type="programlisting"><code class="k">class</code> <code class="nc">TemperatureControllerManager</code><code class="p">:</code>
   <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">producer</code><code class="p">:</code> <code class="n">KafkaProducer</code><code class="p">):</code>
       <code class="bp">self</code><code class="o">.</code><code class="n">controllers</code> <code class="o">=</code> <code class="p">{}</code>
       <code class="bp">self</code><code class="o">.</code><code class="n">producer</code> <code class="o">=</code> <code class="n">producer</code>

   <code class="k">def</code> <code class="nf">process_controller_message</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">key</code><code class="p">:</code> <code class="nb">str</code><code class="p">,</code>  <code class="n">request</code><code class="p">:</code> <code class="nb">dict</code><code class="p">):</code>
       <code class="k">if</code> <code class="ow">not</code> <code class="n">key</code> <code class="ow">in</code> <code class="bp">self</code><code class="o">.</code><code class="n">controllers</code><code class="p">:</code>   <code class="c1"># Create a new controller</code>
           <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Creating a new controller </code><code class="si">{</code><code class="n">controller_id</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>
           <code class="n">controller</code> <code class="o">=</code> <code class="n">TemperatureController</code><code class="p">(</code><code class="n">producer</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">producer</code><code class="p">,</code> <code class="nb">id</code><code class="o">=</code><code class="n">key</code><code class="p">)</code>
           <code class="bp">self</code><code class="o">.</code><code class="n">controllers</code><code class="p">[</code><code class="n">key</code><code class="p">]</code> <code class="o">=</code> <code class="n">controller</code>
       <code class="bp">self</code><code class="o">.</code><code class="n">controllers</code><code class="p">[</code><code class="n">key</code><code class="p">]</code><code class="o">.</code><code class="n">process_new_message</code><code class="p">(</code><code class="n">request</code><code class="p">)</code></pre></div>

<p>This implementation is based on a dictionary keeping track of temperature controllers based on their IDs. The class provides two methods:</p>
<dl>
<dt>The constructor, taking a Kafka producer actor (<a data-type="xref" href="#Kafka-producer-actor">Example 6-1</a>)</dt>
<dd>
<p>Creates a new empty dictionary of the individual temperature controllers.</p>
</dd>
<dt>The <code>process_controller_message</code> function</dt>
<dd>
<p>Takes every new message received by the <em>local</em> Kafka consumer and, based on a key, decides whether a required temperature controller exists. If not, a new temperature controller is created and stores a reference to it. After it finds or creates the controller, it then passes the message to it for processing.</p>
</dd>
</dl>

<p>To link this implementation to the Kafka consumer, we do need to modify the Kafka consumer (<a data-type="xref" href="#Kafka-consumer-actor">Example 6-2</a>) a little bit (<a data-type="xref" href="#integrating-kafka-consumer-with-temperature-controller-manager">Example 6-9</a>).</p>
<div data-type="example" id="integrating-kafka-consumer-with-temperature-controller-manager">
<h5><span class="label">Example 6-9. </span><a href="https://oreil.ly/5Ycum">Integrating the Kafka consumer with the temperature controller manager</a></h5>

<pre data-code-language="python" data-type="programlisting"><code class="nd">@ray</code><code class="o">.</code><code class="n">remote</code>
<code class="k">class</code> <code class="nc">KafkaConsumer</code><code class="p">:</code>
   <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">producer</code><code class="p">:</code> <code class="n">KafkaProducer</code><code class="p">,</code> <code class="n">group</code><code class="p">:</code> <code class="nb">str</code> <code class="o">=</code> <code class="s1">'ray'</code><code class="p">,</code>
<code class="n">broker</code><code class="p">:</code> <code class="nb">str</code> <code class="o">=</code> <code class="s1">'localhost:9092'</code><code class="p">,</code> <code class="n">topic</code><code class="p">:</code> <code class="nb">str</code> <code class="o">=</code> <code class="s1">'sensor'</code><code class="p">,</code> <code class="n">restart</code><code class="p">:</code> <code class="nb">str</code> <code class="o">=</code> <code class="s1">'earliest'</code><code class="p">):</code>
       <code class="kn">from</code> <code class="nn">confluent_kafka</code> <code class="kn">import</code> <code class="n">Consumer</code>
       <code class="kn">import</code> <code class="nn">logging</code>
       <code class="c1"># Configuration</code>
       <code class="n">consumer_conf</code> <code class="o">=</code> <code class="p">{</code><code class="s1">'bootstrap.servers'</code><code class="p">:</code> <code class="n">broker</code><code class="p">,</code>   <code class="c1"># Bootstrap server</code>
                <code class="s1">'group.id'</code><code class="p">:</code> <code class="n">group</code><code class="p">,</code>                      <code class="c1"># Group ID</code>
                <code class="s1">'session.timeout.ms'</code><code class="p">:</code> <code class="mi">6000</code><code class="p">,</code>            <code class="c1"># Session tmout</code>
                <code class="s1">'auto.offset.reset'</code><code class="p">:</code> <code class="n">restart</code><code class="p">}</code>          <code class="c1"># Restart</code>

       <code class="c1"># Create Consumer instance</code>
       <code class="bp">self</code><code class="o">.</code><code class="n">consumer</code> <code class="o">=</code> <code class="n">Consumer</code><code class="p">(</code><code class="n">consumer_conf</code><code class="p">)</code>
       <code class="bp">self</code><code class="o">.</code><code class="n">topic</code> <code class="o">=</code> <code class="n">topic</code>
       <code class="bp">self</code><code class="o">.</code><code class="n">callback</code> <code class="o">=</code> <code class="n">TemperatureControllerManager</code><code class="p">(</code><code class="n">producer</code><code class="p">)</code><code class="o">.</code>
<code class="n">process_controller_message</code>

   <code class="k">def</code> <code class="nf">start</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
       <code class="bp">self</code><code class="o">.</code><code class="n">run</code> <code class="o">=</code> <code class="kc">True</code>
       <code class="k">def</code> <code class="nf">print_assignment</code><code class="p">(</code><code class="n">consumer</code><code class="p">,</code> <code class="n">partitions</code><code class="p">):</code>
       	<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Assignment: </code><code class="si">{</code><code class="n">partitions</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>

       <code class="c1"># Subscribe to topics</code>
       <code class="bp">self</code><code class="o">.</code><code class="n">consumer</code><code class="o">.</code><code class="n">subscribe</code><code class="p">([</code><code class="bp">self</code><code class="o">.</code><code class="n">topic</code><code class="p">],</code> <code class="n">on_assign</code> <code class="o">=</code> <code class="n">print_assignment</code><code class="p">)</code>
       <code class="k">while</code> <code class="bp">self</code><code class="o">.</code><code class="n">run</code><code class="p">:</code>
       	<code class="n">msg</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">consumer</code><code class="o">.</code><code class="n">poll</code><code class="p">(</code><code class="n">timeout</code><code class="o">=</code><code class="mf">1.0</code><code class="p">)</code>
       	<code class="k">if</code> <code class="n">msg</code> <code class="ow">is</code> <code class="kc">None</code><code class="p">:</code>
                 <code class="k">continue</code>
       	<code class="n">If</code> <code class="n">msg</code><code class="o">.</code><code class="n">error</code><code class="p">():</code>
                  <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Consumer error: </code><code class="si">{</code><code class="n">msg</code><code class="o">.</code><code class="n">error</code><code class="p">()</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>
                <code class="k">continue</code>
       	<code class="k">else</code><code class="p">:</code>
                <code class="c1"># Proper message</code>
                <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"New message: topic=</code><code class="si">{</code><code class="n">msg</code><code class="o">.</code><code class="n">topic</code><code class="p">()</code><code class="si">}</code><code class="s2"> "</code> <code class="o">+</code>
                <code class="sa">f</code><code class="s2">"partition= </code><code class="si">{</code><code class="n">msg</code><code class="o">.</code><code class="n">partition</code><code class="p">()</code><code class="si">}</code><code class="s2"> offset=</code><code class="si">{</code><code class="n">msg</code><code class="o">.</code><code class="n">offset</code><code class="p">()</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>
                <code class="n">key</code> <code class="o">=</code> <code class="kc">None</code>
                <code class="k">if</code> <code class="n">msg</code><code class="o">.</code><code class="n">key</code><code class="p">()</code> <code class="o">!=</code> <code class="kc">None</code><code class="p">:</code>
                    <code class="n">key</code> <code class="o">=</code> <code class="n">msg</code><code class="o">.</code><code class="n">key</code><code class="p">()</code><code class="o">.</code><code class="n">decode</code><code class="p">(</code><code class="s2">"UTF8"</code><code class="p">)</code>
                <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'key=</code><code class="si">{</code><code class="n">key</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>
                <code class="n">value</code> <code class="o">=</code> <code class="n">json</code><code class="o">.</code><code class="n">loads</code><code class="p">(</code><code class="n">msg</code><code class="o">.</code><code class="n">value</code><code class="p">()</code><code class="o">.</code><code class="n">decode</code><code class="p">(</code><code class="s2">"UTF8"</code><code class="p">))</code>
                <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'value = </code><code class="si">{</code><code class="n">value</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>
                <code class="bp">self</code><code class="o">.</code><code class="n">callback</code><code class="p">(</code><code class="n">key</code><code class="p">,</code> <code class="n">value</code><code class="p">)</code>

   <code class="k">def</code> <code class="nf">stop</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
       <code class="bp">self</code><code class="o">.</code><code class="n">run</code> <code class="o">=</code> <code class="kc">False</code>

   <code class="k">def</code> <code class="nf">destroy</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
       <code class="bp">self</code><code class="o">.</code><code class="n">consumer</code><code class="o">.</code><code class="n">close</code><code class="p">()</code></pre></div>

<p>A couple of notable differences exist between this and the original implementations:</p>

<ul>
<li>
<p>The constructor takes an additional parameter—the Kafka producer—which is used internally to create a <em>temperature controller manager</em> as part of the actor’s initialization.</p>
</li>
<li>
<p>For every incoming message, in addition to printing it out, we are invoking the temperature <em>controller manager</em> to process it.</p>
</li>
</ul>

<p>With these changes in place, you can implement the <a href="https://oreil.ly/e9Lm0">main program</a>, similar to (<a data-type="xref" href="#bringing-it-all-together">Example 6-4</a>), and start an execution. The partial execution result (in <a data-type="xref" href="#controller-execution-results">Example 6-10</a>) shows the output of processing.</p>
<div data-type="example" id="controller-execution-results">
<h5><span class="label">Example 6-10. </span>Controller execution results</h5>

<pre data-type="programlisting">(pid=29041) New message: topic= sensor  partition= 9  offset= 18
(pid=29041) key  1234  value  {'measurement': 45.0}
(pid=29041) New message: topic= sensor  partition= 9  offset= 19
(pid=29041) key  1234  value  {'measurement': 45.2}
(pid=29041) New message: topic= sensor  partition= 9  offset= 20
(pid=29041) key  1234  value  {'measurement': 45.3}
(pid=29041) New message: topic= sensor  partition= 9  offset= 21
(pid=29041) key  1234  value  {'measurement': 45.5}
(pid=29041) New message: topic= sensor  partition= 9  offset= 22
(pid=29041) key  1234  value  {'measurement': 45.7}
(pid=29041) New message: topic= sensor  partition= 9  offset= 23
(pid=29041) key  1234  value  {'measurement': 45.9}
(pid=29041) New message: topic= sensor  partition= 9  offset= 24
(pid=29041) key  1234  value  {'measurement': 46.0}
(pid=29041) New message: topic= sensor  partition= 9  offset= 25
(pid=29041) key  1234  value  {'measurement': 46.2}
(pid=29040) Message delivered to topic  heatercontrol  partition  9  offset 0
(pid=29041) New message: topic= sensor  partition= 9  offset= 26
(pid=29041) key  1234  value  {'measurement': 46.1}
(pid=29041) New message: topic= sensor  partition= 9  offset= 27
(pid=29041) key  1234  value  {'measurement': 46.0}
(pid=29041) New message: topic= sensor  partition= 9  offset= 28
(pid=29041) key  1234  value  {'measurement': 46.0}
(pid=29041) New message: topic= sensor  partition= 9  offset= 29
(pid=29041) key  1234  value  {'measurement': 45.9}
(pid=29041) New message: topic= sensor  partition= 9  offset= 30
(pid=29041) key  1234  value  {'measurement': 45.7}</pre></div>

<p>This listing shows the behavior of the controller when the temperature is around the desired value (45 degrees). As expected, the temperature keeps growing until it gets above 46 degrees (to avoid constant switching on and off of the controller, no actions are performed when the difference between desired and actual temperature is less than 1 degree). When the measurement is 46.2, the new message is sent to the heater to switch off and the temperature starts to decrease. Also looking at this listing, we can see that the requests are always delivered to the same partition (they have the same key).</p>

<p>A key-based approach is a good option for many real-world implementations. The advantage of this approach is that all of the data processing is done locally, inside the same Kafka consumer actor.</p>

<p>Two potential pitfalls exist with such implementations:</p>

<ul>
<li>
<p>As the number of keys grows, it is necessary to ensure that the keys are evenly distributed across Kafka topic partitions. Ensuring this key distribution can sometimes require additional key design procedures, but the default hashing is often sufficient.</p>
</li>
<li>
<p>Execution locality can become a problem when executions are CPU and memory expensive. Because all the executions are part of the Kafka consumer actor, its scaling can become insufficient for keeping up with high-volume traffic.</p>
</li>
</ul>

<p>Some of these drawbacks can be rectified in a key-independent <a data-primary="streaming applications" data-secondary="stateful stream processing" data-startref="streaming-stateful-keybased" data-tertiary="key-based approach" data-type="indexterm" id="idm45354773658544"/><a data-primary="stateful stream processing" data-secondary="key-based approach" data-startref="stateful-stream-keybased" data-type="indexterm" id="idm45354773656944"/><a data-primary="Apache Kafka" data-secondary="stateful stream processing" data-startref="apache-kafka-stateful-keybased" data-tertiary="key-based approach" data-type="indexterm" id="idm45354773655696"/><a data-primary="Kafka" data-secondary="stateful stream processing" data-startref="kafka-stateful-keybased" data-tertiary="key-based approach" data-type="indexterm" id="idm45354773654176"/>approach.</p>
</div></section>








<section class="pagebreak-before less_space" data-pdf-bookmark="Key-Independent Approach" data-type="sect2"><div class="sect2" id="idm45354774544400">
<h2>Key-Independent Approach</h2>

<p>The <a data-primary="streaming applications" data-secondary="stateful stream processing" data-tertiary="key-independent approach" data-type="indexterm" id="idm45354773651040"/><a data-primary="stateful stream processing" data-secondary="key-independent approach" data-type="indexterm" id="idm45354773649728"/><a data-primary="Apache Kafka" data-secondary="stateful stream processing" data-tertiary="key-independent approach" data-type="indexterm" id="idm45354773648752"/><a data-primary="Kafka" data-secondary="stateful stream processing" data-tertiary="key-independent approach" data-type="indexterm" id="idm45354773647504"/>difference in this approach compared to the previous one is that both the temperature controller (<a data-type="xref" href="#implementation-of-temperature-controller-manager">Example 6-8</a>) and temperature controller manager (<a data-type="xref" href="#integrating-kafka-consumer-with-temperature-controller-manager">Example 6-9</a>) are converted from Python objects to <a href="https://oreil.ly/b7hSK">Ray actors</a>. By doing this, both become individually addressable and can be located anywhere. Such an approach loses execution locality (which can lead to a slight execution time increase), but can improve overall scalability of the solution (each actor can run on a separate node). If necessary, you can improve scalability even further by leveraging an actor’s pool (described in <a data-type="xref" href="ch04.html#ch04">Chapter 4</a>) and thus allowing Ray to split execution to even more nodes.</p>
</div></section>
</div></section>






<section data-pdf-bookmark="Going Beyond Kafka" data-type="sect1"><div class="sect1" id="idm45354774572768">
<h1>Going Beyond Kafka</h1>

<p>In this <a data-primary="streaming applications" data-secondary="alternatives to Apache Kafka" data-type="indexterm" id="streaming-alternative"/>chapter, you learned how to use Ray’s native capabilities to implement streaming by directly integrating Ray with Kafka. But what if you need to use a different messaging infrastracture? If your favorite communication backbone provides Python APIs, you can integrate it with Ray, similar to the Kafka integration described 
<span class="keep-together">previously.</span></p>

<p>Another option, as mentioned at the beginning of this chapter, is to use an external library—for example, project <a href="https://oreil.ly/xv2xB">Rayvens</a>, which internally leverages <a href="https://oreil.ly/HT77R">Apache Camel</a> (a generic integration framework) to make it possible to use a wide range of messaging backbones. You can find a description of the supported messaging backbones and an example of their usage in <a href="https://oreil.ly/y4kYx">“Accessing Hundreds of Event Sources and Sinks with Rayvens”</a> by Gheorghe-Teodor Bercea and Olivier Tardieu.</p>

<p class="pagebreak-after">Similar to the Kafka integration we’ve described, under the hood, <a data-primary="Rayvens" data-type="indexterm" id="idm45354773607488"/><a data-primary="Apache Camel" data-type="indexterm" id="idm45354773606816"/><a data-primary="Camel" data-type="indexterm" id="idm45354773606144"/>Rayvens is implemented as a set of Ray actors. The Rayvens base class <code>Stream</code> is a stateless, serializable, wrapper around the <code>Stream</code> Ray actor class, which is responsible for keeping track of the current Rayvens state (see <a data-type="xref" href="ch04.html#ch04">Chapter 4</a> for using actors to manage global variables), including currently defined sources and sinks and their connectivity. The <code>Stream</code> class hides the remote nature of a <code>Stream</code> actor and implements wrappers that internally implement all communications with the underlying remote actor. If you want more control (in terms of execution timing), you can invoke methods directly on the <code>Stream</code> actor. The <code>Stream</code> actor will be reclaimed when the original stream handle goes out of scope.</p>

<p>As Rayvens is based on Camel, it requires a setting of Camel to make it work. Ravens supports two main options of Camel usage:</p>
<dl>
<dt>Local mode</dt>
<dd>
<p>The Camel source or sink runs in the same execution context as the <code>Stream</code> actor that is attached to using the Camel client: same container, same virtual or physical machine.</p>
</dd>
<dt>Operator mode</dt>
<dd>
<p>The Camel source or sink runs inside a Kubernetes cluster relying on the Camel operator to manage dedicated <a data-primary="streaming applications" data-secondary="alternatives to Apache Kafka" data-startref="streaming-alternative" data-type="indexterm" id="idm45354773597856"/>Camel pods.</p>
</dd>
</dl>
</div></section>






<section data-pdf-bookmark="Conclusion" data-type="sect1"><div class="sect1" id="idm45354773596080">
<h1>Conclusion</h1>

<p>In this chapter, you learned one option to use Ray for implementing streaming. You first learned the basics of Kafka—the most popular streaming application backbone used today—and ways to integrate it with Ray. You then learned how to scale Kafka-based applications with Ray. We have also outlined implementation approaches for both stateless and stateful streaming applications with Ray that you can use as a foundation for your custom implementations.</p>

<p>Finally, we briefly discussed alternatives to using Kafka as a transport. Rayvens, a general-purpose integration framework based on Apache Camel, can be used for integration of a wide variety of streaming backbones. You can use this discussion to decide how to implement your specific transports.</p>

<p>In the next chapter, we will introduce Ray’s microservices framework and how to use it for model serving.</p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="idm45354776016592"><sup><a href="ch06.html#idm45354776016592-marker">1</a></sup> Other examples of distributed log implementation are <a href="https://oreil.ly/4Km4h">Apache BookKeeper</a>, <a href="https://oreil.ly/ChJdY">Apache Pulsar</a>, and <a href="https://oreil.ly/getrt">Pravega</a>.</p><p data-type="footnote" id="idm45354775801840"><sup><a href="ch06.html#idm45354775801840-marker">2</a></sup> Although we tend to think about infinite logs, in reality a Kafka log is limited to the amount of disk space available to the corresponding Kafka server. Kafka introduces <a href="https://oreil.ly/0wudH">log retention and cleanup policies</a>, which prevent logs from growing indefinitely and consequently crashing Kafka servers. As a result, when we are talking about log replay in a production system, we are talking about replay within a retention window.</p><p data-type="footnote" id="idm45354775789952"><sup><a href="ch06.html#idm45354775789952-marker">3</a></sup> Refer to <a href="https://oreil.ly/rC2RY">“Capacity Planning Your Kafka Cluster”</a> by Jason Bell for more details. Kafka is also available as a serverless product from vendors such as Confluent Cloud.</p><p data-type="footnote" id="idm45354775738704"><sup><a href="ch06.html#idm45354775738704-marker">4</a></sup> For another example of the same approach, see <a href="https://oreil.ly/iRxWq">“Serverless Kafka Stream Processing with Ray”</a> by Javier Redondo.</p><p data-type="footnote" id="idm45354775188896"><sup><a href="ch06.html#idm45354775188896-marker">5</a></sup> Allowed values for <code>reset</code> are <code>earliest</code>, which automatically resets the offset to the beginning of the log, and <code>latest</code>, which automatically resets the offset to the latest offset processed by the consumer group.</p><p data-type="footnote" id="idm45354774560064"><sup><a href="ch06.html#idm45354774560064-marker">6</a></sup> This example is described further in <a href="https://oreil.ly/jekKs">“How to Serve Machine Learning Models with Dynamically Controlled Streams”</a>, a blog post by Boris.</p><p data-type="footnote" id="idm45354774537376"><sup><a href="ch06.html#idm45354774537376-marker">7</a></sup> As described in <a data-type="xref" href="ch04.html#ch04">Chapter 4</a>, Ray’s actors are not persistent. Therefore, in the case of node failures, the actor state will be lost. We can implement persistence here as described in <a data-type="xref" href="ch04.html#ch04">Chapter 4</a> to overcome this.</p><p data-type="footnote" id="idm45354774533936"><sup><a href="ch06.html#idm45354774533936-marker">8</a></sup> Note the use of threading to ensure that the Kafka consumer is running forever without interference with measurement computations. Again, this is a simplification we made for our toy example; in real implementations, every request to the temperature controller should contain a <code>replyTo</code> topic, thus ensuring that any replies will get to the correct instance of the heater.</p></div></div></section></body></html>