- en: Chapter 1\. An Overview of Ray
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A distributed system is one in which the failure of a computer you didn’t even
    know existed can render your own computer unusable.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Leslie Lamport
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: One of the reasons we need efficient distributed computing is that we’re collecting
    ever more data with a large variety at increasing speeds. The storage systems,
    data processing and analytics engines that have emerged in the last decade are
    crucially important to the success of many companies. Interestingly, most “big
    data” technologies are built for and operated by (data) engineers, that are in
    charge of data collection and processing tasks. The rationale is to free up data
    scientists to do what they’re best at. As a data science practitioner you might
    want to focus on training complex machine learning models, running efficient hyperparameter
    selection, building entirely new and custom models or simulations, or serving
    your models to showcase them. At the same time you simply might *have to* scale
    them to a compute cluster. To do that, the distributed system of your choice needs
    to support all of these fine-grained “big compute” tasks, potentially on specialized
    hardware. Ideally, it also fits into the big data tool chain you’re using and
    is fast enough to meet your latency requirements. In other words, distributed
    computing has to be powerful and flexible enough for complex data science workloads — and
    Ray can help you with that.
  prefs: []
  type: TYPE_NORMAL
- en: Python is likely the most popular language for data science today, and it’s
    certainly the one I find the most useful for my daily work. By now it’s over 30
    years old, but has a still growing and active community. The rich [PyData ecosystem](https://pydata.org/)
    is an essential part of a data scientist’s toolbox. How can you make sure to scale
    out your workloads while still leveraging the tools you need? That’s a difficult
    problem, especially since communities can’t be forced to just toss their toolbox,
    or programming language. That means distributed computing tools for data science
    have to be built for their existing community.
  prefs: []
  type: TYPE_NORMAL
- en: What Is Ray?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What I like about Ray is that it checks all the above boxes. It’s a flexible
    distributed computing framework build for the Python data science community. Ray
    is easy to get started and keeps simple things simple. Its core API is as lean
    as it gets and helps you reason effectively about the distributed programs you
    want to write. You can efficiently parallelize Python programs on your laptop,
    and run the code you tested locally on a cluster practically without any changes.
    Its high-level libraries are easy to configure and can seamlessly be used together.
    Some of them, like Ray’s reinforcement learning library, would have a bright future
    as standalone projects, distributed or not. While Ray’s core is built in C++,
    it’s been a Python-first framework since day one, integrates with many important
    data science tools, and can count on a growing ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed Python is not new, and Ray is not the first framework in this space
    (nor will it be the last), but it is special in what it has to offer. Ray is particularly
    strong when you combine several of its modules and have custom, machine learning
    heavy workloads that would be difficult to implement otherwise. It makes distributed
    computing easy enough to run your complex workloads flexibly by leveraging the
    Python tools you know and want to use. In other words, by *learning Ray* you get
    to know *flexible distributed Python for data science*.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter you’ll get a first glimpse at what Ray can do for you. We will
    discuss the three layers that make up Ray, namely its core engine, its high-level
    libraries and its ecosystem. Throughout the chapter we’ll show you first code
    examples to give you a feel for Ray, but we defer any in-depth treatment of Ray’s
    APIs and components to later chapters. You can view this chapter as an overview
    of the whole book as well.
  prefs: []
  type: TYPE_NORMAL
- en: What Led to Ray?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Programming distributed systems is hard. It requires specific knowledge and
    experience you might not have. Ideally, such systems get out of your way and provide
    abstractions to let you focus on your job. But in practice “all non-trivial abstractions,
    to some degree, are leaky” ([Spolsky](https://www.joelonsoftware.com/2002/11/11/the-law-of-leaky-abstractions/)),
    and getting clusters of computers to do what you want is undoubtedly difficult.
    Many software systems require resources that far exceed what single servers can
    do. Even if one server was enough, modern systems need to be failsafe and provide
    features like high availability. That means your applications might have to run
    on multiple machines, or even datacenters, just to make sure they’re running reliably.
  prefs: []
  type: TYPE_NORMAL
- en: Even if you’re not too familiar with machine learning (ML) or more generally
    artificial intelligence (AI) as such, you must have heard of recent breakthroughs
    in the field. To name just two, systems like [Deepmind’s AlphaFold](https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology)
    for solving the protein folding problem, or [OpenAI’s Codex](https://openai.com/blog/openai-codex/)
    that’s helping software developers with the tedious parts of their job, have made
    the news lately. You might also have heard that ML systems generally require large
    amounts of data to be trained. OpenAI has shown exponential growth in compute
    needed to train AI models in their paper [“AI and Compute”](https://openai.com/blog/ai-and-compute/).
    The operations needed for AI systems in their study is measured in petaflops (thousands
    of trillion operations per second), and has been *doubling every 3.4 months* since
    2012.
  prefs: []
  type: TYPE_NORMAL
- en: Compare this to Moore’s Law^([1](ch01.xhtml#idm44990045517696)), which states
    that the number of transistors in computers would double every two years. Even
    if you’re bullish on Moore’s law, you can see how there’s a clear need for distributed
    computing in ML. You should also understand that many tasks in ML can be naturally
    decomposed to run in parallel. So, why not speed things up if you can?
  prefs: []
  type: TYPE_NORMAL
- en: Distributed computing is generally perceived as hard. But why is that? Shouldn’t
    it be realistic to find good abstractions to run your code on clusters, without
    having to constantly think about individual machines and how they interoperate?
    What if we specifically focused on AI workloads?
  prefs: []
  type: TYPE_NORMAL
- en: Researchers at [RISELab](https://rise.cs.berkeley.edu/) at UC Berkeley created
    Ray to address these questions. None of the tools existing at the time met their
    needs. They were looking for easy ways to speed up their workloads by distributing
    them to compute clusters. The workloads they had in mind were quite flexible in
    nature and didn’t fit into the analytics engines available. At the same time,
    RISELab wanted to build a system that took care of how the work was distributed.
    With reasonable default behaviors in place, researchers should be able to focus
    on their work. And ideally they should have access to all their favorite tools
    in Python. For this reason, Ray was built with an emphasis on high-performance
    and heterogeneous workloads. [Anyscale](https://www.anyscale.com/), the company
    behind Ray, is building a managed Ray Platform and offers hosted solutions for
    your Ray applications. Let’s have a look at an example of what kinds of applications
    Ray was designed for.
  prefs: []
  type: TYPE_NORMAL
- en: Flexible Workloads in Python and Reinforcement Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of my favorite apps on my phone can automatically classify or “label” individual
    plants in our garden. It works by simply showing it a picture of the plant in
    question. That’s immensely helpful, as I’m terrible at distinguishing them all.
    (I’m not bragging about the size of my garden, I’m just bad at it.) In the last
    couple of years we’ve seen a surge of impressive applications like that.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, the promise of AI is to build intelligent agents that go far beyond
    classifying objects. Imagine an AI application that not only knows your plants,
    but can take care of to them, too. Such an application would have to
  prefs: []
  type: TYPE_NORMAL
- en: Operate in dynamic environments (like the change of seasons)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: React to changes in the environment (like a heavy storm or pests attacking your
    plants)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Take sequences of actions (like watering and fertilizing plants)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accomplish long-term goals (like prioritizing plant health)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By observing its environment such an AI would also learn to explore the possible
    actions it could take and come up with better solutions over time. If you feel
    like this example is artificial or too far out, it’s not difficult to come up
    with examples on your own that share all the above requirements. Think of managing
    and optimizing a supply chain, strategically restocking a warehouse considering
    fluctuating demands, or orchestrating the processing steps in an assembly line.
    Another famous example of what you could expect from an AI would be Stephen Wozniak’s
    famous “Coffee Test”. If you’re invited to a friend’s house, you can navigate
    to the kitchen, spot the coffee machine and all necessary ingredients, figure
    out how to brew a cup of coffee, and sit down to enjoy it. A machine should be
    able to do the same, except the last part might be a bit of a stretch. What other
    examples can you think of?
  prefs: []
  type: TYPE_NORMAL
- en: You can frame all the above requirements naturally in a subfield of machine
    learning called reinforcement learning (RL). We’ve dedicated all of [Chapter 4](ch04.xhtml#chapter_04)
    to RL. For now, it’s enough to understand that it’s about agents interacting with
    their environment by observing it and emitting actions. In RL, agents evaluate
    their environments by attributing a reward (e.g., how healthy is my plant on a
    scale from 1 to 10). The term “reinforcement” comes from the fact that agents
    will hopefully learn to seek out behaviour that leads to good outcomes (high reward),
    and shy away from punishing situations (low or negative reward). The interaction
    of agents with their environment is usually modeled by creating a computer simulation
    of it. These simulations can become complicated quite quickly, as you might imagine
    from the examples we’ve given.
  prefs: []
  type: TYPE_NORMAL
- en: We don’t have gardening robots like the one I’ve sketched yet. And we don’t
    know which AI paradigm will get us there.^([2](ch01.xhtml#idm44990045384464))
    What I do know is that the world is full of complex, dynamic and interesting examples
    that we need to tackle. For that we need computational frameworks that help us
    do that, and Ray was built to do exactly that. RISELab created Ray to build and
    run complex AI applications at scale, and reinforcement learning has been an integral
    part of Ray from the start.
  prefs: []
  type: TYPE_NORMAL
- en: 'Three Layers: Core, Libraries and Ecosystem'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you know why Ray was built and what its creators had in mind, let’s
    look at the three layers of Ray.
  prefs: []
  type: TYPE_NORMAL
- en: A low-level, distributed computing framework for Python with a concise core
    API.^([3](ch01.xhtml#idm44990045356496))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A set of high-level libraries for data science built and maintained by the creators
    of Ray.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A growing ecosystem of integrations and partnerships with other notable projects.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There’s a lot to unpack here, and we’ll look into each of these layers individually
    in the remainder of this chapter. You can imagine Ray’s core engine with its API
    at the center of things, on which everything else builds. Ray’s data science libraries
    build on top of it. In practice, most data scientists will use these higher level
    libraries directly and won’t often need to resort to the core API. The growing
    number of third-party integrations for Ray is another great entrypoint for experienced
    practitioners. Let’s look into each one of the layers one by one.
  prefs: []
  type: TYPE_NORMAL
- en: A Distributed Computing Framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At its core, Ray is a distributed computing framework. We’ll provide you with
    just the basic terminology here, and talk about Ray’s architecture in depth in
    [Chapter 2](ch02.xhtml#chapter_02). In short, Ray sets up and manages clusters
    of computers so that you can run distributed tasks on them. A ray cluster consists
    of nodes that are connected to each other via a network. You program against the
    so-called *driver*, the program root, which lives on the *head node*. The driver
    can run *jobs*, that is a collection of tasks, that are run on the nodes in the
    cluster. Specifically, the individual tasks of a job are run on *worker* processes
    on *worker nodes*. Figure [Figure 1-1](#fig_simple_cluster) illustrates the basic
    structure of a Ray cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '![Ray cluster schematics](assets/simple_cluster.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-1\. The basic components of a Ray cluster
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: What’s interesting is that a Ray cluster can also be a *local cluster*, i.e.
    a cluster consisting just of your own computer. In this case, there’s just one
    node, namely the head node, which has the driver process and some worker processes.
    The default number of worker processes is the number of CPUs available on your
    machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'With that knowledge at hand, it’s time to get your hands dirty and run your
    first local Ray cluster. Installing Ray^([4](ch01.xhtml#idm44990042817344)) on
    any of the major operating systems should work seamlessly using `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: With a simple `pip install ray` you would have installed just the very basics
    of Ray. Since we want to explore some advanced features, we installed the “extras”
    `rllib`, `serve` and `tune`, which we’ll discuss in a bit. Depending on your system
    configuration you may not need the quotation marks in the above installation command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, go ahead and start a Python session. You could use the `ipython` interpreter,
    which I find to be the most suitable environment for following along simple examples.
    If you don’t feel like typing in the commands yourself, you can also jump into
    the [jupyter notebook for this chapter](https://github.com/maxpumperla/learning_ray/blob/main/notebooks/ch_01_overview.ipynb)
    and run the code there. The choice is up to you, but in any case please remember
    to use Python version `3.7` or later. In your Python session you can now easily
    import and initialize Ray as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Example 1-1\.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: With those two lines of code you’ve started a Ray cluster on your local machine.
    This cluster can utilize all the cores available on your computer as workers.
    In this case you didn’t provide any arguments to the `init` function. If you wanted
    to run Ray on a “real” cluster, you’d have to pass more arguments to `init`. The
    rest of your code would stay the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'After running this code you should see output of the following form (we use
    ellipses to remove the clutter):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This indicates that your Ray cluster is up and running. As you can see from
    the first line of the output, Ray comes with its own, pre-packaged dashboard.
    In all likelihood you can check it out at [*http://127.0.0.1:8265*](http://127.0.0.1:8265),
    unless your output shows a different port. If you want you can take your time
    to explore the dashboard for a little. For instance, you should see all your CPU
    cores listed and the total utilization of your (trivial) Ray application. We’ll
    come back to the dashboard in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: We’re not quite ready to dive into all the details of a Ray cluster here. To
    jump ahead just a little, you might see the `raylet_ip_address`, which is a reference
    to a so-called *Raylet*, which is responsible for scheduling tasks on your worker
    nodes. Each Raylet has a store for distributed objects, which is hinted at by
    the `object_store_address` above. Once tasks are scheduled, they get executed
    by worker processes. In [Chapter 2](ch02.xhtml#chapter_02) you’ll get a much better
    understanding of all these components and how they make up a Ray cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Before moving on, we should also briefly mention that the Ray core API is very
    accessible and easy to use. But since it is also a rather low-level interface,
    it takes time to build interesting examples with it. [Chapter 2](ch02.xhtml#chapter_02)
    has an extensive first example to get you started with the Ray core API, and in
    [Chapter 3](ch03.xhtml#chapter_03) you’ll see how to build a more interesting
    Ray application for reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: Right now your Ray cluster doesn’t do much, but that’s about to change. After
    giving you a quick introduction to the data science workflow in the following
    section, you’ll run your first concrete Ray examples.
  prefs: []
  type: TYPE_NORMAL
- en: A Suite of Data Science Libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Moving on to the second layer of Ray, in this section we’ll introduce all the
    data science libraries that Ray comes with. To do so, let’s first take a bird’s
    eye view on what it means to do data science. Once you understand this context,
    it’s much easier to place Ray’s higher-level libraries and see how they can be
    useful to you. If you have a good idea of the data science process, you can safely
    skip ahead to section [“Data Processing with Ray Data”](#section_data_processing).
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning and the Data Science Workflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The somewhat elusive term “data science” (DS) evolved quite a bit in recent
    years, and you can find many definitions of varying usefulness online.^([5](ch01.xhtml#idm44990042948624))
    To me, it’s *the practice of gaining insights and building real-world applications
    by leveraging data*. That’s quite a broad definition, and you don’t have to agree
    with me. My point is that data science is an inherently practical and applied
    field that centers around building and understanding things, which makes fairly
    little sense in a *purely* academic context. In that sense, describing practitioners
    of this field as “data scientists” is about as bad of a misnomer as describing
    hackers as “computer scientists”.^([6](ch01.xhtml#idm44990044482384))
  prefs: []
  type: TYPE_NORMAL
- en: 'Since you are familiar with Python and hopefully bring a certain craftsmanship
    attitude with you, we can approach the Ray’s data science libraries from a very
    pragmatic angle. Doing data science in practice is an iterative process that goes
    something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Requirements engineering
  prefs: []
  type: TYPE_NORMAL
- en: You talk to stakeholders to identify the problems you need to solve and clarify
    the requirements for this project.
  prefs: []
  type: TYPE_NORMAL
- en: Data collection
  prefs: []
  type: TYPE_NORMAL
- en: Then you source, collect and inspect the data.
  prefs: []
  type: TYPE_NORMAL
- en: Data processing
  prefs: []
  type: TYPE_NORMAL
- en: Afterwards you process the data such that you can tackle the problem.
  prefs: []
  type: TYPE_NORMAL
- en: Model building
  prefs: []
  type: TYPE_NORMAL
- en: You then move on to build a model (in the broadest sense) using the data. That
    could be a dashboard with important metrics, a visualisation, or a machine learning
    model, among many other things.
  prefs: []
  type: TYPE_NORMAL
- en: Model evaluation
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to evaluate your model against the requirements in the first
    step.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment
  prefs: []
  type: TYPE_NORMAL
- en: If all goes well (it likely doesn’t), you deploy your solution in a production
    environment. You should understand this as an ongoing process that needs to be
    monitored, not as a one-off step.
  prefs: []
  type: TYPE_NORMAL
- en: Otherwise, you need to circle back and start from the top. The most likely outcome
    is that you need to improve your solution in various ways, even after initial
    deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Machine learning is not necessarily part of this process, but you can see how
    building smart applications or gaining insights might benefit from ML. Building
    a face detection app into your social media platform, for better or worse, might
    be one example of that. When the data science process just described explicitly
    involves building machine learning models, you can further specify some steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Data processing
  prefs: []
  type: TYPE_NORMAL
- en: To train machine learning models, you need data in a format that is understood
    by your ML model. The process of transforming and selecting what data should be
    fed into your model is often called *feature engineering*. This step can be messy.
    You’ll benefit a lot if you can rely on common tools to do the job.
  prefs: []
  type: TYPE_NORMAL
- en: Model training
  prefs: []
  type: TYPE_NORMAL
- en: In ML you need to train your algorithms on data that got processed in the last
    step. This includes selecting the right algorithm for the job, and it helps if
    you can choose from a wide variety.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tuning
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning models have parameters that are tuned in the model training
    step. Most ML models also have another set of parameters, called *hyperparameters*
    that can be modified prior to training. These parameters can heavily influence
    the performance of your resulting ML model and need to be tuned properly. There
    are good tools to help automate that process.
  prefs: []
  type: TYPE_NORMAL
- en: Model serving
  prefs: []
  type: TYPE_NORMAL
- en: Trained models need to be deployed. To serve a model means to make it available
    to whoever needs access by whatever means necessary. In prototypes, you often
    use simple HTTP servers, but there are many specialised software packages for
    ML model serving.
  prefs: []
  type: TYPE_NORMAL
- en: This list is by no means exhaustive. Don’t worry if you’ve never gone through
    these steps or struggle with the terminology, we’ll come back to this in much
    more detail in later chapters. If you want to understand more about the holistic
    view of the data science process when building machine learning applications,
    the book [Building Machine Learning Powered Applications](https://www.oreilly.com/library/view/building-machine-learning/9781492045106/)
    is dedicated to it entirely.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [Figure 1-2](#fig_ds_experimentation) gives an overview of the steps
    we just discussed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Data science experimentation workflow](assets/ds_workflow.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-2\. An overview of the data science experimentation workflow using
    machine learning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: At this point you might be wondering how any of this relates to Ray. The good
    news is that Ray has a dedicated library for each of the four ML-specific tasks
    above, covering data processing, model training, hyperparameter tuning and model
    serving. And the way Ray is designed, all these libraries are *distributed by
    construction*. Let’s walk through each of them one-by-one.
  prefs: []
  type: TYPE_NORMAL
- en: Data Processing with Ray Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first high-level library of Ray we talk about is called “Ray Data”. This
    library contains a data structure aptly called `Dataset`, a multitude of connectors
    for loading data from various formats and systems, an API for transforming such
    datasets, a way to build data processing pipelines with them, and many integrations
    with other data processing frameworks. The `Dataset` abstraction builds on the
    powerful [Arrow framework](https://arrow.apache.org/).
  prefs: []
  type: TYPE_NORMAL
- en: 'To use Ray Data, you need to install Arrow for Python, for instance by running
    `pip install pyarrow`. We’ll now discuss a simple example that creates a distributed
    `Dataset` on your local Ray cluster from a Python data structure. Specifically,
    you’ll create a dataset from a Python dictionary containing a string `name` and
    an integer-valued `data` for `10000` entries:'
  prefs: []
  type: TYPE_NORMAL
- en: Example 1-2\.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_an_overview_of_ray_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a `Dataset` by using `from_items` from the `ray.data` module.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_an_overview_of_ray_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Printing the first 10 items of the `Dataset`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To `show` a `Dataset` means to print some of its values. You should see precisely
    `5` so-called `ArrowRow` elements on your command line, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Great, now you have some distributed rows, but what can you do with that data?
    The `Dataset` API bets heavily on functional programming, as it is very well suited
    for data transformations. Even though Python 3 made a point of hiding some of
    its functional programming capabilities, you’re probably familiar with functionality
    such as `map`, `filter` and others. If not, it’s easy enough to pick up. `map`
    takes each element of your dataset and transforms is into something else, in parallel.
    `filter` removes data points according to a boolean filter function. And the slightly
    more elaborate `flat_map` first maps values similarly to `map`, but then also
    “flattens” the result. For instance, if `map` would produce a list of lists, `flat_map`
    would flatten out the nested lists and give you just a list. Equipped with these
    three functional API calls, let’s see how easily you can transform your dataset
    `ds`:'
  prefs: []
  type: TYPE_NORMAL
- en: Example 1-3\. Transforming a `Dataset` with common functional programming routines
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_an_overview_of_ray_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: We `map` each row of `ds` to only keep the square value of its `data` entry.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_an_overview_of_ray_CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Then we `filter` the `squares` to only keep even numbers (a total of 5000 elements).
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_an_overview_of_ray_CO2-3)'
  prefs: []
  type: TYPE_NORMAL
- en: We then use `flat_map` to augment the remaining values with their respective
    cubes.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_an_overview_of_ray_CO2-4)'
  prefs: []
  type: TYPE_NORMAL
- en: To `take` a total of `10` values means to leave Ray and return a Python list
    with these values that we can print.
  prefs: []
  type: TYPE_NORMAL
- en: The drawback of `Dataset` transformations is that each step gets executed synchronously.
    In example [Example 1-3](#ray_data_transform) this is a non-issue, but for complex
    tasks that e.g. mix reading files and processing data, you want an execution that
    can overlap individual tasks. `DatasetPipeline` does exactly that. Let’s rewrite
    the last example into a pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Example 1-4\.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_an_overview_of_ray_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: You can turn a `Dataset` into a pipeline by calling `.window()` on it.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_an_overview_of_ray_CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline steps can be chained to yield the same result as before.
  prefs: []
  type: TYPE_NORMAL
- en: There’s a lot more to be said about Ray Data, especially its integration with
    notable data processing systems, but we’ll have to defer an in-depth discussion
    until [Chapter 7](ch07.xhtml#chapter_07).
  prefs: []
  type: TYPE_NORMAL
- en: Model Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Moving on to the next set of libraries, let’s look at the distributed training
    capabilities of Ray. For that, you have access to two libraries. One is dedicated
    to reinforcement learning specifically, the other one has a different scope and
    is aimed primarily at supervised learning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning with Ray RLlib
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s start with *Ray RLlib* for reinforcement learning. This library is powered
    by the modern ML frameworks TensorFlow and PyTorch, and you can choose which one
    to use. Both frameworks seem to converge more and more conceptually, so you can
    pick the one you like most without losing much in the process. Throughout the
    book we use TensorFlow for consistency. Go ahead and install it with `pip install
    tensorflow` right now.
  prefs: []
  type: TYPE_NORMAL
- en: One of the easiest ways to run examples with RLlib is to use the command line
    tool `rllib`, which we’ve already implicitly installed earlier with `pip`. Once
    you run more complex examples in [Chapter 4](ch04.xhtml#chapter_04), you will
    mostly rely on its Python API, but for now we just want to get a first taste of
    running RL experiments.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll look at a fairly classical control problem of balancing a pendulum. Imagine
    you have a pendulum like the one in figure [Figure 1-3](#fig_pendulum), fixed
    at as single point and subject to gravity. You can manipulate that pendulum by
    giving it a push from the left or the right. If you assert just the right amount
    of force, the pendulum might remain in an upright position. That’s our goal -
    and the question is whether we can teach a reinforcement learning algorithm to
    do so for us.
  prefs: []
  type: TYPE_NORMAL
- en: '![Pendulum](assets/pendulum.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-3\. Controlling a simple pendulum by asserting force to the left or
    the right
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Specifically, we want to train a reinforcement learning agent that can push
    to the left or right, thereby acting on its environment (manipulating the pendulum)
    to reach the “upright position” goal for which it will be rewarded. To tackle
    this problem with Ray RLlib, store the following content in a file called `pendulum.yml`.
  prefs: []
  type: TYPE_NORMAL
- en: Example 1-5\.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_an_overview_of_ray_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The `Pendulum-v1` environment simulates the pendulum problem we just described.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_an_overview_of_ray_CO4-2)'
  prefs: []
  type: TYPE_NORMAL
- en: We use a powerful RL algorithm called Proximal Policy Optimization, or PPO.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_an_overview_of_ray_CO4-3)'
  prefs: []
  type: TYPE_NORMAL
- en: After every five “training iterations” we checkpoint a model.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_an_overview_of_ray_CO4-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Once we reach a reward of `-800` , we stop the experiment.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_an_overview_of_ray_CO4-5)'
  prefs: []
  type: TYPE_NORMAL
- en: The PPO needs some RL-specific configuration to make it work for this problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The details of this configuration file don’t matter much at this point, don’t
    get distracted by them. The important part is that you specify the built-in `Pendulum-v1`
    environment and sufficient RL-specific configuration to ensure the training procedure
    works. The configuration is a simplified version of one of Ray’s [tuned examples](https://github.com/ray-project/ray/tree/master/rllib/tuned_examples).
    We chose this one because it doesn’t require any special hardware and finishes
    in a matter of minutes. If your computer is powerful enough, you can try to run
    the tuned example as well, which should yield much better results. To train this
    pendulum example you can now simply run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want, you can check the output of this Ray program and see how the different
    metrics evolve during the training procedure. In case you don’t want to create
    this file on your own, and want to run an experiment which gives you much better
    results, you can also run this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In any case, assuming the training program finished, we can now check how well
    it worked. To visualize the trained pendulum you need to install one more Python
    library with `pip install pyglet`. The only other thing you need to figure out
    is where Ray stored your training progress. When you run `rllib train` for an
    experiment, Ray will create a unique experiment ID for you and stores results
    in a sub-folder of `~/ray-results` by default. For the training configuration
    we used, you should see a folder with results that looks like `~/ray_results/pendulum-ppo/PPO_Pendulum-v1_<experiment_id>`.
    During the training procedure intermediate model checkpoints get generated in
    the same folder. For instance, I have a folder on my machine called:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you figured out the experiment ID and chose a checkpoint ID (as a rule
    of thumb the larger the ID, the better the results), you can evaluate the training
    performance of your pendulum training run like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: You should see an animation of a pendulum controlled by an agent that looks
    like figure [Figure 1-3](#fig_pendulum). Since we opted for a quick training procedure
    instead of maximizing performance, you should see the agent struggle with the
    pendulum exercise. We could have done much better, and if you’re interested to
    scan Ray’s tuned examples for the `Pendulum-v1` environment, you’ll find an abundance
    of solutions to this exercise. The point of this example was to show you how simple
    it can be to train and evaluate reinforcement learning tasks with RLlib, using
    just two command line calls to `rllib`.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed training with Ray Train
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Ray RLlib is dedicated to reinforcement learning, but what do you do if you
    need to train models for other types of machine learning, like supervised learning?
    You can use another Ray library for distributed training in this case, called
    *Ray Train*. At this point, we don’t have built up enough knowledge of frameworks
    such as `TensorFlow` to give you a concrete and informative example for Ray Train.
    We’ll discuss all of that in [Chapter 6](ch06.xhtml#chapter_06), when it’s time
    to. But we can at least roughly sketch what a distributed training “wrapper” for
    an ML model would look like, which is simple enough conceptually:'
  prefs: []
  type: TYPE_NORMAL
- en: Example 1-6\.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_an_overview_of_ray_CO5-1)'
  prefs: []
  type: TYPE_NORMAL
- en: First, define your ML model training function. We simply pass here.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_an_overview_of_ray_CO5-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Then initialize a `Trainer` instance with TensorFlow as the backend.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_an_overview_of_ray_CO5-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, scale out your training function on a Ray cluster.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re interested in distributed training, you could jump ahead to [Chapter 6](ch06.xhtml#chapter_06).
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter Tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Naming things is hard, but the Ray team hit the spot with *Ray Tune*, which
    you can use to tune all sorts of parameters. Specifically, it was built to find
    good hyperparameters for machine learning models. The typical setup is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: You want to run an extremely computationally expensive training function. In
    ML it’s not uncommon to run training procedures that take days, if not weeks,
    but let’s say you’re dealing with just a couple of minutes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As result of training, you compute a so-called objective function. Usually you
    either want to maximize your gains or minimize your losses in terms of performance
    of your experiment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The tricky bit is that your training function might depend on certain parameters,
    hyperparameters, that influence the value of your objective function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You may have a hunch what individual hyperparameters should be, but tuning them
    all can be difficult. Even if you can restrict these parameters to a sensible
    range, it’s usually prohibitive to test a wide range of combinations. Your training
    function is simply too expensive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What can you do to efficiently sample hyperparameters and get “good enough”
    results on your objective? The field concerned with solving this problem is called
    *hyperparameter optimization* (HPO), and Ray Tune has an enormous suite of algorithms
    for tackling it. Let’s look at a first example of Ray Tune used for the situation
    we just explained. The focus is yet again on Ray and its API, and not on a specific
    ML task (which we simply simulate for now).
  prefs: []
  type: TYPE_NORMAL
- en: Example 1-7\. Minimizing an objective for an expensive training function with
    Ray Tune
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_an_overview_of_ray_CO6-1)'
  prefs: []
  type: TYPE_NORMAL
- en: We simulate an expensive training function that depends on two hyperparameters
    `x` and `y`, read from a `config`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_an_overview_of_ray_CO6-2)'
  prefs: []
  type: TYPE_NORMAL
- en: After sleeping for 5 seconds to simulate training and computing the objective
    we report back the score to `tune`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_an_overview_of_ray_CO6-3)'
  prefs: []
  type: TYPE_NORMAL
- en: The objective computes the mean of the squares of `x` and `y` and returns the
    square root of this term. This type of objective is fairly common in ML.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_an_overview_of_ray_CO6-4)'
  prefs: []
  type: TYPE_NORMAL
- en: We then use `tune.run` to initialize hyperparameter optimization on our `training_function`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_an_overview_of_ray_CO6-5)'
  prefs: []
  type: TYPE_NORMAL
- en: A key part is to provide a parameter space for `x` and `y` for `tune` to search
    over.
  prefs: []
  type: TYPE_NORMAL
- en: The Tune example in [Example 1-7](#ray_tune) finds the best possible choices
    of parameters `x` and `y` for a `training_function` with a given `objective` we
    want to minimize. Even though the objective function might look a little intimidating
    at first, since we compute the sum of squares of `x` and `y`, all values will
    be non-negative. That means the smallest value is obtained at `x=0` and `y=0`
    which evaluates the objective function to `0`.
  prefs: []
  type: TYPE_NORMAL
- en: We do a so-called *grid search* over all possible parameter combinations. As
    we explicitly pass in five possible values for both `x` and `y` that’s a total
    of `25` combinations that get fed into the training function. Since we instruct
    `training_function` to sleep for `10` seconds, testing all combinations of hyperparameters
    sequentially would take more than four minutes total. Since Ray is smart about
    parallelizing this workload, on my laptop this whole experiment only takes about
    `35` seconds. Now, imagine each training run would have taken several hours, and
    we’d have 20 instead of two hyperparameters. That makes grid search infeasible,
    especially if you don’t have educated guesses on the parameter range. In such
    situations you’ll have to use more elaborate HPO methods from Ray Tune, as discussed
    in [Chapter 5](ch05.xhtml#chapter_05).
  prefs: []
  type: TYPE_NORMAL
- en: Model Serving
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The last of Ray’s high-level libraries we’ll discuss specializes on model serving
    and is simply called *Ray Serve*. To see an example of it in action, you need
    a trained ML model to serve. Luckily, nowadays you can find many interesting models
    on the internet that have already been trained for you. For instance, *Hugging
    Face* has a variety of models available for you to download directly in Python.
    The model we’ll use is a language model called *GPT-2* that takes text as input
    and produces text to continue or complete the input. For example, you can prompt
    a question and GPT-2 will try to complete it.
  prefs: []
  type: TYPE_NORMAL
- en: Serving such a model is a good way to make it accessible. You may not now how
    to load and run a TensorFlow model on your computer, but you do now how to ask
    a question in plain English. Model serving hides the implementation details of
    a solution and lets users focus on providing inputs and understanding outputs
    of a model.
  prefs: []
  type: TYPE_NORMAL
- en: 'To proceed, make sure to run `pip install transformers` to install the Hugging
    Face library that has the model we want to use. With that we can now import and
    start an instance of Ray’s `serve` library, load and deploy a GPT-2 model and
    ask it for the meaning of life, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: Example 1-8\.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_an_overview_of_ray_CO7-1)'
  prefs: []
  type: TYPE_NORMAL
- en: We start `serve` locally.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_an_overview_of_ray_CO7-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The `@serve.deployment` decorator turns a function with a `request` parameter
    into a `serve` deployment.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_an_overview_of_ray_CO7-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Loading `language_model` inside the `model` function for every request is inefficient,
    but it’s the quickest way to show you a deployment.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_an_overview_of_ray_CO7-4)'
  prefs: []
  type: TYPE_NORMAL
- en: We ask the model to give us at most `100` characters to continue our query.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_an_overview_of_ray_CO7-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Then we formally deploy the model so that it can start receiving requests over
    HTTP.
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](assets/6.png)](#co_an_overview_of_ray_CO7-6)'
  prefs: []
  type: TYPE_NORMAL
- en: We use the indispensable `requests` library to get a response for any question
    you might have.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [Link to Come] you will learn how to properly deploy models in various scenarios,
    but for now I encourage you to play around with this example and test different
    queries. Running the last two lines of code repeatedly will give you different
    answers practically every time. Here’s a darkly poetic gem, raising more questions,
    that I queried on my machine and slightly censored for underaged readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This concludes our whirlwind tour of Ray’s data science libraries, the second
    of Ray’s layers. Before we wrap up this chapter, let’s have a very brief look
    at the third layer, the growing ecosystem around Ray.
  prefs: []
  type: TYPE_NORMAL
- en: A Growing Ecosystem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ray’s high-level libraries are powerful and deserve a much deeper treatment
    throughout the book. While their usefulness for the data science experimentation
    lifecycle is undeniable, I also don’t want to give off the impression that Ray
    is all you need from now on. In fact, I believe the best and most successful frameworks
    are the ones that integrate well with existing solutions and ideas. It’s better
    to focus on your core strengths and leverage other tools for what’s missing in
    your solution. There’s usually no reason to re-invent the wheel.
  prefs: []
  type: TYPE_NORMAL
- en: How Ray Integrates and Extends
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To give you an example for how Ray integrates with other tools, consider that
    Ray Data is a relatively new addition to its libraries. If you want to boil it
    down, and maybe oversimplify a little, Ray is a compute-first framework. In contrast,
    distributed frameworks like Apache Spark^([7](ch01.xhtml#idm44990033933984)) or
    Dask can be considered data-first. Pretty much anything you do with Spark starts
    with the definition of a distributed dataset and transformations thereof. Dask
    bets on bringing common data structures like Pandas dataframes or Numpy arrays
    to a distributed setup. Both are immensely powerful in their own regard, and we’ll
    give you a more detailed and fair comparison to Ray in [Link to Come]. The gist
    of it is that Ray Data does not attempt to replace these tools. Instead, it integrates
    well with both. As you’ll come to see, that’s a common theme with Ray.
  prefs: []
  type: TYPE_NORMAL
- en: Ray as Distributed Interface
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One aspect of Ray that’s vastly understated in my eyes is that its libraries
    seamlessly integrate common tools as *backends*. Ray often creates common interfaces,
    instead of trying to create new standards^([8](ch01.xhtml#idm44990033930720)).
    These interfaces allow you to run tasks in a distributed fashion, a property most
    of the respective backends don’t have, or not to the same extent. For instance,
    Ray RLlib and Train are backed by the full power of TensorFlow and PyTorch. Ray
    Tune supports algorithms from practically every notable HPO tool available, including
    Hyperopt, Optuna, Nevergrad, Ax, SigOpt and many others. None of these tools are
    distributed by default, but Tune unifies them in a common interface. Ray Serve
    can be used with frameworks such as FastAPI, and Ray Data is backed by Arrow and
    comes with many integrations to other frameworks, such as Spark and Dask. Overall
    this seems to be a robust design pattern that can be used to extend current Ray
    projects or integrate new backends in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To sum up what we’ve discussed in this chapter, [Figure 1-4](#fig_ray_layers)
    gives you an overview of the three layers of Ray as we laid them out. Ray’s core
    distributed execution engine sits at the center of the framework. For practical
    data science workflows you can use Ray Data for data processing, Ray RLlib for
    reinforcement learning, Ray Train for distributed model training, Ray Tune for
    hyperparameter tuning and Ray Serve for model serving. You’ve seen examples for
    each of these libraries and have an idea of what their APIs entail. On top of
    that, Ray’s ecosystem has many extensions that we’ll look more into later on.
    Maybe you can already spot a few tools you know and like in [Figure 1-4](#fig_ray_layers)^([9](ch01.xhtml#idm44990033926608))?
  prefs: []
  type: TYPE_NORMAL
- en: '![Ray layers](assets/ray_layers.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1-4\. Ray in three layers: its core API, the libraries RLlib, Tune,
    Ray Train, Ray Serve, Ray Data, and some of the many third-party integrations'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: ^([1](ch01.xhtml#idm44990045517696-marker)) Moore’s Law held for a long time,
    but there might be signs that it’s slowing down. We’re not here to argue it, though.
    What’s important is not that our computers generally keep getting faster, but
    the relation to the amount of compute we need.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch01.xhtml#idm44990045384464-marker)) For the experts among you, I don’t
    claim that RL is the answer. RL is just a paradigm that naturally fits into this
    discussion of AI goals.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch01.xhtml#idm44990045356496-marker)) This is a Python book, so we’ll
    exclusively focus on it. But you should at least know that Ray also has a Java
    API, which at this point is less mature than its Python equivalent.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch01.xhtml#idm44990042817344-marker)) We’re using Ray version `1.9.0`
    at this point, as it’s the latest version available as of this writing.
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch01.xhtml#idm44990042948624-marker)) I never liked the categorization
    of data science as an intersection of disciplines, like maths, coding and business.
    Ultimately, that doesn’t tell you what practitioners *do*. It doesn’t do a cook
    justice to tell them they sit at the intersection of agriculture, thermodynamics
    and human relations. It’s not wrong, but also not very helpful.
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch01.xhtml#idm44990044482384-marker)) As a fun exercise, I recommend reading
    Paul Graham’s famous [“Hackers and Painters”](http://www.paulgraham.com/hp.xhtml)
    essay on this topic and replace “computer science” with “data science”. What would
    hacking 2.0 be?
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch01.xhtml#idm44990033933984-marker)) Spark has been created by another
    lab in Berkeley, AMPLab. The internet is full of blog posts claiming that Ray
    should therefore be seen as a replacement of Spark. It’s better to think of them
    as tools with different strengths that are both likely here to stay.
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch01.xhtml#idm44990033930720-marker)) Before the deep learning framework
    [Keras](https://keras.io) became an official part of a corporate flagship, it
    started out as a convenient API specification for various lower-level frameworks
    such as Theano, CNTK, or TensorFlow. In that sense Ray RLlib has the chance to
    become Keras for RL. Ray Tune might just be Keras for HPO. The missing piece for
    more adoption is probably a more elegant API for both.
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch01.xhtml#idm44990033926608-marker)) Note that “Ray Train” has been called
    “raysgd” in older versions of Ray, and does not have a new logo yet.
  prefs: []
  type: TYPE_NORMAL
