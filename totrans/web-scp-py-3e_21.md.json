["```py\nimport threading\nimport time\n\ndef print_time(threadName, delay, iterations):\n    start = int(time.time())\n    for i in range(0,iterations):\n        time.sleep(delay)\n        print(f'{int(time.time() - start)} - {threadName}')\n\nthreads = [\n    threading.Thread(target=print_time, args=('Fizz', 3, 33)),\n    threading.Thread(target=print_time, args=('Buzz', 5, 20)),\n    threading.Thread(target=print_time, args=('Counter', 1, 100))\n]\n\n[t.start() for t in threads]\n[t.join() for t in threads]\n\n```", "```py\n1 Counter\n2 Counter\n3 Fizz\n3 Counter\n4 Counter\n5 Buzz\n5 Counter\n6 Fizz\n6 Counter\n\n```", "```py\nfrom urllib.request import urlopen\nfrom bs4 import BeautifulSoup\nimport re\nimport random\nimport threading\nimport time\n\n# Recursively find links on a Wikipedia page, \n# then follow a random link, with artificial 5 sec delay\ndef scrape_article(thread_name, path):\n    time.sleep(5)\n    print(f'{thread_name}: Scraping {path}')\n    html = urlopen('http://en.wikipedia.org{}'.format(path))\n    bs = BeautifulSoup(html, 'html.parser')\n    title = bs.find('h1').get_text()\n    links = bs.find('div', {'id':'bodyContent'}).find_all('a',\n        href=re.compile('^(/wiki/)((?!:).)*$'))\n    if len(links) > 0:\n        newArticle = links[random.randint(0, len(links)-1)].attrs['href']\n        scrape_article(thread_name, newArticle)\n\nthreads = [\n    threading.Thread(\n        target=scrape_article,\n        args=('Thread 1', '/wiki/Kevin_Bacon',)\n    ),\n    threading.Thread(\n        target=scrape_article,\n        args=('Thread 2', '/wiki/Monty_Python',)\n    ),\n]\n[t.start() for t in threads]\n[t.join() for t in threads]\n\n```", "```py\ntime.sleep(5)\n\n```", "```py\nvisited = []\ndef get_links(thread_name, bs):\n    print('Getting links in {}'.format(thread_name))\n    links = bs.find('div', {'id':'bodyContent'}).find_all('a',\n        href=re.compile('^(/wiki/)((?!:).)*$')\n    )\n    return [link for link in links if link not in visited]\n\ndef scrape_article(thread_name, path):\n    visited.append(path)\n    ...\n    links = get_links(thread_name, bs)\n    ...\n\n```", "```py\nmyList.pop(0)\n\n```", "```py\nmyList[len(myList)-1]\n\n```", "```py\nmy_list[i] = my_list[i] + 1\nmy_list.append(my_list[-1])\n\n```", "```py\n# Read the message in from the global list\nmy_message = global_message\n# Write a message back\nglobal_message = 'I've retrieved the message'\n# do something with my_message\n\n```", "```py\ndef storage(queue):\n    conn = pymysql.connect(host='127.0.0.1', unix_socket='/tmp/mysql.sock',\n    user='root', passwd='password', db='mysql', charset='utf8')\n    cur = conn.cursor()\n    cur.execute('USE wikipedia')\n    while 1:\n        if not queue.empty():\n            path = queue.get()\n            cur.execute('SELECT * FROM pages WHERE url = %s', (path))\n            if cur.rowcount == 0:\n                print(f'Storing article {path}')\n                cur.execute('INSERT INTO pages (url) VALUES (%s)', (path))\n                conn.commit()\n            else:\n                print(\"Article already exists: {}\".format(path))\n\nvisited = set()\ndef get_links(thread_name, bs):\n    print('Getting links in {}'.format(thread_name))\n    links = bs.find('div', {'id':'bodyContent'}).find_all(\n        'a',\n        href=re.compile('^(/wiki/)((?!:).)*$')\n    )\n    links = [link.get('href') for link in links]\n    return [link for link in links if link and link not in visited]\n\ndef scrape_article(thread_name, path):\n    time.sleep(5)\n    visited.add(path)\n    print(f'{thread_name}: Scraping {path}')\n    bs = BeautifulSoup(\n        urlopen('http://en.wikipedia.org{}'.format(path)),\n        'html.parser'\n    )\n    links = get_links(thread_name, bs)\n    if len(links) > 0:\n        [queue.put(link) for link in links]\n        newArticle = links[random.randint(0, len(links)-1)].attrs['href']\n        scrape_article(thread_name, newArticle)\n\nqueue = Queue()\n\nthreads = [\n    threading.Thread(\n    ​    target=scrape_article,\n​    ​    args=('Thread 1', '/wiki/Kevin_Bacon',)\n​    ),\n    threading.Thread(\n​    ​    target=scrape_article,\n​    ​    args=('Thread 2', '/wiki/Monty_Python',)\n​    ),\n    threading.Thread(\n​    ​    target=storage,\n​    ​    args=(queue,)\n​    )\n]\n[t.start() for t in threads]\n[t.join() for t in threads]\n\n```", "```py\nimport threading\n\ndef crawler(url):\n    data = threading.local()\n    data.visited = []\n    # Crawl site\n\nthreading.Thread(target=crawler, args=('http://brookings.edu')).start()\n\n```", "```py\nthreading.Thread(target=crawler)\nt.start()\n\nwhile True:\n    time.sleep(1)\n    if not t.isAlive():\n        t = threading.Thread(target=crawler)\n        t.start()\n\n```", "```py\nimport threading\nimport time\n\nclass Crawler(threading.Thread):\n    def __init__(self):\n        threading.Thread.__init__(self)\n        self.done = False\n\n    def isDone(self):\n        return self.done\n\n    def run(self):\n        time.sleep(5)\n        self.done = True\n        raise Exception('Something bad happened!')\n\nt = Crawler()\nt.start()\n\nwhile True:\n    time.sleep(1)\n    if t.isDone():\n        print('Done')\n        break\n    if not t.isAlive():\n        t = Crawler()\n        t.start()\n\n```", "```py\nfrom multiprocessing import Process\nimport time\n\ndef print_time(threadName, delay, iterations):\n    start = int(time.time())\n    for i in range(0,iterations):\n        time.sleep(delay)\n        seconds_elapsed = str(int(time.time()) - start)\n        print (threadName if threadName else seconds_elapsed)\n\nprocesses = [\n    Process(target=print_time, args=('Counter', 1, 100)),\n    Process(target=print_time, args=('Fizz', 3, 33)),\n    Process(target=print_time, args=('Buzz', 5, 20)) \n]\n\n[p.start() for p in processes]\n[p.join() for p in processes]\n\n```", "```py\nimport os\n...\n`# prints the child PID`\n`os``.``getpid``(``)`\n`# prints the parent PID`\nos.getppid()\n\n```", "```py\n[p.join() for p in processes]\n```", "```py\n[p.start() for p in processes]\nprint('Program complete')\n\n```", "```py\nProgram complete\n1\n2\n\n```", "```py\n[p.start() for p in processes]\n[p.join() for p in processes]\nprint('Program complete')\n\n```", "```py\n...\nFizz\n99\nBuzz\n100\nProgram complete\n\n```", "```py\nfrom urllib.request import urlopen\nfrom bs4 import BeautifulSoup\nimport re\nimport random\n\nfrom multiprocessing import Process\nimport os\nimport time\n\nvisited = []\ndef get_links(bs):\n    links = bs.find('div', {'id':'bodyContent'})\n        .find_all('a', href=re.compile('^(/wiki/)((?!:).)*$'))\n    return [link for link in links if link not in visited]\n\ndef scrape_article(path):\n    visited.append(path)\n    html = urlopen('http://en.wikipedia.org{}'.format(path))\n    time.sleep(5)\n    bs = BeautifulSoup(html, 'html.parser')\n    print(f'Scraping {bs.find(\"h1\").get_text()} in process {os.getpid()}')\n    links = get_links(bs)\n    if len(links) > 0:\n        scrape_article(links[random.randint(0, len(links)-1)].attrs['href'])\n\nprocesses = [\n    Process(target=scrape_article, args=('/wiki/Kevin_Bacon',)),\n    Process(target=scrape_article, args=('/wiki/Monty_Python',)) \n]\n[p.start() for p in processes]\n\n```", "```py\nScraping Kevin Bacon in process 4067\nScraping Monty Python in process 4068\nScraping Ewan McGregor in process 4067\nScraping Charisma Records in process 4068\nScraping Renée Zellweger in process 4067\nScraping Genesis (band) in process 4068\nScraping Alana Haim in process 4067\nScraping Maroon 5 in process 4068\n\n```", "```py\ndef scrape_article(path): \n    visited.append(path)\n    print(\"Process {} list is now: {}\".format(os.getpid(), visited))\n\n```", "```py\nProcess 84552 list is now: ['/wiki/Kevin_Bacon']\nProcess 84553 list is now: ['/wiki/Monty_Python']\nScraping Kevin Bacon in process 84552\n/wiki/Desert_Storm\nProcess 84552 list is now: ['/wiki/Kevin_Bacon', '/wiki/Desert_Storm']\nScraping Monty Python in process 84553\n/wiki/David_Jason\nProcess 84553 list is now: ['/wiki/Monty_Python', '/wiki/David_Jason']\n\n```", "```py\ndef task_delegator(taskQueue, urlsQueue):\n    #Initialize with a task for each process\n    visited = ['/wiki/Kevin_Bacon', '/wiki/Monty_Python']\n    taskQueue.put('/wiki/Kevin_Bacon')\n    taskQueue.put('/wiki/Monty_Python')\n\n    while 1:\n        # Check to see if there are new links in the urlsQueue\n        # for processing\n        if not urlsQueue.empty():\n            links = [link for link in urlsQueue.get() if link not in visited]\n            for link in links:\n                #Add new link to the taskQueue\n                taskQueue.put(link)\n\ndef get_links(bs):\n    links = bs.find('div', {'id':'bodyContent'}).find_all('a',\n        href=re.compile('^(/wiki/)((?!:).)*$'))\n    return [link.attrs['href'] for link in links]\n\ndef scrape_article(taskQueue, urlsQueue):\n    while 1:\n        while taskQueue.empty():\n            #Sleep 100 ms while waiting for the task queue\n            #This should be rare\n            time.sleep(.1)\n        path = taskQueue.get()\n        html = urlopen('http://en.wikipedia.org{}'.format(path))\n        time.sleep(5)\n        bs = BeautifulSoup(html, 'html.parser')\n        title = bs.find('h1').get_text()\n        print(f'Scraping {bs.find('h1').get_text()} in process {os.getpid()}')\n        links = get_links(bs)\n        #Send these to the delegator for processing\n        urlsQueue.put(links)\n\nprocesses = []\ntaskQueue = Queue()\nurlsQueue = Queue()\nprocesses.append(Process(target=task_delegator, args=(taskQueue, urlsQueue,)))\nprocesses.append(Process(target=scrape_article, args=(taskQueue, urlsQueue,)))\nprocesses.append(Process(target=scrape_article, args=(taskQueue, urlsQueue,)))\n\nfor p in processes:\n    p.start()\n\n```", "```py\nScraping Kevin Bacon in process 97023\nScraping Monty Python in process 97024\nScraping Kevin Bacon (disambiguation) in process 97023\nScraping Philadelphia in process 97024\nScraping Kevin Bacon filmography in process 97023\nScraping Kyra Sedgwick in process 97024\nScraping Sosie Bacon in process 97023\nScraping Edmund Bacon (architect) in process 97024\nScraping Michael Bacon (musician) in process 97023\nScraping Holly Near in process 97024\nScraping Leading actor in process 97023\n```", "```py\n$ python my_crawler.py website1\n```", "```py\n$ python my_crawler.py website2\n```"]