- en: Chapter 6\. Writing Web Crawlers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章 写作网络爬虫
- en: So far, you’ve seen single static pages with somewhat artificial canned examples.
    In this chapter, you’ll start looking at real-world problems, with scrapers traversing
    multiple pages and even multiple sites.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经看到了一些带有人为干预的静态页面示例。在本章中，你将开始研究真实世界中的问题，使用爬虫来遍历多个页面，甚至是多个站点。
- en: '*Web crawlers* are called such because they crawl across the web. At their
    core is an element of recursion. They must retrieve page contents for a URL, examine
    that page for other URLs, and retrieve *those* pages, ad infinitum.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*网络爬虫*之所以被称为如此，是因为它们横跨整个网络。它们的核心是递归的元素。它们必须获取URL的页面内容，检查该页面中的其他URL，并递归地获取*那些*页面。'
- en: Beware, however: just because you can crawl the web doesn’t mean that you always
    should. The scrapers used in previous examples work great in situations where
    all the data you need is on a single page. With web crawlers, you must be extremely
    conscientious of how much bandwidth you are using and make every effort to determine
    whether there’s a way to make the target server’s load easier.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 但要注意：仅仅因为你可以爬取网页，并不意味着你总是应该这样做。在前面的示例中使用的爬虫在需要获取的数据都在单个页面上的情况下效果很好。使用网络爬虫时，你必须非常注意你使用了多少带宽，并且要尽一切努力确定是否有办法减轻目标服务器的负载。
- en: Traversing a Single Domain
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 遍历单个域
- en: Even if you haven’t heard of Six Degrees of Wikipedia, you may have heard of
    its namesake, Six Degrees of Kevin Bacon.^([1](ch06.html#id458)) In both games,
    the goal is to link two unlikely subjects (in the first case, Wikipedia articles
    that link to each other, and in the second case, actors appearing in the same
    film) by a chain containing no more than six total (including the two original
    subjects).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你没有听说过维基百科的六度分隔，你可能听说过它的名字起源，即凯文·贝肯的六度分隔^([1](ch06.html#id458))。在这两个游戏中，目标是通过包含不超过六个总数的链条（包括两个初始主题）来链接两个不太可能的主题（在第一种情况下，是互相链接的维基百科文章；在第二种情况下，是出演同一部电影的演员）。
- en: For example, Eric Idle appeared in *Dudley Do-Right* with Brendan Fraser, who
    appeared in *The Air I Breathe* with Kevin Bacon.^([2](ch06.html#id459)) In this
    case, the chain from Eric Idle to Kevin Bacon is only three subjects long.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，埃里克·艾德尔与布伦丹·弗雷泽一起出演了*Dudley Do-Right*，而布伦丹·弗雷泽又与凯文·贝肯一起出演了*我呼吸的空气*^([2](ch06.html#id459))。在这种情况下，从埃里克·艾德尔到凯文·贝肯的链只有三个主题。
- en: 'In this section, you’ll begin a project that will become a Six Degrees of Wikipedia
    solution finder: you’ll be able to take [the Eric Idle page](https://en.wikipedia.org/wiki/Eric_Idle)
    and find the fewest number of link clicks that will take you to [the Kevin Bacon
    page](https://en.wikipedia.org/wiki/Kevin_Bacon).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将开始一个项目，这个项目将成为一个维基百科六度分隔解决方案的发现者：你将能够从[埃里克·艾德尔页面](https://en.wikipedia.org/wiki/Eric_Idle)出发，找到需要最少的链接点击数将你带到[凯文·贝肯页面](https://en.wikipedia.org/wiki/Kevin_Bacon)。
- en: 'You should already know how to write a Python script that retrieves an arbitrary
    Wikipedia page and produces a list of links on that page:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该已经知道如何编写一个Python脚本，用于获取任意维基百科页面并生成该页面上的链接列表：
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If you look at the list of links produced, you’ll notice that all the articles
    you’d expect are there: *Apollo 13*, *Philadelphia*, *Primetime Emmy Award*, and
    other films that Kevin Bacon appeared in. However, there are some things that
    you may not want as well:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看生成的链接列表，你会注意到所有你期望的文章都在那里：*阿波罗13号*、*费城*、*黄金时段艾美奖*，以及凯文·贝肯出演的其他电影。然而，也有一些你可能不想要的东西：
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In fact, Wikipedia is full of sidebar, footer, and header links that appear
    on every page, along with links to the category pages, talk pages, and other pages
    that do not contain different articles:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，维基百科中充满了侧边栏、页脚和页眉链接，这些链接出现在每个页面上，还有链接到分类页面、讨论页面以及不包含不同文章的其他页面：
- en: '[PRE2]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Recently, a friend of mine, while working on a similar Wikipedia-scraping project,
    mentioned he had written a large filtering function, with more than 100 lines
    of code, to determine whether an internal Wikipedia link was an article page.
    Unfortunately, he had not spent much time up-front trying to find patterns between
    “article links” and “other links,” or he might have discovered the trick. If you
    examine the links that point to article pages (as opposed to other internal pages),
    you’ll see that they all have three things in common:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，我的一个朋友在进行类似的维基百科爬取项目时提到，他编写了一个大型的过滤函数，超过100行代码，用于确定内部维基百科链接是否是文章页面。不幸的是，他没有花太多时间在前期尝试找出“文章链接”和“其他链接”之间的模式，否则他可能已经发现了窍门。如果你检查指向文章页面的链接，你会发现它们有三个共同点：
- en: They reside within the `div` with the `id` set to `bodyContent`.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们位于`id`设置为`bodyContent`的`div`内。
- en: The URLs do not contain colons.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些URL不包含冒号。
- en: The URLs begin with */wiki/*.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些URL以*/wiki/*开头。
- en: 'You can use these rules to revise the code slightly to retrieve only the desired
    article links by using the regular expression `^(/wiki/)((?!:).)*$`:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用这些规则稍微修改代码，只检索所需的文章链接，方法是使用正则表达式`^(/wiki/)((?!:).)*$`：
- en: '[PRE3]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Running this, you should see a list of all article URLs that the Wikipedia article
    on Kevin Bacon links to.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此程序，你应该会看到维基百科关于Kevin Bacon的所有文章URL列表。
- en: 'Of course, having a script that finds all article links in one, hardcoded Wikipedia
    article, while interesting, is fairly useless in practice. You need to be able
    to take this code and transform it into something more like the following:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，拥有一个在一个硬编码的维基百科文章中找到所有文章链接的脚本，虽然很有趣，但在实践中却相当无用。你需要能够拿着这段代码，将其转换成更像下面这样的东西：
- en: A single function, `getLinks`, that takes in a Wikipedia article URL of the
    form `/wiki/<Article_Name>` and returns a list of all linked article URLs in the
    same form.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个单独的函数，`getLinks`，它接受形式为`/wiki/<Article_Name>`的维基百科文章URL，并返回相同形式的所有链接的文章URL列表。
- en: A main function that calls `getLinks` with a starting article, chooses a random
    article link from the returned list, and calls `getLinks` again, until you stop
    the program or until no article links are found on the new page.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个调用`getLinks`的主函数，以一个起始文章作为参数，从返回的列表中选择一个随机文章链接，并再次调用`getLinks`，直到停止程序或者在新页面上找不到文章链接为止。
- en: 'Here is the complete code that accomplishes this:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这是完成此操作的完整代码：
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The first thing the program does, after importing the needed libraries, is set
    the random-number generator seed with the current system time. This practically
    ensures a new and interesting random path through Wikipedia articles every time
    the program is run.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 程序导入所需的库后，第一件事是使用当前系统时间设置随机数生成器的种子。这实际上确保了每次运行程序时都会得到一个新的有趣的随机路径通过维基百科文章。
- en: Next, the program defines the `getLinks` function, which takes in an article
    URL of the form `/wiki/...`, prepends the Wikipedia domain name, `http://en.wikipedia.org`,
    and retrieves the `BeautifulSoup` object for the HTML at that domain. It then
    extracts a list of article link tags, based on the parameters discussed previously,
    and returns them.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，程序定义了`getLinks`函数，该函数接受形式为`/wiki/...`的文章URL，加上维基百科域名`http://en.wikipedia.org`，并检索该域中HTML的`BeautifulSoup`对象。然后根据前面讨论的参数提取文章链接标签列表，并返回它们。
- en: The main body of the program begins with setting a list of article link tags
    (the `links` variable) to the list of links in the initial page: *https://en.wikipedia.org/wiki/Kevin_Bacon*. It
    then goes into a loop, finding a random article link tag in the page, extracting
    the `href` attribute from it, printing the page, and getting a new list of links
    from the extracted URL.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 程序的主体从设置文章链接标签列表（`links`变量）为初始页面中的链接列表开始：*https://en.wikipedia.org/wiki/Kevin_Bacon*。然后进入循环，找到页面中的一个随机文章链接标签，提取其中的`href`属性，打印页面，并从提取的URL获取一个新的链接列表。
- en: Of course, there’s a bit more to solving a Six Degrees of Wikipedia problem
    than building a scraper that goes from page to page. You also must be able to
    store and analyze the resulting data. For a continuation of the solution to this
    problem, see [Chapter 9](ch09.html#c-9).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，解决维基百科的六度问题比构建一个从页面到页面的爬虫更多一些。你还必须能够存储和分析所得到的数据。要继续解决此问题的解决方案，请参见[第九章](ch09.html#c-9)。
- en: Handle Your Exceptions!
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理你的异常！
- en: Although these code examples omit most exception handling for the sake of brevity,
    be aware that many potential pitfalls could arise. What if Wikipedia changes the
    name of the `bodyContent` tag, for example? When the program attempts to extract
    the text from the tag, it throws an `AttributeError`.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些代码示例为简洁起见省略了大部分异常处理，但请注意可能会出现许多潜在问题。例如，如果维基百科更改了`bodyContent`标签的名称会怎样？当程序尝试从标签中提取文本时，它会抛出`AttributeError`。
- en: So although these scripts might be fine to run as closely watched examples,
    autonomous production code requires far more exception handling than can fit into
    this book. Look back to [Chapter 4](ch04.html#c-4) for more information about
    this.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，虽然这些脚本可能很适合作为密切观察的例子运行，但自主生产代码需要比本书中所能容纳的异常处理要多得多。查看[第四章](ch04.html#c-4)了解更多信息。
- en: Crawling an Entire Site
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 爬取整个站点
- en: In the previous section, you took a random walk through a website, going from
    link to link. But what if you need to systematically catalog or search every page
    on a site? Crawling an entire site, especially a large one, is a memory-intensive
    process that is best suited to applications for which a database to store crawling
    results is readily available. However, you can explore the behavior of these types
    of applications without running them full-scale. To learn more about running these
    applications by using a database, see [Chapter 9](ch09.html#c-9).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，你随机地浏览了一个网站，从一个链接跳到另一个链接。但是如果你需要系统地编目或搜索站点上的每一页呢？爬行整个站点，特别是大型站点，是一个占用内存的过程，最适合于那些可以轻松存储爬行结果的应用程序。然而，即使不是全面运行，你也可以探索这些类型应用程序的行为。要了解更多关于使用数据库运行这些应用程序的信息，请参见[第9章](ch09.html#c-9)。
- en: 'When might crawling an entire website be useful, and when might it be harmful?
    Web scrapers that traverse an entire site are good for many things, including:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 什么时候爬整个网站可能有用，什么时候可能有害？遍历整个站点的网页抓取器适合于许多用途，包括：
- en: '*Generating a site map*'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*生成站点地图*'
- en: 'A few years ago, I was faced with a problem: an important client wanted an
    estimate for a website redesign but did not want to provide my company with access
    to the internals of their current content management system and did not have a
    publicly available site map. I was able to use a crawler to cover the entire site,
    gather all internal links, and organize the pages into the actual folder structure
    used on the site. This allowed me to quickly find sections of the site I wasn’t
    even aware existed and accurately count how many page designs would be required
    and how much content would need to be migrated.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 几年前，我面临一个问题：一个重要的客户想要对网站重新设计提供估价，但不愿向我的公司提供其当前内容管理系统的内部访问权限，也没有公开的站点地图。我能够使用爬虫覆盖整个站点，收集所有内部链接，并将页面组织成实际在站点上使用的文件夹结构。这让我能够快速找到我甚至不知道存在的站点部分，并准确计算所需的页面设计数量以及需要迁移的内容量。
- en: '*Gathering data*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*收集数据*'
- en: Another client wanted to gather articles (stories, blog posts, news articles,
    etc.) in order to create a working prototype of a specialized search platform.
    Although these website crawls didn’t need to be exhaustive, they did need to be
    fairly expansive (we were interested in getting data from only a few sites). I
    was able to create crawlers that recursively traversed each site and collected
    only data found on article pages.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个客户想要收集文章（故事、博客文章、新闻文章等），以创建一个专业搜索平台的工作原型。虽然这些网站爬行不需要详尽，但它们确实需要相当广泛（我们只对从几个站点获取数据感兴趣）。我能够创建爬虫，递归地遍历每个站点，并仅收集在文章页面上找到的数据。
- en: The general approach to an exhaustive site crawl is to start with a top-level
    page (such as the home page) and search for a list of all internal links on that
    page. Every one of those links is then crawled, and additional lists of links
    are found on each one of them, triggering another round of crawling.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于详尽的网站爬行，一般的方法是从顶级页面（比如首页）开始，并搜索该页面上的所有内部链接列表。然后对每个链接进行爬行，并在每个链接上找到其他链接列表，触发另一轮爬行。
- en: Clearly, this is a situation that can blow up quickly. If every page has 10
    internal links, and a website is 5 pages deep (a fairly typical depth for a medium-size
    website), then the number of pages you need to crawl is 10⁵, or 100,000 pages,
    before you can be sure that you’ve exhaustively covered the website. Strangely
    enough, although “5 pages deep and 10 internal links per page” are fairly typical
    dimensions for a website, very few websites have 100,000 or more pages. The reason,
    of course, is that the vast majority of internal links are duplicates.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这是一个可能迅速扩展的情况。如果每页有10个内部链接，而网站深度为5页（对于中等大小的网站来说是一个相当典型的深度），那么你需要爬行的页面数为10⁵，即100,000页，才能确保你已经详尽地覆盖了整个网站。奇怪的是，虽然“每页5个深度和每页10个内部链接”是网站的相当典型的维度，但很少有网站拥有100,000页或更多页面。当然，原因在于绝大多数的内部链接是重复的。
- en: 'To avoid crawling the same page twice, it is extremely important that all internal
    links discovered are formatted consistently and kept in a running set for easy
    lookups, while the program is running. A *set* is similar to a list, but elements
    do not have a specific order, and only unique elements are stored, which is ideal
    for our needs. Only links that are “new” should be crawled and searched for additional
    links:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免两次爬取同一页，非常重要的是，发现的所有内部链接都要一致地格式化，并在程序运行时保持在一个运行集合中进行简单查找。*集合* 类似于列表，但元素没有特定的顺序，并且只存储唯一的元素，这对我们的需求是理想的。只有“新”的链接应该被爬取，并搜索其他链接：
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To show you the full effect of how this web crawling business works, I’ve relaxed
    the standards of what constitutes an internal link (from previous examples). Rather
    than limit the scraper to article pages, it looks for all links that begin with
    */wiki/*, regardless of where they are on the page, and regardless of whether
    they contain colons. Remember: article pages do not contain colons, but file-upload
    pages, talk pages, and the like do contain colons in the URL.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了向你展示这种网络爬行业务的完整效果，我放宽了什么构成内部链接的标准（来自之前的示例）。不再将爬虫限制于文章页面，而是查找所有以 */wiki/* 开头的链接，无论它们在页面的何处，也无论它们是否包含冒号。请记住：文章页面不包含冒号，但文件上传页面、讨论页面等在
    URL 中包含冒号。
- en: Initially, `getLinks` is called with an empty URL. This is translated as “the
    front page of Wikipedia” as soon as the empty URL is prepended with `http://en.wikipedia.org`
    inside the function. Then, each link on the first page is iterated through and
    a check is made to see whether it is in the set of pages that the script has encountered
    already. If not, it is added to the list, printed to the screen, and the `getLinks` function
    is called recursively on it.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，使用空 URL 调用 `getLinks`。这被翻译为“维基百科的首页”，因为空 URL 在函数内部以 `http://en.wikipedia.org`
    开头。然后，对首页上的每个链接进行迭代，并检查它是否在脚本已遇到的页面集合中。如果没有，将其添加到列表中，打印到屏幕上，并递归调用 `getLinks`。
- en: A Warning About Recursion
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 递归的警告
- en: 'This is a warning rarely seen in software books, but I thought you should be
    aware: if left running long enough, the preceding program will almost certainly
    crash.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在软件书籍中很少见的警告，但我认为你应该知道：如果运行时间足够长，前面的程序几乎肯定会崩溃。
- en: Python has a default recursion limit (the number of times a program can recursively
    call itself) of 1,000\. Because Wikipedia’s network of links is extremely large,
    this program will eventually hit that recursion limit and stop, unless you put
    in a recursion counter or something to prevent that from happening.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Python 的默认递归限制（程序可以递归调用自身的次数）为 1,000。由于维基百科的链接网络非常庞大，这个程序最终会达到递归限制并停止，除非你在其中加入递归计数器或其他东西以防止这种情况发生。
- en: For “flat” sites that are fewer than 1,000 links deep, this method usually works
    well, with a few unusual exceptions. For instance, I once encountered a bug in
    a dynamically generated URL that depended on the address of the current page to
    write the link on that page. This resulted in infinitely repeating paths like
    */blogs/blogs.../blogs/blog-post.php*.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于少于 1,000 个链接深度的“平”站点，这种方法通常效果良好，但也有一些特殊情况。例如，我曾经遇到过一个动态生成的 URL 的 bug，它依赖于当前页面的地址来在该页面上写入链接。这导致了无限重复的路径，如
    */blogs/blogs.../blogs/blog-post.php*。
- en: For the most part, however, this recursive technique should be fine for any
    typical website you’re likely to encounter.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数情况下，这种递归技术对于你可能遇到的任何典型网站应该都没问题。
- en: Collecting Data Across an Entire Site
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在整个站点收集数据
- en: Web crawlers would be fairly boring if all they did was hop from one page to
    the other. To make them useful, you need to be able to do something on the page
    while you’re there. Let’s look at how to build a scraper that collects the title,
    the first paragraph of content, and the link to edit the page (if available).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果网络爬虫只是从一个页面跳到另一个页面，那么它们会变得相当无聊。为了使它们有用，你需要在访问页面时能够做些事情。让我们看看如何构建一个爬虫，收集标题、内容的第一个段落，并且（如果有的话）编辑页面的链接。
- en: 'As always, the first step to determine how best to do this is to look at a
    few pages from the site and determine a pattern. By looking at a handful of Wikipedia
    pages (both articles and nonarticle pages such as the privacy policy page), the
    following things should be clear:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 与往常一样，确定最佳方法的第一步是查看站点的几个页面并确定一个模式。通过查看一些维基百科页面（包括文章和非文章页面，如隐私政策页面），应该清楚以下几点：
- en: All titles (on all pages, regardless of their status as an article page, an
    edit history page, or any other page) have titles under `h1` → `span` tags, and
    these are the only `h1` tags on the page.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有标题（无论其是否作为文章页面、编辑历史页面或任何其他页面存在）都在`h1` → `span`标签下，这些是页面上唯一的`h1`标签。
- en: As mentioned before, all body text lives under the `div#bodyContent` tag. However,
    if you want to get more specific and access just the first paragraph of text,
    you might be better off using `div#mw-content-text` → `​p` (selecting the first
    paragraph tag only). This is true for all content pages except file pages (for
    example, [*https://en.wikipedia.org/wiki/File:Orbit_of_274301_Wikipedia.svg*](https://en.wikipedia.org/wiki/File:Orbit_of_274301_Wikipedia.svg)),
    which do not have sections of content text.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正如之前提到的，所有正文文本都位于`div#bodyContent`标签下。然而，如果你想更加具体并且只访问文本的第一个段落，那么最好使用`div#mw-content-text`
    → `​p`（仅选择第一个段落标签）。这对所有内容页面都适用，但不适用于文件页面（例如，[*https://en.wikipedia.org/wiki/File:Orbit_of_274301_Wikipedia.svg*](https://en.wikipedia.org/wiki/File:Orbit_of_274301_Wikipedia.svg)），因为这些页面没有内容文本的部分。
- en: Edit links occur only on article pages. If they occur, they will be found in
    the `li#ca-edit` tag, under `li#ca-edit` →​ `span` →​ `a`.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编辑链接仅出现在文章页面上。如果它们存在，它们将在`li#ca-edit`标签下找到，位于`li#ca-edit` →​ `span` →​ `a`。
- en: 'By modifying your basic crawling code, you can create a combination crawler/data-gathering
    (or at least, data-printing) program:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 通过修改您的基本爬取代码，您可以创建一个组合爬虫/数据收集（或至少是数据打印）程序：
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `for` loop in this program is essentially the same as it was in the original
    crawling program (with the addition of printed dashes for clarity, separating
    the printed content).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序中的`for`循环基本上与原始爬虫程序中的相同（增加了打印内容之间的打印破折号以增加清晰度）。
- en: Because you can never be entirely sure that all the data is on each page, each
    `print` statement is arranged in the order that it is likeliest to appear on the
    site. That is, the `h1` title tag appears on every page (as far as I can tell,
    at any rate), so you attempt to get that data first. The text content appears
    on most pages (except for file pages), so that is the second piece of data retrieved.
    The Edit button appears only on pages in which both titles and text content already
    exist, but it does not appear on all of those pages.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你永远不能完全确定每个页面上是否有所有数据，因此每个`print`语句都按照最有可能出现在页面上的顺序排列。也就是说，`h1`标题标签似乎出现在每个页面上（至少在我所知的范围内），因此首先尝试获取该数据。文本内容出现在大多数页面上（除了文件页面），因此这是第二个检索到的数据片段。编辑按钮仅出现在已经存在标题和文本内容的页面上，但并非所有这些页面都有该按钮。
- en: Different Patterns for Different Needs
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不同的需求有不同的模式
- en: Obviously, some dangers are involved with wrapping multiple lines in an exception
    handler. You cannot tell which line threw the exception, for one thing. Also,
    if for some reason a page contains an Edit button but no title, the Edit button
    would never be logged. However, it suffices for many instances in which there
    is an order of likeliness of items appearing on the site, and inadvertently missing
    a few data points or keeping detailed logs is not a problem.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，将多行包装在异常处理程序中存在一些危险。例如，你无法判断是哪一行抛出了异常。此外，如果某个页面包含编辑按钮但没有标题，编辑按钮将永远不会被记录。然而，在许多实例中，这足以满足页面上物品出现的可能顺序，并且无意中漏掉一些数据点或保留详细日志并不是问题。
- en: You might notice that in this and all the previous examples, you haven’t been
    “collecting” data so much as “printing” it. Obviously, data in your terminal is
    hard to manipulate. For more on storing information and creating databases, see [Chapter 9](ch09.html#c-9).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到，在此及之前的所有示例中，你不是“收集”数据，而是“打印”它。显然，在终端中操作数据是很困难的。有关存储信息和创建数据库的更多信息，请参阅[第9章](ch09.html#c-9)。
- en: Crawling Across the Internet
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 横跨互联网的爬虫
- en: 'Whenever I give a talk on web scraping, someone inevitably asks, “How do you
    build Google?” My answer is always twofold: “First, you get many billions of dollars
    so that you can buy the world’s largest data warehouses and place them in hidden
    locations all around the world. Second, you build a web crawler.”'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我在网页抓取方面发表演讲时，总会有人不可避免地问：“你怎么建造Google？”我的答案总是双重的：“首先，你需要获得数十亿美元，这样你就能购买世界上最大的数据仓库，并将它们放置在世界各地的隐藏位置。其次，你需要构建一个网络爬虫。”
- en: When Google started in 1996, it was just two Stanford graduate students with
    an old server and a Python web crawler. Now that you know how to scrape the web, you’re
    just some VC funding away from becoming the next tech multibillionaire!
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当Google于1996年开始时，只是两位斯坦福大学研究生和一台旧服务器以及一个Python网络爬虫。现在，你已经知道如何抓取网络，只需一些风险投资，你就可能成为下一个科技亿万富翁！
- en: In all seriousness, web crawlers are at the heart of what drives many modern
    web technologies, and you don’t necessarily need a large data warehouse to use
    them. To do any cross-domain data analysis, you do need to build crawlers that
    can interpret and store data across the myriad of pages on the internet.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 网络爬虫是许多现代网络技术的核心，不一定需要大型数据仓库来使用它们。要进行跨域数据分析，确实需要构建能够解释和存储互联网上大量页面数据的爬虫。
- en: Just as in the previous example, the web crawlers you are going to build will
    follow links from page to page, building out a map of the web. But this time,
    they will not ignore external links; they will follow them.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面的例子一样，你将要构建的网络爬虫将跟随页面到页面的链接，构建出网络的地图。但是这次，它们不会忽略外部链接；它们将会跟随它们。
- en: Unknown Waters Ahead
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 未知的前方
- en: Keep in mind that the code from the next section can go *anywhere* on the internet.
    If we’ve learned anything from Six Degrees of Wikipedia, it’s that it’s entirely
    possible to go from a site such as [*http://www.sesamestreet.org/*](http://www.sesamestreet.org/)
    to something less savory in just a few hops.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，下一节中的代码可以放在互联网的*任何地方*。如果我们从《维基百科的六度》中学到了什么，那就是从[*http://www.sesamestreet.org/*](http://www.sesamestreet.org/)这样的网站到一些不那么愉快的地方只需几个跳转。
- en: Kids, ask your parents before running this code. For those with sensitive constitutions
    or with religious restrictions that might prohibit seeing text from a prurient
    site, follow along by reading the code examples but be careful when running them.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 孩子们，在运行此代码之前，请征得父母的同意。对于那些有敏感情绪或因宗教限制而禁止查看淫秽网站文本的人，请通过阅读代码示例跟随进行，但在运行时要小心。
- en: 'Before you start writing a crawler that follows all outbound links willy-nilly,
    you should ask yourself a few questions:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在你开始编写任意跟踪所有出站链接的爬虫之前，请先问自己几个问题：
- en: What data am I trying to gather? Can this be accomplished by scraping just a
    few predefined websites (almost always the easier option), or does my crawler
    need to be able to discover new websites I might not know about?
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我试图收集哪些数据？这是否可以通过仅抓取几个预定义的网站（几乎总是更简单的选项）来完成，还是我的爬虫需要能够发现我可能不知道的新网站？
- en: When my crawler reaches a particular website, will it immediately follow the
    next outbound link to a new website, or will it stick around for a while and drill
    down into the current website?
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我的爬虫到达特定网站时，它会立即跟随下一个出站链接到一个新的网站，还是会停留一段时间并深入到当前网站？
- en: Are there any conditions under which I would not want to scrape a particular
    site? Am I interested in non-English content?
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否存在任何条件使我不想抓取特定网站？我对非英语内容感兴趣吗？
- en: How am I protecting myself against legal action if my web crawler catches the
    attention of a webmaster on one of the sites it runs across? (Check out [Chapter 2](ch02.html#c-2)
    for more information on this subject.)
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我的网络爬虫引起某个网站管理员的注意，我如何保护自己免受法律行动？（查看[第二章](ch02.html#c-2)获取更多信息。）
- en: 'A flexible set of Python functions that can be combined to perform a variety
    of types of web scraping can be easily written in fewer than 60 lines of code.
    Here, I’ve omitted the library imports for brevity and broken the code up into
    multiple sections for the purposes of discussion. However, a full working version
    can be found in the [GitHub repository](https://github.com/REMitchell/python-scraping)
    for this book:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 可以轻松编写一组灵活的Python函数，这些函数可以组合执行各种类型的网络抓取，代码行数少于60行。这里，出于简洁考虑，我省略了库导入，并将代码分成多个部分进行讨论。然而，完整的工作版本可以在本书的[GitHub存储库](https://github.com/REMitchell/python-scraping)中找到：
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The first function is `getInternalLinks`. This takes, as arguments, a BeautifulSoup
    object and the URL of the page. This URL is used only to identify the `netloc`
    (network location) and `scheme` (usually `http` or `https`) of the internal site,
    so it’s important to note that any internal URL for the target site can be used
    here—it doesn’t need to be the exact URL of the BeautifulSoup object passed in.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个函数是`getInternalLinks`。它以BeautifulSoup对象和页面的URL作为参数。此URL仅用于标识内部站点的`netloc`（网络位置）和`scheme`（通常为`http`或`https`），因此重要的是要注意，可以在此处使用目标站点的任何内部URL——不需要是传入的BeautifulSoup对象的确切URL。
- en: 'This function creates a set called `internalLinks` used to track all internal
    links found on the page. It checks all anchor tags for an `href` attribute that
    either doesn’t contain a `netloc` (is a relative URL like “/careers/”) or has
    a `netloc` that matches that of the URL passed in:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数创建了一个名为`internalLinks`的集合，用于跟踪页面上找到的所有内部链接。它检查所有锚标签的`href`属性，如果`href`属性不包含`netloc`（即像“/careers/”这样的相对URL）或者具有与传入的URL相匹配的`netloc`，则会检查：
- en: '[PRE8]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The function `getExternalLinks` works similarly to `getInternalLinks`. It examines
    all anchor tags with an `href` attribute and looks for those that have a populated
    `netloc` that does *not* match that of the URL passed in:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`getExternalLinks`的工作方式与`getInternalLinks`类似。它检查所有带有`href`属性的锚标签，并寻找那些具有不与传入的URL相匹配的填充`netloc`的标签：
- en: '[PRE9]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The function `getRandomExternalLink` uses the function `getExternalLinks` to
    get a list of all external links on the page. If at least one link is found, it
    picks a random link from the list and returns it:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`getRandomExternalLink`使用函数`getExternalLinks`获取页面上所有外部链接的列表。如果找到至少一个链接，则从列表中选择一个随机链接并返回：
- en: '[PRE10]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The function `followExternalOnly` uses `getRandomExternalLink` and then recursively
    traverses across the internet. You can call it like this:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`followExternalOnly`使用`getRandomExternalLink`然后递归地遍历整个互联网。你可以这样调用它：
- en: '[PRE11]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This program starts at [*http://oreilly.com*](http://oreilly.com) and randomly
    hops from external link to external link. Here’s an example of the output it produces:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这个程序从[*http://oreilly.com*](http://oreilly.com)开始，然后随机跳转到外部链接。以下是它产生的输出示例：
- en: '[PRE12]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: External links are not always guaranteed to be found on the first page of a
    website. To find external links in this case, a method similar to the one used
    in the previous crawling example is employed to recursively drill down into a
    website until it finds an external link.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 外部链接并不总是能在网站的第一页找到。在这种情况下查找外部链接，会采用类似于前面爬虫示例中使用的方法，递归地深入网站，直到找到外部链接。
- en: '[Figure 6-1](#flowchart_crawl) illustrates the operation as a flowchart.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6-1](#flowchart_crawl) 以流程图形式说明了其操作。'
- en: '![Alt Text](assets/wsp3_0601.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![Alt Text](assets/wsp3_0601.png)'
- en: Figure 6-1\. Flowchart for the script that crawls through sites on the internet
  id: totrans-93
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-1\. 爬取互联网站点的脚本流程图
- en: Don’t Put Example Programs into Production
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不要将示例程序投入生产
- en: I keep bringing this up, but for the sake of space and readability, the example
    programs in this book do not always contain the necessary checks and exception
    handling required for production-ready code. For example, if an external link
    is not found anywhere on a site that this crawler encounters (unlikely, but it’s
    bound to happen at some point if you run it long enough), this program will keep
    running until it hits Python’s recursion limit.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我一直强调这一点，但为了节省空间和提高可读性，本书中的示例程序并不总是包含生产级代码所需的必要检查和异常处理。例如，如果在此爬虫遇到的网站上未找到任何外部链接（这不太可能，但如果你运行足够长的时间，总会发生），这个程序将继续运行，直到达到Python的递归限制。
- en: One easy way to increase the robustness of this crawler would be to combine
    it with the connection exception-handling code in [Chapter 4](ch04.html#c-4).
    This would allow the code to choose a different URL to go to if an HTTP error
    or server exception was encountered when retrieving the page.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 增加此爬虫的健壮性的一种简单方法是将其与[第四章](ch04.html#c-4)中的连接异常处理代码结合起来。这将允许代码在检索页面时遇到HTTP错误或服务器异常时选择不同的URL进行跳转。
- en: Before running this code for any serious purpose, make sure that you are putting
    checks in place to handle potential pitfalls.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在为任何严肃的目的运行此代码之前，请确保您正在采取措施来处理潜在的陷阱。
- en: 'The nice thing about breaking up tasks into simple functions such as “find
    all external links on this page” is that the code can later be refactored to perform
    a different crawling task. For example, if your goal is to crawl an entire site
    for external links and make a note of each one, you can add the following function:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 将任务分解为简单函数（如“查找此页面上的所有外部链接”）的好处在于，稍后可以重构代码以执行不同的爬虫任务。例如，如果你的目标是爬取整个站点的外部链接并记录每一个，可以添加以下函数：
- en: '[PRE13]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This code can be thought of as two loops—one gathering internal links, one gathering
    external links—working in conjunction with each other. The flowchart looks something
    like [Figure 6-2](#flow_diagram_crawl).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码可以看作是两个循环的组合——一个收集内部链接，一个收集外部链接。流程图看起来类似于[图 6-2](#flow_diagram_crawl)。
- en: '![Alt Text](assets/wsp3_0602.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![Alt Text](assets/wsp3_0602.png)'
- en: Figure 6-2\. Flow diagram for the website crawler that collects all external
    links
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-2\. 收集所有外部链接的网站爬虫流程图
- en: Jotting down or making diagrams of what the code should do before you write
    the code itself is a fantastic habit to get into, and one that can save you a
    lot of time and frustration as your crawlers get more complicated.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写代码之前记下或制作代码应该完成的内容的图表是一个极好的习惯，并且可以在爬虫变得更加复杂时为你节省大量时间和烦恼。
- en: ^([1](ch06.html#id458-marker)) A popular parlor game created in the 1990s, [*https://en.wikipedia.org/wiki/Six_Degrees_of_Kevin_Bacon*](https://en.wikipedia.org/wiki/Six_Degrees_of_Kevin_Bacon).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch06.html#id458-marker)) 1990年代创造的一种流行的客厅游戏，[*https://en.wikipedia.org/wiki/Six_Degrees_of_Kevin_Bacon*](https://en.wikipedia.org/wiki/Six_Degrees_of_Kevin_Bacon)。
- en: ^([2](ch06.html#id459-marker)) Thanks to [The Oracle of Bacon](http://oracleofbacon.org)
    for satisfying my curiosity about this particular chain.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch06.html#id459-marker)) 感谢[培根的神谕](http://oracleofbacon.org) 满足了我对这一特定链条的好奇心。
- en: ^([3](ch06.html#id473-marker)) See [“Exploring a ‘Deep Web’ That Google Can’t
    Grasp”](http://nyti.ms/2pohZmu) by Alex Wright.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch06.html#id473-marker)) 参见[“探索谷歌无法理解的‘深网’”](http://nyti.ms/2pohZmu) 由亚历克斯·赖特撰写。
- en: '^([4](ch06.html#id475-marker)) See [“Hacker Lexicon: What Is the Dark Web?”](http://bit.ly/2psIw2M)
    by Andy Greenberg.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch06.html#id475-marker)) 参见[“黑客词汇表：什么是暗网？”](http://bit.ly/2psIw2M) 由安迪·格林伯格撰写。
