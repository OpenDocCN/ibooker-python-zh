["```py\n    ds = ray.data.read_csv(\n        \"2021\",\n        partition_filter=None # Since the file doesn't end in .csv\n    )                   \n```", "```py\nfs = fsspec.filesystem('https')\nds = ray.data.read_csv(\n    \"https://https://gender-pay-gap.service.gov.uk/viewing/download-data/2021\",\n    filesystem=fs,\n    partition_filter=None # Since the file doesn't end in .csv\n    )\n```", "```py\nword_count.write_csv(\"s3://ray-demo/wc\")\n```", "```py\nfrom ray.util.dask import enable_dask_on_ray, disable_dask_on_ray\nenable_dask_on_ray() # Routes all Dask calls through the Ray scheduler\n```", "```py\nimport raydp\nspark = raydp.init_spark(\n  app_name = \"sleepy\",\n  num_executors = 2,\n  executor_cores = 1,\n  executor_memory = \"2GB\"\n)\n```", "```py\ndask_df = ds.to_dask()\n```", "```py\nmini_sf_covid_df = sf_covid_df[ sf_covid_df['vaccination_status'] == \n  'All'][['specimen_collection_date', 'new_cases']]\n```", "```py\ndef process_overlapped(df):\n     df.rolling('5D').mean()\nrolling_avg = partitioned_df.map_overlap(process_overlapped, pd.Timedelta('5D'), 0)\n```", "```py\ndask.compute(\n    raw_grouped[[\"new_cases\"]].max(),\n    raw_grouped[[\"new_cases\"]].mean())\n```", "```py\n# Write a custom weighted mean, we get either a DataFrameGroupBy with\n# multiple columns or SeriesGroupBy for each chunk\ndef process_chunk(chunk):\n    def weighted_func(df):\n        return (df[\"EmployerSize\"] * df[\"DiffMeanHourlyPercent\"]).sum()\n    return (chunk.apply(weighted_func), chunk.sum()[\"EmployerSize\"])\n\ndef agg(total, weights):\n    return (total.sum(), weights.sum())\n\ndef finalize(total, weights):\n    return total / weights\n\nweighted_mean = dd.Aggregation(\n    name='weighted_mean',\n    chunk=process_chunk,\n    agg=agg,\n    finalize=finalize)\n\naggregated = df_diff_with_emp_size.groupby(\"PostCode\")\n    [\"EmployerSize\", \"DiffMeanHourlyPercent\"].agg(weighted_mean)\n```", "```py\nraw_grouped = sf_covid_df.groupby(lambda x: 0)\n```", "```py\n# Wrap Dask's hyperloglog in dd.Aggregation\n\nfrom dask.dataframe import hyperloglog\n\napprox_unique = dd.Aggregation(\n    name='aprox_unique',\n    chunk=hyperloglog.compute_hll_array,\n    agg=hyperloglog.reduce_state,\n    finalize=hyperloglog.estimate_count)\n\naggregated = df_diff_with_emp_size.groupby(\"PostCode\")\n    [\"EmployerSize\", \"DiffMeanHourlyPercent\"].agg(weighted_mean)\n```", "```py\ndivisions = pd.date_range(\n    start=\"2021-01-01\", end=datetime.today(), freq='7D').tolist()\npartitioned_df_as_part_of_set_index = mini_sf_covid_df.set_index(\n    'specimen_collection_date', divisions=divisions)\n```", "```py\nreparted = indexed.repartition(partition_size=\"20kb\")\n```", "```py\ndef fillna(df):\n    return df.fillna(value={\"PostCode\": \"UNKNOWN\"}).fillna(value=0)\n\nnew_df = df.map_partitions(fillna)\n# Since there could be an NA in the index clear the partition/division information\nnew_df.clear_divisions()\n```", "```py\ndef tokenize_batch(batch):\n    nested_tokens = map(lambda s: s.split(\" \"), batch)\n    # Flatten the result\n    nr = []\n    for r in nested_tokens:\n        nr.extend(r)\n    return nr\n\ndef pair_batch(batch):\n    return list(map(lambda w: (w, 1), batch))\n\ndef filter_for_interesting(batch):\n    return list(filter(lambda wc: wc[1] > 1, batch))\n\nwords = pages.map_batches(tokenize_batch).map_batches(pair_batch)\n# The one part we can't rewrite with map_batches since it involves a shuffle\ngrouped_words = words.groupby(lambda wc: wc[0]) \ninteresting_words = groupd_words.map_batches(filter_for_interesting)\n```", "```py\n# Kind of hacky string munging to get a median-ish to weight our values.\ndef update_empsize_to_median(df):\n    def to_median(value):\n        if \" to \" in value:\n            f , t = value.replace(\",\", \"\").split(\" to \")\n            return (int(f) + int(t)) / 2.0\n        elif \"Less than\" in value:\n            return 100\n        else:\n            return 10000\n    df[\"EmployerSize\"] = df[\"EmployerSize\"].apply(to_median)\n    return df\n\nds_with_median = ds.map_batches(update_empsize_to_median, batch_format=\"pandas\")\n```", "```py\ndef extract_text_for_batch(sites):\n    text_futures = map(lambda s: extract_text.remote(s), sites)\n    result = ray.get(list(text_futures))\n    # ray.get returns None on an empty input, but map_batches requires lists\n    if result is None:\n        return []\n    return result\n\ndef tokenize_batch(texts):\n    token_futures = map(lambda s: tokenize.remote(s), texts)\n    result = ray.get(list(token_futures))\n    if result is None:\n        return []\n    # Flatten the result\n    nr = []\n    for r in result:\n        nr.extend(r)\n    return nr\n\n# Exercise for the reader: generalize the preceding patterns - \n# note the flatten magic difference\n\nurls = ray.data.from_items([\"http://www.holdenkarau.com\", \"http://www.google.com\"])\n\npages = urls.map(fetch)\n\npage_text = pages.map_batches(extract_text_for_batch)\nwords = page_text.map_batches(tokenize_batch)\nword_count = words.groupby(lambda x: x).count()\nword_count.show()\n```", "```py\ndef init_func(key):\n    # First elem is weighted total, second is weights\n    return [0, 0]\n\ndef accumulate_func(accumulated, row):\n    return [\n        accumulated[0] + \n        (float(row[\"EmployerSize\"]) * float(row[\"DiffMeanHourlyPercent\"])),\n        accumulated[1] + row[\"DiffMeanHourlyPercent\"]]\n\ndef combine_aggs(agg1, agg2):\n    return (agg1[0] + agg2[0], agg1[1] + agg2[1])\n\ndef finalize(agg):\n    if agg[1] != 0:\n        return agg[0] / agg[1]\n    else:\n        return 0\n\nweighted_mean = ray.data.aggregate.AggregateFn(\n    name='weighted_mean',\n    init=init_func,\n    merge=combine_aggs,\n    accumulate_row=accumulate_func, # Used to be accumulate\n    # There is a higher performance option called accumulate_block for vectorized op\n    finalize=finalize)\naggregated = ds_with_median.groupby(\"PostCode\").aggregate(weighted_mean)\n```"]