["```py\nfrom dask_yarn import YarnCluster\nfrom dask.distributed import Client\n\n# Create a cluster where each worker has two cores and 8 GiB of memory\ncluster = YarnCluster(\n    environment='your_environment.tar.gz',\n    worker_vcores=2,\n    worker_memory=\"4GiB\")\n\n# Scale out to num_workers such workers\ncluster.scale(num_workers)\n\n# Connect to the cluster\nclient = Client(cluster)\n```", "```py\nfrom dask_jobqueue import SLURMCluster\nfrom dask.distributed import Client\n\ncluster = SLURMCluster(\n    queue='regular',\n    account=\"slurm_caccount\",\n    cores=24,\n    memory=\"500 GB\"\n)\ncluster.scale(jobs=SLURM_JOB_COUNT)  # Ask for N jobs from Slurm\n\nclient = Client(cluster)\n\n# Auto-scale between 10 and 100 jobs\ncluster.adapt(minimum_jobs=10, maximum_jobs=100)\ncluster.adapt(maximum_memory=\"10 TB\")  # Or use core/memory limits\n```", "```py\nimport s3fs\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\nminio_storage_options = {\n    \"key\": MINIO_KEY,\n    \"secret\": MINIO_SECRET,\n    \"client_kwargs\": {\n        \"endpoint_url\": \"http://ENDPOINT_URL\",\n        \"region_name\": 'us-east-1'\n    },\n    \"config_kwargs\": {\"s3\": {\"signature_version\": 's3v4'}},\n}\n\ndf.to_parquet(f's3://s3_destination/{filename}',\n              compression=\"gzip\",\n              storage_options=minio_storage_options,\n              engine=\"fastparquet\")\n\ndf = dd.read_parquet(\n    f's3://s3_source/',\n    storage_options=minio_storage_options,\n    engine=\"pyarrow\"\n)\n```", "```py\ndf = dd.read_sql_table('accounts', 'sqlite:///path/to/your.db',\n                       npartitions=10, index_col='id')\n```", "```py\nfrom fugue_notebook import setup\nsetup (is_lab=True)\nur = ('https://d37ci6vzurychx.cloudfront.net/trip-data/'\n      'yellow_tripdata_2018-01.parquet')\ndf = dd.read_parquet(url)\n\n%%fsql dask\ntempdf = SELECT VendorID, AVG (total_amount) AS average_fare FROM df\nGROUP BY VendorID\n\nSELECT *\nFROM tempdf\nORDER BY average fare DESC\nLIMIT 5\nPRINT\n```", "```py\nschema: VendorID:long, average_fare:double\n```", "```py\nfrom dask.distributed import Client\n\nclient = Client()\nclient.log_event(topic=\"custom_events\", msg=\"hello world\")\nclient.get_events(\"custom_events\")\n```", "```py\nfrom dask.distributed import Client, LocalCluster\n\nclient = Client(cluster)  # Connect to distributed cluster and override default\n\nd = {'x': [3.0, 1.0, 0.2], 'y': [2.0, 0.5, 0.1], 'z': [1.0, 0.2, 0.4]}\nscores_df = dd.from_pandas(pd.DataFrame(data=d), npartitions=1)\n\ndef compute_softmax(partition, axis=0):\n    \"\"\" computes the softmax of the logits\n :param logits: the vector to compute the softmax over\n :param axis: the axis we are summing over\n :return: the softmax of the vector\n \"\"\"\n    if partition.empty:\n        return\n    import timeit\n    x = partition[['x', 'y', 'z']].values.tolist()\n    start = timeit.default_timer()\n    axis = 0\n    e = np.exp(x - np.max(x))\n    ret = e / np.sum(e, axis=axis)\n    stop = timeit.default_timer()\n    partition.log_event(\"softmax\", {\"start\": start, \"x\": x, \"stop\": stop})\n    dask.distributed.get_worker().log_event(\n        \"softmax\", {\"start\": start, \"input\": x, \"stop\": stop})\n    return ret\n\nscores_df.apply(compute_softmax, axis=1, meta=object).compute()\nclient.get_events(\"softmax\")\n```"]