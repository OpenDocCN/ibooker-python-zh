- en: Chapter 11\. Using Less RAM
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11章。使用更少的RAM
- en: We rarely think about how much RAM we’re using until we run out of it. If you
    run out while scaling your code, it can become a sudden blocker. Fitting more
    into a machine’s RAM means fewer machines to manage, and it gives you a route
    to planning capacity for larger projects. Knowing why RAM gets eaten up and considering
    more efficient ways to use this scarce resource will help you deal with scaling
    issues. We’ll use the Memory Profiler and IPython Memory Usage tools to measure
    the actual RAM usage, along with some tools that introspect objects to try to
    guess how much RAM they’re using.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很少考虑我们使用了多少 RAM，直到我们用尽它。如果在扩展代码时用尽了RAM，它可能会成为一个突如其来的阻碍。将更多内容适应到机器的RAM中意味着需要管理的机器更少，而且它为你规划更大项目的容量提供了一条途径。知道RAM被耗尽的原因，并考虑更有效地使用这种稀缺资源的方法，将有助于你处理扩展问题。我们将使用
    Memory Profiler 和 IPython 内存使用工具来测量实际的RAM使用量，以及一些内省对象的工具，试图猜测它们使用了多少RAM。
- en: Another route to saving RAM is to use containers that utilize features in your
    data for compression. In this chapter, we’ll look at a trie (ordered tree data
    structures) and a directed acyclic word graph (DAWG) that can compress a 1.2 GB
    `set` of strings down to just 30 MB with little change in performance. A third
    approach is to trade storage for accuracy. For this we’ll look at approximate
    counting and approximate set membership, which use dramatically less RAM than
    their exact counterparts.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 节省RAM的另一种方法是使用利用数据特性进行压缩的容器。在这一章中，我们将研究一种 trie（有序树数据结构）和一个有向无环图（DAWG），它们可以将一个
    1.2GB 的字符串集合压缩到只有 30MB，而性能几乎没有变化。第三种方法是为准确性交换存储空间。为此，我们将研究近似计数和近似集合成员资格，它们比其精确对应物使用的RAM少得多。
- en: A consideration with RAM usage is the notion that “data has mass.” The more
    there is of it, the slower it moves around. If you can be parsimonious in your
    use of RAM, your data will probably get consumed faster, as it’ll move around
    buses faster and more of it will fit into constrained caches. If you need to store
    it in offline storage (e.g., a hard drive or a remote data cluster), it’ll move
    far more slowly to your machine. Try to choose appropriate data structures so
    all your data can fit onto one machine. We’ll use NumExpr to efficiently calculate
    with NumPy and Pandas with fewer data movements than the more direct method, which
    will save us time and make certain larger calculations feasible in a fixed amount
    of RAM.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 关于RAM使用的一个考虑是“数据具有质量”的概念。它越多，移动速度就越慢。如果你在使用RAM时节俭，你的数据可能会被更快地消耗掉，因为它会更快地在总线上移动，并且更多的数据将适应受限的缓存中。如果你需要将它存储在离线存储器中（例如，硬盘或远程数据集群），它将以更慢的速度传输到你的机器上。尽量选择适当的数据结构，以便所有数据都能适应一个机器。我们将使用
    NumExpr 来使用比更直接的方法少得多的数据移动效率地进行NumPy和Pandas计算，这将节省我们时间，并使一些更大的计算在固定RAM量中变得可行。
- en: Counting the amount of RAM used by Python objects is surprisingly tricky. We
    don’t necessarily know how an object is represented behind the scenes, and if
    we ask the operating system for a count of bytes used, it will tell us about the
    total amount allocated to the process. In both cases, we can’t see exactly how
    each individual Python object adds to the total.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 计算Python对象使用的RAM量是令人惊讶地棘手。我们并不一定知道对象在幕后是如何表示的，如果我们向操作系统询问已使用的字节计数，它会告诉我们关于分配给进程的总量。在这两种情况下，我们都无法准确地看到每个单独的Python对象如何增加到总量中。
- en: As some objects and libraries don’t report their full internal allocation of
    bytes (or they wrap external libraries that do not report their allocation at
    all), this has to be a case of best-guessing. The approaches explored in this
    chapter can help us to decide on the best way to represent our data so we use
    less RAM overall.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 由于一些对象和库不会报告它们的完整内部分配字节数（或者它们包装的外部库根本不会报告它们的分配情况），这必须是一个最佳猜测的情况。本章探讨的方法可以帮助我们决定如何以更少的RAM总体上使用最佳的方式来表示我们的数据。
- en: We’ll also look at several lossy methods for storing strings in scikit-learn
    and counts in data structures. This works a little like a JPEG compressed image—we
    lose some information (and we can’t undo the operation to recover it), and we
    gain a lot of compression as a result. By using hashes on strings, we compress
    the time and memory usage for a natural language processing task in scikit-learn,
    and we can count huge numbers of events with only a small amount of RAM.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将看几种在 scikit-learn 中存储字符串和数据结构中的计数的损失方法。这有点像 JPEG 压缩图像——我们会失去一些信息（并且无法撤消操作以恢复它），但作为结果我们获得了很大的压缩。通过对字符串使用哈希，我们在
    scikit-learn 中为自然语言处理任务压缩了时间和内存使用，并且可以用很少的 RAM 计算大量事件。
- en: Objects for Primitives Are Expensive
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 原始数据类型的对象成本高昂
- en: It’s common to work with containers like the `list`, storing hundreds or thousands
    of items. As soon as you store a large number, RAM usage becomes an issue.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 像 `list` 这样的容器通常用于存储数百或数千个项。一旦存储大量数据，RAM 的使用就成为一个问题。
- en: A `list` with 100 million items consumes approximately 760 MB of RAM, *if the
    items are the same object*. If we store 100 million *different* items (e.g., unique
    integers), we can expect to use gigabytes of RAM! Each unique object has a memory
    cost.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 一个包含 1 亿项的 `list` 大约消耗 760 MB RAM，*如果这些项是相同对象*。如果我们存储 1 亿个*不同*的项（例如唯一的整数），我们可以期望使用几
    GB 的 RAM！每个唯一对象都有内存成本。
- en: In [Example 11-1](ch11_split_000.xhtml#lessram-memory-profiler-1e8-same-items),
    we store many `0` integers in a `list`. If you stored 100 million references to
    any object (regardless of how large one instance of that object was), you’d still
    expect to see a memory cost of roughly 760 MB, as the `list` is storing references
    to (not copies of) the object. Refer back to [“Using memory_profiler to Diagnose
    Memory Usage”](ch02.xhtml#memory_profiler) for a reminder of how to use `memory_profiler`;
    here, we load it as a new magic function in IPython using `%load_ext memory_profiler`.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [示例 11-1](ch11_split_000.xhtml#lessram-memory-profiler-1e8-same-items) 中，我们将许多
    `0` 整数存储在一个 `list` 中。如果您将 1 亿个对象的引用存储在列表中（无论这个对象的一个实例有多大），您仍然期望看到大约 760 MB 的内存成本，因为
    `list` 存储的是对对象的引用（而不是副本）。参考 [“使用 memory_profiler 诊断内存使用情况”](ch02.xhtml#memory_profiler)
    来了解如何使用 `memory_profiler`；在这里，我们通过 `%load_ext memory_profiler` 将其加载为 IPython 中的新魔术函数。
- en: Example 11-1\. Measuring memory usage of 100 million of the same integer in
    a list
  id: totrans-11
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-1\. 测量列表中 1 亿个相同整数的内存使用情况
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: For our next example, we’ll start with a fresh shell. As the results of the
    first call to `memit` in [Example 11-2](ch11_split_000.xhtml#lessram-memory-profiler-1e8-different-items)
    reveal, a fresh IPython shell consumes approximately 40 MB of RAM. Next, we can
    create a temporary list of 100 million *unique* numbers. In total, this consumes
    approximately 3.8 GB.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的下一个示例，我们将从一个新的 shell 开始。正如在 [示例 11-2](ch11_split_000.xhtml#lessram-memory-profiler-1e8-different-items)
    中第一次调用 `memit` 的结果显示的那样，一个新的 IPython shell 大约消耗 40 MB RAM。接下来，我们可以创建一个临时的包含 1
    亿个*唯一*数字的列表。总共，这大约消耗了 3.8 GB。
- en: Warning
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Memory can be cached in the running process, so it is always safer to exit and
    restart the Python shell when using `memit` for profiling.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 内存可以在运行进程中缓存，因此在使用 `memit` 进行分析时，退出并重新启动 Python shell 总是更安全的选择。
- en: After the `memit` command finishes, the temporary list is deallocated. The final
    call to `memit` shows that the memory usage drops to its previous level.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '`memit` 命令完成后，临时列表被释放。最后一次调用 `memit` 显示内存使用量降至之前的水平。'
- en: Example 11-2\. Measuring memory usage of 100 million different integers in a
    list
  id: totrans-17
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-2\. 测量列表中 1 亿个不同整数的内存使用情况
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: A subsequent `memit` in [Example 11-3](ch11_split_000.xhtml#lessram-memory-profiler-1e8-different-items2)
    to create a second 100-million-item list consumes approximately 3.8 GB again.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 后续在 [示例 11-3](ch11_split_000.xhtml#lessram-memory-profiler-1e8-different-items2)
    中执行 `memit` 来创建第二个 1 亿项目列表，内存消耗约为 3.8 GB。
- en: Example 11-3\. Measuring memory usage again for 100 million different integers
    in a list
  id: totrans-20
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-3\. 再次测量列表中 1 亿个不同整数的内存使用情况
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Next, we’ll see that we can use the `array` module to store 100 million integers
    far more cheaply.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看到可以使用 `array` 模块更便宜地存储 1 亿个整数。
- en: The array Module Stores Many Primitive Objects Cheaply
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '`array` 模块以便宜的方式存储许多原始对象'
- en: The `array` module efficiently stores primitive types like integers, floats,
    and characters, but *not* complex numbers or classes. It creates a contiguous
    block of RAM to hold the underlying data.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '`array` 模块高效地存储诸如整数、浮点数和字符等基本类型，但*不*包括复数或类。它创建一个连续的 RAM 块来保存底层数据。'
- en: In [Example 11-4](ch11_split_000.xhtml#lessram-array1), we allocate 100 million
    integers (8 bytes each) into a contiguous chunk of memory. In total, approximately
    760 MB is consumed by the process. The difference between this approach and the
    previous list-of-unique-integers approach is `3100MB - 760MB == 2.3GB`. This is
    a huge savings in RAM.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例 11-4](ch11_split_000.xhtml#lessram-array1)中，我们分配了1亿个整数（每个8字节）到一个连续的内存块中。总计，该过程消耗约760
    MB。这种方法与之前的唯一整数列表方法之间的差异为`3100MB - 760MB == 2.3GB`。这在RAM上是巨大的节省。
- en: Example 11-4\. Building an array of 100 million integers with 760 MB of RAM
  id: totrans-26
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-4\. 使用760 MB RAM构建一个包含1亿个整数的数组
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that the unique numbers in the `array` are *not* Python objects; they are
    bytes in the `array`. If we were to dereference any of them, a new Python `int`
    object would be constructed. If you’re going to compute on them, no overall savings
    will occur, but if instead you’re going to pass the array to an external process
    or use only some of the data, you should see a good savings in RAM compared to
    using a `list` of integers.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`array`中的唯一数字不是Python对象；它们是`array`中的字节。如果我们对它们进行解引用，将构造一个新的Python `int`对象。如果你要对它们进行计算，则不会有整体节省，但如果你要将数组传递给外部进程或仅使用部分数据，与使用整数列表相比，RAM的节省将是显著的。
- en: Note
  id: totrans-29
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: If you’re working with a large array or matrix of numbers with Cython and you
    don’t want an external dependency on `numpy`, be aware that you can store your
    data in an `array` and pass it into Cython for processing without any additional
    memory overhead.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用Cython处理大型数组或矩阵，并且不希望依赖于`numpy`，请注意，你可以将数据存储在一个`array`中，并将其传递给Cython进行处理，而无需额外的内存开销。
- en: The `array` module works with a limited set of datatypes with varying precisions
    (see [Example 11-5](ch11_split_000.xhtml#lessram-array2)). Choose the smallest
    precision that you need, so that you allocate just as much RAM as needed and no
    more. Be aware that the byte size is platform-dependent—the sizes here refer to
    a 32-bit platform (it states *minimum* size), whereas we’re running the examples
    on a 64-bit laptop.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '`array`模块与有限的数据类型一起工作，具有不同的精度（参见[示例 11-5](ch11_split_000.xhtml#lessram-array2)）。选择你所需的最小精度，这样你只分配所需的RAM，而不是更多。请注意，字节大小是依赖于平台的——这里的大小是针对32位平台的（它指定了*最小*大小），而我们在64位笔记本电脑上运行示例。'
- en: Example 11-5\. The basic types provided by the `array` module
  id: totrans-32
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-5\. `array`模块提供的基本类型
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'NumPy has arrays that can hold a wider range of datatypes—you have more control
    over the number of bytes per item, and you can use complex numbers and `datetime`
    objects. A `complex128` object takes 16 bytes per item: each item is a pair of
    8-byte floating-point numbers. You can’t store `complex` objects in a Python array,
    but they come for free with `numpy`. If you’d like a refresher on `numpy`, look
    back to [Chapter 6](ch06_split_000.xhtml#matrix_computation).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy拥有可以容纳更广泛数据类型的数组——你可以更好地控制每个项的字节数，可以使用复数和`datetime`对象。`complex128`对象每个项占用16字节：每个项是一对8字节浮点数。你无法在Python数组中存储`complex`对象，但在`numpy`中可以免费使用它们。如果你需要重新熟悉`numpy`，请回顾[第6章](ch06_split_000.xhtml#matrix_computation)。
- en: In [Example 11-6](ch11_split_000.xhtml#lessram-numpy1), you can see an additional
    feature of `numpy` arrays; you can query for the number of items, the size of
    each primitive, and the combined total storage of the underlying block of RAM.
    Note that this doesn’t include the overhead of the Python object (typically, this
    is tiny in comparison to the data you store in the arrays).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例 11-6](ch11_split_000.xhtml#lessram-numpy1)中，你可以看到`numpy`数组的另一个特性；你可以查询项目数、每个原始的大小以及底层RAM块的总存储。请注意，这不包括Python对象的开销（通常来说，与存储在数组中的数据相比，这是微不足道的）。
- en: Tip
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Be wary of lazy allocation with zeros. In the following example the call to
    `zeros` costs “zero” RAM, whilst the call to `ones` costs 1.5 GB. Both calls will
    ultimately cost 1.5 GB, but the call to `zeros` will allocate the RAM only after
    it is used, so the cost is seen later.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 要小心使用零进行惰性分配。在以下示例中，对`zeros`的调用“零”成本的RAM，而对`ones`的调用成本为1.5 GB。这两个调用最终都会消耗1.5
    GB，但是对`zeros`的调用仅在使用后才分配RAM，因此成本稍后才会显现。
- en: Example 11-6\. Storing more complex types in a `numpy` array
  id: totrans-38
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-6\. 在`numpy`数组中存储更复杂的类型
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Using a regular `list` to store many numbers is much less efficient in RAM than
    using an `array` object. More memory allocations have to occur, which each take
    time; calculations also occur on larger objects, which will be less cache friendly,
    and more RAM is used overall, so less RAM is available to other programs.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 使用普通的`list`来存储许多数字在RAM中比使用`array`对象要低效得多。需要进行更多的内存分配，每次分配都需要时间；还会对较大的对象进行计算，这些对象将不太适合缓存，并且总体上使用的RAM更多，因此其他程序可用的RAM更少。
- en: However, if you do any work on the contents of the `array` in Python numex,
    the primitives are likely to be converted into temporary objects, negating their
    benefit. Using them as a data store when communicating with other processes is
    a great use case for the `array`.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你在Python中对`array`的内容进行任何操作，原语很可能会被转换为临时对象，从而抵消其效益。在与其他进程通信时，将它们用作数据存储的一个很好的用例是`array`。
- en: '`numpy` arrays are almost certainly a better choice if you are doing anything
    heavily numeric, as you get more datatype options and many specialized and fast
    functions. You might choose to avoid `numpy` if you want fewer dependencies for
    your project, though Cython works equally well with `array` and `numpy` arrays;
    Numba works with `numpy` arrays only.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在做大量数值计算，那么`numpy`数组几乎肯定是更好的选择，因为你可以获得更多的数据类型选项和许多专门的快速函数。如果你希望项目依赖较少，可能会选择避免使用`numpy`，尽管Cython与`array`和`numpy`数组同样有效；Numba仅与`numpy`数组配合使用。
- en: Python provides a few other tools to understand memory usage, as we’ll see in
    the following section.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Python还提供了一些其他工具来理解内存使用，我们将在以下部分中看到。
- en: Using Less RAM in NumPy with NumExpr
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用NumPy和NumExpr节省RAM
- en: 'Large vectorized expressions in NumPy (which also happen behind the scenes
    in Pandas) can create intermediate large arrays during complex operations. These
    occur invisibly and may draw attention to themselves only when an out-of-memory
    error occurs. These calculations can also be slow, as large vectors will not be
    cache friendly—a cache can be megabytes or smaller, and large vectors of hundreds
    of megabytes or gigabytes of data will stop the cache from being used effectively.
    NumExpr is a tool that both speeds up and reduces the size of intermediate operations;
    we introduced it in [“numexpr: Making In-Place Operations Faster and Easier”](ch06_split_001.xhtml#matrix_vector_numexpr).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在NumPy中（这也会在Pandas背后发生）的大型向量化表达式可能会在复杂操作期间创建中间大型数组。这些操作通常是看不见的，只有在发生内存不足错误时才会引起注意。这些计算也可能很慢，因为大型向量不会很好地利用缓存——缓存可能是兆字节或更小，而数百兆字节或几十亿字节的大型向量数据将阻止缓存的有效使用。NumExpr是一个既加速又减少中间操作大小的工具；我们在[“numexpr：使原地操作更快更容易”](ch06_split_001.xhtml#matrix_vector_numexpr)中介绍了它。
- en: We’ve also introduced the Memory Profiler before, in [“Using memory_profiler
    to Diagnose Memory Usage”](ch02.xhtml#memory_profiler). Here, we build on it with
    the [IPython Memory Usage tool](https://oreil.ly/i9Vc3), which reports line-by-line
    memory changes inside the IPython shell or in a Jupyter Notebook. Let’s look at
    how these can be used to check that NumExpr is generating a result more efficiently.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前在[“使用memory_profiler诊断内存使用”](ch02.xhtml#memory_profiler)中介绍过Memory Profiler。在这里，我们通过[IPython内存使用工具](https://oreil.ly/i9Vc3)进一步扩展，它会报告IPython
    shell或Jupyter Notebook中逐行的内存变化。让我们看看如何使用这些工具来检查NumExpr是否更有效地生成结果。
- en: Tip
  id: totrans-47
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Remember to install the optional NumExpr when using Pandas. If NumExpr is installed
    in Pandas, calls to `eval` will run more quickly—but note that Pandas does not
    tell you if you *haven’t* installed NumExpr.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Pandas时，请记得安装可选的NumExpr。如果在Pandas中安装了NumExpr，调用`eval`会更快，但请注意，Pandas不会告诉您如果*未*安装NumExpr。
- en: 'We’ll use the cross entropy formula to calculate the error for a machine learning
    classification challenge. *Cross entropy* (or *Log Loss*) is a common metric for
    classification challenges; it penalizes large errors significantly more than small
    errors. Each row in a machine learning problem needs to be scored during the training
    and prediction phases:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用交叉熵公式来计算机器学习分类挑战的误差。*交叉熵*（或*对数损失*）是分类挑战的常见度量标准；它对大误差的惩罚要比小误差显著。在训练和预测阶段，需要为机器学习问题中的每一行打分：
- en: <math alttext="minus l o g upper P left-parenthesis y t vertical-bar y p right-parenthesis
    equals minus left-parenthesis y t l o g left-parenthesis y p right-parenthesis
    plus left-parenthesis 1 minus y t right-parenthesis l o g left-parenthesis 1 minus
    y p right-parenthesis right-parenthesis" display="block"><mrow><mo>-</mo> <mi>l</mi>
    <mi>o</mi> <mi>g</mi> <mi>P</mi> <mo>(</mo> <mi>y</mi> <mi>t</mi> <mo>|</mo> <mi>y</mi>
    <mi>p</mi> <mo>)</mo> <mo>=</mo> <mo>-</mo> <mo>(</mo> <mi>y</mi> <mi>t</mi> <mi>l</mi>
    <mi>o</mi> <mi>g</mi> <mo>(</mo> <mi>y</mi> <mi>p</mi> <mo>)</mo> <mo>+</mo> <mo>(</mo>
    <mn>1</mn> <mo>-</mo> <mi>y</mi> <mi>t</mi> <mo>)</mo> <mi>l</mi> <mi>o</mi> <mi>g</mi>
    <mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>y</mi> <mi>p</mi> <mo>)</mo> <mo>)</mo></mrow></math>
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="minus l o g upper P left-parenthesis y t vertical-bar y p right-parenthesis
    equals minus left-parenthesis y t l o g left-parenthesis y p right-parenthesis
    plus left-parenthesis 1 minus y t right-parenthesis l o g left-parenthesis 1 minus
    y p right-parenthesis right-parenthesis" display="block"><mrow><mo>-</mo> <mi>l</mi>
    <mi>o</mi> <mi>g</mi> <mi>P</mi> <mo>(</mo> <mi>y</mi> <mi>t</mi> <mo>|</mo> <mi>y</mi>
    <mi>p</mi> <mo>)</mo> <mo>=</mo> <mo>-</mo> <mo>(</mo> <mi>y</mi> <mi>t</mi> <mi>l</mi>
    <mi>o</mi> <mi>g</mi> <mo>(</mo> <mi>y</mi> <mi>p</mi> <mo>)</mo> <mo>+</mo> <mo>(</mo>
    <mn>1</mn> <mo>-</mo> <mi>y</mi> <mi>t</mi> <mo>)</mo> <mi>l</mi> <mi>o</mi> <mi>g</mi>
    <mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>y</mi> <mi>p</mi> <mo>)</mo> <mo>)</mo></mrow></math>
- en: We’ll use random numbers in the range [0, 1] here to simulate the results of
    a machine learning system from a package like scikit-learn or TensorFlow. [Figure 11-1](ch11_split_000.xhtml#FIG-cross-entropy)
    shows the natural logarithm for the range [0, 1] on the right, and on the left
    it shows the result of calculating the cross entropy for any probability if the
    target is either 0 or 1.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里使用范围为[0, 1]的随机数来模拟类似于scikit-learn或TensorFlow等包的机器学习系统的结果。[图 11-1](ch11_split_000.xhtml#FIG-cross-entropy)展示了右侧范围为[0,
    1]的自然对数，左侧展示了计算交叉熵的结果，当目标值为0或1时。
- en: If the target `yt` is 1, the first half of the formula is active and the second
    half goes to zero. If the target is 0, the second half of the formula is active
    and the first part goes to zero. This result is calculated for every row of data
    that needs to be scored and often for many iterations of the machine learning
    algorithm.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果目标`yt`为1，则公式的前半部分生效，而后半部分为零。如果目标为0，则公式的后半部分生效，而第一部分为零。这个结果是针对需要评分的每一行数据计算的，并且通常需要进行多次机器学习算法的迭代。
- en: '![hpp2 1101](Images/hpp2_1101.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![hpp2 1101](Images/hpp2_1101.png)'
- en: Figure 11-1\. Cross entropy for `yt` (the “truth”) with values 0 and 1
  id: totrans-54
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-1\. `yt`（“真实值”）的交叉熵，其取值为0和1
- en: In [Example 11-7](ch11_split_000.xhtml#lessram-numexpr), we generate 200 million
    random numbers in the range [0, 1] as `yp`. `yt` is the desired truth—in this
    case, an array of 1s. In a real application, we’d see `yp` generated by a machine
    learning algorithm, and `yt` would be the ground truth mixing 0s and 1s for the
    target we’d be learning provided by the machine learning researcher.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例 11-7](ch11_split_000.xhtml#lessram-numexpr)中，我们生成了2亿个位于[0, 1]范围内的随机数作为`yp`。`yt`是期望的真值
    —— 在这种情况下是由1组成的数组。在实际应用中，`yp`由机器学习算法生成，而`yt`则是由机器学习研究人员提供的混合了0和1的目标真值。
- en: Example 11-7\. The hidden cost of temporaries with large NumPy arrays
  id: totrans-56
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-7\. 大型NumPy数组临时变量的隐藏成本
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Both `yp` and `yt` take 1.5 GB each, bringing the total RAM usage to just over
    3.1 GB. The `answer` vector has the same dimension as the inputs and thus adds
    a further 1.5 GB. Note that the calculation peaks at 4.5 GB over the current RAM
    usage, so while we end with a 4.6 GB result, we had over 9 GB allocated during
    the calculation. The cross entropy calculation creates several temporaries (notably
    `1 – yt`, `np.log(1 – yp)`, and their multiplication). If you had an 8 GB machine,
    you’d have failed to calculate this result because of memory exhaustion.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`yp`和`yt`都各自占用了1.5 GB内存，使得总内存使用量略高于3.1 GB。`answer`向量与输入具有相同的维度，因此额外增加了1.5 GB。请注意，计算在当前RAM使用量上峰值达到了4.5
    GB，因此虽然我们最终得到了4.6 GB的结果，但在计算过程中分配了超过9 GB的内存。交叉熵计算创建了几个临时变量（特别是`1 – yt`、`np.log(1
    – yp)`及其乘积）。如果你使用的是8 GB的机器，由于内存耗尽的原因，可能无法计算出这个结果。'
- en: 'In [Example 11-8](ch11_split_000.xhtml#lessram-numexpr2), we see the same expression
    placed as a string inside `numexpr.evaluate`. It peaks at 0 GB above the current
    usage—it doesn’t need any additional RAM in this case. Significantly, it also
    calculates much more quickly: the previous direct vector calculation in `In[6]`
    took 18 seconds, while here with NumExpr the same calculation takes 2.6 seconds.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例 11-8](ch11_split_000.xhtml#lessram-numexpr2)，我们看到相同的表达式作为字符串放置在`numexpr.evaluate`内部。它在当前使用量之上峰值为0
    GB —— 在这种情况下不需要任何额外的RAM。值得注意的是，它还计算得更快：之前直接向量计算在`In[6]`中花费了18秒，而在这里使用NumExpr进行相同计算只需要2.6秒。
- en: NumExpr breaks the long vectors into shorter, cache-friendly chunks and processes
    each in series, so local chunks of results are calculated in a cache-friendly
    way. This explains both the requirement for no extra RAM and the increased speed.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: NumExpr将长向量分解为更短、友好缓存的块，并依次处理每个块，因此可以以友好缓存的方式计算本地块的结果。这解释了不需要额外RAM的需求以及增加的速度。
- en: Example 11-8\. NumExpr breaks the vectorized calculations into cache-efficient
    chunks
  id: totrans-61
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-8\. NumExpr将向量化计算分解为高效利用缓存的块
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We can see a similar benefit in Pandas in [Example 11-9](ch11_split_000.xhtml#lessram-numexpr3).
    We construct a DataFrame with the same items as in the preceding example and invoke
    NumExpr by using `df.eval`. The Pandas machinery has to unpack the DataFrame for
    NumExpr, and more RAM is used overall; behind the scenes, NumExpr is still calculating
    the result in a cache-friendly manner. Note that here NumExpr was installed in
    addition to Pandas.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在 Pandas 中看到类似的好处，[示例 11-9](ch11_split_000.xhtml#lessram-numexpr3) 中。我们构造一个与前面示例相同的
    DataFrame，并使用 `df.eval` 调用 NumExpr。Pandas 机制必须为 NumExpr 解包 DataFrame，并且总体使用更多
    RAM；在幕后，NumExpr 仍以一种缓存友好的方式计算结果。请注意，在这里，NumExpr 除了 Pandas 外还安装了。
- en: Example 11-9\. Pandas `eval` uses NumExpr if it is available
  id: totrans-64
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-9\. Pandas 的 `eval` 如果可用将使用 NumExpr
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Contrast the preceding example with [Example 11-10](ch11_split_000.xhtml#lessram-numexpr4),
    where NumExpr has *not* been installed. The call to `df.eval` falls back on the
    Python interpreter—the same result is calculated but with a 34-second execution
    time (compared to 5.2 seconds before) and a much larger peak memory usage. You
    can test whether NumExpr is installed with `import numexpr`—if this fails, you’ll
    want to install it.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 与前面的示例对比，[示例 11-10](ch11_split_000.xhtml#lessram-numexpr4) 中没有安装 NumExpr。调用
    `df.eval` 回退到 Python 解释器——结果相同，但执行时间为 34 秒（之前为 5.2 秒），内存使用峰值更高。您可以使用 `import numexpr`
    测试是否安装了 NumExpr——如果失败，您将需要安装它。
- en: Example 11-10\. Beware that Pandas without NumExpr makes a slow and costly call
    to `eval`!
  id: totrans-67
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-10\. 注意，没有安装 NumExpr 的 Pandas 将对 `eval` 进行缓慢和昂贵的调用！
- en: '[PRE9]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Complex vector operations on large arrays will run faster if you can use NumExpr.
    Pandas will not warn you that NumExpr hasn’t been installed, so we recommend that
    you add it as part of your setup if you use `eval`. The IPython Memory Usage tool
    will help you to diagnose where your RAM is being spent if you have large arrays
    to process; this can help you fit more into RAM on your current machine so you
    don’t have to start dividing your data and introducing greater engineering effort.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在大数组上进行复杂的向量操作将更快地运行，如果可以使用 NumExpr 的话。Pandas 不会警告您未安装 NumExpr，因此建议在设置中添加它，如果您使用
    `eval`。IPython Memory Usage 工具将帮助您诊断您的大数组消耗了多少 RAM；这可以帮助您在当前机器上更多地放入 RAM，以免您不得不开始分割数据并引入更大的工程工作。
- en: Understanding the RAM Used in a Collection
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解集合中使用的 RAM
- en: You may wonder if you can ask Python about the RAM that’s used by each object.
    Python’s `sys.getsizeof(obj)` call will tell us *something* about the memory used
    by an object (most but not all objects provide this). If you haven’t seen it before,
    be warned that it won’t give you the answer you’d expect for a container!
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道是否可以询问 Python 每个对象使用的 RAM。Python 的 `sys.getsizeof(obj)` 调用将告诉我们关于对象内存使用的*一些*信息（大多数但不是所有对象都提供此信息）。如果你以前没见过它，要注意它不会为容器给出你期望的答案！
- en: 'Let’s start by looking at some primitive types. An `int` in Python is a variable-sized
    object of arbitrary size, well above the range of an 8-byte C integer. The basic
    object costs 24 bytes in Python 3.7 when initialized with 0\. More bytes are added
    as you count to larger numbers:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先看一些基本类型。在 Python 中，`int` 是一个大小可变的对象，大小任意，当用 0 初始化时，基本对象在 Python 3.7 中的成本是
    24 字节。随着数值变大，会添加更多字节：
- en: '[PRE10]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Behind the scenes, sets of 4 bytes are added each time the size of the number
    you’re counting steps above the previous limit. This affects only the memory usage;
    you don’t see any difference externally.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，每次数到前一个限制之上的数字大小时，会增加 4 字节的集合。这仅影响内存使用；外部看不到任何差异。
- en: 'We can do the same check for byte strings. An empty byte sequence costs 33
    bytes, and each additional character adds 1 byte to the cost:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对字节字符串执行相同的检查。空字节序列占用 33 字节，每个额外字符增加 1 字节的成本：
- en: '[PRE11]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'When we use a list, we see different behavior. `getsizeof` isn’t counting the
    cost of the contents of the list—just the cost of the list itself. An empty list
    costs 64 bytes, and each item in the list takes another 8 bytes on a 64-bit laptop:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用列表时，我们会看到不同的行为。`getsizeof` 并不计算列表内容的成本，只计算列表本身的成本。空列表占用 64 字节，在 64 位笔记本上，列表中的每个项目另外占用
    8 字节：
- en: '[PRE12]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This is more obvious if we use byte strings—we’d expect to see much larger
    costs than `getsizeof` is reporting:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用字节字符串，我们会看到比 `getsizeof` 报告的成本要大得多：
- en: '[PRE13]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '`getsizeof` reports only some of the cost, and often just for the parent object.
    As noted previously, it also isn’t always implemented, so it can have limited
    usefulness.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`getsizeof`仅报告了部分成本，通常只报告父对象的成本。正如前面所述，它也并非总是实现的，因此其实用性有限。'
- en: A better tool is `asizeof` in [`pympler`](https://oreil.ly/HGCj5). This will
    walk a container’s hierarchy and make a best guess about the size of each object
    it finds, adding the sizes to a total. Be warned that it is quite slow.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在[`pympler`](https://oreil.ly/HGCj5)中，更好的工具是`asizeof`。它将遍历容器的层次结构，并对找到的每个对象的大小进行最佳猜测，将大小添加到总数中。请注意，它运行速度较慢。
- en: In addition to relying on guesses and assumptions, `asizeof` also cannot count
    memory allocated behind the scenes (such as a module that wraps a C library may
    not report the bytes allocated in the C library). It is best to use this as a
    guide. We prefer to use `memit`, as it gives us an accurate count of memory usage
    on the machine in question.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 除了依赖猜测和假设外，`asizeof`也无法计算幕后分配的内存（例如，包装C库的模块可能不会报告C库中分配的字节）。最好将其用作指南。我们更喜欢使用`memit`，因为它能准确地计算所涉及机器的内存使用量。
- en: 'We can check the estimate it makes for a large list—here we’ll use 10 million
    integers:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以检查它对大列表的估计——这里我们将使用1000万个整数：
- en: '[PRE14]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We can validate this estimate by using `memit` to see how the process grew.
    Both reports are approximate—`memit` takes snapshots of the RAM usage reported
    by the operating system while the statement is executing, and `asizeof` asks the
    objects about their size (which may not be reported correctly). We can conclude
    that 10 million integers in a list cost between 320 and 400 MB of RAM.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用`memit`来验证这一估算，看看这个过程是如何增长的。两个报告都是大致的——`memit`在执行语句时获取操作系统报告的RAM使用快照，而`asizeof`则询问对象的大小（可能报告不准确）。我们可以得出结论，一个包含1000万个整数的列表大约需要320到400
    MB的RAM。
- en: Generally, the `asizeof` process is slower than using `memit`, but `asizeof`
    can be useful when you’re analyzing small objects. `memit` is probably more useful
    for real-world applications, as the actual memory usage of the process is measured
    rather than inferred.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，`asizeof`过程比使用`memit`慢，但在分析小对象时，`asizeof`可能很有用。对于实际应用程序而言，`memit`可能更有用，因为它测量了进程的实际内存使用情况，而不是推测。
- en: Bytes Versus Unicode
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 字节与Unicode
- en: One of the (many!) advantages of Python 3.*x* over Python 2.*x* is the switch
    to Unicode-by-default. Previously, we had a mix of single-byte strings and multibyte
    Unicode objects, which could cause a headache during data import and export. In
    Python 3.*x*, all strings are Unicode by default, and if you want to deal in bytes,
    you’ll explicitly create a `byte` sequence.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Python 3.*x* 相对于 Python 2.*x* 的（许多！）优势之一是默认使用Unicode。以前，我们有单字节字符串和多字节Unicode对象混合，这在数据导入和导出过程中可能会带来麻烦。在Python
    3.*x*中，所有字符串默认都是Unicode，如果需要操作字节，则必须显式创建字节序列。
- en: Unicode objects have more efficient RAM usage in Python 3.7 than in Python 2.*x*.
    In [Example 11-11](ch11_split_000.xhtml#lessram-memory-profiler-stringvsunicode1),
    we can see a 100-million-character sequence being built as a collection of bytes
    and as a Unicode object. The Unicode variant for common characters (here we’re
    assuming UTF 8 as the system’s default encoding) costs the same—a single-byte
    implementation is used for these common characters.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Python 3.7中的Unicode对象比Python 2.*x*中的RAM使用效率更高。在[Example 11-11](ch11_split_000.xhtml#lessram-memory-profiler-stringvsunicode1)中，我们可以看到一个包含1亿字符序列，一部分作为字节集合，一部分作为Unicode对象。对于常见字符，Unicode变体（假设系统的默认编码是UTF
    8）成本相同——这些常见字符使用单字节实现。
- en: Example 11-11\. Unicode objects can be as cheap as bytes in Python 3.x
  id: totrans-91
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例11-11。在Python 3.x中，Unicode对象可以和字节一样便宜。
- en: '[PRE15]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The sigma character (Σ) is more expensive—it is represented in UTF 8 as 2 bytes.
    We gained the flexible Unicode representation from Python 3.3 thanks to [PEP 393](https://oreil.ly/_wNrP).
    It works by observing the range of characters in the string and using a smaller
    number of bytes to represent the lower-order characters, if possible.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Sigma字符（Σ）更昂贵——在UTF 8中表示为2字节。我们通过[PEP 393](https://oreil.ly/_wNrP)从Python 3.3中获得了灵活的Unicode表示。它通过观察字符串中字符的范围，并在可能的情况下使用更少的字节来表示低序字符来工作。
- en: The UTF-8 encoding of a Unicode object uses 1 byte per ASCII character and more
    bytes for less frequently seen characters. If you’re not sure about Unicode encodings
    versus Unicode objects, go and watch [Net Batchelder’s “Pragmatic Unicode, or,
    How Do I Stop the Pain?”](https://oreil.ly/udL3A).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Unicode对象的UTF-8编码每个ASCII字符使用1个字节，对于不经常见到的字符使用更多字节。如果您对Unicode编码与Unicode对象不确定，请观看[Net
    Batchelder的“Pragmatic Unicode, or, How Do I Stop The Pain?”](https://oreil.ly/udL3A)。
- en: Efficiently Storing Lots of Text in RAM
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高效地在RAM中存储大量文本
- en: A common problem with text is that it occupies a lot of RAM—but if we want to
    test if we have seen strings before or count their frequency, having them in RAM
    is more convenient than paging them to and from a disk. Storing the strings naively
    is expensive, but tries and directed acyclic word graphs (DAWGs) can be used to
    compress their representation and still allow fast operations.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 文本的一个常见问题是它占用大量的RAM——但如果我们想要测试我们以前是否看到过字符串或计算它们的频率，将它们存储在RAM中比从磁盘中分页更方便。简单地存储字符串是昂贵的，但tries和有向无环单词图（DAWG）可以用于压缩它们的表示，并仍然允许快速操作。
- en: These more advanced algorithms can save you a significant amount of RAM, which
    means that you might not need to expand to more servers. For production systems,
    the savings can be huge. In this section we’ll look at compressing a `set` of
    strings costing 1.2 GB down to 30 MB using a trie, with only a small change in
    performance.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这些更先进的算法可以节省大量RAM，这意味着您可能不需要扩展到更多的服务器。对于生产系统来说，这样的节省是巨大的。在本节中，我们将探讨如何使用trie将占用1.2
    GB的字符串`set`压缩到30 MB，性能几乎没有变化。
- en: For this example, we’ll use a text set built from a partial dump of Wikipedia.
    This set contains 11 million unique tokens from a portion of the English Wikipedia
    and takes up 120 MB on disk.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们将使用从维基百科的部分转储构建的文本集。这个集合包含了来自英文维基百科的1100万个唯一令牌，占据了磁盘上的120 MB。
- en: 'The tokens are split on whitespace from their original articles; they have
    variable length and contain Unicode characters and numbers. They look like this:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 令牌按照它们原始文章中的空白分隔开来；它们长度不固定，包含Unicode字符和数字。它们看起来像这样：
- en: '[PRE16]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We’ll use this text sample to test how quickly we can build a data structure
    holding one instance of each unique word, and then we’ll see how quickly we can
    query for a known word (we’ll use the uncommon “Zwiebel,” from the painter Alfred
    Zwiebel). This lets us ask, “Have we seen Zwiebel before?” Token lookup is a common
    problem, and being able to do it quickly is important.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这个文本样本来测试我们可以多快地构建一个包含每个唯一单词实例的数据结构，然后我们将看看我们可以多快地查询一个已知的单词（我们将使用不常见的“Zwiebel”，出自画家Alfred
    Zwiebel）。这让我们可以问：“我们以前见过Zwiebel吗？”令牌查找是一个常见的问题，能够快速地执行这些操作非常重要。
- en: Note
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: When you try these containers on your own problems, be aware that you will probably
    see different behaviors. Each container builds its internal structures in different
    ways; passing in different types of token is likely to affect the build time of
    the structure, and different lengths of token will affect the query time. Always
    test in a methodical way.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 当您在自己的问题上尝试这些容器时，请注意您可能会看到不同的行为。每个容器以不同的方式构建其内部结构；传递不同类型的令牌可能会影响结构的构建时间，而令牌的不同长度会影响查询时间。请始终以一种系统的方式进行测试。
- en: Trying These Approaches on 11 Million Tokens
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在1100万个令牌上尝试这些方法
- en: '[Figure 11-2](ch11_split_000.xhtml#FIG-less-ram-8mil_tokens) shows the 11-million-token
    text file (120 MB raw data) stored using a number of containers that we’ll discuss
    in this section. The x-axis shows RAM usage for each container, the y-axis tracks
    the query time, and the size of each point relates to the time taken to build
    the structure (larger means it took longer).'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[Figure 11-2](ch11_split_000.xhtml#FIG-less-ram-8mil_tokens)展示了存储在一些容器中的1100万个令牌文本文件（120
    MB原始数据）。x轴显示了每个容器的RAM使用情况，y轴跟踪查询时间，并且每个点的大小与构建结构所花费的时间成比例（更大表示花费的时间更长）。'
- en: As we can see in this diagram, the `set` and `list` examples use a lot of RAM;
    the `list` example is both large *and* slow! The Marisa trie example is the most
    RAM-efficient for this dataset, while the DAWG runs twice as fast for a relatively
    small increase in RAM usage.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，`set`和`list`示例使用了大量RAM；`list`示例既大又*慢*！对于这个数据集来说，Marisa trie示例在内存使用效率上是最高的，而DAWG运行速度是RAM使用量相对较小的两倍。
- en: '![DAWG and Tries versus built-in containers for 11 million tokens](Images/hpp2_1102.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![DAWG和tries与1100万令牌的内置容器的比较](Images/hpp2_1102.png)'
- en: Figure 11-2\. DAWG and tries versus built-in containers
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-2\. DAWG和tries与内置容器的比较
- en: The figure doesn’t show the lookup time for the naive `list` without sort approach,
    which we’ll introduce shortly, as it takes far too long. Do be aware that you
    must test your problem with a variety of containers—each offers different trade-offs,
    such as construction time and API flexibility.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图中没有显示使用未排序方法的`列表`的查找时间，我们将很快介绍，因为它太耗时了。请注意，您必须使用各种容器测试您的问题——每种容器都提供不同的权衡，如构建时间和
    API 灵活性。
- en: Next, we’ll build up a process to test the behavior of each container.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将建立一个过程来测试每个容器的行为。
- en: list
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 列表
- en: Let’s start with the simplest approach. We’ll load our tokens into a `list`
    and then query it using an `O(n)` linear search. You can’t do this on the large
    example that we’ve already mentioned—the search takes far too long—so we’ll demonstrate
    the technique with a much smaller (500,000 tokens) example.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从最简单的方法开始。我们将我们的标记加载到一个`列表`中，然后使用`O(n)`线性搜索进行查询。你不能在我们已经提到的大例子上这样做——搜索时间太长了——所以我们将用一个小得多的例子（50
    万标记）来演示这种技术。
- en: 'In each of the following examples, we use a generator, `text_example.readers`,
    that extracts one Unicode token at a time from the input file. This means that
    the read process uses only a tiny amount of RAM:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下每个示例中，我们使用一个生成器`text_example.readers`，它从输入文件中一次提取一个 Unicode 标记。这意味着读取过程只使用了极少量的
    RAM：
- en: '[PRE17]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We’re interested in how quickly we can query this `list`. Ideally, we want
    to find a container that will store our text and allow us to query it and modify
    it without penalty. To query it, we look for a known word a number of times by
    using `timeit`:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对能够多快地查询这个`列表`很感兴趣。理想情况下，我们希望找到一个可以存储我们的文本并允许我们进行查询和修改而没有惩罚的容器。为了查询它，我们使用`timeit`多次查找一个已知的单词：
- en: '[PRE18]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Our test script reports that approximately 34 MB was used to store the original
    5 MB file as a list, and that the aggregate lookup time was 53 seconds:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的测试脚本报告显示，将原始的 5 MB 文件存储为一个列表大约使用了 34 MB，聚合查找时间为 53 秒：
- en: '[PRE19]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Storing text in an unsorted `list` is obviously a poor idea; the `O(n)` lookup
    time is expensive, as is the memory usage. This is the worst of all worlds! If
    we tried this method on the following larger dataset, we’d expect an aggregate
    lookup time of 25 minutes rather than a fraction of a second for the methods we
    discuss.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，将文本存储在未排序的`列表`中是一个糟糕的主意；`O(n)`的查找时间代价高昂，内存使用也是如此。这是所有世界中最糟糕的情况！如果我们在以下更大的数据集上尝试这种方法，我们期望的聚合查找时间将是
    25 分钟，而不是我们讨论的方法的一小部分秒数。
- en: We can improve the lookup time by sorting the `list` and using a binary search
    via the [`bisect` module](https://oreil.ly/Uk6ry); this gives us a sensible lower
    bound for future queries. In [Example 11-12](ch11_split_000.xhtml#less_ram_list_sorted_code),
    we time how long it takes to `sort` the `list`. Here, we switch to the larger
    11-million-token set.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过对`列表`进行排序并使用[`bisect`模块](https://oreil.ly/Uk6ry)进行二分查找来改善查找时间；这为未来的查询提供了一个合理的下限。在
    [示例 11-12](ch11_split_000.xhtml#less_ram_list_sorted_code) 中，我们测量了对`列表`进行`排序`所需的时间。在这里，我们转向更大的
    1100 万标记集。
- en: Example 11-12\. Timing the `sort` operation to prepare for using `bisect`
  id: totrans-121
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-12\. 对`排序`操作计时，为使用`bisect`做准备
- en: '[PRE20]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, we do the same lookup as before, but with the addition of the `index`
    method, which uses `bisect`:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们进行与之前相同的查找，但增加了使用`bisect`的`index`方法：
- en: '[PRE21]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In [Example 11-13](ch11_split_000.xhtml#less_ram_bisect_list_output), we see
    that the RAM usage is much larger than before, as we’re loading significantly
    more data. The sort takes a further 0.6 seconds, and the cumulative lookup time
    is 0.01 seconds.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [示例 11-13](ch11_split_000.xhtml#less_ram_bisect_list_output) 中，我们看到 RAM 使用量比以前大得多，因为我们加载了更多的数据。排序需要额外的
    0.6 秒，累积查找时间为 0.01 秒。
- en: Example 11-13\. Timings for using `bisect` on a sorted `list`
  id: totrans-126
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-13\. 在排序`列表`上使用`bisect`的时间
- en: '[PRE22]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We now have a sensible baseline for timing string lookups: RAM usage must get
    better than 871 MB, and the total lookup time should be better than 0.01 seconds.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们对字符串查找定时有了一个合理的基线：RAM 使用必须优于 871 MB，并且总查找时间应优于 0.01 秒。
- en: set
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集合
- en: Using the built-in `set` might seem to be the most obvious way to tackle our
    task. In [Example 11-14](ch11_split_000.xhtml#less_ram_set_code), the `set` stores
    each string in a hashed structure (see [Chapter 4](ch04.xhtml#section-dictionary-sets)
    if you need a refresher). It is quick to check for membership, but each string
    must be stored separately, which is expensive on RAM.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 使用内置的`set`似乎是解决我们任务的最明显方式。在[示例 11-14](ch11_split_000.xhtml#less_ram_set_code)中，`set`使用哈希结构存储每个字符串（如果需要复习，请参阅[第四章](ch04.xhtml#section-dictionary-sets)）。检查成员资格很快，但每个字符串必须单独存储，这在RAM上很昂贵。
- en: Example 11-14\. Using a `set` to store the data
  id: totrans-131
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-14\. 使用`set`存储数据
- en: '[PRE23]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: As we can see in [Example 11-15](ch11_split_000.xhtml#running_the_set_example),
    the `set` uses more RAM than the `list` by a further 250 MB; however, it gives
    us a very fast lookup time without requiring an additional `index` function or
    an intermediate sorting operation.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在[示例 11-15](ch11_split_000.xhtml#running_the_set_example)中所见，`set`比`list`使用更多的RAM，额外多了250
    MB；然而，它提供了非常快的查找时间，而无需额外的`index`函数或中间排序操作。
- en: Example 11-15\. Running the `set` example
  id: totrans-134
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-15\. 运行`set`示例
- en: '[PRE24]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: If RAM isn’t at a premium, this might be the most sensible first approach.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果RAM不是问题，这可能是最明智的第一种方法。
- en: We have now lost the *ordering* of the original data, though. If that’s important
    to you, note that you could store the strings as keys in a dictionary, with each
    value being an index connected to the original read order. This way, you could
    ask the dictionary if the key is present and for its index.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经丢失了原始数据的*顺序*。如果这对您很重要，请注意，您可以将字符串存储为字典中的键，每个值都是与原始读取顺序相关联的索引。这样，您可以询问字典是否存在该键及其索引。
- en: More efficient tree structures
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更有效的树结构
- en: Let’s introduce a set of algorithms that use RAM more efficiently to represent
    our strings.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们介绍一组算法，它们更有效地使用RAM来表示我们的字符串。
- en: '[Figure 11-3](ch11_split_000.xhtml#FIG-less-ram-trie-dawg-picture) from [Wikimedia
    Commons](http://commons.wikimedia.org) shows the difference in representation
    of four words—“tap”, “taps”, “top”, and “tops”—between a trie and a DAWG.^([1](ch11_split_001.xhtml#idm46122400697944))
    With a `list` or a `set`, each of these words would be stored as a separate string.
    Both the DAWG and the trie share parts of the strings, so that less RAM is used.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11-3](ch11_split_000.xhtml#FIG-less-ram-trie-dawg-picture)来自[维基共享资源](http://commons.wikimedia.org)，展示了“tap”、“taps”、“top”和“tops”四个单词在trie和DAWG之间的表示差异。^([1](ch11_split_001.xhtml#idm46122400697944))使用`list`或`set`，每个单词都将存储为单独的字符串。DAWG和trie都共享字符串的部分，因此使用的RAM更少。'
- en: The main difference between these is that a trie shares just common prefixes,
    while a DAWG shares common prefixes and suffixes. In languages (like English)
    that have many common word prefixes and suffixes, this can save a lot of repetition.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这些之间的主要区别在于trie仅共享公共前缀，而DAWG共享公共前缀和后缀。在具有许多常见单词前缀和后缀的语言（如英语）中，这可以节省大量重复。
- en: Exact memory behavior will depend on your data’s structure. Typically, a DAWG
    cannot assign a value to a key because of the multiple paths from the start to
    the end of the string, but the version shown here can accept a value mapping.
    Tries can also accept a value mapping. Some structures have to be constructed
    in a pass at the start, and others can be updated at any time.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 精确的内存行为将取决于您数据的结构。通常，由于字符串从开头到结尾有多条路径，DAWG不能为键分配值，但这里显示的版本可以接受值映射。Trie也可以接受值映射。某些结构必须在开始时进行构建，而其他结构可以随时更新。
- en: A big strength of some of these structures is that they provide a *common prefix
    search*; that is, you can ask for all words that share the prefix you provide.
    With our list of four words, the result when searching for “ta” would be “tap”
    and “taps.” Furthermore, since these are discovered through the graph structure,
    the retrieval of these results is very fast. If you’re working with DNA, for example,
    compressing millions of short strings by using a trie can be an efficient way
    to reduce RAM usage.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结构的一个重要优势是它们提供了*公共前缀搜索*；也就是说，您可以请求所有具有您提供的前缀的单词。对于我们的四个单词列表，搜索“ta”将得到“tap”和“taps”的结果。此外，由于这些是通过图结构发现的，检索这些结果非常快速。例如，如果您处理的是DNA，使用trie来压缩数百万个短字符串可以有效减少RAM使用量。
- en: '![DAWG and Trie data structures](Images/hpp2_1103.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![DAWG和Trie数据结构](Images/hpp2_1103.png)'
- en: Figure 11-3\. Trie and DAWG structures (image by [Chkno](https://oreil.ly/w71ZI)
    [CC BY-SA 3.0])
  id: totrans-145
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-3\. Trie和DAWG结构（图片由[Chkno](https://oreil.ly/w71ZI) [CC BY-SA 3.0]提供）
- en: In the following sections, we take a closer look at DAWGs, tries, and their
    usage.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将更详细地了解DAWG、trie及其用法。
- en: Directed acyclic word graph
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 有向无环字图
- en: The [directed acyclic word graph](https://oreil.ly/4KfVO) (MIT license) attempts
    to efficiently represent strings that share common prefixes and suffixes.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[有向无环字图](https://oreil.ly/4KfVO)（MIT许可证）尝试高效表示共享公共前缀和后缀的字符串。'
- en: Note that at the time of writing, an [open Pull Request on GitHub](https://oreil.ly/6T5le)
    has to be applied to make this DAWG work with Python 3.7.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在撰写本文时，GitHub上的一个[开放的Pull Request](https://oreil.ly/6T5le)需要应用才能使此DAWG与Python
    3.7配合工作。
- en: In [Example 11-16](ch11_split_000.xhtml#less_ram_dawg_code), you see the very
    simple setup for a DAWG. For this implementation, the DAWG cannot be modified
    after construction; it reads an iterator to construct itself once. The lack of
    post-construction updates might be a deal breaker for your use case. If so, you
    might need to look into using a trie instead. The DAWG does support rich queries,
    including prefix lookups; it also allows persistence and supports storing integer
    indices as values along with byte and record values.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例 11-16](ch11_split_000.xhtml#less_ram_dawg_code)中，您可以看到一个非常简单的DAWG设置。对于此实现，构建后的DAWG无法修改；它读取一个迭代器来构建自身一次。缺乏构建后更新可能会成为您用例的破坏者。如果是这样，您可能需要考虑改用trie。DAWG支持丰富的查询，包括前缀查找；它还允许持久性，并支持存储整数索引作为值以及字节和记录值。
- en: Example 11-16\. Using a DAWG to store the data
  id: totrans-151
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-16\. 使用DAWG存储数据
- en: '[PRE25]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: As you can see in [Example 11-17](ch11_split_000.xhtml#less-ram-dawg-example),
    for the same set of strings it uses significantly less RAM than the earlier `set`
    example during the *construction* phase. More similar input text will cause stronger
    compression.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在[示例 11-17](ch11_split_000.xhtml#less-ram-dawg-example)中所见，对于相同的字符串集合，在*构建*阶段使用的RAM比之前的`set`示例少得多。更相似的输入文本将导致更强的压缩。
- en: Example 11-17\. Running the DAWG example
  id: totrans-154
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-17\. 运行DAWG示例
- en: '[PRE26]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: More importantly, if we persist the DAWG to disk, as shown in [Example 11-18](ch11_split_000.xhtml#less-ram-dawg-example2),
    and then load it back into a fresh Python instance, we see a dramatic reduction
    in RAM usage—the disk file and the memory usage after loading are both 70 MB;
    this is a significant savings compared to the 1.2 GB `set` variant we built earlier!
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 更重要的是，如果我们将DAWG持久化到磁盘，如[示例 11-18](ch11_split_000.xhtml#less-ram-dawg-example2)所示，然后将其加载回到一个新的Python实例中，我们将看到RAM使用量显著减少——磁盘文件和加载后的内存使用量都是70
    MB；与我们之前构建的1.2 GB `set` 变体相比，这是显著的节省！
- en: Example 11-18\. Loading the DAWG that was built and saved in an earlier session
    is more RAM efficient
  id: totrans-157
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-18\. 加载之前构建并保存的DAWG更节省内存
- en: '[PRE27]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Given that you’ll typically create a DAWG once and then load it many times,
    you’ll benefit from the construction costs repeatedly after you’ve persisted the
    structure to disk.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于您通常只需创建一次DAWG，然后多次加载，您在将结构持久化到磁盘后将受益于重复的构建成本。
- en: Marisa trie
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Marisa trie
- en: The [Marisa trie](https://oreil.ly/tDvVQ) (dual-licensed LGPL and BSD) is a
    static [trie](https://oreil.ly/suBhE) using Cython bindings to an external library.
    As it is static, it cannot be modified after construction. Like the DAWG, it supports
    storing integer indices as values, as well as byte values and record values.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[Marisa trie](https://oreil.ly/tDvVQ)（双许可LGPL和BSD）是一个静态的[trie](https://oreil.ly/suBhE)，使用Cython绑定到外部库。因为它是静态的，所以在构建后无法修改。与DAWG类似，它支持将整数索引作为值存储，以及字节值和记录值。'
- en: A key can be used to look up a value, and vice versa. All keys sharing the same
    prefix can be found efficiently. The trie’s contents can be persisted. [Example 11-19](ch11_split_001.xhtml#less_ram_marisa_trie_code)
    illustrates using a Marisa trie to store our sample data.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 一个关键字可以用来查找一个值，反之亦然。可以高效地找到所有共享相同前缀的键。trie的内容可以持久化。[示例 11-19](ch11_split_001.xhtml#less_ram_marisa_trie_code)说明了使用Marisa
    trie存储我们的示例数据。
- en: Example 11-19\. Using a Marisa trie to store the data
  id: totrans-163
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-19\. 使用Marisa trie存储数据
- en: '[PRE28]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: In [Example 11-20](ch11_split_001.xhtml#less-ram-marisa-trie-example), we can
    see that lookup times are slower than those offered by the DAWG.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例 11-20](ch11_split_001.xhtml#less-ram-marisa-trie-example)中，我们可以看到查找时间比DAWG提供的要慢。
- en: Example 11-20\. Running the Marisa trie example
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-20\. 运行Marisa trie示例
- en: '[PRE29]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The trie offers a further saving in memory on this dataset. While the lookups
    are a little slower in [Example 11-21](ch11_split_001.xhtml#less-ram-marisa-trie-example2),
    the disk and RAM usage are approximately 30 MB in the following snippet, if we
    save the trie to disk and then load it back into a fresh process; this is twice
    as good as what the DAWG achieved.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 字典树在这个数据集上进一步节省了内存。虽然查找速度稍慢（在[示例 11-21](ch11_split_001.xhtml#less-ram-marisa-trie-example2)中），但是在下面的代码片段中，如果我们将字典树保存到磁盘，然后重新加载到一个新进程中，磁盘和RAM使用量大约为30
    MB；这比DAWG实现的效果好一倍。
- en: Example 11-21\. Loading the Trie that was built and saved in an earlier session
    is more RAM efficient
  id: totrans-169
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例11-21\. 加载先前会话中构建和保存的字典树更节省RAM
- en: '[PRE30]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The trade-off between storage sizes after construction and lookup times will
    need to be investigated for your application. You may find that using one of these
    “works just well enough,” so you might avoid benchmarking other options and simply
    move on to your next challenge. We suggest that the Marisa trie is your first
    choice in this case; it has more stars than the DAWG on GitHub.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建后的存储大小和查找时间之间的权衡需要针对您的应用程序进行调查。您可能会发现其中一种“效果足够好”，因此可以避免基准测试其他选项，而是直接继续下一个挑战。在这种情况下，我们建议Marisa字典树是您的首选；它在GitHub上的星数比DAWG多。
- en: Using tries (and DAWGs) in production systems
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在生产系统中使用字典树（和DAWG）
- en: The trie and DAWG data structures offer good benefits, but you must still benchmark
    them on your problem rather than blindly adopting them. If you have overlapping
    sequences in your strings, you’ll likely see a RAM improvement.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 字典树和DAWG数据结构提供了很多好处，但在采用它们之前，你仍然需要根据自己的问题进行基准测试，而不是盲目地采用。
- en: 'Tries and DAWGs are less well known, but they can provide strong benefits in
    production systems. We have an impressive success story in [“Large-Scale Social
    Media Analysis at Smesh (2014)”](ch12.xhtml#lessons-from-field-alex). Jamie Matthews
    at DabApps (a Python software house based in the United Kingdom) also has a story
    about the use of tries in client systems to enable more efficient and cheaper
    deployments for customers:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 字典树（trie）和DAWG虽然不那么出名，但它们在生产系统中能提供显著的好处。我们在[“Smesh的大规模社交媒体分析（2014年）”](ch12.xhtml#lessons-from-field-alex)中有一个令人印象深刻的成功案例。Jamie
    Matthews在DabApps（一家位于英国的Python软件公司）也有一个关于在客户系统中使用字典树来实现更高效、更便宜部署的案例：
- en: At DabApps, we often try to tackle complex technical architecture problems by
    dividing them into small, self-contained components, usually communicating over
    the network using HTTP. This approach (referred to as a *service-oriented* or
    *microservice* architecture) has all sorts of benefits, including the possibility
    of reusing or sharing the functionality of a single component between multiple
    projects.
  id: totrans-175
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在DabApps，我们经常试图通过将复杂的技术架构问题分解为小型、自包含的组件来解决，通常使用HTTP在网络上进行通信。这种方法（称为*面向服务*或*微服务*架构）有各种好处，包括可以在多个项目之间重用或共享单个组件的功能。
- en: ''
  id: totrans-176
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'One such task that is often a requirement in our consumer-facing client projects
    is postcode geocoding. This is the task of converting a full UK postcode (for
    example: BN1 1AG) into a latitude and longitude coordinate pair, to enable the
    application to perform geospatial calculations such as distance measurement.'
  id: totrans-177
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们在面向消费者的客户项目中经常需要处理的任务之一是邮政编码地理编码。这个任务是将完整的英国邮政编码（例如：BN1 1AG）转换为经纬度坐标对，以便应用程序执行地理空间计算，比如距离测量。
- en: ''
  id: totrans-178
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: At its most basic, a geocoding database is a simple mapping between strings
    and can conceptually be represented as a dictionary. The dictionary keys are the
    postcodes, stored in a normalized form (BN11AG), and the values are a representation
    of the coordinates (we used a geohash encoding, but for simplicity imagine a comma-separated
    pair such as 50.822921,-0.142871).
  id: totrans-179
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在其最基本的形式下，地理编码数据库是字符串之间的简单映射，概念上可以表示为字典。字典的键是邮政编码，以规范化形式存储（例如BN11AG），值是坐标的表示（我们使用了地理哈希编码，但为简单起见，可以想象为逗号分隔的对，如50.822921,-0.142871）。
- en: ''
  id: totrans-180
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The UK has approximately 1.7 million postcodes. Naively loading the full dataset
    into a Python dictionary, as described previously, uses several hundred megabytes
    of memory. Persisting this data structure to disk using Python’s native Pickle
    format requires an unacceptably large amount of storage space. We knew we could
    do better.
  id: totrans-181
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 英国大约有170万个邮政编码。如前所述，将整个数据集朴素地加载到Python字典中需要几百兆字节的内存。使用Python的本机Pickle格式将这些数据结构持久化到磁盘需要大量的存储空间，这是不可接受的。我们知道我们可以做得更好。
- en: ''
  id: totrans-182
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We experimented with several different in-memory and on-disk storage and serialization
    formats, including storing the data externally in databases such as Redis and
    LevelDB, and compressing the key/value pairs. Eventually, we hit on the idea of
    using a trie. Tries are extremely efficient at representing large numbers of strings
    in memory, and the available open source libraries (we chose “marisa-trie”) make
    them very simple to use.
  id: totrans-183
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们尝试了几种不同的内存和磁盘存储及序列化格式，包括将数据存储在外部数据库如Redis和LevelDB中，并压缩键/值对。最终，我们想到了使用trie树的想法。Trie树在内存中表示大量字符串非常高效，并且可用的开源库（我们选择了“marisa-trie”）使它们非常简单易用。
- en: ''
  id: totrans-184
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The resulting application, including a tiny web API built with the Flask framework,
    uses only 30 MB of memory to represent the entire UK postcode database, and can
    comfortably handle a high volume of postcode lookup requests. The code is simple;
    the service is very lightweight and painless to deploy and run on a free hosting
    platform such as Heroku, with no external requirements or dependencies on databases.
    Our implementation is open source, available at [*https://github.com/j4mie/postcodeserver*](https://github.com/j4mie/postcodeserver).
  id: totrans-185
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 结果应用程序，包括使用Flask框架构建的微小Web API，仅使用30MB内存来表示整个英国邮政编码数据库，并且可以轻松处理大量的邮政编码查询请求。代码简洁；服务非常轻量级且无痛地可以在免费托管平台如Heroku上部署和运行，不依赖外部数据库或其他依赖。我们的实现是开源的，可在[*https://github.com/j4mie/postcodeserver*](https://github.com/j4mie/postcodeserver)获取。
- en: ''
  id: totrans-186
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Jamie Matthews, technical director of DabApps.com (UK)
  id: totrans-187
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Jamie Matthews，DabApps.com（英国）的技术总监
- en: DAWG and tries are powerful data structures that can help you save RAM and time
    in exchange for a little additional effort in preparation. These data structures
    will be unfamiliar to many developers, so consider separating this code into a
    module that is reasonably isolated from the rest of your code to simplify maintenance.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: DAWG和trie是强大的数据结构，可以帮助您节省RAM和时间，但需要在准备阶段额外付出一些努力。这些数据结构对许多开发者来说可能不太熟悉，因此考虑将此代码分离到一个与其余代码相对隔离的模块中，以简化维护。
- en: Modeling More Text with Scikit-Learn’s FeatureHasher
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Scikit-Learn的FeatureHasher进行更多文本建模
- en: Scikit-learn is Python’s best-known machine learning framework, and it has excellent
    support for text-based natural language processing (NLP) challenges. Here, we’ll
    look at classifying public posts from Usenet archives to one of 20 prespecified
    categories; this is similar to the two-category spam classification process that
    cleans up our email inboxes.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 是 Python 最知名的机器学习框架，对基于文本的自然语言处理（NLP）挑战有着出色的支持。在这里，我们将会看到如何将来自Usenet档案的公共帖子分类到20个预定类别中；这与清理电子邮件收件箱中的二类垃圾分类过程类似。
- en: One difficulty with text processing is that the vocabulary under analysis quickly
    explodes. The English language uses many nouns (e.g., the names of people and
    places, medical labels, and religious terms) and verbs (the “doing words” that
    often end in “-ing,” like “running,” “taking,” “making,” and “talking”) and their
    conjugations (turning the verb “talk” into “talked,” “talking,” “talks”), along
    with all the other rich forms of language. Punctuation and capitalization add
    an extra nuance to the representation of words.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 文本处理的一个困难在于分析中的词汇量迅速爆炸。英语使用许多名词（例如人名、地名、医学术语和宗教术语）和动词（“-ing”结尾的“doing words”，例如“running,”
    “taking,” “making,” 和 “talking”），以及它们的各种变形（将动词“talk”转变为“talked,” “talking,” “talks”）。此外，标点符号和大写字母为词语的表示增添了额外的细微差别。
- en: A powerful and simple technique for classifying text is to break the original
    text into *n-grams*, often unigrams, bigrams, and trigrams (also known as 1-grams,
    2-grams, and 3-grams). A sentence like “there is a cat and a dog” can be turned
    into unigrams (“there,” “is,” “a,” and so on), bigrams (“there is,” “is a,” “a
    cat,” etc.), and trigrams (“there is a,” “is a cat,” “a cat and,” …).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 一种强大而简单的文本分类技术是将原始文本分解为*n-grams*，通常是unigrams（一元组）、bigrams（二元组）和trigrams（三元组）（也称为1-gram、2-gram和3-gram）。例如，“there
    is a cat and a dog”这样的句子可以转换为unigrams（“there,” “is,” “a,” 等等）、bigrams（“there is,”
    “is a,” “a cat,” 等等）和trigrams（“there is a,” “is a cat,” “a cat and,” …）。
- en: There are 7 unigrams, 6 bigrams, and 5 trigrams for this sentence; in total
    this sentence can be represented in this form by a vocabulary of 6 unique unigrams
    (since the term “a” is used twice), 6 unique bigrams, and 5 unique trigrams, making
    17 descriptive items in total. As you can see, the n-gram vocabulary used to represent
    a sentence quickly grows; some terms are very common, and some are very rare.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这个句子有 7 个一元词项，6 个二元词项和 5 个三元词项；总计这个句子可以通过一个包含 6 个唯一一元词项（因为术语 “a” 被使用了两次）、6 个唯一二元词项和
    5 个唯一三元词项的词汇表以这种形式表示，总共有 17 个描述性项。如你所见，用于表示句子的 n-gram 词汇表会快速增长；有些术语非常常见，而有些则非常罕见。
- en: There are techniques to control the explosion of a vocabulary, such as eliminating
    stop-words (removing the most common and often uninformative terms, like “a,”
    “the,” and “of”), lowercasing everything, and ignoring less frequent types of
    terms (such as punctuation, numbers, and brackets). If you practice natural language
    processing, you’ll quickly come across these approaches.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 有控制词汇表爆炸的技术，例如消除停用词（删除最常见且通常无信息的词项，如 “a”、“the” 和 “of”）、将所有内容转换为小写，并忽略较少频繁的类型的词项（例如标点符号、数字和括号）。如果你从事自然语言处理，你很快就会接触到这些方法。
- en: Introducing DictVectorizer and FeatureHasher
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 `DictVectorizer` 和 `FeatureHasher`
- en: Before we look at the Usenet classification task, let’s look at two of scikit-learn’s
    feature processing tools that help with NLP challenges. The first is `DictVectorizer`,
    which takes a dictionary of terms and their frequencies and converts them into
    a variable-width sparse matrix (we will discuss sparse matrices in [“SciPy’s Sparse
    Matrices”](ch11_split_001.xhtml#SEC-sparse)). The second is `FeatureHasher`, which
    converts the same dictionary of terms and frequencies into a fixed-width sparse
    matrix.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们看 Usenet 分类任务之前，让我们看一下两个帮助处理 NLP 挑战的 scikit-learn 特征处理工具。首先是 `DictVectorizer`，它接受一个词汇表及其频率的字典，并将其转换为一个宽度可变的稀疏矩阵（我们将在
    [“SciPy's Sparse Matrices”](ch11_split_001.xhtml#SEC-sparse) 中讨论稀疏矩阵）。第二个是 `FeatureHasher`，它将相同的词汇表及其频率转换为固定宽度的稀疏矩阵。
- en: '[Example 11-22](ch11_split_001.xhtml#less_ram_dict_vectorizer) shows two sentences—“there
    is a cat” and “there is a cat and a dog”—in which terms are shared between the
    sentences and the term “a” is used twice in one of the sentences. `DictVectorizer`
    is given the sentences in the call to `fit`; in a first pass it builds a list
    of words into an internal `vocabulary_`, and in a second pass it builds up a sparse
    matrix containing a reference to each term and its count.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 11-22](ch11_split_001.xhtml#less_ram_dict_vectorizer) 展示了两个句子：“there is
    a cat” 和 “there is a cat and a dog”，这两个句子之间共享了词项，“a” 在其中一个句子中使用了两次。在调用 `fit` 时，`DictVectorizer`
    获得了这些句子；第一次遍历时，它构建了一个内部 `vocabulary_` 单词列表，第二次遍历时，它构建了一个稀疏矩阵，包含每个词项及其计数的引用。'
- en: Doing two passes takes longer than the one pass of `FeatureHasher`, and storing
    a vocabulary costs additional RAM. Building a vocabulary is often a serial process;
    by avoiding this stage, the feature hashing can potentially operate in parallel
    for additional speed.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 进行两次遍历比 `FeatureHasher` 的一次遍历需要更长时间，并且构建词汇表会额外消耗内存。构建词汇表通常是一个串行过程；通过避免这一阶段，特征哈希可以潜在地并行操作以获得额外的速度。
- en: Example 11-22\. Lossless text representation with `DictVectorizer`
  id: totrans-199
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-22\. 使用 `DictVectorizer` 进行无损文本表示
- en: '[PRE31]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: To make the output a little clearer, see the Pandas DataFrame view of the matrix
    `X` in [Figure 11-4](ch11_split_001.xhtml#FIG-less-ram-dictvect-dataframe), where
    the columns are set to the vocabulary. Note that here we’ve made a *dense* representation
    of the matrix—we have 2 rows and 6 columns, and each of the 12 cells contains
    a number. In the sparse form, we store only the 10 counts that are present and
    we do not store anything for the 2 items that are missing. With a larger corpus,
    the larger storage required by a dense representation, containing mostly 0s, quickly
    becomes prohibitive. For NLP the sparse representation is standard.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使输出更加清晰，请参阅矩阵 `X` 在 [图 11-4](ch11_split_001.xhtml#FIG-less-ram-dictvect-dataframe)
    中的 Pandas DataFrame 视图，其中列被设置为词汇表。注意，这里我们已经制作了一个*密集*的矩阵表示——我们有 2 行和 6 列，每个 12
    个单元格都包含一个数字。在稀疏形式中，我们仅存储存在的 10 个计数，对于缺失的 2 个项目我们不存储任何内容。对于较大的语料库，密集表示所需的存储空间，大部分是
    0，很快变得难以承受。对于自然语言处理来说，稀疏表示是标准的。
- en: '![DictVectorizer transformed ouptput](Images/hpp2_1104.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![DictVectorizer 转换输出](Images/hpp2_1104.png)'
- en: Figure 11-4\. Transformed output of `DictVectorizer` shown in a Pandas DataFrame
  id: totrans-203
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-4\. `DictVectorizer` 转换输出显示在 Pandas DataFrame 中
- en: One feature of `DictVectorizer` is that we can give it a matrix and reverse
    the process. In [Example 11-23](ch11_split_001.xhtml#less_ram_dict_vectorizer_reverse),
    we use the vocabulary to recover the original frequency representation. Note that
    this does *not* recover the original sentence; there’s more than one way to interpret
    the ordering of words in the first example (both “there is a cat” and “a cat is
    there” are valid interpretations). If we used bigrams, we’d start to introduce
    a constraint on the ordering of words.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '`DictVectorizer` 的一个特点是我们可以给它一个矩阵，并且反向进行处理。在 [示例 11-23](ch11_split_001.xhtml#less_ram_dict_vectorizer_reverse)
    中，我们使用词汇表来恢复原始的频率表示。请注意，这并*不*恢复原始句子；在第一个示例中，有多种解释单词顺序的方式（“there is a cat” 和 “a
    cat is there” 都是有效的解释）。如果我们使用了二元组，我们会开始引入对单词顺序的约束。'
- en: Example 11-23\. Reversing the output of matrix `X` to the original dictionary
    representation
  id: totrans-205
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-23\. 将矩阵 `X` 的输出反向转换为原始字典表示
- en: '[PRE32]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '`FeatureHasher` takes the same input and generates a similar output but with
    one key difference: it does not store a vocabulary and instead employs a hashing
    algorithm to assign token frequencies to columns.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '`FeatureHasher` 接受相同的输入并生成类似的输出，但有一个关键的区别：它不存储词汇表，而是使用散列算法将标记频率分配到列中。'
- en: We’ve already looked at hash functions in [“How Do Dictionaries and Sets Work?”](ch04.xhtml#dict_set_how_work).
    A hash converts a unique item (in this case a text token) into a number, where
    multiple unique items might map to the same hashed value, in which case we get
    a collision. Good hash functions cause few collisions. Collisions are inevitable
    if we’re hashing many unique items to a smaller representation. One feature of
    a hash function is that it can’t easily be reversed, so we can’t take a hashed
    value and convert it back to the original token.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在 [“字典和集合是如何工作的？”](ch04.xhtml#dict_set_how_work) 中看过哈希函数。哈希将唯一项（在本例中是文本标记）转换为一个数字，其中多个唯一项可能映射到相同的哈希值，这时我们会发生冲突。良好的哈希函数会导致很少的冲突。如果我们将许多唯一项哈希到一个较小的表示中，冲突是不可避免的。哈希函数的一个特点是它不容易反向操作，因此我们无法将哈希值转换回原始标记。
- en: In [Example 11-24](ch11_split_001.xhtml#less_ram_featurehasher), we ask for
    a fixed-width 10-column matrix—the default is a fixed-width matrix of 1 million
    elements, but we’ll use a tiny matrix here to show a collision. The default 1-million-element
    width is a sensible default for many applications.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [示例 11-24](ch11_split_001.xhtml#less_ram_featurehasher) 中，我们要求一个固定宽度为 10 列的矩阵——默认情况下是一个包含
    100 万个元素的固定宽度矩阵，但我们将在这里使用一个小矩阵来展示一个冲突。对于许多应用程序来说，默认的 100 万元素宽度是一个合理的默认值。
- en: The hashing process uses the fast MurmurHash3 algorithm, which transforms each
    token into a number; this is then converted into the range we specify. Larger
    ranges have few collisions; a small range like our range of 10 will have many
    collisions. Since every token has to be mapped to one of only 10 columns, we’ll
    get many collisions if we add a lot of sentences.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 哈希过程使用了快速的 MurmurHash3 算法，它将每个标记转换为一个数字；然后将其转换为我们指定的范围内。较大的范围有较少的冲突；像我们的 10
    的范围将会有许多冲突。由于每个标记必须映射到仅有的 10 列之一，如果我们添加了许多句子，我们将会得到许多冲突。
- en: The output `X` has 2 rows and 10 columns; each token maps to one column, and
    we don’t immediately know which column represents each word since hashing functions
    are one-way, so we can’t map the output back to the input. In this case we can
    deduce, using `extra_token_dict`, that the tokens `there` and `is` both map to
    column 8, so we get nine 0s and one count of 2 in column 8.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 输出 `X` 具有 2 行和 10 列；每个标记映射到一列，并且我们不能立即知道哪一列代表每个单词，因为哈希函数是单向的，所以我们无法将输出映射回输入。在这种情况下，我们可以推断，使用
    `extra_token_dict`，标记 `there` 和 `is` 都映射到列 8，因此在列 8 中我们得到九个 0 和一个计数为 2。
- en: Example 11-24\. Using a 10-column `FeatureHasher` to show a hash collision
  id: totrans-212
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-24\. 使用一个包含 10 列的 `FeatureHasher` 来展示哈希冲突
- en: '[PRE33]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Despite the occurrence of collisions, more than enough signal is often retained
    in this representation (assuming the default number of columns is used) to enable
    similar quality machine learning results with `FeatureHasher` compared to `DictVectorizer`.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在冲突的发生，通常在这种表示中保留了足够的信号（假设使用了默认列数），以便与 `DictVectorizer` 相比，`FeatureHasher`
    能够获得类似的优质机器学习结果。
- en: Comparing DictVectorizer and FeatureHasher on a Real Problem
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在一个真实问题上比较 DictVectorizer 和 FeatureHasher
- en: If we take the full 20 Newsgroups dataset, we have 20 categories, with approximately
    18,000 emails spread across the categories. While some categories such as “sci.med”
    are relatively unique, others like “comp.os.ms-windows.misc” and “comp.windows.x”
    will contain emails that share similar terms. The machine learning task is to
    correctly identify the correct newsgroup from the 20 options for each item in
    the test set. The test set has approximately 4,000 emails; the training set used
    to learn the mapping of terms to the matching category has approximately 14,000
    emails.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用完整的20个新闻组数据集，我们有20个类别，大约18,000封电子邮件分布在这些类别中。虽然某些类别如“sci.med”相对独特，但其他类别如“comp.os.ms-windows.misc”和“comp.windows.x”将包含共享相似术语的电子邮件。机器学习任务是为测试集中的每个项目正确识别出20个选项中的正确新闻组。测试集大约有4,000封电子邮件；用于学习术语到匹配类别映射的训练集大约有14,000封电子邮件。
- en: Note that this example does *not* deal with some of the necessities of a realistic
    training challenge. We haven’t stripped newsgroup metadata, which can be used
    to overfit on this challenge; rather than generalize just from the text of the
    emails, some extraneous metadata artificially boosts the scores. We have randomly
    shuffled the emails. Here, we’re not trying to achieve a single excellent machine
    learning result; instead, we’re demonstrating that a lossy hashed representation
    can be equivalent to a nonlossy and more memory-hungry variant.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，本示例*不*涉及一些现实训练挑战的必要性。我们未剥离新闻组元数据，这些元数据可能会用于这一挑战的过度拟合；与仅从电子邮件文本泛化不同，某些外部元数据人为地提高了分数。我们已随机打乱了电子邮件。在这里，我们不试图获得单一优秀的机器学习结果；而是演示了一个损失哈希表示可以等效于一个非损失且更占内存的变体。
- en: In [Example 11-25](ch11_split_001.xhtml#less_ram_featurehasher_real_world),
    we take 18,846 documents and build up a training and test set representation using
    both `DictVectorizer` and `FeatureHasher` with unigrams, bigrams, and trigrams.
    The `DictVectorizer` sparse array has shape `(14,134, 4,335,793)` for the training
    set, where our 14,134 emails are represented using 4 million tokens. Building
    the vocabulary and transforming the training data takes 42 seconds.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例 11-25](ch11_split_001.xhtml#less_ram_featurehasher_real_world)中，我们采用18,846份文档，并使用`DictVectorizer`和`FeatureHasher`分别基于单字、双字和三字组建立训练和测试集表示。对于训练集，`DictVectorizer`的稀疏数组形状为`(14,134,
    4,335,793)`，我们的14,134封电子邮件使用了4百万个标记。构建词汇表和转换训练数据耗时42秒。
- en: Contrast this with the `FeatureHasher`, which has a fixed 1-million-element-wide
    hashed representation, and where the transformation takes 21 seconds. Note that
    in both cases, roughly 9.8 million nonzero items are stored in the sparse matrices,
    so they’re storing similar quantities of information. The hashed version stores
    approximately 10,000 fewer items because of collisions.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 与此形成对比的是`FeatureHasher`，它具有固定的100万元素宽度的哈希表示，转换耗时21秒。请注意，在这两种情况下，稀疏矩阵存储了大约980万个非零项，因此它们存储了类似数量的信息。由于冲突，哈希版本存储了大约10,000个较少的项。
- en: If we’d used a dense matrix, we’d have 14 thousand rows by 10 million columns,
    making 140,000,000,000 cells of 8 bytes each—significantly more RAM than is typically
    available in any current machine. Only a tiny fraction of this matrix would be
    nonzero. Sparse matrices avoid this RAM consumption.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用了一个密集矩阵，我们将有14万行和1千万列，每个单元格为8字节——远超过当前任何一台机器的典型可用内存。只有这个矩阵的微小部分是非零的。稀疏矩阵避免了这种内存消耗。
- en: Example 11-25\. Comparing `DictVectorizer` and `FeatureHasher` on a real machine
    learning problem
  id: totrans-221
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-25\. 在一个真实的机器学习问题上比较`DictVectorizer`和`FeatureHasher`
- en: '[PRE34]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Critically, the `LogisticRegression` classifier used on `DictVectorizer` takes
    30% longer to train with 4 million columns compared to the 1 million columns used
    by the `FeatureHasher`. Both show a score of 0.89, so for this challenge the results
    are reasonably equivalent.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 关键是，在`DictVectorizer`上使用的`LogisticRegression`分类器，与使用`FeatureHasher`的100万列相比，训练时间长了30%。两者都显示了0.89的得分，因此对于这一挑战，结果基本上是等效的。
- en: Using `FeatureHasher`, we’ve achieved the same score on the test set, built
    our training matrix faster, and avoided building and storing a vocabulary, and
    we’ve trained faster than using the more common `DictVectorizer` approach. In
    exchange, we’ve lost the ability to transform a hashed representation back into
    the original features for debugging and explanation, and since we often want the
    ability to diagnose *why* a decision was made, this might be too costly a trade
    for you to make.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `FeatureHasher`，我们在测试集上达到了相同的分数，更快地构建了训练矩阵，避免了构建和存储词汇表，训练速度也比常见的 `DictVectorizer`
    方法快。但作为交换，我们失去了将散列表示转换回原始特征以进行调试和解释的能力，而且由于我们经常希望能够诊断*为什么*做出决策，这可能是一个过于昂贵的交易。
- en: SciPy’s Sparse Matrices
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SciPy 的稀疏矩阵
- en: In [“Introducing DictVectorizer and FeatureHasher”](ch11_split_001.xhtml#dictvectorizer),
    we created a large feature representation using `DictVectorizer`, which uses a
    sparse matrix in the background. These sparse matrices can be used for general
    computation as well and are extremely useful when working with sparse data.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [“介绍 DictVectorizer 和 FeatureHasher”](ch11_split_001.xhtml#dictvectorizer)
    中，我们使用 `DictVectorizer` 创建了一个大特征表示，它在后台使用稀疏矩阵。这些稀疏矩阵也可以用于一般计算，并且在处理稀疏数据时非常有用。
- en: 'A *sparse matrix* is a matrix in which most matrix elements are 0\. For these
    sorts of matrices, there are many ways to encode the nonzero values and then simply
    say “all other values are zero.” In addition to these memory savings, many algorithms
    have special ways of dealing with sparse matrices that give additional computational
    benefits:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '*稀疏矩阵* 是大多数矩阵元素为 0 的矩阵。对于这类矩阵，有许多方法可以编码非零值，然后简单地说“所有其他值都是零”。除了这些内存节省外，许多算法还有处理稀疏矩阵的特殊方法，提供额外的计算优势：'
- en: '[PRE35]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The simplest implementation of this is for `COO` matrices in SciPy, where for
    each non-zero element we store the value in addition to the location of the value.
    This means that for each nonzero value, we store three numbers in total. As long
    as our matrix has at least 66% zero entries, we are reducing the amount of memory
    needed to represent the data as a sparse matrix as opposed to a standard `numpy`
    array. However, `COO` matrices are generally used only to *construct* sparse matrices
    and not for doing actual computation (for that, [CSR/CSC](https://oreil.ly/nHc3h)
    is preferred).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这种最简单的实现是在 SciPy 的 `COO` 矩阵中，对于每个非零元素，我们存储该值以及该值的位置。这意味着对于每个非零值，我们总共存储三个数字。只要我们的矩阵至少有
    66% 的零条目，我们就可以减少用于表示数据的内存量，这与标准的 `numpy` 数组相比是一种节省。然而，`COO` 矩阵通常仅用于构造稀疏矩阵而不是进行实际计算（对于这一点，更喜欢使用
    CSR/CSC）。
- en: We can see in [Figure 11-5](ch11_split_001.xhtml#FIG-sparse) that for low densities,
    sparse matrices are much faster than their dense counterparts. On top of this,
    they also use much less memory.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到在 [图 11-5](ch11_split_001.xhtml#FIG-sparse) 中，对于低密度，稀疏矩阵比其密集对应物速度快得多。此外，它们还使用更少的内存。
- en: '![Testing the runtime speed of sparse matrices vs dense matrices for various
    values for matrix density. Sparse matrix is created with `sparse = scipy.sparse.random(2024,
    2024, density).tocsr()` and dense matrix is created with `dense = sparse.todense()`](Images/hpp2_1105.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![测试稀疏矩阵与密集矩阵在各种矩阵密度值下的运行时速度。稀疏矩阵通过 `sparse = scipy.sparse.random(2024, 2024,
    density).tocsr()` 创建，密集矩阵通过 `dense = sparse.todense()` 创建](Images/hpp2_1105.png)'
- en: Figure 11-5\. Sparse versus dense matrix multiplication
  id: totrans-232
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-5\. 稀疏与密集矩阵乘法
- en: In [Figure 11-6](ch11_split_001.xhtml#FIG-sparse-footprint), the dense matrix
    is always using 32.7 MB of memory (2048 × 2048 × 64-bit). However, a sparse matrix
    of 20% density uses only 10 MB, representing a 70% savings! As the density of
    the sparse matrix goes up, `numpy` quickly dominates in terms of speed because
    of the benefits of vectorization and better cache performance.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 11-6](ch11_split_001.xhtml#FIG-sparse-footprint) 中，密集矩阵始终使用 32.7 MB 内存（2048
    × 2048 × 64 位）。然而，20% 密度的稀疏矩阵仅使用 10 MB，节省了 70%！随着稀疏矩阵密度的增加，由于向量化和更好的缓存性能带来的好处，`numpy`
    的速度迅速超越了。
- en: '![Footprint for 2048 x 2048 dense versus sparse matrices at different densities](Images/hpp2_1106.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![2048 x 2048 稠密与稀疏矩阵在不同密度下的足迹](Images/hpp2_1106.png)'
- en: Figure 11-6\. Sparse versus dense memory footprint
  id: totrans-235
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-6\. 稀疏与密集内存足迹
- en: This extreme reduction in memory use is partly why the speeds are so much better.
    In addition to running only the multiplication operation for elements that are
    nonzero (thus reducing the number of operations needed), we also don’t need to
    allocate such a large amount of space to save our result in. This is the push
    and pull of speedups with sparse arrays—it is a balance between losing the use
    of efficient caching and vectorization versus not having to do a lot of the calculations
    associated with the zero values of the matrix.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这种极端的内存使用减少部分是速度提升如此明显的原因之一。除了仅对非零元素执行乘法运算（从而减少所需的操作次数）外，我们还不需要为保存结果分配如此大量的空间。这就是稀疏数组加速的推拉之道——在失去高效缓存和向量化的同时，还不必做与矩阵零值相关的大量计算。
- en: One operation that sparse matrices are particularly good at is cosine similarity.
    In fact, when creating a `DictVectorizer`, as we did in [“Introducing DictVectorizer
    and FeatureHasher”](ch11_split_001.xhtml#dictvectorizer), it’s common to use cosine
    similarity to see how similar two pieces of text are. In general for these item-to-item
    comparisons (where the value of a particular matrix element is compared to another
    matrix element), sparse matrices do quite well. Since the calls to `numpy` are
    the same whether we are using a normal matrix or a sparse matrix, we can benchmark
    the benefits of using a sparse matrix without changing the code of the algorithm.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏矩阵特别擅长的一种操作是余弦相似度。事实上，在创建`DictVectorizer`时，如我们在[“介绍DictVectorizer和FeatureHasher”](ch11_split_001.xhtml#dictvectorizer)中所做的那样，通常使用余弦相似度来查看两段文本的相似度。总体而言，对于这些项目对项目的比较（其中特定矩阵元素的值与另一个矩阵元素进行比较），稀疏矩阵表现非常出色。由于调用`numpy`的方式无论我们使用普通矩阵还是稀疏矩阵都是相同的，我们可以在不改变算法代码的情况下对使用稀疏矩阵的好处进行基准测试。
- en: While this is impressive, there are severe limitations. The amount of support
    for sparse matrices is quite low, and unless you are running special sparse algorithms
    or doing only basic operations, you’ll probably hit a wall in terms of support.
    In addition, SciPy’s `sparse` module offers multiple implementations of sparse
    matrices, all of which have different benefits and drawbacks. Understanding which
    is the best one to use and when to use it demands some expert knowledge and often
    leads to conflicting requirements. As a result, sparse matrices probably aren’t
    something you’ll be using often, but when they are the correct tool, they are
    invaluable.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这很令人印象深刻，但也存在严重的限制。稀疏矩阵的支持程度相当有限，除非你运行特殊的稀疏算法或者仅进行基本操作，否则在支持方面可能会遇到障碍。此外，SciPy的`sparse`模块提供了多种稀疏矩阵的实现，每种都有不同的优缺点。理解哪种是最佳选择，并在何时使用它需要一些专业知识，并经常会导致冲突的需求。因此，稀疏矩阵可能不是你经常使用的工具，但当它们是正确的工具时，它们是无价的。
- en: Tips for Using Less RAM
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用较少内存的技巧
- en: Generally, if you can avoid putting it into RAM, do. Everything you load costs
    you RAM. You might be able to load just a part of your data, for example, using
    a [memory-mapped file](https://oreil.ly/l7ekl); alternatively, you might be able
    to use generators to load only the part of the data that you need for partial
    computations rather than loading it all at once.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，如果可以避免将其放入RAM中，请避免。加载的每一项都会消耗你的RAM。例如，你可能可以只加载部分数据，例如使用[内存映射文件](https://oreil.ly/l7ekl)；或者，你可以使用生成器仅加载需要进行部分计算的数据部分，而不是一次性加载所有数据。
- en: If you are working with numeric data, you’ll almost certainly want to switch
    to using `numpy` arrays—the package offers many fast algorithms that work directly
    on the underlying primitive objects. The RAM savings compared to using lists of
    numbers can be huge, and the time savings can be similarly amazing. Furthermore,
    if you are dealing with very sparse arrays, using SciPy’s sparse array functionality
    can save incredible amounts of memory, albeit with a reduced feature set as compared
    to normal `numpy` arrays.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在处理数值数据，几乎可以肯定地说你会想要切换到使用`numpy`数组——该包提供了许多直接在底层基本对象上运行的快速算法。与使用数字列表相比，RAM的节省可能非常巨大，时间的节省同样令人惊讶。此外，如果处理的是非常稀疏的数组，使用SciPy的稀疏数组功能可以节省大量内存，尽管与普通的`numpy`数组相比功能集合有所减少。
- en: If you’re working with strings, stick to `str` rather than `bytes` unless you
    have strong reasons to work at the byte level. Dealing with a myriad set of text
    encodings is painful by hand, and UTF-8 (or other Unicode formats) tends to make
    these problems disappear. If you’re storing many Unicode objects in a static structure,
    you probably want to investigate the DAWG and trie structures that we’ve just
    discussed.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在处理字符串，请使用`str`而不是`bytes`，除非您有充分理由在字节级别工作。手动处理各种文本编码非常麻烦，而UTF-8（或其他Unicode格式）倾向于解决这些问题。如果您在静态结构中存储许多Unicode对象，您可能想研究我们刚讨论过的DAWG和trie结构。
- en: If you’re working with lots of bit strings, investigate `numpy` and the [`bitarray`](https://oreil.ly/Oz4-2)
    package; both have efficient representations of bits packed into bytes. You might
    also benefit from looking at Redis, which offers efficient storage of bit patterns.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在处理大量的位串，请研究`numpy`和[`bitarray`](https://oreil.ly/Oz4-2)包；两者都具有有效的位打包表示。您还可以通过查看Redis来受益于有效存储位模式。
- en: 'The PyPy project is experimenting with more efficient representations of homogeneous
    data structures, so long lists of the same primitive type (e.g., integers) might
    cost much less in PyPy than the equivalent structures in CPython. The [MicroPython](http://micropython.org)
    project will be interesting to anyone working with embedded systems: this tiny-memory-footprint
    implementation of Python is aiming for Python 3 compatibility.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: PyPy项目正在尝试更高效的同质数据结构表示，因此在PyPy中，长列表的同一种原始类型（例如整数）可能比CPython中的等效结构成本低得多。[MicroPython](http://micropython.org)项目对于任何与嵌入式系统一起工作的人都很有趣：这个内存占用极小的Python实现正在争取实现Python
    3兼容性。
- en: It goes (almost!) without saying that you know you have to benchmark when you’re
    trying to optimize on RAM usage, and that it pays handsomely to have a unit test
    suite in place before you make algorithmic changes.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎可以肯定，当您试图优化RAM使用时，您知道您必须进行基准测试，并且在进行算法更改之前建立一个单元测试套件将会非常有益。
- en: Having reviewed ways of compressing strings and storing numbers efficiently,
    we’ll now look at trading accuracy for storage space.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在回顾了压缩字符串和高效存储数字的方法之后，我们现在将探讨为存储空间而牺牲精度的方法。
- en: Probabilistic Data Structures
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概率数据结构
- en: Probabilistic data structures allow you to make trade-offs in accuracy for immense
    decreases in memory usage. In addition, the number of operations you can do on
    them is much more restricted than with a `set` or a trie. For example, with a
    single HyperLogLog++ structure using 2.56 KB, you can count the number of unique
    items up to approximately 7,900,000,000 items with 1.625% error.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 概率数据结构允许您在精度上做出牺牲以极大减少内存使用。此外，您可以对它们执行的操作数量远远少于`set`或trie。例如，使用单个2.56 KB的HyperLogLog++结构，您可以计算约7,900,000,000个项目的唯一项数，误差为1.625%。
- en: This means that if we’re trying to count the number of unique license plate
    numbers for cars, and our HyperLogLog++ counter said there were 654,192,028, we
    would be confident that the actual number is between 643,561,407 and 664,822,648\.
    Furthermore, if this accuracy isn’t sufficient, you can simply add more memory
    to the structure and it will perform better. Giving it 40.96 KB of resources will
    decrease the error from 1.625% to 0.4%. However, storing this data in a `set`
    would take 3.925 GB, even assuming no overhead!
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，如果我们试图统计汽车的唯一车牌号码数量，而我们的HyperLogLog++ 计数器显示有654,192,028个，我们可以确信实际数字在643,561,407和664,822,648之间。此外，如果这种精度不够，您可以简单地向结构添加更多内存，它将表现得更好。给它40.96
    KB的资源将把误差从1.625%降低到0.4%。然而，将这些数据存储在一个`set`中将需要3.925 GB，即使假设没有任何额外开销！
- en: On the other hand, the HyperLogLog++ structure would only be able to count a
    `set` of license plates and merge with another `set`. So, for example, we could
    have one structure for every state, find how many unique license plates are in
    each of those states, and then merge them all to get a count for the whole country.
    If we were given a license plate, we couldn’t tell you with very good accuracy
    whether we’ve seen it before, and we couldn’t give you a sample of license plates
    we have already seen.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，HyperLogLog++ 结构只能计数一组`set`的车牌并与另一个`set`合并。例如，我们可以为每个州创建一个结构，找出每个州中有多少个独特的车牌，然后将它们全部合并以获得整个国家的计数。如果我们拿到一个车牌号，我们无法非常准确地告诉您我们之前是否见过它，也无法给您一些我们已经见过的车牌的样本。
- en: Probabilistic data structures are fantastic when you have taken the time to
    understand the problem and need to put something into production that can answer
    a very small set of questions about a very large set of data. Each structure has
    different questions it can answer at different accuracies, so finding the right
    one is just a matter of understanding your requirements.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 当你花时间理解问题并需要将某些内容投入生产以回答大量数据中的一小部分问题时，概率性数据结构非常棒。每种结构可以以不同的精度回答不同的问题，因此找到适合的结构只是理解你需求的问题。
- en: Warning
  id: totrans-252
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Much of this section goes into a deep dive into the mechanisms that power many
    of the popular probabilistic data structures. This is useful, because once you
    understand the mechanisms, you can use parts of them in algorithms you are designing.
    If you are just beginning with probabilistic data structures, it may be useful
    to first look at the real-world example ([“Real-World Example”](ch11_split_001.xhtml#pds_example))
    before diving into the internals.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分大部分内容深入探讨了许多流行的概率性数据结构的动力机制。这是有用的，因为一旦你理解了这些机制，你可以在设计算法时使用它们的部分。如果你刚开始接触概率性数据结构，可能先看一下实际例子（[“实际例子”](ch11_split_001.xhtml#pds_example)）再深入了解内部机制会更有帮助。
- en: In almost all cases, probabilistic data structures work by finding an alternative
    representation for the data that is more compact and contains the relevant information
    for answering a certain set of questions. This can be thought of as a type of
    lossy compression, where we may lose some aspects of the data but we retain the
    necessary components. Since we are allowing the loss of data that isn’t necessarily
    relevant for the particular set of questions we care about, this sort of lossy
    compression can be much more efficient than the lossless compression we looked
    at before with tries. It is because of this that the choice of which probabilistic
    data structure you will use is quite important—you want to pick one that retains
    the right information for your use case!
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎在所有情况下，概率性数据结构的工作方式是找到数据的替代表示，这种表示更紧凑并包含回答特定问题所需的相关信息。这可以被看作是一种有损压缩，我们可能会丢失一些数据的特定方面，但保留必要的组成部分。由于我们允许丢失对于特定问题集并不必要的数据，这种有损压缩比我们之前看到的基于tries的无损压缩效率要高得多。正是因为这个原因，选择将要使用的概率性数据结构非常重要——你希望选择一个能够保留适合你使用情况的正确信息的结构！
- en: Before we dive in, it should be made clear that all the “error rates” here are
    defined in terms of *standard deviations*. This term comes from describing Gaussian
    distributions and says how spread out the function is around a center value. When
    the standard deviation grows, so do the number of values further away from the
    center point. Error rates for probabilistic data structures are framed this way
    because all the analyses around them are probabilistic. So, for example, when
    we say that the HyperLogLog++ algorithm has an error of <math alttext="e r r equals
    StartFraction 1.04 Over StartRoot m EndRoot EndFraction"><mrow><mi>e</mi> <mi>r</mi>
    <mi>r</mi> <mo>=</mo> <mfrac><mrow><mn>1</mn><mo>.</mo><mn>04</mn></mrow> <msqrt><mi>m</mi></msqrt></mfrac></mrow></math>
    , we mean that 68% of the time the error will be smaller than *err*, 95% of the
    time it will be smaller than 2 × *err*, and 99.7% of the time it will be smaller
    than 3 × *err*.^([2](ch11_split_001.xhtml#idm46122399754392))
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入讨论之前，需要明确的是这里所有的“误差率”都是以*标准差*为定义。这个术语来自于描述高斯分布，并且说明了函数围绕中心值的分布有多广。当标准差增大时，远离中心点的数值也会增多。概率性数据结构的误差率被这样框定，因为它们周围的所有分析都是概率性的。因此，例如，当我们说HyperLogLog++算法的误差率为<math
    alttext="e r r equals StartFraction 1.04 Over StartRoot m EndRoot EndFraction"><mrow><mi>e</mi>
    <mi>r</mi> <mi>r</mi> <mo>=</mo> <mfrac><mrow><mn>1</mn><mo>.</mo><mn>04</mn></mrow>
    <msqrt><mi>m</mi></msqrt></mfrac></mrow></math>时，我们的意思是有68%的时间误差小于*err*，95%的时间误差小于2
    × *err*，99.7%的时间误差小于3 × *err*。^([2](ch11_split_001.xhtml#idm46122399754392))
- en: Very Approximate Counting with a 1-Byte Morris Counter
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用一个字节的莫里斯计数器进行非常粗略的计数
- en: We’ll introduce the topic of probabilistic counting with one of the earliest
    probabilistic counters, the Morris counter (by Robert Morris of the NSA and Bell
    Labs). Applications include counting millions of objects in a restricted-RAM environment
    (e.g., on an embedded computer), understanding large data streams, and working
    on problems in AI like image and speech recognition.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将介绍概率计数的主题，其中包括最早的概率计数器之一，即 Morris 计数器（由 NSA 和贝尔实验室的 Robert Morris 设计）。应用场景包括在受限
    RAM 环境中计数数百万个对象（例如嵌入式计算机）、理解大数据流以及解决人工智能中的问题，如图像和语音识别。
- en: The Morris counter keeps track of an exponent and models the counted state as
    <math alttext="2 Superscript e x p o n e n t"><msup><mn>2</mn> <mrow><mi>e</mi><mi>x</mi><mi>p</mi><mi>o</mi><mi>n</mi><mi>e</mi><mi>n</mi><mi>t</mi></mrow></msup></math>
    (rather than a correct count)—it provides an *order of magnitude* estimate. This
    estimate is updated using a probabilistic rule.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: Morris 计数器跟踪一个指数，并将被计数的状态建模为 <math alttext="2 Superscript e x p o n e n t"><msup><mn>2</mn>
    <mrow><mi>e</mi><mi>x</mi><mi>p</mi><mi>o</mi><mi>n</mi><mi>e</mi><mi>n</mi><mi>t</mi></mrow></msup></math>（而不是正确的计数）—它提供一个
    *数量级* 的估计。此估计是使用概率规则更新的。
- en: We start with the exponent set to 0\. If we ask for the *value* of the counter,
    we’ll be given `pow(2,*exponent*)=1` (the keen reader will note that this is off
    by one—we did say this was an *approximate* counter!). If we ask the counter to
    increment itself, it will generate a random number (using the uniform distribution),
    and it will test if `random.uniform(0, 1)` `<=` `1/pow(2,*exponent*)`, which will
    always be true (`pow(2,0) == 1`). The counter increments, and the exponent is
    set to 1.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从指数设置为 0 开始。如果我们请求计数器的 *值*，我们将得到 `pow(2,*exponent*)=1`（敏锐的读者会注意到这与实际值相差一步，我们确实说过这是一个
    *近似* 计数器！）。如果我们要求计数器自增，它将生成一个随机数（使用均匀分布），并且会测试 `random.uniform(0, 1) <= 1/pow(2,*exponent*)`，这个测试总是成立（`pow(2,0)
    == 1`）。计数器自增，指数设为 1。
- en: The second time we ask the counter to increment itself, it will test if `random.uniform(0,
    1) <= 1/pow(2,1)`. This will be true 50% of the time. If the test passes, the
    exponent is incremented. If not, the exponent is not incremented for this increment
    request.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 第二次请求计数器自增时，它会测试 `random.uniform(0, 1) <= 1/pow(2,1)` 是否成立。这个测试有50%的概率会通过。如果测试通过，则指数会增加。否则，在这次自增请求中指数不会增加。
- en: '[Table 11-1](ch11_split_001.xhtml#table_morris_counter) shows the likelihoods
    of an increment occurring for each of the first exponents.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 11-1](ch11_split_001.xhtml#table_morris_counter) 展示了每个初始指数下增加发生的可能性。'
- en: Table 11-1\. Morris counter details
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11-1\. Morris 计数器详细信息
- en: '| Exponent | pow(2,*exponent*) | P(*increment*) |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 指数 | pow(2,*exponent*) | P(*increment*) |'
- en: '| --- | --- | --- |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 0 | 1 | 1 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1 | 1 |'
- en: '| 1 | 2 | 0.5 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2 | 0.5 |'
- en: '| 2 | 4 | 0.25 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 4 | 0.25 |'
- en: '| 3 | 8 | 0.125 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 8 | 0.125 |'
- en: '| 4 | 16 | 0.0625 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 16 | 0.0625 |'
- en: '| … | … | … |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| … | … | … |'
- en: '| 254 | 2.894802e+76 | 3.454467e-77 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 254 | 2.894802e+76 | 3.454467e-77 |'
- en: The maximum we could approximately count where we use a single unsigned byte
    for the exponent is `math.pow(2,255) == 5e76`. The error relative to the actual
    count will be fairly large as the counts increase, but the RAM savings is tremendous,
    as we use only 1 byte rather than the 32 unsigned bytes we’d otherwise have to
    use. [Example 11-26](ch11_split_001.xhtml#memory_morris_example_code) shows a
    simple implementation of the Morris counter.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们大约能计数的最大值，当我们使用单个无符号字节作为指数时，是 `math.pow(2,255) == 5e76`。随着计数增加，与实际计数的相对误差将会很大，但是相对于需要使用的
    32 个无符号字节，内存节省是巨大的。[示例 11-26](ch11_split_001.xhtml#memory_morris_example_code)
    展示了 Morris 计数器的简单实现。
- en: Example 11-26\. Simple Morris counter implementation
  id: totrans-273
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-26\. 简单的 Morris 计数器实现
- en: '[PRE36]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Using this implementation, we can see in [Example 11-27](ch11_split_001.xhtml#morris_counter_example)
    that the first request to increment the counter succeeds, the second succeeds,
    and the third doesn’t.^([3](ch11_split_001.xhtml#idm46122399671816))
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个实现，我们可以在 [示例 11-27](ch11_split_001.xhtml#morris_counter_example) 中看到，第一次请求增加计数器成功，第二次也成功，但第三次失败了。^([3](ch11_split_001.xhtml#idm46122399671816))
- en: Example 11-27\. Morris counter library example
  id: totrans-276
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-27\. Morris 计数器库示例
- en: '[PRE37]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: In [Figure 11-7](ch11_split_001.xhtml#FIG-morris-counter), the thick black line
    shows a normal integer incrementing on each iteration. On a 64-bit computer, this
    is an 8-byte integer. The evolution of three 1-byte Morris counters is shown as
    dotted lines; the y-axis shows their values, which approximately represent the
    true count for each iteration. Three counters are shown to give you an idea about
    their different trajectories and the overall trend; the three counters are entirely
    independent of one another.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 11-7](ch11_split_001.xhtml#FIG-morris-counter)中，粗黑线显示了每次迭代中正常整数的增加。在64位计算机上，这是一个8字节的整数。三个1字节的Morris计数器的演变显示为虚线；y轴显示它们的值，这大致代表了每次迭代的真实计数。展示三个计数器是为了让你了解它们不同的轨迹和整体趋势；这三个计数器完全独立于彼此。
- en: '![Three 1-byte Morris Counters](Images/hpp2_1107.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![三个1字节的Morris计数器](Images/hpp2_1107.png)'
- en: Figure 11-7\. Three 1-byte Morris counters versus an 8-byte integer
  id: totrans-280
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-7\. 三个1字节的Morris计数器与一个8字节的整数对比
- en: This diagram gives you some idea about the error to expect when using a Morris
    counter. Further details about the error behavior are available [online](http://bit.ly/Morris_error).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图表可以让你对使用Morris计数器时可能遇到的误差有所了解。关于误差行为的更多详细信息可以在[在线](http://bit.ly/Morris_error)获取。
- en: K-Minimum Values
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K-最小值
- en: In the Morris counter, we lose any sort of information about the items we insert.
    That is to say, the counter’s internal state is the same whether we do `.add("micha")`
    or `.add("ian")`. This extra information is useful and, if used properly, could
    help us have our counters count only unique items. In this way, calling `.add("micha")`
    thousands of times would increase the counter only once.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在Morris计数器中，我们失去了关于插入的任何信息。也就是说，计数器的内部状态无论我们是执行`.add("micha")`还是`.add("ian")`都是相同的。这些额外的信息是有用的，如果正确使用，可以帮助我们的计数器仅计算唯一的项。这样，调用`.add("micha")`数千次只会增加计数器一次。
- en: 'To implement this behavior, we will exploit properties of hashing functions
    (see [“Hash Functions and Entropy”](ch04.xhtml#SEC-dict-set-hash-and-entropy)
    for a more in-depth discussion of hash functions). The main property we would
    like to take advantage of is the fact that the hash function takes input and *uniformly*
    distributes it. For example, let’s assume we have a hash function that takes in
    a string and outputs a number between 0 and 1\. For that function to be uniform
    means that when we feed it in a string, we are equally likely to get a value of
    0.5 as a value of 0.2 or any other value. This also means that if we feed it in
    many string values, we would expect the values to be relatively evenly spaced.
    Remember, this is a probabilistic argument: the values won’t always be evenly
    spaced, but if we have many strings and try this experiment many times, they will
    tend to be evenly spaced.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现这种行为，我们将利用哈希函数的特性（参见[“哈希函数和熵”](ch04.xhtml#SEC-dict-set-hash-and-entropy)以获取哈希函数的更深入讨论）。我们希望利用的主要属性是哈希函数接受输入并*均匀地*分布它。例如，假设我们有一个哈希函数，它接受一个字符串并输出一个在0到1之间的数字。对于函数是均匀的意味着当我们输入一个字符串时，得到0.5的值和得到0.2或任何其他值的概率是相等的。这也意味着，如果我们输入许多字符串值，我们期望这些值相对均匀地分布。请记住，这是一个概率论的论点：这些值不会总是均匀分布，但如果我们有许多字符串并多次尝试这个实验，它们会趋向于均匀分布。
- en: Suppose we took 100 items and stored the hashes of those values (the hashes
    being numbers from 0 to 1). Knowing the spacing is even means that instead of
    saying, “We have 100 items,” we could say, “We have a distance of 0.01 between
    every item.” This is where the K-Minimum Values algorithm finally comes in^([4](ch11_split_001.xhtml#idm46122399315416))--if
    we keep the `k` smallest unique hash values we have seen, we can approximate the
    overall spacing between hash values and infer the total number of items. In [Figure 11-8](ch11_split_001.xhtml#FIG-kmv-hash-density),
    we can see the state of a K-Minimum Values structure (also called a KMV) as more
    and more items are added. At first, since we don’t have many hash values, the
    largest hash we have kept is quite large. As we add more and more, the largest
    of the `k` hash values we have kept gets smaller and smaller. Using this method,
    we can get error rates of <math alttext="upper O left-parenthesis StartRoot StartFraction
    2 Over pi left-parenthesis k minus 2 right-parenthesis EndFraction EndRoot right-parenthesis"><mrow><mi>O</mi>
    <mo>(</mo> <msqrt><mfrac><mn>2</mn> <mrow><mi>π</mi><mo>(</mo><mi>k</mi><mo>-</mo><mn>2</mn><mo>)</mo></mrow></mfrac></msqrt>
    <mo>)</mo></mrow></math> .
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们取了 100 个项并存储了这些值的哈希（哈希值为 0 到 1 的数字）。知道间隔是均匀的意味着，不是说，“我们有 100 个项”，而是说，“每个项之间的距离是
    0.01”。这就是 K-Minimum Values 算法最终发挥作用的地方^([4](ch11_split_001.xhtml#idm46122399315416))——如果我们保留了我们见过的
    `k` 个最小的唯一哈希值，我们可以近似推断出哈希值之间的总体间隔，并推断出总项数。在 [图 11-8](ch11_split_001.xhtml#FIG-kmv-hash-density)
    中，我们可以看到 K-Minimum Values 结构（也称为 KMV）在添加更多项时的状态。起初，由于我们没有很多哈希值，我们保留的最大哈希值相当大。随着我们添加更多的哈希值，我们保留的
    `k` 个哈希值中的最大值变得越来越小。使用这种方法，我们可以获得 <math alttext="upper O left-parenthesis StartRoot
    StartFraction 2 Over pi left-parenthesis k minus 2 right-parenthesis EndFraction
    EndRoot right-parenthesis"><mrow><mi>O</mi> <mo>(</mo> <msqrt><mfrac><mn>2</mn>
    <mrow><mi>π</mi><mo>(</mo><mi>k</mi><mo>-</mo><mn>2</mn><mo>)</mo></mrow></mfrac></msqrt>
    <mo>)</mo></mrow></math> 的错误率。
- en: The larger `k` is, the more we can account for the hashing function we are using
    not being completely uniform for our particular input and for unfortunate hash
    values. An example of unfortunate hash values would be hashing `['A', 'B', 'C']`
    and getting the values `[0.01, 0.02, 0.03]`. If we start hashing more and more
    values, it is less and less probable that they will clump up.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 当 `k` 越大时，我们能更好地考虑我们使用的哈希函数对于特定输入和不幸的哈希值并非完全均匀的影响。一个不幸的哈希值的例子是对 `['A', 'B',
    'C']` 进行哈希后得到 `[0.01, 0.02, 0.03]`。如果我们开始哈希更多的值，它们聚集在一起的可能性就越来越小。
- en: Furthermore, since we are keeping only the smallest *unique* hash values, the
    data structure considers only unique inputs. We can see this easily because if
    we are in a state where we store only the smallest three hashes and currently
    `[0.1, 0.2, 0.3]` are the smallest hash values, then if we add in something with
    the hash value of `0.4`, our state also will not change. Similarly, if we add
    more items with a hash value of `0.3`, our state will also not change. This is
    a property called *idempotence*; it means that if we do the same operation, with
    the same inputs, on this structure multiple times, the state will not be changed.
    This is in contrast to, for example, an `append` on a `list`, which will always
    change its value. This concept of idempotence carries on to all of the data structures
    in this section except for the Morris counter.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于我们只保留了最小的*唯一*哈希值，数据结构仅考虑唯一的输入。我们可以轻松看到这一点，因为如果我们处于仅存储最小的三个哈希值的状态，并且当前 `[0.1,
    0.2, 0.3]` 是最小的哈希值，那么如果我们添加一个哈希值为 `0.4` 的项，我们的状态也不会改变。同样地，如果我们添加更多哈希值为 `0.3` 的项，我们的状态也不会改变。这是一种称为*幂等性*的属性；这意味着如果我们在这个结构上多次进行相同的操作，使用相同的输入，状态不会改变。这与例如在
    `list` 上的 `append` 不同，后者总是会改变其值。这种幂等性的概念延续到本节中所有的数据结构，除了 Morris 计数器。
- en: '[Example 11-28](ch11_split_001.xhtml#memory_simple_kmv) shows a very basic
    K-Minimum Values implementation. Of note is our use of a `sortedset`, which, like
    a set, can contain only unique items. This uniqueness gives our `KMinValues` structure
    idempotence for free. To see this, follow the code through: when the same item
    is added more than once, the `data` property does not change.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 11-28](ch11_split_001.xhtml#memory_simple_kmv) 展示了一个非常基本的 K-Minimum Values
    实现。值得注意的是我们使用了 `sortedset`，它像集合一样只能包含唯一的项。这种唯一性使得我们的 `KMinValues` 结构自动具有幂等性。要看到这一点，请跟随代码：当同一项被多次添加时，`data`
    属性不会改变。'
- en: '![Density of hash space for K-Min Value structures](Images/hpp2_1108.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![K-Min Value 结构的哈希空间密度](Images/hpp2_1108.png)'
- en: Figure 11-8\. The value stores in a KMV structure as more elements are added
  id: totrans-290
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-8\. 随着更多元素添加到KMV结构中存储的值
- en: Example 11-28\. Simple `KMinValues` implementation
  id: totrans-291
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-28\. 简单的`KMinValues`实现
- en: '[PRE38]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Using the `KMinValues` implementation in the Python package [`countmemaybe`](https://oreil.ly/YF6uO)
    ([Example 11-29](ch11_split_001.xhtml#countmemaybe_kminvalues_implementation)),
    we can begin to see the utility of this data structure. This implementation is
    very similar to the one in [Example 11-28](ch11_split_001.xhtml#memory_simple_kmv),
    but it fully implements the other set operations, such as union and intersection.
    Also note that “size” and “cardinality” are used interchangeably (the word “cardinality”
    is from set theory and is used more in the analysis of probabilistic data structures).
    Here, we can see that even with a reasonably small value for `k`, we can store
    50,000 items and calculate the cardinality of many set operations with relatively
    low error.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Python包[`countmemaybe`](https://oreil.ly/YF6uO)中的`KMinValues`实现（[示例 11-29](ch11_split_001.xhtml#countmemaybe_kminvalues_implementation)），我们可以开始看到这种数据结构的实用性。这个实现与[示例 11-28](ch11_split_001.xhtml#memory_simple_kmv)中的实现非常相似，但它完全实现了其他集合操作，如并集和交集。还要注意，“size”和“cardinality”可以互换使用（“cardinality”这个词来自集合理论，在概率数据结构的分析中更常用）。在这里，我们可以看到，即使对于一个相对较小的`k`值，我们也可以存储50,000个项目，并计算许多集合操作的基数，误差相对较低。
- en: Example 11-29\. `countmemaybe` `KMinValues` implementation
  id: totrans-294
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-29\. `countmemaybe` `KMinValues`实现
- en: '[PRE39]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[![1](Images/1.png)](ch11_split_001.xhtml#co_using_less_ram_CO1-1)'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](ch11_split_001.xhtml#co_using_less_ram_CO1-1)'
- en: We put 50,000 elements into `kmv1`.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将50,000个元素放入`kmv1`中。
- en: '[![2](Images/2.png)](ch11_split_001.xhtml#co_using_less_ram_CO1-2)'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](ch11_split_001.xhtml#co_using_less_ram_CO1-2)'
- en: '`kmv2` also gets 50,000 elements, 25,000 of which are also in `kmv1`.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '`kmv2`也获得了50,000个元素，其中25,000个也在`kmv1`中。'
- en: Note
  id: totrans-300
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: With these sorts of algorithms, the choice of hash function can have a drastic
    effect on the quality of the estimates. Both of these implementations use [`mmh3`](https://pypi.org/project/mmh3),
    a Python implementation of `murmurhash3` that has nice properties for hashing
    strings. However, different hash functions could be used if they are more convenient
    for your particular dataset.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种类型的算法，哈希函数的选择可能会对估计的质量产生重大影响。这两种实现都使用了[`mmh3`](https://pypi.org/project/mmh3)，这是`murmurhash3`的Python实现，具有良好的哈希字符串属性。但是，如果对于您的特定数据集更方便，也可以使用不同的哈希函数。
- en: Bloom Filters
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 布隆过滤器
- en: Sometimes we need to be able to do other types of set operations, for which
    we need to introduce new types of probabilistic data structures. *Bloom filters*
    were created to answer the question of whether we’ve seen an item before.^([5](ch11_split_001.xhtml#idm46122398960952))
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们需要能够执行其他类型的集合操作，为此我们需要引入新类型的概率数据结构。*布隆过滤器*就是为了回答我们是否以前见过某个项目的问题而创建的。^([5](ch11_split_001.xhtml#idm46122398960952))
- en: Bloom filters work by having multiple hash values in order to represent a value
    as multiple integers. If we later see something with the same set of integers,
    we can be reasonably confident that it is the same value.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 布隆过滤器通过具有多个哈希值来表示一个值的方式工作，如果以后看到具有相同整数集合的东西，我们可以合理地认为它是相同的值。
- en: To do this in a way that efficiently utilizes available resources, we implicitly
    encode the integers as the indices of a list. This could be thought of as a list
    of `bool` values that are initially set to `False`. If we are asked to add an
    object with hash values `[10, 4, 7]`, we set the tenth, fourth, and seventh indices
    of the list to `True`. In the future, if we are asked if we have seen a particular
    item before, we simply find its hash values and check if all the corresponding
    spots in the `bool` list are set to `True`.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 为了以一种有效利用可用资源的方式来实现这一点，我们将整数隐式编码为列表的索引。这可以看作是一个初始设置为`False`的`bool`值列表。如果我们被要求添加具有哈希值`[10,
    4, 7]`的对象，我们将列表的第十、第四和第七个索引设置为`True`。将来，如果我们被问及之前是否见过特定项，我们只需找到其哈希值并检查`bool`列表中对应的位置是否都设置为`True`。
- en: This method gives us no false negatives and a controllable rate of false positives.
    If the Bloom filter says we have not seen an item before, we can be 100% sure
    that we haven’t seen the item before. On the other hand, if the Bloom filter states
    that we *have* seen an item before, there is a probability that we actually have
    not and we are simply seeing an erroneous result. This erroneous result comes
    from the fact that we will have hash collisions, and sometimes the hash values
    for two objects will be the same even if the objects themselves are not the same.
    However, in practice Bloom filters are set to have error rates below 0.5%, so
    this error can be acceptable.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法既不会产生误报，又可以控制误判率。如果布隆过滤器表示我们以前没有看到某个项目，我们可以百分之百确定我们确实没有看到过这个项目。另一方面，如果布隆过滤器表示我们*曾经*看到过某个项目，实际上有可能我们并没有，我们只是看到了一个错误的结果。这种错误结果来自哈希冲突的事实，有时两个对象的哈希值可能相同，即使对象本身并不相同。然而，在实践中，布隆过滤器的误差率通常低于0.5%，因此这种误差是可以接受的。
- en: Note
  id: totrans-307
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'We can simulate having as many hash functions as we want simply by having two
    hash functions that are independent of each other. This method is called *double
    hashing*. If we have a hash function that gives us two independent hashes, we
    can do this:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以简单地通过两个相互独立的哈希函数来模拟任意多个哈希函数。这种方法称为*双重哈希*。
- en: '[PRE40]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The modulo ensures that the resulting hash values are 32-bit (we would modulo
    by `2^64 - 1` for 64-bit hash functions).
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 取模操作确保生成的哈希值为32位（对于64位哈希函数，我们会取模`2^64 - 1`）。
- en: 'The exact length of the `bool` list and the number of hash values per item
    we need will be fixed based on the capacity and the error rate we require. With
    some reasonably simple statistical arguments,^([6](ch11_split_001.xhtml#idm46122398840536))
    we see that the ideal values are as follows:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 布尔列表的确切长度和每个项目所需的哈希值数量将根据我们需要的容量和误差率进行固定。通过一些相当简单的统计论证，^([6](ch11_split_001.xhtml#idm46122398840536))
    我们得出理想值如下：
- en: <math display="block" alttext="n u m normal bar b i t s equals minus c a p a
    c i t y dot StartFraction l o g left-parenthesis e r r o r right-parenthesis Over
    l o g left-parenthesis 2 right-parenthesis squared EndFraction"><mrow><mi>n</mi>
    <mi>u</mi> <mi>m</mi> <mo>_</mo> <mi>b</mi> <mi>i</mi> <mi>t</mi> <mi>s</mi> <mo>=</mo>
    <mo>–</mo> <mi>c</mi> <mi>a</mi> <mi>p</mi> <mi>a</mi> <mi>c</mi> <mi>i</mi> <mi>t</mi>
    <mi>y</mi> <mo>×</mo> <mfrac><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mo>(</mo><mi>e</mi><mi>r</mi><mi>r</mi><mi>o</mi><mi>r</mi><mo>)</mo></mrow>
    <mrow><mi>l</mi><mi>o</mi><mi>g</mi><msup><mrow><mo>(</mo><mn>2</mn><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></mfrac></mrow></math><math display="block" alttext="n
    u m normal bar h a s h e s equals n u m normal bar b i t s dot StartFraction l
    o g left-parenthesis 2 right-parenthesis Over c a p a c i t y EndFraction"><mrow><mi>n</mi>
    <mi>u</mi> <mi>m</mi> <mo>_</mo> <mi>h</mi> <mi>a</mi> <mi>s</mi> <mi>h</mi> <mi>e</mi>
    <mi>s</mi> <mo>=</mo> <mi>n</mi> <mi>u</mi> <mi>m</mi> <mo>_</mo> <mi>b</mi> <mi>i</mi>
    <mi>t</mi> <mi>s</mi> <mo>×</mo> <mfrac><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mo>(</mo><mn>2</mn><mo>)</mo></mrow>
    <mrow><mi>c</mi><mi>a</mi><mi>p</mi><mi>a</mi><mi>c</mi><mi>i</mi><mi>t</mi><mi>y</mi></mrow></mfrac></mrow></math>
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block" alttext="n u m normal bar b i t s equals minus c a p a
    c i t y dot StartFraction l o g left-parenthesis e r r o r right-parenthesis Over
    l o g left-parenthesis 2 right-parenthesis squared EndFraction"><mrow><mi>n</mi>
    <mi>u</mi> <mi>m</mi> <mo>_</mo> <mi>b</mi> <mi>i</mi> <mi>t</mi> <mi>s</mi> <mo>=</mo>
    <mo>–</mo> <mi>c</mi> <mi>a</mi> <mi>p</mi> <mi>a</mi> <mi>c</mi> <mi>i</mi> <mi>t</mi>
    <mi>y</mi> <mo>×</mo> <mfrac><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mo>(</mo><mi>e</mi><mi>r</mi><mi>r</mi><mi>o</mi><mi>r</mi><mo>)</mo></mrow>
    <mrow><mi>l</mi><mi>o</mi><mi>g</mi><msup><mrow><mo>(</mo><mn>2</mn><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></mfrac></mrow></math><math display="block" alttext="n
    u m normal bar h a s h e s equals n u m normal bar b i t s dot StartFraction l
    o g left-parenthesis 2 right-parenthesis Over c a p a c i t y EndFraction"><mrow><mi>n</mi>
    <mi>u</mi> <mi>m</mi> <mo>_</mo> <mi>h</mi> <mi>a</mi> <mi>s</mi> <mi>h</mi> <mi>e</mi>
    <mi>s</mi> <mo>=</mo> <mi>n</mi> <mi>u</mi> <mi>m</mi> <mo>_</mo> <mi>b</mi> <mi>i</mi>
    <mi>t</mi> <mi>s</mi> <mo>×</mo> <mfrac><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mo>(</mo><mn>2</mn><mo>)</mo></mrow>
    <mrow><mi>c</mi><mi>a</mi><mi>p</mi><mi>a</mi><mi>c</mi><mi>i</mi><mi>t</mi><mi>y</mi></mrow></mfrac></mrow></math>
- en: If we wish to store 50,000 objects (no matter how big the objects themselves
    are) at a false positive rate of 0.05% (that is to say, 0.05% of the times we
    say we have seen an object before, we actually have not), it would require 791,015
    bits of storage and 11 hash functions.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望以0.05%的误报率（即我们声称曾经见过一个对象，实际上却没有的情况）存储50,000个对象（无论这些对象本身有多大），则需要791,015位的存储空间和11个哈希函数。
- en: To further improve our efficiency in terms of memory use, we can use single
    bits to represent the `bool` values (a native `bool` actually takes 4 bits). We
    can do this easily by using the `bitarray` module. [Example 11-30](ch11_split_001.xhtml#simple_bloom_filter_implemintation)
    shows a simple Bloom filter implementation.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步提高我们在内存使用方面的效率，我们可以使用单个位来表示`bool`值（本地`bool`实际上占据4位）。我们可以通过使用`bitarray`模块轻松实现这一点。[示例11-30](ch11_split_001.xhtml#simple_bloom_filter_implemintation)展示了一个简单的Bloom过滤器实现。
- en: Example 11-30\. Simple Bloom filter implementation
  id: totrans-315
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例11-30。简单的Bloom过滤器实现
- en: '[PRE41]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: What happens if we insert more items than we specified for the capacity of the
    Bloom filter? At the extreme end, all the items in the `bool` list will be set
    to `True`, in which case we say that we have seen every item. This means that
    Bloom filters are very sensitive to what their initial capacity was set to, which
    can be quite aggravating if we are dealing with a set of data whose size is unknown
    (for example, a stream of data).
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们插入的项目比Bloom过滤器的容量指定的要多会发生什么？在极端情况下，`bool`列表中的所有项目都将设置为`True`，在这种情况下，我们说我们已经见过每个项目。这意味着Bloom过滤器对其初始容量设置非常敏感，如果我们正在处理一个大小未知的数据集（例如数据流），这可能会非常令人恼火。
- en: One way of dealing with this is to use a variant of Bloom filters called *scalable*
    Bloom filters.^([7](ch11_split_001.xhtml#idm46122398750824)) They work by chaining
    together multiple Bloom filters whose error rates vary in a specific way.^([8](ch11_split_001.xhtml#idm46122398488968))
    By doing this, we can guarantee an overall error rate and add a new Bloom filter
    when we need more capacity. To check if we’ve seen an item before, we iterate
    over all of the sub-Blooms until either we find the object or we exhaust the list.
    A sample implementation of this structure can be seen in [Example 11-31](ch11_split_001.xhtml#memory_scaling_bloom),
    where we use the previous Bloom filter implementation for the underlying functionality
    and have a counter to simplify knowing when to add a new Bloom.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 处理这个问题的一种方法是使用一种称为*可伸缩*的Bloom过滤器的变体。^([7](ch11_split_001.xhtml#idm46122398750824))
    它们通过将多个Bloom过滤器链接在一起，这些过滤器的错误率以特定的方式变化。^([8](ch11_split_001.xhtml#idm46122398488968))
    通过这样做，我们可以保证整体的错误率，并在需要更多容量时添加一个新的Bloom过滤器。要检查我们以前是否见过一个项目，我们需要遍历所有的子Bloom，直到我们找到该对象或者我们用尽了列表。这种结构的一个示例实现可以在[示例11-31](ch11_split_001.xhtml#memory_scaling_bloom)中看到，我们在基础功能中使用了之前的Bloom过滤器实现，并有一个计数器来简化知道何时添加新的Bloom。
- en: Example 11-31\. Simple scaling Bloom filter implementation
  id: totrans-319
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例11-31。简单的Bloom过滤器实现
- en: '[PRE42]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Another way of dealing with this is using a method called *timing Bloom filters*.
    This variant allows elements to be expired out of the data structure, thus freeing
    up space for more elements. This is especially nice for dealing with streams,
    since we can have elements expire after, say, an hour and have the capacity set
    large enough to deal with the amount of data we see per hour. Using a Bloom filter
    this way would give us a nice view into what has been happening in the last hour.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 处理这个问题的另一种方法是使用一种称为*定时Bloom过滤器*的方法。这种变体允许元素从数据结构中过期，从而为更多元素释放空间。这对处理流非常方便，因为我们可以让元素在一个小时后过期，并且将容量设置得足够大，以处理每小时所见到的数据量。这样使用Bloom过滤器会给我们一个对过去一小时发生了什么的良好视图。
- en: 'Using this data structure will feel much like using a `set` object. In the
    following interaction, we use the scalable Bloom filter to add several objects,
    test if we’ve seen them before, and then try to experimentally find the false
    positive rate:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种数据结构会感觉很像使用`set`对象。在以下交互中，我们使用可伸缩的Bloom过滤器添加了几个对象，测试我们以前是否见过它们，然后尝试实验性地找出误报率：
- en: '[PRE43]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We can also do unions with Bloom filters to join multiple sets of items:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用Bloom过滤器对多个项目集进行并集操作：
- en: '[PRE44]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[![1](Images/1.png)](ch11_split_001.xhtml#co_using_less_ram_CO2-1)'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](ch11_split_001.xhtml#co_using_less_ram_CO2-1)'
- en: The value of `51` is not in `bloom_a`.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 值`51`不在`bloom_a`中。
- en: '[![2](Images/2.png)](ch11_split_001.xhtml#co_using_less_ram_CO2-2)'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](ch11_split_001.xhtml#co_using_less_ram_CO2-2)'
- en: Similarly, the value of `24` is not in `bloom_b`.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，值`24`不在`bloom_b`中。
- en: '[![3](Images/3.png)](ch11_split_001.xhtml#co_using_less_ram_CO2-3)'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](ch11_split_001.xhtml#co_using_less_ram_CO2-3)'
- en: However, the `bloom` object contains all the objects in both `bloom_a` and `bloom_b`!
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，`bloom`对象包含了`bloom_a`和`bloom_b`中的所有对象！
- en: One caveat is that you can take the union of only two Blooms with the same capacity
    and error rate. Furthermore, the final Bloom’s used capacity can be as high as
    the sum of the used capacities of the two Blooms unioned to make it. This means
    that you could start with two Bloom filters that are a little more than half full
    and, when you union them together, get a new Bloom that is over capacity and not
    reliable!
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 一个注意事项是，只能对两个容量和误差率相同的布隆过滤器执行并集。此外，最终布隆过滤器的使用容量可以高达将两个并集成它的布隆过滤器的使用容量之和。这意味着你可以从两个稍微超过一半满的布隆过滤器开始，并将它们联合起来，得到一个超过容量且不可靠的新布隆过滤器！
- en: Note
  id: totrans-333
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: A [Cuckoo filter](https://oreil.ly/oD6UM) is a modern Bloom filter–like data
    structure that provides similar functionality to a Bloom filter with the addition
    of better object deletion. Furthermore, the Cuckoo filter in most cases has lower
    overhead, leading to better space efficiencies than the Bloom filter. When a fixed
    number of objects needs to be kept track of, it is often a better option. However,
    its performance degrades dramatically when its load limit is reached and there
    are no options for automatic scaling of the data structure (as we saw with the
    scaling Bloom filter).
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '[布谷鸟过滤器](https://oreil.ly/oD6UM)是一种现代布隆过滤器类似的数据结构，提供了与布隆过滤器类似的功能，并且具有更好的对象删除功能。此外，大多数情况下，布谷鸟过滤器的开销更低，导致比布隆过滤器更好的空间效率。当需要跟踪固定数量的对象时，它通常是一个更好的选择。然而，当其负载限制达到并且没有数据结构自动缩放选项时（就像我们看到的缩放布隆过滤器），其性能会急剧下降。'
- en: The work of doing fast set inclusion in a memory-efficient way is a very important
    and active part of database research. Cuckoo filters, [Bloomier filters](https://arxiv.org/abs/0807.0928),
    [Xor filters](https://arxiv.org/abs/1912.08258), and more are being constantly
    released. However, for most applications, it is still best to stick with the well-known,
    well-supported Bloom filter.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在内存高效方式中进行快速集合包含的工作是数据库研究中非常重要和活跃的部分。布谷鸟过滤器、[布隆曼过滤器](https://arxiv.org/abs/0807.0928)、[XOR
    过滤器](https://arxiv.org/abs/1912.08258)等不断被发布。然而，对于大多数应用程序，最好还是坚持使用众所周知、得到良好支持的布隆过滤器。
- en: LogLog Counter
  id: totrans-336
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LogLog 计数器
- en: '[LogLog-type counters](http://bit.ly/LL-type_counters) are based on the realization
    that the individual bits of a hash function can also be considered random. That
    is to say, the probability of the first bit of a hash being `1` is 50%, the probability
    of the first two bits being `01` is 25%, and the probability of the first three
    bits being `001` is 12.5%. Knowing these probabilities, and keeping the hash with
    the most `0`s at the beginning (i.e., the least probable hash value), we can come
    up with an estimate of how many items we’ve seen so far.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '[LogLog 类型计数器](http://bit.ly/LL-type_counters)基于以下认识：哈希函数的每个比特也可以被视为随机的。也就是说，哈希的第一个比特为`1`的概率为50%，前两个比特为`01`的概率为25%，前三个比特为`001`的概率为12.5%。通过了解这些概率，并保留具有最多`0`的哈希（即最不可能的哈希值），我们可以估算出到目前为止我们见过多少项。'
- en: A good analogy for this method is flipping coins. Imagine we would like to flip
    a coin 32 times and get heads every time. The number 32 comes from the fact that
    we are using 32-bit hash functions. If we flip the coin once and it comes up tails,
    we will store the number `0`, since our best attempt yielded 0 heads in a row.
    Since we know the probabilities behind this coin flip, we can also tell you that
    our longest series was `0` long, and you can estimate that we’ve tried this experiment
    `2^0 = 1` time. If we keep flipping our coin and we’re able to get 10 heads before
    getting a tail, then we would store the number `10`. Using the same logic, you
    could estimate that we’ve tried the experiment `2^10 = 1024` times. With this
    system, the highest we could count would be the maximum number of flips we consider
    (for 32 flips, this is `2^32 = 4,294,967,296`).
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的一个很好的类比是抛硬币。想象一下我们想抛硬币32次并每次都得到正面。数字32来自于我们使用32位哈希函数的事实。如果我们抛一次硬币得到反面，我们会记录数字`0`，因为我们最好的尝试没有一次得到正面。由于我们知道这个硬币翻转背后的概率，我们也可以告诉你我们最长的系列是`0`，你可以估算我们已经尝试了这个实验`2^0
    = 1`次。如果我们继续抛硬币并在得到反面之前得到10次正面，那么我们会记录数字`10`。使用相同的逻辑，你可以估算我们尝试了`2^10 = 1024`次实验。使用这个系统，我们能够计数的最高数字将是我们考虑的最大抛硬币次数（32次抛硬币的情况下是`2^32
    = 4,294,967,296`）。
- en: 'To encode this logic with LogLog-type counters, we take the binary representation
    of the hash value of our input and see how many `0`s there are before we see our
    first `1`. The hash value can be thought of as a series of 32 coin flips, where
    `0` means a flip for heads and `1` means a flip for tails (i.e., `000010101101`
    means we flipped four heads before our first tails, and `010101101` means we flipped
    one head before flipping our first tail). This gives us an idea of how many tries
    happened before this hash value was reached. The mathematics behind this system
    is almost equivalent to that of the Morris counter, with one major exception:
    we acquire the “random” values by looking at the actual input instead of using
    a random number generator. This means that if we keep adding the same value to
    a LogLog counter, its internal state will not change. [Example 11-32](ch11_split_001.xhtml#simple_implementation_of_loglog_register)
    shows a simple implementation of a LogLog counter.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 要用 LogLog 类型计数器来编码这个逻辑，我们取输入的哈希值的二进制表示，看看第一个 `1` 之前有多少个 `0`。可以将哈希值视为一系列32次硬币翻转，其中
    `0` 表示正面，`1` 表示反面（即 `000010101101` 意味着在第一个反面前我们翻了四次正面，而 `010101101` 意味着在第一个反面前我们翻了一次正面）。这给我们一个概念，即在达到这个哈希值之前发生了多少次尝试。这个系统背后的数学几乎等同于
    Morris 计数器，但有一个主要的例外：我们通过查看实际输入来获取“随机”值，而不是使用随机数生成器。这意味着如果我们持续向 LogLog 计数器添加相同的值，其内部状态不会改变。[Example 11-32](ch11_split_001.xhtml#simple_implementation_of_loglog_register)
    展示了 LogLog 计数器的简单实现。
- en: Example 11-32\. Simple implementation of LogLog register
  id: totrans-340
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-32\. LogLog 寄存器的简单实现
- en: '[PRE45]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The biggest drawback of this method is that we may get a hash value that increases
    the counter right at the beginning and skews our estimates. This would be similar
    to flipping 32 tails on the first try. To remedy this, we should have many people
    flipping coins at the same time and combine their results. The law of large numbers
    tells us that as we add more and more flippers, the total statistics become less
    affected by anomalous samples from individual flippers. The exact way that we
    combine the results is the root of the difference between LogLog-type methods
    (classic LogLog, SuperLogLog, HyperLogLog, HyperLogLog++, etc.).
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的最大缺点是，我们可能会得到一个在一开始就增加计数器的哈希值，从而扭曲我们的估计。这类似于第一次尝试就翻了32次反面。为了解决这个问题，我们应该让多人同时翻硬币并结合他们的结果。大数定律告诉我们，随着我们增加越来越多的翻转器，总体统计数据受单个翻转器的异常样本影响越少。我们组合结果的确切方式是
    LogLog 类型方法（经典 LogLog、SuperLogLog、HyperLogLog、HyperLogLog++ 等）之间差异的根源。
- en: We can accomplish this “multiple flipper” method by taking the first couple
    of bits of a hash value and using that to designate which of our flippers had
    that particular result. If we take the first 4 bits of the hash, this means we
    have `2^4 = 16` flippers. Since we used the first 4 bits for this selection, we
    have only 28 bits left (corresponding to 28 individual coin flips per coin flipper),
    meaning each counter can count only up to `2^28 = 268,435,456`. In addition, there
    is a constant (alpha) that depends on the number of flippers, which normalizes
    the estimation.^([9](ch11_split_001.xhtml#idm46122397477912)) All of this together
    gives us an algorithm with <math alttext="1.05 slash StartRoot m EndRoot"><mrow><mn>1</mn>
    <mo>.</mo> <mn>05</mn> <mo>/</mo> <msqrt><mi>m</mi></msqrt></mrow></math> accuracy,
    where *m* is the number of registers (or flippers) used. [Example 11-33](ch11_split_001.xhtml#simple_implementation_of_loglog_example11-26)
    shows a simple implementation of the LogLog algorithm.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过取哈希值的前几位来实现这种“多次翻转”方法，并使用它来指定哪一个翻转器具有特定的结果。如果我们取哈希的前4位，这意味着我们有 `2^4 =
    16` 个翻转器。由于我们用前4位进行选择，剩下的28位（每个翻转器对应28次独立的硬币翻转）意味着每个计数器只能计数到 `2^28 = 268,435,456`。此外，还有一个常数（alpha），它取决于翻转器的数量，用于归一化估计。^([9](ch11_split_001.xhtml#idm46122397477912))
    所有这些组合在一起给了我们一个具有 <math alttext="1.05 slash StartRoot m EndRoot"><mrow><mn>1</mn>
    <mo>.</mo> <mn>05</mn> <mo>/</mo> <msqrt><mi>m</mi></msqrt></mrow></math> 精度的算法，其中
    *m* 是使用的寄存器（或翻转器）的数量。[Example 11-33](ch11_split_001.xhtml#simple_implementation_of_loglog_example11-26)
    展示了 LogLog 算法的简单实现。
- en: Example 11-33\. Simple implementation of LogLog
  id: totrans-344
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-33\. LogLog 的简单实现
- en: '[PRE46]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: In addition to this algorithm deduplicating similar items by using the hash
    value as an indicator, it has a tunable parameter that can be used to dial whatever
    sort of accuracy versus storage compromise you are willing to make.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用哈希值作为指示符来去重类似项之外，这个算法还有一个可调参数，用于调节您愿意做的精度与存储折衷之间的平衡。
- en: In the `__len__` method, we are averaging the estimates from all of the individual
    LogLog registers. This, however, is not the most efficient way to combine the
    data! This is because we may get some unfortunate hash values that make one particular
    register spike up while the others are still at low values. Because of this, we
    are able to achieve an error rate of only <math alttext="upper O left-parenthesis
    StartFraction 1.30 Over StartRoot m EndRoot EndFraction right-parenthesis"><mrow><mi>O</mi>
    <mo>(</mo> <mfrac><mrow><mn>1</mn><mo>.</mo><mn>30</mn></mrow> <msqrt><mi>m</mi></msqrt></mfrac>
    <mo>)</mo></mrow></math> , where *m* is the number of registers used.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `__len__` 方法中，我们对所有单独的 LogLog 寄存器的估计进行了平均。然而，这并不是组合数据的最有效方式！这是因为我们可能会得到一些不幸的哈希值，使得一个特定的寄存器数值急剧上升，而其他寄存器仍然保持较低数值。因此，我们只能实现
    <math alttext="upper O left-parenthesis StartFraction 1.30 Over StartRoot m EndRoot
    EndFraction right-parenthesis"><mrow><mi>O</mi> <mo>(</mo> <mfrac><mrow><mn>1</mn><mo>.</mo><mn>30</mn></mrow>
    <msqrt><mi>m</mi></msqrt></mfrac> <mo>)</mo></mrow></math> 的错误率，其中 *m* 是使用的寄存器数量。
- en: SuperLogLog was devised as a fix to this problem.^([10](ch11_split_001.xhtml#idm46122397252728))
    With this algorithm, only the lowest 70% of the registers were used for the size
    estimate, and their value was limited by a maximum value given by a restriction
    rule. This addition decreased the error rate to <math alttext="upper O left-parenthesis
    StartFraction 1.05 Over StartRoot m EndRoot EndFraction right-parenthesis"><mrow><mi>O</mi>
    <mo>(</mo> <mfrac><mrow><mn>1</mn><mo>.</mo><mn>05</mn></mrow> <msqrt><mi>m</mi></msqrt></mfrac>
    <mo>)</mo></mrow></math> . This is counterintuitive, since we got a better estimate
    by disregarding information!
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: SuperLogLog 被设计为解决这个问题。^([10](ch11_split_001.xhtml#idm46122397252728)) 使用这种算法时，仅使用寄存器中最低的
    70% 进行尺寸估算，并且它们的值受限于一个限制规则给出的最大值。这种添加将错误率降低到 <math alttext="upper O left-parenthesis
    StartFraction 1.05 Over StartRoot m EndRoot EndFraction right-parenthesis"><mrow><mi>O</mi>
    <mo>(</mo> <mfrac><mrow><mn>1</mn><mo>.</mo><mn>05</mn></mrow> <msqrt><mi>m</mi></msqrt></mfrac>
    <mo>)</mo></mrow></math> 。这是反直觉的，因为通过忽略信息，我们得到了更好的估计！
- en: 'Finally, HyperLogLog came out in 2007 and gave us further accuracy gains.^([11](ch11_split_001.xhtml#idm46122397244632))
    It did so by changing the method of averaging the individual registers: instead
    of just averaging, we use a spherical averaging scheme that also has special considerations
    for different edge cases the structure could be in. This brings us to the current
    best error rate of <math alttext="upper O left-parenthesis StartFraction 1.04
    Over StartRoot m EndRoot EndFraction right-parenthesis"><mrow><mi>O</mi> <mo>(</mo>
    <mfrac><mrow><mn>1</mn><mo>.</mo><mn>04</mn></mrow> <msqrt><mi>m</mi></msqrt></mfrac>
    <mo>)</mo></mrow></math> . In addition, this formulation removes a sorting operation
    that is necessary with SuperLogLog. This can greatly speed up the performance
    of the data structure when you are trying to insert items at a high volume. [Example 11-34](ch11_split_001.xhtml#example_11-27)
    shows a basic implementation of HyperLogLog.'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，HyperLogLog 在 2007 年问世，并为我们带来了进一步的准确性提升。^([11](ch11_split_001.xhtml#idm46122397244632))
    它通过改变个体寄存器的平均方法来实现这一点：不再仅仅是平均，而是使用了一个球形平均方案，还考虑了结构可能处于的不同边缘情况。这使得我们达到了当前最佳的 <math
    alttext="upper O left-parenthesis StartFraction 1.04 Over StartRoot m EndRoot
    EndFraction right-parenthesis"><mrow><mi>O</mi> <mo>(</mo> <mfrac><mrow><mn>1</mn><mo>.</mo><mn>04</mn></mrow>
    <msqrt><mi>m</mi></msqrt></mfrac> <mo>)</mo></mrow></math> 错误率。此外，这种公式还移除了超级LogLog
    中必需的排序操作。当你尝试高速插入项目时，这可以极大地提升数据结构的性能。[示例 11-34](ch11_split_001.xhtml#example_11-27)
    展示了 HyperLogLog 的基本实现。
- en: Example 11-34\. Simple implementation of HyperLogLog
  id: totrans-350
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-34\. HyperLogLog 的简单实现
- en: '[PRE47]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: The only further increase in accuracy was given by the HyperLogLog++ algorithm,
    which increases the accuracy of the data structure while it is relatively empty.
    When more items are inserted, this scheme reverts to standard HyperLogLog. This
    is actually quite useful, since the statistics of the LogLog-type counters require
    a lot of data to be accurate—having a scheme for allowing better accuracy with
    fewer number items greatly improves the usability of this method. This extra accuracy
    is achieved by having a smaller but more accurate HyperLogLog structure that can
    later be converted into the larger structure that was originally requested. Also,
    some empirically derived constants are used in the size estimates that remove
    biases.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一进一步提高准确性的方法是使用 HyperLogLog++ 算法，它在数据结构相对空时增加了准确性。当插入更多项目时，此方案会恢复到标准 HyperLogLog。这实际上非常有用，因为
    LogLog 类型计数器的统计需要大量数据才能准确——使用一种允许在较少项目时提高准确性的方案极大地改进了这种方法的可用性。通过具有更小但更精确的 HyperLogLog
    结构来实现额外的准确性，稍后可以将其转换为最初请求的较大结构。此外，大小估计中使用了一些经验推导的常数以消除偏差。
- en: Real-World Example
  id: totrans-353
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 现实世界示例
- en: To obtain a better understanding of the data structures, we first created a
    dataset with many unique keys, and then one with duplicate entries. Figures [11-9](ch11_split_001.xhtml#FIG-prob-ds-comparison_repeating)
    and [11-10](ch11_split_001.xhtml#FIG-prob-ds-comparison_unique) show the results
    when we feed these keys into the data structures we’ve just looked at and periodically
    query, “How many unique entries have there been?” We can see that the data structures
    that contain more stateful variables (such as HyperLogLog and `KMinValues`) do
    better, since they more robustly handle bad statistics. On the other hand, the
    Morris counter and the single LogLog register can quickly have very high error
    rates if one unfortunate random number or hash value occurs. For most of the algorithms,
    however, we know that the number of stateful variables is directly correlated
    with the error guarantees, so this makes sense.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解数据结构，我们首先创建了一个具有许多唯一键的数据集，然后创建了一个带有重复条目的数据集。图表 [11-9](ch11_split_001.xhtml#FIG-prob-ds-comparison_repeating)
    和 [11-10](ch11_split_001.xhtml#FIG-prob-ds-comparison_unique) 展示了当我们将这些键输入到刚刚查看的数据结构中并定期查询“有多少个唯一条目？”时的结果。我们可以看到，包含更多状态变量的数据结构（如
    HyperLogLog 和 `KMinValues`）表现更好，因为它们更稳健地处理了不良统计。另一方面，如果出现一个不幸的随机数或哈希值，Morris 计数器和单个
    LogLog 寄存器很快就会有非常高的误差率。然而，对于大多数算法来说，我们知道状态变量的数量与误差保证直接相关，这是有道理的。
- en: '![hpp2 1109](Images/hpp2_1109.png)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
  zh: '![hpp2 1109](Images/hpp2_1109.png)'
- en: Figure 11-9\. The approximate count of duplicate data using multiple probabilistic
    data structures. To do this, we generate 60,000 items with many duplicates and
    insert them into the various probabilistic data structures. Graphed is the structures
    prediction of the number of unique items during the process.
  id: totrans-356
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图表 11-9\. 使用多个概率数据结构近似计算重复数据的数量。为此，我们生成了 60,000 个具有许多重复项的项目，并将它们插入到各种概率数据结构中。图表展示了这些数据结构在处理过程中唯一项目数量的预测。
- en: Looking just at the probabilistic data structures that have the best performance
    (which really are the ones you will probably use), we can summarize their utility
    and their approximate memory usage (see [Table 11-2](ch11_split_001.xhtml#memory_pd_comparison)).
    We can see a huge change in memory usage depending on the questions we care to
    ask. This simply highlights the fact that when using a probabilistic data structure,
    you must first consider what questions you really need to answer about the dataset
    before proceeding. Also note that only the Bloom filter’s size depends on the
    number of elements. The sizes of the HyperLogLog and `KMinValues` structures are
    sizes dependent *only* on the error rate.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 仅看性能最佳的概率数据结构（这些可能是你真正会用到的），我们可以总结它们的实用性及其大致的内存使用情况（参见[表 11-2](ch11_split_001.xhtml#memory_pd_comparison)）。我们可以看到内存使用量因关心的问题而有巨大变化。这简单地突显了使用概率数据结构时，你必须首先考虑对数据集真正需要回答的问题。同时要注意，只有布隆过滤器的大小依赖于元素数量。HyperLogLog
    和 `KMinValues` 结构的大小仅依赖于误差率。
- en: As another, more realistic test, we chose to use a dataset derived from the
    text of Wikipedia. We ran a very simple script in order to extract all single-word
    tokens with five or more characters from all articles and store them in a newline-separated
    file. The question then was, “How many unique tokens are there?” The results can
    be seen in [Table 11-3](ch11_split_001.xhtml#memory_pd_wiki_comparison).
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 作为另一个更为现实的测试，我们选择使用从维基百科文本派生的数据集。我们运行了一个非常简单的脚本，以提取所有文章中长度为五个或更多字符的单词令牌，并将它们存储在一个以换行符分隔的文件中。然后的问题是，“有多少个唯一的令牌？”结果可以在[表11-3](ch11_split_001.xhtml#memory_pd_wiki_comparison)中看到。
- en: Table 11-2\. Comparison of major probabilistic data structures and the set operations
    available on them
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 表11-2。主要概率数据结构及其上可用的集合操作对比
- en: '|  | Size | Union^([a](ch11_split_001.xhtml#idm46122397003048)) | Intersection
    | Contains | Size^([b](ch11_split_001.xhtml#idm46122397001176)) |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '|  | 大小 | 联合^([a](ch11_split_001.xhtml#idm46122397003048)) | 交集 | 包含 | 大小^([b](ch11_split_001.xhtml#idm46122397001176))
    |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| HyperLogLog | Yes ( <math alttext="upper O left-parenthesis StartFraction
    1.04 Over StartRoot m EndRoot EndFraction right-parenthesis"><mrow><mi>O</mi>
    <mo>(</mo> <mfrac><mrow><mn>1</mn><mo>.</mo><mn>04</mn></mrow> <msqrt><mi>m</mi></msqrt></mfrac>
    <mo>)</mo></mrow></math> ) | Yes | No^([c](ch11_split_001.xhtml#memory_caveat))
    | No | 2.704 MB |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| HyperLogLog | 是（ <math alttext="upper O left-parenthesis StartFraction 1.04
    Over StartRoot m EndRoot EndFraction right-parenthesis"><mrow><mi>O</mi> <mo>(</mo>
    <mfrac><mrow><mn>1</mn><mo>.</mo><mn>04</mn></mrow> <msqrt><mi>m</mi></msqrt></mfrac>
    <mo>)</mo></mrow></math> ） | 是 | 否^([c](ch11_split_001.xhtml#memory_caveat)) |
    否 | 2.704 MB |'
- en: '| KMinValues | Yes ( <math alttext="upper O left-parenthesis StartRoot StartFraction
    2 Over pi left-parenthesis m minus 2 right-parenthesis EndFraction EndRoot right-parenthesis"><mrow><mi>O</mi>
    <mo>(</mo> <msqrt><mfrac><mn>2</mn> <mrow><mi>π</mi><mo>(</mo><mi>m</mi><mo>-</mo><mn>2</mn><mo>)</mo></mrow></mfrac></msqrt>
    <mo>)</mo></mrow></math> ) | Yes | Yes | No | 20.372 MB |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| KMinValues | 是（ <math alttext="upper O left-parenthesis StartRoot StartFraction
    2 Over pi left-parenthesis m minus 2 right-parenthesis EndFraction EndRoot right-parenthesis"><mrow><mi>O</mi>
    <mo>(</mo> <msqrt><mfrac><mn>2</mn> <mrow><mi>π</mi><mo>(</mo><mi>m</mi><mo>-</mo><mn>2</mn><mo>)</mo></mrow></mfrac></msqrt>
    <mo>)</mo></mrow></math> ） | 是 | 是 | 否 | 20.372 MB |'
- en: '| Bloom filter | Yes ( <math alttext="upper O left-parenthesis StartFraction
    0.78 Over StartRoot m EndRoot EndFraction right-parenthesis"><mrow><mi>O</mi>
    <mo>(</mo> <mfrac><mrow><mn>0</mn><mo>.</mo><mn>78</mn></mrow> <msqrt><mi>m</mi></msqrt></mfrac>
    <mo>)</mo></mrow></math> ) | Yes | No^([c](ch11_split_001.xhtml#memory_caveat))
    | Yes | 197.8 MB |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| 布隆过滤器 | 是（ <math alttext="upper O left-parenthesis StartFraction 0.78 Over
    StartRoot m EndRoot EndFraction right-parenthesis"><mrow><mi>O</mi> <mo>(</mo>
    <mfrac><mrow><mn>0</mn><mo>.</mo><mn>78</mn></mrow> <msqrt><mi>m</mi></msqrt></mfrac>
    <mo>)</mo></mrow></math> ） | 是 | 否^([c](ch11_split_001.xhtml#memory_caveat)) |
    是 | 197.8 MB |'
- en: '| ^([a](ch11_split_001.xhtml#idm46122397003048-marker)) Union operations occur
    without increasing the error rate.^([b](ch11_split_001.xhtml#idm46122397001176-marker))
    Size of data structure with 0.05% error rate, 100 million unique elements, and
    using a 64-bit hashing function.^([c](ch11_split_001.xhtml#memory_caveat-marker))
    These operations *can* be done but at a considerable penalty in terms of accuracy.
    |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| ^([a](ch11_split_001.xhtml#idm46122397003048-marker)) 联合操作在不增加错误率的情况下进行。^([b](ch11_split_001.xhtml#idm46122397001176-marker))
    数据结构的大小，具有0.05%的错误率，1亿个唯一元素，并使用64位哈希函数。^([c](ch11_split_001.xhtml#memory_caveat-marker))
    这些操作*可以*完成，但在精度上会受到相当大的惩罚。 |'
- en: 'The major takeaway from this experiment is that if you are able to specialize
    your code, you can get amazing speed and memory gains. This has been true throughout
    the entire book: when we specialized our code in [“Selective Optimizations: Finding
    What Needs to Be Fixed”](ch06_split_001.xhtml#matrix_selective_optimizations),
    we were similarly able to get speed increases.'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实验的主要收获是，如果您能够专门优化您的代码，您可以获得惊人的速度和内存增益。这在整本书中都是真实的：当我们在[“选择性优化：找到需要修复的问题”](ch06_split_001.xhtml#matrix_selective_optimizations)中专门优化我们的代码时，我们同样能够获得速度提升。
- en: '![hpp2 1110](Images/hpp2_1110.png)'
  id: totrans-367
  prefs: []
  type: TYPE_IMG
  zh: '![hpp2 1110](Images/hpp2_1110.png)'
- en: Figure 11-10\. The approximate count of unique data using multiple probabilistic
    data structures. To do this, we insert the numbers 1 through 100,000 into the
    data structures. Graphed is the structures prediction of the number of unique
    items during the process.
  id: totrans-368
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-10。使用多种概率数据结构的唯一数据近似计数。为此，我们将数字1到100,000插入到数据结构中。图中显示了在过程中唯一项目数量的结构预测。
- en: Probabilistic data structures are an algorithmic way of specializing your code.
    We store only the data we need in order to answer specific questions with given
    error bounds. By having to deal with only a subset of the information given, we
    can not only make the memory footprint much smaller, but also perform most operations
    over the structure faster.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 概率数据结构是一种算法方式，可以专门化您的代码。我们仅存储需要的数据，以便以给定的误差界限回答特定问题。通过只处理给定信息的子集，我们不仅可以将内存占用量大大减小，而且还可以更快地执行大多数操作。
- en: Table 11-3\. Size estimates for the number of unique words in Wikipedia
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11-3\. Wikipedia 中唯一单词数量的大小估计
- en: '|  | Elements | Relative error | Processing time^([a](ch11_split_001.xhtml#idm46122396962408))
    | Structure size^([b](ch11_split_001.xhtml#idm46122396961256)) |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '|  | 元素 | 相对误差 | 处理时间^([a](ch11_split_001.xhtml#idm46122396962408)) | 结构大小^([b](ch11_split_001.xhtml#idm46122396961256))
    |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Morris counter^([c](ch11_split_001.xhtml#idm46122396958776)) | 1,073,741,824
    | 6.52% | 751s | 5 bits |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| 莫里斯计数器^([c](ch11_split_001.xhtml#idm46122396958776)) | 1,073,741,824 | 6.52%
    | 751s | 5 位 |'
- en: '| LogLog register | 1,048,576 | 78.84% | 1,690s | 5 bits |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| LogLog 寄存器 | 1,048,576 | 78.84% | 1,690s | 5 位 |'
- en: '| LogLog | 4,522,232 | 8.76% | 2,112s | 5 bits |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| LogLog | 4,522,232 | 8.76% | 2,112s | 5 位 |'
- en: '| HyperLogLog | 4,983,171 | –0.54% | 2,907s | 40 KB |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| HyperLogLog | 4,983,171 | –0.54% | 2,907s | 40 KB |'
- en: '| KMinValues | 4,912,818 | 0.88% | 3,503s | 256 KB |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| KMinValues | 4,912,818 | 0.88% | 3,503s | 256 KB |'
- en: '| Scaling Bloom | 4,949,358 | 0.14% | 10,392s | 11,509 KB |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| Scaling Bloom | 4,949,358 | 0.14% | 10,392s | 11,509 KB |'
- en: '| True value | 4,956,262 | 0.00% | ----- | 49,558 KB^([d](ch11_split_001.xhtml#idm46122396935256))
    |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| 真实值 | 4,956,262 | 0.00% | ----- | 49,558 KB^([d](ch11_split_001.xhtml#idm46122396935256))
    |'
- en: '| ^([a](ch11_split_001.xhtml#idm46122396962408-marker)) Processing time has
    been adjusted to remove the time to read the dataset from disk. We also use the
    simple implementations provided earlier for testing.^([b](ch11_split_001.xhtml#idm46122396961256-marker))
    Structure size is theoretical given the amount of data since the implementations
    used were not optimized.^([c](ch11_split_001.xhtml#idm46122396958776-marker))
    Since the Morris counter doesn’t deduplicate input, the size and relative error
    are given with regard to the total number of values.^([d](ch11_split_001.xhtml#idm46122396935256-marker))
    The dataset is 49,558 KB considering only unique tokens, or 8.742 GB with all
    tokens. |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| ^([a](ch11_split_001.xhtml#idm46122396962408-marker)) 处理时间已经调整，以排除从磁盘读取数据集的时间。我们还使用了早期提供的简单实现进行测试。^([b](ch11_split_001.xhtml#idm46122396961256-marker))
    结构大小是基于数据量的理论值，因为使用的实现并没有经过优化。^([c](ch11_split_001.xhtml#idm46122396958776-marker))
    由于莫里斯计数器不会对输入进行去重，因此给出的大小和相对误差是针对总值的。^([d](ch11_split_001.xhtml#idm46122396935256-marker))
    数据集仅考虑唯一令牌时为 49,558 KB，或者包括所有令牌时为 8.742 GB。|'
- en: As a result, whether or not you use probabilistic data structures, you should
    always keep in mind what questions you are going to be asking of your data and
    how you can most effectively store that data in order to ask those specialized
    questions. This may come down to using one particular type of list over another,
    using one particular type of database index over another, or maybe even using
    a probabilistic data structure to throw out all but the relevant data!
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，无论您是否使用概率数据结构，您都应该始终牢记您将要对数据进行何种问题，以及如何最有效地存储该数据以便提出这些专门的问题。这可能涉及使用一种特定类型的列表而不是另一种，使用一种特定类型的数据库索引而不是另一种，或者甚至使用概率数据结构来丢弃除相关数据之外的所有数据！
- en: ^([1](ch11_split_000.xhtml#idm46122400697944-marker)) This example is taken
    from the Wikipedia article on the [deterministic acyclic finite state automaton](https://oreil.ly/M_pYe)
    (DAFSA). DAFSA is another name for DAWG. The accompanying image is from Wikimedia
    Commons.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch11_split_000.xhtml#idm46122400697944-marker)) 这个例子摘自维基百科上关于[确定性无环有限状态自动机](https://oreil.ly/M_pYe)（DAFSA）的文章。DAFSA
    是 DAWG 的另一个名称。附带的图片来自维基媒体共享资源。
- en: ^([2](ch11_split_001.xhtml#idm46122399754392-marker)) These numbers come from
    the 68-95-99.7 rule of Gaussian distributions. More information can be found in
    the [Wikipedia entry](http://bit.ly/Gaussian).
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch11_split_001.xhtml#idm46122399754392-marker)) 这些数字来自于高斯分布的 68-95-99.7
    法则。更多信息可以在 [Wikipedia 条目](http://bit.ly/Gaussian) 中找到。
- en: ^([3](ch11_split_001.xhtml#idm46122399671816-marker)) A more fully fleshed-out
    implementation that uses an `array` of bytes to make many counters is available
    at [*https://github.com/ianozsvald/morris_counter*](https://github.com/ianozsvald/morris_counter).
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch11_split_001.xhtml#idm46122399671816-marker)) 一个更完整的实现使用一个 `array` 字节来创建多个计数器，可以在
    [*https://github.com/ianozsvald/morris_counter*](https://github.com/ianozsvald/morris_counter)
    上找到。
- en: '^([4](ch11_split_001.xhtml#idm46122399315416-marker)) Kevin Beyer et al., “On
    Synopses for Distinct-Value Estimation under Multiset Operations,” in *Proceedings
    of the 2007 ACM SIGMOD International Conference on Management of Data* (New York:
    ACM, 2007), 199–210, [*https://doi.org/10.1145/1247480.1247504*](https://doi.org/10.1145/1247480.1247504).'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch11_split_001.xhtml#idm46122399315416-marker)) Kevin Beyer等人在《Proceedings
    of the 2007 ACM SIGMOD International Conference on Management of Data》中提到，“On
    Synopses for Distinct-Value Estimation under Multiset Operations”（纽约：ACM，2007），199–210，[*https://doi.org/10.1145/1247480.1247504*](https://doi.org/10.1145/1247480.1247504)。
- en: '^([5](ch11_split_001.xhtml#idm46122398960952-marker)) Burton H. Bloom, “Space/Time
    Trade-Offs in Hash Coding with Allowable Errors,” *Communications of the ACM*
    13, no. 7 (1970): 422–26, [*http://doi.org/10.1145/362686.362692*](http://doi.org/10.1145/362686.362692).'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch11_split_001.xhtml#idm46122398960952-marker)) Burton H. Bloom在《Communications
    of the ACM》13卷7期（1970年）的文章中提到，“Space/Time Trade-Offs in Hash Coding with Allowable
    Errors”，422–26，[*http://doi.org/10.1145/362686.362692*](http://doi.org/10.1145/362686.362692)。
- en: ^([6](ch11_split_001.xhtml#idm46122398840536-marker)) The [Wikipedia page on
    Bloom filters](http://bit.ly/Bloom_filter) has a very simple proof for the properties
    of a Bloom filter.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch11_split_001.xhtml#idm46122398840536-marker)) [Bloom filters的Wikipedia页面](http://bit.ly/Bloom_filter)中对Bloom过滤器的性质有一个非常简单的证明。
- en: ^([7](ch11_split_001.xhtml#idm46122398750824-marker)) Paolo Sérgio Almeida et
    al., “Scalable Bloom Filters,” *Information Processing Letters* 101, no. 6 (2007)
    255–61, [*https://doi.org/10.1016/j.ipl.2006.10.007*](https://doi.org/10.1016/j.ipl.2006.10.007).
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch11_split_001.xhtml#idm46122398750824-marker)) Paolo Sérgio Almeida等人在《Information
    Processing Letters》101卷6期（2007年）的文章中提到，“Scalable Bloom Filters”，255–61，[*https://doi.org/10.1016/j.ipl.2006.10.007*](https://doi.org/10.1016/j.ipl.2006.10.007)。
- en: ^([8](ch11_split_001.xhtml#idm46122398488968-marker)) The error values actually
    decrease like the geometric series. This way, when you take the product of all
    the error rates, it approaches the desired error rate.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch11_split_001.xhtml#idm46122398488968-marker)) 错误值实际上像几何级数一样递减。这样，当你取所有错误率的乘积时，它接近所需的错误率。
- en: ^([9](ch11_split_001.xhtml#idm46122397477912-marker)) A full description of
    the basic LogLog and SuperLogLog algorithms can be found at [*http://bit.ly/algorithm_desc*](http://bit.ly/algorithm_desc).
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch11_split_001.xhtml#idm46122397477912-marker)) 关于基本的LogLog和SuperLogLog算法的完整描述可参考[*http://bit.ly/algorithm_desc*](http://bit.ly/algorithm_desc)。
- en: '^([10](ch11_split_001.xhtml#idm46122397252728-marker)) Marianne Durand and
    Philippe Flajolet, “LogLog Counting of Large Cardinalities,” in *Algorithms—ESA
    2003*, ed. Giuseppe Di Battista and Uri Zwick, vol. 2832 (Berlin, Heidelberg:
    Springer, 2003), 605–17, [*https://doi.org/10.1007/978-3-540-39658-1_55*](https://doi.org/10.1007/978-3-540-39658-1_55).'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch11_split_001.xhtml#idm46122397252728-marker)) Marianne Durand和Philippe
    Flajolet在《Algorithms—ESA 2003》中提到，“LogLog Counting of Large Cardinalities”，由Giuseppe
    Di Battista和Uri Zwick编辑，卷2832（柏林，海德堡：施普林格，2003），605–17，[*https://doi.org/10.1007/978-3-540-39658-1_55*](https://doi.org/10.1007/978-3-540-39658-1_55)。
- en: '^([11](ch11_split_001.xhtml#idm46122397244632-marker)) Philippe Flajolet et
    al., “HyperLogLog: The Analysis of a Near-Optimal Cardinality Estimation Algorithm,”
    in *AOFA ’07: Proceedings of the 2007 International Conference on Analysis of
    Algorithms*, (AOFA, 2007), 127–46.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '^([11](ch11_split_001.xhtml#idm46122397244632-marker)) Philippe Flajolet等人在《AOFA
    ’07: Proceedings of the 2007 International Conference on Analysis of Algorithms》中提到，“HyperLogLog:
    The Analysis of a Near-Optimal Cardinality Estimation Algorithm”（AOFA，2007），127–46。'
