["```py\ndef make_url(idx):\n    page_size = 100\n    start = idx * page_size\n    u = f\"https://api.fda.gov/food/enforcement.json?limit={page_size}&skip={start}\"\n    return u\n\nurls = list(map(make_url, range(0, 10)))\n# Since they are multi-line json we can't use the default \\n line delim\nraw_json = bag.read_text(urls, linedelimiter=\"NODELIM\")\n\ndef clean_records(raw_records):\n    import json\n    # We don't need the meta field just the results field\n    return json.loads(raw_records)[\"results\"]\n\ncleaned_records = raw_json.map(clean_records).flatten()\n# And now we can convert it to a DataFrame\ndf = bag.Bag.to_dataframe(cleaned_records)\n```", "```py\ndef discover_files(path: str):\n    (fs, fspath) = fsspec.core.url_to_fs(path)\n    return (fs, fs.expand_path(fspath, recursive=\"true\"))\n\ndef load_file(fs, file):\n    \"\"\"Load (and initially process) the data.\"\"\"\n    from PyPDF2 import PdfReader\n    try:\n        file_contents = fs.open(file)\n        pdf = PdfReader(file_contents)\n        return (file, pdf.pages[0].extract_text())\n    except Exception as e:\n        return (file, e)\n\ndef load_data(path: str):\n    (fs, files) = discover_files(path)\n    bag_filenames = bag.from_sequence(files)\n    contents = bag_filenames.map(lambda f: load_file(fs, f))\n    return contents\n```", "```py\ndef special_load_function(x):\n    ## Do your special loading logic in this function, like reading a database\n    return [\"Timbit\", \"Is\", \"Awesome\"][0: x % 4]\n\npartitions = bag.from_sequence(range(20), npartitions=5)\nraw_data = partitions.map(special_load_function).flatten()\n```", "```py\ndef parallel_recursive_list(path: str, fs=None) -> List[str]:\n    print(f\"Listing {path}\")\n    if fs is None:\n        (fs, path) = fsspec.core.url_to_fs(path)\n    info = []\n    infos = fs.ls(path, detail=True)\n    # Above could throw PermissionError, but if we can't list the dir it's\n    # probably wrong so let it bubble up\n    files = []\n    dirs = []\n    for i in infos:\n        if i[\"type\"] == \"directory\":\n            # You can speed this up by using futures; covered in Chapter 6\n            dir_list = dask.delayed(parallel_recursive_list)(i[\"name\"], fs=fs)\n            dirs += dir_list\n        else:\n            files.append(i[\"name\"])\n    for sub_files in dask.compute(dirs):\n        files.extend(sub_files)\n    return files\n```", "```py\ndef parallel_list_directories_recursive(path: str, fs=None) -> List[str]:\n    \"\"\"\n Recursively find all the sub-directories.\n \"\"\"\n    if fs is None:\n        (fs, path) = fsspec.core.url_to_fs(path)\n    info = []\n    # Ideally, we could filter for directories here, but fsspec lacks that (for\n    # now)\n    infos = fs.ls(path, detail=True)\n    # Above could throw PermissionError, but if we can't list the dir, it's\n    # probably wrong, so let it bubble up\n    dirs = []\n    result = []\n    for i in infos:\n        if i[\"type\"] == \"directory\":\n            # You can speed this up by using futures; covered in Chapter 6\n            result.append(i[\"name\"])\n            dir_list = dask.delayed(\n                parallel_list_directories_recursive)(i[\"name\"], fs=fs)\n            dirs += dir_list\n    for sub_dirs in dask.compute(dirs):\n        result.extend(sub_dirs)\n    return result\n\ndef list_files(path: str, fs=None) -> List[str]:\n    \"\"\"List files at a given depth with no recursion.\"\"\"\n    if fs is None:\n        (fs, path) = fsspec.core.url_to_fs(path)\n    info = []\n    # Ideally, we could filter for directories here, but fsspec lacks that (for\n    # now)\n    return map(lambda i: i[\"name\"], filter(\n        lambda i: i[\"type\"] == \"directory\", fs.ls(path, detail=True)))\n\ndef parallel_list_large(path: str, npartitions=None, fs=None) -> bag:\n    \"\"\"\n Find all of the files (potentially too large to fit on the head node).\n \"\"\"\n    directories = parallel_list_directories_recursive(path, fs=fs)\n    dir_bag = dask.bag.from_sequence(directories, npartitions=npartitions)\n    return dir_bag.map(lambda dir: list_files(dir, fs=fs)).flatten()\n```"]