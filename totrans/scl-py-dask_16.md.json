["```py\nimport dask\nimport dask.dataframe as dd\nfrom streamz import Stream\nfrom dask.distributed import Client\n\nclient = Client()\n```", "```py\nlocal_stream = Stream.from_iterable(\n    [\"Fight\",\n     \"Flight\",\n     \"Freeze\",\n     \"Fawn\"])\ndask_stream = local_stream.scatter()\n```", "```py\nbatched_kafka_stream = Stream.from_kafka_batched(\n    topic=\"quickstart-events\",\n    dask=True, # Streamz will call scatter internally for us\n    max_batch_size=2, # We want this to run quickly, so small batches\n    consumer_params={\n        'bootstrap.servers': 'localhost:9092',\n        'auto.offset.reset': 'earliest', # Start from the start\n        # Consumer group id\n        # Kafka will only deliver messages once per consumer group\n        'group.id': 'my_special_streaming_app12'},\n    # Note some sources take a string and some take a float :/\n    poll_interval=0.01) \n```", "```py `` ```", "```py\nlocal_wc_stream = (batched_kafka_stream\n                   # .map gives us a per batch view, starmap per elem\n                   .map(lambda batch: map(lambda b: b.decode(\"utf-8\"), batch))\n                   .map(lambda batch: map(lambda e: e.split(\" \"), batch))\n                   .map(list)\n                   .gather()\n                   .flatten().flatten() # We need to flatten twice.\n                   .frequencies()\n                   ) # Ideally, we'd call flatten frequencies before the gather, \n                     # but they don't work on DaskStream\nlocal_wc_stream.sink(lambda x: print(f\"WC {x}\"))\n# Start processing the stream now that we've defined our sinks\nbatched_kafka_stream.start()\n```"]