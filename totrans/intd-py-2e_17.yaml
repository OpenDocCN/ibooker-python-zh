- en: 'Chapter 15\. Data in Time: Processes and Concurrency'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One thing a computer can do that most humans can’t is be sealed up in a cardboard
    box and sit in a warehouse.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Jack Handey
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This chapter and the next two are a bit more challenging than earlier ones.
    In this one we cover data in time (sequential and concurrent access on a single
    computer), and following that we look at data in a box (storage and retrieval
    with special files and databases) in [Chapter 16](ch16.html#ch_databases) and
    then data in space (networking) in [Chapter 17](ch17.html#ch_networks).
  prefs: []
  type: TYPE_NORMAL
- en: Programs and Processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you run an individual program, your operating system creates a single *process*.
    It uses system resources (CPU, memory, disk space) and data structures in the
    operating system’s *kernel* (file and network connections, usage statistics, and
    so on). A process is isolated from other processes—it can’t see what other processes
    are doing or interfere with them.
  prefs: []
  type: TYPE_NORMAL
- en: The operating system keeps track of all the running processes, giving each a
    little time to run and then switching to another, with the twin goals of spreading
    the work around fairly and being responsive to the user. You can see the state
    of your processes with graphical interfaces such as the Mac’s Activity Monitor
    (macOS), Task Manager on Windows-based computers, or the `top` command in Linux.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also access process data from your own programs. The standard library’s
    `os` module provides a common way of accessing some system information. For instance,
    the following functions get the *process ID* and the *current working directory*
    of the running Python interpreter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'And these get my *user ID* and *group ID*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Create a Process with subprocess
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All of the programs that you’ve seen here so far have been individual processes.
    You can start and stop other existing programs from Python by using the standard
    library’s `subprocess` module. If you just want to run another program in a shell
    and grab whatever output it created (both standard output and standard error output),
    use the `getoutput()` function. Here, we get the output of the Unix `date` program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'You won’t get anything back until the process ends. If you need to call something
    that might take a lot of time, see the discussion on *concurrency* in [“Concurrency”](#concurrency).
    Because the argument to `getoutput()` is a string representing a complete shell
    command, you can include arguments, pipes, `<` and `>` I/O redirection, and so
    on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Piping that output string to the `wc` command counts one line, six “words,”
    and 29 characters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'A variant method called `check_output()` takes a list of the command and arguments.
    By default it returns standard output only as type bytes rather than a string,
    and does not use the shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'To show the exit status of the other program, `getstatusoutput()` returns a
    tuple with the status code and output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'If you don’t want to capture the output but might want to know its exit status,
    use `call()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: (In Unix-like systems, `0` is usually the exit status for success.)
  prefs: []
  type: TYPE_NORMAL
- en: That date and time was printed to output but not captured within our program.
    So, we saved the return code as `ret`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can run programs with arguments in two ways. The first is to specify them
    in a single string. Our sample command is `date -u`, which prints the current
    date and time in UTC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: You need that `shell=True` to recognize the command line `date -u`, splitting
    it into separate strings and possibly expanding any wildcard characters such as
    `*` (we didn’t use any in this example).
  prefs: []
  type: TYPE_NORMAL
- en: 'The second method makes a list of the arguments, so it doesn’t need to call
    the shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Create a Process with multiprocessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can run a Python function as a separate process, or even create multiple
    independent processes with the `multiprocessing` module. The sample code in [Example 15-1](#ch15_ex01)
    is short and simple; save it as *mp.py* and then run it by typing `python mp.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: Example 15-1\. mp.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'When I run this, my output looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The `Process()` function spawned a new process and ran the `do_this()` function
    in it. Because we did this in a loop that had four passes, we generated four new
    processes that executed `do_this()` and then exited.
  prefs: []
  type: TYPE_NORMAL
- en: The `multiprocessing` module has more bells and whistles than a clown on a calliope.
    It’s really intended for those times when you need to farm out some task to multiple
    processes to save overall time; for example, downloading web pages for scraping,
    resizing images, and so on. It includes ways to queue tasks, enable intercommunication
    among processes, and wait for all the processes to finish. [“Concurrency”](#concurrency)
    delves into some of these details.
  prefs: []
  type: TYPE_NORMAL
- en: Kill a Process with terminate()
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you created one or more processes and want to terminate one for some reason
    (perhaps it’s stuck in a loop, or maybe you’re bored, or you want to be an evil
    overlord), use `terminate()`. In [Example 15-2](#ch15_ex1), our process would
    count to a million, sleeping at each step for a second, and printing an irritating
    message. However, our main program runs out of patience in five seconds and nukes
    it from orbit.
  prefs: []
  type: TYPE_NORMAL
- en: Example 15-2\. mp2.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'When I run this program, I get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Get System Info with os
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The standard `os` package provides a lot of details on your system, and lets
    you control some of it if you run your Python script as a privileged user (root
    or administrator). Besides file and directory functions that are covered in [Chapter 14](ch14.html#ch_files),
    it has informational functions like these (run on an iMac):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'A useful function is `system()`, which executes a command string as though
    you typed it at a terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: It’s a grab bag. See the [docs](https://oreil.ly/3r6xN) for interesting tidbits.
  prefs: []
  type: TYPE_NORMAL
- en: Get Process Info with psutil
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The third-party package [psutil](https://oreil.ly/pHpJD) also provides system
    and process information for Linux, Unix, macOS, and Windows systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can guess how to install it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Coverage includes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: System
  prefs: []
  type: TYPE_NORMAL
- en: CPU, memory, disk, network, sensors
  prefs: []
  type: TYPE_NORMAL
- en: Processes
  prefs: []
  type: TYPE_NORMAL
- en: id, parent id, CPU, memory, open files, threads
  prefs: []
  type: TYPE_NORMAL
- en: We already saw (in the previous `os` discussion) that my computer has four CPUs.
    How much time (in seconds) have they been using?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: And how busy are they now?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: You might never need this kind of data, but it’s good to know where to look
    if you do.
  prefs: []
  type: TYPE_NORMAL
- en: Command Automation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You often run commands from the shell (either with manually typed commands or
    shell scripts), but Python has more than one good third-party management tool.
  prefs: []
  type: TYPE_NORMAL
- en: A related topic, *task queues*, is discussed in [“Queues”](#queues).
  prefs: []
  type: TYPE_NORMAL
- en: Invoke
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Version 1 of the `fabric` tool let you define local and remote (networked) tasks
    with Python code. The developers split this original package into `fabric2` (remote)
    and `invoke` (local).
  prefs: []
  type: TYPE_NORMAL
- en: 'Install `invoke` by running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: One use of `invoke` is to make functions available as command-line arguments.
    Let’s make a *tasks.py* file with the lines shown in [Example 15-3](#ex1503).
  prefs: []
  type: TYPE_NORMAL
- en: Example 15-3\. tasks.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: (That `ctx` argument is the first argument for each task function, but it’s
    used only internally by `invoke`. It doesn’t matter what you call it, but an argument
    needs to be there.)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the argument `-l` or `--list` to see what tasks are available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Tasks can have arguments, and you can invoke multiple tasks at one time from
    the command line (similar to `&&` use in shell scripts).
  prefs: []
  type: TYPE_NORMAL
- en: 'Other uses include:'
  prefs: []
  type: TYPE_NORMAL
- en: Running local shell commands with the `run()` function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Responding to string output patterns of programs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This was a brief glimpse. See the [docs](http://docs.pyinvoke.org) for all the
    details.
  prefs: []
  type: TYPE_NORMAL
- en: Other Command Helpers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'These Python packages have some similarity to `invoke`, but one or another
    may be a better fit when you need it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[`click`](https://click.palletsprojects.com)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`doit`](http://pydoit.org)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`sh`](http://amoffat.github.io/sh)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`delegator`](https://github.com/kennethreitz/delegator.py)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`pypeln`](https://cgarciae.github.io/pypeln)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The official Python site discusses concurrency in general and [in the standard
    library](http://bit.ly/concur-lib). Those pages have many links to various packages
    and techniques; in this chapter, we show the most useful ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'In computers, if you’re waiting for something, it’s usually for one of two
    reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: I/O bound
  prefs: []
  type: TYPE_NORMAL
- en: This is by far the most common. Computer CPUs are ridiculously fast—hundreds
    of times faster than computer memory and many thousands of times faster than disks
    or networks.
  prefs: []
  type: TYPE_NORMAL
- en: CPU bound
  prefs: []
  type: TYPE_NORMAL
- en: The CPU keeps busy. This happens with *number crunching* tasks such as scientific
    or graphic calculations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two more terms are related to concurrency:'
  prefs: []
  type: TYPE_NORMAL
- en: Synchronous
  prefs: []
  type: TYPE_NORMAL
- en: One thing follows the other, like a line of goslings behind their parents.
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous
  prefs: []
  type: TYPE_NORMAL
- en: Tasks are independent, like random geese splashing down in a pond.
  prefs: []
  type: TYPE_NORMAL
- en: As you progress from simple systems and tasks to real-life problems, you’ll
    need at some point to deal with concurrency. Consider a website, for example.
    You can usually provide static and dynamic pages to web clients fairly quickly.
    A fraction of a second is considered interactive, but if the display or interaction
    takes longer people become impatient. Tests by companies such as Google and Amazon
    showed that traffic drops off quickly if the page loads even a little slower.
  prefs: []
  type: TYPE_NORMAL
- en: But what if you can’t help it when something takes a long time, such as uploading
    a file, resizing an image, or querying a database? You can’t do it within your
    synchronous web server code anymore, because someone’s waiting.
  prefs: []
  type: TYPE_NORMAL
- en: On a single machine, if you want to perform multiple tasks as fast as possible,
    you want to make them independent. Slow tasks shouldn’t block all the others.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter showed earlier how multiprocessing can be used to overlap work
    on a single machine. If you needed to resize an image, your web server code could
    call a separate, dedicated-image resizing process to run asynchronously and concurrently.
    It could scale your application horizontally by invoking multiple resizing processes.
  prefs: []
  type: TYPE_NORMAL
- en: The trick is getting them all to work with one another. Any shared control or
    state means that there will be bottlenecks. An even bigger trick is dealing with
    failures, because concurrent computing is harder than regular computing. Many
    more things can go wrong, and your odds of end-to-end success are lower.
  prefs: []
  type: TYPE_NORMAL
- en: 'All right. What methods can help you to deal with these complexities? Let’s
    begin with a good way to manage multiple tasks: *queues*.'
  prefs: []
  type: TYPE_NORMAL
- en: Queues
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A queue is like a list: things are added at one end and taken away from the
    other. The most common is referred to as *FIFO* (first in, first out).'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that you’re washing dishes. If you’re stuck with the entire job, you
    need to wash each dish, dry it, and put it away. You can do this in a number of
    ways. You might wash the first dish, dry it, and then put it away. You then repeat
    with the second dish, and so on. Or, you might *batch* operations and wash all
    the dishes, dry them all, and then put them away; this assumes you have space
    in your sink and drainer for all the dishes that accumulate at each step. These
    are all synchronous approaches—one worker, one thing at a time.
  prefs: []
  type: TYPE_NORMAL
- en: As an alternative, you could get a helper or two. If you’re the washer, you
    can hand each cleaned dish to the dryer, who hands each dried dish to the put-away-er.
    As long as each of you works at the same pace, you should finish much faster than
    by yourself.
  prefs: []
  type: TYPE_NORMAL
- en: However, what if you wash faster than the dryer dries? Wet dishes either fall
    on the floor, or you pile them up between you and the dryer, or you just whistle
    off-key until the dryer is ready. And if the last person is slower than the dryer,
    dry dishes can end up falling on the floor, or piling up, or the dryer does the
    whistling. You have multiple workers, but the overall task is still synchronous
    and can proceed only as fast as the slowest worker.
  prefs: []
  type: TYPE_NORMAL
- en: '*Many hands make light work*, goes the old saying (I always thought it was
    Amish, because it makes me think of barn building). Adding workers can build a
    barn or do the dishes, faster. This involves *queues*.'
  prefs: []
  type: TYPE_NORMAL
- en: In general, queues transport *messages*, which can be any kind of information.
    In this case, we’re interested in queues for distributed task management, also
    known as *work queues*, *job queues*, or *task queues*. Each dish in the sink
    is given to an available washer, who washes and hands it off to the first available
    dryer, who dries and hands it to a put-away-er. This can be synchronous (workers
    wait for a dish to handle and another worker to whom to give it), or asynchronous
    (dishes are stacked between workers with different paces). As long as you have
    enough workers, and they keep up with the dishes, things move a lot faster.
  prefs: []
  type: TYPE_NORMAL
- en: Processes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can implement queues in many ways. For a single machine, the standard library’s
    `multiprocessing` module (which you saw earlier) contains a `Queue` function.
    Let’s simulate just a single washer and multiple dryer processes (someone can
    put the dishes away later) and an intermediate `dish_queue`. Call this program
    *dishes.py* ([Example 15-4](#ex1504)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 15-4\. dishes.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Run your new program, thusly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This queue looked a lot like a simple Python iterator, producing a series of
    dishes. It actually started up separate processes along with the communication
    between the washer and dryer. I used a `JoinableQueue` and the final `join()`
    method to let the washer know that all the dishes have been dried. There are other
    queue types in the `multiprocessing` module, and you can read the [documentation](http://bit.ly/multi-docs)
    for more examples.
  prefs: []
  type: TYPE_NORMAL
- en: Threads
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A *thread* runs within a process with access to everything in the process, similar
    to a multiple personality. The `multiprocessing` module has a cousin called `threading`
    that uses threads instead of processes (actually, `multiprocessing` was designed
    later as its process-based counterpart). Let’s redo our process example with threads,
    as shown in [Example 15-5](#ex1505).
  prefs: []
  type: TYPE_NORMAL
- en: Example 15-5\. thread1.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s what prints for me:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: We can reproduce our process-based dish example by using threads, as shown in
    [Example 15-6](#ex1506).
  prefs: []
  type: TYPE_NORMAL
- en: Example 15-6\. thread_dishes.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: One difference between `multiprocessing` and `threading` is that `threading`
    does not have a `terminate()` function. There’s no easy way to terminate a running
    thread, because it can cause all sorts of problems in your code, and possibly
    in the space-time continuum itself.
  prefs: []
  type: TYPE_NORMAL
- en: Threads can be dangerous. Like manual memory management in languages such as
    C and C++, they can cause bugs that are extremely hard to find, let alone fix.
    To use threads, all the code in the program (and in external libraries that it
    uses) must be *thread safe*. In the preceding example code, the threads didn’t
    share any global variables, so they could run independently without breaking anything.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that you’re a paranormal investigator in a haunted house. Ghosts roam
    the halls, but none are aware of the others, and at any time, any of them can
    view, add, remove, or move any of the house’s contents.
  prefs: []
  type: TYPE_NORMAL
- en: You’re walking apprehensively through the house, taking readings with your impressive
    instruments. Suddenly you notice that the candlestick you passed seconds ago is
    now missing.
  prefs: []
  type: TYPE_NORMAL
- en: The contents of the house are like the variables in a program. The ghosts are
    threads in a process (the house). If the ghosts only cast spectral glances at
    the house’s contents, there would be no problem. It’s like a thread reading the
    value of a constant or variable without trying to change it.
  prefs: []
  type: TYPE_NORMAL
- en: Yet, some unseen entity could grab your flashlight, blow cold air down your
    neck, put marbles on the stairs, or make the fireplace come ablaze. The *really*
    subtle ghosts would change things in other rooms that you might never notice.
  prefs: []
  type: TYPE_NORMAL
- en: Despite your fancy instruments, you’d have a very hard time figuring out who
    did what, how, when, and where.
  prefs: []
  type: TYPE_NORMAL
- en: If you used multiple processes instead of threads, it would be like having a
    number of houses but with only one (living) person in each. If you put your brandy
    in front of the fireplace, it would still be there an hour later—some lost to
    evaporation, but in the same place.
  prefs: []
  type: TYPE_NORMAL
- en: Threads can be useful and safe when global data is not involved. In particular,
    threads are useful for saving time while waiting for some I/O operation to complete.
    In these cases, they don’t have to fight over data, because each has completely
    separate variables.
  prefs: []
  type: TYPE_NORMAL
- en: But threads do sometimes have good reasons to change global data. In fact, one
    common reason to launch multiple threads is to let them divide up the work on
    some data, so a certain degree of change to the data is expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'The usual way to share data safely is to apply a software *lock* before modifying
    a variable in a thread. This keeps the other threads out while the change is made.
    It’s like having a Ghostbuster guard the room you want to remain unhaunted. The
    trick, though, is that you need to remember to unlock it. Plus, locks can be nested:
    what if another Ghostbuster is also watching the same room, or the house itself?
    The use of locks is traditional but hard to get right.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In Python, threads do not speed up CPU-bound tasks because of an implementation
    detail in the standard Python system called the *Global Interpreter Lock* (GIL).
    This exists to avoid threading problems in the Python interpreter, and can actually
    make a multithreaded program slower than its single-threaded counterpart, or even
    a multi-process version.
  prefs: []
  type: TYPE_NORMAL
- en: 'So for Python, the recommendations are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Use threads for I/O-bound problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use processes, networking, or events (discussed in the next section) for CPU-bound
    problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: concurrent.futures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you’ve just seen, using threads or multiple processes involves a number of
    details. The `concurrent.futures` module was added to the Python 3.2 standard
    library to simplify these. It lets you schedule an asynchronous pool of workers,
    using threads (when I/O-bound) or processes (when CPU-bound). You get back a *future*
    to track their state and collect the results.
  prefs: []
  type: TYPE_NORMAL
- en: '[Example 15-7](#ex1507) contains a test program that you can save as *cf.py*.
    The task function `calc()` sleeps for one second (our way of faking being busy
    with something), calculates the square root of its argument, and returns it. The
    program takes an optional command-line argument of the number of workers to use,
    which defaults to 3. It starts this number of workers in a thread pool, then a
    process pool, and then prints the elapsed times. The `values` list contains five
    numbers, sent to `calc()` one at time in a worker thread or process.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 15-7\. cf.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are some results that I got:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'That one-second `sleep()` forced each worker to take a second for each calculation:'
  prefs: []
  type: TYPE_NORMAL
- en: With only one worker at a time, everything was serial, and the total time was
    more than five seconds.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Five workers matched the size of the values being tested, so we had an elapsed
    time just more than a second.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With three workers, we needed two runs to handle all five values, so two seconds
    elapsed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the program, I ignored the actual `results` (the square roots that we calculated)
    to emphasize the elapsed times. Also, using `map()` to define the pool causes
    us to wait for all workers to finish before returning `results`. If you wanted
    to get each result as it completed, let’s try another test (call it *cf2.py*)
    in which each worker returns the value and its square root as soon as it calculates
    it ([Example 15-8](#ex1508)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 15-8\. cf2.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Our `use_threads()` and `use_processes()` functions are now generator functions
    that call `yield` to return on each iteration. From one run on my machine, you
    can see how the workers don’t always finish `1` through `5` in order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'You can use `concurrent.futures` any time you want to launch a bunch of concurrent
    tasks, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Crawling URLs on the web
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing files, such as resizing images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calling service APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As usual, the [docs](https://oreil.ly/dDdF-) provide additional details, but
    are much more technical.
  prefs: []
  type: TYPE_NORMAL
- en: Green Threads and gevent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you’ve seen, developers traditionally avoid slow spots in programs by running
    them in separate threads or processes. The Apache web server is an example of
    this design.
  prefs: []
  type: TYPE_NORMAL
- en: One alternative is *event-based* programming. An event-based program runs a
    central *event loop*, doles out any tasks, and repeats the loop. The NGINX web
    server follows this design, and is generally faster than Apache.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `gevent` library is event-based and accomplishes a neat trick: you write
    normal imperative code, and it magically converts pieces to *coroutines*. These
    are like generators that can communicate with one another and keep track of where
    they are. `gevent` modifies many of Python’s standard objects such as `socket`
    to use its mechanism instead of blocking. This does not work with Python add-in
    code that was written in C, as some database drivers are.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You install `gevent` by using `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Here’s a variation of [sample code at the `gevent` website](http://www.gevent.org).
    You’ll see the `socket` module’s `gethostbyname()` function in the upcoming DNS
    section. This function is synchronous, so you wait (possibly many seconds) while
    it chases name servers around the world to look up that address. But you could
    use the `gevent` version to look up multiple sites independently. Save this as
    *gevent_test.py* ([Example 15-9](#ex1509)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 15-9\. gevent_test.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: There’s a one-line for-loop in the preceding example. Each hostname is submitted
    in turn to a `gethostbyname()` call, but they can run asynchronously because it’s
    the `gevent` version of `gethostbyname()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run *gevent_test.py*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '`gevent.spawn()` creates a *greenlet* (also known sometimes as a *green thread*
    or a *microthread*) to execute each `gevent.socket.gethostbyname(url)`.'
  prefs: []
  type: TYPE_NORMAL
- en: The difference from a normal thread is that it doesn’t block. If something occurred
    that would have blocked a normal thread, `gevent` switches control to one of the
    other greenlets.
  prefs: []
  type: TYPE_NORMAL
- en: The `gevent.joinall()` method waits for all the spawned jobs to finish. Finally,
    we dump the IP addresses that we got for these hostnames.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of the `gevent` version of `socket`, you can use its evocatively named
    *monkey-patching* functions. These modify standard modules such as `socket` to
    use greenlets rather than calling the `gevent` version of the module. This is
    useful when you want `gevent` to be applied all the way down, even into code that
    you might not be able to access.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the top of your program, add the following call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: This inserts the `gevent` socket everywhere the normal `socket` is called, anywhere
    in your program, even in the standard library. Again, this works only for Python
    code, not libraries written in C.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another function monkey-patches even more standard library modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Use this at the top of your program to get as many `gevent` speedups as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Save this program as *gevent_monkey.py* ([Example 15-9](#ex1509)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 15-10\. gevent_monkey.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, run the program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: There are potential dangers when using `gevent`. As with any event-based system,
    each chunk of code that you execute should be relatively quick. Although it’s
    nonblocking, code that does a lot of work is still slow.
  prefs: []
  type: TYPE_NORMAL
- en: The very idea of monkey-patching makes some people nervous. Yet, many large
    sites such as Pinterest use `gevent` to speed up their sites significantly. Like
    the fine print on a bottle of pills, use `gevent` as directed.
  prefs: []
  type: TYPE_NORMAL
- en: For more examples, see this thorough gevent [tutorial](https://oreil.ly/BWR_q).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You might also want to consider [`tornado`](http://www.tornadoweb.org) or [`gunicorn`](http://gunicorn.org),
    two other popular event-driven frameworks. They provide both the low-level event
    handling and a fast web server. They’re worth a look if you’d like to build a
    fast website without messing with a traditional web server such as Apache.
  prefs: []
  type: TYPE_NORMAL
- en: twisted
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[`twisted`](http://twistedmatrix.com/trac) is an asynchronous, event-driven
    networking framework. You connect functions to events such as data received or
    connection closed, and those functions are called when those events occur. This
    is a *callback* design, and if you’ve written anything in JavaScript, it might
    seem familiar. If it’s new to you, it can seem backwards. For some developers,
    callback-based code becomes harder to manage as the application grows.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You install it by running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '`twisted` is a large package, with support for many internet protocols on top
    of TCP and UDP. To be short and simple, we show a little knock-knock server and
    client, adapted from [twisted examples](http://bit.ly/twisted-ex). First, let’s
    look at the server, *knock_server.py*: ([Example 15-11](#ex1511)).'
  prefs: []
  type: TYPE_NORMAL
- en: Example 15-11\. knock_server.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Now let’s take a glance at its trusty companion, *knock_client.py* ([Example 15-12](#ex1512)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 15-12\. knock_client.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Start the server first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, start the client:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The server and client exchange messages, and the server prints the conversation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Our trickster client then ends, keeping the server waiting for the punch line.
  prefs: []
  type: TYPE_NORMAL
- en: If you’d like to enter the `twisted` passages, try some of the other examples
    from its documentation.
  prefs: []
  type: TYPE_NORMAL
- en: asyncio
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Python added the `asyncio` library in version 3.4. It’s a way of defining concurrent
    code using the new `async` and `await` capabilities. It’s a big topic with many
    details. To avoid overstuffing this chapter, I’ve moved the discussion of `asyncio`
    and related topics to [Appendix C](app03.html#app_async).
  prefs: []
  type: TYPE_NORMAL
- en: Redis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our earlier dishwashing code examples, using processes or threads, were run
    on a single machine. Let’s take another approach to queues that can run on a single
    machine or across a network. Even with multiple singing processes and dancing
    threads, sometimes one machine isn’t enough, You can treat this section as a bridge
    between single-box (one machine) and multiple-box concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: To try the examples in this section, you’ll need a Redis server and its Python
    module. You can see where to get them in [“Redis”](ch16.html#redis). In that chapter,
    Redis’s role is that of a database. Here, we’re featuring its concurrency personality.
  prefs: []
  type: TYPE_NORMAL
- en: A quick way to make a queue is with a Redis list. A Redis server runs on one
    machine; this can be the same one as its clients, or another that the clients
    can access through a network. In either case, clients talk to the server via TCP,
    so they’re networking. One or more provider clients pushes messages onto one end
    of the list. One or more client workers watches this list with a *blocking pop*
    operation. If the list is empty, they all just sit around playing cards. As soon
    as a message arrives, the first eager worker gets it.
  prefs: []
  type: TYPE_NORMAL
- en: Like our earlier process- and thread-based examples, *redis_washer.py* generates
    a sequence of dishes ([Example 15-13](#ex1513)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 15-13\. redis_washer.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The loop generates four messages containing a dish name, followed by a final
    message that says “quit.” It appends each message to a list called `dishes` in
    the Redis server, similar to appending to a Python list.
  prefs: []
  type: TYPE_NORMAL
- en: And as soon as the first dish is ready, *redis_dryer.py* does its work ([Example 15-14](#ex1514)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 15-14\. redis_dryer.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: This code waits for messages whose first token is “dishes” and prints that each
    one is dried. It obeys the *quit* message by ending the loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start the dryer and then the washer. Using the `&` at the end puts the first
    program in the *background*; it keeps running, but doesn’t listen to the keyboard
    anymore. This works on Linux, macOS, and Windows, although you might see different
    output on the next line. In this case (macOS), it’s some information about the
    background dryer process. Then, we start the washer process normally (in the *foreground*).
    You’ll see the mingled output of the two processes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'As soon as dish IDs started arriving at Redis from the washer process, our
    hard-working dryer process started pulling them back out. Each dish ID was a number,
    except the final *sentinel* value, the string `''quit''`. When the dryer process
    read that `quit` dish ID, it quit, and some more background process information
    printed to the terminal (also system dependent). You can use a sentinel (an otherwise
    invalid value) to indicate something special from the data stream itself—in this
    case, that we’re done. Otherwise, we’d need to add a lot more program logic, such
    as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Agreeing ahead of time on some maximum dish number, which would kind of be a
    sentinel anyway.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doing some special *out-of-band* (not in the data stream) interprocess communication.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Timing out after some interval with no new data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s make a few last changes:'
  prefs: []
  type: TYPE_NORMAL
- en: Create multiple `dryer` processes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add a timeout to each dryer rather than looking for a sentinel.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The new *redis_dryer2.py* is shown in [Example 15-15](#ex1515).
  prefs: []
  type: TYPE_NORMAL
- en: Example 15-15\. redis_dryer2.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Start the dryer processes in the background and then the washer process in
    the foreground:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'One dryer process reads the `quit` ID and quits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'After 20 seconds, the other dryer processes get a return value of `None` from
    their `blpop` calls, indicating that they’ve timed out. They say their last words
    and exit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'After the last dryer subprocess quits, the main dryer program ends:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Beyond Queues
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With more moving parts, there are more possibilities for our lovely assembly
    lines to be disrupted. If we need to wash the dishes from a banquet, do we have
    enough workers? What if the dryers get drunk? What if the sink clogs? Worries,
    worries!
  prefs: []
  type: TYPE_NORMAL
- en: 'How will you cope with it all? Common techniques include these:'
  prefs: []
  type: TYPE_NORMAL
- en: Fire and forget
  prefs: []
  type: TYPE_NORMAL
- en: Just pass things on and don’t worry about the consequences, even if no one is
    there. That’s the dishes-on-the-floor approach.
  prefs: []
  type: TYPE_NORMAL
- en: Request-reply
  prefs: []
  type: TYPE_NORMAL
- en: The washer receives an acknowledgment from the dryer, and the dryer from the
    put-away-er, for each dish in the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Back pressure or throttling
  prefs: []
  type: TYPE_NORMAL
- en: This technique directs a fast worker to take it easy if someone downstream can’t
    keep up.
  prefs: []
  type: TYPE_NORMAL
- en: 'In real systems, you need to be careful that workers are keeping up with the
    demand; otherwise, you hear the dishes hitting the floor. You might add new tasks
    to a *pending* list, while some worker process pops the latest message and adds
    it to a *working* list. When the message is done, it’s removed from the working
    list and added to a *completed* list. This lets you know what tasks have failed
    or are taking too long. You can do this with Redis yourself, or use a system that
    someone else has already written and tested. Some Python-based queue packages
    that add this extra level of management include:'
  prefs: []
  type: TYPE_NORMAL
- en: '[`celery`](http://www.celeryproject.org) can execute distributed tasks synchronously
    or asynchronously, using the methods we’ve discussed: `multiprocessing`, `gevent`,
    and others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[rq](http://python-rq.org) is a Python library for job queues, also based on
    Redis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Queues](http://queues.io) offers a discussion of queuing software, Python-based
    and otherwise.'
  prefs: []
  type: TYPE_NORMAL
- en: Coming Up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we flowed data through processes. In the next chapter, you’ll
    see how to store and retrieve data in various file formats and databases.
  prefs: []
  type: TYPE_NORMAL
- en: Things to Do
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 15.1 Use `multiprocessing` to create three separate processes. Make each one
    wait a random number of seconds between zero and one, print the current time,
    and then exit.
  prefs: []
  type: TYPE_NORMAL
