- en: Chapter 11\. Machine Learning with Dask
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11章 用Dask进行机器学习
- en: Now that you know Dask’s many different data types, computation patterns, deployment
    options, and libraries, we are ready to tackle machine learning. You will quickly
    find that ML with Dask is quite intuitive to use, as it runs on the same Python
    environment as the many other popular ML libraries. Much of the heavy work is
    done by Dask’s built-in data types and Dask’s distributed schedulers, making writing
    code an enjoyable experience for the user.^([1](ch11.xhtml#id811))
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了Dask的许多不同数据类型、计算模式、部署选项和库，我们准备开始机器学习。你会很快发现，使用Dask进行机器学习非常直观，因为它与许多其他流行的机器学习库运行在相同的Python环境中。Dask的内置数据类型和分布式调度器大部分完成了繁重的工作，使得编写代码成为用户的愉快体验。^([1](ch11.xhtml#id811))
- en: This chapter will primarily use the Dask-ML library, a robustly supported ML
    library from the Dask open source project, but we will also highlight other libraries,
    such as XGBoost and scikit-learn. The Dask-ML library is designed to run both
    in clusters and locally.^([2](ch11.xhtml#id816)) Dask-ML provides familiar interfaces
    by extending many common ML libraries. ML is different from many of the tasks
    discussed so far, as it requires the framework (here Dask-ML) to coordinate work
    more closely. In this chapter we’ll show some of the ways you can use it in your
    own programs, and we’ll also offer tips.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将主要使用Dask-ML库，这是Dask开源项目中得到强力支持的机器学习库，但我们也会突出显示其他库，如XGBoost和scikit-learn。Dask-ML库旨在在集群和本地运行。^([2](ch11.xhtml#id816))
    Dask-ML通过扩展许多常见的机器学习库提供了熟悉的接口。机器学习与我们迄今讨论的许多任务有所不同，因为它需要框架（这里是Dask-ML）更密切地协调工作。在本章中，我们将展示如何在自己的程序中使用它，并提供一些技巧。
- en: Since ML is such a wide and varied discipline, we are able to cover only some
    of the situations where Dask-ML is useful. This chapter will discuss some of the
    common work patterns, such as exploratory data analysis, random split, featurization,
    regression, and deep learning inferences, from a practitioner’s perspective on
    ramping up on Dask. If you don’t see your particular library or use case represented,
    it may still be possible to accelerate with Dask, and you should look at [Dask-ML’s
    API guide](https://oreil.ly/eJGHU). However, ML is not Dask’s primary focus, so
    you may find that you need to use other tools, like Ray.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 由于机器学习是如此广泛和多样化的学科，我们只能涵盖Dask-ML有用的一些情况。本章将讨论一些常见的工作模式，例如探索性数据分析、随机拆分、特征化、回归和深度学习推断，从实践者的角度来看，逐步了解Dask。如果您没有看到您特定的库或用例被涵盖，仍然可能通过Dask加速，您应该查看[Dask-ML的API指南](https://oreil.ly/eJGHU)。然而，机器学习并非Dask的主要关注点，因此您可能需要使用其他工具，如Ray。
- en: Parallelizing ML
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并行化机器学习
- en: 'Many ML workloads face scaling challenges in two dimensions: model size and
    data size. Training models with large features or components, like many deep learning
    models, often become compute-bound, where training, predicting, and evaluating
    the model becomes slow and harder to manage. On the other hand, many ML models,
    even seemingly simple ones like regression, often get stretched to their limits
    with large volumes of training datasets that don’t fit into one machine, getting
    memory-bound in their scaling challenges.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习工作负载在两个维度上面临扩展挑战：模型大小和数据大小。训练具有大特征或组件的模型（例如许多深度学习模型）通常会变得计算受限，其中训练、预测和评估模型变得缓慢和难以管理。另一方面，许多机器学习模型，甚至看似简单的模型如回归模型，通常会在处理大量训练数据集时超出单台机器的限制，从而在扩展挑战中变得内存受限。
- en: On memory-bound workloads, Dask’s high-level collections that we have covered
    (such as Dask array, DataFrame, and bag) combine with Dask-ML libraries to offer
    native scaling. For compute-bound workloads, Dask parallelizes training through
    integrations such as Dask-ML and Dask-joblib. In the case of scikit-learn, Dask
    can manage cluster-wide work allocation, using Dask-joblib. You might notice each
    workflow requires a different library; this is because each ML tool uses its own
    method of parallelization that Dask extends.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在内存受限的工作负载上，我们已经涵盖的Dask的高级集合（如Dask数组、DataFrame和bag）与Dask-ML库结合，提供了本地扩展能力。对于计算受限的工作负载，Dask通过Dask-ML和Dask-joblib等集成实现了训练的并行化。在使用scikit-learn时，Dask可以管理集群范围的工作分配，使用Dask-joblib。你可能会注意到，每个工作流需要不同的库，这是因为每个机器学习工具都使用自己的并行化方法，而Dask扩展了这些方法。
- en: You can use Dask in conjunction with many popular machine learning libraries,
    including scikit-learn and XGBoost. You may already be familiar with single-machine
    parallelism inside your favorite ML library. Dask takes these single-machine frameworks,
    like Dask-joblib, and extends them to machines connected over the network.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将Dask与许多流行的机器学习库一起使用，包括scikit-learn和XGBoost。您可能已经熟悉您喜爱的机器学习库内部的单机并行处理。Dask将这些单机框架（如Dask-joblib）扩展到通过网络连接的多台机器上。
- en: When to Use Dask-ML
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Dask-ML的时机
- en: Dask excels in parallel tasks with limited distributed mutable state (like large
    model weights). Dask is commonly used for inference/predictions on ML models,
    which is simpler than training. Training models, on the other hand, often require
    more inter-worker communication in the form of model weight updates and repeated
    loops, with sometimes variable amounts of computation per training cycle. You
    can use Dask for both of these use cases, but adoption and tooling for training
    is not as widespread.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Dask在具有有限分布式可变状态（如大型模型权重）的并行任务中表现突出。Dask通常用于对机器学习模型进行推断/预测，这比训练要简单。另一方面，训练模型通常需要更多的工作器间通信，例如模型权重更新和重复循环，有时每个训练周期的计算量可能不同。您可以在这两种用例中都使用Dask，但是对于训练的采用和工具支持并不如推断广泛。
- en: Dask’s integration with common data preparation tools—including pandas, NumPy,
    PyTorch, and TensorFlow—makes it easier to build inference pipelines. In JVM-based
    tools, like Spark, working with those libraries comes at a higher overhead.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Dask与常见的数据准备工具（包括pandas、NumPy、PyTorch和TensorFlow）的集成使得构建推断管道变得更加容易。在像Spark这样的基于JVM的工具中，使用这些库会增加更多的开销。
- en: Another great use case for Dask is feature engineering and plotting large datasets
    before training. Dask’s pre-processing functions often use the same signatures,
    and function the same way as scikit-learn, while distributing the work across
    machines. Similarly with plotting and visualization, Dask is able to generate
    a beautiful plot of a large dataset beyond the usual limits of matplotlib/seaborn.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Dask的另一个很好的用例是在训练之前进行特征工程和绘制大型数据集。Dask的预处理函数通常使用与scikit-learn相同的签名和方式，同时跨多台机器分布工作。类似地，对于绘图和可视化，Dask能够生成一个大型数据集的美观图表，超出了matplotlib/seaborn的常规限制。
- en: For more involved ML and deep learning work, some users opt to generate PyTorch
    or TensorFlow models separately and then use the models generated for inference
    workloads using Dask. This keeps the workload on the Dask side embarrassingly
    parallel. Alternatively, some users opt to write the training data as a Dask DataFrame
    using the delayed pattern, which is fed into Keras or Torch. Be warned that there
    is a medium level of effort to do so.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更复杂的机器学习和深度学习工作，一些用户选择单独生成PyTorch或TensorFlow模型，然后使用生成的模型进行基于Dask的推断工作负载。这样可以使Dask端的工作负载具有令人尴尬的并行性。另外，一些用户选择使用延迟模式将训练数据写入Dask
    DataFrame，然后将其馈送到Keras或Torch中。请注意，这样做需要一定的中等工作量。
- en: As discussed in previous chapters, the Dask project is still in the early phase
    of its life, and some of these libraries are still a work in progress and come
    with disclaimers. We took extra caution to validate most of the numerical methods
    used in the Dask-ML library to make sure the logic and mathematics are sound and
    work as expected. However, some dependent libraries come with warnings that it’s
    not yet ready for prime time, especially as it relates to GPU-aware workloads
    and massively distributed workloads. We expect these to get sorted out as the
    community grows and users contribute their feedback.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面章节所述，Dask项目仍处于早期阶段，其中一些库仍在进行中并带有免责声明。我们格外小心地验证了Dask-ML库中使用的大部分数值方法，以确保逻辑和数学是正确的，并按预期工作。然而，一些依赖库有警告称其尚未准备好供主流使用，特别是涉及GPU感知工作负载和大规模分布式工作负载时。我们预计随着社区的增长和用户贡献反馈，这些问题将得到解决。
- en: Getting Started with Dask-ML and XGBoost
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用Dask-ML和XGBoost
- en: Dask-ML is the officially supported ML library for Dask. Here, we will walk
    through the functionalities provided in the Dask-ML API; how it connects Dask,
    pandas, and scikit-learn into its functions; and some differences between Dask
    and its scikit-learn equivalents. Additionally, we will walk through a few XGBoost
    gradient boost integrations. We will primarily use the New York City yellow taxicab
    data we used previously to walk through examples. You can access the dataset directly
    from the [New York City website](https://oreil.ly/lbU5V).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Dask-ML 是 Dask 的官方支持的机器学习库。在这里，我们将介绍 Dask-ML API 提供的功能，以及它如何将 Dask、pandas 和
    scikit-learn 集成到其功能中，以及 Dask 和其 scikit-learn 等效物之间的一些区别。此外，我们还将详细讲解几个 XGBoost
    梯度提升集成案例。我们将主要使用之前用过的纽约市黄色出租车数据进行示例演示。您可以直接从[纽约市网站](https://oreil.ly/lbU5V)访问数据集。
- en: Feature Engineering
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征工程
- en: As with any good data science workflow, we start with clean-up, applying scalers,
    and transforms. Dask-ML has drop-in replacements for most of the pre-processing
    API from scikit-learn, including `StandardScaler`, `PolynomialFeatures`, `MinMax​Scaler`, etc.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 就像任何优秀的数据科学工作流一样，我们从清理、应用缩放器和转换开始。Dask-ML 提供了大部分来自 scikit-learn 的预处理 API 的即插即用替代品，包括
    `StandardScaler`、`PolynomialFeatures`、`MinMaxScaler` 等。
- en: You can pass multiple columns to the transformers, and each will be normalized,
    resulting in a delayed Dask DataFrame that you should call `compute` on.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将多个列传递给转换器，每个列都将被归一化，最终生成一个延迟的 Dask DataFrame，您应该调用 `compute` 方法来执行计算。
- en: In [Example 11-1](#ex_scaling_variables), we scale trip distances, which are
    in miles, and total amount, which is in dollars, to their own scaled variables.
    This is a continuation of the exploratory data analysis we did in [Chapter 4](ch04.xhtml#ch04).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [示例 11-1](#ex_scaling_variables) 中，我们对行程距离（单位为英里）和总金额（单位为美元）进行了缩放，生成它们各自的缩放变量。这是我们在[第四章](ch04.xhtml#ch04)中进行的探索性数据分析的延续。
- en: Example 11-1\. Dask DataFrame pre-processing with `StandardScaler`
  id: totrans-20
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-1\. 使用 `StandardScaler` 对 Dask DataFrame 进行预处理
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: For categorical variables, while there is `OneHotEncoder` in Dask-ML, it’s not
    as efficient or as one-to-one in replacement as its scikit-learn equivalent. At
    this point we recommend using `Categorizer` to encode a categorical dtype.^([3](ch11.xhtml#id823))
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类变量，在 Dask-ML 中虽然有 `OneHotEncoder`，但其效率和一对一替代程度都不及其 scikit-learn 的等效物。因此，我们建议使用
    `Categorizer` 对分类 dtype 进行编码。^([3](ch11.xhtml#id823))
- en: '[Example 11-2](#ex_categorical_variables) shows how you would categorize a
    particular column while preserving the existing DataFrame. We select `payment_type`,
    which is encoded originally as an integer but is really a four-category categorical
    variable. We call Dask-ML’s `Categorizer`, while using pandas’s `CategoricalDtype`
    to give type hints. While Dask does have type inference (e.g., it can auto-infer
    the type), it is always better to be explicit in your program.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 11-2](#ex_categorical_variables) 展示了如何对特定列进行分类，同时保留现有的 DataFrame。我们选择 `payment_type`，它最初被编码为整数，但实际上是一个四类别分类变量。我们调用
    Dask-ML 的 `Categorizer`，同时使用 pandas 的 `CategoricalDtype` 给出类型提示。虽然 Dask 具有类型推断能力（例如，它可以自动推断类型），但在程序中明确指定类型总是更好的做法。'
- en: Example 11-2\. Dask DataFrame pre-processing as categorical variable using Dask-ML
  id: totrans-24
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-2\. 使用 Dask-ML 对 Dask DataFrame 进行分类变量预处理
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Alternatively, you can opt to use Dask DataFrame’s built-in categorizer. While
    pandas is permissive with Object and String as categorical data types, Dask will
    reject these columns unless they are read first as a categorical variable. There
    are two ways you can do this: declare a column as categorical at reading the data,
    with `dtype={col: categorical}`, or convert before invoking `get_dummies`, by
    using `df​.catego⁠rize(“col1”)`. The reasoning here is that Dask is lazily evaluated
    and cannot create a dummy variable out of a column without having a full list
    of unique values seen. Calling `.categorize()` is convenient and allows for dynamic
    handling of additional categories, but keep in mind that it does need to scan
    the entire column first to get the categories then do another full scan to transform
    the column. So if you know the categories already and they won’t change, you should
    just invoke `DummyEncoder`.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '或者，您可以选择使用Dask DataFrame的内置分类器。虽然pandas对Object和String作为分类数据类型是宽容的，但除非首先将这些列读取为分类变量，否则Dask将拒绝这些列。有两种方法可以做到这一点：在读取数据时将列声明为分类，使用`dtype={col:
    categorical}`，或在调用`get_dummies`之前转换，使用`df​.catego⁠rize(“col1”)`。这里的理由是Dask是惰性评估的，不能在没有看到完整唯一值列表的情况下创建列的虚拟变量。调用`.categorize()`很方便，并允许动态处理额外的类别，但请记住，它确实需要先扫描整个列以获取类别，然后再扫描转换列。因此，如果您已经知道类别并且它们不会改变，您应该直接调用`DummyEncoder`。'
- en: '[Example 11-3](#ex_categorical_variables_alt) categorizes multiple columns
    at once. Nothing is materialized until you call `execute`, so you can chain many
    of these pre-processes at once.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[Example 11-3](#ex_categorical_variables_alt) 一次对多列进行分类。在调用`execute`之前，没有任何实质性的东西被实现，因此你可以一次链式地连接许多这样的预处理步骤。'
- en: Example 11-3\. Dask DataFrame pre-processing as categorical variable using the
    Dask DataFrame built-in
  id: totrans-28
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 11-3\. 使用Dask DataFrame内置的分类变量作为预处理
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`DummyEncoder` is the Dask-ML equivalent to scikit-learn’s `OneHotEncoder`,
    which will turn the variables into uint8, an eight-bit unsigned integer, which
    is more memory efficient.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '`DummyEncoder`是Dask-ML中类似于scikit-learn的`OneHotEncoder`，它将变量转换为uint8，即一个8位无符号整数，更节省内存。'
- en: Again, there is a Dask DataFrame function that gives you a similar result. [Example 11-4](#ex_dummy_variables)
    demonstrates this on categorical columns, and [Example 11-5](#ex_datetime_dummy_alt)
    pre-processes datetime. Datetime can be tricky to work with. In this case, Python
    natively deserializes the datetime. Remember to always check datetime conversion
    and apply the necessary transforms beforehand.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，有一个Dask DataFrame函数可以给您类似的结果。[Example 11-4](#ex_dummy_variables) 在分类列上演示了这一点，而[Example 11-5](#ex_datetime_dummy_alt)
    则预处理了日期时间。日期时间可能会带来一些棘手的问题。在这种情况下，Python原生反序列化日期时间。请记住，始终在转换日期时间之前检查并应用必要的转换。
- en: Example 11-4\. Dask DataFrame pre-processing as dummy variable using the Dask
    DataFrame built-in
  id: totrans-32
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 11-4\. 使用Dask DataFrame内置的虚拟变量作为哑变量的预处理
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Example 11-5\. Dask DataFrame pre-processing datetime as dummy variable using
    the Dask DataFrame built-in
  id: totrans-34
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 11-5\. 使用Dask DataFrame内置的虚拟变量作为日期时间的预处理
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Dask-ML’s `train_test_split` method has more flexibility than the Dask DataFrames
    version. Both are partition-aware, and we using them instead of the scikit-learn
    equivalent. scikit-learn’s `train_test_split` can be called here, but it is not
    partition-aware and can result in a large data movement between workers, whereas
    the Dask implementations would split the train-test over each partition, avoiding
    the shuffle (see [Example 11-6](#ex_dask_random_split)).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Dask-ML的`train_test_split`方法比Dask DataFrames版本更灵活。两者都支持分区感知，我们使用它们而不是scikit-learn的等价物。scikit-learn的`train_test_split`可以在此处调用，但它不具备分区感知性，可能导致大量数据在工作节点之间移动，而Dask的实现则会在每个分区上分割训练集和测试集，避免洗牌（参见[Example 11-6](#ex_dask_random_split)）。
- en: Example 11-6\. Dask DataFrame pseudorandom split
  id: totrans-37
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 11-6\. Dask DataFrame伪随机分割
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As a side effect of random splits happening by each partition block, the random
    behavior is not uniformly guaranteed over the whole of the DataFrame. If you suspect
    that some partition may have skews, you should compute, redistribute, and then
    shuffle-split.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 每个分区块的随机分割的副作用是，整个DataFrame的随机行为不能保证是均匀的。如果你怀疑某些分区可能存在偏差，你应该计算、重新分配，然后洗牌分割。
- en: Model Selection and Training
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型选择和训练
- en: Many of scikit-learn’s model-selection-related functions, including cross-validation,
    hyperparameter search, clustering, regression, imputation, and scoring methods,
    are ported into Dask as a drop-in replacement. There are a few noteworthy improvements
    that make the functions run more efficiently than a simple parallel computing
    architecture, by using Dask’s task-graph views.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 很多 scikit-learn 的模型选择相关功能，包括交叉验证、超参数搜索、聚类、回归、填充以及评分方法，都已经作为一个可替换组件移植到 Dask 中。有几个显著的改进使得这些功能比简单的并行计算架构更高效，这些改进利用了
    Dask 的任务图形视图。
- en: Most regression-based models have been implemented for Dask use and can be used
    as a replacement for scikit-learn.^([4](ch11.xhtml#id829)) Many scikit-learn users
    would be familiar with the task of `.reshape()` for pandas, needing them to convert
    a pandas DataFrame into a 2D array in order for scikit-learn to work. For some
    Dask-ML functions you still need to also call `ddf.to_dask_array()` in order to
    convert a DataFrame to an array before training. Lately, some Dask-ML has been
    improved to directly work on Dask DataFrames, but not all libraries.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数基于回归的模型已经为 Dask 实现，并且可以作为 scikit-learn 的替代品使用。^[4](ch11.xhtml#id829) 许多 scikit-learn
    用户熟悉使用 `.reshape()` 对 pandas 进行操作，需要将 pandas DataFrame 转换为二维数组以便 scikit-learn
    使用。对于一些 Dask-ML 的函数，你仍然需要调用 `ddf.to_dask_array()` 将 DataFrame 转换为数组以进行训练。最近，一些
    Dask-ML 已经改进，可以直接在 Dask DataFrames 上工作，但并非所有库都支持。
- en: '[Example 11-7](#linear_regression) runs through a straightforward multi-variate
    linear regression using Dask-ML. Say you want to build a regression model on two
    predictor columns and one output column. You would apply `.to_array()` to convert
    the data type to Dask arrays and then pass them into Dask-ML’s implementation
    of `LinearRegression`. Note how we needed to materialize the conversion into arrays,
    and we gave explicit chunk size. This is because Dask-ML’s underlying implementation
    with linear models is not quite fully able to infer chunk sizes from previous
    steps. We also purposefully use scikit-learn’s scoring library, not Dask-ML. We
    are noticing implementation issues where Dask-ML doesn’t play well with chunk
    sizes.^([5](ch11.xhtml#id833)) Thankfully, at this point, this calculation is
    a reduce step, which works without any Dask-specific logic.^([6](ch11.xhtml#id834))'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 11-7](#linear_regression) 展示了使用 Dask-ML 进行直观的多变量线性回归。假设您希望在两个预测列和一个输出列上构建回归模型。您将应用
    `.to_array()` 将数据类型转换为 Dask 数组，然后将它们传递给 Dask-ML 的 `LinearRegression` 实现。请注意，我们需要明确指定分块大小，这是因为
    Dask-ML 在线性模型的底层实现中并未完全能够从前一步骤中推断出分块大小。我们还特意使用 scikit-learn 的评分库，而不是 Dask-ML。我们注意到
    Dask-ML 在处理分块大小时存在实施问题。^[5](ch11.xhtml#id833) 幸运的是，在这一点上，这个计算是一个简化步骤，它可以在没有任何特定于
    Dask 的逻辑的情况下工作。^[6](ch11.xhtml#id834)'
- en: Example 11-7\. Linear regression with Dask-ML
  id: totrans-44
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-7\. 使用 Dask-ML 进行线性回归
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note that function parameters for models for scikit-learn and Dask-ML are identical,
    but some are not supported as of now. For example, `LogisticRegression` is available
    in Dask-ML, but multi-class solver is not supported, meaning that there is no
    exact equivalent for multi-class solvers implemented in Dask-ML yet. So, if you
    want to use multinomial loss solver newton-cg or newton-cholesky, it might not
    work. For most uses of `LogisticRegression`, a default liblinear solver would
    do the trick. In practice, this limitation would pertain only to more niche and
    advanced use cases.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，scikit-learn 和 Dask-ML 模型的函数参数是相同的，但是目前不支持的一些功能。例如，`LogisticRegression`
    在 Dask-ML 中是可用的，但是不支持多类别求解器，这意味着 Dask-ML 中尚未实现多类别求解器的确切等效。因此，如果您想要使用 multinomial
    loss solver newton-cg 或 newton-cholesky，可能不会起作用。对于大多数 `LogisticRegression` 的用途，缺省的
    liblinear 求解器会起作用。在实践中，此限制仅适用于更为专业和高级的用例。
- en: For hyperparameter search, Dask-ML has the scikit-learn equivalent of `GridSearchCV`
    for exhaustive search over parameter values, and `RandomizedSearchCV` for randomly
    trying hyperparameters from a list. These can be run directly, similar to the
    scikit-learn variant, if the data and resulting model do not require much scaling.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于超参数搜索，Dask-ML 具有与 scikit-learn 类似的 `GridSearchCV` 用于参数值的详尽搜索，以及 `RandomizedSearchCV`
    用于从列表中随机尝试超参数。如果数据和结果模型不需要太多的缩放，可以直接运行这些功能，与 scikit-learn 的变体类似。
- en: Cross-validation and hyperparameter tuning often is a costly process even with
    a smaller dataset, as anyone who has run the scikit-learn cross-validate would
    attest. Dask users often deal with datasets large enough that use of exhaustive
    search algorithms is not feasible. As an alternative, Dask-ML implements several
    additional adaptive algorithms and hyperband-based methods that approach the tuned
    parameter more quickly with robust mathematical foundation.^([7](ch11.xhtml#id837))
    The authors of the `HyperbandSearchCV` methods do ask that the use be cited.^([8](ch11.xhtml#id840))
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证和超参数调优通常是一个昂贵的过程，即使在较小的数据集上也是如此，任何运行过 scikit-learn 交叉验证的人都会证明。Dask 用户通常处理足够大的数据集，使用详尽搜索算法是不可行的。作为替代方案，Dask-ML
    实现了几种额外的自适应算法和基于超带的方法，这些方法通过稳健的数学基础更快地接近调整参数。^([7](ch11.xhtml#id837)) `HyperbandSearchCV`
    方法的作者要求引用使用。^([8](ch11.xhtml#id840))
- en: When There Is No Dask-ML Equivalent
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 当没有 Dask-ML 等效时。
- en: If there is a function that exists in scikit-learn or other data science libraries
    but not in Dask-ML, you can write the distributed version of your desired code.
    After all, Dask-ML can be thought of as a convenience wrapper around scikit-learn.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在 scikit-learn 或其他数据科学库中存在但不在 Dask-ML 中存在的函数，则可以编写所需代码的分布式版本。毕竟，Dask-ML 可以被视为
    scikit-learn 的便利包装器。
- en: '[Example 11-8](#ex_daskml_port) uses scikit-learn’s learning functions `SGDRegressor`
    and `LinearRegression`, and uses `dask.delayed` to wrap the delayed functionality
    around the method. You can do this over any piece of code you may want to parallelize.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[Example 11-8](#ex_daskml_port) 使用 scikit-learn 的学习函数 `SGDRegressor` 和 `LinearRegression`，并使用
    `dask.delayed` 将延迟功能包装在该方法周围。您可以对您想要并行化的任何代码片段执行此操作。'
- en: Example 11-8\. Linear regression with Dask-ML
  id: totrans-52
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 11-8\. 使用 Dask-ML 进行线性回归
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Use with Dask’s joblib
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Dask 的 joblib。
- en: Alternatively, you can use scikit-learn along with joblib (see [Example 11-9](#ex_joblib)),
    a package that can take any Python function as pipelined steps to be computed
    on a single machine. Joblib works well with a lot of parallel computations that
    are not dependent on each other. In this case, having hundreds of cores on a single
    machine would be helpful. While a typical laptop does not have hundreds of cores,
    using the four or so that it does have can still be beneficial. With Dask’s version
    of joblib you can use cores from multiple machines. This can work for ML workloads
    that are compute-bound on a single machine.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以使用 scikit-learn 与 joblib（参见 [Example 11-9](#ex_joblib)），这是一个可以将任何 Python
    函数作为流水线步骤在单台机器上计算的包。Joblib 在许多不依赖于彼此的并行计算中表现良好。在这种情况下，拥有单台机器上的数百个核心将非常有帮助。虽然典型的笔记本电脑没有数百个核心，但使用它所拥有的四个核心仍然是有益的。使用
    Dask 版本的 joblib，您可以使用来自多台机器的核心。这对于在单台机器上计算受限的 ML 工作负载是有效的。
- en: Example 11-9\. Parallelizing computation using joblib
  id: totrans-56
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 11-9\. 使用 joblib 进行计算并行化
- en: '[PRE8]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: XGBoost with Dask
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Dask 的 XGBoost。
- en: XGBoost is a popular Python gradient boosting library, used for parallel tree
    boosting. Well-known gradient boosting methods include bootstrap aggregation (bagging).
    Various gradient boosting methods have been used in high-energy physics data analysis
    at the Large Hadron Collider, used to train deep neural networks to confirm the
    discovery of the Higgs boson. Gradient boosting methods are currently used in
    scientific areas such as geological or climate studies. Given its importance,
    we found XGBoost on Dask-ML to be a well-implemented library, ready for users.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 是一个流行的 Python 梯度增强库，用于并行树增强。众所周知的梯度增强方法包括自举聚合（bagging）。各种梯度增强方法已在大型强子对撞机的高能物理数据分析中使用，用于训练深度神经网络以确认发现希格斯玻色子。梯度增强方法目前在地质或气候研究等科学领域中被广泛使用。考虑到其重要性，我们发现在
    Dask-ML 上的 XGBoost 是一个良好实现的库，可以为用户准备就绪。
- en: Dask-ML has built-in support for XGBoost to work with Dask arrays and DataFrames.
    By using XGBClassifiers within Dask-ML, you will be setting up XGBoost in distributed
    mode, which works with your Dask cluster. When you do so, XGBoost’s master process
    lives in Dask scheduler, and XGBoost’s worker processes will be on Dask’s worker
    processes. The data distribution is handled using Dask DataFrame, split into pandas
    DataFrame, and is talking between Dask worker and XGBoost worker on the same machine.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Dask-ML 具有内置支持以与 Dask 数组和 DataFrame 一起使用 XGBoost。通过在 Dask-ML 中使用 XGBClassifiers，您将在分布式模式下设置
    XGBoost，它可以与您的 Dask 集群一起使用。在这种情况下，XGBoost 的主进程位于 Dask 调度程序中，XGBoost 的工作进程将位于 Dask
    的工作进程上。数据分布使用 Dask DataFrame 处理，拆分为 pandas DataFrame，并在同一台机器上的 Dask 工作进程和 XGBoost
    工作进程之间通信。
- en: XGBoost uses a `DMatrix` (data matrix) as the standard data format it works
    with. XGBoost has a built-in Dask-compatible `DMatrix`, which takes in Dask array
    and Dask DataFrame. Once the Dask environment is set up, the use of gradient booster
    is as you would expect. Specify the learning rate, threads, and objective functions,
    as usual. [Example 11-10](#ex_xgb_basic_usage) works with a Dask CUDA cluster
    and runs a standard gradient booster training.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 使用 `DMatrix`（数据矩阵）作为其标准数据格式。XGBoost 有内置的 Dask 兼容的 `DMatrix`，可以接受 Dask
    数组和 Dask DataFrame。一旦设置了 Dask 环境，梯度提升器的使用就像您期望的那样。像往常一样指定学习率、线程和目标函数。[示例 11-10](#ex_xgb_basic_usage)
    使用 Dask CUDA 集群，并运行标准的梯度提升器训练。
- en: Example 11-10\. Gradient-boosted trees with Dask-ML
  id: totrans-62
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-10\. 使用 Dask-ML 进行梯度提升树
- en: '[PRE9]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In [Example 11-11](#ex_xgb_train_plot_importance), we run a simple training
    run and plot feature importance. Note when we define `DMatrix`, we explicitly
    specify the labels, and the label names are taken from Dask DataFrame into `DMatrix`.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [示例 11-11](#ex_xgb_train_plot_importance) 中，我们进行了简单的训练并绘制了特征重要性图。注意，当我们定义
    `DMatrix` 时，我们明确指定了标签，标签名称来自于 Dask DataFrame 到 `DMatrix`。
- en: Example 11-11\. Dask-ML using the XGBoost library
  id: totrans-65
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-11\. 使用 XGBoost 库的 Dask-ML
- en: '[PRE10]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Putting the previous examples together, you can now compose a function that
    can fit a model, provide early stopping arguments, and also define predictions
    using XGBoost for Dask (see [Example 11-12](#ex_xgb_early_stopping)). These would
    be called in your main client code.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 将之前的示例结合起来，您现在可以编写一个函数，该函数可以适配模型、提供早停参数，并使用 Dask 进行 XGBoost 预测（见 [示例 11-12](#ex_xgb_early_stopping)）。这些将在您的主客户端代码中调用。
- en: Example 11-12\. Gradient-boosted tree training and inference using the Dask
    XGBoost library
  id: totrans-68
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-12\. 使用 Dask XGBoost 库进行梯度提升树训练和推断
- en: '[PRE11]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ML Models with Dask-SQL
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Dask-SQL 中使用 ML 模型
- en: A much newer addition is another library, Dask-SQL, that provides a convenient
    wrapper around simple ML model training workloads. [Example 11-13](#Dask_sql_define_tables)
    loads the same NYC yellow taxicab data as a Dask DataFrame and then registers
    the view to Dask-SQL context.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个较新的添加是 Dask-SQL 库，它提供了一个便捷的包装器，用于简化 ML 模型训练工作负载。 [示例 11-13](#Dask_sql_define_tables)
    加载与之前相同的 NYC 黄色出租车数据作为 Dask DataFrame，然后将视图注册到 Dask-SQL 上下文中。
- en: Example 11-13\. Registering datasets into Dask-SQL
  id: totrans-72
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-13\. 将数据集注册到 Dask-SQL 中
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Dask-SQL implements similar ML SQL language to BigQuery ML, allowing you to
    simply define models, define the training data as a SQL select statement, and
    then run inference on a different select statement as well.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Dask-SQL 实现了类似 BigQuery ML 的 ML SQL 语言，使您能够简单地定义模型，将训练数据定义为 SQL select 语句，然后在不同的
    select 语句上运行推断。
- en: You can define the model with most of the ML models we discussed, and this will
    run the scikit-learn ML models in the background. In [Example 11-14](#Dask_sql_linear_regression),
    we train the `LinearRegression` model we trained earlier, using Dask-SQL. We first
    define the model, telling it to use scikit-learn’s `LinearRegression`, and the
    target column. Then we feed the training data with requisite columns. You can
    inspect the model trained using the `DESCRIBE` statement; then you can see in
    the `FROM PREDICT` statement how the model is used to run inference on another
    SQL-defined dataset.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用我们讨论过的大多数 ML 模型定义模型，并在后台运行 scikit-learn ML 模型。在 [示例 11-14](#Dask_sql_linear_regression)
    中，我们使用 Dask-SQL 训练了之前训练过的 `LinearRegression` 模型。我们首先定义模型，告诉它使用 scikit-learn 的
    `LinearRegression` 和目标列。然后，我们使用必要的列传递训练数据。您可以使用 `DESCRIBE` 语句检查训练的模型；然后您可以在 `FROM
    PREDICT` 语句中看到模型如何在另一个 SQL 定义的数据集上运行推断。
- en: Example 11-14\. Defining, training, and predicting a linear regression on Dask-SQL
  id: totrans-76
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-14\. 在 Dask-SQL 上定义、训练和预测线性回归模型
- en: '[PRE13]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Similarly, as shown in [Example 11-15](#Dask_sql_XGBClassifier), you can run
    classification models, similar to the XGBoost model we have discussed earlier
    using the Dask-ML library.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，如 [示例 11-15](#Dask_sql_XGBClassifier) 所示，您可以使用 Dask-ML 库运行分类模型，类似于我们之前讨论过的
    XGBoost 模型。
- en: Example 11-15\. Defining, training, and predicting a classifier built using
    XGBoost with Dask-SQL
  id: totrans-79
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-15\. 在 Dask-SQL 上使用 XGBoost 定义、训练和预测分类器
- en: '[PRE14]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Inference and Deployment
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推断和部署
- en: Regardless of the libraries chosen to train and validate your model (which could
    be using some of the Dask-ML libraries, or trained without using Dask at all),
    here are some of the considerations to keep in mind when using Dask for model
    inference deployment.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您选择使用哪些库来训练和验证您的模型（可以使用一些 Dask-ML 库，或完全不使用 Dask 训练），在使用 Dask 进行模型推断部署时，需要考虑以下一些事项。
- en: Distributing Data and Models Manually
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 手动分发数据和模型
- en: 'When loading data and pre-trained models to Dask workers, `dask.delayed` is
    the main tool (see [Example 11-16](#dask_delayed_load)). When distributing data,
    you should choose to use Dask’s collections: array and DataFrame. As you recall
    from [Chapter 4](ch04.xhtml#ch04), each Dask DataFrame is made up of a pandas
    DataFrame. This is useful since you can write a method that takes each smaller
    DataFrame and returns a computed output. Custom functions and tasks can also be
    given per partition using Dask DataFrame’s `map​_par⁠titions` function.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当将数据和预训练模型加载到 Dask 工作节点时，`dask.delayed` 是主要工具（参见 [示例 11-16](#dask_delayed_load)）。在分发数据时，您应选择使用
    Dask 的集合：数组和 DataFrame。正如您从 [第四章](ch04.xhtml#ch04) 中记得的那样，每个 Dask DataFrame 由一个
    pandas DataFrame 组成。这非常有用，因为您可以编写一个方法，该方法接受每个较小的 DataFrame，并返回计算输出。还可以使用 Dask
    DataFrame 的 `map_partitions` 函数为每个分区提供自定义函数和任务。
- en: Remember to use delayed notation if you are reading in a large dataset, to delay
    materialization and avoid reading in unnecessarily early.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在读取大型数据集，请记得使用延迟表示法，以延迟实体化并避免过早读取。
- en: Tip
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: '`map_partitions` passes in a row-wise operation that is meant to be fit into
    a serializable code that is marshaled to workers. You can define a custom class
    that handles inference to be called, but a static method needs to be called, not
    an instance-dependent method. We covered this further in [Chapter 4](ch04.xhtml#ch04).'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`map_partitions` 是一种逐行操作，旨在适合序列化代码并封送到工作节点。您可以定义一个处理推理的自定义类来调用，但需要调用静态方法，而不是依赖实例的方法。我们在
    [第四章](ch04.xhtml#ch04) 进一步讨论了这一点。'
- en: Example 11-16\. Loading large files on Dask workers
  id: totrans-88
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-16\. 在 Dask 工作节点上加载大文件
- en: '[PRE15]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Large-Scale Inferences with Dask
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Dask 进行大规模推理
- en: 'When using Dask for inference on scale, you would distribute trained models
    to each worker, and then distribute Dask collections (DataFrame or array) over
    these partitions to work on a portion of the collection at a time, parallelizing
    the workflow. This strategy would work well in a straightforward inference deployment.
    We will cover one of the ways to achieve this: defining the workflow manually
    using `map_partitions`, and then wrapping existing functions with PyTorch or Keras/​Ten⁠sor⁠Flow
    models. For PyTorch-based models, you can wrap Skorch with the model, which allows
    it to be used with the Dask-ML API. For TensorFlow models, you would use SciKeras
    to create a scikit-learn-compatible model, which would allow it to be used for
    Dask-ML. For PyTorch, the dask-pytorch-ddp library from SaturnCloud is currently
    the most widely used. As for Keras and TensorFlow, be aware that while it’s doable,
    there are some issues with TensorFlow not liking some of its threads being moved
    to other workers.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 Dask 进行规模推理时，您会将训练好的模型分发到每个工作节点，然后将 Dask 集合（DataFrame 或数组）分发到这些分区，以便一次处理集合的一部分，从而并行化工作流程。这种策略在简单的推理部署中效果良好。我们将讨论其中一种实现方式：手动定义工作流程，使用
    `map_partitions`，然后用 PyTorch 或 Keras/TensorFlow 模型包装现有函数。对于基于 PyTorch 的模型，您可以使用
    Skorch 将模型包装起来，从而使其能够与 Dask-ML API 一起使用。对于 TensorFlow 模型，您可以使用 SciKeras 创建一个与
    scikit-learn 兼容的模型，这样就可以用于 Dask-ML。对于 PyTorch，SaturnCloud 的 dask-pytorch-ddp 库目前是最广泛使用的。至于
    Keras 和 TensorFlow，请注意，虽然可以做到，但 TensorFlow 不喜欢一些线程被移动到其他工作节点。
- en: The most generic way to deploy inference is using Dask DataFrame’s `map_partitions`
    (see [Example 11-17](#Dask_DataFrame_map_partition_inference)). You can take your
    custom inference function that will be run on each row, with the data mapped over
    each worker by partition.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 部署推理最通用的方式是使用 Dask DataFrame 的 `map_partitions`（参见 [示例 11-17](#Dask_DataFrame_map_partition_inference)）。您可以使用自定义推理函数，在每行上运行该函数，数据映射到每个工作节点的分区。
- en: Example 11-17\. Distributed inference using Dask DataFrame
  id: totrans-93
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-17\. 使用 Dask DataFrame 进行分布式推理
- en: '[PRE16]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: One of the interesting ways that Dask offers more than other scalable libraries
    is flexibility in parallel behavior. In the preceding example, we define a function
    that works row-wise and then give that function to a partition-wise logic that
    will be run by each partition over the entire DataFrame. We can use this as a
    boilerplate to define more fine-grained batched functions (see [Example 11-18](#batched_operations)).
    Keep in mind that behaviors you define within the row-wise function should be
    free of side effects, as in, you should avoid mutating the inputs to the function,
    as is the general best practice in Dask distributed delayed computations. Also,
    as the comments in the preceding example say, if you do .apply within a partition-wise
    lambda, this calls `.apply()` from pandas. Within Pandas, `.apply()` defaults
    to `axis = 0`, so if you want otherwise, you should remember to specify `axis
    = 1`.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Dask提供比其他可扩展库更多的灵活性，特别是在并行行为方面。在前面的示例中，我们定义了一个逐行工作的函数，然后将该函数提供给分区逻辑，每个分区在整个DataFrame上运行。我们可以将其作为样板来定义更精细的批处理函数（见[示例 11-18](#batched_operations)）。请记住，在逐行函数中定义的行为应该没有副作用，即，应避免突变函数的输入，这是Dask分布式延迟计算的一般最佳实践。此外，正如前面示例中的注释所述，如果在分区式lambda内执行`.apply()`，这会调用pandas的`.apply()`。在Pandas中，`.apply()`默认为`axis
    = 0`，如果你想要其他方式，应记得指定`axis = 1`。
- en: Example 11-18\. Distributed inference using Dask DataFrame
  id: totrans-96
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-18\. 使用Dask DataFrame进行分布式推断
- en: '[PRE17]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Conclusion
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this chapter, you have learned how to use the building blocks of Dask to
    write data science and ML workflows, combining core Dask libraries with other
    ML libraries you might be familiar with to achieve your desired task. You have
    also learned how you can use Dask to scale both compute- and memory-bound ML workloads.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您已经学习了如何使用Dask的构建模块来编写数据科学和ML工作流程，将核心Dask库与您可能熟悉的其他ML库结合起来，以实现您所需的任务。您还学习了如何使用Dask来扩展计算和内存密集型ML工作负载。
- en: Dask-ML provides an almost functionally equivalent library to scikit-learn,
    oftentimes calling scikit-learn with the additional awareness of task and data
    parallelism that Dask brings. Dask-ML is actively being developed by the community
    and will evolve to add more use cases and examples. Check the Dask documentation
    for the latest updates.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Dask-ML几乎提供了与scikit-learn功能相当的库，通常使用Dask带来的任务和数据并行意识调用scikit-learn。Dask-ML由社区积极开发，并将进一步增加用例和示例。查阅Dask文档以获取最新更新。
- en: In addition, you have learned methods of parallelizing ML training with models
    from scikit-learn libraries by using joblib for compute-intensive workloads, and
    batched operations for data-intensive workloads, so that you can write any custom
    implementations yourself.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您已经学会了如何通过使用joblib进行计算密集型工作负载的并行化ML训练方法，并使用批处理操作处理数据密集型工作负载，以便自己编写任何定制实现。
- en: Finally, you have learned the use cases for Dask-SQL and its SQL ML statements
    in providing high-level abstraction for model creation, hyperparameter tuning,
    and inference.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你已经学习了Dask-SQL的用例及其SQL ML语句，在模型创建、超参数调整和推断中提供高级抽象。
- en: Since ML can be very computation- and memory-heavy, it’s important to deploy
    your ML work on a correctly configured cluster and monitor the progress and output
    closely. We will cover deployment, profiling, and troubleshooting in the next
    chapter.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 由于ML可能需要大量计算和内存，因此在正确配置的集群上部署您的ML工作并密切监视进度和输出非常重要。我们将在下一章中介绍部署、分析和故障排除。
- en: ^([1](ch11.xhtml#id811-marker)) For those inclined to think that writing data
    engineering code is “fun.”
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch11.xhtml#id811-marker)) 如果你认为编写数据工程代码是“有趣”的人。
- en: ^([2](ch11.xhtml#id816-marker)) This is especially important for non-batch inference,
    where being able to use the same code can be of great benefit.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch11.xhtml#id816-marker)) 这对于非批量推断尤为重要，可以很大程度上提高使用相同代码的便利性。
- en: ^([3](ch11.xhtml#id823-marker)) For performance reasons—at the time of writing,
    Dask’s `OneHotEncoder` calls the `get_dummies` method from pandas, which is a
    slower implementation than scikit-learn’s `OneHotEncoder`. `Categorizer`, on the
    other hand, uses a Dask DataFrame aggregation method to scan through categories
    efficiently.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch11.xhtml#id823-marker)) 由于性能原因，在撰写本文时，Dask的`OneHotEncoder`调用了pandas的`get_dummies`方法，这比scikit-learn的`OneHotEncoder`实现较慢。另一方面，`Categorizer`使用了Dask
    DataFrame的聚合方法，以高效地扫描类别。
- en: ^([4](ch11.xhtml#id829-marker)) Most linear models in Dask-ML use a base implementation
    of the Generalized Linear Model library that has been implemented for Dask. We
    have verified the code for mathematical correctness, but the writers of this library
    have not endorsed the use of their GLM library for prime time yet.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch11.xhtml#id829-marker)) Dask-ML 中的大多数线性模型使用了为 Dask 实现的广义线性模型库的基本实现。我们已经验证了代码的数学正确性，但这个库的作者尚未认可其在主流应用中的使用。
- en: ^([5](ch11.xhtml#id833-marker)) Dask-ML version 2023.3.24; some of the generalized
    linear models used rely on dask-glm 0.1.0.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch11.xhtml#id833-marker)) Dask-ML 版本 2023.3.24；部分广义线性模型依赖于 dask-glm 0.1.0。
- en: ^([6](ch11.xhtml#id834-marker)) Because it’s a simple reduce operation, we don’t
    need to preserve the chunking from previous steps.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch11.xhtml#id834-marker)) 因为这是一个简单的归约操作，我们不需要保留之前步骤中的分块。
- en: ^([7](ch11.xhtml#id837-marker)) Dask-ML’s own documentation has more info on
    adaptive and approximation CV methods implemented and use cases.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch11.xhtml#id837-marker)) Dask-ML 的官方文档提供了有关实现的自适应和近似交叉验证方法以及使用案例的更多信息。
- en: '^([8](ch11.xhtml#id840-marker)) They note in the documentation that the following
    paper should be cited if using this method: S. Sievert, T. Augspurger, and M.
    Rocklin, “Better and Faster Hyperparameter Optimization with Dask,” *Proceedings
    of the 18th Python in Science Conference* (2019), *[*https://doi.org/10.25080/Majora-7ddc1dd1-011*](https://doi.org/10.25080/Majora-7ddc1dd1-011)*.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch11.xhtml#id840-marker)) 他们在文档中指出，如果使用此方法，应引用以下论文：S. Sievert, T. Augspurger,
    和 M. Rocklin, “Better and Faster Hyperparameter Optimization with Dask,” *Proceedings
    of the 18th Python in Science Conference* (2019), *[https://doi.org/10.25080/Majora-7ddc1dd1-011](https://doi.org/10.25080/Majora-7ddc1dd1-011)*.
