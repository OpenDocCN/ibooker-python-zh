- en: Chapter 1\. Understanding the Architecture of Dask DataFrames
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 1 章。理解 Dask DataFrames 的架构
- en: 'Dask DataFrames allow you to scale your pandas workflows. Dask DataFrames overcome
    two key limitations of pandas:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Dask DataFrames 允许您扩展您的 pandas 工作流。Dask DataFrames 克服了 pandas 的两个关键限制：
- en: pandas cannot run on datasets larger than memory
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pandas 无法运行大于内存的数据集
- en: pandas only uses one core when running analyses, which can be slow
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在运行分析时，pandas 只使用一个核心，这可能会很慢。
- en: 'Dask DataFrames are designed to overcome these pandas limitations. They can
    be run on datasets that are larger than memory and use all cores by default for
    fast execution. Here are the key Dask DataFrame architecture components that allow
    for Dask to overcome the limitation of pandas:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Dask DataFrames 的设计目的是克服这些 pandas 的限制。它们可以运行在大于内存的数据集上，并默认使用所有核心进行快速执行。以下是允许
    Dask 克服 pandas 限制的关键 Dask DataFrame 架构组件：
- en: Partitioning data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据分区
- en: Lazy execution
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 懒执行
- en: Not loading all data into memory at once
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不一次性加载所有数据到内存中
- en: Let’s take a look at the pandas architecture first, so we can better understand
    how it’s related to Dask DataFrames.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先查看 pandas 的架构，以便更好地理解它与 Dask DataFrames 的关系。
- en: You’ll need to build some new mental models about distributed processing to
    fully leverage the power of Dask DataFrames. Luckily for pandas programmers, Dask
    was intentionally designed to have similar syntax. pandas programmers just need
    to learn the key differences when working with distributed computing systems to
    make the Dask transition easily.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要构建一些新的关于分布式处理的思维模型，以充分利用 Dask DataFrames 的强大功能。幸运的是，对于 pandas 程序员来说，Dask
    的语法意图设计为具有相似性。只需在使用分布式计算系统时学习关键差异，就能轻松过渡到 Dask。
- en: pandas Architecture
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: pandas 架构
- en: pandas DataFrames are in widespread use today partly because they are easy to
    use, powerful, and efficient. We don’t dig into them deeply in this book, but
    will quickly review some of their key characteristics. First, they contain rows
    and values with an index.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: pandas DataFrames 今天广泛使用部分是因为它们易于使用、功能强大且高效。我们在本书中不深入探讨它们，但将快速回顾一些其关键特征。首先，它们包含具有索引的行和值。
- en: 'Let’s create a pandas DataFrame with `name` and `balance` columns to illustrate:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个带有 `name` 和 `balance` 列的 pandas DataFrame 来说明：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This DataFrame has 4 rows of data, as illustrated in [Figure 1-1](#fig_1_pandas_dataframe_with_four_rows_of_data).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 此 DataFrame 有 4 行数据，如图 [1-1](#fig_1_pandas_dataframe_with_four_rows_of_data)
    所示。
- en: '![pandas DataFrame with four rows of data](Images/understanding_the_architecture_of_dask_dataframes_803261_01.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![带有四行数据的 pandas DataFrame](Images/understanding_the_architecture_of_dask_dataframes_803261_01.png)'
- en: Figure 1-1\. pandas DataFrame with four rows of data
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-1。带有四行数据的 pandas DataFrame
- en: The DataFrame in [Figure 1-2](#fig_2_pandas_dataframe_has_an_index) also has
    an index.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 [1-2](#fig_2_pandas_dataframe_has_an_index) 中的 DataFrame 也有一个索引。
- en: '![pandas DataFrame has an index](Images/understanding_the_architecture_of_dask_dataframes_803261_02.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![带有索引的 pandas DataFrame](Images/understanding_the_architecture_of_dask_dataframes_803261_02.png)'
- en: Figure 1-2\. pandas DataFrame has an index
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-2。pandas DataFrame 具有索引
- en: pandas makes it easy to run analytical queries on the data. It can also be leveraged
    to build complex models and is a great option for small datasets, but does not
    work well for larger datasets. Let’s look at why pandas doesn’t work well for
    bigger datasets.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 可以轻松运行数据的分析查询。它还可用于构建复杂模型，并且对于小数据集是一个很好的选择，但对于较大的数据集效果不佳。让我们看看为什么 pandas
    对于更大的数据集效果不佳。
- en: pandas Limitations
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: pandas 的限制
- en: 'pandas has two key limitations:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 有两个关键限制：
- en: Its DataFrames are limited by the amount of computer memory
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它的 DataFrame 受计算机内存量的限制
- en: Its computations only use a single core, which is slow for large datasets
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它的计算仅使用单个核心，这对于大型数据集来说速度较慢
- en: pandas DataFrames are loaded into the memory of a single computer. The amount
    of data that can be stored in the RAM of a single computer is limited to the size
    of the computer’s RAM. A computer with 8 GB of memory can only hold 8 GB of data
    in memory. In practice, pandas requires the memory to be much larger than the
    dataset size. A 2 GB dataset may require 8 GB of memory for example (the exact
    memory requirement depends on the operations performed and pandas version). [Figure 1-3](#fig_3_dataset_sizes_pandas_can_handle_on_a_computer_with)
    illustrates the types of datasets pandas can handle on a computer with 16 GB of
    RAM.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: pandas DataFrames加载到单台计算机的内存中。单台计算机RAM中可以存储的数据量受限于计算机RAM的大小。例如，具有8 GB内存的计算机只能在内存中存储8
    GB的数据。实际上，pandas要求内存远大于数据集大小。例如，一个2 GB的数据集可能需要8 GB的内存（确切的内存需求取决于执行的操作和pandas版本）。[图1-3](#fig_3_dataset_sizes_pandas_can_handle_on_a_computer_with)说明了16
    GB RAM计算机上pandas可以处理的数据集类型。
- en: '![Dataset sizes pandas can handle on a computer with 16 GB of RAM](Images/understanding_the_architecture_of_dask_dataframes_803261_03.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![16 GB RAM计算机上pandas可以处理的数据集大小](Images/understanding_the_architecture_of_dask_dataframes_803261_03.png)'
- en: Figure 1-3\. Dataset sizes pandas can handle on a computer with 16 GB of RAM
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-3。16 GB RAM计算机上pandas可以处理的数据集大小
- en: Furthermore, pandas does not support parallelism. This means that even if you
    have multiple cores in your CPU, with pandas you are always limited to using only
    one of the CPU cores at a time. And that means you are regularly leaving much
    of your hardware potential untapped (see FIgure 3-4).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，pandas不支持并行处理。这意味着即使CPU中有多个核心，使用pandas时始终限制为一次仅使用一个CPU核心。这意味着您经常未能充分利用硬件潜力（见图3-4）。
- en: '![pandas only uses a single core and don t leverage all available computation
    power](Images/understanding_the_architecture_of_dask_dataframes_803261_04.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![pandas仅使用单个核心，不充分利用所有可用的计算能力](Images/understanding_the_architecture_of_dask_dataframes_803261_04.png)'
- en: Figure 1-4\. pandas only uses a single core and don’t leverage all available
    computation power
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-4。pandas仅使用单个核心，不充分利用所有可用的计算能力
- en: Let’s turn our attention to Dask and see how it’s architected to overcome the
    scaling and performance limitations of pandas.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们转向Dask，看看它是如何设计来克服pandas的扩展性和性能限制的。
- en: How Dask DataFrames Differ from pandas
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dask DataFrames与pandas的不同之处
- en: Dask DataFrames have the same logical structure as pandas DataFrames, and share
    a lot of the same internals, but have a couple of important architectural differences.
    As you can see in [Figure 1-5](#fig_5_each_partition_in_a_dask_dataframe_is_a_pandas_dat),
    pandas stores all data in a single DataFrame, whereas Dask splits up the dataset
    into a bunch of little pandas DataFrames.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Dask DataFrames与pandas DataFrames具有相同的逻辑结构，并共享许多相同的内部结构，但在架构上有几个重要的差异。正如您在[图1-5](#fig_5_each_partition_in_a_dask_dataframe_is_a_pandas_dat)中所看到的，pandas将所有数据存储在单个DataFrame中，而Dask将数据集拆分为许多小的pandas
    DataFrames。
- en: '![Each partition in a Dask DataFrame is a pandas DataFrame](Images/understanding_the_architecture_of_dask_dataframes_803261_05.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![每个Dask DataFrame中的分区都是一个pandas DataFrame](Images/understanding_the_architecture_of_dask_dataframes_803261_05.png)'
- en: Figure 1-5\. Each partition in a Dask DataFrame is a pandas DataFrame
  id: totrans-35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-5。每个Dask DataFrame中的分区都是一个pandas DataFrame
- en: 'Suppose you have a pandas DataFrame with the following contents:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您有一个pandas DataFrame，内容如下：
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This pandas DataFrame can be converted to a Dask DataFrame, which will split
    up the data into a bunch of smaller partitions. Each partition in a Dask DataFrame
    is a pandas DataFrame. A Dask DataFrame consists of a bunch of smaller pandas
    DataFrames.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这个pandas DataFrame可以转换为Dask DataFrame，它将数据拆分为许多较小的分区。每个Dask DataFrame中的分区都是一个pandas
    DataFrame。一个Dask DataFrame由许多较小的pandas DataFrames组成。
- en: Similar to a pandas DataFrame, a Dask DataFrame also has columns, values, and
    an index. Notice that Dask splits up the pandas DataFrame by rows.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 与pandas DataFrame类似，Dask DataFrame也有列、值和索引。请注意，Dask通过行将pandas DataFrame拆分。
- en: Dask DataFrames coordinate many pandas DataFrames in parallel. They arrange
    many pandas dataframes split along the index.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Dask DataFrames可以并行地协调多个pandas DataFrames。它们将许多pandas数据帧按索引拆分排列。
- en: Dask DataFrames don’t have to be in memory at once because the values are now
    split into many different pieces. [Figure 1-6](#fig_6_dataset_sizes_that_dask_dataframes_can_handle)
    shows how Dask DataFrames can load the pieces one at a time, allowing us to compute
    on datasets that are larger than memory.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 因为现在数据分成了许多不同的片段，所以Dask DataFrames不必一次性加载到内存中。[图 1-6](#fig_6_dataset_sizes_that_dask_dataframes_can_handle)展示了Dask
    DataFrames如何逐个加载这些片段，使我们能够处理比内存更大的数据集。
- en: '![Dataset sizes that Dask DataFrames can handle](Images/understanding_the_architecture_of_dask_dataframes_803261_06.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![Dask DataFrames可以处理的数据集大小](Images/understanding_the_architecture_of_dask_dataframes_803261_06.png)'
- en: Figure 1-6\. Dataset sizes that Dask DataFrames can handle
  id: totrans-43
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-6\. Dask DataFrames可以处理的数据集大小
- en: Dask DataFrames can also be processed in parallel because the data is split
    into pieces, which often leads to faster processing. [Figure 1-7](#fig_7_dask_dataframes_run_computations_with_all_availabl)
    shows how each Dask DataFrame partition (which is just a pandas DataFrame) can
    be processed on a separate CPU core.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 因为数据被分成多个片段，所以Dask DataFrames也可以并行处理，这通常导致处理速度更快。[图 1-7](#fig_7_dask_dataframes_run_computations_with_all_availabl)展示了每个Dask
    DataFrame分区（即pandas DataFrame）如何在单独的CPU核心上进行处理。
- en: '![Dask DataFrames run computations with all available cores](Images/understanding_the_architecture_of_dask_dataframes_803261_07.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![Dask DataFrames使用所有可用核心运行计算](Images/understanding_the_architecture_of_dask_dataframes_803261_07.png)'
- en: Figure 1-7\. Dask DataFrames run computations with all available cores
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-7\. Dask DataFrames使用所有可用核心运行计算
- en: However, because not all the data is in memory at once, some operations are
    slower or more complicated. For example, operations like sorting a DataFrame or
    finding a median value can be more difficult. See Chapter 4 for more information
    and best practices.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于并非所有数据同时存在于内存中，因此某些操作可能会变慢或变得更加复杂。例如，像排序DataFrame或查找中位数值这样的操作可能会更加困难。有关更多信息和最佳实践，请参阅第4章。
- en: Example illustrating Dask DataFrame Architectural Components
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例说明了Dask DataFrame的架构组件。
- en: Let’s illustrate the architectural concepts discussed in the previous section
    with a simple code example. We’ll create a pandas DataFrame and then convert it
    to a Dask DataFrame to highlight the differences.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个简单的代码示例来说明前一节讨论的架构概念。我们将创建一个pandas DataFrame，然后将其转换为Dask DataFrame以突显其区别。
- en: 'Here’s the code to create a pandas DataFrame with `col1` and `col2` columns:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是创建带有`col1`和`col2`列的pandas DataFrame的代码：
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now convert the pandas DataFrame into a Dask DataFrame (`ddf`) with two partitions.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在将pandas DataFrame转换为具有两个分区的Dask DataFrame（`ddf`）。
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The data in the Dask DataFrame is split into two partitions because we set `npartitions=2`
    when creating the Dask DataFrame.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Dask DataFrame中的数据被分成两个分区，因为我们在创建Dask DataFrame时设置了`npartitions=2`。
- en: Dask intentionally splits up data into different partitions, so it can run computations
    on the partitions in parallel. Dask’s speed and scalability hinge on its ability
    to break up computations into smaller chunks and run them using all the computational
    cores available on a machine.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Dask故意将数据分成不同的分区，以便可以并行运行分区上的计算。Dask的速度和可伸缩性取决于其能够将计算分解为较小的块并利用机器上所有可用的计算核心来运行它们。
- en: '[Figure 1-8](#fig_8_pandas_dataframe_is_split_into_dask_dataframe_part) illustrates
    how the Dask DataFrame is split into two partitions, each of which is a pandas
    DataFrame:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 1-8](#fig_8_pandas_dataframe_is_split_into_dask_dataframe_part)展示了Dask DataFrame如何分成两个分区，每个分区都是一个pandas
    DataFrame：'
- en: '![pandas DataFrame is split into Dask DataFrame partitions](Images/understanding_the_architecture_of_dask_dataframes_803261_08.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![pandas DataFrame被拆分成Dask DataFrame分区](Images/understanding_the_architecture_of_dask_dataframes_803261_08.png)'
- en: Figure 1-8\. pandas DataFrame is split into Dask DataFrame partitions
  id: totrans-58
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-8\. pandas DataFrame被拆分成Dask DataFrame分区
- en: Dask’s architecuture of splitting up the data also allows for computations to
    be lazily executed.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Dask将数据分成不同分区的架构还允许惰性执行计算。
- en: Lazy Execution
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 惰性执行
- en: 'Dask DataFrames use *lazy* execution, whereas pandas uses *eager* execution.
    Dask will put off running computations till the last minute in contrast with pandas,
    which executes computations immediately. This allows Dask to do two things pandas
    can’t do:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Dask DataFrames使用*惰性*执行，而pandas使用*急切*执行。Dask会推迟运行计算直到最后一刻，与pandas立即执行计算的方式形成对比。这使得Dask可以做到两件pandas无法做到的事情：
- en: Process datasets that are larger than memory
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 处理比内存更大的数据集
- en: Collect as much data as possible about the computation you want to run and then
    optimize the computation for maximum performance.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集关于要运行的计算尽可能多的数据，然后优化计算以实现最大性能。
- en: 'Let’s create a pandas DataFrame and run a filtering operation to demonstrate
    that it runs computations immediately (eager execution). Then let’s run the same
    filtering operation on a Dask DataFrame to observe the lazy execution:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个 pandas DataFrame，并运行一个过滤操作来演示它立即运行计算（急切执行）。然后让我们在 Dask DataFrame 上运行相同的过滤操作，以观察延迟执行：
- en: '[PRE4]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Filter the pandas DataFrame to only include the rows with a `number` value greater
    than 25.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤 pandas DataFrame，仅包括`number`值大于25的行。
- en: '[PRE5]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: pandas immediately executes the computation and returns the result.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 会立即执行计算并返回结果。
- en: 'Let’s convert the pandas DataFrame to a Dask DataFrame with two partitions:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将 pandas DataFrame 转换为具有两个分区的 Dask DataFrame：
- en: '[PRE6]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now let’s run the same filtering operation on the Dask DataFrame and see that
    no actual results are returned:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在 Dask DataFrame 上运行相同的过滤操作，并看到实际上没有返回任何结果：
- en: '[PRE7]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here’s what’s output:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出的内容：
- en: '[PRE8]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Dask doesn’t run the filtering operation computations unless you explicitly
    ask for results. In this case, you’ve just asked Dask to filter and haven’t asked
    for results to be returned, and that’s why the resulting Dask DataFrame doesn’t
    contain data yet. pandas users find Dask’s lazy execution strange at first, and
    it takes them a while to get used to explicitly requesting results (rather than
    eagerly receiving results).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 除非显式请求结果，否则 Dask 不会运行过滤操作的计算。在这种情况下，你只是要求 Dask 进行过滤，但并没有要求返回结果，这就是为什么生成的 Dask
    DataFrame 目前不包含数据。pandas 用户起初会觉得 Dask 的延迟执行很奇怪，需要一段时间才能习惯显式请求结果（而不是急切地接收结果）。
- en: 'In this case, you can get results by calling the `compute()` method which tells
    Dask to execute the filtering operation and collect the results in a pandas DataFrame:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，您可以通过调用`compute()`方法来获取结果，该方法告诉 Dask 执行过滤操作并在 pandas DataFrame 中收集结果：
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Let’s turn our attention to another key architectural difference between Dask
    DataFrames and pandas.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将注意力转向 Dask DataFrames 和 pandas 之间的另一个关键架构差异。
- en: Dask DataFrame Divisions
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dask DataFrame 的分区
- en: The pandas index is key to many performant operations like time series operations,
    efficient joins, finding specific values quickly, and so on. Dask DataFrames don’t
    store the entire pandas index in memory, but they do track index ranges for each
    partition, called *divisions*.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 索引是许多高性能操作的关键，如时间序列操作、高效的连接、快速查找特定值等等。Dask DataFrames 不会将整个 pandas 索引存储在内存中，但它们确实跟踪每个分区的索引范围，称为*分区*。
- en: '[Figure 1-9](#fig_9_dask_dataframe_that_s_partitioned_by_month) shows an example
    of a Dask DataFrame that’s partitioned by month and divisions that are stored
    in the Dask DataFrame. Dask divisions track the starting index value of each partition
    as well as the ending index value of the last partition.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 1-9](#fig_9_dask_dataframe_that_s_partitioned_by_month) 显示了一个按月分区的 Dask
    DataFrame 示例，以及存储在 Dask DataFrame 中的分区。Dask 分区跟踪每个分区的起始索引值以及最后一个分区的结束索引值。'
- en: '![Dask DataFrame that s partitioned by month](Images/understanding_the_architecture_of_dask_dataframes_803261_09.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![按月分区的 Dask DataFrame](Images/understanding_the_architecture_of_dask_dataframes_803261_09.png)'
- en: Figure 1-9\. Dask DataFrame that’s partitioned by month
  id: totrans-83
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-9\. 按月分区的 Dask DataFrame
- en: Divisions are key to Dask DataFrames in much the same way that the pandas index
    is critical to pandas DataFrames. Chapter 4 will show you how good tracking of
    index/division information can lead to greatly improved performance.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 分区对于 Dask DataFrames 非常重要，正如 pandas 索引对于 pandas DataFrames 的重要性一样。第四章将向您展示良好的索引/分区信息跟踪如何显著提高性能。
- en: '[Figure 1-10](#fig_10_dask_dataframe_with_divisions_by_partition) looks at
    the same Dask DataFrame from earlier and explore the DataFrame divisions in more
    detail.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 1-10](#fig_10_dask_dataframe_with_divisions_by_partition) 查看之前的同一个 Dask
    DataFrame，并更详细地探索 DataFrame 的分区。'
- en: '![Dask DataFrame with divisions by partition](Images/understanding_the_architecture_of_dask_dataframes_803261_10.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![带有分区的 Dask DataFrame](Images/understanding_the_architecture_of_dask_dataframes_803261_10.png)'
- en: Figure 1-10\. Dask DataFrame with divisions by partition
  id: totrans-87
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-10\. 带有分区的 Dask DataFrame
- en: Notice that the first partition contains rows with index 0 and index 1, and
    the second partition contains rows with index 2 and index 3.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，第一个分区包含索引 0 和索引 1 的行，第二个分区包含索引 2 和索引 3 的行。
- en: You can access the `known_divisions` property to figure out if Dask is aware
    of the partition bounds for a given DataFrame. `ddf.known_divisions` will return
    `True` in this example because Dask knows the partition bounds.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以访问 `known_divisions` 属性，以确定 Dask 是否意识到给定 DataFrame 的分区边界。在本例中，`ddf.known_divisions`
    将返回 `True`，因为 Dask 知道分区边界。
- en: 'The `divisions` property will tell you the exact division bounds for your DataFrame:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`divisions` 属性将告诉您 DataFrame 的确切分区边界：'
- en: '[PRE10]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Here’s how to interpret the `(0, 2, 3)` tuple that’s returned:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何解释返回的 `(0, 2, 3)` 元组的内容：
- en: The first partition contains index values that span from zero to two (not inclusive
    upper boundary)
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个分区包含从零到二（不含上界）的索引值
- en: The second partition contains index values that span from two to three (inclusive
    upper boundary)
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个分区包含从两到三（含上界）的索引值
- en: Suppose you ask Dask to fetch you the row with index 3\. Dask doesn’t need to
    scan over all the partitions to find the value. It knows that the row with index
    3 is in the second partition from the `divisions` metadata, so it can narrow the
    search for index 3 to a single partition. This is a significant performance optimization,
    especially when there are thousands of partitions.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您要求 Dask 获取索引为 3 的行。Dask 无需扫描所有分区来查找该值。它知道索引为 3 的行在 `divisions` 元数据的第二个分区中，因此可以将索引
    3 的搜索缩小到单个分区。这是一个重要的性能优化，特别是当存在数千个分区时。
- en: Note
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: '[TIP] Knowing how to properly set the index and manage divisions is necessary
    when optimizing Dask DataFrame performance.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[提示] 在优化 Dask DataFrame 性能时，正确设置索引和管理分区是必要的。'
- en: pandas vs. Dask DataFrame on Larger than RAM Datasets
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: pandas 与 Dask DataFrame 在大于内存的数据集上的比较
- en: This section creates a larger than RAM dataset and demonstrates how pandas cannot
    run queries on this data, but Dask can query the data.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 本节创建了一个大于内存的数据集，并演示了 pandas 无法对该数据运行查询，但是 Dask 可以查询数据。
- en: 'Let’s use Dask to create 1,095 Snappy compressed Parquet files of data (58.2
    GB) on your local machine in the *~/data/timeseries/20-years/parquet/* directory:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 Dask 在本地机器上的 *~/data/timeseries/20-years/parquet/* 目录下创建 1,095 个 Snappy
    压缩的 Parquet 文件（58.2 GB）：
- en: '[PRE11]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Here are the files that are output to disk:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出到磁盘的文件：
- en: '[PRE12]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Here’s what the data looks like:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这是数据的样子：
- en: '| timestamp | id | name | x | y |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 时间戳 | id | 名称 | x | y |'
- en: '| 2000-01-01 00:00:00 | 1008 | Dan | -0.259374 | -0.118314 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 2000-01-01 00:00:00 | 1008 | 丹 | -0.259374 | -0.118314 |'
- en: '| 2000-01-01 00:00:01 | 987 | Patricia | 0.069601 | 0.755351 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 2000-01-01 00:00:01 | 987 | 帕特里夏 | 0.069601 | 0.755351 |'
- en: '| 2000-01-01 00:00:02 | 980 | Zelda | -0.281843 | -0.510507 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 2000-01-01 00:00:02 | 980 | 赛尔达 | -0.281843 | -0.510507 |'
- en: '| 2000-01-01 00:00:03 | 1020 | Ursula | -0.569904 | 0.523132 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 2000-01-01 00:00:03 | 1020 | 乌苏拉 | -0.569904 | 0.523132 |'
- en: '| 2000-01-01 00:00:04 | 967 | Michael | -0.251460 | 0.810930 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 2000-01-01 00:00:04 | 967 | 迈克尔 | -0.251460 | 0.810930 |'
- en: There is one row of data per second for 20 years, so the entire dataset contains
    662 million rows.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 每秒有一行数据持续 20 年，因此整个数据集包含 6.62 亿行。
- en: 'Let’s calculate the mean value of the `id` column for one of the data files
    with pandas:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 pandas 计算一个数据文件中 `id` 列的平均值：
- en: '[PRE13]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'pandas works great when analyzing a single data file. Now let’s try to run
    the same computation on all the data files with pandas:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 在分析单个数据文件时效果很好。现在让我们尝试在所有数据文件上使用 pandas 运行相同的计算：
- en: '[PRE14]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Whoops! This errors out with an out of memory exception. Most personal computers
    don’t have even nearly enough memory to hold a 58.2 GB dataset. This pandas computation
    will fill up all the computer’s RAM and then error out.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 糟糕！这个计算因内存不足而出错。大多数个人计算机甚至远远不够容纳 58.2 GB 的数据集。这个 pandas 计算将填满所有计算机的 RAM，然后出错。
- en: 'Let’s run this same computation with Dask:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用 Dask 运行相同的计算：
- en: '[PRE15]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This computation takes 8 seconds to execute on a computer with 8 GB of RAM and
    four cores. A computer with more cores would be able to execute the Dask computation
    with more parallelism and run even faster.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这个计算在具有 8 GB RAM 和四个核心的计算机上执行需要 8 秒。具有更多核心的计算机能够以更多的并行性执行 Dask 计算，并且运行得更快。
- en: As this example demonstrates, Dask makes it easy to scale up a localhost workflow
    to run on all the cores of a machine. Dask doesn’t load all of the data into memory
    at once and can run queries in a streaming manner. This allows Dask to perform
    analytical queries on datasets that are bigger than memory.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本例所示，Dask 可以轻松地将本地主机工作流扩展到机器的所有核心。Dask 不会一次性加载所有数据到内存中，可以以流式方式运行查询。这使得 Dask
    能够对比内存更大的数据集执行分析查询。
- en: This example shows how Dask can scale a localhost computation, but that’s not
    the only type of scaling that Dask allows for.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例显示了 Dask 如何扩展本地主机计算，但这并不是 Dask 允许的唯一类型的扩展。
- en: Scaling Up vs Scaling Out
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展 vs 分布
- en: 'Dask DataFrame can scale pandas computations in two different ways:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Dask DataFrame 可以通过两种不同的方式扩展 pandas 计算：
- en: '*Scale up:* Use all the cores of a computer rather than one core like pandas'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Scale up:* 使用计算机的所有核心，而不是像 pandas 一样使用单个核心。'
- en: '*Scale out:* Execute queries on multiple computers (called a cluster) rather
    than on a single computer'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Scale out:* 在多台计算机（称为集群）上执行查询，而不是在单台计算机上执行。'
- en: As previously discussed, pandas workflows only use a single core of a single
    machine. So pandas will only use one core, even if 8 or 96 are available. Dask
    scales up single machine workflows by running computations with all the cores,
    in parallel. So if Dask is run on a machine with 96 cores, it will split up the
    work and leverage all the available hardware to process the computation.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前讨论的，pandas 工作流只使用单台计算机的单个核心。因此，即使有 8 或 96 个可用，pandas 也只会使用一个核心。Dask 通过并行运行计算，使用所有核心，从而扩展了单机工作流。因此，如果在具有
    96 个核心的计算机上运行 Dask，它将分割工作并利用所有可用的硬件来处理计算。
- en: The Dask DataFrame architecture is what allows for this parallelism. On a 96-core
    machine, Dask can split up the data into 96 partitions, and process each partition
    at the same time.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Dask DataFrame 的架构允许并行处理。在一台拥有 96 核心的机器上，Dask 可以将数据分割成 96 个分区，并同时处理每个分区。
- en: Dask can also scale out a computation to run on multiple machines. Suppose you
    have a cluster of 3 computers, each with 24 cores. Dask can split up the data
    on multiple different machines and run the computations on all of the 72 available
    cores in the cluster.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 还可以将计算扩展到多台计算机上运行。假设您有一个拥有 3 台计算机的集群，每台计算机有 24 个核心。Dask 可以将数据分割到多台不同的计算机上，并在集群中所有
    72 个可用核心上运行计算。
- en: Scaling up is scaling a workflow from using a single core to all the cores of
    a given machine. Scaling out is scaling a workflow to run on multiple computers
    in a cluster. Dask allows you to scale up or scale out, both of which are useful
    in different scenarios.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展是将工作流从使用单个核心扩展到使用给定机器的所有核心。分布是将工作流扩展到在集群中的多台计算机上运行。Dask 允许您扩展或分布，两者在不同场景中都很有用。
- en: Summary
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter explained how Dask DataFrames are architected to overcome the scaling
    and big data performance issues of pandas. A good foundational understanding of
    Dask DataFrames will help you harness their power effectively.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 本章解释了 Dask DataFrames 如何通过架构来克服 pandas 的扩展和大数据性能问题。对 Dask DataFrames 有一个良好的基础理解将帮助您有效地利用它们的功能。
- en: Dask DataFrames add additional overhead, so they’re not always faster than pandas.
    For small datasets, loading data into memory and running computations is fast
    and pandas performs quite well. Dask gains a performance advantage as dataset
    sizes grow and more powerful machines are used. The larger the data and the more
    cores a computer has, the more parallelism helps.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Dask DataFrames 增加了额外的开销，因此并不总是比 pandas 更快。对于小数据集，将数据加载到内存并运行计算非常快，pandas 的性能也相当不错。随着数据集大小的增加和使用更强大的机器，Dask
    获得了性能优势。数据越大，计算机核心越多，并行处理效果越好。
- en: Dask is obviously the better option when the dataset is bigger than memory.
    pandas cannot work with datasets that are bigger than memory. Dask can run queries
    on datasets much larger than memory, as illustrated in our example.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据集大于内存时，显然 Dask 是更好的选择。pandas 无法处理大于内存的数据集。正如我们的例子所示，Dask 可以在比内存大得多的数据集上运行查询。
- en: Chapter 4 continues by digging more into Dask DataFrames. Chapter 4 will show
    you more important operations you can perform to manipulate your data. You’re
    in a great place to understand the different types of Dask DataFrame operations
    now that you’re familiar with how Dask DataFrames are architected.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 第 4 章继续深入探讨 Dask DataFrames。第 4 章将展示更多重要的操作，帮助您操作数据。现在您已经了解了 Dask DataFrames
    的架构，能够深入理解不同类型的 Dask DataFrame 操作。
