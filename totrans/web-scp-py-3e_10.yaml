- en: Chapter 9\. Storing Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although printing to the terminal is a lot of fun, it’s not incredibly useful
    when it comes to data aggregation and analysis. To make the majority of web scrapers
    remotely useful, you need to be able to save the information that they scrape.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter covers three main methods of data management that are sufficient
    for almost any imaginable application. Do you need to power the backend of a website
    or create your own API? You’ll probably want your scrapers to write to a database.
    Need a fast and easy way to collect documents off the internet and put them on
    your hard drive? You’ll probably want to create a file stream for that. Need occasional
    alerts, or aggregated data once a day? Send yourself an email!
  prefs: []
  type: TYPE_NORMAL
- en: Above and beyond web scraping, the ability to store and interact with large
    amounts of data is incredibly important for just about any modern programming
    application. In fact, the information in this chapter is necessary for implementing
    many of the examples in later sections of the book. I highly recommend that you
    at least skim this chapter if you’re unfamiliar with automated data storage.
  prefs: []
  type: TYPE_NORMAL
- en: Media Files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can store media files in two main ways: by reference and by downloading
    the file itself. Storing a file by reference is as simple as saving the text URL
    where the file is located on the host server but not actually downloading the
    file. This has several advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Scrapers run much faster and require much less bandwidth when they don’t have
    to download files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You save space on your own machine by storing only the URLs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is easier to write code that stores only URLs and doesn’t need to deal with
    additional file downloads.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can lessen the load on the host server by avoiding large file downloads.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are the disadvantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Embedding these URLs in your own website or application is known as *hotlinking*, and
    doing it is a quick way to get you in hot water on the internet.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You do not want to use someone else’s server cycles to host media for your own
    applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The file hosted at any particular URL is subject to change. This might lead
    to embarrassing effects if, say, you’re embedding a hotlinked image on a public
    blog, the blog owner finds out, and they decide to change the image to something
    unsavory. Less serious but still inconvenient: if you’re storing the URLs with
    the intent to use them later, they might eventually go missing at a later date.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real web browsers do not just request a page’s HTML and move on. They download
    all of the assets required by the page as well. Downloading files can make your
    scraper look like a human browsing the site, an advantage over merely recording
    links.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you’re debating whether to store a file or a URL to a file, you should ask
    yourself whether you’re likely to view or read that file more than once or twice,
    or if this database of files is going to be sitting around gathering electronic
    dust for most of its life. If the answer is the latter, it’s probably best to
    simply store the URL. If it’s the former, read on!
  prefs: []
  type: TYPE_NORMAL
- en: 'The urllib library, used to retrieve the content of web pages, also contains
    functions to retrieve the content of files. The following program uses `urllib.request.urlretrieve`
    to download images from a remote URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This downloads the Python logo from *http://pythonscraping.com* and stores it
    as *logo.jpg* in the same directory from which the script is running.
  prefs: []
  type: TYPE_NORMAL
- en: 'This works well if you need to download only a single file and know what to
    call it and what the file extension is. But most scrapers don’t download a single
    file and call it a day. The following downloads all internal files, linked to
    by any tag’s `src` attribute, from the home page of *http://pythonscraping.com*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Run with Caution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You know all those warnings you hear about downloading unknown files off the
    internet? This script downloads everything it comes across to your computer’s
    hard drive. This includes random bash scripts, *.exe* files, and other potential
    malware.
  prefs: []
  type: TYPE_NORMAL
- en: Think you’re safe because you’d never actually execute anything sent to your
    downloads folder? Especially if you run this program as an administrator, you’re
    asking for trouble. What happens if you run across a file on a website that sends
    itself to *../../../../usr/bin/python*? The next time you run a Python script
    from the command line, you could be deploying malware on your machine!
  prefs: []
  type: TYPE_NORMAL
- en: This program is written for illustrative purposes only; it should not be randomly
    deployed without more extensive filename checking, and it should be run only in
    an account with limited permissions. As always, backing up your files, not storing
    sensitive information on your hard drive, and using a little common sense go a
    long way.
  prefs: []
  type: TYPE_NORMAL
- en: This script uses a lambda function (introduced in [Chapter 5](ch05.html#c-5))
    to select all tags on the front page that have the `src` attribute, and then cleans
    and normalizes the URLs to get an absolute path for each download (making sure
    to discard external links). Then, each file is downloaded to its own path in the
    local folder *downloaded* on your own machine.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that Python’s `os` module is used briefly to retrieve the target directory
    for each download and create missing directories along the path if needed. The
    `os` module acts as an interface between Python and the operating system, allowing
    it to manipulate file paths, create directories, get information about running
    processes and environment variables, and many other useful things.
  prefs: []
  type: TYPE_NORMAL
- en: Storing Data to CSV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*CSV*, or *comma-separated values*, is one of the most popular file formats
    in which to store spreadsheet data. It is supported by Microsoft Excel and many
    other applications because of its simplicity. The following is an example of a
    perfectly valid CSV file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As with Python, whitespace is important here: each row is separated by a newline
    character, while columns within the row are separated by commas (hence the name
    “comma-separated”). Other forms of CSV files (sometimes called *character-separated
    value* files) use tabs or other characters to separate rows, but these file formats
    are less common and less widely supported.'
  prefs: []
  type: TYPE_NORMAL
- en: If you’re looking to download CSV files directly off the web and store them
    locally, without any parsing or modification, you don’t need this section. Download
    them like you would any other file and save them with the CSV file format by using
    the methods described in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Modifying a CSV file, or even creating one entirely from scratch, is extremely
    easy with Python’s *csv* library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'A precautionary reminder: file creation in Python is fairly bulletproof. If
    *test.csv* does not already exist, Python will create the file  (but not the directory) automatically.
    If it already exists, Python will overwrite *test.csv* with the new data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After running, you should see a CSV file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'One common web scraping task is to retrieve an HTML table and write it as a
    CSV file. [Wikipedia’s List of Countries with McDonald’s Restaurants](https://en.wikipedia.org/wiki/List_of_countries_with_McDonald%27s_restaurants)
    provides a fairly complex HTML table with links, sorting, and other HTML garbage
    that needs to be discarded before it can be written to CSV. Using BeautifulSoup
    and the `get_text()` function copiously, you can do that in fewer than 20 lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: There Is an Easier Way to Fetch a Single Table
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This script is great to integrate into scrapers if you encounter many HTML
    tables that need to be converted to CSV files, or many HTML tables that need to
    be collected into a single CSV file. However, if you only need to do it just once,
    there’s a better tool for that: copying and pasting. Selecting and copying all
    the content of an HTML table and pasting it into Excel or Google Docs will get
    you the CSV file you’re looking for without running a script!'
  prefs: []
  type: TYPE_NORMAL
- en: The result should be a well-formatted CSV file saved locally, at *countries.csv*.
  prefs: []
  type: TYPE_NORMAL
- en: MySQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*MySQL* (officially pronounced “my es-kew-el,” although many say, “my sequel”)
    is the most popular open source relational database management system today. Somewhat
    unusually for an open source project with large competitors, its popularity has
    historically been neck and neck with the two other major closed source database
    systems: Microsoft’s SQL Server and Oracle’s DBMS.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Its popularity is not without cause. For most applications, it’s hard to go
    wrong with MySQL. It’s a scalable, robust, and full-featured DBMS, used by top
    websites: YouTube,^([1](ch09.html#id556)) Twitter,^([2](ch09.html#id557)) and
    Facebook,^([3](ch09.html#id558)) among many others.'
  prefs: []
  type: TYPE_NORMAL
- en: Because of its ubiquity, price (“free” is a pretty great price), and out-of-box
    usability, it makes a fantastic database for web scraping projects, and we will
    use it throughout the remainder of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Installing MySQL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you’re new to MySQL, installing a database might sound a little intimidating
    (if you’re an old hat at it, feel free to skip this section). In reality, it’s
    as simple as installing just about any other kind of software. At its core, MySQL
    is powered by a set of data files, stored on your server or local machine, that
    contain all the information stored in your database. The MySQL software layer
    on top of that provides a convenient way of interacting with the data via a command-line
    interface. For example, the following command digs through the data files and
    returns a list of all users in your database whose first name is “Ryan”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'If you’re on a Debian-based Linux distribution (or anything with `apt-get`),
    installing MySQL is as easy as this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Just keep an eye on the installation process, approve the memory requirements,
    and enter a new password for your new root user when prompted.
  prefs: []
  type: TYPE_NORMAL
- en: For macOS and Windows, things are a little trickier. If you haven’t already,
    you need to create an Oracle account before downloading the package.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re on macOS, you first need to get [the installation package](http://dev.mysql.com/downloads/mysql/).
  prefs: []
  type: TYPE_NORMAL
- en: Select the *.dmg* package, and log in with or create your Oracle account to
    download the file. After the file opens, you should be guided through a fairly
    straightforward installation wizard (see [Figure 9-1](#mac_installer)).
  prefs: []
  type: TYPE_NORMAL
- en: The default installation steps should suffice, and for the purposes of this
    book, I assume you have a default MySQL installation.
  prefs: []
  type: TYPE_NORMAL
- en: 'After MySQL is installed on macOS, you can start the MySQL server as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: On Windows, installing and running MySQL is slightly more complicated, but the
    good news is that [a convenient installer](http://dev.mysql.com/downloads/windows/installer/)
    simplifies the process. Once downloaded, it will guide you through the steps you
    need to take (see [Figure 9-2](#mysql_installer)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Alt Text](assets/wsp3_0901.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-1\. The macOS MySQL installer
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Alt Text](assets/wsp3_0902.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-2\. The Windows MySQL Installer
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'You should be able to install MySQL by using the default selections, with one
    exception: on the Setup Type page, I recommend you choose Server Only to avoid
    installing a lot of additional Microsoft software and libraries. From there, you
    should be able to use the default installation settings and follow the prompts
    to start your MySQL server.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After your MySQL server is installed and running, you will still need to be
    able to interact with it using the command line. On Windows, you can use the [MySQL
    Shell](https://dev.mysql.com/downloads/shell/) tools. On Macs, I like to install
    the command-line tools with the [Homebrew package manager](https://brew.sh):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'After installing the command-line tools, you should be able to connect to your
    MySQL server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This will prompt you to enter the root password that you created during installation.
  prefs: []
  type: TYPE_NORMAL
- en: Some Basic Commands
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After your MySQL server is running, you have many options for interacting with
    the database. Plenty of software tools act as an intermediary so that you don’t
    have to deal with MySQL commands (or at least deal with them less often). Tools
    such as phpMyAdmin and MySQL Workbench can make it easy to quickly view, sort,
    and insert data. However, it’s still important to know your way around the command
    line.
  prefs: []
  type: TYPE_NORMAL
- en: Except for variable names, MySQL is case insensitive; for example, `SELECT` is
    the same as `sElEcT`. However, by convention, all MySQL keywords are in all caps
    when you are writing a MySQL statement. Conversely, most developers prefer to
    name their tables and databases in lowercase, although this standard is often
    ignored.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you first log in to MySQL, there are no databases to add data to, but
    you can create one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Because every MySQL instance can have multiple databases, before you can start
    interacting with a database, you need to specify to MySQL which database you want
    to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: From this point on (at least until you close the MySQL connection or switch
    to another database), all commands entered will be run against the new `scraping`
    database.
  prefs: []
  type: TYPE_NORMAL
- en: 'That all seems pretty straightforward. It must be similarly easy to create
    a table in the database, right? Let’s try to create a table to store a collection
    of scraped web pages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in an error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Unlike a database, which can exist without any tables, a table in MySQL cannot
    exist without columns. To define columns in MySQL, you must enter them in a comma-delimited
    list, within parentheses, after the `CREATE TABLE <tablename>` statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Each column definition has three parts:'
  prefs: []
  type: TYPE_NORMAL
- en: The name (`id`, `title`, `created`, etc.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The variable type (`BIGINT(7)`, `VARCHAR`, `TIMESTAMP`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optionally, any additional attributes (`NOT NULL AUTO_INCREMENT`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the end of the list of columns, you must define a table’s *key*. MySQL uses
    keys to organize the content in the table for fast lookups. Later in this chapter,
    I’ll describe how to use these keys to your advantage for speedier databases,
    but for now, using a table’s `id` column as the key is generally the best way
    to go.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the query executes, you can see what the structure of the table looks
    like at any time by using `DESCRIBE`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Of course, this is still an empty table. You can insert test data into the
    *pages* table by using the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Notice that although the table has four columns (`id`, `title`, `content`, `created`),
    you need to define only two of them (`title` and `content`) in order to insert
    a row. That’s because the `id` column is autoincremented (MySQL automatically
    adds a 1 each time a new row is inserted) and generally can take care of itself.
    In addition, the `timestamp` column is set to contain the current time as a default.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, you *can* override these defaults:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: As long as the integer you provide for the `id` column doesn’t already exist
    in the database, this override will work perfectly fine. However, it is generally
    bad practice to do this; it’s best to let MySQL handle the `id` and `timestamp`
    columns unless there is a compelling reason to do it differently.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you have some data in the table, you can use a wide variety of methods
    to select this data. Here are a few examples of `SELECT` statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This statement tells MySQL, “Select all from pages where `id` equals 2.” The
    asterisk (*) acts as a wildcard, returning all the rows where the clause (`where
    id equals 2`) is true. It returns the second row in the table, or an empty result
    if there is no row with an `id` of 2\. For example, the following case-insensitive
    query returns all the rows where the `title` field contains “test” (the % symbol
    acts as a wildcard in MySQL strings):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'But what if you have a table with many columns, and you want only a particular
    piece of data returned? Rather than selecting all, you can do something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This returns just the `id` and `title` where the content contains the phrase
    “page content.”
  prefs: []
  type: TYPE_NORMAL
- en: '`DELETE` statements have much the same syntax as `SELECT` statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: For this reason, it is a good idea, especially when working on important databases
    that can’t be easily restored, to write any `DELETE` statements as a `SELECT`
    statement first (in this case, `SELECT * FROM pages WHERE  id = 1`), test to make
    sure only the rows you want to delete are returned, and then replace `SELECT *`
    with `DELETE`. Many programmers have horror stories of miscoding the clause on
    a `DELETE` statement, or worse, leaving it off entirely when they were in a hurry
    and ruining customer data. Don’t let it happen to you!
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar precautions should be taken with `UPDATE` statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: For the purposes of this book, you will be working with only simple MySQL statements,
    doing basic selecting, inserting, and updating. If you’re interested in learning
    more commands and techniques with this powerful database tool, I recommend Paul
    DuBois’s [*MySQL Cookbook*](http://shop.oreilly.com/product/0636920032274.do)
    (O’Reilly).
  prefs: []
  type: TYPE_NORMAL
- en: Integrating with Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unfortunately, Python support for MySQL is not built in. However, many open
    source libraries allow you to interact with a MySQL database. One of the most
    popular of these is [PyMySQL](https://pypi.python.org/pypi/PyMySQL).
  prefs: []
  type: TYPE_NORMAL
- en: 'As of this writing, the current version of PyMySQL is 1.0.3, which can be installed
    using pip:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'After installation, you should have access to the PyMySQL package automatically.
    While your local MySQL server is running, you should be able to execute the following
    script successfully (remember to add the root password for your database):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Two new types of objects are at work in this example: the connection object
    (`conn`) and the cursor object (`cur`).'
  prefs: []
  type: TYPE_NORMAL
- en: The connection/cursor model is commonly used in database programming, although
    some users might find it tricky to differentiate between the two at first. The
    connection is responsible for, well, connecting to the database, of course, but
    also sending the database information, handling rollbacks (when a query or set
    of queries needs to be aborted and the database needs to be returned to its previous
    state), and creating new cursor objects.
  prefs: []
  type: TYPE_NORMAL
- en: A connection can have many cursors. A cursor keeps track of certain *state*
    information, such as which database it is using. If you have multiple databases
    and need to write information across all of them, you might have multiple cursors
    to handle this. A cursor also contains the results of the latest query it has
    executed. By calling functions on the cursor, such as `cur.fetchone()`, you can
    access this information.
  prefs: []
  type: TYPE_NORMAL
- en: It is important that both the cursor and the connection are closed after you
    are finished using them. Not doing this might result in *connection leaks*, a
    buildup of unclosed connections that are no longer being used, but the software
    isn’t able to close because it’s under the impression that you might still use
    them. This is the sort of thing that brings databases down all the time (I have
    both written and fixed many connection leak bugs), so remember to close your connections!
  prefs: []
  type: TYPE_NORMAL
- en: 'The most common thing you’ll probably want to do, starting out, is to be able
    to store your scraping results in a database. Let’s take a look at how this could
    be done, using a previous example: the Wikipedia scraper.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Dealing with Unicode text can be tough when web scraping. By default, MySQL
    does not handle Unicode. Fortunately, you can turn on this feature (just keep
    in mind that doing so will increase the size of your database). Because you’re
    bound to run into a variety of colorful characters on Wikipedia, now is a good
    time to tell your database to expect some Unicode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: These four lines change the default character set for the database, for the
    table, and for both of the two columns—from `utf8mb4` (still technically Unicode,
    but with notoriously terrible support for most Unicode characters) to `utf8mb4_unicode_ci`.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll know that you’re successful if you try inserting a few umlauts or Mandarin
    characters into the `title` or `content` field in the database and it succeeds
    with no errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the database is prepared to accept a wide variety of all that Wikipedia
    can throw at it, you can run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'There are a few things to note here: first, `"charset=''utf8''"` is added to
    the database connection string. This tells the connection that it should send
    all information to the database as UTF-8 (and, of course, the database should
    already be configured to handle this).'
  prefs: []
  type: TYPE_NORMAL
- en: Second, note the addition of a `store` function. This takes in two string variables,
    `title` and `content`, and adds them to an `INSERT` statement that is executed
    by the cursor and then committed by the cursor’s connection. This is an excellent
    example of the separation of the cursor and the connection; while the cursor has
    stored information about the database and its own context, it needs to operate
    through the connection in order to send information back to the database and insert
    information.
  prefs: []
  type: TYPE_NORMAL
- en: Last, you’ll see that a `finally` statement is added to the program’s main loop,
    at the bottom of the code. This ensures that, regardless of how the program is
    interrupted or the exceptions that might be thrown during its execution (and because
    the web is messy, you should always assume exceptions will be thrown), the cursor
    and the connection will both be closed immediately before the program ends. It
    is a good idea to include a `try...finally` statement like this whenever you are
    scraping the web and have an open database connection.
  prefs: []
  type: TYPE_NORMAL
- en: Although PyMySQL is not a huge package, there are a fair number of useful functions
    that this book can’t accommodate. You can check out their [documentation](https://pymysql.readthedocs.io/en/latest/)
    at the PyMySQL site.
  prefs: []
  type: TYPE_NORMAL
- en: Database Techniques and Good Practice
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some people spend their entire careers studying, tuning, and inventing databases.
    I am not one of those people, and this is not that kind of book. However, as with
    many subjects in computer science, there are a few tricks you can learn quickly
    to at least make your databases sufficient, and sufficiently speedy, for most
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: First, with few exceptions, always add `id` columns to your tables. All tables
    in MySQL must have at least one primary key (the key column that MySQL sorts on),
    so that MySQL knows how to order it, and it can often be difficult to choose these
    keys intelligently.
  prefs: []
  type: TYPE_NORMAL
- en: The debate over whether to use an artificially created `id` column for this
    key or a unique attribute such as `username` has raged among data scientists and
    software engineers for years, although I tend to lean on the side of creating
    `id` columns. This is *especially* true when you’re dealing with web scraping
    and storing someone else’s data. You have no idea what’s actually unique or not
    unique, and I’ve been surprised before.
  prefs: []
  type: TYPE_NORMAL
- en: Your `id` column should be autoincremented and used as the primary key for all
    of your tables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, use intelligent indexing. A dictionary (like the book, not the Python
    object) is a list of words indexed alphabetically. This allows quick lookups whenever
    you need a word, as long as you know how it’s spelled. You could also imagine
    a dictionary that is organized alphabetically by the word’s definition. This wouldn’t
    be nearly as useful unless you were playing some strange game of *Jeopardy!* in
    which a definition was presented and you needed to come up with the word. But
    in the world of database lookups, these sorts of situations happen. For example,
    you might have a field in your database that you will often be querying against:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'You might very well want to add an index to this table (in addition to the
    index presumably already in place on the `id`) to the `definition` column to make
    lookups on this column faster. Keep in mind, though, that adding indexing requires
    more space for the new index, as well as additional processing time when inserting
    new rows. Especially when you’re dealing with large amounts of data, you should
    carefully consider the trade-offs of your indexes and how much you need to index.
    To make this “definitions” index a little lighter, you can tell MySQL to index
    only the first few characters in the column value. This command creates an index
    on the first 16 characters in the `definition` field:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: This index will make your lookups much faster when searching for words by their
    full definition (especially if the first 16 characters in definition values tend
    to be very different from each other), and not add too much in the way of extra
    space and up-front processing time.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the subject of query time versus database size (one of the fundamental balancing
    acts in database engineering), one of the common mistakes made, especially with
    web scraping of large amounts of natural text data, is to store lots of repeating
    data. For example, say you want to measure the frequency of certain phrases that
    crop up across websites. These phrases might be found from a given list or automatically
    generated via a text-analysis algorithm. You might be tempted to store the data
    as something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This adds a row to the database each time you find a phrase on a site and records
    the URL where it was found. However, by splitting the data into three separate
    tables, you can shrink your dataset enormously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Although the table definitions are larger, you can see that the majority of
    the columns are just integer `id` fields. These take up far less space. In addition,
    the full text of each URL and phrase is stored exactly once.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unless you install a third-party package or keep meticulous logs, it can be
    impossible to tell when a piece of data was added, updated, or removed from your
    database. Depending on the available space for your data, the frequency of changes,
    and the importance of determining when those changes happened, you might want
    to consider keeping several timestamps in place: `created`, `updated`, and `deleted`.'
  prefs: []
  type: TYPE_NORMAL
- en: “Six Degrees” in MySQL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Chapter 6](ch06.html#c-6) introduced the Six Degrees of Wikipedia problem,
    in which the goal is to find the connection between any two Wikipedia articles
    through a series of links (i.e., find a way to get from one Wikipedia article
    to the next just by clicking links from one page to the next). To solve this problem,
    it is necessary not only to build bots that can crawl the site (which you have
    already done) but also store the information in an architecturally sound way to
    make data analysis easy later.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Autoincremented `id` columns, timestamps, and multiple tables: they all come
    into play here. To figure out how to best store this information, you need to
    think abstractly. A link is simply something that connects Page A to Page B. It
    could just as easily connect Page B to Page A, but this would be a separate link.
    You can uniquely identify a link by saying, “There exists a link on page A, which
    connects to page B. That is, `INSERT INTO` links (`fromPageId`, `toPageId`) VALUES
    (A, B); (where A and B are the unique IDs for the two pages).”'
  prefs: []
  type: TYPE_NORMAL
- en: 'A two-table system designed to store pages and links, along with creation dates
    and unique IDs, can be constructed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Notice that, unlike with previous crawlers that print the title of the page,
    you’re not even storing the title of the page in the `pages` table. Why is that?
    Well, recording the title of the page requires that you visit the page to retrieve
    it. If you want to build an efficient web crawler to fill out these tables, you
    want to be able to store the page, as well as links to it, even if you haven’t
    necessarily visited the page yet.
  prefs: []
  type: TYPE_NORMAL
- en: Although this doesn’t hold true for all sites, the nice thing about Wikipedia
    links and page titles is that one can be turned into the other through simple
    manipulation. For example, *http://en.wikipedia.org/wiki/Monty_Python* indicates
    that the title of the page is “Monty Python.”
  prefs: []
  type: TYPE_NORMAL
- en: 'The following will store all pages on Wikipedia that have a “Bacon number”
    (the number of links between it and the page for Kevin Bacon, inclusive) of 6
    or less:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Three functions here use PyMySQL to interface with the database:'
  prefs: []
  type: TYPE_NORMAL
- en: '`insertPageIfNotExists`'
  prefs: []
  type: TYPE_NORMAL
- en: As its name indicates, this function inserts a new page record if it does not
    exist already. This, along with the running list of all collected pages stored
    in `pages`, ensures that page records are not duplicated. It also serves to look
    up `pageId` numbers in order to create new links.
  prefs: []
  type: TYPE_NORMAL
- en: '`insertLink`'
  prefs: []
  type: TYPE_NORMAL
- en: This creates a new link record in the database. It will not create a link if
    that link already exists. Even if two or more identical links *do* exist on the
    page, for our purposes, they are the same link, represent the same relationship,
    and should be counted as only one record. This also helps maintain the integrity
    of the database if the program is run multiple times, even over the same pages.
  prefs: []
  type: TYPE_NORMAL
- en: '`loadPages`'
  prefs: []
  type: TYPE_NORMAL
- en: This loads all current pages from the database into a list, so that it can be
    determined whether a new page should be visited. Pages are also collected during
    runtime, so if this crawler is run only once, starting with an empty database,
    in theory `loadPage` should not be needed. In practice, however, problems may
    arise. The network might go down, or you might want to collect links over several
    periods of time, and it’s important for the crawler to be able to reload itself
    and not lose any ground.
  prefs: []
  type: TYPE_NORMAL
- en: 'You should be aware of one potentially problematic subtlety of using `loadPages`,
    and the `pages` list it generates, to determine whether or not to visit a page:
    as soon as each page is loaded, all the links on that page are stored as pages,
    even though they have not been visited yet—just their links have been seen. If
    the crawler is stopped and restarted, all of these “seen but not visited” pages
    will never be visited, and links coming from them will not be recorded. This might
    be fixed by adding a boolean `visited` variable to each page record and setting
    it to `True` only if that page has been loaded and its own outgoing links recorded.'
  prefs: []
  type: TYPE_NORMAL
- en: For our purposes, however, this solution is fine as is. If you can ensure fairly
    long runtimes (or just a single runtime), and it isn’t a necessity to ensure a
    complete set of links (just a large dataset to experiment with), the addition
    of the `visited` variable is not necessary.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the continuation of this problem and the final solution for getting from
    [Kevin Bacon](https://en.wikipedia.org/wiki/Kevin_Bacon) to [Eric Idle](https://en.wikipedia.org/wiki/Eric_Idle),
    see [“Six Degrees of Wikipedia: Conclusion”](ch12.html#six-degrees-conclusion)
    on solving directed graph problems.'
  prefs: []
  type: TYPE_NORMAL
- en: Email
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Just as web pages are sent over HTTP, email is sent over SMTP (Simple Mail Transfer
    Protocol). And just as you use a web server client to handle sending out web pages
    over HTTP, servers use various email clients, such as Sendmail, Postfix, or Mailman,
    to send and receive email.
  prefs: []
  type: TYPE_NORMAL
- en: Although sending email with Python is relatively easy, it does require that
    you have access to a server running SMTP. Setting up an SMTP client on your server
    or local machine is tricky and outside the scope of this book, but many excellent
    resources can help with this task, particularly if you are running Linux or macOS.
  prefs: []
  type: TYPE_NORMAL
- en: The following code examples assume that you are running an SMTP client locally.
    (To modify this code for a remote SMTP client, change `localhost` to your remote
    server’s address.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Sending an email with Python requires just nine lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Python contains two important packages for sending email: *smtplib* and *email*.'
  prefs: []
  type: TYPE_NORMAL
- en: Python’s email module contains useful formatting functions for creating email
    packets to send. The `MIMEText` object, used here, creates an empty email formatted
    for transfer with the low-level MIME (Multipurpose Internet Mail Extensions) protocol,
    across which the higher-level SMTP connections are made. The `MIMEText` object,
    `msg`, contains to/from email addresses, as well as a body and a header, which
    Python uses to create a properly formatted email.
  prefs: []
  type: TYPE_NORMAL
- en: The smtplib package contains information for handling the connection to the
    server. Just like a connection to a MySQL server, this connection must be torn
    down every time it is created to avoid creating too many connections.
  prefs: []
  type: TYPE_NORMAL
- en: 'This basic email function can be extended and made more useful by enclosing
    it in a function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: This particular script checks the website [*https://isitchristmas.com*](https://isitchristmas.com) (the
    main feature of which is a giant YES or NO, depending on the day of the year)
    once an hour. If it sees anything other than a NO, it will send you an email alerting
    you that it’s Christmas.
  prefs: []
  type: TYPE_NORMAL
- en: Although this particular program might not seem much more useful than a calendar
    hanging on your wall, it can be slightly tweaked to do a variety of extremely
    useful things. It can email you alerts in response to site outages, test failures,
    or even the appearance of an out-of-stock product you’re waiting for on Amazon—none
    of which your wall calendar can do.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch09.html#id556-marker)) Joab Jackson, [“YouTube Scales MySQL with Go
    Code”](http://bit.ly/1LWVmc8), *PCWorld*, December 15, 2012.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch09.html#id557-marker)) Jeremy Cole and Davi Arnaut, [“MySQL at Twitter”](http://bit.ly/1KHDKns),
    *The Twitter Engineering Blog*, April 9, 2012.
  prefs: []
  type: TYPE_NORMAL
- en: '^([3](ch09.html#id558-marker)) [“MySQL and Database Engineering: Mark Callaghan”](http://on.fb.me/1RFMqvw),
    Facebook Engineering, March 4, 2012.'
  prefs: []
  type: TYPE_NORMAL
