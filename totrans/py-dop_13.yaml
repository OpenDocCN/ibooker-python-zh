- en: Chapter 13\. Serverless Technologies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Serverless* is a term that generates a lot of buzz in the IT industry these
    days. As often happens with these kinds of terms, people have different opinions
    about what they actually mean. At face value, *serverless* implies a world where
    you do not need to worry about managing servers anymore. To some extent, this
    is true, but only for the developers who are using the functionality offered by
    *serverless* technologies. This chapter shows there is a *lot* of work that needs
    to happen behind the scenes for this magical world of no servers to come into
    being.'
  prefs: []
  type: TYPE_NORMAL
- en: Many people equate the term *serverless* with Function as a Service (FaaS).
    This is partially true, and it mostly came about when AWS launched the Lambda
    service in 2015\. AWS Lambdas are functions that can be run in the cloud without
    deploying a traditional server to host the functions. Hence the word *serverless*.
  prefs: []
  type: TYPE_NORMAL
- en: However, FaaS is not the only service that can be dubbed serverless. These days
    the Big Three public cloud providers (Amazon, Microsoft, and Google) all offer
    Containers as a Service (CaaS), which allows you to deploy full-blown Docker containers
    to their clouds without provisioning servers to host those containers. These services
    can also be called serverless. Examples of such services are AWS Fargate, Microsoft
    Azure Container Instances, and Google Cloud Run.
  prefs: []
  type: TYPE_NORMAL
- en: 'What are some use cases for serverless technologies? For FaaS technologies
    such as AWS Lambda, especially due to the event-driven manner in which Lambda
    functions can be triggered by other cloud services, use cases include:'
  prefs: []
  type: TYPE_NORMAL
- en: Extract-Transform-Load (ETL) data processing, where, as an example, a file is
    uploaded to S3, which triggers the execution of a Lambda function that does ETL
    processing on the data and sends it to a queue or a backend database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ETL processing on logs sent by other services to CloudWatch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scheduling tasks in a cron-like manner based on CloudWatch Events triggering
    Lambda functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-time notifications based on Amazon SNS triggering Lambda functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Email processing using Lambda and Amazon SES
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serverless website hosting, with the static web resources such as Javascript,
    CSS, and HTML stored in S3 and fronted by the CloudFront CDN service, and a REST
    API handled by an API Gateway routing the API requests to Lambda functions, which
    communicate with a backend such as Amazon RDS or Amazon DynamoDB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many serverless use cases are identified in each of the cloud service providers’
    online documentation. For example, in the Google Cloud serverless ecosystem, web
    applications are handled best by Google AppEngine, APIs are handled best by Google
    Functions, and CloudRun is preferred for running processes in Docker containers.
    For a concrete example, consider a service that needs to perform machine learning
    tasks such as object detection with the TensorFlow framework. Due to the compute,
    memory, and disk resource limitations of FaaS, combined with the limited availability
    of libraries in a FaaS setup, it is probably better to run such a service using
    a CaaS service such as Google Cloud Run, as opposed to a FaaS service such as
    Google Cloud Functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Big Three cloud providers also offer a rich DevOps toolchain around their
    FaaS platforms. For example, when you use AWS Lambda, with little effort, you
    can also add these services from AWS:'
  prefs: []
  type: TYPE_NORMAL
- en: AWS X-Ray for tracing/observability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon CloudWatch for logging, alerting, and event scheduling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS Step Functions for serverless workflow coordination
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS Cloud9 for an in-browser development environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do you choose between FaaS and CaaS? In one dimension, it depends on the
    unit of deployment. If you only care about short-lived functions, with few dependencies
    and small amounts of data processing, then FaaS can work for you. If, on the other
    hand, you have long-running processes with lots of dependencies and heavy computing
    power requirements, then you may be better off using CaaS. Most FaaS services
    have severe limits for running time (15 minutes maximum for Lambda), computing
    power, memory size, disk space, and HTTP request and response limits. The upside
    to FaaS’ short execution times is that you only pay for the duration of the function.
  prefs: []
  type: TYPE_NORMAL
- en: If you remember the discussion at the beginning of [Chapter 12](ch12.html#containers-kubernetes)
    on pets versus cattle versus insects, functions can truly be considered ephemeral
    insects that briefly come into existence, perform some processing, and disappear.
    Because of their ephemeral nature, functions in FaaS are also stateless, which
    is an important fact to keep in mind as you architect your application.
  prefs: []
  type: TYPE_NORMAL
- en: Another dimension for choosing between FaaS and CaaS is the number and type
    of interactions that your service has with other services. For example, an AWS
    Lambda function can be triggered asynchronously by no less than eight other AWS
    services, including S3, Simple Notification Service (SNS), Simple Email Service
    (SES), and CloudWatch. This richness of interactions makes it easier to write
    functions that respond to events, so FaaS wins in this case.
  prefs: []
  type: TYPE_NORMAL
- en: As you’ll see in this chapter, many FaaS services are actually based on Kubernetes,
    which these days is the de facto container orchestration standard. Even though
    your unit of deployment is a function, behind the scenes the FaaS tooling creates
    and pushes Docker containers to a Kubernetes cluster that you might or might not
    manage. OpenFaas and OpenWhisk are examples of such Kubernetes-based FaaS technologies.
    When you self-host these FaaS platforms, you very quickly become aware that *server*
    makes up most of the word serverless. All of a sudden you have to worry a lot
    about the care and feeding of your Kubernetes clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we split the word DevOps into its parts, Dev and Ops, serverless technologies
    are targeted more toward the Dev side. They help developers feel less friction
    when it comes to deploying their code. The burden, especially in a self-hosted
    scenario, is on Ops to provision the infrastructure (sometimes very complex) that
    will support the FaaS or CaaS platforms. However, even if the Dev side might feel
    there is little need for Ops when it comes to serverless (which happens, although
    by definition this split makes it a non-DevOps situation), there are still plenty
    of Ops-related issues to worry about when it comes to using a Serverless platform:
    security, scalability, resource limitations and capacity planning, monitoring,
    logging, and observability. These have traditionally been considered the domain
    of Ops, but in the brave new DevOps world we are talking about, they need to be
    tackled by both Dev and Ops in tandem and with cooperation. A Dev team should
    not feel that its task is done when it finishes writing the code. Instead, it
    should take ownership and yes, pride, in getting the service all the way to production,
    with good monitoring, logging, and tracing built in.'
  prefs: []
  type: TYPE_NORMAL
- en: We start this chapter with examples of how to deploy the same Python function,
    representing a simple HTTP endpoint, to the Big Three cloud providers using their
    FaaS offerings.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Some of the commands used in the following examples produce large amounts of
    output. Except for cases where it is critical to the understanding of the command,
    we will omit the majority of the output lines to save trees and enable the reader
    to focus better on the text.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the Same Python Function to the “Big Three” Cloud Providers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For AWS and Google, we use the Serverless platform, which simplifies these deployments
    by abstracting the creation of cloud resources that are involved in the FaaS runtime
    environments. The Serverless platform does not yet support Python functions for
    Microsoft Azure, so in that case we show how to use Azure-specific CLI tooling.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Serverless Framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[The Serverless platform](https://serverless.com) is based on nodejs. To install
    it, use `npm`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Deploying Python Function to AWS Lambda
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start by cloning the Serverless platform examples GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The Python HTTP endpoint is defined in the file *handler.py*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The Serverless platform uses a declarative approach for specifying the resources
    it needs to create with a YAML file called *serverless.yaml*. Here is file that
    declares a function called `currentTime`, corresponding to the Python function
    `endpoint` from the `handler` module defined previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Modify the Python version to 3.7 in *serverless.yaml*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Deploy the function to AWS Lambda by running the `serverless deploy` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Test the deployed AWS Lambda function by hitting its endpoint with `curl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Invoke the Lambda function directly with the `serverless invoke` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Invoke the Lambda function directly and inspect the log (which is sent to AWS
    CloudWatch Logs) at the same time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Note how the `Billed Duration` in the preceding output is 100 ms. This shows
    one of the advantages of using FaaS—being billed in very short increments of time.
  prefs: []
  type: TYPE_NORMAL
- en: 'One other thing we want to draw your attention to is the heavy lifting behind
    the scenes by the Serverless platform in the creation of AWS resources that are
    part of the Lambda setup. Serverless creates a CloudFormation stack called, in
    this case, `aws-python-simple-http-endpoint-dev`. You can inspect it with the
    `aws` CLI tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Note how this CloudFormation stack contains no less than 10 AWS resource types
    that you would have had to otherwise create or associate with one another manually.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Python Function to Google Cloud Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will take as an example the code from the `google-python-simple-http-endpoint`
    directory from the Serverless platform examples GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a new GCP project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the local `gcloud` environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Authorize local shell with GCP:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the Serverless framework to deploy the same Python HTTP endpoint as in
    the AWS Lambda example, but this time as a Google Cloud Function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The error we just encountered is due to the fact that the dependencies specified
    in *package.json* have not been installed yet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The Serverless platform is written in node.js, so its packages need to be installed
    with `npm install`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Try deploying again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: To generate a credentials key, create a new service account named `sa` on the
    GCP IAM service account page. In this case, the email for the new service account
    was set to `sa-255@pythonfordevops-cloudfunction.iam.gserviceaccount.com`.
  prefs: []
  type: TYPE_NORMAL
- en: Create a credentials key and download it as `~/.gcloud/pythonfordevops-cloudfunction.json`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specify the project and the path to the key in *serverless.yml*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Go to the GCP Deployment Manager page and enable the Cloud Deployment Manager
    API; then also enable billing for Google Cloud Storage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Try deploying again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Read through [the Serverless platform documentation on GCP credentials and roles](https://oreil.ly/scsRg).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following roles need to be assigned to the service account used for the
    deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: Deployment Manager Editor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storage Admin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logging Admin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud Functions Developer roles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also read through [the Serverless platform documentation on the GCP APIs that
    need to be enabled](https://oreil.ly/rKiHg).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following APIs need to be enabled in the GCP console:'
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud Functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google Cloud Deployment Manager
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google Cloud Storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stackdriver Logging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Go to Deployment Manager in the GCP console and the inspect error messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Delete the `sls-python-simple-http-endpoint-dev` deployment in the GCP console
    and run `serverless deploy` again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The `serverless deploy` command kept failing because initially we did not enable
    billing for Google Cloud Storage. The deployment was marked as failed for the
    service specified in *serverless.yml*, and subsequent `serverless deploy` commands
    failed even after enabling Cloud Storage billing. Once the failed deployment was
    deleted in the GCP console, the `serverless deploy` command started to work.
  prefs: []
  type: TYPE_NORMAL
- en: 'Invoke the deployed Google Cloud Function directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `serverless logs` command to inspect the logs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Test the function endpoint with `curl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Since we didn’t define a region in *serverless.yml*, the endpoint URL starts
    with `undefined` and returns an error.
  prefs: []
  type: TYPE_NORMAL
- en: 'Set the region to `us-central1` in *serverless.yml*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Deploy the new version with `serverless deploy` and test the function endpoint
    with `curl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Deploying Python Function to Azure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Serverless platform does not yet support [Azure Functions](https://oreil.ly/4WQKG)
    based on Python. We will demonstrate how to deploy Azure Python Functions using
    Azure-native tools.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sign up for a Microsoft Azure account and install the Azure Functions runtime
    for your specific operating system, following the [official Microsoft documentation](https://oreil.ly/GHS4c).
    If you are on a macOS, use `brew`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a new directory for the Python function code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Install Python 3.6 because 3.7 is not supported by Azure Functions. Create
    and activate `virtualenv`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `Azure func` utility, create a local Functions project called `python-simple-http-endpoint`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Change directories to the newly created *python-simple-http-endpoint* directory
    and create an Azure HTTP Trigger Function with the `func new` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Inspect the Python code created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the function locally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Test from another terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Change HTTP handler in *currentTime/init.py* to include the current time in
    its response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Test the new function with `curl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Install the Azure CLI with `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Create an Azure Resource Group, Storage Account, and Function App using the
    `az` CLI utility in interactive mode. This mode places you in an interactive shell
    with auto-completion, command descriptions, and examples. Note that if you want
    to follow along, you will need to specify a different and unique `functionapp`
    name. You might also need to specify a different Azure region, such as `eastus`,
    that supports free trial accounts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Deploy the `functionapp` project to Azure using the `func` utility:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Test the deployed function in Azure by hitting its endpoint with `curl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'It is always a good idea to remove any cloud resources you don’t need anymore.
    In this case, you can run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Deploying a Python Function to Self-Hosted FaaS Platforms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned earlier in this chapter, many FaaS platforms are running on top
    of Kubernetes clusters. One advantage of this approach is that the functions you
    deploy run as regular Docker containers inside Kubernetes, so you can use your
    existing Kubernetes tooling, especially when it comes to observability (monitoring,
    logging, and tracing). Another advantage is potential cost savings. By running
    your serverless functions as containers inside an existing Kubernetes cluster,
    you can use the existing capacity of the cluster and not pay per function call
    as you would if you deployed your functions to a third-party FaaS platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we consider one of these platforms: [OpenFaaS](https://www.openfaas.com).
    Some other examples of similar FaaS platforms running on Kubernetes include the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Kubeless](https://kubeless.io)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Fn Project](https://fnproject.io) (the underlying technology powering the
    Oracle FaaS offering called Oracle Functions)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Fission](https://fission.io)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Apache OpenWhisk](https://openwhisk.apache.org)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying Python Function to OpenFaaS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this example, we use a “Kubernetes-lite” distribution from Rancher called
    `k3s`. We use `k3s` instead of `minikube` to showcase the wide variety of tools
    available in the Kubernetes ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: Start by running the [`k3sup`](https://oreil.ly/qK0xJ) utility to provision
    a `k3s` Kubernetes cluster on an Ubuntu EC2 instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Download and install `k3sup`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Verify SSH connectivity into the remote EC2 instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Install `k3s` via `k3sup install`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Inspect the *kubeconfig* file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Point the `KUBECONFIG` environment variable to the local *kubeconfig* file
    and test `kubectl` commands against the remote k3s cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: The next step is to install the OpenFaas Serverless platform on the k3s Kubernetes
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Install `faas-cli` on the local macOS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Create RBAC permissions for Tiller, which is the server component of Helm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Install Tiller via `helm init`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Download, configure, and install the Helm chart for OpenFaaS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Generate a random password for basic authentication to the OpenFaaS gateway:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Deploy OpenFaaS by installing the Helm chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The `basic_auth` setup used here without TLS should ONLY be used for experimenting/learning.
    Any environment of consquence should be configured to ensure that credentials
    are passed over a secure TLS connection.
  prefs: []
  type: TYPE_NORMAL
- en: 'Verify the services running in the `openfaas` namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Forward port 8080 from the remote instance to port 8080 locally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Go to the OpenFaaS web UI at [*http://localhost:8080*](http://localhost:8080)
    and log in using username `admin` and password `$PASSWORD`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continue by creating an OpenFaaS Python function. Use the `faas-cli` tool to
    create a new OpenFaaS function called `hello-python`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Inspect the configuration file for the `hello-python` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Inspect the automatically created directory *hello-python*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Edit *handler.py* and bring over the code that prints the current time from
    the Serverless platform’s simple-http-example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to build the OpenFaaS Python function. Use the `faas-cli build`
    command, which will build a Docker image based on an autogenerated Dockerfile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Check that the Docker image is present locally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Tag and push the Docker image to Docker Hub registry so it can be used on the
    remote Kubernetes cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Edit *hello-python.yml* and change:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `faas-cli push` command to push the image to Docker Hub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, deploy the OpenFaaS Python function to the remote `k3s` cluster. Use
    the `faas-cli deploy` command to deploy the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `faas-cli login` command to obtain authenication credentials:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Edit *hello-python.yml* and change:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Because we are returning JSON from our handler, add these lines to *hello-python.yml*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Contents of *hello-python.yml*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the `faas-cli deploy` command again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'If a code change is needed, use the following commands to rebuild and redeploy
    the function. Note that the `faas-cli remove` command will delete the current
    version of the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Now test the deployed function with `curl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Test by invoking the function directly with `faas-cli`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: The next example will be more full featured. We will demonstrate how to use
    the AWS CDK to provision several Lambda functions behind an API Gateway for create/read/update/delete
    (CRUD) REST access to `todo` items stored in a DynamoDB table. We will also show
    how to load test our REST API with containers deployed in AWS Fargate and running
    the Locust load-testing tool against the API. The Fargate containers will also
    be provisioned with the AWS CDK.
  prefs: []
  type: TYPE_NORMAL
- en: Provisioning DynamoDB Table, Lambda Functions, and API Gateway Methods Using
    the AWS CDK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We briefly mentioned the AWS CDK in [Chapter 10](ch10.html#infra-as-code). AWS
    CDK is a product that allows you to define the desired state of the infrastructure
    using real code (currently supported languages are TypeScript and Python), as
    opposed to using a YAML definition file (as the Serverless platform does).
  prefs: []
  type: TYPE_NORMAL
- en: 'Install CDK CLI with `npm` at the global level (depending on your operating
    system, you may need to run the following command with `sudo`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a directory for the CDK application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a sample Python application with `cdk init`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'List the files created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Inspect the main file *app.py*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: A CDK program is composed of an *app* that can contain one or more *stacks*.
    A stack corresponds to a CloudFormation stack object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inspect the module defining the CDK stack:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: Because we are going to have two stacks, one for the DynamoDB/Lambda/API Gateway
    resources, and one for the Fargate resources, rename
  prefs: []
  type: TYPE_NORMAL
- en: '*cdk_lambda_dynamodb_fargate/cdk_lambda_dynamodb_fargate_stack.py*'
  prefs: []
  type: TYPE_NORMAL
- en: to *cdk_lambda_dynamodb_fargate/cdk_lambda_dynamodb_stack.py*
  prefs: []
  type: TYPE_NORMAL
- en: and the class `CdkLambdaDynamodbFargateStack` to `CdkLambdaDynamodbStack`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also change *app.py* to refer to the changed module and class names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Activate `virtualenv`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: We are going to take the [URL shortener CDK example](https://oreil.ly/q2dDF)
    and modify it with code from the [Serverless platform AWS Python REST API example](https://oreil.ly/o_gxS)
    to build a REST API for creating, listing, getting, updating, and deleting `todo`
    items. Amazon DynamoDB is used to store the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inspect the *serverless.yml* file from *examples/aws-python-rest-api-with-dynamodb*
    and deploy it with the `serverless` command to see what AWS resources get created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: The previous command created five Lambda functions, one API Gateway, and one
    DynamoDB table.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the CDK directory, add a DynamoDB table to the stack we are building:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'Install the required Python modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Inspect the CloudFormation stack that will be created by running `cdk synth`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'Pass a variable called `variable` containing the region value to the constructor
    `CdkLambdaDynamodbStack` in *app.py*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'Run `cdk synth` again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'Deploy the CDK stack by running `cdk deploy`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: The next step is to add Lambda functions and the API Gateway resource to the
    stack.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the CDK code directory, create a *lambda* directory and copy the Python
    modules from the [Serverless platform AWS Python REST API example](https://oreil.ly/mRSjn):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the required modules to *requirements.txt* and install them with `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Lambda and API Gateway constructs in the stack module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'It is worth noting several features of the code we just reviewed:'
  prefs: []
  type: TYPE_NORMAL
- en: We were able to use the `add_environment` method on each `handler` object to
    pass the environment variable `DYNAMODB_TABLE` used in the Python code for the
    Lambda functions and set it to `table.table_name`. The name of the DynamoDB table
    is not known at construction time, so the CDK will replace it with a token and
    will set the token to the correct name of the table when it deploys the stack
    (see the [Tokens](https://oreil.ly/XfdEU) documentation for more details).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We made full use of a simple programming language construct, the `for` loop,
    when we iterated over the list of all Lambda handlers. While this may seem natural,
    it is still worth pointing out because loops and variable passing are features
    that are awkwardly implemented, if at all, in YAML-based Infrastructure as Code
    tools such as Terraform.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We defined the HTTP methods (GET, POST, PUT, DELETE) associated with various
    endpoints of the API Gateway and associated the correct Lambda function with each
    of them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deploy the stack with `cdk deploy`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'Fix by running `cdk bootstrap`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'Deploy the CDK stack again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: The next step is to test the REST API with `curl`.
  prefs: []
  type: TYPE_NORMAL
- en: 'First create a new `todo` item:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a second `todo` item:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'Try getting the details for the item just created by specifying its ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: Investigate by inspecting the CloudWatch Logs for the Lambda function `TodoGetFunction:`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'To fix, change the line in *lambda/get.py* from:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: Redeploy the stack with `cdk deploy`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Try getting the `todo` item details with `curl` again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: Make the `import decimalencoder` change to all modules in the *lambda* directory
    that need the decimalencoder module and redeploy with `cdk deploy`.
  prefs: []
  type: TYPE_NORMAL
- en: 'List all `todos` and format the output with the `jq` utility:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'Delete a `todo` and verify that the list does not contain it anymore:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'Now test updating an existing `todo` item with `curl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: 'Inspecting the CloudWatch logs for the Lambda function associated with this
    endpoint shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: 'Change the validation test in *lambda/update.py* to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'Also change the value for `checked` to `True`, since we have already seen a
    post that we are trying to update:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: Redeploy the stack with `cdk deploy_`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Test updating the `todo` item with `curl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'List the `todo` items to verify the update:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: The next step is to provision AWS Fargate containers that will run a load test
    against the REST API we just deployed. Each container will run a Docker image
    that uses [the Taurus test automation framework](https://gettaurus.org) to run
    the [Molotov load-testing tool](https://oreil.ly/OGDne). We introduced Molotov
    in [Chapter 5](ch05.html#package_management) as a simple and very useful Python-based
    load-testing tool.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by creating a Dockerfile for running Taurus and Molotov in a directory
    called *loadtest*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: 'The Dockerfile runs the Taurus `bzt` command line using the *taurus.yaml* configuration
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: 'In this configuration file, the value for `concurrency` is set to 10, which
    means that we are simulating 10 concurrent users or virtual users (VUs). The `executor`
    is defined as a `molotov` test based on a script called *loadtest.py* in the *scripts*
    directory. Here is the script, which is a Python module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: The script has four functions decorated as `scenarios` to be run by Molotov.
    They exercise various endpoints of the CRUD REST API. The weights indicate the
    approximate percentage of the time of the overall test duration that each scenario
    will be invoked. For example, the `_test_list_todos` function will be invoked
    in this example approximately 50% of the time, `_test_create_todo` will run approximately
    30% of the time, and `_test_update_todo` and `_test_delete_todo` will each run
    approximately 10% of the time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Build the local Docker image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the local *artifacts* directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the local Docker image and mount the local *artifacts* directory as */tmp/artifacts*
    inside the Docker container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: Debug the Molotov script by inspecting the *artifacts/molotov.out* file.
  prefs: []
  type: TYPE_NORMAL
- en: Taurus results can be inspected either with `docker logs CONTAINER_ID` or by
    inspecting the file *artifacts/bzt.log*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Results obtained by inspecting the Docker logs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: Create CloudWatch dashboards for the Lambda duration ([Figure 13-1](#Figure-13-1))
    and DynamoDB provisioned and consumed read and write capacity units ([Figure 13-2](#Figure-13-2)).
  prefs: []
  type: TYPE_NORMAL
- en: '![pydo 1301](assets/pydo_1301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-1\. Lambda duration
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![pydo 1302](assets/pydo_1302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-2\. DynamoDB provisioned and consumed read and write capacity units
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The DynamoDB metrics show that we underprovisioned the DynamoDB read capacity
    units. This introduced latency, especially for the List function (shown in the
    Lambda duration graph as the red line going to 14.7 seconds), which retrieves
    all `todo` items from the DynamoDB table, and thus is heavy on read operations.
    We set the value of the provisioned read capacity units to 10 when we created
    the DynamoDB table, and the CloudWatch graph shows it going to 25.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s change the DynamoDB table type from `PROVISIONED` to `PAY_PER_REQUEST`.
    Make the change in *cdk_lambda_dynamodb_fargate/cdk_lambda_dynamodb_stack.py*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: Run `cdk deploy` and then run the local Docker load-testing container.
  prefs: []
  type: TYPE_NORMAL
- en: 'This time the results are much better:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: The graphs for Lambda duration ([Figure 13-3](#Figure-13-3)) and DynamoDB consumed
    read and write capacity units ([Figure 13-4](#Figure-13-4)) look much better as
    well.
  prefs: []
  type: TYPE_NORMAL
- en: '![pydo 1303](assets/pydo_1303.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-3\. Lambda duration
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![pydo 1304](assets/pydo_1304.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-4\. DynamoDB consumed read and write capacity units
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note that the DynamoDB consumed read capacity units are automatically allocated
    on demand by DynamoDB, and are scaling up to sustain the increased number of read
    requests from the Lambda functions. The function that contributes the most to
    the read requests is the List function that is called in the list, update, and
    delete scenarios in the Molotov *loadtest.py* script via `session.get(base_url
    + /todos)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will create a Fargate CDK stack that will run containers based on
    the Docker image created previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: 'A few things to note in the code for the `FargateStack` class:'
  prefs: []
  type: TYPE_NORMAL
- en: A new VPC is created by using the `aws_ec2.Vpc` CDK construct.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An ECS cluster is created in the new VPC.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Fargate task definition is created based on the Dockerfile from the *loadtest*
    directory; the CDK is smart enough to build a Docker image based on this Dockerfile
    and then push it to the ECR Docker registry.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An ECS service is created to run Fargate containers based on the image pushed
    to ECR; the `desired_count` parameter specifies how many containers we want to
    run.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Call the `FargateStack` constructor in *app.py*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: 'Deploy the `cdk-fargate` stack:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: Go to the AWS console and inspect the ECS cluster with the running Fargate container
    ([Figure 13-5](#Figure-13-5)).
  prefs: []
  type: TYPE_NORMAL
- en: '![pydo 1305](assets/pydo_1305.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-5\. ECS cluster with running Fargate container
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Inspect the CloudWatch dashboards for Lambda duration ([Figure 13-6](#Figure-13-6))
    and DynamoDB consumed read and write capacity units ([Figure 13-7](#Figure-13-7)),
    noting that latency looks good.
  prefs: []
  type: TYPE_NORMAL
- en: '![pydo 1306](assets/pydo_1306.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-6\. Lambda duration
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![pydo 1307](assets/pydo_1307.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-7\. DynamoDB consumed read and write capacity units
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Increase the Fargate container count to 5 in *cdk_lambda_dynamodb_fargate/cdk_fargate_stack.py*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: 'Redeploy the `cdk-fargate` stack:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: Inspect the CloudWatch dashboards for Lambda duration ([Figure 13-8](#Figure-13-8))
    and DynamoDB consumed read and write capacity units ([Figure 13-9](#Figure-13-9)).
  prefs: []
  type: TYPE_NORMAL
- en: '![pydo 1308](assets/pydo_1308.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-8\. Lambda duration
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![pydo 1309](assets/pydo_1309.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-9\. DynamoDB consumed read and write capacity units
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Both DynamoDB read capacity units and Lambda duration metrics increased as expected
    because we are now simulating 5 × 10 = 50 concurrent users.
  prefs: []
  type: TYPE_NORMAL
- en: To simulate more users, we can both increase the `concurrency` value in the
    *taurus.yaml* configuration file, and increase the `desired_count` for the Fargate
    containers. Between these two values, we can easily increase the load on our REST
    API endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: 'Delete the CDK stacks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: It is worth noting that the serverless architecture we deployed (API Gateway
    + five Lambda functions + DynamoDB table) turned out to be a good fit for our
    CRUD REST API application. We also followed best practices and defined all our
    infrastructure in Python code by using the AWS CDK.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Run a simple HTTP endpoint using Google’s CaaS platform: [Cloud Run](https://cloud.google.com/run).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Run a simple HTTP endpoint on the other FaaS platforms we mentioned that are
    based on Kubernetes: [Kubeless](https://kubeless.io), [Fn Project](https://fnproject.io),
    and [Fission](https://fission.io).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Install and configure [Apache OpenWhisk](https://openwhisk.apache.org) inside
    a production-grade Kubernetes cluster such as Amazon EKS, Google GKE, or Azure
    AKS.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Port the AWS REST API example to GCP and Azure. GCP offers [Cloud Endpoints](https://cloud.google.com/endpoints)
    to manage multiple APIs. Similarly, Azure offers [API Management](https://oreil.ly/tmDh7).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
