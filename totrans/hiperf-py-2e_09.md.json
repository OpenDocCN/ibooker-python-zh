["```py\ndef estimate_nbr_points_in_quarter_circle(nbr_estimates):\n    \"\"\"Monte Carlo estimate of the number of points in a\n quarter circle using pure Python\"\"\"\n    print(f\"Executing estimate_nbr_points_in_quarter_circle \\\n with {nbr_estimates:,} on pid {os.getpid()}\")\n    nbr_trials_in_quarter_unit_circle = 0\n    for step in range(int(nbr_estimates)):\n        x = random.uniform(0, 1)\n        y = random.uniform(0, 1)\n        is_in_unit_circle = x * x + y * y <= 1.0\n        nbr_trials_in_quarter_unit_circle += is_in_unit_circle\n\n    return nbr_trials_in_quarter_unit_circle\n```", "```py\nfrom multiprocessing import Pool\n...\n\nif __name__ == \"__main__\":\n    nbr_samples_in_total = 1e8\n    nbr_parallel_blocks = 4\n    pool = Pool(processes=nbr_parallel_blocks)\n    nbr_samples_per_worker = nbr_samples_in_total / nbr_parallel_blocks\n    print(\"Making {:,} samples per {} worker\".format(nbr_samples_per_worker,\n                                                     nbr_parallel_blocks))\n    nbr_trials_per_process = [nbr_samples_per_worker] * nbr_parallel_blocks\n    t1 = time.time()\n    nbr_in_quarter_unit_circles = pool.map(estimate_nbr_points_in_quarter_circle,\n                                           nbr_trials_per_process)\n    pi_estimate = sum(nbr_in_quarter_unit_circles) * 4 / float(nbr_samples_in_total)\n    print(\"Estimated pi\", pi_estimate)\n    print(\"Delta:\", time.time() - t1)\n```", "```py\n...\nfrom joblib import Parallel, delayed\n\nif __name__ == \"__main__\":\n    ...\n    nbr_in_quarter_unit_circles = Parallel(n_jobs=nbr_parallel_blocks, verbose=1) \\\n          (delayed(estimate_nbr_points_in_quarter_circle)(nbr_samples_per_worker) \\\n           for sample_idx in range(nbr_parallel_blocks))\n    ...\n```", "```py\nMaking 12,500,000 samples per 8 worker\n[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\nExecuting estimate_nbr_points_in_quarter_circle with 12,500,000 on pid 10313\nExecuting estimate_nbr_points_in_quarter_circle with 12,500,000 on pid 10315\nExecuting estimate_nbr_points_in_quarter_circle with 12,500,000 on pid 10311\nExecuting estimate_nbr_points_in_quarter_circle with 12,500,000 on pid 10316\nExecuting estimate_nbr_points_in_quarter_circle with 12,500,000 on pid 10312\nExecuting estimate_nbr_points_in_quarter_circle with 12,500,000 on pid 10314\nExecuting estimate_nbr_points_in_quarter_circle with 12,500,000 on pid 10317\nExecuting estimate_nbr_points_in_quarter_circle with 12,500,000 on pid 10318\n[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:   18.9s remaining:   56.6s\n[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   19.3s finished\nEstimated pi 3.14157744\nDelta: 19.32842755317688\n```", "```py\n...\nfrom joblib import Memory\n\nmemory = Memory(\"./joblib_cache\", verbose=0)\n\n@memory.cache\ndef estimate_nbr_points_in_quarter_circle_with_idx(nbr_estimates, idx):\n    print(f\"Executing estimate_nbr_points_in_quarter_circle with \\\n {nbr_estimates} on sample {idx} on pid {os.getpid()}\")\n    ...\n\nif __name__ == \"__main__\":\n    ...\n    nbr_in_quarter_unit_circles = Parallel(n_jobs=nbr_parallel_blocks) \\\n       (delayed(\n\t    estimate_nbr_points_in_quarter_circle_with_idx) \\\n        (nbr_samples_per_worker, idx) for idx in range(nbr_parallel_blocks))\n    ...\n```", "```py\n$ python pi_lists_parallel_joblib_cache.py\nMaking 12,500,000 samples per 8 worker\nExecuting estimate_nbr_points_in_... with 12500000 on sample 0 on pid 10672\nExecuting estimate_nbr_points_in_... with 12500000 on sample 1 on pid 10676\nExecuting estimate_nbr_points_in_... with 12500000 on sample 2 on pid 10677\nExecuting estimate_nbr_points_in_... with 12500000 on sample 3 on pid 10678\nExecuting estimate_nbr_points_in_... with 12500000 on sample 4 on pid 10679\nExecuting estimate_nbr_points_in_... with 12500000 on sample 5 on pid 10674\nExecuting estimate_nbr_points_in_... with 12500000 on sample 6 on pid 10673\nExecuting estimate_nbr_points_in_... with 12500000 on sample 7 on pid 10675\nEstimated pi 3.14179964\nDelta: 19.28862953186035\n\n$ python %run pi_lists_parallel_joblib_cache.py\nMaking 12,500,000 samples per 8 worker\nEstimated pi 3.14179964\nDelta: 0.02478170394897461\n```", "```py\ndef estimate_nbr_points_in_quarter_circle(nbr_samples):\n    \"\"\"Estimate pi using vectorized numpy arrays\"\"\"\n    np.random.seed() # remember to set the seed per process\n    xs = np.random.uniform(0, 1, nbr_samples)\n    ys = np.random.uniform(0, 1, nbr_samples)\n    estimate_inside_quarter_unit_circle = (xs * xs + ys * ys) <= 1\n    nbr_trials_in_quarter_unit_circle = np.sum(estimate_inside_quarter_unit_circle)\n    return nbr_trials_in_quarter_unit_circle\n```", "```py\ndef check_prime(n):\n    if n % 2 == 0:\n        return False\n    for i in range(3, int(math.sqrt(n)) + 1, 2):\n        if n % i == 0:\n            return False\n    return True\n```", "```py\nFLAG_ALL_DONE = b\"WORK_FINISHED\"\nFLAG_WORKER_FINISHED_PROCESSING = b\"WORKER_FINISHED_PROCESSING\"\n\ndef check_prime(possible_primes_queue, definite_primes_queue):\n    while True:\n        n = possible_primes_queue.get()\n        if n == FLAG_ALL_DONE:\n            # flag that our results have all been pushed to the results queue\n            definite_primes_queue.put(FLAG_WORKER_FINISHED_PROCESSING)\n            break\n        else:\n            if n % 2 == 0:\n                continue\n            for i in range(3, int(math.sqrt(n)) + 1, 2):\n                if n % i == 0:\n                    break\n            else:\n                definite_primes_queue.put(n)\n```", "```py\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Project description\")\n    parser.add_argument(\n        \"nbr_workers\", type=int, help=\"Number of workers e.g. 1, 2, 4, 8\"\n    )\n    args = parser.parse_args()\n    primes = []\n\n    manager = multiprocessing.Manager()\n    possible_primes_queue = manager.Queue()\n    definite_primes_queue = manager.Queue()\n\n    pool = Pool(processes=args.nbr_workers)\n    processes = []\n    for _ in range(args.nbr_workers):\n        p = multiprocessing.Process(\n            target=check_prime, args=(possible_primes_queue,\n                                      definite_primes_queue)\n        )\n        processes.append(p)\n        p.start()\n\n    t1 = time.time()\n    number_range = range(100_000_000, 101_000_000)\n\n    # add jobs to the inbound work queue\n    for possible_prime in number_range:\n        possible_primes_queue.put(possible_prime)\n\n    # add poison pills to stop the remote workers\n    for n in range(args.nbr_workers):\n        possible_primes_queue.put(FLAG_ALL_DONE)\n```", "```py\n    processors_indicating_they_have_finished = 0\n    while True:\n        new_result = definite_primes_queue.get()  # block while waiting for results\n        if new_result == FLAG_WORKER_FINISHED_PROCESSING:\n            processors_indicating_they_have_finished += 1\n            if processors_indicating_they_have_finished == args.nbr_workers:\n                break\n        else:\n            primes.append(new_result)\n    assert processors_indicating_they_have_finished == args.nbr_workers\n\n    print(\"Took:\", time.time() - t1)\n    print(len(primes), primes[:10], primes[-10:])\n```", "```py\ndef feed_new_jobs(number_range, possible_primes_queue, nbr_poison_pills):\n    for possible_prime in number_range:\n        possible_primes_queue.put(possible_prime)\n    # add poison pills to stop the remote workers\n    for n in range(nbr_poison_pills):\n        possible_primes_queue.put(FLAG_ALL_DONE)\n```", "```py\nif __name__ == \"__main__\":\n    primes = []\n    manager = multiprocessing.Manager()\n    possible_primes_queue = manager.Queue()\n\n    ...\n\n    import threading\n    thrd = threading.Thread(target=feed_new_jobs,\n                            args=(number_range,\n                                  possible_primes_queue,\n                                  NBR_PROCESSES))\n    thrd.start()\n\n    # deal with the results\n```", "```py\ndef check_prime(n):\n    if n % 2 == 0:\n        return False\n    from_i = 3\n    to_i = math.sqrt(n) + 1\n    for i in range(from_i, int(to_i), 2):\n        if n % i == 0:\n            return False\n    return True\n```", "```py\ndef check_prime(n, pool, nbr_processes):\n    from_i = 3\n    to_i = int(math.sqrt(n)) + 1\n    ranges_to_check = create_range.create(from_i, to_i, nbr_processes)\n    ranges_to_check = zip(len(ranges_to_check) * [n], ranges_to_check)\n    assert len(ranges_to_check) == nbr_processes\n    results = pool.map(check_prime_in_range, ranges_to_check)\n    if False in results:\n        return False\n    return True\n\nif __name__ == \"__main__\":\n    NBR_PROCESSES = 4\n    pool = Pool(processes=NBR_PROCESSES)\n    ...\n```", "```py\ndef check_prime_in_range(n_from_i_to_i):\n    (n, (from_i, to_i)) = n_from_i_to_i\n    if n % 2 == 0:\n        return False\n    assert from_i % 2 != 0\n    for i in range(from_i, int(to_i), 2):\n        if n % i == 0:\n            return False\n    return True\n```", "```py\ndef check_prime(n, pool, nbr_processes):\n    # cheaply check high-probability set of possible factors\n    from_i = 3\n    to_i = 21\n    if not check_prime_in_range((n, (from_i, to_i))):\n        return False\n\n    # continue to check for larger factors in parallel\n    from_i = to_i\n    to_i = int(math.sqrt(n)) + 1\n    ranges_to_check = create_range.create(from_i, to_i, nbr_processes)\n    ranges_to_check = zip(len(ranges_to_check) * [n], ranges_to_check)\n    assert len(ranges_to_check) == nbr_processes\n    results = pool.map(check_prime_in_range, ranges_to_check)\n    if False in results:\n        return False\n    return True\n```", "```py\nSERIAL_CHECK_CUTOFF = 21\nCHECK_EVERY = 1000\nFLAG_CLEAR = b'0'\nFLAG_SET = b'1'\nprint(\"CHECK_EVERY\", CHECK_EVERY)\n\nif __name__ == \"__main__\":\n    NBR_PROCESSES = 4\n    manager = multiprocessing.Manager()\n    value = manager.Value(b'c', FLAG_CLEAR)  # 1-byte character\n    ...\n```", "```py\ndef check_prime(n, pool, nbr_processes, value):\n    # cheaply check high-probability set of possible factors\n    from_i = 3\n    to_i = SERIAL_CHECK_CUTOFF\n    value.value = FLAG_CLEAR\n    if not check_prime_in_range((n, (from_i, to_i), value)):\n        return False\n\n    from_i = to_i\n    ...\n```", "```py\ndef check_prime_in_range(n_from_i_to_i):\n    (n, (from_i, to_i), value) = n_from_i_to_i\n    if n % 2 == 0:\n        return False\n    assert from_i % 2 != 0\n    check_every = CHECK_EVERY\n    for i in range(from_i, int(to_i), 2):\n        check_every -= 1\n        if not check_every:\n            if value.value == FLAG_SET:\n                return False\n            check_every = CHECK_EVERY\n\n        if n % i == 0:\n            value.value = FLAG_SET\n            return False\n    return True\n```", "```py\nFLAG_NAME = b'redis_primes_flag'\nFLAG_CLEAR = b'0'\nFLAG_SET = b'1'\n\nrds = redis.StrictRedis()\n\ndef check_prime_in_range(n_from_i_to_i):\n    (n, (from_i, to_i)) = n_from_i_to_i\n    if n % 2 == 0:\n        return False\n    assert from_i % 2 != 0\n    check_every = CHECK_EVERY\n    for i in range(from_i, int(to_i), 2):\n        check_every -= 1\n        if not check_every:\n            flag = rds[FLAG_NAME]\n            if flag == FLAG_SET:\n                return False\n            check_every = CHECK_EVERY\n\n        if n % i == 0:\n            rds[FLAG_NAME] = FLAG_SET\n            return False\n    return True\n\ndef check_prime(n, pool, nbr_processes):\n    # cheaply check high-probability set of possible factors\n    from_i = 3\n    to_i = SERIAL_CHECK_CUTOFF\n    rds[FLAG_NAME] = FLAG_CLEAR\n    if not check_prime_in_range((n, (from_i, to_i))):\n        return False\n\n    ...\n    if False in results:\n        return False\n    return True\n```", "```py\n$ redis-cli\nredis 127.0.0.1:6379> GET \"redis_primes_flag\"\n\"0\"\n```", "```py\nif __name__ == \"__main__\":\n    NBR_PROCESSES = 4\n    value = multiprocessing.RawValue('b', FLAG_CLEAR)  # 1-byte character\n    pool = Pool(processes=NBR_PROCESSES)\n    ...\n```", "```py\nsh_mem = mmap.mmap(-1, 1)  # memory map 1 byte as a flag\n\ndef check_prime_in_range(n_from_i_to_i):\n    (n, (from_i, to_i)) = n_from_i_to_i\n    if n % 2 == 0:\n        return False\n    assert from_i % 2 != 0\n    check_every = CHECK_EVERY\n    for i in range(from_i, int(to_i), 2):\n        check_every -= 1\n        if not check_every:\n            sh_mem.seek(0)\n            flag = sh_mem.read_byte()\n            if flag == FLAG_SET:\n                return False\n            check_every = CHECK_EVERY\n\n        if n % i == 0:\n            sh_mem.seek(0)\n            sh_mem.write_byte(FLAG_SET)\n            return False\n    return True\n\ndef check_prime(n, pool, nbr_processes):\n    # cheaply check high-probability set of possible factors\n    from_i = 3\n    to_i = SERIAL_CHECK_CUTOFF\n    sh_mem.seek(0)\n    sh_mem.write_byte(FLAG_CLEAR)\n    if not check_prime_in_range((n, (from_i, to_i))):\n        return False\n\n    ...\n    if False in results:\n        return False\n    return True\n```", "```py\ndef check_prime_in_range(n_from_i_to_i):\n    (n, (from_i, to_i)) = n_from_i_to_i\n    if n % 2 == 0:\n        return False\n    assert from_i % 2 != 0\n    check_next = from_i + CHECK_EVERY\n    for i in range(from_i, int(to_i), 2):\n        if check_next == i:\n            sh_mem.seek(0)\n            flag = sh_mem.read_byte()\n            if flag == FLAG_SET:\n                return False\n            check_next += CHECK_EVERY\n\n        if n % i == 0:\n            sh_mem.seek(0)\n            sh_mem.write_byte(FLAG_SET)\n            return False\n    return True\n```", "```py\ndef check_prime_in_range(n_from_i_to_i):\n    (n, (from_i, to_i)) = n_from_i_to_i\n    if n % 2 == 0:\n        return False\n    assert from_i % 2 != 0\n    for outer_counter in range(from_i, int(to_i), CHECK_EVERY):\n        upper_bound = min(int(to_i), outer_counter + CHECK_EVERY)\n        for i in range(outer_counter, upper_bound, 2):\n            if n % i == 0:\n                sh_mem.seek(0)\n                sh_mem.write_byte(FLAG_SET)\n                return False\n        sh_mem.seek(0)\n        flag = sh_mem.read_byte()\n        if flag == FLAG_SET:\n            return False\n    return True\n```", "```py\n$ python np_shared.py\nCreated shared array with 25,600,000,000 nbytes\nShared array id is 139636238840896 in PID 27628\nStarting with an array of 0 values:\n[[ 0\\.  0\\.  0\\. ...,  0\\.  0\\.  0.]\n ...,\n [ 0\\.  0\\.  0\\. ...,  0\\.  0\\.  0.]]\n\nOriginal array filled with value 42:\n[[ 42\\.  42\\.  42\\. ...,  42\\.  42\\.  42.]\n ...,\n [ 42\\.  42\\.  42\\. ...,  42\\.  42\\.  42.]]\nPress a key to start workers using multiprocessing...\n```", "```py\n worker_fn: with idx 0\n  id of local_nparray_in_process is 139636238840896 in PID 27751\n worker_fn: with idx 2000\n  id of local_nparray_in_process is 139636238840896 in PID 27754\n worker_fn: with idx 1000\n  id of local_nparray_in_process is 139636238840896 in PID 27752\n worker_fn: with idx 4000\n  id of local_nparray_in_process is 139636238840896 in PID 27753\n ...\n worker_fn: with idx 8000\n  id of local_nparray_in_process is 139636238840896 in PID 27752\n\nThe default value has been overwritten with worker_fn's result:\n[[27751\\. 27751\\. 27751\\. ... 27751\\. 27751\\. 27751.]\n ...\n [27751\\. 27751\\. 27751\\. ... 27751\\. 27751\\. 27751.]]\n```", "```py\nVerification - extracting unique values from 3,200,000,000 items\nin the numpy array (this might be slow)...\nUnique values in main_nparray:\n+---------+-----------+\n|   PID   |   Count   |\n+---------+-----------+\n| 27751.0 | 800000000 |\n| 27752.0 | 800000000 |\n| 27753.0 | 800000000 |\n| 27754.0 | 800000000 |\n+---------+-----------+\nPress a key to exit...\n```", "```py\n$ ps -A -o pid,size,vsize,cmd | grep np_shared\n27628 279676 25539428 python np_shared.py\n27751 279148 25342688 python np_shared.py\n27752 279148 25342688 python np_shared.py\n27753 279148 25342688 python np_shared.py\n27754 279148 25342688 python np_shared.py\n\nian@ian-Latitude-E6420 $ pmap -x 27628 | grep s-\nAddress           Kbytes     RSS   Dirty Mode   Mapping\n00007ef9a2853000 25000000 25000000 2584636 rw-s- pym-27628-npfjsxl6 (deleted)\n...\nian@ian-Latitude-E6420 $ pmap -x 27751 | grep s-\nAddress           Kbytes     RSS   Dirty Mode   Mapping\n00007ef9a2853000 25000000 6250104 1562508 rw-s- pym-27628-npfjsxl6 (deleted)\n...\n```", "```py\nimport os\nimport multiprocessing\nfrom collections import Counter\nimport ctypes\nimport numpy as np\nfrom prettytable import PrettyTable\n\nSIZE_A, SIZE_B = 10_000, 320_000  # 24GB\n\ndef worker_fn(idx):\n    \"\"\"Do some work on the shared np array on row idx\"\"\"\n    # confirm that no other process has modified this value already\n    assert main_nparray[idx, 0] == DEFAULT_VALUE\n    # inside the subprocess print the PID and ID of the array\n    # to check we don't have a copy\n    if idx % 1000 == 0:\n        print(\" {}: with idx {}\\n id of local_nparray_in_process is {} in PID {}\"\\\n            .format(worker_fn.__name__, idx, id(main_nparray), os.getpid()))\n    # we can do any work on the array; here we set every item in this row to\n    # have the value of the process ID for this process\n    main_nparray[idx, :] = os.getpid()\n```", "```py\nif __name__ == '__main__':\n    DEFAULT_VALUE = 42\n    NBR_OF_PROCESSES = 4\n\n    # create a block of bytes, reshape into a local numpy array\n    NBR_ITEMS_IN_ARRAY = SIZE_A * SIZE_B\n    shared_array_base = multiprocessing.Array(ctypes.c_double,\n                                              NBR_ITEMS_IN_ARRAY, lock=False)\n    main_nparray = np.frombuffer(shared_array_base, dtype=ctypes.c_double)\n    main_nparray = main_nparray.reshape(SIZE_A, SIZE_B)\n    # assert no copy was made\n    assert main_nparray.base.base is shared_array_base\n    print(\"Created shared array with {:,} nbytes\".format(main_nparray.nbytes))\n    print(\"Shared array id is {} in PID {}\".format(id(main_nparray), os.getpid()))\n    print(\"Starting with an array of 0 values:\")\n    print(main_nparray)\n    print()\n```", "```py\n    # Modify the data via our local numpy array\n    main_nparray.fill(DEFAULT_VALUE)\n    print(\"Original array filled with value {}:\".format(DEFAULT_VALUE))\n    print(main_nparray)\n\n    input(\"Press a key to start workers using multiprocessing...\")\n    print()\n\n    # create a pool of processes that will share the memory block\n    # of the global numpy array, share the reference to the underlying\n    # block of data so we can build a numpy array wrapper in the new processes\n    pool = multiprocessing.Pool(processes=NBR_OF_PROCESSES)\n    # perform a map where each row index is passed as a parameter to the\n    # worker_fn\n    pool.map(worker_fn, range(SIZE_A))\n```", "```py\n    print(\"Verification - extracting unique values from {:,} items\\n in the numpy \\\n array (this might be slow)...\".format(NBR_ITEMS_IN_ARRAY))\n    # main_nparray.flat iterates over the contents of the array, it doesn't\n    # make a copy\n    counter = Counter(main_nparray.flat)\n    print(\"Unique values in main_nparray:\")\n    tbl = PrettyTable([\"PID\", \"Count\"])\n    for pid, count in list(counter.items()):\n        tbl.add_row([pid, count])\n    print(tbl)\n\n    total_items_set_in_array = sum(counter.values())\n\n    # check that we have set every item in the array away from DEFAULT_VALUE\n    assert DEFAULT_VALUE not in list(counter.keys())\n    # check that we have accounted for every item in the array\n    assert total_items_set_in_array == NBR_ITEMS_IN_ARRAY\n    # check that we have NBR_OF_PROCESSES of unique keys to confirm that every\n    # process did some of the work\n    assert len(counter) == NBR_OF_PROCESSES\n\n    input(\"Press a key to exit...\")\n```", "```py\ndef work(filename, max_count):\n    for n in range(max_count):\n        f = open(filename, \"r\")\n        try:\n            nbr = int(f.read())\n        except ValueError as err:\n            print(\"File is empty, starting to count from 0, error: \" + str(err))\n            nbr = 0\n        f = open(filename, \"w\")\n        f.write(str(nbr + 1) + '\\n')\n        f.close()\n```", "```py\n$ python ex1_nolock1.py\nStarting 1 process(es) to count to 1000\nFile is empty, starting to count from 0,\nerror: invalid literal for int() with base 10: ''\nExpecting to see a count of 1000\ncount.txt contains:\n1000\n```", "```py\n$ python ex1_nolock4.py\nStarting 4 process(es) to count to 4000\nFile is empty, starting to count from 0,\nerror: invalid literal for int() with base 10: ''\n*# many errors like these*\nFile is empty, starting to count from 0,\nerror: invalid literal for int() with base 10: ''\nExpecting to see a count of 4000\ncount.txt contains:\n112\n\n$ python -m timeit -s \"import ex1_nolock4\" \"ex1_nolock4.run_workers()\"\n2 loops, best of 5: 158 msec per loop\n```", "```py\nimport multiprocessing\nimport os\n\n...\nMAX_COUNT_PER_PROCESS = 1000\nFILENAME = \"count.txt\"\n...\n\ndef run_workers():\n    NBR_PROCESSES = 4\n    total_expected_count = NBR_PROCESSES * MAX_COUNT_PER_PROCESS\n    print(\"Starting {} process(es) to count to {}\".format(NBR_PROCESSES,\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t                                            total_expected_count))\n    # reset counter\n    f = open(FILENAME, \"w\")\n    f.close()\n\n    processes = []\n    for process_nbr in range(NBR_PROCESSES):\n        p = multiprocessing.Process(target=work, args=(FILENAME,\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t                                          MAX_COUNT_PER_PROCESS))\n        p.start()\n        processes.append(p)\n\n    for p in processes:\n        p.join()\n\n    print(\"Expecting to see a count of {}\".format(total_expected_count))\n    print(\"{} contains:\".format(FILENAME))\n    os.system('more ' + FILENAME)\n\nif __name__ == \"__main__\":\n    run_workers()\n```", "```py\n$ python ex1_lock.py\nStarting 4 process(es) to count to 4000\nFile is empty, starting to count from 0,\nerror: invalid literal for int() with base 10: ''\nExpecting to see a count of 4000\ncount.txt contains:\n4000\n$ python -m timeit -s \"import ex1_lock\" \"ex1_lock.run_workers()\"\n10 loops, best of 3: 401 msec per loop\n```", "```py\n@fasteners.interprocess_locked('/tmp/tmp_lock')\ndef work(filename, max_count):\n    for n in range(max_count):\n        f = open(filename, \"r\")\n        try:\n            nbr = int(f.read())\n        except ValueError as err:\n            print(\"File is empty, starting to count from 0, error: \" + str(err))\n            nbr = 0\n        f = open(filename, \"w\")\n        f.write(str(nbr + 1) + '\\n')\n        f.close()\n```", "```py\n$ python ex2_nolock.py\nExpecting to see a count of 4000\nWe have counted to 2340\n$ python -m timeit -s \"import ex2_nolock\" \"ex2_nolock.run_workers()\"\n20 loops, best of 5: 9.97 msec per loop\n```", "```py\nimport multiprocessing\n\ndef work(value, max_count):\n    for n in range(max_count):\n        value.value += 1\n\ndef run_workers():\n...\n    value = multiprocessing.Value('i', 0)\n    for process_nbr in range(NBR_PROCESSES):\n        p = multiprocessing.Process(target=work, args=(value, MAX_COUNT_PER_PROCESS))\n        p.start()\n        processes.append(p)\n...\n```", "```py\n*`# lock on the update, but this isn't atomic`*\n$ `python` `ex2_lock``.``py`\nExpecting to see a count of 4000\nWe have counted to 4000\n$ `python` `-``m` `timeit` `-``s` `\"``import ex2_lock``\"` `\"``ex2_lock.run_workers()``\"`\n20 loops, best of 5: 19.3 msec per loop\n```", "```py\nimport multiprocessing\n\ndef work(value, max_count, lock):\n    for n in range(max_count):\n        with lock:\n            value.value += 1\n\ndef run_workers():\n...\n    processes = []\n    lock = multiprocessing.Lock()\n    value = multiprocessing.Value('i', 0)\n    for process_nbr in range(NBR_PROCESSES):\n        p = multiprocessing.Process(target=work,\n                                    args=(value, MAX_COUNT_PER_PROCESS, lock))\n        p.start()\n        processes.append(p)\n...\n```", "```py\nlock.acquire()\nvalue.value += 1\nlock.release()\n```", "```py\n*`# RawValue has no lock on it`*\n$ `python` `ex2_lock_rawvalue``.``py`\nExpecting to see a count of 4000\nWe have counted to 4000\n$ `python` `-``m` `timeit` `-``s` `\"``import ex2_lock_rawvalue``\"` `\"``ex2_lock_rawvalue.run_workers()``\"`\n50 loops, best of 5: 9.49 msec per loop\n```", "```py\n...\ndef run_workers():\n...\n    lock = multiprocessing.Lock()\n    value = multiprocessing.RawValue('i', 0)\n    for process_nbr in range(NBR_PROCESSES):\n        p = multiprocessing.Process(target=work,\n                                    args=(value, MAX_COUNT_PER_PROCESS, lock))\n        p.start()\n        processes.append(p)\n```"]