- en: Chapter 20\. Web Scraping Proxies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'That this is the last chapter in the book is somewhat appropriate. Until now
    you have been running all the Python applications from the command line, within
    the confines of your home computer. As the saying goes: “If you love something,
    set it free.”'
  prefs: []
  type: TYPE_NORMAL
- en: Although you might be tempted to put off this step as something you don’t *need*
    right now, you might be surprised at how much easier your life becomes when you
    stop trying to run Python scrapers from your laptop.
  prefs: []
  type: TYPE_NORMAL
- en: What’s more, since the first edition of this book was published in 2015, a whole
    industry of web scraping proxy companies has emerged and flourished. Paying someone
    to run a web scraper for you used to be a matter of paying for the cloud server
    instance and running your scraper on it like you would any other software. Now,
    you can make an API request to, essentially, say “fetch this website,” and a remote
    program will take care of the details, handle any security issues, and return
    the data to you (for a fee, of course!).
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll look at some methods that will allow you to route your
    requests through remote IP addresses, host and run your software elsewhere, and
    even offload the work to a web scraping proxy entirely.
  prefs: []
  type: TYPE_NORMAL
- en: Why Use Remote Servers?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although using a remote server might seem like an obvious step when launching
    a web application intended for use by a wide audience, often the tools programmers
    build for their own purposes are left running locally. In the absence of a motivation
    for moving the program elsewhere, why do anything? A reason to move it usually
    falls into one of two camps: the need for an alternate IP address (either because
    yours is blocked, or to prevent it from getting blocked), and the need for greater
    power and flexibility.'
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding IP Address Blocking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When building web scrapers, the rule of thumb is: almost everything can be
    faked. You can send emails from addresses you don’t own, automate mouse-movement
    data from a command line, or even horrify web administrators by sending their
    website traffic from Internet Explorer 9.0.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The one thing that cannot be faked is your IP address. In the real world, anyone
    can send you a letter with the return address: “The President, 1600 Pennsylvania
    Avenue Northwest, Washington, DC 20500.” However, if the letter is postmarked
    from Albuquerque, NM, you can be fairly certain you’re not corresponding with
    the President of the United States.^([1](ch20.html#id921))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Most efforts to stop scrapers from accessing websites focus on detecting the
    difference between humans and bots. Going so far as to block IP addresses is a
    little like a farmer giving up spraying pesticides in favor of just torching the
    field. It’s a last-ditch but effective method of discarding packets sent from
    troublesome IP addresses. However, there are problems with this solution:'
  prefs: []
  type: TYPE_NORMAL
- en: IP address access lists are painful to maintain. Although large websites most
    often have their own programs automating some of the routine management of these
    lists (bots blocking bots!), someone has to occasionally check them or at least
    monitor their growth for problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each address adds a tiny amount of processing time to receive packets, as the
    server must check received packets against the list to decide whether to approve
    them. Many addresses multiplied by many packets can add up quickly. To save on
    processing time and complexity, admins often group these IP addresses into blocks
    and make rules such as “all 256 addresses in this range are blocked” if there
    are a few tightly clustered offenders. Which leads us to the third point.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IP address blocking can lead to blocking the “good guys” as well. For example,
    while I was an undergrad at Olin College of Engineering, one student wrote some
    software that attempted to rig votes for content on the then-popular [*http://digg.com*](http://digg.com/).
    This software was blocked, and that single blocked IP address led to an entire
    dormitory being unable to access the site. The student simply moved his software
    to another server; in the meantime, Digg lost page visits from many regular users
    in its prime target demographic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Despite its drawbacks, IP address blocking remains an extremely common method
    for server administrators to stop suspected web scrapers from accessing servers.
    If an IP address is blocked, the only real solution is to scrape from a different
    IP address. This can be accomplished by moving the scraper to a new server or
    routing your traffic through a different server using a service such as Tor.
  prefs: []
  type: TYPE_NORMAL
- en: Portability and Extensibility
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some tasks are too large for a home computer and internet connection. Although
    you don’t want to put a large load on any single website, you might be collecting
    data across a wide range of sites and thus require a lot more bandwidth and storage
    than your current setup can provide.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, by offloading computationally intensive processing, you can free up
    your home machine’s cycles for more important tasks (*World of Warcraft*, anyone?).
    You don’t have to worry about maintaining power and an internet connection. You
    can launch your app at a Starbucks, pack up your laptop, and leave knowing that
    everything’s still running safely. Similarly, later on you can access your collected
    data anywhere there’s an internet connection.
  prefs: []
  type: TYPE_NORMAL
- en: If you have an application that requires so much computing power that a single
    Amazon extra-large computing instance won’t satisfy you, you can also look into
    *distributed computing*. This allows multiple machines to work in parallel to
    accomplish your goals. As a simple example, you might have one machine crawl one
    set of sites and another crawl a second set of sites, and have both of them store
    collected data in the same database.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, as noted in previous chapters, many can replicate what Google search
    does, but few can replicate the scale at which Google search does it. Distributed
    computing is a large field of computer science that is outside the scope of this
    book. However, learning how to launch your application onto a remote server is
    a necessary first step, and you might be surprised at what computers are capable
    of these days.
  prefs: []
  type: TYPE_NORMAL
- en: Tor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Onion Router network, better known by the acronym *Tor*, is a network of
    volunteer servers set up to route and reroute traffic through many layers (hence
    the onion reference) of different servers in order to obscure its origin. Data
    is encrypted before it enters the network so that if any particular server is
    eavesdropped on, the nature of the communication cannot be revealed. In addition,
    although the inbound and outbound communications of any particular server can
    be compromised, one would need to know the details of inbound and outbound communication
    for *all* the servers along the path of communication in order to decipher the
    true start and endpoints of a communication—a near-impossible feat.
  prefs: []
  type: TYPE_NORMAL
- en: Tor is commonly used by human rights workers and political whistleblowers to
    communicate with journalists, and it receives much of its funding from the US
    government. Of course, it is also commonly used for illegal activities, and so
    remains a constant target for government surveillance—although it’s unclear how
    useful this surveillance is.
  prefs: []
  type: TYPE_NORMAL
- en: Limits of Tor Anonymity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although the reason you are using Tor in this book is to change your IP address,
    not achieve complete anonymity per se, it is worth taking a moment to address
    some of the strengths and limitations of Tor’s ability to anonymize traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Although you can assume when using Tor that the IP address you are coming from,
    according to a web server, is not an IP address that can be traced back to you,
    any information you share with that web server might expose you. For instance,
    if you log in to your own Gmail account and then make incriminating Google searches,
    those searches can now be tied back to your identity.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond the obvious, however, even the act of logging in to Tor might be hazardous
    to your anonymity. In December 2013, a Harvard undergraduate student, in an attempt
    to get out of final exams, emailed a bomb threat to the school through the Tor
    network, using an anonymous email account. When the Harvard IT team looked at
    their logs, they found traffic going out to the Tor network from only a single
    machine, registered to a known student, during the time that the bomb threat was
    sent. Although they could not identify the eventual destination of this traffic
    (only that it was sent across Tor), the fact that the times matched up and only
    a single machine was logged in at the time was damning enough to prosecute the
    student.^([2](ch20.html#id930))
  prefs: []
  type: TYPE_NORMAL
- en: Logging in to Tor is not an automatic invisibility cloak, nor does it give you
    free rein to do as you please on the internet. Although it is a useful tool, be
    sure to use it with caution, intelligence, and, of course, morality.
  prefs: []
  type: TYPE_NORMAL
- en: Having Tor installed and running is a requirement for using Python with Tor,
    as you will see in the next section. Fortunately, the Tor service is extremely
    easy to install and start running with. Just go to the [Tor downloads page](https://www.torproject.org/download)
    and download, install, open, and connect. Keep in mind that your internet speed
    might appear to be slower while using Tor. Be patient—it might be going around
    the world several times!
  prefs: []
  type: TYPE_NORMAL
- en: PySocks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PySocks is a remarkably simple Python module that is capable of routing traffic
    through proxy servers and works fantastically in conjunction with Tor. You can
    download it from [its website](https://pypi.python.org/pypi/PySocks/1.5.0) or
    use any number of third-party module managers to install it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although not much in the way of documentation exists for this module, using
    it is extremely straightforward. The Tor service must be running on port 9150
    (the default port) while running this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The website [*http://icanhazip.com*](http://icanhazip.com) displays only the
    IP address for the client connecting to the server and can be useful for testing
    purposes. When this script is run, it should display an IP address that is not
    your own.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to use Selenium and ChromeDriver with Tor, you don’t need PySocks
    at all—just make sure that Tor is currently running and add the optional `proxy-server`
    Chrome option, specifying that Selenium should connect on the socks5 protocol
    on port 9150:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Again, this should print out an IP address that is not your own but the one
    that your running Tor client is currently using.
  prefs: []
  type: TYPE_NORMAL
- en: Remote Hosting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although complete anonymity is lost after you pull out your credit card, hosting
    your web scrapers remotely may dramatically improve their speed. This is because
    you’re able to purchase time on much larger machines than you likely own, but
    also because the connection no longer has to bounce through layers of a Tor network
    to reach its destination.
  prefs: []
  type: TYPE_NORMAL
- en: Running from a Website-Hosting Account
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you have a personal or business website, you might already likely have the
    means to run your web scrapers from an external server. Even with relatively locked-down
    web servers, where you have no access to the command line, it is possible to trigger
    scripts to start and stop through a web interface.
  prefs: []
  type: TYPE_NORMAL
- en: If your website is hosted on a Linux server, the server likely already runs
    Python. If you’re hosting on a Windows server, you might be out of luck; you’ll
    need to check specifically to see if Python is installed, or if the server administrator
    is willing to install it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most small web-hosting providers come with software called *cPanel*, used to
    provide basic administration services and information about your website and related
    services. If you have access to cPanel, you can make sure that Python is set up
    to run on your server by going to Apache Handlers and adding a new handler (if
    it is not already present):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This tells your server that all Python scripts should be executed as a *CGI-script*.
    CGI, which stands for *Common Gateway Interface*, is any program that can be run
    on a server and dynamically generate content that is displayed on a website. By
    explicitly defining Python scripts as CGI scripts, you’re giving the server permission
    to execute them, rather than just display them in a browser or send the user a
    download.
  prefs: []
  type: TYPE_NORMAL
- en: 'Write your Python script, upload it to the server, and set the file permissions
    to 755 to allow it to be executed. To execute the script, navigate to the place
    you uploaded it to through your browser (or even better, write a scraper to do
    it for you). If you’re worried about the general public accessing and executing
    the script, you have two options:'
  prefs: []
  type: TYPE_NORMAL
- en: Store the script at an obscure or hidden URL and make sure to never link to
    the script from any other accessible URL to avoid search engines indexing it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Protect the script with a password, or require that a password or secret token
    be sent to it before it can execute.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, running a Python script from a service that is specifically designed
    to display websites is a bit of a hack. For instance, you’ll probably notice that
    your web scraper-cum-website is a little slow to load. In fact, the page doesn’t
    actually load (complete with the output of all `print` statements you might have
    written in) until the entire scrape is complete. This might take minutes, hours,
    or never complete at all, depending on how it is written. Although it certainly
    gets the job done, you might want more real-time output. For that, you’ll need
    a server that’s designed for more than just the web.
  prefs: []
  type: TYPE_NORMAL
- en: Running from the Cloud
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Back in the olden days of computing, programmers paid for or reserved time on
    computers in order to execute their code. With the advent of personal computers,
    this became unnecessary—you simply write and execute code on your own computer.
    Now programmers are once again moving to pay-per-hour computing instances.
  prefs: []
  type: TYPE_NORMAL
- en: This time around, however, users aren’t paying for time on a single, physical
    machine but on its equivalent computing power, often spread among many machines.
    The nebulous structure of this system allows computing power to be priced according
    to times of peak demand. For instance, Amazon allows for bidding on “spot instances”
    when low costs are more important than immediacy.
  prefs: []
  type: TYPE_NORMAL
- en: Compute instances are also more specialized and can be selected based on the
    needs of your application, with options like “high memory,” “fast computing,”
    and “large storage.” Although web scrapers don’t typically use much in the way
    of memory, you may want to consider large storage or fast computing in lieu of
    a more general-purpose instance for your scraping application. If you’re doing
    large amounts of natural language processing, OCR work, or path finding (such
    as with the Six Degrees of Wikipedia problem), a fast computing instance might
    work well. If you’re scraping large amounts of data, storing files, or doing large-scale
    analytics, you might want to go for an instance with storage optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Although the sky is the limit as far as spending goes, at the time of this writing,
    instances start at just 0.9 cents (less than a penny) an hour for the cheapest
    Google instance, the f1-micro, and 0.8 cents an hour for a comparable Amazon EC2
    micro instance. Thanks to the economies of scale, buying a small compute instance
    with a large company is almost always cheaper than buying your own physical, dedicated
    machine. Because now you don’t need to hire an IT guy to keep it running.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, step-by-step instructions for setting up and running cloud computing
    instances are somewhat outside the scope of this book, but you will likely find
    that step-by-step instructions are not needed. With both Amazon and Google (not
    to mention the countless smaller companies in the industry) vying for cloud computing
    dollars, they’ve made setting up new instances as easy as following a simple prompt,
    thinking of an app name, and providing a credit card number. As of this writing,
    both Amazon and Google also offer hundreds of dollars’ worth of free computing
    hours to further tempt new clients.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re new to cloud computing, DigitalOcean is also a great provider of compute
    instances (which they call droplets), starting at 0.6 cents an hour. They have
    an incredibly easy user interface and simply email you the IP address and credentials
    for any new droplet they create so that you can log in and start running. Although
    they specialize more in web app hosting, DNS management, and load balancing, you
    can run anything you want from your instance!
  prefs: []
  type: TYPE_NORMAL
- en: Once you have an instance set up, you should be the proud new owner of an IP
    address, username, and public/private keys that can be used to connect to your
    instance through SSH. From there, everything should be the same as working with
    a server that you physically own—except, of course, you no longer have to worry
    about hardware maintenance or running your own plethora of advanced monitoring
    tools.
  prefs: []
  type: TYPE_NORMAL
- en: For quick and dirty jobs, especially if you don’t have a lot of experience dealing
    with SSH and key pairs, I’ve found that Google’s Cloud Platform instances can
    be easier to get up and running right away. They have a simple launcher and even
    have a button available after launch to view an SSH terminal right in the browser,
    as shown in [Figure 20-1](#browser-based-terminal).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/wsp3_2001.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 20-1\. Browser-based terminal from a running Google Cloud Platform VM
    instance
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Moving Forward
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The web is constantly changing. The technologies that bring us images, video,
    text, and other data files are constantly being updated and reinvented. To keep
    pace, the collection of technologies used to scrape data from the internet must
    also change.
  prefs: []
  type: TYPE_NORMAL
- en: Who knows? Future versions of this text may omit JavaScript entirely as an obsolete
    and rarely used technology and instead focus on HTML8 hologram parsing. However,
    what won’t change is the mindset and general approach needed to successfully scrape
    any website (or whatever we use for “websites” in the future).
  prefs: []
  type: TYPE_NORMAL
- en: 'When encountering any web scraping project, you should always ask yourself:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the question I want answered or the problem I want solved?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What data will help me achieve this and where is it?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How is the website displaying this data? Can I identify exactly which part of
    the website’s code contains this information?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can I isolate the data and retrieve it?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What processing or analysis needs to be done to make it more useful?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can I make this process better, faster, and more robust?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, you need to understand not just how to use the tools presented
    in this book in isolation but how they can work together to solve a larger problem.
    Sometimes the data is easily available and well formatted, allowing a simple scraper
    to do the trick. Other times you have to put some thought into it.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 16](ch16.html#c-16), for example, you combined the Selenium library
    to identify Ajax-loaded images on Amazon and Tesseract to use OCR to read them.
    In the Six Degrees of Wikipedia problem, you used regular expressions to write
    a crawler that stored link information in a database, and then used a graph-solving
    algorithm to answer the question, “What is the shortest path of links between
    Kevin Bacon and Eric Idle?”
  prefs: []
  type: TYPE_NORMAL
- en: 'There is rarely an unsolvable problem when it comes to automated data collection
    on the internet. Just remember: the internet is one giant API with a somewhat
    poor user interface.'
  prefs: []
  type: TYPE_NORMAL
- en: Web Scraping Proxies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This book discusses many products and technologies, with a focus on free and
    open-source software. In cases where paid products are discussed, it’s generally
    because a free alternative doesn’t exist, isn’t practical, and/or the paid products
    are so ubiquitous I’d feel remiss not to mention them.
  prefs: []
  type: TYPE_NORMAL
- en: The web scraping proxy and API service industry is an odd one, as far as industries
    go. It’s new, relatively niche, but still extremely crowded with a low barrier
    to entry. Because of this, there aren’t any big “household names” yet that all
    programmers would agree *require* discussion. Yes, some names are bigger than
    others, and some services are better than others, but it’s still quite the jungle
    out there.
  prefs: []
  type: TYPE_NORMAL
- en: Also, because web scraping proxying requires vast amounts of equipment and electricity
    to run, a viable free alternative does not exist and is unlikely to exist in the
    future.
  prefs: []
  type: TYPE_NORMAL
- en: This puts me in the precarious position of writing about an assortment of companies
    that you may not have heard of but that want your money. Rest assured, while I
    have opinions about these companies, I have not been paid for those opinions.
    I have used their services, spoken with their representatives, and in several
    cases been given free account credits for research purposes, but I do not have
    any incentive to promote them. I am not invested, either financially or emotionally,
    in any of these companies.
  prefs: []
  type: TYPE_NORMAL
- en: When you read this section, I suggest that you think more generally about the
    attributes of web scraping proxies and API services, their specialties, your budget,
    and your project requirements. These profiles are designed to be read as case
    studies and examples of “what’s out there” rather than specific endorsements.
    And if you do feel like giving any of these particular companies money, that’s
    between you and them!
  prefs: []
  type: TYPE_NORMAL
- en: ScrapingBee
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ScrapingBee is the smallest of the companies in this list. It has a strong focus
    on JavaScript automation, headless browsers, and innocuous-looking IP addresses.
    Its API is well documented but, if you prefer not to do any reading, ScrapingBee
    also has an API request generation tool on its website that reduces the problem
    to button clicking and copy/pasting.
  prefs: []
  type: TYPE_NORMAL
- en: An important feature to consider when evaluating proxy services is the amount
    of time it takes to return request data to you. Not only does the request have
    to be routed from your computer to their server to the target’s server and back
    again, but the proxy service may actually be buffering these requests on its end
    and not sending them out immediately. It’s not unusual for a request to take a
    minute or longer to return. During a formal evaluation, it’s important to time
    these requests throughout the day and time multiple types of requests for any
    features you might want to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using ScrapingBee’s API directly, we can scrape a product page and print both
    the results and the time it took to fetch them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'ScrapingBee also has a [Python package](https://pypi.org/project/scrapingbee/)
    that can be installed with pip:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a Software Development Kit (SDK) that let you use various features
    of the API in a slightly more convenient way. For example, the request above can
    be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the response is a Python requests response, and it can be used in
    the same way as in the previous example.
  prefs: []
  type: TYPE_NORMAL
- en: Scraping API services usually deal in units of “credits,” where one basic API
    request costs one credit. Features such as JavaScript rendering with a headless
    browser or a residential IP address may cost anywhere from 5 credits to 75 credits. Each
    paid account level gives you a certain number of credits per month.
  prefs: []
  type: TYPE_NORMAL
- en: While there is a free trial with 1,000 credits, ScrapingBee’s paid subscriptions
    start at $50/month for 150,000 credits, or 3,000 credits per dollar. Like with
    most of these services, there are large volume discounts—credits can be 13,000
    per dollar or less with greater monthly spend.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to maximize your requests, keep in mind that ScrapingBee charges
    5 credits for JavaScript rendering and turns it on by default. This means that
    the requests above will cost 5 credits each, not 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'This makes it convenient for customers who may not have read [Chapter 14](ch14.html#c-14)
    of this book and do not understand why the data appearing in their web browser
    does not appear in the scraping results coming back from ScrapingBee. If those
    customers read [Chapter 15](ch15.html#c-15), they would also understand how to
    get the data they want without JavaScript rendering at all. If you have read both
    of these chapters, you can turn off JavaScript rendering and reduce request costs
    by 80% using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Like many of these services, ScrapingBee offers the option of using “premium”
    IP addresses, which may prevent your scrapers from getting blocked by websites
    wary of IP addresses frequently used by bots. These IP addresses are reported
    as residential addresses owned by smaller telecommunication companies. If that’s
    not enough, ScrapingBee also offers a “stealth” IP address for 75 credits per
    request. The stealth IP addresses I was given were listed as datacenters and VPN
    servers, so it’s unclear what, exactly, the stealth IP addresses are and what
    real advantages they offer over the premium addresses.
  prefs: []
  type: TYPE_NORMAL
- en: ScraperAPI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ScraperAPI, true to its name, has a mostly clean and REST-ful API with tons
    of features. It supports asynchronous requests, which allow you to make the scraping
    request and fetch the results later in a separate API call. Alternatively, you
    can provide a webhook endpoint that the results are sent to after the request
    is complete.
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple one-credit call with ScraperAPI looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'ScraperAPI also has an SDK that can be installed with pip:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Like with most of these SDKs, it is a very thin wrapper around the Python requests
    library. As with the ScrapingBee API, a Python Requests response is returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'When evaluating web scraping services, it may be tempting to prefer those that
    have Python SDKs built around their APIs. However, you should carefully consider
    how much programming effort it will reduce or convenience it will provide. Technically,
    a Python “SDK” can be written around any scraping API with very little effort,
    including your own. This example SDK is written around an imaginary API in just
    a few lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'But one unique feature of ScraperAPI is its auto-parsing tools for Amazon products
    and Google search results. A request for an Amazon product page or an Amazon or
    Google search results page has a cost of 5 credits, rather the 1 credit for most
    requests. Although the documentation does mention an explicit call to the Amazon
    Product Endpoint at [*https://api.scraperapi.com/structured/amazon/product*](https://api.scraperapi.com/structured/amazon/product),
    this service appears to be turned on by default:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'With the response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: While writing an Amazon product parsing tool is hardly an insurmountable challenge,
    offloading the responsibility of testing and maintaining that parsing tool over
    the years may be well worth the costs.
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned before, ScraperAPI also allows you to make asynchronous requests
    to its API and fetch the results at a later time. This request takes less than
    100 ms to return:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Note that this is a `POST` request rather than a `GET` request, as shown in
    previous examples. We are, in a sense, posting data for the creation of a stored
    entity on Scraper​A⁠PI’s server. Also, the attribute used to send the key changes
    from `api_key` to `apiKey`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The response body contains only a URL where the job can be fetched:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Calling it does not require an API key—the UUID is sufficient security here—and,
    assuming the request has been completed on their end, it returns the target’s
    body:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The results of these async requests are stored for up to four hours, or until
    you retrieve the data. While you could accomplish a similar result at home with
    a multi-threaded scraper and a little code, you could not easily do it while rotating
    residential and mobile IP addresses, changing countries of origin, managing session
    data, rendering all the JavaScript (which will quickly bog down a machine), and
    tracking all successes and failures in a dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous requests and webhooks (where the proxy service returns the results
    to the URL you provide) are excellent features in an API service, especially for
    larger and longer-running scraping projects. ScraperAPI provides this at no extra
    cost per request, which is especially nice.
  prefs: []
  type: TYPE_NORMAL
- en: Oxylabs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Oxylabs is a large Lithuanian-based company with a focus on search engine results
    page (SERP) and product page scraping. Its product ecosystem and API have a bit
    of a learning curve. After creating an account, you must activate (either with
    a one-week trial or paid subscription) every “product” that you want to use and
    create separate username/password credentials specific to each product. These
    username/password credentials work a bit like an API key.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Web Scraper API product allows you to make calls that look like this, with
    a Web Scraper API username and password:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'However, the user may be in for a surprise if the target URL is switched to
    one from the amazon.com domain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This code prints an error message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Like ScraperAPI, Oxylabs has parsing tools predesigned for sites like Amazon
    and Google. However, to scrape those domains—with or without the special parsing
    tools—you must subscribe specifically to the SERP Scraper API product (to scrape
    Google, Bing, Baidu, or Yandex) or the E-Commerce Scraper API product (to scrape
    Amazon, Aliexpress, eBay, and many others).
  prefs: []
  type: TYPE_NORMAL
- en: 'If subscribed to the E-Commerce Scraper API product, the Amazon domain can
    be successfully scraped by changing the `source` attribute to `amazon` and passing
    in the E-Commerce-specific credentials:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This does not do anything special; it simply returns the content of the page
    as usual. To use the product information formatting templates, we must also set
    the attribute `parse` to `True`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This parses the website and returns formatted JSON data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'It’s important to keep in mind that parsing tools themselves are not specific
    to the E-Commerce Scraper API product. We can also parse the target.com domain
    using the regular Web Scraper API product, setting the source back to universal
    and using the Web Scraper API credentials:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Which returns JSON-formatted product data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Because it was attempting to parse pages at the domain target.com automatically,
    it is liable to run into errors here and there, like it did with the description.
    Fortunately, users can also write custom parsers, which are compatible with any
    API product type (Web Scraper API, SERP Scraper API, E-Commerce Scraper API, etc.).
    These custom parsers take the form of JSON files with a format specified by Oxylabs,
    which defines the various fields and the XPath selectors that collect data for
    them.
  prefs: []
  type: TYPE_NORMAL
- en: These custom parsers are essentially the “business logic” of the web scraper
    itself. It may be worth considering that, if you move to another web scraping
    API or proxy platform, these templates would be essentially useless and would
    need to be heavily modified, rewritten, or your new code base would need to be
    written specifically to work with them. Writing web scraping templates in the
    Oxylabs-specific language may be somewhat limiting if you choose to go elsewhere.
  prefs: []
  type: TYPE_NORMAL
- en: It’s also important to stress that these different API “products” (which, in
    fact, use the same API endpoint and call structure) are defined, based not on
    their particular features but on the domains they’re allowed to send requests
    to, which could change at any time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The domains under the purview of a specific product may not necessarily be
    well-supported by that product either. Oxylab’s SERP Scraping API advertises support
    for sites such as Baidu and Bing, but it does not have parsing templates developed
    for them. This “support” may be as simple as the ability to specify a search like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'instead of writing out the full URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Note that, while I am critical of some aspects of Oxylab’s API products, this
    criticism is not directed at the company per se and should not be interpreted
    as a comprehensive review or recommendation. I intend it only as a case study,
    or as an example for consideration, for those who might be evaluating similar
    products in the future.
  prefs: []
  type: TYPE_NORMAL
- en: When evaluating APIs and web scraping services, it’s always important to consider
    what is being advertised, what is being provided, and who the target audience
    is. The structure of an API call may reveal important information about the actual
    construction of a product, and even the documentation can be misleading.
  prefs: []
  type: TYPE_NORMAL
- en: Oxylabs has many excellent qualities as well. It is one of the best providers
    of proxy IP addresses. Oxylabs continuously sources a wide variety and large number
    of IP addresses, listed publicly as being residential, mobile, and data centers.
    Like other proxy services, these IP addresses are available at a higher cost,
    depending on the type. However, Oxylabs charges by the gigabyte for these proxy
    services, rather than the request. Currently, costs range from $22/GB (low-volume
    mobile IP addresses) to $8/GB (high-volume residential IP addresses).
  prefs: []
  type: TYPE_NORMAL
- en: Zyte
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Zyte, formerly Scrapinghub, is another large web scraping proxy and API service
    company. It’s also one of the oldest, founded in 2010. While I have no particular
    attachment to any of these companies, I would be lying if I said that, as the
    maintainers of Scrapy, Zyte doesn’t stand out from the crowd somewhat. And, beginning
    in 2019,  it also hosts the [Web Data Extraction Summit](https://www.extractsummit.io).
  prefs: []
  type: TYPE_NORMAL
- en: As a large company, Zyte has most of the features of the previous companies
    mentioned, and more. Unlike most others, it also sells data outright. If you need,
    for example, job postings, real estate data, or product information, it can provide
    those datasets or provide consultants who can build custom datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Zyte maintains Scrapy and has incorporated it into its product lineup in the
    form of Scrapy Cloud. This tool allows you to deploy and run Scrapy projects in
    the cloud from either a GitHub repository or from your local machine using the
    [Scrapinghub command-line client](https://pypi.org/project/shub/). This allows
    you to keep your web scrapers platform agnostic and portable but still interface
    tightly with the Zyte ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: Once a Scrapy project is deployed, Zyte finds all of the spider classes in the
    project and automatically loads them into your dashboard. You can use Zyte’s dashboard
    UI to launch and monitor these spiders as they run in the cloud, then view or
    download the resulting data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, Zyte also has an API. It is somewhat similar to the other APIs in
    that it heavily relies on the Python requests package. It is also similar to Oxylab’s
    API in that it uses the POST method entirely along with HTTP Basic Authentication.
    However, unlike Oxylab, only a Zyte key is sent over Basic Auth, rather than the
    username and password:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The other major difference is that all response bodies are returned as base64
    encoded strings, rather than HTML or JSON text. This is trivial to handle with
    Python’s `base64` package. It also allows you to retrieve binary data, images,
    and other files just like any other request response by simply decoding the response
    as that file type.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you don’t feel like using Scrapy and have a fairly straightforward project,
    Zyte’s Automatic Extraction API uses AI to detect various fields on a page and
    return them as JSON-formatted data. Currently, it works with both articles and
    product types. Obviously, it does not need to use base64 encoding because all
    the pages it parses must be text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The documentation for Zyte’s Automatic Extraction API provides the URL *https://autoextract.scrapinghub.com/v1/extract*,
    as an artifact of their previous name, ScrapingHub. If you see this, know that
    you can usually replace `zyte.com` with `scrapinghub.com` and give your code some
    backwards compatibility if Zyte decides to shut down the old domain.
  prefs: []
  type: TYPE_NORMAL
- en: Zyte’s products are heavily geared toward developers working in an enterprise
    environment who want full transparency and control over their scrapers. However,
    Zyte prefers to take IP address management out of the hands of users with its
    Zyte Smart Proxy Manager. Zyte controls which IP addresses the traffic is proxied
    through. IP addresses are maintained between sessions, but IP addresses are switched
    if one is being blocked. Zyte attempts to use IP address switching to create an
    organic-looking flow of traffic to a site that avoids suspicion.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the Smart Proxy Manager is straightforward, although installing certificates
    on your machine may add complexity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'If you don’t want to use a certificate (although this is not recommended) you
    can turn off verification in the requests module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Of course, Zyte also has instructions for integrating its [proxy services with
    Scrapy](https://scrapy-zyte-smartproxy.readthedocs.io/en/latest/), which can then
    be run in its Scrapy Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Proxy requests are around 1,600 per dollar (or less with more expensive monthly
    plans), API requests start around 12,000 per dollar. The Scrapy Cloud plans are
    relatively inexpensive, with a generous free tier and a $9/month “Professional”
    tier. This is likely to encourage the use of Scrapy and promote integration with
    the Zyte platform.
  prefs: []
  type: TYPE_NORMAL
- en: Additional Resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many years ago, running “in the cloud” was mostly the domain of those who felt
    like slogging through the documentation and already had some server administration
    experience. Today, the tools have improved dramatically due to increased popularity
    and competition among cloud computing providers.
  prefs: []
  type: TYPE_NORMAL
- en: Still, for building large-scale or more-complex scrapers and crawlers, you might
    want a little more guidance on creating a platform for collecting and storing
    data.
  prefs: []
  type: TYPE_NORMAL
- en: '[*Google Compute Engine*](http://oreil.ly/1FVOw6y) by Marc Cohen, Kathryn Hurley,
    and Paul Newson (O’Reilly) is a straightforward resource on using Google Cloud
    Computing with both Python and JavaScript. It covers not only Google’s user interface
    but also the command-line and scripting tools that you can use to give your application
    greater flexibility.'
  prefs: []
  type: TYPE_NORMAL
- en: If you prefer to work with Amazon, Mitch Garnaat’s [*Python and AWS Cookbook*](http://oreil.ly/VSctQP)
    (O’Reilly) is a brief but extremely useful guide that will get you started with
    Amazon Web Services and show you how to get a scalable application up and running.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch20.html#id921-marker)) Technically, IP addresses can be spoofed in outgoing
    packets, which is a technique used in distributed denial-of-service attacks, where
    the attackers don’t care about receiving return packets (which, if sent, will
    be sent to the wrong address). But web scraping is, by definition, an activity
    in which a response from the web server is required, so we think of IP addresses
    as one thing that can’t be faked.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch20.html#id930-marker)) See Nicholas P. Fandos, “Harvard Sophomore Charged
    in Bomb Threat,” *The Harvard Crimson*, December 17, 2023, [*https://www.thecrimson.com/article/2013/12/17/student-charged-bomb-threat*](https://www.thecrimson.com/article/2013/12/17/student-charged-bomb-threat).
  prefs: []
  type: TYPE_NORMAL
