- en: Chapter 8\. Scrapy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Chapter 7](ch07.html#c-7) presented some techniques and patterns for building
    large, scalable, and (most important!) maintainable web crawlers. Although this
    is easy enough to do by hand, many libraries, frameworks, and even GUI-based tools
    will do this for you or at least try to make your life a little easier.'
  prefs: []
  type: TYPE_NORMAL
- en: Since its release in 2008, Scrapy has quickly grown into the largest and best-maintained
    web scraping framework in Python. It is currently maintained by Zyte (formerly
    Scrapinghub).
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the challenges of writing web crawlers is that you’re often performing
    the same tasks again and again: find all links on a page, evaluate the difference
    between internal and external links, and go to new pages. These basic patterns
    are useful to know and to be able to write from scratch, but the Scrapy library
    handles many of these details for you.'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, Scrapy isn’t a mind reader. You still need to define page templates,
    give it locations to start scraping from, and define URL patterns for the pages
    that you’re looking for. But in these cases, it provides a clean framework to
    keep your code organized.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Scrapy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scrapy offers the tool for [download](http://scrapy.org/download/) from its
    website, as well as instructions for installing Scrapy with third-party installation
    managers such as pip.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because of its relatively large size and complexity, Scrapy is not usually
    a framework that can be installed in the traditional way with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note that I say “usually” because, though it is theoretically possible, I usually
    run into one or more tricky dependency issues, version mismatches, and unsolvable
    bugs.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re determined to install Scrapy from pip, using a virtual environment
    is highly recommended (see [“Keeping Libraries Straight with Virtual Environments”](ch04.html#KLSwVE_01)
    for more on virtual environments).
  prefs: []
  type: TYPE_NORMAL
- en: The installation method I prefer is through the [Anaconda package manager](https://docs.continuum.io/anaconda/).
    Anaconda is a product from the company Continuum, designed to reduce friction
    when it comes to finding and installing popular Python data science packages.
    Many of the packages it manages, such as NumPy and NLTK, will be used in later
    chapters as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'After you install Anaconda, you can install Scrapy by using this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: If you run into issues, or need up-to-date information, check out the [Scrapy](https://doc.scrapy.org/en/latest/intro/install.html)
    installation guide for more information.
  prefs: []
  type: TYPE_NORMAL
- en: Initializing a New Spider
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once you’ve installed the Scrapy framework, a small amount of setup needs to
    be done for each spider. A *spider* is a Scrapy project that, like its arachnid
    namesake, is designed to crawl webs. Throughout this chapter, I use “spider” to
    describe a Scrapy project in particular, and “crawler” to mean “any generic program
    that crawls the web, using Scrapy or not.”
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a new spider in the current directory, run the following from the
    command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This creates a new subdirectory in the directory the project was created in,
    with the title *wikiSpider*. Inside this directory is the following file structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '*scrapy.cfg*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*wikiSpider*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*spiders*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*__init.py__*'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*items.py*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*middlewares.py*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*pipelines.py*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*settings.py*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*__init.py__*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: These Python files are initialized with stub code to provide a fast means of
    creating a new spider project. Each section in this chapter works with this *wikiSpider*
    project.
  prefs: []
  type: TYPE_NORMAL
- en: Writing a Simple Scraper
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To create a crawler, you will add a new file inside the child *spiders* directory
    at *wiki​S⁠pider/wikiSpider/spiders/article.py*. This is where all the spiders,
    or things that extend scrapy.Spider will go. In your newly created *article.py* file,
    write:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The name of this class (`ArticleSpider`) does not reference “wiki” or “Wikipedia”
    at all, indicating that this class in particular is responsible for spidering
    through only article pages, under the broader category of *wikiSpider*, which
    you may later want to use to search for other page types.
  prefs: []
  type: TYPE_NORMAL
- en: For large sites with many types of content, you might have separate Scrapy items
    for each type (blog posts, press releases, articles, etc.), each with different
    fields but all running under the same Scrapy project. The name of each spider
    must be unique within the project.
  prefs: []
  type: TYPE_NORMAL
- en: 'The other key things to notice about this spider are the two functions `start_requests` and `parse`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`start_requests`'
  prefs: []
  type: TYPE_NORMAL
- en: A Scrapy-defined entry point to the program used to generate `Request` objects
    that Scrapy uses to crawl the website.
  prefs: []
  type: TYPE_NORMAL
- en: '`parse`'
  prefs: []
  type: TYPE_NORMAL
- en: A callback function defined by the user and passed to the `Request` object with `callback=self.parse`.
    Later, you’ll look at more powerful things that can be done with the `parse` function,
    but for now it prints the title of the page.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can run this `article` spider by navigating to the outer *wikiSpider* directory
    and running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The default Scrapy output is fairly verbose. Along with debugging information,
    this should print out lines like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The scraper goes to the three pages listed as the URLs, gathers information,
    and then terminates.
  prefs: []
  type: TYPE_NORMAL
- en: Spidering with Rules
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The spider in the previous section isn’t much of a crawler, confined to scraping
    only the list of URLs it’s provided. It has no ability to seek new pages on its
    own. To turn it into a fully fledged crawler, you need to use the `CrawlSpider`
    class provided by Scrapy.
  prefs: []
  type: TYPE_NORMAL
- en: Code Organization Within the GitHub Repository
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unfortunately, the Scrapy framework cannot be run easily from within a Jupyter
    notebook, making a linear progression of code difficult to capture. For the purpose
    of presenting all code samples in the text, the scraper from the previous section
    is stored in the *article.py* file, while the following example, creating a Scrapy
    spider that traverses many pages, is stored in *articles.py* (note the use of
    the plural).
  prefs: []
  type: TYPE_NORMAL
- en: Later examples will also be stored in separate files, with new filenames given
    in each section. Make sure you are using the correct filename when running these
    examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'This class can be found in the spiders file *articles.py* in the GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This new `ArticleSpider` extends the `CrawlSpider` class. Rather than providing
    a `start_requests` function, it provides a list of `start_urls` and `allowed_domains`.
    This tells the spider where to start crawling from and whether it should follow
    or ignore a link based on the domain.
  prefs: []
  type: TYPE_NORMAL
- en: A list of `rules` is also provided. This provides further instructions on which
    links to follow or ignore (in this case, you are allowing all URLs with the regular
    expression `.*`).
  prefs: []
  type: TYPE_NORMAL
- en: In addition to extracting the title and URL on each page, a couple of new items
    have been added. The text content of each page is extracted using an XPath selector.
    XPath is often used when retrieving text content including text in child tags
    (for example, an `<a>` tag inside a block of text). If you use the CSS selector
    to do this, all text within child tags will be ignored.
  prefs: []
  type: TYPE_NORMAL
- en: The last updated date string is also parsed from the page footer and stored
    in the `lastUpdated` variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can run this example by navigating to the *wikiSpider *directory and running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Warning: This Will Run Forever'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While this new spider runs in the command line in the same way as the simple
    spider built in the previous section, it will not terminate (at least not for
    a very, very long time) until you halt execution by using Ctrl-C or by closing
    the terminal. Please be kind to Wikipedia’s server load and do not run it for
    long.
  prefs: []
  type: TYPE_NORMAL
- en: 'When run, this spider traverses *wikipedia.org*, following all links under
    the domain *wikipedia.org*, printing titles of pages, and ignoring all external
    (offsite) links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a pretty good crawler so far, but it could use a few limits. Instead
    of just visiting article pages on Wikipedia, it’s free to roam to nonarticle pages
    as well, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s take a closer look at the line by using Scrapy’s `Rule` and `LinkExtractor`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This line provides a list of Scrapy `Rule` objects that define the rules that
    all found links are filtered through. When multiple rules are in place, each link
    is checked against the rules, in order. The first rule that matches is the one
    that is used to determine how the link is handled. If the link doesn’t match any
    rules, it is ignored.
  prefs: []
  type: TYPE_NORMAL
- en: 'A `Rule` can be provided with four arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '`link_extractor`'
  prefs: []
  type: TYPE_NORMAL
- en: The only mandatory argument, a `LinkExtractor` object.
  prefs: []
  type: TYPE_NORMAL
- en: '`callback`'
  prefs: []
  type: TYPE_NORMAL
- en: The function that should be used to parse the content on the page.
  prefs: []
  type: TYPE_NORMAL
- en: '`cb_kwargs`'
  prefs: []
  type: TYPE_NORMAL
- en: 'A dictionary of arguments to be passed to the callback function. This dictionary
    is formatted as `{arg_name1: arg_value1, arg_name2: arg_value2}` and can be a
    handy tool for reusing the same parsing functions for slightly different tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '`follow`'
  prefs: []
  type: TYPE_NORMAL
- en: Indicates whether you want links found at that page to be included in a future
    crawl. If no callback function is provided, this defaults to `True` (after all,
    if you’re not doing anything with the page, it makes sense that you’d at least
    want to use it to continue crawling through the site). If a callback function
    is provided, this defaults to `False`.
  prefs: []
  type: TYPE_NORMAL
- en: '`LinkExtractor` is a simple class designed solely to recognize and return links
    in a page of HTML content based on the rules provided to it. It has a number of
    arguments that can be used to accept or deny a link based on CSS and XPath selectors,
    tags (you can look for links in more than just anchor tags!), domains, and more.'
  prefs: []
  type: TYPE_NORMAL
- en: The `LinkExtractor` class can even be extended, and custom arguments can be
    created. See Scrapy’s [documentation on link extractors](https://doc.scrapy.org/en/latest/topics/link-extractors.html)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite all the flexible features of the `LinkExtractor` class, the most common
    arguments you’ll use are these:'
  prefs: []
  type: TYPE_NORMAL
- en: '`allow`'
  prefs: []
  type: TYPE_NORMAL
- en: Allow all links that match the provided regular expression.
  prefs: []
  type: TYPE_NORMAL
- en: '`deny`'
  prefs: []
  type: TYPE_NORMAL
- en: Deny all links that match the provided regular expression.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using two separate `Rule` and `LinkExtractor` classes with a single parsing
    function, you can create a spider that crawls Wikipedia, identifying all article
    pages and flagging nonarticle pages (*articleMoreRules.py*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Recall that the rules are applied to each link in the order that they are presented
    in the list. All article pages (pages that start with */wiki/* and do not contain
    a colon) are passed to the `parse_items` function first with the default parameter
    `is_article=True`. Then all the other nonarticle links are passed to the `parse_items`
    function with the argument `is_article=False`.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, if you’re looking to collect only article-type pages and ignore all
    others, this approach would be impractical. It would be much easier to ignore
    pages that don’t match the article URL pattern and leave out the second rule (and
    the `is_article` variable) altogether. However, this type of approach may be useful
    in odd cases where information from the URL, or information collected during crawling,
    impacts the way the page should be parsed.
  prefs: []
  type: TYPE_NORMAL
- en: Creating Items
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, you’ve looked at many ways of finding, parsing, and crawling websites
    with Scrapy, but Scrapy also provides useful tools to keep your collected items
    organized and stored in custom objects with well-defined fields.
  prefs: []
  type: TYPE_NORMAL
- en: To help organize all the information you’re collecting, you need to create an
    `Article` object. Define a new item called `Article` inside the *items.py* file.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you open the *items.py* file, it should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Replace this default `Item` stub with a new `Article` class extending `scrapy.Item`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'You are defining four fields that will be collected from each page: URL, title,
    text content, and the date the page was last edited.'
  prefs: []
  type: TYPE_NORMAL
- en: If you are collecting data for multiple page types, you should define each separate
    type as its own class in *items.py*. If your items are large, or you start to
    move more parsing functionality into your item objects, you may also wish to extract
    each item into its own file. While the items are small, however, I like to keep
    them in a single file.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the file *articleItems.py*, note the changes that were made to the `ArticleSpider`
    class in order to create the new `Article` item:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: When this file is run with
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'it will output the usual Scrapy debugging data along with each article item
    as a Python dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Using Scrapy `Items` isn’t just for promoting good code organization or laying
    things out in a readable way. Items provide many tools for outputting and processing
    data, covered in the next sections.
  prefs: []
  type: TYPE_NORMAL
- en: Outputting Items
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Scrapy uses the `Item` objects to determine which pieces of information it
    should save from the pages it visits. This information can be saved by Scrapy
    in a variety of ways, such as CSV, JSON, or XML files, using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Each of these runs the scraper `articleItems` and writes the output in the specified
    format to the provided file. This file will be created if it does not exist already.
  prefs: []
  type: TYPE_NORMAL
- en: You may have noticed that in the articles the spider created in previous examples,
    the text variable is a list of strings rather than a single string. Each string
    in this list represents text inside a single HTML element, whereas the content
    inside `<div id="mw-content-text">`, from which you are collecting the text data,
    is composed of many child elements.
  prefs: []
  type: TYPE_NORMAL
- en: Scrapy manages these more complex values well. In the CSV format, for example,
    it converts lists to strings and escapes all commas so that a list of text displays
    in a single CSV cell.
  prefs: []
  type: TYPE_NORMAL
- en: 'In XML, each element of this list is preserved inside child value tags:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In the JSON format, lists are preserved as lists.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, you can use the `Item` objects yourself and write them to a file
    or a database in whatever way you want, simply by adding the appropriate code
    to the parsing function in the crawler.
  prefs: []
  type: TYPE_NORMAL
- en: The Item Pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although Scrapy is single threaded, it is capable of making and handling many
    requests asynchronously. This makes it faster than the scrapers written so far
    in this book, although I have always been a firm believer that faster is not always
    better when it comes to web scraping.
  prefs: []
  type: TYPE_NORMAL
- en: The web server for the site you are trying to scrape must handle each of these
    requests, and it’s important to be a good citizen and evaluate whether this sort
    of server hammering is appropriate (or even wise for your own self-interests,
    as many websites have the ability and the will to block what they might see as
    malicious scraping activity). For more information about the ethics of web scraping,
    as well as the importance of appropriately throttling scrapers, see [Chapter 19](ch19.html#c-19).
  prefs: []
  type: TYPE_NORMAL
- en: With that said, using Scrapy’s item pipeline can improve the speed of your web
    scraper even further by performing all data processing while waiting for requests
    to be returned, rather than waiting for data to be processed before making another
    request. This type of optimization can even be necessary when data processing
    requires a great deal of time or processor-heavy calculations.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create an item pipeline, revisit the *settings.py* file created at the beginning
    of the chapter. You should see the following commented lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Uncomment the last three lines and replace them with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This provides a Python class, `wikiSpider.pipelines.WikispiderPipeline`, that
    will be used to process the data, as well as an integer that represents the order
    in which to run the pipeline if there are multiple processing classes. Although
    any integer can be used here, the numbers 0–1,000 are typically used and will
    be run in ascending order.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you need to add the pipeline class and rewrite your original spider so
    that the spider collects data and the pipeline does the heavy lifting of the data
    processing. It might be tempting to write the `parse_items` method in your original
    spider to return the response and let the pipeline create the `Article` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'However, the Scrapy framework does not allow this, and an `Item` object (such
    as an `Article`, which extends `Item`) must be returned. So the goal of `parse_items`
    is now to extract the raw data, doing as little processing as possible, so that
    it can be passed to the pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This file is saved as *articlePipelines.py* in the GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, now you need to tie the *pipelines.py* file and the updated spider
    together by adding the pipeline. When the Scrapy project was first initialized,
    a file was created at *wikiSpider/wikiSpider/pipelines.py*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This stub class should be replaced with your new pipeline code. In previous
    sections, you’ve been collecting two fields in a raw format, and these could use
    additional processing: `lastUpdated` (which is a badly formatted string object
    representing a date) and `text` (a messy array of string fragments).'
  prefs: []
  type: TYPE_NORMAL
- en: The following should be used to replace the stub code in *wikiSpider/wikiSpider/​pipe⁠lines.py:*
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The class `WikispiderPipeline` has a method `process_item` that takes in an
    `Article` object, parses the `lastUpdated` string into a Python `datetime` object,
    and cleans and joins the text into a single string from a list of strings.
  prefs: []
  type: TYPE_NORMAL
- en: '`process_item` is a mandatory method for every pipeline class. Scrapy uses
    this method to asynchronously pass `Items` that are collected by the spider. The
    parsed `Article` object that is returned here will be logged or printed by Scrapy
    if, for example, you are outputting items to JSON or CSV, as was done in the previous
    section.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You now have two choices when it comes to deciding where to do your data processing:
    the `parse_items` method in the spider, or the `process_items` method in the pipeline.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiple pipelines with different tasks can be declared in the *settings.py*
    file. However, Scrapy passes all items, regardless of item type, to each pipeline
    in order. Item-specific parsing may be better handled in the spider, before the
    data hits the pipeline. However, if this parsing takes a long time, you may want
    to consider moving it to the pipeline (where it can be processed asynchronously)
    and adding a check on the item type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Which processing to do and where to do it is an important consideration when
    it comes to writing Scrapy projects, especially large ones.
  prefs: []
  type: TYPE_NORMAL
- en: Logging with Scrapy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The debug information generated by Scrapy can be useful, but, as you’ve likely
    noticed, it is often too verbose. You can easily adjust the level of logging by
    adding a line to the *settings.py* file in your Scrapy project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Scrapy uses a standard hierarchy of logging levels, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`CRITICAL`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ERROR`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WARNING`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DEBUG`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`INFO`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If logging is set to `ERROR`, only `CRITICAL` and `ERROR` logs will be displayed.
    If logging is set to `INFO`, all logs will be displayed, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to controlling logging through the *settings.py* file, you can
    control where the logs go from the command line. To output logs to a separate
    logfile instead of the terminal, define a logfile when running from the command
    line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This creates a new logfile, if one does not exist, in your current directory
    and outputs all logs to it, leaving your terminal clear to display only the Python
    print statements you manually add.
  prefs: []
  type: TYPE_NORMAL
- en: More Resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scrapy is a powerful tool that handles many problems associated with crawling
    the web. It automatically gathers all URLs and compares them against predefined
    rules, makes sure all URLs are unique, normalizes relative URLs where needed,
    and recurses to go more deeply into pages.
  prefs: []
  type: TYPE_NORMAL
- en: I encourage you to check out the [Scrapy documentation](https://doc.scrapy.org/en/latest/news.html)
    as well as [Scrapy’s official tutorial pages](https://docs.scrapy.org/en/latest/intro/tutorial.html),
    which provide a comprehensive discourse on the framework.
  prefs: []
  type: TYPE_NORMAL
- en: Scrapy is an extremely large, sprawling library with many features. Its features
    work together seamlessly but have many areas of overlap that allow users to easily
    develop their own particular style within it. If there’s something you’d like
    to do with Scrapy that has not been mentioned here, there is likely a way (or
    several) to do it!
  prefs: []
  type: TYPE_NORMAL
