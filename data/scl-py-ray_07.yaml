- en: Chapter 6\. Implementing Streaming Applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章 实施流处理应用程序
- en: So far in the book, we have been using Ray to implement serverless batch applications.
    In this case, data is collected, or provided from the user, and then used for
    calculations. Another important group of use cases are the situations requiring
    you to process data in real time. We use the overloaded term *real* time to mean
    processing the data as it arrives within some latency constraints. This type of
    data processing is called *streaming*.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本书中，我们一直在使用Ray来实现无服务器批处理应用程序。在这种情况下，数据被收集或者从用户那里提供，然后用于计算。另一个重要的用例组是需要您实时处理数据的情况。我们使用过载的术语*实时*来表示在某些延迟约束条件下处理数据。这种类型的数据处理称为*流处理*。
- en: In this book, we define *streaming* as taking action on a series of data close
    to the time that the data is created.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将*流*定义为在数据生成接近时间点时采取行动。
- en: 'Some common streaming [use cases](https://oreil.ly/QQnmm) include the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 一些常见的流处理[用例](https://oreil.ly/QQnmm)包括以下内容：
- en: Log analysis
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 日志分析
- en: A way of gaining insights into the state of your hardware and software. It is
    typically implemented as a distributed processing of streams of logs as they are
    being produced.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 一种了解硬件和软件状态的方式。通常作为流日志的分布式处理实现，即在产生日志时即时处理流。
- en: Fraud detection
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 欺诈检测
- en: The monitoring of financial transactions and watching for anomalies that signal
    fraud in real time and stopping fraudulent transactions.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 监控金融交易并查找实时信号的异常，以及停止欺诈交易。
- en: Cybersecurity
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 网络安全
- en: The monitoring of interactions with the system to detect anomalies, allowing
    the identification of security issues in real time to isolate threats.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 监控系统与用户的互动以检测异常，允许实时识别安全问题并隔离威胁。
- en: Streaming logistics
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理物流
- en: The monitoring of cars, trucks, fleets, and shipments in real time, to optimize
    routing.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 实时监控汽车、卡车、车队和货物，以优化路由。
- en: IoT data processing
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 物联网数据处理
- en: An example is collecting data about an engine to gain insights that can detect
    a faulty situation before becoming a major problem.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，收集关于引擎的数据，以便在问题变成主要问题之前检测到故障情况。
- en: Recommendation engines
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐引擎
- en: Used to understand user interests based on online behavior for serving ads,
    recommending products and services, etc.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 用于根据在线行为了解用户兴趣，用于提供广告、推荐产品和服务等。
- en: 'When it comes to implementing streaming applications in Ray, you currently
    have two main options:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当谈到在Ray中实现流处理应用程序时，您目前有两个主要选项：
- en: Ray’s ecosystem provides a lot of underlying components, described in the previous
    chapters, that can be used for custom implementations of streaming applications.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ray的生态系统提供了许多底层组件，这些组件在前几章中已经描述过，可以用于自定义实现流处理应用程序。
- en: External libraries and tools can be used with Ray to implement streaming.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用外部库和工具与Ray一起实现流处理。
- en: Ray is not built as a streaming system. It is an ecosystem that enables companies
    to build streaming systems on these lower-level primitives. You can find several
    stories of users from big and small companies building streaming applications
    on top of Ray.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Ray并非作为一个流处理系统构建。它是一个生态系统，使公司能够在这些低级基元上构建流处理系统。你可以找到多个大公司和小公司用户构建基于Ray的流处理应用程序的故事。
- en: With that being said, building a small streaming application on Ray will give
    you a perfect example of how to think about Ray application and how to use Ray
    effectively, and will allow you to understand the basics of streaming applications
    and how Ray’s capabilities can be leveraged for its implementation. Even if you
    decide to use external libraries, this material will help you make better decisions
    on whether and how to use these libraries.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 说到这里，在Ray上构建一个小型流处理应用程序将为您提供一个完美的例子，展示如何思考Ray应用程序以及如何有效使用Ray，并将帮助您理解流处理应用程序的基础知识以及如何利用Ray的能力来实现它。即使您决定使用外部库，这些材料也将帮助您更好地决定是否以及如何使用这些库。
- en: One of the most popular approaches for implementing streaming applications is
    using [Apache Kafka](https://oreil.ly/kMiQC) to connect data producers with consumers
    implementing data processing. Before delving into Ray’s streaming implementation,
    let’s start with a quick introduction to Kafka.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 实施流处理应用程序的一个最受欢迎的方法是使用[Apache Kafka](https://oreil.ly/kMiQC)连接数据生产者和实施数据处理的消费者。在深入研究Ray的流处理实现之前，让我们先快速介绍一下Kafka。
- en: Apache Kafka
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[Apache Kafka](https://oreil.ly/kMiQC)'
- en: Here we describe only features of Kafka that are relevant for our discussion.
    For in-depth information, refer to the [Kafka documentation](https://oreil.ly/E9Inp).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这里仅描述了对我们讨论有关的Kafka功能。有关详细信息，请参阅[Kafka文档](https://oreil.ly/E9Inp)。
- en: Basic Kafka Concepts
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kafka基础概念
- en: Although many people consider Kafka to be a type of messaging system—similar
    to, for example, [RabbitMQ](https://oreil.ly/UD8ov)—it is a very different thing.
    Kafka is a [distributed log](https://oreil.ly/zXwQs) that stores records sequentially
    (see [Figure 6-1](#Distributed-log)).^([1](ch06.html#idm45354776016592))
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管许多人认为Kafka是一种类似于例如[RabbitMQ](https://oreil.ly/UD8ov)的消息系统，但实际上它是完全不同的东西。Kafka是一个[分布式日志](https://oreil.ly/zXwQs)，按顺序存储记录（见[图6-1](#Distributed-log)）^([1](ch06.html#idm45354776016592))。
- en: '![spwr 0601](assets/spwr_0601.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![spwr 0601](assets/spwr_0601.png)'
- en: Figure 6-1\. Distributed log
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-1\. 分布式日志
- en: Kafka records are key/value pairs. (Both the key and value are optional, and
    an empty value can be used to tombstone an existing value.) Both keys and values
    are represented in Kafka as byte arrays and are opaque to Kafka itself. Producers
    always write to the end of the log, while consumers can choose the position (offset)
    where they want to read from.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka记录是键值对。（键和值都是可选的，可以使用空值来标记现有值的删除。）键和值都被表示为字节数组，并且对Kafka本身是不透明的。生产者始终写入日志的末尾，而消费者可以选择他们希望从哪个位置（偏移量）读取。
- en: 'The main differences between log-oriented systems like Kafka and messaging
    systems like RabbitMQ are as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka等面向日志的系统与RabbitMQ等消息系统的主要区别如下：
- en: Messages in queue systems are ephemeral; they are kept in the system only until
    they are delivered. Messages in log-based systems, on the other hand, are persistent.
    As a result, you can replay messages in a log-based system, which is impossible
    in traditional messaging.^([2](ch06.html#idm45354775801840))
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 队列系统中的消息是短暂的；它们只在系统中保留到被传递。另一方面，日志系统中的消息是持久的。因此，您可以在日志系统中重播消息，这在传统消息系统中是不可能的^([2](ch06.html#idm45354775801840))。
- en: While traditional message brokers manage consumers and their offsets, in log
    systems consumers are responsible for managing their offsets. This allows a log-based
    system to support significantly more consumers.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在传统的消息代理中，管理消费者及其偏移量，而在日志系统中，消费者负责管理其偏移量。这使得基于日志的系统能够支持更多的消费者。
- en: Similar to messaging systems, Kafka organizes data into *topics*. Unlike messaging
    systems, topics in Kafka are purely logical constructs, composed of multiple partitions
    ([Figure 6-2](#Anatomy-of-topic)).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 与消息系统类似，Kafka将数据组织成*主题*。不同于消息系统的是，Kafka中的主题是纯逻辑构造，由多个分区组成（见[图6-2](#Anatomy-of-topic)）。
- en: '![spwr 0602](assets/spwr_0602.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![spwr 0602](assets/spwr_0602.png)'
- en: Figure 6-2\. Anatomy of a topic
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-2\. 主题的解剖
- en: Data in a partition is sequential and can be replicated across multiple brokers.
    Partitioning is a vital scalability mechanism, allowing individual consumers to
    read dedicated partitions in parallel and allowing Kafka to store the partitions
    separately.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 分区中的数据是顺序的，并可以在多个代理（broker）之间复制。分区是一种重要的可扩展性机制，允许单独的消费者并行读取专用分区，并允许Kafka将分区分开存储。
- en: 'When writing to topics, Kafka supports two main partitioning mechanisms during
    the write operation: if a key is not defined, it uses round-robin partitioning,
    distributing the topic’s messages equally across partitions; if the key is defined,
    the partition to write to is determined by the key. By default, Kafka uses key
    hashing for partioning. You can also implement custom partitioning mechanisms
    with Kafka. Message ordering happens only within a partition, so any messages
    to be processed in order must be in the same partition.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在向主题写入时，Kafka支持两种主要的分区机制：如果未定义键，则使用轮询分区，将主题的消息均匀分布在分区中；如果定义了键，则写入的分区由键确定。默认情况下，Kafka使用键哈希进行分区。您还可以使用Kafka实现自定义的分区机制。消息排序仅在分区内部进行，因此要按顺序处理的任何消息必须位于同一分区中。
- en: You deploy Kafka in the form of a cluster composed of multiple (1 to *n*) brokers
    (servers) to maintain load balancing.^([3](ch06.html#idm45354775789952)) Depending
    on the configured replication factor, each partition can exist on one or more
    brokers, and this can improve Kafka’s throughput. Kafka clients can connect to
    any broker, and the broker routes the requests transparently to one of the correct
    brokers.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 将 Kafka 以多个（1 至 *n*）经纪人（服务器）组成的集群形式部署，以维护负载平衡。^([3](ch06.html#idm45354775789952))
    根据配置的复制因子，每个分区可以存在于一个或多个经纪人上，这可以提高 Kafka 的吞吐量。Kafka 客户端可以连接到任何经纪人，并且经纪人会将请求透明地路由到正确的经纪人之一。
- en: To understand how applications scale with Kafka, you need to understand how
    Kafka’s consumer groups work ([Figure 6-3](#Kafka-consumer-group)).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解应用程序如何与 Kafka 一起扩展，您需要理解 Kafka 消费者组的工作方式（[图 6-3](#Kafka-consumer-group)）。
- en: '![spwr 0603](assets/spwr_0603.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![spwr 0603](assets/spwr_0603.png)'
- en: Figure 6-3\. Kafka consumer group
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-3\. Kafka 消费者组
- en: You can assign consumers that read from the same set of topics to a *consumer
    group*. Kafka then gives each consumer in the group a subset of the partitions.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以分配从相同主题集中读取的消费者到一个 *消费者组*。然后 Kafka 为组中的每个消费者分配一个分区子集。
- en: For example, if you have a topic with 10 partitions and a single consumer in
    a group, this consumer will read all of the topics’ partitions. With the same
    topic, if you instead have 5 consumers in the group, each consumer will read two
    partitions from the topic. If you have 11 consumers, 10 of them will each read
    a single partition, and the 11th one will not read any data.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果您有一个具有 10 个分区和单个消费者的主题，这个消费者将读取所有主题的分区。对于相同的主题，如果您的消费者组中有 5 个消费者，每个消费者将从主题中读取两个分区。如果您有
    11 个消费者，则其中 10 个消费者将各自读取一个分区，而第 11 个将不读取任何数据。
- en: As you can see, the two main factors in how much you can scale your Kafka reading
    is the number of partitions and the number of consumers in your consumer group.
    Adding more consumers to a consumer group is easier than adding new partitions,
    so overprovisioning the number of partitions is a best practice.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，您可以扩展 Kafka 读取的两个主要因素是分区数量和消费者组中的消费者数量。向消费者组添加更多消费者比添加新分区更容易，因此超额配置分区数量是最佳实践。
- en: Kafka APIs
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kafka API
- en: 'As defined in the [Kafka documentation](https://oreil.ly/1Edbr), Kafka has
    five core API groups:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [Kafka 文档](https://oreil.ly/1Edbr) 所定义，Kafka 具有五个核心 API 组：
- en: Producer API
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 生产者 API
- en: Allows applications to send streams of data to topics in the Kafka cluster
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 允许应用程序向 Kafka 集群中的主题发送数据流
- en: Consumer API
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 消费者 API
- en: Allows applications to read streams of data from topics in the Kafka cluster
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 允许应用程序从 Kafka 集群中的主题读取数据流
- en: AdminClient API
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: AdminClient API
- en: Allows managing and inspecting topics, brokers, and other Kafka objects
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 允许管理和检查主题、经纪人和其他 Kafka 对象
- en: Streams API
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 流 API
- en: Allows transforming streams of data from input topics to output topics
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 允许将输入主题的数据流转换到输出主题
- en: Connect API
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 连接 API
- en: Allows implementing connectors that continually pull from a source system or
    application into Kafka or push from Kafka into a sink system or application
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 允许实现连接器，持续地从源系统或应用程序拉取到 Kafka，或者从 Kafka 推送到接收系统或应用程序
- en: These APIs are implemented in multiple [languages](https://oreil.ly/gPVs8),
    including Java, C/C++, Go, C#, and Python. We will be using Kafka’s [Python APIs](https://oreil.ly/c7g3l)
    for integration with Ray, implementing the first three APIs groups, which is sufficient
    for our purposes. For a simple example of using Python Kafka APIs, see this book’s
    [GitHub repo](https://oreil.ly/0VJ3D).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这些 API 实现了多种 [语言](https://oreil.ly/gPVs8)，包括 Java、C/C++、Go、C# 和 Python。我们将使用
    Kafka 的 [Python API](https://oreil.ly/c7g3l) 与 Ray 集成，实现前三个 API 组，这对我们的目的足够了。关于使用
    Python Kafka API 的简单示例，请参阅本书的 [GitHub 仓库](https://oreil.ly/0VJ3D)。
- en: Unlike other messaging systems, Kafka does not guarantee nonduplicate messages.
    Instead, each Kafka consumer is responsible for ensuring that messages are processed
    only once.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他消息传递系统不同，Kafka 不保证不重复的消息。相反，每个 Kafka 消费者负责确保消息仅被处理一次。
- en: Note
  id: totrans-58
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: If you are interested in learning more, the Confluent [“Kafka Python Client”
    documentation](https://oreil.ly/QOfMd) has more information on commit options
    and their implications on delivery guarantees. By default, the Python client uses
    automatic commit, which is what we use in our examples. For real-life implementation,
    consider delivery guarantees (exactly once, at least once, etc.) that you need
    to provide and use an appropriate commit approach.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有兴趣了解更多信息，Confluent的[“Kafka Python Client”文档](https://oreil.ly/QOfMd)提供了有关提交选项及其对交付保证的影响的更多信息。默认情况下，Python客户端使用自动提交，这也是我们在示例中使用的。对于实际实现，请考虑您需要提供的交付保证（精确一次、至少一次等），并使用适当的提交方法。
- en: Using Kafka with Ray
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Kafka与Ray
- en: 'Now that you know about Kafka and its basic APIs, let’s take a look at options
    for integrating Kafka with Ray. We will implement both the Kafka consumer and
    producer as Ray actors.^([4](ch06.html#idm45354775738704)) You can benefit from
    using Ray actors with Kafka for these reasons:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了Kafka及其基本API，让我们来看看如何将Kafka与Ray集成的选项。我们将实现Kafka消费者和生产者都作为Ray actor。^([4](ch06.html#idm45354775738704))
    使用Ray actor与Kafka的原因如下：
- en: Kafka consumers run in an infinite loop, waiting for new records to arrive,
    and need to keep track of messages consumed. Being a stateful service, the Ray
    actor provides an ideal paradigm for implementing a Kafka consumer.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka消费者在一个无限循环中运行，等待新记录的到来，并且需要跟踪已消费的消息。作为一个有状态的服务，Ray actor提供了实现Kafka消费者的理想范式。
- en: By putting your Kafka producer in an actor, you can write records to any Kafka
    topic asynchronously without having to create separate producers.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将Kafka生产者放入actor中，您可以异步地将记录写入任何Kafka主题，而无需创建单独的生产者。
- en: A simple implementation of a Kafka producer actor looks like [Example 6-1](#Kafka-producer-actor).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的Kafka生产者actor的实现如[示例 6-1](#Kafka-producer-actor)所示。
- en: Example 6-1\. [Kafka producer actor](https://oreil.ly/5Ycum)
  id: totrans-65
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-1\. [Kafka生产者actor](https://oreil.ly/5Ycum)
- en: '[PRE0]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The actor implementation in this example includes the following methods:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例中的actor实现包括以下方法：
- en: The constructor
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 构造函数
- en: This method initializes the Kafka producer based on the location of the Kafka
    cluster.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法根据Kafka集群的位置初始化Kafka生产者。
- en: '`produce`'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`produce`'
- en: This is the method you will call to send data. It takes data to write to Kafka
    (as a Python dictionary), an optional key (as a string), and the Kafka topic to
    write to. Here we chose to use a dictionary for the data as it is a fairly generic
    way to represent data and can be easily marshaled/unmarshaled to JSON. For debugging,
    we added an internal `delivery_callback` method that prints out when a message
    is written or an error has occurred.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这是您调用以发送数据的方法。它将要写入Kafka的数据（作为Python字典）、可选键（作为字符串）和要写入的Kafka主题。在这里，我们选择使用字典来表示数据，因为这是一种通用的表示数据的方式，并且可以很容易地编组/解组为JSON。为了调试，我们添加了一个内部的`delivery_callback`方法，用于在写入消息或发生错误时打印输出。
- en: '`destroy`'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`destroy`'
- en: Ray calls this method before exiting the application. Our `destroy` method waits
    for up to 30 seconds for any outstanding messages to be delivered and for delivery
    report callbacks to be triggered.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Ray在退出应用程序之前调用此方法。我们的`destroy`方法等待最多30秒，以便交付任何未完成的消息并触发交付报告的回调。
- en: '[Example 6-2](#Kafka-consumer-actor) shows a simple implementation of a Kafka
    consumer actor.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 6-2](#Kafka-consumer-actor)展示了一个Kafka消费者actor的简单实现。'
- en: Example 6-2\. [Kafka consumer actor](https://oreil.ly/5Ycum)
  id: totrans-75
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-2\. [Kafka消费者actor](https://oreil.ly/5Ycum)
- en: '[PRE1]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The consumer actor in this example has the following methods:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例中的消费者actor具有以下方法：
- en: The constructor
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 构造函数
- en: 'Initializes the Kafka consumer. Here we have more parameters compared to a
    producer. In addition to the broker location, you need to specify the following:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化Kafka消费者。与生产者相比，这里有更多的参数。除了经纪人位置外，您还需要指定以下内容：
- en: Topic name
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主题名称
- en: Consumer group name (for parallel runs)
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消费者组名称（用于并行运行）
- en: Restart, which configures how the client behaves when starting with no offset
    or if the current offset does not exist anymore on the server^([5](ch06.html#idm45354775188896))
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新启动，配置客户端在从无偏移量启动或者当前偏移量在服务器上不存在时的行为^([5](ch06.html#idm45354775188896))
- en: Callback, which is a pointer to the customer’s function that is used to process
    a message
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回调函数，指向客户端用于处理消息的函数
- en: '`start`'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`start`'
- en: Runs an infinite loop polling for records. In our example, new records are just
    printed. For debugging, we also print the consumer’s assignment (which partitions
    it is consuming).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 运行一个无限循环，轮询记录。在我们的示例中，新记录只是被打印出来。为了调试，我们还打印了消费者的分区分配情况。
- en: '`stop`'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`stop`'
- en: Updates the class property that stops the infinite loop.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 更新停止无限循环的类属性。
- en: '`destroy`'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`destroy`'
- en: Called by Ray before exiting the application to terminate the consumers.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用程序退出之前由 Ray 调用以终止消费者。
- en: In addition to these two actors, we also need to set up the Kafka topics. While
    Kafka auto-creates new topics as they are used, the default parameters for the
    number of partitions and [replication factor](https://oreil.ly/ew9Oc) may not
    match your needs. We create the topic with our preferred settings in [Example 6-3](#Topics-set-up-function).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这两个参与者外，我们还需要设置 Kafka 主题。虽然 Kafka 在使用时会自动创建新的主题，但默认的分区数和[复制因子](https://oreil.ly/ew9Oc)可能不符合您的需求。我们将使用我们的首选设置在
    [示例 6-3](#Topics-set-up-function) 中创建主题。
- en: Example 6-3\. [Topics setup function](https://oreil.ly/cKafn)
  id: totrans-91
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-3\. [主题设置函数](https://oreil.ly/cKafn)
- en: '[PRE2]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Because the topics may already exist, the code first deletes them. Once the
    deletion is completed, the code waits a short time to make sure that deletion
    took place on the cluster and then re-creates topics with the target number of
    partitions and replication factor.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 因为主题可能已经存在，所以代码首先删除它们。一旦删除完成，代码会等待一段时间，以确保删除在集群上生效，然后使用目标分区数和复制因子重新创建主题。
- en: With these three components in place, you can now create a Ray application to
    publish and read from Kafka. You can run this application either locally or on
    a cluster. The Ray application itself looks like [Example 6-4](#bringing-it-all-together).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这三个组件，您现在可以创建一个 Ray 应用程序来发布和从 Kafka 中读取消息。您可以在本地或集群上运行此应用程序。Ray 应用程序本身看起来像
    [示例 6-4](#bringing-it-all-together)。
- en: Example 6-4\. [Bringing it all together](https://oreil.ly/97nue)
  id: totrans-95
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-4\. [将所有内容整合在一起](https://oreil.ly/97nue)
- en: '[PRE3]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This code does the following:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码执行以下操作：
- en: Defines a simple callback function for the Kafka consumer that just prints the
    message.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义了一个简单的回调函数，用于 Kafka 消费者，只是打印消息。
- en: Initializes Ray.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化 Ray。
- en: Creates required topics.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建所需的主题。
- en: Starts both producer and consumers (the code allows us to specify the number
    of consumers we want to use).
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同时启动生产者和消费者（该代码允许我们指定要使用的消费者数量）。
- en: Calls the `start` method on all created consumers.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对所有创建的消费者调用 `start` 方法。
- en: Once all consumers are created, the producer starts sending Kafka requests every
    second.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦所有消费者都创建完毕，生产者就开始每秒发送一次 Kafka 请求。
- en: Additionally, the code implements graceful termination, ensuring that all resources
    are cleaned up, once the job is interrupted.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，代码还实现了优雅的终止，确保在作业被中断时清理所有资源。
- en: Once the code runs, it produces the output shown in [Example 6-5](#execution-results-for-a-single-consumer).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦代码运行，它将生成 [示例 6-5](#execution-results-for-a-single-consumer) 中显示的输出。
- en: Example 6-5\. Execution results for a single consumer
  id: totrans-106
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-5\. 单一消费者的执行结果
- en: '[PRE4]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As you can see from the results, the execution does the following:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从结果中可以看到的那样，执行以下操作：
- en: Deletes and re-creates the topic `test`.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除并重新创建主题 `test`。
- en: Creates a consumer listening to all the partitions of a topic (we are running
    a single consumer here).
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个消费者，监听一个主题的所有分区（这里我们只运行一个消费者）。
- en: Processes messages. Note here that the producer’s messages are delivered to
    different partitions but are always received and processed by a single consumer.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 处理消息。请注意，生产者的消息会被传递到不同的分区，但始终由单个消费者接收和处理。
- en: Scaling Our Implementation
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展我们的实现
- en: Now that everything is working, let’s see how to scale our implementation. As
    discussed earlier in this chapter, the basic approach to scale an application
    that reads from Kafka is to increase the number of Kafka consumers (assuming that
    the topic has enough partitions). Luckily, the code ([Example 6-4](#bringing-it-all-together))
    already supports this, so we can easily increase the number of consumers by setting
    `n_consumer=5`. Once this update is done, rerunning the code will produce the
    output in [Example 6-6](#execution-results-for-five-consumers).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在一切都正常运行，让我们看看如何扩展我们的实现。正如本章前面讨论的那样，扩展从 Kafka 读取消息的应用程序的基本方法是增加 Kafka 消费者的数量（假设主题有足够的分区）。幸运的是，代码（[示例 6-4](#bringing-it-all-together)）已经支持这一点，所以我们可以通过设置
    `n_consumer=5` 来轻松增加消费者的数量。一旦完成这个更新，重新运行代码将生成 [示例 6-6](#execution-results-for-five-consumers)
    的输出。
- en: Example 6-6\. Execution results for five consumers
  id: totrans-114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 6-6\. 五个消费者的执行结果
- en: '[PRE5]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, unlike [Example 6-5](#execution-results-for-a-single-consumer), each of
    the five Kafka consumers starts listening on 2 partitions (remember, our topic
    uses 10 partitions). You can also see that as messages are delivered on different
    partitions, they are being processed by different consumer instances. So we can
    scale our Kafka applications manually, but what about autoscaling?
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，与[Example 6-5](#execution-results-for-a-single-consumer)不同，五个Kafka消费者中的每一个都开始监听2个分区（请记住，我们的主题使用10个分区）。您还可以看到消息被传递到不同分区时，它们由不同的消费者实例处理。因此，我们可以手动扩展我们的Kafka应用程序，但是自动扩展呢？
- en: Unlike native Kubernetes autoscalers—for example, [KEDA](https://oreil.ly/zJ3yw),
    which scales consumers based on the [queue depth](https://oreil.ly/u7uEE)—Ray
    uses a [different approach](https://oreil.ly/4cgo2). Instead of bringing up and
    down Kafka consumers, Ray uses a fixed number of consumers and spreads them across
    nodes (adding nodes if required). This gives better performance for each consumer
    but still runs into issues when there are not enough partitions.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 与本地Kubernetes自动扩展器不同—例如，[KEDA](https://oreil.ly/zJ3yw)，它基于[queue depth](https://oreil.ly/u7uEE)来扩展消费者—Ray使用了[不同的方法](https://oreil.ly/4cgo2)。Ray固定数量的消费者并将它们分布在节点上（如果需要，添加节点）。这为每个消费者提供了更好的性能，但是当分区不足时仍会遇到问题。
- en: Now that you know how to integrate Ray with Kafka, let’s discuss how to use
    this technique for building streaming applications.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经知道如何将Ray与Kafka集成，让我们讨论如何利用这种技术来构建流应用程序。
- en: Building Stream-Processing Applications with Ray
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Ray构建流处理应用程序
- en: 'There are two important classes of stream processing:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理有两个重要的类别：
- en: Stateless stream processing
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 无状态流处理
- en: Each event is handled completely independently from any previous events or mutable
    shared state. Given an event, the stream processor will treat it exactly the same
    way every time, no matter what data arrived beforehand or the state of the execution.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 每个事件完全独立于任何先前事件或可变共享状态进行处理。给定一个事件，流处理器将无论先前到达的数据是什么或执行状态如何，每次都以完全相同的方式处理它。
- en: Stateful stream processing
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 有状态流处理
- en: A state is shared among events and can influence the way current events are
    processed. The state, in this case, can be a result of previous events or produced
    by an external system, controlling stream processing.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 事件之间共享状态，可以影响当前事件的处理方式。在这种情况下，状态可以是先前事件的结果或由外部系统产生，用于控制流处理。
- en: Stateless stream processing implementations are typically simple and straightforward.
    They require an extension of the `start` method of the Kafka consumer ([Example 6-2](#Kafka-consumer-actor))
    to implement any required transformation of the incoming messages. The result
    of these transformations can be sent either to different Kafka topics or to any
    other part of the code. [“Serverless Kafka Stream Processing with Ray”](https://oreil.ly/S51JF)
    by Javier Redondo describes an example stateless streaming application.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 无状态流处理实现通常简单而直接。它们需要扩展Kafka消费者的`start`方法（见[Example 6-2](#Kafka-consumer-actor)）来实现对传入消息的任何必要转换。这些转换的结果可以发送到不同的Kafka主题或代码的任何其他部分。Javier
    Redondo的[“Serverless Kafka Stream Processing with Ray”](https://oreil.ly/S51JF)描述了一个无状态流处理应用程序的示例。
- en: Implementing stateful stream processing is typically more involved. Let’s take
    a look at options for implementing stateful stream processing based on [dynamically
    controlled streams](https://oreil.ly/AliOW).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 实施有状态流处理通常更加复杂。让我们看看基于[dynamically controlled streams](https://oreil.ly/AliOW)实施有状态流处理的选项。
- en: Our sample implementation uses a heater controller example with the following
    characteristics:^([6](ch06.html#idm45354774560064))
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的样本实现使用了一个具有以下特征的加热器控制器示例：^([6](ch06.html#idm45354774560064))
- en: A message producer provides a constant stream of temperature measurements from
    the sensor.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消息生产者提供传感器的温度测量的恒定流。
- en: The thermostat settings are defined as the desired temperature Td and ∆t.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 温控器的设置被定义为所需的温度Td和∆t。
- en: The thermostat settings can arrive at any point.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 温控器设置可以在任何时候到达。
- en: When the temperature falls below Td – ∆t, an implementation sends a signal to
    the heater to start.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当温度低于Td - ∆t时，实施会向加热器发送信号以启动。
- en: When the temperature goes above Td + ∆t, a signal is sent to the heater to stop.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当温度高于Td + ∆t时，会向加热器发送信号以停止。
- en: A very simple heater model is used here, where temperature increases by 1 degree
    every *N* (configurable) minutes when the heater is on, and decreases by 1 degree
    every *M* (configurable) minutes when it is off.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这里使用了一个非常简单的加热器模型，当加热器开启时，每 *N*（可配置）分钟温度增加 1 度，关闭时每 *M*（可配置）分钟温度降低 1 度。
- en: 'The following are simplifications that we made to the original example:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们对原始示例所做的简化：
- en: Instead of using Protobuf marshaling, we are using JSON marshaling (the same
    as in the previous examples), which allows us to marshal/unmarshal Python dictionary
    messages generically.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不使用 Protobuf 编组，而是使用 JSON 编组（与前面的示例相同），这允许我们通用地编组/解组 Python 字典消息。
- en: To simplify our implementation, instead of using two queues as in the original
    sample, we are using a single queue containing both control and sensor messages,
    discriminating between the two as we receive them. Although it works in our toy
    example, it might not be a good solution in a real-life implementation with a
    large volume of messages, because it can slow down sensor message processing.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了简化我们的实现，我们不像原始示例中那样使用两个队列，而是使用一个包含控制和传感器消息的单个队列，在接收它们时区分两者。尽管在我们的示例中可以工作，但在具有大量消息的实际实现中可能不是一个好的解决方案，因为它可能会减慢传感器消息的处理速度。
- en: 'With these simplifications in place, we will now demonstrate two approaches
    to implement stateful stream processing with Ray: a key-based approach and a key-independent
    one.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些简化，我们现在将演示两种使用 Ray 实现有状态流处理的方法：基于键的方法和基于键独立的方法。
- en: Key-Based Approach
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于键的方法
- en: Many stateful streaming applications rely on Kafka message keys. Remember that
    Kafka partitioning uses a key hash to determine which partition a message is written
    to. This means that Kafka guarantees that all messages with the same key are always
    picked up by the same consumer. In this case, it is possible to implement stateful
    stream processing locally on the Kafka consumer that receives them. Because the
    consumer is implemented as a Ray actor, Ray keeps track of the data inside the
    actor.^([7](ch06.html#idm45354774537376))
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 许多有状态的流式应用程序依赖于 Kafka 消息键。请记住，Kafka 分区使用键散列来确定将消息写入到哪个分区。这意味着 Kafka 保证所有具有相同键的消息始终由同一个消费者接收。在这种情况下，可以在接收它们的
    Kafka 消费者上本地实现有状态的流处理。因为消费者是作为 Ray actor 实现的，Ray 会跟踪 actor 内部的数据。[^7](ch06.html#idm45354774537376)
- en: For this implementation, we created a small heater simulator program that you
    can find in the [accompanying GitHub project](https://oreil.ly/A6iTb) that publishes
    and gets data based on the heater ID.^([8](ch06.html#idm45354774533936)) With
    this in place, you can implement the temperature controller as in [Example 6-7](#implementation-of-temperature-controller).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个实现，我们创建了一个小型的加热器模拟程序，您可以在[附带的 GitHub 项目](https://oreil.ly/A6iTb)中找到，该程序基于加热器
    ID 发布和获取数据。[^8](ch06.html#idm45354774533936)有了这个，你可以像 [示例 6-7](#implementation-of-temperature-controller)
    中那样实现温度控制器。
- en: Example 6-7\. [Implementation of temperature controller](https://oreil.ly/VoVAk)
  id: totrans-141
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-7\. [温度控制器的实现](https://oreil.ly/VoVAk)
- en: '[PRE6]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The implementation is a Python class with the following methods:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 实现是一个 Python 类，具有以下方法：
- en: The constructor, taking a Kafka producer actor ([Example 6-1](#Kafka-producer-actor))
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 构造函数接受一个 Kafka 生产者 actor（[示例 6-1](#Kafka-producer-actor)）。
- en: Used by this class to write control data to Kafka and an ID of this temperature
    controller (which is the same as heater device ID).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 此类使用 `set_temperature` 将控制数据写入 Kafka，以及此温度控制器的 ID（与加热器设备 ID 相同）。
- en: '`process_new_message`'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`process_new_message`'
- en: Receives messages and, depending on their content, calls either `set_temperature`
    or `process_sensordata`.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 接收消息，并根据其内容调用 `set_temperature` 或 `process_sensordata` 中的一个。
- en: '`set_temperature`'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '`set_temperature`'
- en: Processes a new set temperature method from the thermostat. This message contains
    the new desired temperature along with additional heater-specific parameters (temperature
    intervals where controls are ignored).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 处理来自恒温器的新设置温度方法。此消息包含新的期望温度以及额外的加热器特定参数（在其中控制被忽略的温度间隔）。
- en: '`process_sensordata`'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`process_sensordata`'
- en: Handles the temperature control. If the desired temperature is set, this method
    compares the current temperature with the desired one and calculates the desired
    control (heater on/off). To avoid resending the same control over and over again,
    this method additionally compares the calculated control value with the current
    (cached) and submits a new control value only if it has changed.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 处理温度控制。如果设置了期望温度，该方法将当前温度与期望温度进行比较，并计算所需的控制（加热器开/关）。为了避免重复发送相同的控制值，该方法还将计算的控制值与当前（缓存的）进行比较，并仅在更改时提交新的控制值。
- en: Because Kafka calculates partitions based on the key hash, the same partition
    can serve many keys. To manage multiple keys per partition, we introduced a `TemperatureControllerManager`
    class whose purpose is to manage individual temperature controllers ([Example 6-8](#implementation-of-temperature-controller-manager)).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Kafka 根据键的哈希计算分区，同一个分区可以服务于多个键。为了管理每个分区的多个键，我们引入了一个 `TemperatureControllerManager`
    类，其目的是管理各个温度控制器（[Example 6-8](#implementation-of-temperature-controller-manager)）。
- en: Example 6-8\. [Implementation of temperature controller manager](https://oreil.ly/sQbyW)
  id: totrans-153
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 6-8\. [温度控制器管理实现](https://oreil.ly/sQbyW)
- en: '[PRE7]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This implementation is based on a dictionary keeping track of temperature controllers
    based on their IDs. The class provides two methods:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 此实现基于一个字典来跟踪基于其 ID 的温度控制器。该类提供了两种方法：
- en: The constructor, taking a Kafka producer actor ([Example 6-1](#Kafka-producer-actor))
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 构造函数接受一个 Kafka 生产者演员（[Example 6-1](#Kafka-producer-actor)）
- en: Creates a new empty dictionary of the individual temperature controllers.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个新的空字典来管理各个温度控制器。
- en: The `process_controller_message` function
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`process_controller_message` 函数'
- en: Takes every new message received by the *local* Kafka consumer and, based on
    a key, decides whether a required temperature controller exists. If not, a new
    temperature controller is created and stores a reference to it. After it finds
    or creates the controller, it then passes the message to it for processing.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个*本地* Kafka 消费者接收到的新消息，根据一个键来决定是否存在所需的温度控制器。如果不存在，将创建一个新的温度控制器并存储对其的引用。找到或创建控制器后，然后将消息传递给它进行处理。
- en: To link this implementation to the Kafka consumer, we do need to modify the
    Kafka consumer ([Example 6-2](#Kafka-consumer-actor)) a little bit ([Example 6-9](#integrating-kafka-consumer-with-temperature-controller-manager)).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 要将此实现链接到 Kafka 消费者，我们需要稍微修改 Kafka 消费者（[Example 6-2](#Kafka-consumer-actor)）和温度控制器管理器集成（[Example 6-9](#integrating-kafka-consumer-with-temperature-controller-manager)）。
- en: Example 6-9\. [Integrating the Kafka consumer with the temperature controller
    manager](https://oreil.ly/5Ycum)
  id: totrans-161
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 6-9\. [将 Kafka 消费者与温度控制器管理器集成](https://oreil.ly/5Ycum)
- en: '[PRE8]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'A couple of notable differences exist between this and the original implementations:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这种实现与原始实现之间存在一些显著差异：
- en: The constructor takes an additional parameter—the Kafka producer—which is used
    internally to create a *temperature controller manager* as part of the actor’s
    initialization.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构造函数接受一个额外的参数——Kafka 生产者——该参数在演员初始化时用于创建*温度控制器管理器*。
- en: For every incoming message, in addition to printing it out, we are invoking
    the temperature *controller manager* to process it.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个传入的消息，除了打印它之外，我们还调用温度*控制器管理器*来处理它。
- en: With these changes in place, you can implement the [main program](https://oreil.ly/e9Lm0),
    similar to ([Example 6-4](#bringing-it-all-together)), and start an execution.
    The partial execution result (in [Example 6-10](#controller-execution-results))
    shows the output of processing.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些变更，您可以实现类似于([Example 6-4](#bringing-it-all-together))的[主程序](https://oreil.ly/e9Lm0)并启动执行。部分执行结果（在[Example 6-10](#controller-execution-results)）显示了处理的输出。
- en: Example 6-10\. Controller execution results
  id: totrans-167
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 6-10\. 控制器执行结果
- en: '[PRE9]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This listing shows the behavior of the controller when the temperature is around
    the desired value (45 degrees). As expected, the temperature keeps growing until
    it gets above 46 degrees (to avoid constant switching on and off of the controller,
    no actions are performed when the difference between desired and actual temperature
    is less than 1 degree). When the measurement is 46.2, the new message is sent
    to the heater to switch off and the temperature starts to decrease. Also looking
    at this listing, we can see that the requests are always delivered to the same
    partition (they have the same key).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 此列表显示了在温度接近期望值（45度）时控制器的行为。正如预期的那样，温度会持续增加，直到超过46度（为了避免控制器的频繁开关，当期望和实际温度之间的差异小于1度时，不执行任何操作）。当测量值为46.2时，新消息将发送到加热器以关闭，温度开始下降。同时查看此列表，我们可以看到请求始终传递到同一分区（它们具有相同的键）。
- en: A key-based approach is a good option for many real-world implementations. The
    advantage of this approach is that all of the data processing is done locally,
    inside the same Kafka consumer actor.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多实际的实现来说，基于键的方法是一个很好的选择。这种方法的优势在于所有的数据处理都在同一个Kafka消费者Actor内部完成。
- en: 'Two potential pitfalls exist with such implementations:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的实现有两个潜在的缺陷：
- en: As the number of keys grows, it is necessary to ensure that the keys are evenly
    distributed across Kafka topic partitions. Ensuring this key distribution can
    sometimes require additional key design procedures, but the default hashing is
    often sufficient.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着键的数量增加，确保这些键均匀分布在Kafka主题分区中是必要的。确保这种键分布有时可能需要额外的键设计过程，但默认的哈希通常已经足够。
- en: Execution locality can become a problem when executions are CPU and memory expensive.
    Because all the executions are part of the Kafka consumer actor, its scaling can
    become insufficient for keeping up with high-volume traffic.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当执行是CPU和内存密集型时，执行局部性可能会成为问题。因为所有的执行都是Kafka消费者Actor的一部分，它的扩展性可能不足以应对高流量。
- en: Some of these drawbacks can be rectified in a key-independent approach.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 一些这些缺点可以在独立于键的方法中得到纠正。
- en: Key-Independent Approach
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 独立于键的方法
- en: The difference in this approach compared to the previous one is that both the
    temperature controller ([Example 6-8](#implementation-of-temperature-controller-manager))
    and temperature controller manager ([Example 6-9](#integrating-kafka-consumer-with-temperature-controller-manager))
    are converted from Python objects to [Ray actors](https://oreil.ly/b7hSK). By
    doing this, both become individually addressable and can be located anywhere.
    Such an approach loses execution locality (which can lead to a slight execution
    time increase), but can improve overall scalability of the solution (each actor
    can run on a separate node). If necessary, you can improve scalability even further
    by leveraging an actor’s pool (described in [Chapter 4](ch04.html#ch04)) and thus
    allowing Ray to split execution to even more nodes.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法与之前的方法的不同之处在于，温度控制器（[示例 6-8](#implementation-of-temperature-controller-manager)）和温度控制器管理器（[示例 6-9](#integrating-kafka-consumer-with-temperature-controller-manager)）都被转换为[Ray
    actors](https://oreil.ly/b7hSK)。通过这样做，它们都变成了可单独寻址的，可以放置在任何地方。这种方法会失去执行局部性（可能会导致轻微的执行时间增加），但可以提高解决方案的整体可扩展性（每个Actor可以在单独的节点上运行）。如果需要，您可以通过利用Actor池（在[第四章](ch04.html#ch04)中描述）进一步提高可扩展性，从而允许Ray将执行分割到更多节点上。
- en: Going Beyond Kafka
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超越Kafka
- en: In this chapter, you learned how to use Ray’s native capabilities to implement
    streaming by directly integrating Ray with Kafka. But what if you need to use
    a different messaging infrastracture? If your favorite communication backbone
    provides Python APIs, you can integrate it with Ray, similar to the Kafka integration
    described previously.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您学习了如何使用Ray的本地能力通过直接集成Ray与Kafka来实现流式处理。但是如果您需要使用不同的消息基础设施怎么办？如果您喜欢的通信支撑提供了Python
    API，您可以像之前描述的Kafka集成一样将其与Ray集成。
- en: Another option, as mentioned at the beginning of this chapter, is to use an
    external library—for example, project [Rayvens](https://oreil.ly/xv2xB), which
    internally leverages [Apache Camel](https://oreil.ly/HT77R) (a generic integration
    framework) to make it possible to use a wide range of messaging backbones. You
    can find a description of the supported messaging backbones and an example of
    their usage in [“Accessing Hundreds of Event Sources and Sinks with Rayvens”](https://oreil.ly/y4kYx)
    by Gheorghe-Teodor Bercea and Olivier Tardieu.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择，正如本章开头提到的，是使用外部库——例如项目 [Rayvens](https://oreil.ly/xv2xB)，它在内部利用 [Apache
    Camel](https://oreil.ly/HT77R)（一个通用集成框架），可以使用各种消息背景。您可以在 Gheorghe-Teodor Bercea
    和 Olivier Tardieu 的文章 [“使用 Rayvens 访问数百个事件源和接收器”](https://oreil.ly/y4kYx) 中找到对支持的消息背景的描述以及它们的使用示例。
- en: Similar to the Kafka integration we’ve described, under the hood, Rayvens is
    implemented as a set of Ray actors. The Rayvens base class `Stream` is a stateless,
    serializable, wrapper around the `Stream` Ray actor class, which is responsible
    for keeping track of the current Rayvens state (see [Chapter 4](ch04.html#ch04)
    for using actors to manage global variables), including currently defined sources
    and sinks and their connectivity. The `Stream` class hides the remote nature of
    a `Stream` actor and implements wrappers that internally implement all communications
    with the underlying remote actor. If you want more control (in terms of execution
    timing), you can invoke methods directly on the `Stream` actor. The `Stream` actor
    will be reclaimed when the original stream handle goes out of scope.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于我们描述的 Kafka 集成，在底层，Rayvens 被实现为一组 Ray actors。Rayvens 的基类 `Stream` 是一个无状态、可序列化的包装器，封装了
    `Stream` Ray actor 类，负责跟踪当前的 Rayvens 状态（详见[第四章](ch04.html#ch04)关于使用 actors 管理全局变量的内容），包括当前定义的源和接收器及其连接性。`Stream`
    类隐藏了 `Stream` actor 的远程特性，并实现了内部实现所有与底层远程 actor 的通信的包装器。如果您希望在执行时控制更多（例如执行时机），可以直接调用
    `Stream` actor 上的方法。当原始流句柄超出范围时，将回收 `Stream` actor。
- en: 'As Rayvens is based on Camel, it requires a setting of Camel to make it work.
    Ravens supports two main options of Camel usage:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Rayvens 基于 Camel，它需要设置 Camel 以使其工作。Rayvens 支持两种主要的 Camel 使用选项：
- en: Local mode
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 本地模式
- en: 'The Camel source or sink runs in the same execution context as the `Stream`
    actor that is attached to using the Camel client: same container, same virtual
    or physical machine.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: Camel 源或接收器与使用 Camel 客户端连接的 `Stream` actor 在同一个执行上下文中运行：同一容器，同一虚拟或物理机器。
- en: Operator mode
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 运算符模式
- en: The Camel source or sink runs inside a Kubernetes cluster relying on the Camel
    operator to manage dedicated Camel pods.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: Camel 源或接收器在 Kubernetes 集群内运行，依赖 Camel 运算符来管理专用的 Camel Pod。
- en: Conclusion
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this chapter, you learned one option to use Ray for implementing streaming.
    You first learned the basics of Kafka—the most popular streaming application backbone
    used today—and ways to integrate it with Ray. You then learned how to scale Kafka-based
    applications with Ray. We have also outlined implementation approaches for both
    stateless and stateful streaming applications with Ray that you can use as a foundation
    for your custom implementations.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您了解了使用 Ray 实现流处理的一种选项。您首先了解了 Kafka 的基础知识——今天最流行的流处理应用背骨，并学习了如何将其与 Ray 集成。然后，您学习了如何使用
    Ray 扩展基于 Kafka 的应用程序。我们还概述了使用 Ray 实现无状态和有状态流应用程序的实现方法，这些方法可以作为自定义实现的基础。
- en: Finally, we briefly discussed alternatives to using Kafka as a transport. Rayvens,
    a general-purpose integration framework based on Apache Camel, can be used for
    integration of a wide variety of streaming backbones. You can use this discussion
    to decide how to implement your specific transports.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们简要讨论了使用 Kafka 作为传输方式的替代方案。Rayvens 是一个基于 Apache Camel 的通用集成框架，可用于集成各种流式背景。您可以利用这些讨论来决定如何实现您的特定传输。
- en: In the next chapter, we will introduce Ray’s microservices framework and how
    to use it for model serving.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍 Ray 的微服务框架及其用于模型服务的使用方法。
- en: ^([1](ch06.html#idm45354776016592-marker)) Other examples of distributed log
    implementation are [Apache BookKeeper](https://oreil.ly/4Km4h), [Apache Pulsar](https://oreil.ly/ChJdY),
    and [Pravega](https://oreil.ly/getrt).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch06.html#idm45354776016592-marker)) 分布式日志实现的其他示例包括 [Apache BookKeeper](https://oreil.ly/4Km4h)、[Apache
    Pulsar](https://oreil.ly/ChJdY) 和 [Pravega](https://oreil.ly/getrt)。
- en: ^([2](ch06.html#idm45354775801840-marker)) Although we tend to think about infinite
    logs, in reality a Kafka log is limited to the amount of disk space available
    to the corresponding Kafka server. Kafka introduces [log retention and cleanup
    policies](https://oreil.ly/0wudH), which prevent logs from growing indefinitely
    and consequently crashing Kafka servers. As a result, when we are talking about
    log replay in a production system, we are talking about replay within a retention
    window.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch06.html#idm45354775801840-marker)) 尽管我们倾向于思考无限日志，但实际上，Kafka日志受限于相应Kafka服务器的磁盘空间。Kafka引入了[日志保留和清理策略](https://oreil.ly/0wudH)，防止日志无限增长，从而导致Kafka服务器崩溃。因此，当我们在生产系统中谈论日志重放时，我们是在谈论在保留窗口内的重放。
- en: ^([3](ch06.html#idm45354775789952-marker)) Refer to [“Capacity Planning Your
    Kafka Cluster”](https://oreil.ly/rC2RY) by Jason Bell for more details. Kafka
    is also available as a serverless product from vendors such as Confluent Cloud.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch06.html#idm45354775789952-marker)) 更多详细信息，请参阅[Jason Bell的“Kafka集群的容量规划”](https://oreil.ly/rC2RY)。Kafka也作为Confluent
    Cloud等供应商提供的无服务器产品。
- en: ^([4](ch06.html#idm45354775738704-marker)) For another example of the same approach,
    see [“Serverless Kafka Stream Processing with Ray”](https://oreil.ly/iRxWq) by
    Javier Redondo.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch06.html#idm45354775738704-marker)) 另一个采用同样方法的例子，请参阅[Javier Redondo的“使用Ray进行无服务器Kafka流处理”](https://oreil.ly/iRxWq)。
- en: ^([5](ch06.html#idm45354775188896-marker)) Allowed values for `reset` are `earliest`,
    which automatically resets the offset to the beginning of the log, and `latest`,
    which automatically resets the offset to the latest offset processed by the consumer
    group.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch06.html#idm45354775188896-marker)) `reset`的允许值有`earliest`，它会自动将偏移重置到日志的开头，以及`latest`，它会自动将偏移重置到消费者组最近处理的偏移量。
- en: ^([6](ch06.html#idm45354774560064-marker)) This example is described further
    in [“How to Serve Machine Learning Models with Dynamically Controlled Streams”](https://oreil.ly/jekKs),
    a blog post by Boris.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch06.html#idm45354774560064-marker)) 这个例子在[Boris的“如何使用动态控制流为机器学习模型提供服务”](https://oreil.ly/jekKs)博客文章中进一步描述。
- en: ^([7](ch06.html#idm45354774537376-marker)) As described in [Chapter 4](ch04.html#ch04),
    Ray’s actors are not persistent. Therefore, in the case of node failures, the
    actor state will be lost. We can implement persistence here as described in [Chapter 4](ch04.html#ch04)
    to overcome this.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch06.html#idm45354774537376-marker)) 如第4章所述，Ray的actors不是持久化的。因此，在节点故障的情况下，actor状态将丢失。我们可以实现持久性，如第4章中所述，以克服这一问题。
- en: ^([8](ch06.html#idm45354774533936-marker)) Note the use of threading to ensure
    that the Kafka consumer is running forever without interference with measurement
    computations. Again, this is a simplification we made for our toy example; in
    real implementations, every request to the temperature controller should contain
    a `replyTo` topic, thus ensuring that any replies will get to the correct instance
    of the heater.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch06.html#idm45354774533936-marker)) 注意使用线程来确保Kafka消费者无限运行，而不干扰测量计算。再次强调，这是我们为玩具示例而做的简化；在真实实现中，每个对温度控制器的请求应包含一个`replyTo`主题，以确保任何回复都能到达正确的加热器实例。
