["```py\n# Get data\ndf = pd.read_csv(\"winequality-red.csv\", delimiter=\";\")\nprint(f\"Rows, columns: {str(df.shape)}\")\nprint(df.head)\nprint(df.isna().sum())\n\n# Create Classification version of target variable\ndf['goodquality'] = [1 if x >= 6 else 0 for x in df['quality']]\nX = df.drop(['quality','goodquality'], axis = 1)\ny = df['goodquality']\nprint(df['goodquality'].value_counts())\n\n# Normalize feature variables\nX_features = X\nX = StandardScaler().fit_transform(X)\n# Splitting the data\nX_train, X_test, y_train, y_test = \\\n    train_test_split(X, y, test_size=.25, random_state=0)\n\nparam_model = {'max_depth':range(10, 20),\n                'max_features': range(3,11)}\n\nstart = time.time()\nmodel = GridSearchCV(DecisionTreeClassifier(random_state=1),\n                     param_grid=param_model,\n                     scoring='accuracy',\n                     n_jobs=-1)\n\nmodel = model.fit(X_train, y_train)\nprint(f\"executed in {time.time() - start}, \"\n      f\"nodes {model.best_estimator_.tree_.node_count}, \"\n      f\"max_depth {model.best_estimator_.tree_.max_depth}\")\n\ny_pred = model.predict(X_test)\nprint(classification_report(y_test, y_pred))\n```", "```py\n# Get data\ndf = pd.read_csv(\"winequality-red.csv\", delimiter=\";\")\nprint(f\"Rows, columns: {str(df.shape)}\")\nprint(df.head)\nprint(df.isna().sum())\n\n# Create Classification version of target variable\ndf['goodquality'] = [1 if x >= 6 else 0 for x in df['quality']]\nX = df.drop(['quality','goodquality'], axis = 1)\ny = df['goodquality']\nprint(df['goodquality'].value_counts())\n\n# Normalize feature variables\nX_features = X\nX = StandardScaler().fit_transform(X)\n# Splitting the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, \n                                                    random_state=0)\n\nparam_model = {'max_depth':range(10, 20),\n               'max_features': range(3,11)}\n\nstart = time.time()\n\nmode = GridSearchCV(DecisionTreeClassifier(random_state=1),\n                     param_grid=param_model,\n                     scoring='accuracy',\n                     n_jobs=-1)\n\nregister_ray()\nwith joblib.parallel_backend('ray'):\n    model = mode.fit(X_train, y_train)\n\nmodel = model.fit(X_train, y_train)\nprint(f\"executed in {time.time() - start}, \"\n      f\"nodes {model.best_estimator_.tree_.node_count}, \"\n      f\"max_depth {model.best_estimator_.tree_.max_depth}\")\n\ny_pred = model.predict(X_test)\nprint(classification_report(y_test, y_pred))\n```", "```py\n# Get data\ndf = pd.read_csv(\"winequality-red.csv\", delimiter=\";\")\nprint(f\"Rows, columns: {str(df.shape)}\")\nprint(df.head)\nprint(df.isna().sum())\n\n# Create Classification version of target variable\ndf['goodquality'] = [1 if x >= 6 else 0 for x in df['quality']]\nX = df.drop(['quality','goodquality'], axis = 1)\ny = df['goodquality']\nprint(df['goodquality'].value_counts())\n\n# Normalize feature variables\nX_features = X\nX = StandardScaler().fit_transform(X)\n# Splitting the data\nX_train, X_test, y_train, y_test = \\\n    train_test_split(X, y, test_size=.25, random_state=0)\n\nstart = time.time()\nmodel = xgb.XGBClassifier(random_state=1)\nmodel.fit(X_train, y_train)\nprint(f\"executed XGBoost in {time.time() - start}\")\ny_pred = model.predict(X_test)\nprint(classification_report(y_test, y_pred))\n```", "```py\nstart = time.time()\nmodel = RayXGBClassifier(\n    n_jobs=10,  # In XGBoost-Ray, n_jobs sets the number of actors\n    random_state=1\n)\n\nmodel.fit(X=X_train, y=y_train, ray_params=RayParams(num_actors=3))\nprint(f\"executed XGBoost in {time.time() - start}\")\n```", "```py\n# Get data\ndf = pd.read_csv(\"winequality-red.csv\", delimiter=\";\")\nprint(f\"Rows, columns: {str(df.shape)}\")\nprint(df.head)\nprint(df.isna().sum())\n\n# Create Classification version of target variable\ndf['goodquality'] = [1 if x >= 6 else 0 for x in df['quality']]\nX = df.drop(['quality','goodquality'], axis = 1)\ny = df['goodquality']\nprint(df['goodquality'].value_counts())\n\n# Normalize feature variables\nX_features = X\nX = StandardScaler().fit_transform(X)\n# Splitting the data\nX_train, X_test, y_train, y_test = \\\n    train_test_split(X, y, test_size=.25, random_state=0)\n\ntrain_data = lgb.Dataset(X_train,label=y_train)\nparam = {'num_leaves':150, 'objective':'binary','learning_rate':.05,'max_bin':200}\nparam['metric'] = ['auc', 'binary_logloss']\n\nstart = time.time()\nmodel = lgb.train(param,train_data,100)\nprint(f\"executed LightGBM in {time.time() - start}\")\ny_pred = model.predict(X_test)\n\n# Converting probabilities into 0 or 1\n\nfor i in range(len(y_pred)):\n    if y_pred[i] >= .5:       # Setting threshold to .5\n        y_pred[i] = 1\n    else:\n        y_pred[i] = 0\nprint(classification_report(y_test, y_pred))\n```", "```py\nmodel = RayLGBMClassifier(\n    random_state=42)\n\nstart = time.time()\nmodel.fit(X=X_train, y=y_train, ray_params=RayParams(num_actors=3))\nprint(f\"executed LightGBM in {time.time() - start}\")\n```", "```py\n# dataset\nclass WineQualityDataset(Dataset):\n    # load the dataset\n    def __init__(self, path):\n        # load the csv file as a dataframe\n        df = pd.read_csv(path, delimiter=\";\")\n        print(f\"Rows, columns: {str(df.shape)}\")\n        print(df.head)\n        # create Classification version of target variable\n        df['goodquality'] = [1 if x >= 6 else 0 for x in df['quality']]\n        df = df.drop(['quality'], axis = 1)\n        print(df['goodquality'].value_counts())\n        # store the inputs and outputs\n        self.X = StandardScaler().fit_transform(df.values[:, :-1])\n        self.y = df.values[:, -1]\n        # ensure input data is floats\n        self.X = self.X.astype('float32')\n        self.y = self.y.astype('float32')\n        self.y = self.y.reshape((len(self.y), 1))\n\n    # number of rows in the dataset\n    def __len__(self):\n        return len(self.X)\n\n    # get a row at an index\n    def __getitem__(self, idx):\n        return [self.X[idx], self.y[idx]]\n\n    # get indexes for train and test rows\n    def get_splits(self, n_test=0.33):\n        # determine sizes\n        test_size = round(n_test * len(self.X))\n        train_size = len(self.X) - test_size\n        # calculate the split\n        return random_split(self, [train_size, test_size])\n```", "```py\n# model definition\nclass WineQualityModel(Module):\n    # define model elements\n    def __init__(self, n_inputs):\n        super(WineQualityModel, self).__init__()\n        # input to first hidden layer\n        self.hidden1 = Linear(n_inputs, 10)\n        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n        self.act1 = ReLU()\n        # second hidden layer\n        self.hidden2 = Linear(10, 8)\n        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n        self.act2 = ReLU()\n        # third hidden layer and output\n        self.hidden3 = Linear(8, 1)\n        xavier_uniform_(self.hidden3.weight)\n        self.act3 = Sigmoid()\n\n    # forward-propagate input\n    def forward(self, X):\n        # input to first hidden layer\n        X = self.hidden1(X)\n        X = self.act1(X)\n        # second hidden layer\n        X = self.hidden2(X)\n        X = self.act2(X)\n        # third hidden layer and output\n        X = self.hidden3(X)\n        X = self.act3(X)\n        return X\n```", "```py\n# ensure reproducibility\ntorch.manual_seed(42)\n# load the dataset\ndataset = WineQualityDataset(\"winequality-red.csv\")\n\n# calculate split\ntrain, test = dataset.get_splits()\n# prepare data loaders\ntrain_dl = DataLoader(train, batch_size=32, shuffle=True)\ntest_dl = DataLoader(test, batch_size=32, shuffle=False)\n\n# train the model\nmodel = WineQualityModel(11)\n# define the optimization\ncriterion = BCELoss()\noptimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\nstart = time.time()\n# enumerate epochs\nfor epoch in range(500):\n    # enumerate mini batches\n    for i, (inputs, targets) in enumerate(train_dl):\n        # clear the gradients\n        optimizer.zero_grad()\n        # compute the model output\n        yhat = model(inputs)\n        # calculate loss\n        loss = criterion(yhat, targets)\n        # credit assignment\n        loss.backward()\n        # update model weights\n        optimizer.step()\nprint(f\"Build model in {time.time() - start}\")\nprint(model)\n# evaluate a model\npredictions, actuals = list(), list()\nfor i, (inputs, targets) in enumerate(test_dl):\n    # evaluate the model on the test set\n    yhat = model(inputs)\n    # retrieve numpy array\n    yhat = yhat.detach().numpy()\n    actual = targets.numpy()\n    actual = actual.reshape((len(actual), 1))\n    # round to class values\n    yhat = yhat.round()\n    # store\n    predictions.append(yhat)\n    actuals.append(actual)\npredictions, actuals = vstack(predictions), vstack(actuals)\n# calculate accuracy\nacc = accuracy_score(actuals, predictions)\nprint(\"Model accuracy\", acc)\n```", "```py\n    # training step\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = self.bce(y_hat, y)\n        return loss\n    # optimizer\n    def configure_optimizers(self):\n        return Adam(self.parameters(), lr=0.02)\n```", "```py\n# train\ntrainer = Trainer(max_steps=1000)\ntrainer.fit(model, train_dl)\n```", "```py\n# train\nplugin = RayPlugin(num_workers=6)\ntrainer = Trainer(max_steps=1000, plugins=[plugin])\ntrainer.fit(model, train_dl)\nprint(f\"Build model in {time.time() - start}\")\nprint(model)\n```", "```py\nray.init()\nconfig = {\n    # Environment (RLlib understands OpenAI Gym registered strings).\n    'env': 'CartPole-v0',\n    # Use 4 environment workers (aka \"rollout workers\") that parallelly\n    # collect samples from their own environment clone(s).\n    \"num_workers\": 4,\n    'framework': 'tf2',\n    'eager_tracing': True,\n    # This is just our model arch, choosing the right one is beyond the scope\n    # of this book.\n    'model': {\n        'fcnet_hiddens': [64, 64],\n        'fcnet_activation': 'relu',\n    },\n    # Set up a separate evaluation worker set for the\n    # `trainer.evaluate()` call after training.\n    'evaluation_num_workers': 1,\n    # Only for evaluation runs, render the env.\n    'evaluation_config': {\n        \"render_env\": True,\n    },\n    'gamma': 0.9,\n    'lr': 1e-2,\n    'train_batch_size': 256,\n}\n\n# Create RLlib Trainer.\ntrainer = agents.ppo.PPOTrainer(config=config)\n\n# Run it for n training iterations. A training iteration includes\n# parallel sample collection by the environment workers as well as\n# loss calculation on the collected batch and a model update.\nfor i in range(5):\n    print(f\"Iteration {i}, training results {trainer.train()}\")\n\n# Evaluate the trained Trainer (and render each timestep to the shell's\n# output).\ntrainer.evaluate()\n```", "```py\n# train function\ndef model_train(model, optimizer, criterion, train_loader):\n    # for every mini batch\n    for i, (inputs, targets) in enumerate(train_loader):\n        # clear the gradients\n        optimizer.zero_grad()\n        # compute the model output\n        yhat = model(inputs)\n        # calculate loss\n        loss = criterion(yhat, targets)\n        # credit assignment\n        loss.backward()\n        # update model weights\n        optimizer.step()\n\n# test model\ndef model_test(model, test_loader):\n    predictions, actuals = list(), list()\n    for i, (inputs, targets) in enumerate(test_loader):\n        # evaluate the model on the test set\n        yhat = model(inputs)\n        # retrieve numpy array\n        yhat = yhat.detach().numpy()\n        actual = targets.numpy()\n        actual = actual.reshape((len(actual), 1))\n        # round to class values\n        yhat = yhat.round()\n        # store\n        predictions.append(yhat)\n        actuals.append(actual)\n    predictions, actuals = vstack(predictions), vstack(actuals)\n    # calculate accuracy\n    return accuracy_score(actuals, predictions)\n\n# train wine quality model\ndef train_winequality(config):\n\n    # calculate split\n    train, test = dataset.get_splits()\n    train_dl = DataLoader(train, batch_size=32, shuffle=True)\n    test_dl = DataLoader(test, batch_size=32, shuffle=False)\n\n    # model\n    model = WineQualityModel(11)\n    # define the optimization\n    criterion = BCELoss()\n    optimizer = SGD(\n        model.parameters(), lr=config[\"lr\"], momentum=config[\"momentum\"])\n    for i in range(50):\n        model_train(model, optimizer, criterion, train_dl)\n        acc = model_test(model, test_dl)\n\n        # send the current training result back to Tune\n        tune.report(mean_accuracy=acc)\n\n        if i % 5 == 0:\n            # this saves the model to the trial directory\n            torch.save(model.state_dict(), \"./model.pth\")\n```", "```py\n# load the dataset\ndataset = WineQualityDataset(\"winequality-red.csv\")\n\nsearch_space = {\n    \"lr\": tune.sample_from(lambda spec: 10**(-10 * np.random.rand())),\n    \"momentum\": tune.uniform(0.1, 0.9)\n}\n\nanalysis = tune.run(\n    train_winequality,\n    num_samples=100,\n    scheduler=ASHAScheduler(metric=\"mean_accuracy\", mode=\"max\"),\n    config=search_space\n)\n```", "```py\n+-------------------------------+------------+-----------------+------------- ...\n\n| Trial name | status | loc | lr | momentum | acc | iter | total time (s) |\n\n|-------------------------------+------------+-----------------+------------- ...\n\n| ...00000 | TERMINATED | ... | 2.84411e-07 | 0.170684 | 0.513258 | 50 | 4.6005 |\n\n| ...00001 | TERMINATED | ... | 4.39914e-10 | 0.562412 | 0.530303 | 1 | 0.0829589 |\n\n| ...00002 | TERMINATED | ... | 5.72621e-06 | 0.734167 | 0.587121 | 16 | 1.2244 |\n\n| ...00003 | TERMINATED | ... | 0.104523 | 0.316632 | 0.729167 | 50 | 3.83347 |\n\n……………………………..\n\n| ...00037 | TERMINATED | ... | 5.87006e-09 | 0.566372 | 0.625 | 4 | 2.41358 |\n\n|| ...00043 | TERMINATED | ... | 0.000225694 | 0.567915 | 0.50947 | 1 | 0.130516 |\n\n| ...00044 | TERMINATED | ... | 2.01545e-07 | 0.525888 | 0.405303 | 1 | 0.208055 |\n\n| ...00045 | TERMINATED | ... | 1.84873e-07 | 0.150054 | 0.583333 | 4 | 2.47224 |\n\n| ...00046 | TERMINATED | ... | 0.136969 | 0.567186 | 0.742424 | 50 | 4.52821 |\n\n| ...00047 | TERMINATED | ... | 1.29718e-07 | 0.659875 | 0.443182 | 1 | 0.0634422 |\n\n| ...00048 | TERMINATED | ... | 0.00295002 | 0.349696 | 0.564394 | 1 | 0.107348 |\n\n| ...00049 | TERMINATED | ... | 0.363802 | 0.290659 | 0.725379 | 4 | 0.227807 |\n\n+-------------------------------+------------+-----------------+------------- ...\n```"]