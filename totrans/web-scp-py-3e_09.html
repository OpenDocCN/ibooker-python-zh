<html><head></head><body><section data-pdf-bookmark="Chapter 8. Scrapy" data-type="chapter" epub:type="chapter"><div class="chapter" id="c-8">&#13;
<h1><span class="label">Chapter 8. </span>Scrapy</h1>&#13;
&#13;
<p><a data-type="xref" href="ch07.html#c-7">Chapter 7</a> presented some techniques and patterns for building large, scalable, and (most important!) maintainable web crawlers. Although this is easy enough to do by hand, many libraries, frameworks, and even GUI-based tools will do this for you or at least try to make your life a little easier.</p>&#13;
&#13;
<p>Since its release in 2008, Scrapy has quickly grown into the largest and best-maintained web scraping framework in Python. It is currently maintained by Zyte (formerly Scrapinghub).</p>&#13;
&#13;
<p>One of the challenges of writing web crawlers is that you’re often performing the same tasks again and again: find all links on a page, evaluate the difference between internal and external links, and go to new pages. These basic patterns are useful to know and to be able to write from scratch, but the Scrapy library handles many of these details for you.</p>&#13;
&#13;
<p>Of course, Scrapy isn’t a mind reader. You still need to define page templates, give it locations to start scraping from, and define URL patterns for the pages that you’re looking for. But in these cases, it provides a clean framework to keep your code organized.</p>&#13;
&#13;
<section data-pdf-bookmark="Installing Scrapy" data-type="sect1"><div class="sect1" id="id47">&#13;
<h1>Installing Scrapy</h1>&#13;
&#13;
<p>Scrapy offers the tool for <a href="http://scrapy.org/download/">download</a> from its website, as well as <a contenteditable="false" data-primary="Scrapy" data-secondary="installing" data-type="indexterm" id="scystl"/>instructions for installing Scrapy with third-party installation managers such as pip.</p>&#13;
&#13;
<p class="pagebreak-before">Because of its relatively large size and complexity, Scrapy is not usually a framework that can be installed in the traditional way with:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">&#13;
$<code class="w"> </code>pip<code class="w"> </code>install<code class="w"> </code>Scrapy<code class="w"/>&#13;
</pre>&#13;
&#13;
<p>Note that I say “usually” because, though it is theoretically possible, I usually run into one or more tricky dependency issues, version mismatches, and unsolvable bugs.</p>&#13;
&#13;
<p>If you’re determined to install Scrapy from pip, using a virtual environment is highly recommended (see <a data-type="xref" href="ch04.html#KLSwVE_01">“Keeping Libraries Straight with Virtual Environments”</a> for more on virtual environments).</p>&#13;
&#13;
<p>The installation method I prefer is through the <a href="https://docs.continuum.io/anaconda/">Anaconda package manager</a>. Anaconda is a product from the company Continuum, designed to reduce friction when it comes to finding and installing popular Python <a contenteditable="false" data-primary="Anaconda" data-type="indexterm" id="id508"/>data science packages. Many of the packages it manages, such as NumPy and NLTK, will be used in later chapters as well.</p>&#13;
&#13;
<p>After you install Anaconda, you can install Scrapy by using this command:</p>&#13;
&#13;
<pre data-code-language="bash">&#13;
<span class="n"><code>conda</code></span><code class="w"> </code><span class="n"><code>install</code></span><code class="w"> </code><span class="o"><code>-</code></span><span class="n"><code>c</code></span><code class="w"> </code><span class="n"><code>conda</code></span><span class="o"><code>-</code></span><span class="n"><code>forge</code></span><code class="w"> </code><span class="n"><code>scrapy</code></span></pre>&#13;
&#13;
<p>If you run into issues, or need up-to-date information, check out the <a href="https://doc.scrapy.org/en/latest/intro/install.html">Scrapy </a><a contenteditable="false" data-primary="Scrapy" data-secondary="installing" data-startref="scystl" data-type="indexterm" id="id509"/> installation guide for more information.</p>&#13;
&#13;
<section data-pdf-bookmark="Initializing a New Spider" data-type="sect2"><div class="sect2" id="id146">&#13;
<h2>Initializing a New Spider</h2>&#13;
&#13;
<p>Once you’ve installed the Scrapy framework, a small <a contenteditable="false" data-primary="Scrapy" data-secondary="installing" data-tertiary="spiders, initialization" data-type="indexterm" id="sctpz"/><a contenteditable="false" data-primary="spiders (Scrapy)" data-secondary="initialization" data-type="indexterm" id="spdyz"/>amount of setup needs to be done for each spider. A <em>spider</em> is a Scrapy project that, like its arachnid namesake, is designed to crawl webs. Throughout this chapter, I use “spider” to describe a Scrapy project in particular, and “crawler” to mean “any generic program that crawls the web, using Scrapy or not.”</p>&#13;
&#13;
<p>To create a new spider in the current directory, run the following from the command line:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">&#13;
$<code class="w"> </code>scrapy<code class="w"> </code>startproject<code class="w"> </code>wikiSpider<code class="w"/></pre>&#13;
&#13;
<p>This creates a new subdirectory in the directory the project was created in, with the title <em>wikiSpider</em>. Inside this directory is the following file structure:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p><em>scrapy.cfg</em></p>&#13;
	</li>&#13;
	<li>&#13;
	<p><em>wikiSpider</em></p>&#13;
&#13;
	<ul>&#13;
		<li>&#13;
		<p><em>spiders</em></p>&#13;
&#13;
		<ul>&#13;
			<li>&#13;
			<p><em>__init.py__</em></p>&#13;
			</li>&#13;
		</ul>&#13;
		</li>&#13;
		<li>&#13;
		<p class="pagebreak-before"><em>items.py</em></p>&#13;
		</li>&#13;
		<li>&#13;
		<p><em>middlewares.py</em></p>&#13;
		</li>&#13;
		<li>&#13;
		<p><em>pipelines.py</em></p>&#13;
		</li>&#13;
		<li>&#13;
		<p><em>settings.py</em></p>&#13;
		</li>&#13;
		<li>&#13;
		<p><em>__init.py__</em></p>&#13;
		</li>&#13;
	</ul>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>These Python files are initialized <a contenteditable="false" data-primary="Python" data-secondary="file initiation, stub code and" data-type="indexterm" id="id510"/>with stub code to provide a fast means of creating a new spider project. Each section in <a contenteditable="false" data-primary="Scrapy" data-secondary="installing" data-startref="sctpz" data-tertiary="spiders, initialization" data-type="indexterm" id="id511"/><a contenteditable="false" data-primary="spiders (Scrapy)" data-secondary="initialization" data-startref="spdyz" data-type="indexterm" id="id512"/>this chapter works with this <em>wikiSpider</em> project.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Writing a Simple Scraper" data-type="sect1"><div class="sect1" id="id48">&#13;
<h1>Writing a Simple Scraper</h1>&#13;
&#13;
<p>To create a crawler, you will add a <a contenteditable="false" data-primary="Scrapy" data-secondary="crawlers, creating" data-type="indexterm" id="sccct"/><a contenteditable="false" data-primary="spiders directory" data-type="indexterm" id="id513"/><a contenteditable="false" data-primary="scrapers" data-see="web crawlers" data-type="indexterm" id="id514"/>new file inside the child <em>spiders</em> directory at <em>wiki​S⁠pider/wikiSpider/spiders/article.py</em>. This is where all the spiders, or things that extend scrapy.Spider will go. In your newly created <em>article.py</em> file, write:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">scrapy</code> <code class="kn">import</code> <code class="n">Spider</code><code class="p">,</code> <code class="n">Request</code>&#13;
&#13;
<code class="k">class</code> <code class="nc">ArticleSpider</code><code class="p">(</code><code class="n">Spider</code><code class="p">):</code>&#13;
    <code class="n">name</code><code class="o">=</code><code class="s1">'article'</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">start_requests</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>&#13;
        <code class="n">urls</code> <code class="o">=</code> <code class="p">[</code>&#13;
            <code class="s1">'http://en.wikipedia.org/wiki/Python_%28programming_language%29'</code><code class="p">,</code>&#13;
            <code class="s1">'https://en.wikipedia.org/wiki/Functional_programming'</code><code class="p">,</code>&#13;
            <code class="s1">'https://en.wikipedia.org/wiki/Monty_Python'</code><code class="p">]</code>&#13;
        <code class="k">return</code> <code class="p">[</code><code class="n">Request</code><code class="p">(</code><code class="n">url</code><code class="o">=</code><code class="n">url</code><code class="p">,</code> <code class="n">callback</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">parse</code><code class="p">)</code> <code class="k">for</code> <code class="n">url</code> <code class="ow">in</code> <code class="n">urls</code><code class="p">]</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">parse</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">response</code><code class="p">):</code>&#13;
        <code class="n">url</code> <code class="o">=</code> <code class="n">response</code><code class="o">.</code><code class="n">url</code>&#13;
        <code class="n">title</code> <code class="o">=</code> <code class="n">response</code><code class="o">.</code><code class="n">css</code><code class="p">(</code><code class="s1">'h1::text'</code><code class="p">)</code><code class="o">.</code><code class="n">extract_first</code><code class="p">()</code>&#13;
        <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'URL is: </code><code class="si">{</code><code class="n">url</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>&#13;
        <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Title is: </code><code class="si">{</code><code class="n">title</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>&#13;
</pre>&#13;
&#13;
<p>The name of this class (<code>ArticleSpider</code>) does not reference “wiki” or “Wikipedia” at all, indicating that this class in particular is responsible for spidering through only article pages, under the broader category of <em>wikiSpider</em>, which you may later want to use to search for other page types.</p>&#13;
&#13;
<p>For large sites with many types of <a contenteditable="false" data-primary="Scrapy" data-secondary="content types" data-type="indexterm" id="id515"/>content, you might have separate Scrapy items for each type (blog posts, press releases, articles, etc.), each with different fields but all running under the same Scrapy project. The name of each spider must be unique within the project.</p>&#13;
&#13;
<p class="pagebreak-before">The other key things to notice about this spider are the two functions <code>start_requests</code> and <code>parse</code>:</p>&#13;
&#13;
<dl>&#13;
	<dt><code>start_requests</code></dt>&#13;
	<dd>A Scrapy-defined entry point to the program used to generate <code>Request</code> objects that Scrapy uses to crawl the website.</dd>&#13;
	<dt><code>parse</code></dt>&#13;
	<dd>A callback function defined by the user and passed to the <code>Request</code> object with <code>callback=self.parse</code>. Later, you’ll look at more powerful things that can be done with the <code>parse</code> function, but for now it prints the title of the page.</dd>&#13;
</dl>&#13;
&#13;
<p>You can run this <code>article</code> spider by navigating to the outer <em>wikiSpider</em> directory and running:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">&#13;
$<code class="w"> </code>scrapy<code class="w"> </code>runspider<code class="w"> </code>wikiSpider/spiders/article.py<code class="w"/>&#13;
</pre>&#13;
&#13;
<p>The default Scrapy output is <a contenteditable="false" data-primary="Scrapy" data-secondary="output" data-type="indexterm" id="id516"/>fairly verbose. Along with debugging information, this should print out lines like:</p>&#13;
&#13;
<pre data-code-language="text" data-type="programlisting">&#13;
2023-02-11 21:43:13 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://en.wik&#13;
ipedia.org/robots.txt&gt; (referer: None)&#13;
2023-02-11 21:43:14 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (3&#13;
01) to &lt;GET https://en.wikipedia.org/wiki/Python_%28programming_language%29&gt; from&#13;
 &lt;GET http://en.wikipedia.org/wiki/Python_%28programming_language%29&gt;&#13;
2023-02-11 21:43:14 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://en.wik&#13;
ipedia.org/wiki/Functional_programming&gt; (referer: None)&#13;
2023-02-11 21:43:14 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://en.wik&#13;
ipedia.org/wiki/Monty_Python&gt; (referer: None)&#13;
URL is: https://en.wikipedia.org/wiki/Functional_programming&#13;
Title is: Functional programming&#13;
2023-02-11 21:43:14 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://en.wik&#13;
ipedia.org/wiki/Python_%28programming_language%29&gt; (referer: None)&#13;
URL is: https://en.wikipedia.org/wiki/Monty_Python&#13;
Title is: Monty Python&#13;
URL is: https://en.wikipedia.org/wiki/Python_%28programming_language%29&#13;
Title is: Python (programming language)&#13;
</pre>&#13;
&#13;
<p>The scraper goes to the three pages listed as <a contenteditable="false" data-primary="Scrapy" data-secondary="crawlers, creating" data-startref="sccct" data-type="indexterm" id="id517"/>the URLs, gathers information, and then terminates.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Spidering with Rules" data-type="sect1"><div class="sect1" id="id49">&#13;
<h1>Spidering with Rules</h1>&#13;
&#13;
<p>The spider in the previous section isn’t much of a crawler, confined to scraping only the list of URLs it’s provided. It has no ability to seek new pages on its own. To turn it into a fully fledged crawler, you need to use the <code>CrawlSpider</code> class provided by Scrapy.</p>&#13;
&#13;
<div data-type="note" epub:type="note">&#13;
<h1>Code Organization Within the GitHub Repository</h1>&#13;
&#13;
<p>Unfortunately, the Scrapy framework cannot <a contenteditable="false" data-primary="spiders (Scrapy)" data-secondary="GitHub files" data-type="indexterm" id="spscyyg"/><a contenteditable="false" data-primary="Scrapy" data-secondary="GitHub files" data-type="indexterm" id="cpytfl"/><a contenteditable="false" data-primary="GitHub" data-type="indexterm" id="gthb"/>be run easily from within a Jupyter notebook, making a linear progression of code difficult to capture. For the purpose of presenting all code samples in the text, the scraper from the previous section is stored in the <em>article.py</em> file, while the following example, creating a Scrapy spider that traverses many pages, is stored in <em>articles.py</em> (note the use of the plural).</p>&#13;
&#13;
<p>Later examples will also be stored in separate files, with new filenames given in each section. Make sure you are using the correct filename when running these examples.</p>&#13;
</div>&#13;
&#13;
<p>This class can be found in the spiders file <em>articles.py</em> in the GitHub repository:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">scrapy.linkextractors</code> <code class="kn">import</code> <code class="n">LinkExtractor</code>&#13;
<code class="kn">from</code> <code class="nn">scrapy.spiders</code> <code class="kn">import</code> <code class="n">CrawlSpider</code><code class="p">,</code> <code class="n">Rule</code>&#13;
&#13;
<code class="k">class</code> <code class="nc">ArticleSpider</code><code class="p">(</code><code class="n">CrawlSpider</code><code class="p">):</code>&#13;
    <code class="n">name</code> <code class="o">=</code> <code class="s1">'articles'</code>&#13;
    <code class="n">allowed_domains</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'wikipedia.org'</code><code class="p">]</code>&#13;
    <code class="n">start_urls</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'https://en.wikipedia.org/wiki/Benevolent_dictator_for_life'</code><code class="p">]</code>&#13;
    <code class="n">rules</code> <code class="o">=</code> <code class="p">[</code>&#13;
        <code class="n">Rule</code><code class="p">(</code>&#13;
            <code class="n">LinkExtractor</code><code class="p">(</code><code class="n">allow</code><code class="o">=</code><code class="sa">r</code><code class="s1">'.*'</code><code class="p">),</code>&#13;
<code class="err">​</code>            <code class="n">callback</code><code class="o">=</code><code class="s1">'parse_items'</code><code class="p">,</code>&#13;
<code class="err">​</code>            <code class="n">follow</code><code class="o">=</code><code class="kc">True</code>&#13;
<code class="err">​</code>        <code class="p">)</code>&#13;
    <code class="p">]</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">parse_items</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">response</code><code class="p">):</code>&#13;
        <code class="n">url</code> <code class="o">=</code> <code class="n">response</code><code class="o">.</code><code class="n">url</code>&#13;
        <code class="n">title</code> <code class="o">=</code> <code class="n">response</code><code class="o">.</code><code class="n">css</code><code class="p">(</code><code class="s1">'span.mw-page-title-main::text'</code><code class="p">)</code><code class="o">.</code><code class="n">extract_first</code><code class="p">()</code>&#13;
        <code class="n">text</code> <code class="o">=</code> <code class="n">response</code><code class="o">.</code><code class="n">xpath</code><code class="p">(</code><code class="s1">'//div[@id="mw-content-text"]//text()'</code><code class="p">)</code><code class="o">.</code><code class="n">extract</code><code class="p">()</code>&#13;
        <code class="n">lastUpdated</code> <code class="o">=</code> <code class="n">response</code><code class="o">.</code><code class="n">css</code><code class="p">(</code>&#13;
            <code class="s1">'li#footer-info-lastmod::text'</code>&#13;
        <code class="p">)</code><code class="o">.</code><code class="n">extract_first</code><code class="p">()</code>&#13;
        <code class="n">lastUpdated</code> <code class="o">=</code> <code class="n">lastUpdated</code><code class="o">.</code><code class="n">replace</code><code class="p">(</code><code class="s1">'This page was last edited on '</code><code class="p">,</code> <code class="s1">''</code><code class="p">)</code>&#13;
        <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'URL is: </code><code class="si">{</code><code class="n">url</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>&#13;
        <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Title is: </code><code class="si">{</code><code class="n">title</code><code class="si">}</code><code class="s1"> '</code><code class="p">)</code>&#13;
        <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Text is: </code><code class="si">{</code><code class="n">text</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>&#13;
        <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Last updated: </code><code class="si">{</code><code class="n">lastUpdated</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>&#13;
</pre>&#13;
&#13;
<p>This new <code>ArticleSpider</code> extends the <code>CrawlSpider</code> class. Rather than providing a <code>start_requests</code> function, it provides a list of <code>start_urls</code> and <code>allowed_domains</code>. This tells the spider where to start crawling from and whether it should follow or ignore a link based on the domain.</p>&#13;
&#13;
<p>A list of <code>rules</code> is also provided. This provides further instructions on which links to follow or ignore (in this case, you are allowing all URLs with the regular expression <code>.*</code>).</p>&#13;
&#13;
<p>In addition to extracting the title and URL on each page, a couple of new items have been added. The text content of each page <a contenteditable="false" data-primary="XPath" data-secondary="text content extraction" data-type="indexterm" id="id518"/>is extracted using an XPath selector. XPath is often used when retrieving text content including text in child tags (for example, an <code>&lt;a&gt;</code> tag inside a block of text). If you use the CSS selector to do this, all text within child tags will be ignored.</p>&#13;
&#13;
<p>The last updated date string is also parsed from the page footer and stored in the <code>lastUpdated</code> variable.</p>&#13;
&#13;
<p>You can run this example by navigating to the <em>wikiSpider </em>directory and running:</p>&#13;
&#13;
<pre class="pre" data-code-language="bash" data-type="programlisting">&#13;
$<code class="w"> </code>scrapy<code class="w"> </code>runspider<code class="w"> </code>wikiSpider/spiders/articles.py<code class="w"/></pre>&#13;
&#13;
<div data-type="warning" epub:type="warning">&#13;
<h1>Warning: This Will Run Forever</h1>&#13;
&#13;
<p>While this new spider runs in the command line in the same way as the simple spider built in the previous section, it will not terminate (at least not for a very, very long time) until you halt execution by using Ctrl-C or by closing the terminal. Please be kind to Wikipedia’s server load and do not run it for long.</p>&#13;
</div>&#13;
&#13;
<p>When run, this spider traverses <em>wikipedia.org</em>, following all links under the domain <em>wikipedia.org</em>, printing titles of pages, and ignoring all external (offsite) links:</p>&#13;
&#13;
<pre data-code-language="text" data-type="programlisting">&#13;
2023-02-11 22:13:34 [scrapy.spidermiddlewares.offsite] DEBUG: Filtered offsite&#13;
 request to 'drupal.org': &lt;GET https://drupal.org/node/769&gt;&#13;
2023-02-11 22:13:34 [scrapy.spidermiddlewares.offsite] DEBUG: Filtered offsite&#13;
 request to 'groups.drupal.org': &lt;GET https://groups.drupal.org/node/5434&gt;&#13;
2023-02-11 22:13:34 [scrapy.spidermiddlewares.offsite] DEBUG: Filtered offsite&#13;
 request to 'www.techrepublic.com': &lt;GET https://www.techrepublic.com/article/&#13;
open-source-shouldnt-mean-anti-commercial-says-drupal-creator-dries-buytaert/&gt;&#13;
2023-02-11 22:13:34 [scrapy.spidermiddlewares.offsite] DEBUG: Filtered offsite&#13;
 request to 'www.acquia.com': &lt;GET https://www.acquia.com/board-member/dries-b&#13;
uytaert&gt;&#13;
</pre>&#13;
&#13;
<p>This is a pretty good crawler so far, but it could use a few limits. Instead of just visiting article pages on Wikipedia, it’s free to roam to nonarticle pages as well, such as:</p>&#13;
&#13;
<pre data-code-language="text" data-type="programlisting">&#13;
title is: Wikipedia:General disclaimer&#13;
</pre>&#13;
&#13;
<p>Let’s take a closer look at the line by using Scrapy’s <code>Rule</code> and <code>LinkExtractor</code>:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="n">rules</code> <code class="o">=</code> <code class="p">[</code><code class="n">Rule</code><code class="p">(</code><code class="n">LinkExtractor</code><code class="p">(</code><code class="n">allow</code><code class="o">=</code><code class="sa">r</code><code class="s1">'.*'</code><code class="p">),</code> <code class="n">callback</code><code class="o">=</code><code class="s1">'parse_items'</code><code class="p">,</code>&#13;
<code class="err">​</code>    <code class="n">follow</code><code class="o">=</code><code class="kc">True</code><code class="p">)]</code></pre>&#13;
&#13;
<p class="pagebreak-before">This line <a contenteditable="false" data-primary="links" data-secondary="rules" data-type="indexterm" id="id519"/>provides a list of Scrapy <code>Rule</code> objects that define the rules that all found links are filtered through. When multiple rules are in place, each link is checked against the rules, in order. The first rule that matches is the one that is used to determine how the link is handled. If the link doesn’t match any rules, it is ignored.</p>&#13;
&#13;
<p>A <code>Rule</code> can be provided with four arguments:</p>&#13;
&#13;
<dl>&#13;
	<dt><code>link_extractor</code></dt>&#13;
	<dd>The only mandatory argument, a <code>LinkExtractor</code> object.</dd>&#13;
	<dt><code>callback</code></dt>&#13;
	<dd>The function that should be used to parse the content on the page.</dd>&#13;
	<dt><code>cb_kwargs</code></dt>&#13;
	<dd>A dictionary of arguments to be passed to the callback function. This dictionary is formatted as <code>{arg_name1: arg_value1, arg_name2: arg_value2}</code> and can be a handy tool for reusing the same parsing functions for slightly different tasks.</dd>&#13;
	<dt><code>follow</code></dt>&#13;
	<dd>Indicates whether you want links found at that page to be included in a future crawl. If no callback function is provided, this defaults to <code>True</code> (after all, if you’re not doing anything with the page, it makes sense that you’d at least want to use it to continue crawling through the site). If a callback function is provided, this defaults to <code>False</code>. </dd>&#13;
</dl>&#13;
&#13;
<p><code>LinkExtractor</code> is a simple <a contenteditable="false" data-primary="LinkExtractor class" data-type="indexterm" id="lkxtr"/>class designed solely to recognize and return links in a page of HTML content based on the rules provided to it. It has a number of arguments that can be used to accept or deny a link based on CSS and XPath selectors, tags (you can look for links in more than just anchor tags!), domains, and more.</p>&#13;
&#13;
<p>The <code>LinkExtractor</code> class can even be extended, and custom arguments can be created. See Scrapy’s <a href="https://doc.scrapy.org/en/latest/topics/link-extractors.html">documentation on link extractors</a>  for more information.</p>&#13;
&#13;
<p>Despite all the flexible features of the <code>LinkExtractor</code> class, the most common arguments you’ll use are these:</p>&#13;
&#13;
<dl>&#13;
	<dt><code>allow</code></dt>&#13;
	<dd>Allow all links that match the provided regular expression.</dd>&#13;
	<dt><code>deny</code></dt>&#13;
	<dd>Deny all links that match the provided regular expression.</dd>&#13;
</dl>&#13;
&#13;
<p>Using two separate <code>Rule</code> and <code>LinkExtractor</code> classes with a single parsing function, you can create a spider that crawls Wikipedia, identifying all article pages and flagging nonarticle pages (<em>articleMoreRules.py</em>):</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">scrapy.linkextractors</code> <code class="kn">import</code> <code class="n">LinkExtractor</code>&#13;
<code class="kn">from</code> <code class="nn">scrapy.spiders</code> <code class="kn">import</code> <code class="n">CrawlSpider</code><code class="p">,</code> <code class="n">Rule</code>&#13;
&#13;
<code class="k">class</code> <code class="nc">ArticleSpider</code><code class="p">(</code><code class="n">CrawlSpider</code><code class="p">):</code>&#13;
    <code class="n">name</code> <code class="o">=</code> <code class="s1">'articles'</code>&#13;
    <code class="n">allowed_domains</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'wikipedia.org'</code><code class="p">]</code>&#13;
    <code class="n">start_urls</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'https://en.wikipedia.org/wiki/Benevolent_dictator_for_life'</code><code class="p">]</code>&#13;
    <code class="n">rules</code> <code class="o">=</code> <code class="p">[</code>&#13;
        <code class="n">Rule</code><code class="p">(</code>&#13;
            <code class="n">LinkExtractor</code><code class="p">(</code><code class="n">allow</code><code class="o">=</code><code class="s1">'(/wiki/)((?!:).)*$'</code><code class="p">),</code>&#13;
            <code class="n">callback</code><code class="o">=</code><code class="s1">'parse_items'</code><code class="p">,</code>&#13;
            <code class="n">follow</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>&#13;
            <code class="n">cb_kwargs</code><code class="o">=</code><code class="p">{</code><code class="s1">'is_article'</code><code class="p">:</code> <code class="kc">True</code><code class="p">}</code>&#13;
        <code class="p">),</code>&#13;
        <code class="n">Rule</code><code class="p">(</code>&#13;
            <code class="n">LinkExtractor</code><code class="p">(</code><code class="n">allow</code><code class="o">=</code><code class="s1">'.*'</code><code class="p">),</code>&#13;
            <code class="n">callback</code><code class="o">=</code><code class="s1">'parse_items'</code><code class="p">,</code>&#13;
            <code class="n">cb_kwargs</code><code class="o">=</code><code class="p">{</code><code class="s1">'is_article'</code><code class="p">:</code> <code class="kc">False</code><code class="p">}</code>&#13;
        <code class="p">)</code>&#13;
    <code class="p">]</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">parse_items</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">response</code><code class="p">,</code> <code class="n">is_article</code><code class="p">):</code>&#13;
        <code class="nb">print</code><code class="p">(</code><code class="n">response</code><code class="o">.</code><code class="n">url</code><code class="p">)</code>&#13;
        <code class="n">title</code> <code class="o">=</code> <code class="n">response</code><code class="o">.</code><code class="n">css</code><code class="p">(</code><code class="s1">'span.mw-page-title-main::text'</code><code class="p">)</code><code class="o">.</code><code class="n">extract_first</code><code class="p">()</code>&#13;
        <code class="k">if</code> <code class="n">is_article</code><code class="p">:</code>&#13;
            <code class="n">url</code> <code class="o">=</code> <code class="n">response</code><code class="o">.</code><code class="n">url</code>&#13;
            <code class="n">text</code> <code class="o">=</code> <code class="n">response</code><code class="o">.</code><code class="n">xpath</code><code class="p">(</code>&#13;
                <code class="s1">'//div[@id="mw-content-text"]//text()'</code>&#13;
            <code class="p">)</code><code class="o">.</code><code class="n">extract</code><code class="p">()</code>&#13;
            <code class="n">lastUpdated</code> <code class="o">=</code> <code class="n">response</code><code class="o">.</code><code class="n">css</code><code class="p">(</code>&#13;
                <code class="s1">'li#footer-info-lastmod::text'</code>&#13;
            <code class="p">)</code><code class="o">.</code><code class="n">extract_first</code><code class="p">()</code>&#13;
            <code class="n">lastUpdated</code> <code class="o">=</code> <code class="n">lastUpdated</code><code class="o">.</code><code class="n">replace</code><code class="p">(</code>&#13;
                <code class="s1">'This page was last edited on '</code><code class="p">,</code>&#13;
                <code class="s1">''</code>&#13;
            <code class="p">)</code>&#13;
            <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'URL is: </code><code class="si">{</code><code class="n">url</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>&#13;
            <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Title is: </code><code class="si">{</code><code class="n">title</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>&#13;
            <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Text is: </code><code class="si">{</code><code class="n">text</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>&#13;
        <code class="k">else</code><code class="p">:</code>&#13;
            <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'This is not an article: </code><code class="si">{</code><code class="n">title</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>&#13;
</pre>&#13;
&#13;
<p>Recall that the rules are applied to each <a contenteditable="false" data-primary="LinkExtractor class" data-startref="lkxtr" data-type="indexterm" id="id520"/>link in the order that they are presented in the list. All article pages (pages that start with <em>/wiki/</em> and do not contain a colon) are passed to the <code>parse_items</code> function first with the default parameter <code>is_article=True</code>. Then all the other nonarticle links are passed to the <code>parse_items</code> function with the argument <code>is_article=False</code>.</p>&#13;
&#13;
<p>Of course, if you’re looking to collect only article-type pages and ignore all others, this approach would be impractical. It would be much easier to ignore pages that don’t match the article URL pattern and leave out the second rule (and the <span class="keep-together"><code>is_article</code></span> variable) altogether. However, this type of approach may be useful in odd cases where <a contenteditable="false" data-primary="spiders (Scrapy)" data-secondary="GitHub files (GitHub)" data-startref="spscyyg" data-type="indexterm" id="id521"/><a contenteditable="false" data-primary="Scrapy" data-secondary="GitHub files" data-startref="cpytfl" data-type="indexterm" id="id522"/><a contenteditable="false" data-primary="GitHub" data-startref="gthb" data-type="indexterm" id="id523"/>information from the URL, or information collected during crawling, impacts the way the page should be parsed.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Creating Items" data-type="sect1"><div class="sect1" id="id147">&#13;
<h1>Creating Items</h1>&#13;
&#13;
<p>So far, you’ve looked at many ways <a contenteditable="false" data-primary="Scrapy" data-secondary="item creation" data-type="indexterm" id="scrppy"/>of finding, parsing, and crawling websites with Scrapy, but Scrapy also provides useful tools to keep your collected items organized and stored in custom objects with well-defined fields.</p>&#13;
&#13;
<p>To help organize all the information you’re <a contenteditable="false" data-primary="Scrapy" data-secondary="Article class" data-type="indexterm" id="id524"/>collecting, you need to create an <code>Article</code> object. Define a new item called <code>Article</code> inside the <em>items.py</em> file.</p>&#13;
&#13;
<p>When you open the <em>items.py</em> file, it should look like this:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="c1"># -*- coding: utf-8 -*-</code>&#13;
&#13;
<code class="c1"># Define here the models for your scraped items</code>&#13;
<code class="c1">#</code>&#13;
<code class="c1"># See documentation in:</code>&#13;
<code class="c1"># http://doc.scrapy.org/en/latest/topics/items.html</code>&#13;
&#13;
<code class="kn">import</code> <code class="nn">scrapy</code>&#13;
&#13;
<code class="k">class</code> <code class="nc">WikispiderItem</code><code class="p">(</code><code class="n">scrapy</code><code class="o">.</code><code class="n">Item</code><code class="p">):</code>&#13;
    <code class="c1"># define the fields for your item here like:</code>&#13;
    <code class="c1"># name = scrapy.Field()</code>&#13;
    <code class="k">pass</code>&#13;
</pre>&#13;
&#13;
<p>Replace this default <code>Item</code> stub with a new <code>Article</code> class extending <code>scrapy.Item</code>:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="kn">import</code> <code class="nn">scrapy</code>&#13;
&#13;
<code class="k">class</code> <code class="nc">Article</code><code class="p">(</code><code class="n">scrapy</code><code class="o">.</code><code class="n">Item</code><code class="p">):</code>&#13;
    <code class="n">url</code> <code class="o">=</code> <code class="n">scrapy</code><code class="o">.</code><code class="n">Field</code><code class="p">()</code>&#13;
    <code class="n">title</code> <code class="o">=</code> <code class="n">scrapy</code><code class="o">.</code><code class="n">Field</code><code class="p">()</code>&#13;
    <code class="n">text</code> <code class="o">=</code> <code class="n">scrapy</code><code class="o">.</code><code class="n">Field</code><code class="p">()</code>&#13;
    <code class="n">lastUpdated</code> <code class="o">=</code> <code class="n">scrapy</code><code class="o">.</code><code class="n">Field</code><code class="p">()</code>&#13;
</pre>&#13;
&#13;
<p>You are defining four fields that will be collected from each page: URL, title, text content, and the date the page was last edited.</p>&#13;
&#13;
<p>If you are collecting data for multiple page types, you should define each separate type as its own class in <em>items.py</em>. If your items are large, or you start to move more parsing functionality into your item objects, you may also wish to extract each item into its own file. While the items are small, however, I like to keep them in a single file.</p>&#13;
&#13;
<p class="pagebreak-before">In the file <em>articleItems.py</em>, note the changes that were made to the <code>ArticleSpider</code> class in order to create the new <code>Article</code> item:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">scrapy.linkextractors</code> <code class="kn">import</code> <code class="n">LinkExtractor</code>&#13;
<code class="kn">from</code> <code class="nn">scrapy.spiders</code> <code class="kn">import</code> <code class="n">CrawlSpider</code><code class="p">,</code> <code class="n">Rule</code>&#13;
<code class="kn">from</code> <code class="nn">wikiSpider.items</code> <code class="kn">import</code> <code class="n">Article</code>&#13;
&#13;
<code class="k">class</code> <code class="nc">ArticleSpider</code><code class="p">(</code><code class="n">CrawlSpider</code><code class="p">):</code>&#13;
    <code class="n">name</code> <code class="o">=</code> <code class="s1">'articleItems'</code>&#13;
    <code class="n">allowed_domains</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'wikipedia.org'</code><code class="p">]</code>&#13;
    <code class="n">start_urls</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'https://en.wikipedia.org/wiki/Benevolent'</code>&#13;
<code class="err">​</code>    <code class="err">​</code>    <code class="s1">'_dictator_for_life'</code><code class="p">]</code>&#13;
    <code class="n">rules</code> <code class="o">=</code> <code class="p">[</code>&#13;
        <code class="n">Rule</code><code class="p">(</code><code class="n">LinkExtractor</code><code class="p">(</code><code class="n">allow</code><code class="o">=</code><code class="s1">'(/wiki/)((?!:).)*$'</code><code class="p">),</code>&#13;
<code class="err">​</code>    <code class="err">​</code>    <code class="err">​</code>    <code class="n">callback</code><code class="o">=</code><code class="s1">'parse_items'</code><code class="p">,</code> <code class="n">follow</code><code class="o">=</code><code class="kc">True</code><code class="p">),</code>&#13;
    <code class="p">]</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">parse_items</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">response</code><code class="p">):</code>&#13;
        <code class="n">article</code> <code class="o">=</code> <code class="n">Article</code><code class="p">()</code>&#13;
        <code class="n">article</code><code class="p">[</code><code class="s1">'url'</code><code class="p">]</code> <code class="o">=</code> <code class="n">response</code><code class="o">.</code><code class="n">url</code>&#13;
        <code class="n">article</code><code class="p">[</code><code class="s1">'title'</code><code class="p">]</code> <code class="o">=</code> <code class="n">response</code><code class="o">.</code><code class="n">css</code><code class="p">(</code><code class="s1">'h1::text'</code><code class="p">)</code><code class="o">.</code><code class="n">extract_first</code><code class="p">()</code>&#13;
        <code class="n">article</code><code class="p">[</code><code class="s1">'text'</code><code class="p">]</code> <code class="o">=</code> <code class="n">response</code><code class="o">.</code><code class="n">xpath</code><code class="p">(</code><code class="s1">'//div[@id='</code>&#13;
<code class="err">​</code>    <code class="err">​</code>    <code class="err">​</code>    <code class="s1">'"mw-content-text"]//text()'</code><code class="p">)</code><code class="o">.</code><code class="n">extract</code><code class="p">()</code>&#13;
        <code class="n">lastUpdated</code> <code class="o">=</code> <code class="n">response</code><code class="o">.</code><code class="n">css</code><code class="p">(</code><code class="s1">'li#footer-info-lastmod::text'</code>&#13;
<code class="err">​</code>    <code class="err">​</code>    <code class="err">​</code>    <code class="p">)</code><code class="o">.</code><code class="n">extract_first</code><code class="p">()</code>&#13;
        <code class="n">article</code><code class="p">[</code><code class="s1">'lastUpdated'</code><code class="p">]</code> <code class="o">=</code> <code class="n">lastUpdated</code><code class="o">.</code><code class="n">replace</code><code class="p">(</code><code class="s1">'This page was '</code>&#13;
<code class="err">​</code>    <code class="err">​</code>    <code class="err">​</code>    <code class="s1">'last edited on '</code><code class="p">,</code> <code class="s1">''</code><code class="p">)</code>&#13;
        <code class="k">return</code> <code class="n">article</code>&#13;
</pre>&#13;
&#13;
<p>When this file is run with</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">&#13;
$<code class="w"> </code>scrapy<code class="w"> </code>runspider<code class="w"> </code>wikiSpider/spiders/articleItems.py<code class="w"/>&#13;
</pre>&#13;
&#13;
<p>it will output the usual Scrapy debugging data along with each article item as a Python dictionary:</p>&#13;
&#13;
<pre data-code-language="text" data-type="programlisting">&#13;
2023-02-11 22:52:26 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://en.wik&#13;
ipedia.org/wiki/Benevolent_dictator_for_life#bodyContent&gt; (referer: https://en.wi&#13;
kipedia.org/wiki/Benevolent_dictator_for_life)&#13;
2023-02-11 22:52:26 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://en.wik&#13;
ipedia.org/wiki/OCaml&gt; (referer: https://en.wikipedia.org/wiki/Benevolent_dictato&#13;
r_for_life)&#13;
2023-02-11 22:52:26 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://en.wik&#13;
ipedia.org/wiki/Xavier_Leroy&gt; (referer: https://en.wikipedia.org/wiki/Benevolent_&#13;
dictator_for_life)&#13;
2023-02-11 22:52:26 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 https://en.wik&#13;
ipedia.org/wiki/Benevolent_dictator_for_life&gt;&#13;
{'lastUpdated': ' 7 February 2023, at 01:14',&#13;
'text': ['Title given to a small number of open-source software development '&#13;
          'leaders',&#13;
          ...&#13;
</pre>&#13;
&#13;
<p>Using Scrapy <code>Items</code> isn’t just <a contenteditable="false" data-primary="Scrapy" data-secondary="item creation" data-startref="scrppy" data-type="indexterm" id="id525"/>for promoting good code organization or laying things out in a readable way. Items provide many tools for outputting and processing data, covered in the next sections.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Outputting Items" data-type="sect1"><div class="sect1" id="id148">&#13;
<h1>Outputting Items</h1>&#13;
&#13;
<p>Scrapy uses the <code>Item</code> objects to <a contenteditable="false" data-primary="Scrapy" data-secondary="Item class" data-type="indexterm" id="id526"/><a contenteditable="false" data-primary="Scrapy" data-secondary="output" data-type="indexterm" id="id527"/>determine which pieces of information it should save from the pages it visits. This information can be saved by Scrapy in a variety of ways, such as CSV, JSON, or XML files, using the following commands:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">&#13;
$<code class="w"> </code>scrapy<code class="w"> </code>runspider<code class="w"> </code>articleItems.py<code class="w"> </code>-o<code class="w"> </code>articles.csv<code class="w"> </code>-t<code class="w"> </code>csv<code class="w"/>&#13;
$<code class="w"> </code>scrapy<code class="w"> </code>runspider<code class="w"> </code>articleItems.py<code class="w"> </code>-o<code class="w"> </code>articles.json<code class="w"> </code>-t<code class="w"> </code>json<code class="w"/>&#13;
$<code class="w"> </code>scrapy<code class="w"> </code>runspider<code class="w"> </code>articleItems.py<code class="w"> </code>-o<code class="w"> </code>articles.xml<code class="w"> </code>-t<code class="w"> </code>xml<code class="w"/></pre>&#13;
&#13;
<p>Each of these runs the scraper <code>articleItems</code> and writes the output in the specified format to the provided file. This file will be created if it does not exist already.</p>&#13;
&#13;
<p>You may have noticed that in the articles the spider created in previous examples, the text variable is a list of strings rather than a single string. Each string in this list represents text inside a single HTML element, whereas the content inside <code>&lt;div id="mw-content-text"&gt;</code>, from which you are collecting the text data, is composed of many child elements.</p>&#13;
&#13;
<p>Scrapy manages these more complex values <a contenteditable="false" data-primary="CSV (comma-separated values)" data-secondary="Scrapy and" data-type="indexterm" id="id528"/><a contenteditable="false" data-primary="Scrapy" data-secondary="CSV (comma-separated values)" data-type="indexterm" id="id529"/>well. In the CSV format, for example, it converts lists to strings and escapes all commas so that a list of text displays in a single CSV cell.</p>&#13;
&#13;
<p>In XML, each element of this list is preserved inside child value tags:</p>&#13;
&#13;
<pre data-code-language="html" data-type="programlisting">&#13;
<code class="p">&lt;</code><code class="nt">items</code><code class="p">&gt;</code>&#13;
<code class="p">&lt;</code><code class="nt">item</code><code class="p">&gt;</code>&#13;
    <code class="p">&lt;</code><code class="nt">url</code><code class="p">&gt;</code>https://en.wikipedia.org/wiki/Benevolent_dictator_for_life<code class="p">&lt;/</code><code class="nt">url</code><code class="p">&gt;</code>&#13;
    <code class="p">&lt;</code><code class="nt">title</code><code class="p">&gt;</code>Benevolent dictator for life<code class="p">&lt;/</code><code class="nt">title</code><code class="p">&gt;</code>&#13;
    <code class="p">&lt;</code><code class="nt">text</code><code class="p">&gt;</code>&#13;
        <code class="p">&lt;</code><code class="nt">value</code><code class="p">&gt;</code>For the political term, see <code class="p">&lt;/</code><code class="nt">value</code><code class="p">&gt;</code>&#13;
        <code class="p">&lt;</code><code class="nt">value</code><code class="p">&gt;</code>Benevolent dictatorship<code class="p">&lt;/</code><code class="nt">value</code><code class="p">&gt;</code>&#13;
        ...&#13;
    <code class="p">&lt;/</code><code class="nt">text</code><code class="p">&gt;</code>&#13;
    <code class="p">&lt;</code><code class="nt">lastUpdated</code><code class="p">&gt;</code> 7 February 2023, at 01:14.<code class="p">&lt;/</code><code class="nt">lastUpdated</code><code class="p">&gt;</code>&#13;
<code class="p">&lt;/</code><code class="nt">item</code><code class="p">&gt;</code>&#13;
....&#13;
</pre>&#13;
&#13;
<p>In the JSON format, lists are preserved as lists.</p>&#13;
&#13;
<p>Of course, you can use the <code>Item</code> objects yourself and write them to a file or a database in whatever way you want, simply by adding the appropriate code to the parsing function in the crawler.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="The Item Pipeline" data-type="sect1"><div class="sect1" id="id50">&#13;
<h1 class="pagebreak-before less_space">The Item Pipeline</h1>&#13;
&#13;
<p>Although Scrapy is single threaded, it is capable <a contenteditable="false" data-primary="Scrapy" data-secondary="asynchronous requests" data-type="indexterm" id="spyycr"/><a contenteditable="false" data-primary="Scrapy" data-secondary="item pipeline" data-type="indexterm" id="scpytmppl"/><a contenteditable="false" data-primary="item pipeline" data-type="indexterm" id="trmpple"/>of making and handling many requests asynchronously. This makes it faster than the scrapers written so far in this book, although I have always been a firm believer that faster is not always better when it comes to web scraping.</p>&#13;
&#13;
<p>The web server for the site you are trying to scrape must handle each of these requests, and it’s important to be a good citizen and evaluate whether this sort of server hammering is appropriate (or even wise for your own self-interests, as many websites have the ability and the will to block what they might see as malicious scraping activity). For more information about the ethics of web scraping, as well as the importance of appropriately throttling scrapers, see <a data-type="xref" href="ch19.html#c-19">Chapter 19</a>.</p>&#13;
&#13;
<p>With that said, using Scrapy’s item pipeline can improve the speed of your web scraper even further by performing all data processing while waiting for requests to be returned, rather than waiting for data to be processed before making another request. This type of optimization can even be necessary when data processing requires a great deal of time or processor-heavy calculations.</p>&#13;
&#13;
<p>To create an item pipeline, revisit the <em>settings.py</em> file created at the beginning of the chapter. You should see the following commented lines:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="c1"># Configure item pipelines</code>&#13;
<code class="c1"># See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html</code>&#13;
<code class="c1">#ITEM_PIPELINES = {</code>&#13;
<code class="c1">#    'wikiSpider.pipelines.WikispiderPipeline': 300,</code>&#13;
<code class="c1">#}</code>&#13;
</pre>&#13;
&#13;
<p>Uncomment the last three lines and replace them with:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="n">ITEM_PIPELINES</code> <code class="o">=</code> <code class="p">{</code>&#13;
    <code class="s1">'wikiSpider.pipelines.WikispiderPipeline'</code><code class="p">:</code> <code class="mi">300</code><code class="p">,</code>&#13;
<code class="p">}</code>&#13;
</pre>&#13;
&#13;
<p>This provides a Python class, <code>wikiSpider.pipelines.WikispiderPipeline</code>, that will be used to process the data, as well as an integer that represents the order in which to run the pipeline if there are multiple processing classes. Although any integer can be used here, the numbers 0–1,000 are typically used and will be run in ascending order.</p>&#13;
&#13;
<p>Now you need to add <a contenteditable="false" data-primary="spiders (Scrapy)" data-secondary="item pipeline and" data-type="indexterm" id="id530"/>the pipeline class and rewrite your original spider so that the spider collects data and the pipeline does the heavy lifting of the data processing. It might be tempting to write the <code>parse_items</code> method in your original spider to return the response and let the pipeline create the <code>Article</code> object:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
    <code class="k">def</code> <code class="nf">parse_items</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">response</code><code class="p">):</code>&#13;
        <code class="k">return</code> <code class="n">response</code>&#13;
</pre>&#13;
&#13;
<p>However, the Scrapy framework does not allow this, and an <code>Item</code> object (such as an <code>Article</code>, which extends <code>Item</code>) must be returned. So the goal of <code>parse_items</code> is now to extract the raw data, doing as little processing as possible, so that it can be passed to the pipeline:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">scrapy.linkextractors</code> <code class="kn">import</code> <code class="n">LinkExtractor</code>&#13;
<code class="kn">from</code> <code class="nn">scrapy.spiders</code> <code class="kn">import</code> <code class="n">CrawlSpider</code><code class="p">,</code> <code class="n">Rule</code>&#13;
<code class="kn">from</code> <code class="nn">wikiSpider.items</code> <code class="kn">import</code> <code class="n">Article</code>&#13;
&#13;
<code class="k">class</code> <code class="nc">ArticleSpider</code><code class="p">(</code><code class="n">CrawlSpider</code><code class="p">):</code>&#13;
    <code class="n">name</code> <code class="o">=</code> <code class="s1">'articlePipelines'</code>&#13;
    <code class="n">allowed_domains</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'wikipedia.org'</code><code class="p">]</code>&#13;
    <code class="n">start_urls</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'https://en.wikipedia.org/wiki/Benevolent_dictator_for_life'</code><code class="p">]</code>&#13;
    <code class="n">rules</code> <code class="o">=</code> <code class="p">[</code>&#13;
        <code class="n">Rule</code><code class="p">(</code><code class="n">LinkExtractor</code><code class="p">(</code><code class="n">allow</code><code class="o">=</code><code class="s1">'(/wiki/)((?!:).)*$'</code><code class="p">),</code>&#13;
<code class="err">​</code>    <code class="err">​</code>    <code class="err">​</code>    <code class="n">callback</code><code class="o">=</code><code class="s1">'parse_items'</code><code class="p">,</code> <code class="n">follow</code><code class="o">=</code><code class="kc">True</code><code class="p">),</code>&#13;
    <code class="p">]</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">parse_items</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">response</code><code class="p">):</code>&#13;
        <code class="n">article</code> <code class="o">=</code> <code class="n">Article</code><code class="p">()</code>&#13;
        <code class="n">article</code><code class="p">[</code><code class="s1">'url'</code><code class="p">]</code> <code class="o">=</code> <code class="n">response</code><code class="o">.</code><code class="n">url</code>&#13;
        <code class="n">article</code><code class="p">[</code><code class="s1">'title'</code><code class="p">]</code> <code class="o">=</code> <code class="n">response</code><code class="o">.</code><code class="n">css</code><code class="p">(</code><code class="s1">'h1::text'</code><code class="p">)</code><code class="o">.</code><code class="n">extract_first</code><code class="p">()</code>&#13;
        <code class="n">article</code><code class="p">[</code><code class="s1">'text'</code><code class="p">]</code> <code class="o">=</code> <code class="n">response</code><code class="o">.</code><code class="n">xpath</code><code class="p">(</code><code class="s1">'//div[@id='</code>&#13;
<code class="err">​</code>    <code class="err">​</code>    <code class="err">​</code>    <code class="s1">'"mw-content-text"]//text()'</code><code class="p">)</code><code class="o">.</code><code class="n">extract</code><code class="p">()</code>&#13;
        <code class="n">article</code><code class="p">[</code><code class="s1">'lastUpdated'</code><code class="p">]</code> <code class="o">=</code> <code class="n">response</code><code class="o">.</code><code class="n">css</code><code class="p">(</code><code class="s1">'li#'</code>&#13;
<code class="err">​</code>    <code class="err">​</code>    <code class="err">​</code>    <code class="s1">'footer-info-lastmod::text'</code><code class="p">)</code><code class="o">.</code><code class="n">extract_first</code><code class="p">()</code>&#13;
        <code class="k">return</code> <code class="n">article</code>&#13;
</pre>&#13;
&#13;
<p>This file is saved as <em>articlePipelines.py</em> in the GitHub repository.</p>&#13;
&#13;
<p>Of course, now you need <a contenteditable="false" data-primary="Scrapy" data-secondary="pipelines.py file" data-type="indexterm" id="id531"/>to tie the <em>pipelines.py</em> file and the updated spider together by adding the pipeline. When the Scrapy project was first initialized, a file was created at <em>wikiSpider/wikiSpider/pipelines.py</em>:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="c1"># -*- coding: utf-8 -*-</code>&#13;
&#13;
<code class="c1"># Define your item pipelines here</code>&#13;
<code class="c1">#</code>&#13;
<code class="c1"># Don't forget to add your pipeline to the ITEM_PIPELINES setting</code>&#13;
<code class="c1"># See: http://doc.scrapy.org/en/latest/topics/item-pipeline.html</code>&#13;
&#13;
&#13;
<code class="k">class</code> <code class="nc">WikispiderPipeline</code><code class="p">(</code><code class="nb">object</code><code class="p">):</code>&#13;
    <code class="k">def</code> <code class="nf">process_item</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">item</code><code class="p">,</code> <code class="n">spider</code><code class="p">):</code>&#13;
        <code class="k">return</code> <code class="n">item</code></pre>&#13;
&#13;
<p>This stub class should be replaced with your new pipeline code. In previous sections, you’ve been collecting two fields in a raw format, and these could use additional processing: <code>lastUpdated</code> (which is a badly formatted string object representing a date) and <code>text</code> (a messy array of string fragments).</p>&#13;
&#13;
<p>The following should be used to replace the stub code in <em>wikiSpider/wikiSpider/​pipe⁠lines.py:</em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">datetime</code> <code class="kn">import</code> <code class="n">datetime</code>&#13;
<code class="kn">from</code> <code class="nn">wikiSpider.items</code> <code class="kn">import</code> <code class="n">Article</code>&#13;
<code class="kn">from</code> <code class="nn">string</code> <code class="kn">import</code> <code class="n">whitespace</code>&#13;
&#13;
<code class="k">class</code> <code class="nc">WikispiderPipeline</code><code class="p">(</code><code class="nb">object</code><code class="p">):</code>&#13;
    <code class="k">def</code> <code class="nf">process_item</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">article</code><code class="p">,</code> <code class="n">spider</code><code class="p">):</code>&#13;
        <code class="n">dateStr</code> <code class="o">=</code> <code class="n">article</code><code class="p">[</code><code class="s1">'lastUpdated'</code><code class="p">]</code>&#13;
        <code class="n">article</code><code class="p">[</code><code class="s1">'lastUpdated'</code><code class="p">]</code> <code class="o">=</code> <code class="n">article</code><code class="p">[</code><code class="s1">'lastUpdated'</code><code class="p">]</code>&#13;
<code class="err">​</code>    <code class="err">​</code>    <code class="err">​</code>    <code class="o">.</code><code class="n">replace</code><code class="p">(</code><code class="s1">'This page was last edited on'</code><code class="p">,</code> <code class="s1">''</code><code class="p">)</code>&#13;
        <code class="n">article</code><code class="p">[</code><code class="s1">'lastUpdated'</code><code class="p">]</code> <code class="o">=</code> <code class="n">article</code><code class="p">[</code><code class="s1">'lastUpdated'</code><code class="p">]</code><code class="o">.</code><code class="n">strip</code><code class="p">()</code>&#13;
        <code class="n">article</code><code class="p">[</code><code class="s1">'lastUpdated'</code><code class="p">]</code> <code class="o">=</code> <code class="n">datetime</code><code class="o">.</code><code class="n">strptime</code><code class="p">(</code>&#13;
<code class="err">​</code>    <code class="err">​</code>    <code class="err">​</code>    <code class="n">article</code><code class="p">[</code><code class="s1">'lastUpdated'</code><code class="p">],</code> <code class="s1">'</code><code class="si">%d</code><code class="s1"> %B </code><code class="si">%Y</code><code class="s1">, at </code><code class="si">%H</code><code class="s1">:</code><code class="si">%M</code><code class="s1">.'</code><code class="p">)</code>&#13;
        <code class="n">article</code><code class="p">[</code><code class="s1">'text'</code><code class="p">]</code> <code class="o">=</code> <code class="p">[</code><code class="n">line</code> <code class="k">for</code> <code class="n">line</code> <code class="ow">in</code> <code class="n">article</code><code class="p">[</code><code class="s1">'text'</code><code class="p">]</code>&#13;
<code class="err">​</code>    <code class="err">​</code>    <code class="err">​</code>    <code class="k">if</code> <code class="n">line</code> <code class="ow">not</code> <code class="ow">in</code> <code class="n">whitespace</code><code class="p">]</code>&#13;
        <code class="n">article</code><code class="p">[</code><code class="s1">'text'</code><code class="p">]</code> <code class="o">=</code> <code class="s1">''</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">article</code><code class="p">[</code><code class="s1">'text'</code><code class="p">])</code>&#13;
        <code class="k">return</code> <code class="n">article</code>&#13;
</pre>&#13;
&#13;
<p>The class <code>WikispiderPipeline</code> has <a contenteditable="false" data-primary="WikispiderPipeline class (Scrapy)" data-type="indexterm" id="id532"/>a method <code>process_item</code> that takes in an <code>Article</code> object, parses the <code>lastUpdated</code> string into a Python <code>datetime</code> object, and cleans and joins the text into a single string from a list of strings.</p>&#13;
&#13;
<p><code>process_item</code> is a mandatory method for every pipeline class. Scrapy uses this method to asynchronously pass <code>Items</code> that are collected by the spider. The parsed <code>Article</code> object that is returned here will be logged or printed by Scrapy if, for example, you are outputting items to JSON or CSV, as was done in the previous section.</p>&#13;
&#13;
<p>You now have two choices when it comes to deciding where to do your data processing: the <code>parse_items</code> method in the spider, or the <code>process_items</code> method in the pipeline.</p>&#13;
&#13;
<p>Multiple pipelines with different tasks can be declared in the <em>settings.py</em> file. However, Scrapy passes all items, regardless of item type, to each pipeline in order. Item-specific parsing may be better handled in the spider, before the data hits the pipeline. However, if this parsing takes a long time, you may want to consider moving it to the pipeline (where it can be processed asynchronously) and adding a check on the item type:</p>&#13;
&#13;
<pre class="lang-py prettyprint prettyprinted" data-code-language="python">&#13;
<code class="k">def</code> <code class="nf">process_item</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">item</code><code class="p">,</code> <code class="n">spider</code><code class="p">):</code>    &#13;
    <code class="k">if</code> <code class="nb">isinstance</code><code class="p">(</code><code class="n">item</code><code class="p">,</code> <code class="n">Article</code><code class="p">):</code>&#13;
        <code class="c1"># Article-specific processing here</code>&#13;
</pre>&#13;
&#13;
<p>Which processing to do and where to <a contenteditable="false" data-primary="Scrapy" data-secondary="asynchronous requests" data-startref="spyycr" data-type="indexterm" id="id533"/><a contenteditable="false" data-primary="Scrapy" data-secondary="item pipeline" data-startref="scpytmppl" data-type="indexterm" id="id534"/><a contenteditable="false" data-primary="item pipeline" data-startref="trmpple" data-type="indexterm" id="id535"/>do it is an important consideration when it comes to writing Scrapy projects, especially large ones.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Logging with Scrapy" data-type="sect1"><div class="sect1" id="id51">&#13;
<h1>Logging with Scrapy</h1>&#13;
&#13;
<p>The debug information generated by Scrapy <a contenteditable="false" data-primary="Scrapy" data-secondary="logging and" data-type="indexterm" id="id536"/><a contenteditable="false" data-primary="logging, Scrapy and" data-type="indexterm" id="id537"/>can be useful, but, as you’ve likely noticed, it is often too verbose. You can easily adjust the level of logging by adding a line to the <em>settings.py</em> file in your Scrapy project:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="n">LOG_LEVEL</code> <code class="o">=</code> <code class="s1">'ERROR'</code></pre>&#13;
&#13;
<p>Scrapy uses a standard hierarchy of logging levels, as follows:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p><code>CRITICAL</code></p>&#13;
	</li>&#13;
	<li>&#13;
	<p><code>ERROR</code></p>&#13;
	</li>&#13;
	<li>&#13;
	<p><code>WARNING</code></p>&#13;
	</li>&#13;
	<li>&#13;
	<p><code>DEBUG</code></p>&#13;
	</li>&#13;
	<li>&#13;
	<p><code>INFO</code></p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>If logging is set to <code>ERROR</code>, only <code>CRITICAL</code> and <code>ERROR</code> logs will be displayed. If logging is set to <code>INFO</code>, all logs will be displayed, and so on.</p>&#13;
&#13;
<p>In addition to controlling logging through the <em>settings.py</em> file, you can control where the logs go from the command line. To output logs to a separate logfile instead of the terminal, define a logfile when running from the command line:</p>&#13;
&#13;
<pre class="pre" data-code-language="bash" data-type="programlisting">&#13;
$<code class="w"> </code>scrapy<code class="w"> </code>crawl<code class="w"> </code>articles<code class="w"> </code>-s<code class="w"> </code><code class="nv">LOG_FILE</code><code class="o">=</code>wiki.log<code class="w"/></pre>&#13;
&#13;
<p>This creates a new logfile, if one does not exist, in your current directory and outputs all logs to it, leaving your terminal clear to display only the Python print statements you manually add.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="More Resources" data-type="sect1"><div class="sect1" id="id245">&#13;
<h1>More Resources</h1>&#13;
&#13;
<p>Scrapy is a powerful tool that handles many problems associated with crawling the web. It automatically gathers all URLs and compares them against predefined rules, makes sure all URLs are unique, normalizes relative URLs where needed, and recurses to go more deeply into pages.</p>&#13;
&#13;
<p>I encourage you to check out the <a href="https://doc.scrapy.org/en/latest/news.html">Scrapy documentation</a> as well as <a href="https://docs.scrapy.org/en/latest/intro/tutorial.html">Scrapy’s official tutorial pages</a>, which provide a comprehensive discourse on the framework.</p>&#13;
&#13;
<p>Scrapy is an extremely large, sprawling library with many features. Its features work together seamlessly but have many areas of overlap that allow users to easily develop their own particular style within it. If there’s something you’d like to do with Scrapy that has not been mentioned here, there is likely a way (or several) to do it!</p>&#13;
</div></section>&#13;
</div></section></body></html>