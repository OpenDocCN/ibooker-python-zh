- en: Chapter 3\. Remote Functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You often need some form of distributed or parallel computing when building
    modern applications at scale. Many Python developers’ introduction to parallel
    computing is through the [multiprocessing module](https://oreil.ly/qj72E). Multiprocessing
    is limited in its ability to handle the requirements of modern applications. These
    requirements include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Running the same code on multiple cores or machines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using tooling to handle machine and processing failures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Efficiently handling large parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easily passing information between processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unlike multiprocessing, Ray’s remote functions satisfy these requirements. It’s
    important to note that *remote* doesn’t necessarily refer to a separate computer,
    despite its name; the function could be running on the same machine. What Ray
    does provide is mapping function calls to the right process on your behalf. Ray
    takes over distributing calls to that function instead of running in the same
    process. When calling remote functions, you are effectively running asynchronously
    on multiple cores or different machines, without having to concern yourself with
    how or where.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*Asynchronously* is a fancy way of saying running multiple things at the same
    time without waiting on each other.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn how to create remote functions, wait for their
    completion, and fetch results. Once you have the basics down, you will learn to
    compose remote functions together to create more complex operations. Before you
    go too far, let’s start with understanding some of what we glossed over in the
    previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Essentials of Ray Remote Functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Example 2-7](ch02.html#sleepy_task), you learned how to create a basic Ray
    remote function.
  prefs: []
  type: TYPE_NORMAL
- en: When you call a remote function, it immediately returns an `ObjectRef` (a future),
    which is a reference to a remote object. Ray creates and executes a task in the
    background on a separate worker process and writes the result when finished into
    the original reference. You can then call `ray.get` on the `ObjectRef` to obtain
    the value. Note that `ray.get` is a blocking method waiting for task execution
    to complete before returning the result.
  prefs: []
  type: TYPE_NORMAL
- en: Some details in [Example 2-7](ch02.html#sleepy_task) are worth understanding.
    The example converts the iterator to a list before passing it to `ray.get`. You
    need to do this when calling `ray.get` takes in a list of futures or an individual
    future.^([1](ch03.html#idm45354786316576)) The function waits until it has all
    the objects so it can return the list in order.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As with regular Ray remote functions, it’s important to think about the amount
    of work done inside each remote invocation. For example, using `ray.remote` to
    compute factorials recursively will be slower than doing it locally since the
    work inside each function is small even though the overall work can be large.
    The exact amount of time depends on how busy your cluster is, but as a general
    rule, anything executed in under a few seconds without any special resources is
    not worth scheduling remotely.
  prefs: []
  type: TYPE_NORMAL
- en: In our examples so far, using `ray.get` has been fine because the futures all
    had the same execution time. If the execution times are different, such as when
    training a model on different-sized batches of data, and you don’t need all of
    the results at the same time, this can be quite wasteful. Instead of directly
    calling `ray.get`, you should use `ray.wait`, which returns the requested number
    of futures that have already been completed. To see the performance difference,
    you will need to modify your remote function to have a variable sleep time, as
    in [Example 3-1](#variable_sleep_task).
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-1\. [Remote function with different execution times](https://oreil.ly/UdVmt)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As you recall, the example remote function sleeps based on the input argument.
    Since the range is in ascending order, calling the remote function on it will
    result in futures that are completed in order. To ensure that the futures won’t
    complete in order, you will need to modify the list. One way you can do this is
    by calling `things.sort(reverse=True)` prior to mapping your remote function over
    `things`.
  prefs: []
  type: TYPE_NORMAL
- en: To see the difference between using `ray.get` and `ray.wait`, you can write
    a function that collects the values from your futures with some time delay on
    each object to simulate business logic.
  prefs: []
  type: TYPE_NORMAL
- en: The first option, not using `ray.wait`, is a bit simpler and cleaner to read,
    as shown in [Example 3-2](#get_only), but is not recommended for production use.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-2\. [`ray.get` without the wait](https://oreil.ly/UdVmt)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The second option is a bit more complex, as shown in [Example 3-3](#as_available).
    This works by calling `ray.wait` to find the next available future and iterating
    until all the futures have been completed. `ray.wait` returns two lists, one of
    the object references for completed tasks (of the size requested, which defaults
    to 1) and another list of the rest of the object references.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-3\. [Using `ray.wait`](https://oreil.ly/UdVmt)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Running these functions side by side with `timeit.time`, you can see the difference
    in performance. It’s important to note that this performance improvement depends
    on how long the nonparallelized business logic (the logic in the loop) takes.
    If you’re just summing the results, using `ray.get` directly could be OK, but
    if you’re doing something more complex, you should use `ray.wait`. When we run
    this, we see that `ray.wait` performs roughly twice as fast. You can try varying
    the sleep times and see how it works out.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may wish to specify one of the few optional parameters to `ray.wait`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`num_returns`'
  prefs: []
  type: TYPE_NORMAL
- en: The number of `ObjectRef` objects for Ray to wait for completion before returning.
    You should set `num_returns` to less than or equal to the length of the input
    list of `ObjectRef` objects; otherwise, the function throws an exception.^([2](ch03.html#idm45354783539024))
    The default value is 1.
  prefs: []
  type: TYPE_NORMAL
- en: '`timeout`'
  prefs: []
  type: TYPE_NORMAL
- en: The maximum amount of time in seconds to wait before returning. This defaults
    to −1 (which is treated as infinite).
  prefs: []
  type: TYPE_NORMAL
- en: '`fetch_local`'
  prefs: []
  type: TYPE_NORMAL
- en: You can disable fetching of results by setting this to `false` if you are interested
    only in ensuring that the futures are completed.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The `timeout` parameter is extremely important in both `ray.get` and `ray.wait`.
    If this parameter is not specified and one of your remote functions misbehaves
    (never completes), the `ray.get` or `ray.wait` will never return, and your program
    will block forever.^([3](ch03.html#idm45354786609680)) As a result, for any production
    code, we recommend that you use the `timeout` parameter in both to avoid deadlocks.
  prefs: []
  type: TYPE_NORMAL
- en: Ray’s `get` and `wait` functions handle timeouts slightly differently. Ray doesn’t
    raise an exception on `ray.wait` when a timeout occurs; instead, it simply returns
    fewer ready futures than `num_returns`. However, if `ray.get` encounters a timeout,
    Ray will raise a `GetTimeoutError`. Note that the return of the `wait`/`get` function
    does not mean that your remote function will be terminated; it will still run
    in the dedicated process. You can explicitly terminate your future (see the following
    tip) if you want to release the resources.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Since `ray.wait` can return results in any order, it’s essential to not depend
    on the order of the results. If you need to do different processing with different
    records (e.g., test a mix of group A and group B), you should encode this in the
    result (often with types).
  prefs: []
  type: TYPE_NORMAL
- en: If you have a task that does not finish in a reasonable time (e.g., a straggler),
    you can cancel the task by using `ray.cancel` with the same `ObjectRef` used to
    `wait`/`get`. You can modify the previous `ray.wait` example to add a timeout
    and cancel any “bad” tasks, resulting in something like [Example 3-4](#handle_bad_futures).
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-4\. [Using `ray.wait` with a timeout and a cancel](https://oreil.ly/UdVmt)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Canceling a task should not be part of your normal program flow. If you find
    yourself having to frequently cancel tasks, you should investigate what’s going
    on. Any subsequent calls to `wait` or `get` for a canceled task are unspecified
    and could raise an exception or return incorrect results.
  prefs: []
  type: TYPE_NORMAL
- en: Another minor point that we skipped in the previous chapter is that while the
    examples so far return only a single value, Ray remote functions can return multiple
    values, as with regular Python functions.
  prefs: []
  type: TYPE_NORMAL
- en: Fault tolerance is an important consideration for those running in a distributed
    environment. Say the worker executing the task dies unexpectedly (because either
    the process crashed or the machine failed). Ray will rerun the task (after a delay)
    until either the task succeeds or the maximum number of retries is exceeded. We
    cover fault tolerance more in [Chapter 5](ch05.html#ch05).
  prefs: []
  type: TYPE_NORMAL
- en: Composition of Remote Ray Functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can make your remote functions even more powerful by composing them. The
    two most common methods of composition with remote functions in Ray are pipelining
    and nested parallelism. You can compose your functions with nested parallelism
    to express recursive functions. Ray also allows you to express sequential dependencies
    without having to block or collect the result in the driver, known as *pipelining*.
  prefs: []
  type: TYPE_NORMAL
- en: You can build a pipelined function by using `ObjectRef` objects from an earlier
    `ray.remote` as parameters for a new remote function call. Ray will automatically
    fetch the `ObjectRef` objects and pass the underlying objects to your function.
    This approach allows for easy coordination between the function invocations. Additionally,
    such an approach minimizes data transfer; the result will be sent directly to
    the node where execution of the second remote function is executed. A simple example
    of such a sequential calculation is presented in [Example 3-5](#ray_remote_seq).
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-5\. [Ray pipelining/sequential remote execution with task dependency](https://oreil.ly/UdVmt)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This code defines two remote functions and then starts three instances of the
    first one. `ObjectRef` objects for all three instances are then used as input
    for the second function. In this case, Ray will wait for all three instances to
    complete before starting to execute `sum_values`. You can use this approach not
    only for passing data but also for expressing basic workflow style dependencies.
    There is no restriction on the number of `ObjectRef` objects you can pass, and
    you can also pass “normal” Python objects at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: You *cannot* use Python structures (for example, lists, dictionaries, or classes)
    containing `ObjectRef` instead of using `ObjectRef` directly. Ray waits for and
    resolves only `ObjectRef` objects that are passed directly to a function. If you
    attempt to pass a structure, you will have to do your own `ray.wait` and `ray.get`
    inside the function. [Example 3-6](#broken_ray_remote_seq) is a variation of [Example 3-5](#ray_remote_seq)
    that does not work.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-6\. [Broken sequential remote function execution with task dependency](https://oreil.ly/UdVmt)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[Example 3-6](#broken_ray_remote_seq) has been modified from [Example 3-5](#ray_remote_seq)
    to take a list of `ObjectRef` objects as parameters instead of `ObjectRef` objects
    themselves. Ray does not “look inside” any structure being passed in. Therefore,
    the function will be invoked immediately, and since types won’t match, the function
    will fail with an error `TypeError: unsupported operand type(s) for +: ''int''
    and ''ray._raylet.ObjectRef''`. You could fix this error by using `ray.wait` and
    `ray.get`, but this would still launch the function too early, resulting in unnecessary
    blocking.'
  prefs: []
  type: TYPE_NORMAL
- en: In another composition approach, *nested parallelism*, your remote function
    launches additional remote functions. This can be useful in many cases, including
    implementing recursive algorithms and combining hyperparameter tuning with parallel
    model training.^([4](ch03.html#idm45354786147744)) Let’s take a look at two ways
    to implement nested parallelism ([Example 3-7](#nested_par)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-7\. [Implementing nested parallelism](https://oreil.ly/UdVmt)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This code defines three remote functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`generate_numbers`'
  prefs: []
  type: TYPE_NORMAL
- en: A simple function that generates random numbers
  prefs: []
  type: TYPE_NORMAL
- en: '`remote_objrefs`'
  prefs: []
  type: TYPE_NORMAL
- en: Invokes several remote functions and returns resulting `ObjectRef` objects
  prefs: []
  type: TYPE_NORMAL
- en: '`remote_values`'
  prefs: []
  type: TYPE_NORMAL
- en: Invokes several remote functions, waits for their completion, and returns the
    resulting values
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from this example, nested parallelism allows for two approaches.
    In the first case (`remote_objrefs`), you return all the `ObjectRef` objects to
    the invoker of the aggregating function. The invoking code is responsible for
    waiting for all the remote functions’ completion and processing the results. In
    the second case (`remote_values`), the aggregating function waits for all the
    remote functions’ executions to complete and returns the actual execution results.
  prefs: []
  type: TYPE_NORMAL
- en: Returning all of the `ObjectRef` objects allows for more flexibility with nonsequential
    consumption, as described back in `ray.await`, but it is not suitable for many
    recursive algorithms. With many recursive algorithms (e.g., quicksort, factorial,
    etc.) we have many levels of a combination step that need to be performed, requiring
    that the results be combined at each level of recursion.
  prefs: []
  type: TYPE_NORMAL
- en: Ray Remote Best Practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you are using remote functions, keep in mind that you don’t want to make
    them too small. If the tasks are very small, using Ray can take longer than if
    you used Python without Ray. The reason for this is that every task invocation
    has a nontrivial overhead—​for example, scheduling, data passing, inter-process
    communication (IPC), and updating the system state. To get a real advantage from
    parallel execution, you need to make sure that this overhead is negligible compared
    to the execution time of the function itself.^([5](ch03.html#idm45354783975024))
  prefs: []
  type: TYPE_NORMAL
- en: As described in this chapter, one of the most powerful features of Ray `remote`
    is the ability to parallelize functions’ execution. Once you call the remote functions,
    the handle to the remote object (future) is returned immediately, and the invoker
    can continue execution either locally or with additional remote functions. If,
    at this point, you call `ray.get`, your code will block, waiting for a remote
    function to complete, and as a result, you will have no parallelism. To ensure
    parallelization of your code, you should invoke `ray.get` only at the point when
    you absolutely need the data to continue the main thread of execution. Moreover,
    as we’ve described, it is recommended to use `ray.wait` instead of `ray.get` directly.
    Additionally, if the result of one remote function is required for the execution
    of another remote function(s), consider using pipelining (described previously)
    to leverage Ray’s task coordination.
  prefs: []
  type: TYPE_NORMAL
- en: When you submit your parameters to remote functions, Ray does not submit them
    directly to the remote function, but rather copies the parameters into object
    storage and then passes `ObjectRef` as a parameter. As a result, if you send the
    same parameter to multiple remote functions, you are paying a (performance) penalty
    for storing the same data to the object storage several times. The larger the
    size of the data, the larger the penalty. To avoid this, if you need to pass the
    same data to multiple remote functions, a better option is to first put the shared
    data in object storage and use the resulting `ObjectRef` as a parameter to the
    function. We illustrate how to do this in [“Ray Objects”](ch05.html#ray_objects).
  prefs: []
  type: TYPE_NORMAL
- en: As we will show in [Chapter 5](ch05.html#ch05), remote function invocation is
    done by the Raylet component. If you invoke a lot of remote functions from a single
    client, all these invocations are done by a single Raylet. Therefore, it takes
    a certain amount of time for a given Raylet to process these requests, which can
    cause a delay in starting all the functions. A better approach, as described in
    the [“Ray Design Patterns” documentation](https://oreil.ly/PTZOI), is to use an
    invocation tree—​a nested function invocation as described in the previous section.
    Basically, a client creates several remote functions, each of which, in turn,
    creates more remote functions, and so on. In this approach, the invocations are
    spread across multiple Raylets, allowing scheduling to happen faster.
  prefs: []
  type: TYPE_NORMAL
- en: Every time you define a remote function by using the `@ray.remote` decorator,
    Ray exports these definitions to all Ray workers, which takes time (especially
    if you have a lot of nodes). To reduce the number of function exports, a good
    practice is to define as many of the remote tasks on the top level outside the
    loops and local functions using them.
  prefs: []
  type: TYPE_NORMAL
- en: Bringing It Together with an Example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ML models composed of other models (e.g., ensemble models) are well suited to
    evaluation with Ray. [Example 3-8](#ensemble_example) shows what it looks like
    to use Ray’s function composition for a hypothetical spam model for web links.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-8\. [Ensemble model](https://oreil.ly/UdVmt)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: By using Ray instead of taking the summation of the time to evaluate all the
    models, you instead need to wait for only the slowest model, and all other models
    that finish faster are “free.” For example, if the models take equal lengths of
    time to run, evaluating these models serially, without Ray, would take almost
    three times as long.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about a fundamental Ray feature—​remote functions’
    invocation and their use in creating parallel asynchronous execution of Python
    across multiple cores and machines. You also learned multiple approaches for waiting
    for remote functions to complete execution and how to use `ray.wait` to prevent
    deadlocks in your code.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you learned about remote function composition and how to use it for
    rudimentary execution control (mini workflows). You also learned to implement
    nested parallelism, enabling you to invoke several functions in parallel, with
    each of these functions in turn invoking more parallel functions. In the next
    chapter, you will learn how to manage state in Ray by using actors.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch03.html#idm45354786316576-marker)) Ray does not “go inside” classes
    or structures to resolve futures, so if you have a list of lists of futures or
    a class containing a future, Ray will not resolve the “inner” future.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch03.html#idm45354783539024-marker)) Currently, if the list of `ObjectRef`
    objects passed in is empty, Ray treats it as a special case, and returns immediately
    regardless of the value of `num_returns`.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch03.html#idm45354786609680-marker)) If you’re working interactively,
    you can fix this with a `SIGINT` or the stop button in Jupyter.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch03.html#idm45354786147744-marker)) You can then train multiple models
    in parallel and train each of the models using data parallel gradient computations,
    resulting in nested parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch03.html#idm45354783975024-marker)) As an exercise, you can remove `sleep`
    from the function in [Example 2-7](ch02.html#sleepy_task) and you will see that
    execution of remote functions on Ray takes several times longer than regular function
    invocation. Overhead is not constant, but rather depends on your network, size
    of the invocation parameters, etc. For example, if you have only small bits of
    data to transfer, the overhead will be lower than if you are transferring, say,
    the entire text of Wikipedia as a parameter.
  prefs: []
  type: TYPE_NORMAL
