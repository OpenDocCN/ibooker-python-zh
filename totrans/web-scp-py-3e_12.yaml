- en: Chapter 10\. Reading Documents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is tempting to think of the internet primarily as a collection of text-based
    websites interspersed with newfangled web 2.0 multimedia content that can mostly
    be ignored for the purposes of web scraping. However, this ignores what the internet
    most fundamentally is: a content-agnostic vehicle for transmitting files.'
  prefs: []
  type: TYPE_NORMAL
- en: Although the internet has been around in some form or another since the late
    1960s, HTML didn’t debut until 1992\. Until then, the internet consisted mostly
    of email and file transmission; the concept of web pages as we know them today
    didn’t exist. In other words, the internet is not a collection of HTML files.
    It is a collection of many types of documents, with HTML files often being used
    as a frame to showcase them. Without being able to read a variety of document
    types, including text, PDF, images, video, email, and more, we are missing out
    on a huge part of the available data.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter covers dealing with documents, whether you’re downloading them
    to a local folder or reading them and extracting data. You’ll also take a look
    at dealing with various types of text encoding, which can make it possible to
    read even foreign-language HTML pages.
  prefs: []
  type: TYPE_NORMAL
- en: Document Encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A document’s encoding tells applications—whether they are your computer’s operating
    system or your own Python code—how to read it. This encoding can usually be deduced
    from its file extension, although this file extension is not mandated by its encoding.
    I could, for example, save *myImage.jpg* as *myImage.txt* with no problems—at
    least until my text editor tried to open it. Fortunately, this situation is rare,
    and a document’s file extension is usually all you need to know to read it correctly.
  prefs: []
  type: TYPE_NORMAL
- en: On a fundamental level, all documents are encoded in 0s and 1s. On top of that,
    encoding algorithms define things such as “how many bits per character” or “how
    many bits represent the color for each pixel” (in the case of image files). On
    top of that, you might have a layer of compression, or some space-reducing algorithm,
    as is the case with PNG files.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although dealing with non-HTML files might seem intimidating at first, rest
    assured that with the right library, Python will be properly equipped to deal
    with any format of information you want to throw at it. The only difference between
    a text file, a video file, and an image file is how their 0s and 1s are interpreted.
    This chapter covers several commonly encountered types of files: text, CSV, PDFs,
    and Word documents.'
  prefs: []
  type: TYPE_NORMAL
- en: Notice that these are all, fundamentally, files that store text. For information
    about working with images, I recommend that you read through this chapter to get
    used to working with and storing different types of files, and then head to [Chapter 16](ch16.html#c-16)
    for more information on image processing!
  prefs: []
  type: TYPE_NORMAL
- en: Text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is somewhat unusual to have files stored as plain text online, but it is
    popular among bare-bones or old-school sites to have large repositories of text
    files. For example, the Internet Engineering Task Force (IETF) stores all of its
    published documents as HTML, PDF, and text files (see [*https://www.ietf.org/rfc/rfc1149.txt*](https://www.ietf.org/rfc/rfc1149.txt) as
    an example). Most browsers will display these text files just fine, and you should
    be able to scrape them with no problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'For most basic text documents, such as the practice file located at [*http://www.pythonscraping.com/pages/warandpeace/chapter1.txt*](http://www.pythonscraping.com/pages/warandpeace/chapter1.txt), you
    can use the following method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Normally, when you retrieve a page using `urlopen`, you turn it into a [`BeautifulSoup`](ch04.html#c-4) object
    in order to parse the HTML. In this case, you can read the page directly. Turning
    it into a BeautifulSoup object, while perfectly possible, would be counterproductive—there’s
    no HTML to parse, so the library would be useless. Once the text file is read
    in as a string, you merely have to analyze it as you would any other string read
    into Python. The disadvantage here, of course, is that you don’t have the ability
    to use HTML tags as context clues, pointing you in the direction of the text you
    actually need, versus the text you don’t want. This can present a challenge when
    you’re trying to extract certain information from text files.
  prefs: []
  type: TYPE_NORMAL
- en: Text Encoding and the Global Internet
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Most of the time, a file extension is all you need to know how to read a file
    correctly. Strangely enough though, this rule doesn’t apply to the most basic
    of all documents: the *.txt* file.'
  prefs: []
  type: TYPE_NORMAL
- en: Reading in text by using the previously described methods will work just fine
    9 times out of 10\. However, dealing with text on the internet can be a tricky
    business. Next, we’ll cover the basics of English and foreign-language encoding,
    from ASCII to Unicode to ISO, and how to deal with them.
  prefs: []
  type: TYPE_NORMAL
- en: A history of text encoding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ASCII was first developed in the 1960s, when bits were expensive and there was
    no reason to encode anything besides the Latin alphabet and a few punctuation
    characters. For this reason, only 7 bits were used to encode a total of 128 capital
    letters, lowercase letters, and punctuation. Even with all that creativity, they
    were still left with 33 non-printing characters, some of which were used, replaced,
    and/or became obsolete as technologies changed over the years. Plenty of space
    for everyone, right?
  prefs: []
  type: TYPE_NORMAL
- en: As any programmer knows, 7 is a strange number. It’s not a nice power of 2,
    but it’s temptingly close. Computer scientists in the 1960s fought over whether
    an extra bit should be added for the convenience of having a nice round number
    versus the practicality of files requiring less storage space. In the end, 7 bits
    won. However, in modern computing, each 7-bit sequence is padded with an extra
    0 at the beginning,^([1](ch10.html#id592)) leaving us with the worst of both worlds—14%
    larger files, and the lack of flexibility of only 128 characters.
  prefs: []
  type: TYPE_NORMAL
- en: In the early 1990s, people realized that more languages than just English existed,
    and that it would be really nice if computers could display them. A nonprofit
    named The Unicode Consortium attempted to bring about a universal text encoder
    by establishing encodings for every character that needs to be used in any text
    document, in any language. The goal was to include everything from the Latin alphabet
    this book is written in, to Cyrillic (кириллица), Chinese pictograms (象形), math
    and logic symbols (⨊, ≥), and even emoticons and miscellaneous symbols, such as
    the biohazard sign (☣) and peace symbol (☮).
  prefs: []
  type: TYPE_NORMAL
- en: The resulting encoder, as you might already know, was dubbed *UTF-8*, which
    stands for, confusingly, “Universal Character Set—Transformation Format 8 bit.”
    The *8 bit* here refers not to the size of every character but to the smallest
    size that a character requires to be displayed.
  prefs: []
  type: TYPE_NORMAL
- en: The actual size of a UTF-8 character is flexible. It can range from 1 byte to
    4 bytes, depending on where it is placed in the list of possible characters (more
    popular characters are encoded with fewer bytes; more obscure ones require more
    bytes).
  prefs: []
  type: TYPE_NORMAL
- en: 'How is this flexible encoding achieved? The use of 7 bits with an eventual
    useless leading 0 looked like a design flaw in ASCII at first but proved to be
    a huge advantage for UTF-8. Because ASCII was so popular, Unicode decided to take
    advantage of this leading 0 bit by declaring all bytes starting with a 0 to indicate
    that only one byte is used in the character, and making the two encoding schemes
    for ASCII and UTF-8 identical. Therefore, the following characters are valid in
    both UTF-8 and ASCII:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'And the following characters are valid only in UTF-8 and will be rendered as
    nonprintable if the document is interpreted as an ASCII document:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In addition to UTF-8, other UTF standards exist, such as UTF-16, UTF-24, and
    UTF-32, although documents encoded in these formats are rarely encountered except
    in unusual circumstances, which are outside the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: ​While this original “design flaw” of ASCII had a major advantage for UTF-8,
    the disadvantage has not entirely gone away. The first 8 bits of information in
    each character can still encode only 128 characters, not a full 256\. In a UTF-8
    character requiring multiple bytes, additional leading bits are spent, not on
    character encoding but on check bits used to prevent corruption. Of the 32 (8
    x 4) bits in 4-byte characters, only 21 bits are used for character encoding,
    for a total of 2,097,152 possible characters, of which, 1,114,112 are currently
    allocated.
  prefs: []
  type: TYPE_NORMAL
- en: The problem with all universal language-encoding standards, of course, is that
    any document written in a single foreign language may be much larger than it has
    to be. Although your language might consist only of 100 or so characters, you
    will need 16 bits for each character rather than just 8 bits, as is the case for
    the English-specific ASCII. This makes foreign-language text documents in UTF-8
    about twice the size of English-language text documents, at least for foreign
    languages that don’t use the Latin character set.
  prefs: []
  type: TYPE_NORMAL
- en: ISO solves this problem by creating specific encodings for each language. Like
    Unicode, it has the same encodings that ASCII does, but it uses the padding 0
    bit at the beginning of every character to allow it to create 128 special characters
    for all languages that require them. This works best for European languages that
    also rely heavily on the Latin alphabet (which remain in positions 0–127 in the
    encoding), but require additional special characters. This allows ISO-8859-1 (designed
    for the Latin alphabet) to have symbols such as fractions (e.g., ½) or the copyright
    sign (©).
  prefs: []
  type: TYPE_NORMAL
- en: Other ISO character sets, such as ISO-8859-9 (Turkish), ISO-8859-2 (German,
    among other languages), and ISO-8859-15 (French, among other languages) can also
    be found on the internet with some regularity.
  prefs: []
  type: TYPE_NORMAL
- en: Although the popularity of ISO-encoded documents has been declining in recent
    years, about 9% of websites on the internet are still encoded with some flavor
    of ISO,^([2](ch10.html#id598)) making it essential to know about and check for
    encodings before scraping a site.
  prefs: []
  type: TYPE_NORMAL
- en: Encodings in action
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous section, you used the default settings for `urlopen` to read
    text documents you might encounter on the internet. This works great for most
    English text. However, the second you encounter Russian, Arabic, or even a word
    like “résumé,” you might run into problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take the following code, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This reads in the first chapter of the original *War and Peace* (written in
    Russian and French) and prints it to the screen. This screen text reads, in part:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In addition, visiting this page in most browsers results in gibberish (see [Figure 10-1](#text_encoded)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Alt Text](assets/wsp3_1001.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-1\. French and Cyrillic text encoded in ISO-8859-1, the default text
    document encoding in many browsers
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Even for native Russian speakers, that might be a bit difficult to make sense
    of. The problem is that Python is attempting to read the document as an ASCII
    document, whereas the browser is attempting to read it as an ISO-8859-1 encoded
    document. Neither one, of course, realizes it’s a UTF-8 document.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can explicitly define the string to be UTF-8, which correctly formats the
    output into Cyrillic characters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Using this concept with BeautifulSoup looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Python encodes all characters into UTF-8 by default. You might be tempted to
    leave this alone and use UTF-8 encoding for every web scraper you write. After
    all, UTF-8 will also handle ASCII characters as well as foreign languages smoothly.
    However, it’s important to remember the 9% of websites out there that use some
    version of ISO encoding as well, so you can never avoid this problem entirely.
  prefs: []
  type: TYPE_NORMAL
- en: "Unfortunately, in the case of text documents, it’s impossible to concretely\
    \ determine what encoding a document has. Some libraries can examine the document\
    \ and make a best guess (using a little logic to realize that “Ñ€Ð°Ñ\x81Ñ\x81\
    ÐºÐ°Ð·Ñ” is probably not a word), but many times they’re wrong."
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, in the case of HTML pages, the encoding is usually contained in
    a tag found in the `<head>` section of the site. Most sites, particularly English-language
    sites, have this tag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Whereas the [ECMA International’s website](http://www.ecma-international.org)
    has this tag:^([3](ch10.html#id602))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: If you plan on doing a lot of web scraping, particularly of international sites,
    it might be wise to look for this meta tag and use the encoding it recommends
    when reading the contents of the page.
  prefs: []
  type: TYPE_NORMAL
- en: CSV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When web scraping, you are likely to encounter either a CSV file or a coworker
    who likes data formatted in this way. Fortunately, Python has a [fantastic library](https://docs.python.org/3.4/library/csv.html)
    for both reading and writing CSV files. Although this library is capable of handling
    many variations of CSV, this section focuses primarily on the standard format.
    If you have a special case you need to handle, consult the documentation!
  prefs: []
  type: TYPE_NORMAL
- en: Reading CSV Files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Python’s *csv* library is geared primarily toward working with local files,
    on the assumption that the CSV data you need is stored on your machine. Unfortunately,
    this isn’t always the case, especially when you’re web scraping. There are several
    ways to work around this:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the file locally by hand and point Python at the local file location.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write a Python script to download the file, read it, and (optionally) delete
    it after retrieval.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieve the file as a string from the web, and wrap the string in a `StringIO`
    object so that it behaves like a file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Although the first two options are workable, taking up hard drive space with
    files when you could easily keep them in memory is bad practice. It’s much better
    to read the file in as a string and wrap it in an object that allows Python to
    treat it as a file, without ever saving the file. The following script retrieves
    a CSV file from the internet (in this case, a list of Monty Python albums at [*http://pythonscraping.com/files/MontyPythonAlbums.csv*](http://pythonscraping.com/files/MontyPythonAlbums.csv))
    and prints it, row by row, to the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The output looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see from the code sample, the reader object returned by `csv.reader`
    is iterable and composed of Python list objects. Because of this, each row in
    the `csvReader` object is accessible in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice the first line: `The album "Name" was released in Year`. Although this
    might be an easy-to-ignore result when writing example code, you don’t want this
    getting into your data in the real world. A lesser programmer might simply skip
    the first row in the `csvReader` object, or write in a special case to handle
    it. Fortunately, an alternative to the `csv.reader` function takes care of all
    of this for you automatically. Enter `DictReader`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '`csv.DictReader` returns the values of each row in the CSV file as dictionary
    objects rather than list objects, with field names stored in the variable `dictReader.fieldnames`
    and as keys in each dictionary object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The downside, of course, is that it takes slightly longer to create, process,
    and print these `DictReader` objects as opposed to `csvReader`, but the convenience
    and usability are often worth the additional overhead. Also keep in mind that,
    when it comes to web scraping, the overhead required for requesting and retrieving
    website data from an external server will almost always be the unavoidable limiting
    factor in any program you write, so worrying about which technique might shave
    microseconds off your total runtime is often a moot point!
  prefs: []
  type: TYPE_NORMAL
- en: PDF
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As a Linux user, I know the pain of being sent a *.docx* file that my non-Microsoft
    software mangles, and struggling trying to find the codecs to interpret some new
    Apple media format. In some ways, Adobe was revolutionary in creating its Portable
    Document Format in 1993\. PDFs allowed users on different platforms to view image
    and text documents in exactly the same way, regardless of the platform they were
    viewing it on.
  prefs: []
  type: TYPE_NORMAL
- en: Although storing PDFs on the web is somewhat passé (why store content in a static,
    slow-loading format when you could write it up as HTML?), PDFs remain ubiquitous,
    particularly when dealing with official forms and filings.
  prefs: []
  type: TYPE_NORMAL
- en: In 2009, a Briton named Nick Innes made the news when he requested public student
    test result information from the Buckinghamshire City Council, which was available
    under the United Kingdom’s version of the Freedom of Information Act. After some
    repeated requests and denials, he finally received the information he was looking
    for—in the form of 184 PDF documents.
  prefs: []
  type: TYPE_NORMAL
- en: Although Innes persisted and eventually received a more properly formatted database,
    had he been an expert web scraper, he likely could have saved himself a lot of
    time in the courts and used the PDF documents directly, with one of Python’s many
    PDF-parsing modules.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, because the PDF is a relatively simple and open source document
    format, the space is crowded when it comes to PDF-parsing libraries. These projects
    are commonly built, abandoned, revived, and built again as the years go by. The
    most popular, full-featured, and easy-to-use library is currently [pypdf](https://pypi.org/project/pypdf/).
  prefs: []
  type: TYPE_NORMAL
- en: Pypdf is a free, open source library that allows users to extract text and images
    from PDFs. It will also allow you to perform operations on PDF files and generate
    them directly from Python if you want to make them rather than just read them.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can install as usual using pip:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The documentation is located at [*https://pypdf.readthedocs.io/en/latest/index.html*](https://pypdf.readthedocs.io/en/latest/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a basic implementation that allows you to read arbitrary PDFs to a
    string, given a local file object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives the familiar plain-text output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Note that the PDF file argument must be an actual file object. You must download
    the file first locally before you can pass it to the `Pdfreader` class. However,
    if you’re processing large numbers of PDF files and don’t want to keep the original
    files around, you can always overwrite the previous file by sending the same filename
    to `urlretrieve` after you’ve extracted the text.
  prefs: []
  type: TYPE_NORMAL
- en: The output from pypdf might not be perfect, especially for PDFs with images,
    oddly formatted text, or text arranged in tables or charts. However, for most
    text-only PDFs, the output should be no different than if the PDF were a text
    file.
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft Word and .docx
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At the risk of offending my friends at Microsoft: I do not like Microsoft Word.
    Not because it’s necessarily a bad piece of software, but because of the way its
    users misuse it. It has a particular talent for turning what should be simple
    text documents or PDFs into large, slow, difficult-to-open beasts that often lose
    all formatting from machine to machine, and are, for whatever reason, editable
    when the content is often meant to be static.'
  prefs: []
  type: TYPE_NORMAL
- en: Word files are designed for content creation, not content sharing. Nevertheless,
    they are ubiquitous on certain sites, containing important documents, information,
    and even charts and multimedia; in short, everything that can and should be created
    with HTML.
  prefs: []
  type: TYPE_NORMAL
- en: Before about 2008, Microsoft Office products used the proprietary *.doc *file
    format. This binary-file format was difficult to read and poorly supported by
    other word processors. In an effort to get with the times and adopt a standard
    that was used by many other pieces of software, Microsoft decided to use the Open
    Office XML-based standard, which made the files compatible with open source and
    other software.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, Python’s support for this file format, used by Google Docs, Open
    Office, and Microsoft Office, still isn’t great. There is the [python-docx library](http://python-docx.readthedocs.org/en/latest/),
    but this only gives users the ability to create documents and read only basic
    file data such as the size and title of the file, not the actual contents. To
    read the contents of a Microsoft Office file, you’ll need to roll your own solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to read the XML from the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This reads a remote Word document as a binary file object (`BytesIO` is analogous
    to `StringIO`, used earlier in this chapter), unzips it using Python’s core zipfile
    library (all *.docx* files are zipped to save space), and then reads the unzipped
    file, which is XML.
  prefs: []
  type: TYPE_NORMAL
- en: The Word document at [*http://pythonscraping.com/pages/AWordDocument.docx*](http://pythonscraping.com/pages/AWordDocument.docx) is
    shown in [Figure 10-2](#word_website).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/wsp3_1002.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-2\. This is a Word document that’s full of content you might want
    very much, but it’s difficult to access because I’m putting it on my website as
    a *.docx* file instead of publishing it as HTML. The word “unfortunatly” is misspelled.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The output of the Python script reading my simple Word document is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'There’s clearly a lot of metadata here, but the actual text content you want
    is buried. Fortunately, all of the text in the document, including the title at
    the top, is contained in `w:t` tags, which makes it easy to grab:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Note that instead of the *html.parser* parser that you normally use with `BeautifulSoup`,
    you’re passing it the *xml* parser. This is because colons are nonstandard in
    HTML tag names like `w:t`, and *html.parser* does not recognize them.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output isn’t perfect but it’s getting there, and printing each `w:t` tag
    on a new line makes it easy to see how Word is splitting up the text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the word “unfortunatly” is split up across multiple lines. In the
    original XML, it is surrounded with the tag `<w:proofErr w:type="spellStart"/>`.
    This is how Word highlights the misspelling with a red squiggly underline.
  prefs: []
  type: TYPE_NORMAL
- en: 'The title of the document is preceded by the style descriptor tag `<w:pstyle
    w:val="Title">`. Although this doesn’t make it extremely easy for us to identify
    titles (or other styled text) as such, using BeautifulSoup’s navigation features
    can be useful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This function easily can be expanded to print tags around a variety of text
    styles or label them in some other way.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch10.html#id592-marker)) This “padding” bit will come back to haunt us
    with the ISO standards a little later.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch10.html#id598-marker)) According to [W3Techs](https://w3techs.com/technologies/history_overview/character_encoding),
    which uses web crawlers to gather these sorts of statistics.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch10.html#id602-marker)) ECMA was one of the original contributors to
    the ISO standard, so it’s no surprise its website is encoded with a flavor of
    ISO.
  prefs: []
  type: TYPE_NORMAL
