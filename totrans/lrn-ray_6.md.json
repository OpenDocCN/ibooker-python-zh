["```py\npip install \"ray[data]\"==1.9.0\n```", "```py\nimport ray\n\n# Create a dataset containing integers in the range [0, 10000).\nds = ray.data.range(10000)\n\n# Basic operations: show the size of the dataset, get a few samples, print the schema.\nprint(ds.count())  # -> 10000\nprint(ds.take(5))  # -> [0, 1, 2, 3, 4]\nprint(ds.schema())  # -> <class 'int'>\n```", "```py\n# Save the dataset to a local file and load it back.\nray.data.range(10000).write_csv(\"local_dir\")\nds = ray.data.read_csv(\"local_dir\")\nprint(ds.count())\n```", "```py\n# Basic transformations: join two datasets, filter, and sort.\nds1 = ray.data.range(10000)\nds2 = ray.data.range(10000)\nds3 = ds1.union(ds2)\nprint(ds3.count())  # -> 20000\n\n# Filter the combined dataset to only the even elements.\nds3 = ds3.filter(lambda x: x % 2 == 0)\nprint(ds3.count())  # -> 10000\nprint(ds3.take(5))  # -> [0, 2, 4, 6, 8]\n\n# Sort the filtered dataset.\nds3 = ds3.sort()\nprint(ds3.take(5))  # -> [0, 0, 2, 2, 4]\n```", "```py\nds1 = ray.data.range(10000)\nprint(ds1.num_blocks())  # -> 200\nds2 = ray.data.range(10000)\nprint(ds2.num_blocks())  # -> 200\nds3 = ds1.union(ds2)\nprint(ds3.num_blocks())  # -> 400\n\nprint(ds3.repartition(200).num_blocks())  # -> 200\n```", "```py\nds = ray.data.from_items([{\"id\": \"abc\", \"value\": 1}, {\"id\": \"def\", \"value\": 2}])\nprint(ds.schema())  # -> id: string, value: int64\n```", "```py\npandas_df = ds.to_pandas()  # pandas_df will inherit the schema from our Dataset.\n```", "```py\nds = ray.data.range(10000).map(lambda x: x ** 2)\nds.take(5)  # -> [0, 1, 4, 9, 16]\n```", "```py\nimport numpy as np\n\nds = ray.data.range(10000).map_batches(lambda batch: np.square(batch).tolist())\nds.take(5)  # -> [0, 1, 4, 9, 16]\n```", "```py\ndef load_model():\n    # Return a dummy model just for this example.\n    # In reality, this would likely load some model weights onto a GPU.\n    class DummyModel:\n        def __call__(self, batch):\n            return batch\n\n    return DummyModel()\n\nclass MLModel:\n    def __init__(self):\n        # load_model() will only run once per actor that's started.\n        self._model = load_model()\n\n    def __call__(self, batch):\n        return self._model(batch)\n\nds.map_batches(MLModel, compute=\"actors\")\n```", "```py\nds = ray.data.read_parquet(\"s3://my_bucket/input_data\")\\\n        .map(cpu_intensive_preprocessing)\\\n        .map_batches(gpu_intensive_inference, compute=\"actors\", num_gpus=1)\\\n        .repartition(10)\\\n        .write_parquet(\"s3://my_bucket/output_predictions\")\n```", "```py\nds = ray.data.read_parquet(\"s3://my_bucket/input_data\")\\\n        .window(blocks_per_window=5)\\\n        .map(cpu_intensive_preprocessing)\\\n        .map_batches(gpu_intensive_inference, compute=\"actors\", num_gpus=1)\\\n        .repartition(10)\\\n        .write_parquet(\"s3://my_bucket/output_predictions\")\n```", "```py\nfrom sklearn import datasets\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import train_test_split\n\n@ray.remote\nclass TrainingWorker:\n    def __init__(self, alpha: float):\n        self._model = SGDClassifier(alpha=alpha)\n\n    def train(self, train_shard: ray.data.Dataset):\n        for i, epoch in enumerate(train_shard.iter_epochs()):\n            X, Y = zip(*list(epoch.iter_rows()))\n            self._model.partial_fit(X, Y, classes=[0, 1])\n\n        return self._model\n\n    def test(self, X_test: np.ndarray, Y_test: np.ndarray):\n        return self._model.score(X_test, Y_test)\n```", "```py\nALPHA_VALS = [0.00008, 0.00009, 0.0001, 0.00011, 0.00012]\n\nprint(f\"Starting {len(ALPHA_VALS)} training workers.\")\nworkers = [TrainingWorker.remote(alpha) for alpha in ALPHA_VALS]\n```", "```py\n# Generate training & validation data for a classification problem.\nX_train, X_test, Y_train, Y_test = train_test_split(*datasets.make_classification())\n\n# Create a dataset pipeline out of the training data. The data will be randomly\n# shuffled and split across the workers for 10 iterations.\ntrain_ds = ray.data.from_items(list(zip(X_train, Y_train)))\nshards = train_ds.repeat(10)\\\n                 .random_shuffle_each_window()\\\n                 .split(len(workers), locality_hints=workers)\n```", "```py\n# Wait for training to complete on all of the workers.\nray.get([worker.train.remote(shard) for worker, shard in zip(workers, shards)])\n```", "```py\n# Get validation results from each worker.\nprint(ray.get([worker.test.remote(X_test, Y_test) for worker in workers]))\n```", "```py\npip install ray[\"data\"]==1.9.0 dask\n```", "```py\nimport ray\nfrom ray.util.dask import enable_dask_on_ray\n\nray.init()  # Start or connect to Ray.\nenable_dask_on_ray()  # Enable the Ray scheduler backend for Dask.\n```", "```py\nimport dask\n\ndf = dask.datasets.timeseries()\ndf = df[df.y > 0].groupby(\"name\").x.std()\ndf.compute()  # Trigger the task graph to be evaluated.\n```", "```py\nimport ray\nds = ray.data.range(10000)\n\n# Convert the Dataset to a Dask DataFrame.\ndf = ds.to_dask()\nprint(df.std().compute())  # -> 2886.89568\n\n# Convert the Dask DataFrame back to a Dataset.\nds = ray.data.from_dask(df)\nprint(ds.std())  # -> 2886.89568\n```", "```py\npip install ray[\"data\"]==1.9.0 torch dask\n```", "```py\npip install awscli==1.22.1\naws s3 cp --no-sign-request \"s3://nyc-tlc/trip data/\" ./nyc_tlc_data/\n```", "```py\nimport ray\nfrom ray.util.dask import enable_dask_on_ray\n\nimport dask.dataframe as dd\n\nLABEL_COLUMN = \"is_big_tip\"\n\nenable_dask_on_ray()\n\ndef load_dataset(path: str, *, include_label=True):\n    # Load the data and drop unused columns.\n    df = dd.read_csv(path, assume_missing=True,\n                     usecols=[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\",\n                              \"passenger_count\", \"trip_distance\", \"fare_amount\",\n                              \"tip_amount\"])\n\n    # Basic cleaning, drop nulls and outliers.\n    df = df.dropna()\n    df = df[(df[\"passenger_count\"] <= 4) &\n            (df[\"trip_distance\"] < 100) &\n            (df[\"fare_amount\"] < 1000)]\n\n    # Convert datetime strings to datetime objects.\n    df[\"tpep_pickup_datetime\"] = dd.to_datetime(df[\"tpep_pickup_datetime\"])\n    df[\"tpep_dropoff_datetime\"] = dd.to_datetime(df[\"tpep_dropoff_datetime\"])\n\n    # Add three new features: trip duration, hour the trip started, and day of the week.\n    df[\"trip_duration\"] = (df[\"tpep_dropoff_datetime\"] -\n                           df[\"tpep_pickup_datetime\"]).dt.seconds\n    df = df[df[\"trip_duration\"] < 4 * 60 * 60] # 4 hours.\n    df[\"hour\"] = df[\"tpep_pickup_datetime\"].dt.hour\n    df[\"day_of_week\"] = df[\"tpep_pickup_datetime\"].dt.weekday\n\n    if include_label:\n        # Calculate label column: if tip was more or less than 20% of the fare.\n        df[LABEL_COLUMN] = df[\"tip_amount\"] > 0.2 * df[\"fare_amount\"]\n\n    # Drop unused columns.\n    df = df.drop(\n        columns=[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"tip_amount\"]\n    )\n\n    return ray.data.from_dask(df)\n```", "```py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nNUM_FEATURES = 6\n\nclass FarePredictor(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.fc1 = nn.Linear(NUM_FEATURES, 256)\n        self.fc2 = nn.Linear(256, 16)\n        self.fc3 = nn.Linear(16, 1)\n\n        self.bn1 = nn.BatchNorm1d(256)\n        self.bn2 = nn.BatchNorm1d(16)\n\n    def forward(self, *x):\n        x = torch.cat(x, dim=1)\n        x = F.relu(self.fc1(x))\n        x = self.bn1(x)\n        x = F.relu(self.fc2(x))\n        x = self.bn2(x)\n        x = F.sigmoid(self.fc3(x))\n\n        return x\n```", "```py\nimport ray.train as train\n\ndef train_epoch(iterable_dataset, model, loss_fn, optimizer, device):\n    model.train()\n    for X, y in iterable_dataset:\n        X = X.to(device)\n        y = y.to(device)\n\n        # Compute prediction error.\n        pred = torch.round(model(X.float()))\n        loss = loss_fn(pred, y)\n\n        # Backpropagation.\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n```", "```py\ndef validate_epoch(iterable_dataset, model, loss_fn, device):\n    num_batches = 0\n    model.eval()\n    loss = 0\n    with torch.no_grad():\n        for X, y in iterable_dataset:\n            X = X.to(device)\n            y = y.to(device)\n            num_batches += 1\n            pred = torch.round(model(X.float()))\n            loss += loss_fn(pred, y).item()\n    loss /= num_batches\n    result = {\"loss\": loss}\n    return result\n```", "```py\ndef train_func(config):\n    batch_size = config.get(\"batch_size\", 32)\n    lr = config.get(\"lr\", 1e-2)\n    epochs = config.get(\"epochs\", 3)\n\n    train_dataset_pipeline_shard = train.get_dataset_shard(\"train\")\n    validation_dataset_pipeline_shard = train.get_dataset_shard(\"validation\")\n\n    model = train.torch.prepare_model(FarePredictor())\n\n    loss_fn = nn.SmoothL1Loss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    train_dataset_iterator = train_dataset_pipeline_shard.iter_epochs()\n    validation_dataset_iterator = \\\n        validation_dataset_pipeline_shard.iter_epochs()\n\n    for epoch in range(epochs):\n        train_dataset = next(train_dataset_iterator)\n        validation_dataset = next(validation_dataset_iterator)\n\n        train_torch_dataset = train_dataset.to_torch(\n            label_column=LABEL_COLUMN,\n            batch_size=batch_size,\n        )\n        validation_torch_dataset = validation_dataset.to_torch(\n            label_column=LABEL_COLUMN,\n            batch_size=batch_size)\n\n        device = train.torch.get_device()\n\n        train_epoch(train_torch_dataset, model, loss_fn, optimizer, device)\n        result = validate_epoch(validation_torch_dataset, model, loss_fn,\n                                device)\n        train.report(**result)\n        train.save_checkpoint(epoch=epoch, model_weights=model.module.state_dict())\n```", "```py\ndef get_training_datasets(*, test_pct=0.8):\n    ds = load_dataset(\"nyc_tlc_data/yellow_tripdata_2020-01.csv\")\n    ds, _ = ds.split_at_indices([int(0.01 * ds.count())])\n    train_ds, test_ds = ds.split_at_indices([int(test_pct * ds.count())])\n    train_ds_pipeline = train_ds.repeat().random_shuffle_each_window()\n    test_ds_pipeline = test_ds.repeat()\n    return {\"train\": train_ds_pipeline, \"validation\": test_ds_pipeline}\n```", "```py\ntrainer = train.Trainer(\"torch\", num_workers=4)\nconfig = {\"lr\": 1e-2, \"epochs\": 3, \"batch_size\": 64}\ntrainer.start()\ntrainer.run(train_func, config, dataset=get_training_datasets())\nmodel_weights = trainer.latest_checkpoint.get(\"model_weights\")\ntrainer.shutdown()\n```", "```py\nclass InferenceWrapper:\n    def __init__(self):\n        self._model = FarePredictor()\n        self._model.load_state_dict(model_weights)\n        self._model.eval()\n\n    def __call__(self, df):\n        tensor = torch.as_tensor(df.to_numpy(), dtype=torch.float32)\n        with torch.no_grad():\n            predictions = torch.round(self._model(tensor))\n        df[LABEL_COLUMN] = predictions.numpy()\n        return df\n\nds = load_dataset(\"nyc_tlc_data/yellow_tripdata_2021-01.csv\", include_label=False)\nds.map_batches(InferenceWrapper, compute=\"actors\").write_csv(\"output\")\n```"]