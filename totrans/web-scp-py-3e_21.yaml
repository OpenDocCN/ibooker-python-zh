- en: Chapter 19\. Web Scraping in Parallel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Web crawling is fast. At least, it’s usually much faster than hiring a dozen
    interns to copy data from the internet by hand! Of course, the progression of
    technology and the hedonic treadmill demand that at a certain point even this
    will not be “fast enough.” That’s the point at which people generally start to
    look toward distributed computing.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike most other technology fields, web crawling cannot often be improved simply
    by “throwing more cycles at the problem.” Running one process is fast; running
    two processes is not necessarily twice as fast. Running three processes might
    get you banned from the remote server you’re hammering on with all your requests!
  prefs: []
  type: TYPE_NORMAL
- en: 'However, in some situations parallel web crawling, or running parallel threads
    or processes, can still be of benefit:'
  prefs: []
  type: TYPE_NORMAL
- en: Collecting data from multiple sources (multiple remote servers) instead of just
    a single source
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing long or complex operations on the collected data (such as doing image
    analysis or OCR) that could be done in parallel with fetching the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collecting data from a large web service where you are paying for each query,
    or where creating multiple connections to the service is within the bounds of
    your usage agreement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processes Versus Threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Threads and processes are not a Python-specific concept. While the exact implementation
    details differ between (and are dependent on) operating systems, the general consensus
    in computer science is that processes are larger and have their own memory, while
    threads are smaller and share memory within the process that contains them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally, when you run a simple Python program, you are running it within
    its own process which contains a single thread. But Python supports both multiprocessing
    and multithreading. Both multiprocessing and multithreading achieve the same ultimate
    goal: performing two programming tasks in parallel instead of running one function
    after another in a more traditional linear way.'
  prefs: []
  type: TYPE_NORMAL
- en: However, you must consider the pros and cons of each carefully. For example,
    each process has its own memory allocated separately by the operating system. This
    means that memory is not shared between processes. While multiple threads can
    happily write to the same shared Python queues, lists, and other objects, processes
    cannot and must communicate this information more explicitly.
  prefs: []
  type: TYPE_NORMAL
- en: Using multithreaded programming to execute tasks in separate threads with shared
    memory is often considered easier than multiprocess programming. But this convenience
    comes at a cost.
  prefs: []
  type: TYPE_NORMAL
- en: Python’s global interpreter lock (or GIL) acts to prevent threads from executing
    the same line of code at once. The GIL ensures that the common memory shared by
    all processes does not become corrupted (for instance, bytes in memory being half
    written with one value and half written with another value). This locking makes
    it possible to write a multithreaded program and know what you’re getting, within
    the same line, but it also has the potential to create bottlenecks.
  prefs: []
  type: TYPE_NORMAL
- en: Multithreaded Crawling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following example illustrates using multiple threads to perform a task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a reference to the classic [FizzBuzz programming test](http://wiki.c2.com/?FizzBuzzTest),
    with a somewhat more verbose output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The script starts three threads, one that prints “Fizz” every three seconds,
    another that prints “Buzz” every five seconds, and a third that prints “Counter" every
    second.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rather than printing fizzes and buzzes, you can perform a useful task in the
    threads, such as crawling a website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Note the inclusion of this line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Because you are crawling Wikipedia almost twice as fast as you would with just
    a single thread, the inclusion of this line prevents the script from putting too
    much of a load on Wikipedia’s servers. In practice, when running against a server
    where the number of requests is not an issue, this line should be removed.
  prefs: []
  type: TYPE_NORMAL
- en: 'What if you want to rewrite this slightly to keep track of the articles the
    threads have collectively seen so far, so that no article is visited twice? You
    can use a list in a multithreaded environment in the same way that you use it
    in a single-threaded environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Note that you are appending the path to the list of visited paths as the first
    action that `scrape_article` takes. This reduces, but does not entirely eliminate,
    the chances that it will be scraped twice.
  prefs: []
  type: TYPE_NORMAL
- en: If you are unlucky, both threads might still stumble across the same path at
    the same instant, both will see that it is not in the visited list, and both will
    subsequently add it to the list and scrape at the same time. However, in practice
    this is unlikely to happen because of the speed of execution and the number of
    pages that Wikipedia contains.
  prefs: []
  type: TYPE_NORMAL
- en: This is an example of a *race condition*. Race conditions can be tricky to debug,
    even for experienced programmers, so it is important to evaluate your code for
    these potential situations, estimate their likelihood, and anticipate the seriousness
    of their impact.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of this particular race condition, where the scraper goes over the
    same page twice, it may not be worth writing around.
  prefs: []
  type: TYPE_NORMAL
- en: Race Conditions and Queues
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although you can communicate between threads with lists, lists are not specifically
    designed for communication between threads, and their misuse can easily cause
    slow program execution or even errors resulting from race conditions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lists are great for appending to or reading from, but they’re not so great
    for removing items at arbitrary points, especially from the beginning of the list.
    Using a line like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: actually requires Python to rewrite the entire list, slowing program execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'More dangerous, lists also make it convenient to accidentally write in a line
    that isn’t thread-safe. For instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: may not actually get you the last item in the list in a multithreaded environment,
    or it may even throw an exception if the value for `len(myList)-1` is calculated
    immediately before another operation modifies the list.
  prefs: []
  type: TYPE_NORMAL
- en: 'One might argue that the preceding statement can be more “Pythonically” written
    as `myList[-1]`, and of course, no one has *ever* accidentally written non-Pythonic
    code in a moment of weakness (especially not former Java developers like myself,
    thinking back to their days of patterns like `myList[myList.length-1]` )! But
    even if your code is beyond reproach, consider these other forms of nonthread-safe
    lines involving lists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Both of these may result in a race condition that can cause unexpected results.
    You might be tempted to try another approach and use some other variable types
    besides lists. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This seems like an excellent solution until you realize that you might have
    inadvertently overwritten another message coming in from another thread, in the
    instant between the first and second lines, with the text “I’ve retrieved the
    message.” So now you just need to construct an elaborate series of personal message
    objects for each thread with some logic to figure out who gets what...or you could
    use the `Queue` module built for this exact purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Queues are list-like objects that operate on either a first in, first out (FIFO)
    or a last in, first out (LIFO) approach. A queue receives messages from any thread
    via `queue.put('My message')` and can transmit the message to any thread that
    calls `queue.get()`.
  prefs: []
  type: TYPE_NORMAL
- en: Queues are not designed to store static data but to transmit it in a thread-safe
    way. After the data is retrieved from the queue, it should exist only in the thread
    that retrieved it. For this reason, they are commonly used to delegate tasks or
    send temporary notifications.
  prefs: []
  type: TYPE_NORMAL
- en: This can be useful in web crawling. For instance, let’s say that you want to
    persist the data collected by your scraper into a database, and you want each
    thread to be able to persist its data quickly. A single shared connection for
    all threads might cause issues (a single connection cannot handle requests in
    parallel), but it makes no sense to give every single scraping thread its own
    database connection. As your scraper grows in size (eventually you may be collecting
    data from a hundred different websites in a hundred different threads), this might
    translate into a lot of mostly idle database connections doing only an occasional
    write after a page loads.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, you can have a smaller number of database threads, each with its own
    connection, sitting around taking items from a queue and storing them. This provides
    a much more manageable set of database connections:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This script creates three threads: two to scrape pages from Wikipedia in a
    random walk, and a third to store the collected data in a MySQL database. For
    more information about MySQL and data storage, see [Chapter 9](ch09.html#c-9).'
  prefs: []
  type: TYPE_NORMAL
- en: This scraper is also simplified somewhat from the previous one. Rather than
    deal with both the title and the page’s URL, it concerns itself with the URL only. Also,
    as an acknowledgement to the fact that both threads might attempt to add the exact
    same URL to the `visited` list at the exact same time, I’ve turned this list into
    a set. Although it is not strictly thread-safe, the redundancies are built in
    so that any duplicates won’t have any effect on the end result.
  prefs: []
  type: TYPE_NORMAL
- en: More Features of the Threading Module
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Python `threading` module is a higher-level interface built on the lower-level  `_thread`
    module. Although `_thread` is perfectly usable all on its own, it takes a little
    more effort and doesn’t provide the little things that make life so enjoyable—like
    convenience functions and nifty features.
  prefs: []
  type: TYPE_NORMAL
- en: For example, you can use static functions like `enumerate` to get a list of
    all active threads initialized through the `threading` module without needing
    to keep track of them yourself. The `activeCount` function, similarly, provides
    the total number of threads. Many functions from `_thread` are given more convenient
    or memorable names, like `currentThread` instead of `get_ident` to get the name
    of the current thread.
  prefs: []
  type: TYPE_NORMAL
- en: One of the nice things about the threading module is the ease of creating local
    thread data that is unavailable to the other threads. This might be a nice feature
    if you have several threads, each scraping a different website, and each keeping
    track of its own local list of visited pages.
  prefs: []
  type: TYPE_NORMAL
- en: 'This local data can be created at any point within the thread function by calling
    `threading.local()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This solves the problem of race conditions happening between shared objects
    in threads. Whenever an object does not need to be shared, it should not be, and
    should be kept in local thread memory. To safely share objects between threads,
    the `Queue` from the previous section can still be used.
  prefs: []
  type: TYPE_NORMAL
- en: The threading module acts as a thread babysitter of sorts, and it can be highly
    customized to define what that babysitting entails. The `isAlive` function by
    default looks to see if the thread is still active. It will be true until a thread
    completes crawling (or crashes).
  prefs: []
  type: TYPE_NORMAL
- en: 'Often, crawlers are designed to run for a very long time. The `isAlive` method
    can ensure that, if a thread crashes, it restarts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Other monitoring methods can be added by extending the `threading.Thread` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This new `Crawler` class contains an `isDone` method that can be used to check
    if the crawler is done crawling. This may be useful if there are some additional
    logging methods that need to be finished so the thread cannot close, but the bulk
    of the crawling work is done. In general, `isDone` can be replaced with some sort
    of status or progress measure—how many pages logged, or the current page, for
    example.
  prefs: []
  type: TYPE_NORMAL
- en: Any exceptions raised by `Crawler.run` will cause the class to be restarted
    until `isDone` is `True` and the program exits.
  prefs: []
  type: TYPE_NORMAL
- en: Extending `threading.Thread` in your crawler classes can improve their robustness
    and flexibility, as well as your ability to monitor any property of many crawlers
    at once.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple Processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Python `Processing` module creates new process objects that can be started
    and joined from the main process. The following code uses the FizzBuzz example
    from the section on threading processes to demonstrate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Remember that each process is treated as an individual independent program by
    the OS. If you view your processes through your OS’s activity monitor or task
    manager, you should see this reflected, as shown in [Figure 19-1](#five-python-processes-running).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/wsp3_1901.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 19-1\. Five Python processes running while running FizzBuzz
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The fourth process with PID 76154 is a running Jupyter notebook instance, which
    should appear if you are running this from the IPython notebook. The fifth process,
    83560, is the main thread of execution, which starts up when the program is first
    executed. The PIDs are allocated by the OS sequentially. Unless you happen to
    have another program that quickly allocates a PID while the FizzBuzz script is
    running, you should see three more sequential PIDs—in this case 83561, 83562,
    and 83563.
  prefs: []
  type: TYPE_NORMAL
- en: 'These PIDs also can be found in code by using the `os` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Each process in your program should print a different PID for the line `os.getpid()`,
    but will print the same parent PID on `os.getppid()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Technically, a couple of lines of code are not needed for this particular program.
    If the ending `join` statement is not included:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: the parent process will still end and terminate the child processes with it
    automatically. However, this joining is needed if you wish to execute any code
    after these child processes complete.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'If the `join` statement is not included, the output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'If the `join` statement is included, the program waits for each process to
    finish before continuing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: If you want to stop program execution prematurely, you can of course use Ctrl-C
    to terminate the parent process. The termination of the parent process will also
    terminate any child processes that have been spawned, so using Ctrl-C is safe
    to do without worrying about accidentally leaving processes running in the background.
  prefs: []
  type: TYPE_NORMAL
- en: Multiprocess Crawling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The multithreaded Wikipedia crawling example can be modified to use separate
    processes rather than separate threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Again, you are artificially slowing the process of the scraper by including
    a `time.sleep(5)` so that this can be used for example purposes without placing
    an unreasonably high load on Wikipedia’s servers.
  prefs: []
  type: TYPE_NORMAL
- en: Here, you are replacing the user-defined `thread_name`, passed around as an
    argument, with `os.getpid()`, which does not need to be passed as an argument
    and can be accessed at any point.
  prefs: []
  type: TYPE_NORMAL
- en: 'This produces output like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Crawling in separate processes is, in theory, slightly faster than crawling
    in separate threads for two major reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Processes are not subject to locking by the GIL and can execute the same lines
    of code and modify the same (really, separate instantiations of the same) object
    at the same time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processes can run on multiple CPU cores, which may provide speed advantages
    if each of your processes or threads is processor intensive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, these advantages come with one major disadvantage. In the preceding
    program, all found URLs are stored in a global `visited` list. When you were using
    multiple threads, this list was shared among all threads; and one thread, in the
    absence of a rare race condition, could not visit a page that had already been
    visited by another thread. However, each process now gets its own independent
    version of the visited list and is free to visit pages that have already been
    visited by other processes.
  prefs: []
  type: TYPE_NORMAL
- en: Communicating Between Processes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Processes operate in their own independent memory, which can cause problems
    if you want them to share information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Modifying the previous example to print the current output of the visited list,
    you can see this principle in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in output like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'But there is a way to share information between processes on the same machine
    through two types of Python objects: queues and pipes.'
  prefs: []
  type: TYPE_NORMAL
- en: A *queue* is similar to the threading queue seen previously. Information can
    be put into it by one process and removed by another process. After this information
    has been removed, it’s gone from the queue. Because queues are designed as a method
    of “temporary data transmission,” they’re not well suited to hold a static reference
    such as a “list of web pages that have already been visited.”
  prefs: []
  type: TYPE_NORMAL
- en: 'But what if this static list of web pages was replaced with some sort of a
    scraping delegator? The scrapers could pop off a task from one queue in the form
    of a path to scrape (for example, */wiki/Monty_Python*) and in return, add a list
    of “found URLs” back onto a separate queue that would be processed by the scraping
    delegator so that only new URLs were added to the first task queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Some structural differences exist between this scraper and the ones originally
    created. Rather than each process or thread following its own random walk from
    the starting point they were assigned, they work together to do a complete coverage
    crawl of the website. Each process can pull any “task” from the queue, not just
    links that they have found themselves.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see this in action, as process 97024 scrapes both *Monty Python* and
    *Philadelphia* (a Kevin Bacon movie):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Multiprocess Crawling—Another Approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All of the approaches discussed for multithreaded and multiprocess crawling
    assume that you require some sort of “parental guidance” over the child threads
    and processes. You can start them all at once, you can end them all at once, and
    you can send messages or share memory between them.
  prefs: []
  type: TYPE_NORMAL
- en: But what if your scraper is designed in such a way that no guidance or communication
    is required? There may be very little reason to start going crazy with `import
    _thread` just yet.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s say you want to crawl two similar websites in parallel.
    You have a crawler written that can crawl either of these websites, determined
    by a small configuration change or perhaps a command-line argument. There’s absolutely
    no reason you can’t simply do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: And voilà, you’ve just kicked off a multiprocess web crawler, while saving your
    CPU the overhead of keeping around a parent process to boot!
  prefs: []
  type: TYPE_NORMAL
- en: Of course, this approach has downsides. If you want to run two web crawlers
    on the *same* website in this way, you need some way of ensuring that they won’t
    accidentally start scraping the same pages. The solution might be to create a
    URL rule (“crawler 1 scrapes the blog pages, crawler 2 scrapes the product pages”)
    or divide the site in some way.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, you may be able to handle this coordination through some sort
    of intermediate database, such as [Redis](https://redis.io/). Before going to
    a new link, the crawler may make a request to the database to ask, “Has this page
    been crawled?” The crawler is using the database as an interprocess communication
    system. Of course, without careful consideration, this method may lead to race
    conditions or lag if the database connection is slow (likely only a problem if
    connecting to a remote database).
  prefs: []
  type: TYPE_NORMAL
- en: You may also find that this method isn’t quite as scalable. Using the `Process`
    module allows you to dynamically increase or decrease the number of processes
    crawling the site or even storing data. Kicking them off by hand requires either
    a person physically running the script or a separate managing script (whether
    a bash script, a cron job, or something else) doing this.
  prefs: []
  type: TYPE_NORMAL
- en: However, I have used this method with great success in the past. For small,
    one-off projects, it is a great way to get a lot of information quickly, especially
    across multiple websites.
  prefs: []
  type: TYPE_NORMAL
