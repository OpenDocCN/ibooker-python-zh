- en: Chapter 8\. Online Inference with Ray Serve
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 8 章。使用 Ray Serve 进行在线推断
- en: Edward Oakes
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Edward Oakes
- en: Online Inference
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在线推断
- en: 'In previous chapters you’ve learned how to use Ray to process data, train machine
    learning (ML) models, and apply them in a batch inference setting. However, many
    of the most exciting use cases for machine learning involve “online inference:”
    using ML models to enhance API endpoints that users interact with directly or
    indirectly. Online inference is important in situations where latency matters:
    you can’t simply apply models to data behind the scenes and serve the results.
    There are many real-world examples of use cases where online inference can provide
    a lot of value:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，您已经学习了如何使用 Ray 处理数据、训练机器学习（ML）模型，并在批量推断设置中应用它们。然而，许多最令人兴奋的机器学习用例涉及“在线推断”：即使用
    ML 模型增强用户直接或间接交互的 API 端点。在线推断在延迟至关重要的情况下非常重要：您不能简单地将模型应用于后台数据并提供结果。有许多现实世界的用例示例，其中在线推断可以提供很大的价值：
- en: '**Recommendation systems**: Providing recommendations for products (e.g., online
    shopping) or content (e.g., social media) is a bread-and-butter use case for machine
    learning. While it’s possible to do this in an offline manner, recommendation
    systems often benefit from reacting to users’ preferences in real time. This requires
    performing online inference using recent behavior as a key feature.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**推荐系统**：为产品（例如在线购物）或内容（例如社交媒体）提供推荐是机器学习的基础用例。虽然可以以离线方式执行此操作，但推荐系统通常受益于实时反应用户偏好。这要求使用最近的行为作为关键特征进行在线推断。'
- en: '**Chat bots**: Online services often have real-time chat windows to provide
    support to customers from the comfort of their keyboard. Traditionally, these
    chat windows were staffed by customer support staff but there has been a recent
    trend to reduce labor costs and improve time-to-resolution by replacing them with
    ML-powered chat bots that can be online 24/7\. These chat bots require a sophisticaed
    mix of multiple machine learning techniques and must be able to respond to customer
    input in real time.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '**聊天机器人**：在线服务通常具有实时聊天窗口，以便从键盘舒适地为客户提供支持。传统上，这些聊天窗口由客户支持人员负责，但最近的趋势是通过使用全天候在线的
    ML 动力聊天机器人来减少劳动成本并改善解决时间。这些聊天机器人需要多种机器学习技术的复杂混合，并且必须能够实时响应客户输入。'
- en: '**Estimating arrival times**: Ride sharing, navigation, and food delivery services
    all rely on being able to provide an accurate estimation for arrival times (e.g.,
    for your driver, yourself, or your dinner). Providing accurate estimates is very
    difficult because it requires accounting for real-world factors such as traffic
    patterns, weather, and accidents. Estimates are also often refreshed many times
    over the course of one trip.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**估计到达时间**：乘车共享、导航和食品配送服务都依赖于能够提供准确的到达时间估计（例如，为您的司机、您自己或您的晚餐）。提供准确的估算非常困难，因为它需要考虑交通模式、天气和事故等真实世界因素。估计通常在一次行程中多次刷新。'
- en: 'These are just a few examples where applying machine learning in an online
    fashion can provide a lot of value in application domains that are traditionally
    very difficult (imagine hand-writing logic to estimate arrival time!). The list
    of applications goes on: a number of nascent domains such as self-driving cars,
    robotics, video-processing pipelines are also being redefined by machine learning.
    All of these applications share one key feature: latency is crucial. In the case
    of online services, low latency is paramount to providing a good user experience
    and for applications interacting with the real world such as robotics or self-driving
    cars, higher latency can have even stronger implications in safety or correctness.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是一些例子，应用机器学习以在线方式在传统上非常困难的应用领域中提供大量价值。应用领域的应用清单仍在继续扩展：一些新兴领域如自动驾驶汽车、机器人技术、视频处理管道也正在被机器学习重新定义。所有这些应用都共享一个关键特征：延迟至关重要。在在线服务的情况下，低延迟对提供良好的用户体验至关重要，对于涉及到实际世界的应用，如机器人技术或自动驾驶汽车，更高的延迟可能会在安全性或正确性方面产生更强的影响。
- en: This chapter will provide a gentle introduction to Ray Serve, a Ray-native library
    that enables building online inference applications on top of Ray. First, we will
    discuss the challenges of online inference that Ray Serve addresses. Then, we’ll
    cover the architecture of Ray Serve and introduce its core set of functionality.
    Finally, we will use Ray Serve to build an end-to-end online inference API consisting
    of multiple natural language processing models.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍Ray Serve，这是一个基于Ray的本地库，可以在Ray之上构建在线推理应用程序。首先，我们将讨论Ray Serve所解决的在线推理挑战。然后，我们将介绍Ray
    Serve的架构并介绍其核心功能集。最后，我们将使用Ray Serve构建一个由多个自然语言处理模型组成的端到端在线推理API。
- en: Challenges of online inference
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在线推理的挑战
- en: 'In the previous section we discussed that the main goal of online inference
    is to interact with machine learning models with low latency. However, this has
    long been a key requirement for API backends and web servers, so a natural question
    is: what’s different about serving machine learning models?'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们讨论了在线推理的主要目标是与低延迟交互的机器学习模型。然而，这长期以来一直是API后端和Web服务器的关键要求，因此一个自然的问题是：在为机器学习模型提供服务方面有何不同？
- en: 1\. ML models are compute intensive
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 机器学习模型具有计算密集性
- en: 'Many of the challenges in online inference follow from one key characteristic:
    machine learning models are very compute intensive. Compared to traditional web
    serving where requests are primarily handled by I/O intensive database queries
    or other API calls, most machine learning models boil down to performing many
    linear algebra computations, be it to provide a recommendation, estimate an arrival
    time, or detect an object in an image. This is especially true for the recent
    trend of “deep learning,” where the number of weights and computations a single
    model performs is growing larger and larger over time. Often, deep learning models
    can also benefit significantly from using specialized hardware such as GPUs or
    TPUs, which have special-purpose instructions optimized for machine learning computations
    and enable vectorized computations across multiple inputs in parallel.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 许多在线推理的挑战都源于一个关键特征：机器学习模型非常计算密集。与传统的Web服务不同，传统的Web服务主要处理I/O密集型数据库查询或其他API调用，大多数机器学习模型归结为执行许多线性代数计算，无论是提供推荐、估计到达时间还是检测图像中的对象。对于最近的“深度学习”趋势尤其如此，单个模型执行的权重和计算数量随时间增长而增加。深度学习模型通常也可以从使用专用硬件（如GPU或TPU）中获得显著好处，这些硬件具有为机器学习计算优化的专用指令，并且可以跨多个输入并行进行矢量化计算。
- en: 'Many online inference applications are required to be run 24/7 by nature. When
    combined with the fact that machine learning models are compute intensive, operating
    online inference services can be very expensive, requiring allocating many CPUs
    and GPUs at all times. The primary challenges of online inference boil down to
    serving models in a way that minimizes end-to-end latency while also reducing
    cost. There are a few key properties that online inference systems provide in
    order to satisfy these requirements: - Support for specialized hardware such as
    GPUs and TPUs. - The ability to scale up and down the resources used for a model
    in response to request load. - Support for request batching to take advantage
    of vectorized computations.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 许多在线推理应用程序通常需要24/7运行。与机器学习模型计算密集性结合在一起时，运行在线推理服务可能非常昂贵，需要始终分配大量的CPU和GPU。在线推理的主要挑战在于以最小化端到端延迟和降低成本的方式提供模型服务。在线推理系统提供了一些关键特性以满足这些要求：-
    支持诸如GPU和TPU之类的专用硬件。- 能够根据请求负载调整模型使用的资源的能力。- 支持请求批处理以利用矢量化计算。
- en: 2\. ML models aren’t useful in isolation
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 机器学习模型在孤立状态下并不实用
- en: 'Often when machine learning is discussed in the academic or research setting,
    the focus is on an individual, isolated task such as object recognition or classification.
    However, in the real world applications are not usually so clear cut and well-defined.
    Instead, a combination of multiple ML models and business logic is required to
    solve a problem end-to-end. For example, consider a product recommendation use
    case. While there are a multitude of known ML techniques we could apply to the
    core problem of making a recommendation, there are also a lot of equally important
    challenges around the edges, many of which will be specific to each use case:
    - Validating inputs and outputs to ensure the result returned to the user makes
    sense semantically. Often, we may have some manual rules such as avoiding returning
    the same recommendation to a user multiple times in succession. - Fetching up-to-date
    information about the user and available products and converting it into features
    for the model (in some cases, this may be performed by an online feature store).
    - Combining the results of multiple models using manual rules such as filtering
    to the top results or selecting the model with highest confidence.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当讨论机器学习时，通常在学术或研究环境中，重点放在个体、孤立的任务上，比如对象识别或分类。然而，在真实世界的应用中，问题通常不是那么明确和定义明确的。相反，解决问题端到端需要结合多个机器学习模型和业务逻辑。例如，考虑一个产品推荐的使用案例。虽然我们可以应用多种已知的机器学习技术来解决推荐的核心问题，但在边缘处也存在许多同样重要的挑战，其中许多挑战对每个使用案例可能是特定的：
- en: 'Implementing an online inference API requires the ability to integrate all
    of these pieces together into one unified service. Therefore, it’s important to
    have the flexibility to compose multiple models together along with custom business
    logic. These pieces can’t really be viewed completely in isolation: the “glue”
    logic often needs to evolve alongside the models themselves.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 实施在线推理 API 需要能够将所有这些部分整合到一个统一的服务中。因此，有能力将多个模型与自定义业务逻辑组合在一起非常重要。这些部分实际上不能完全孤立地看待：
    "胶水" 逻辑通常需要随着模型本身的演变而演变。
- en: An Introduction to Ray Serve
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 Ray Serve
- en: 'Ray Serve is a scalable compute layer for serving machine learning models on
    top of Ray. Serve is framework-agnostic, meaning that it isn’t tied to a specific
    machine learning library, but rather treats models as ordinary Python code. Additionally,
    it allows you to flexibly combine normal Python business logic alongside machine
    learning models. This makes it possible to build online inference services completely
    end-to-end: a Serve application could validate user input, query a database, perform
    inference scalably across multiple ML models, and combine, filter, and validate
    the output all in the process of handling a single inference request. Indeed,
    combining the results of multiple machine learning models is one of the key strengths
    of Ray Serve, as you’ll see later in the chapter as we explore common multi-model
    serving patterns.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Ray Serve 是建立在 Ray 之上的可扩展计算层，用于为机器学习模型提供服务。Serve 是框架无关的，这意味着它不依赖于特定的机器学习库，而是将模型视为普通的
    Python 代码。此外，它允许您灵活地将普通的 Python 业务逻辑与机器学习模型结合在一起。这使得完全端到端地构建在线推理服务成为可能：Serve 应用程序可以验证用户输入，查询数据库，在多个机器学习模型之间可扩展地执行推理，并在处理单个推理请求的过程中组合、过滤和验证输出。事实上，结合多个机器学习模型的结果是
    Ray Serve 的关键优势之一，正如我们在后面章节中探讨的常见多模型服务模式所示。
- en: 'While being flexible in nature, Serve has purpose-built features for compute-heavy
    machine learning models, enabling dynamic scaling and resource allocation to ensure
    that request load can be handled efficiently across many CPUs and/or GPUs. Here,
    Serve inherits a lot of benefits from being built on top of Ray: it’s scalable
    to hundreds of machines, offers flexible scheduling policies, and offers low-overhead
    communication across processes using Ray’s core APIs.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Serve 虽然在性质上灵活，但为计算密集型的机器学习模型提供了专门构建的功能，可以实现动态扩展和资源分配，以确保可以有效地处理跨多个 CPU 和/或
    GPU 的请求负载。在这里，Serve 从 Ray 构建中继承了许多好处：它可扩展到数百台机器，提供灵活的调度策略，并使用 Ray 的核心 API 在进程间提供低开销的通信。
- en: 'This section will incrementally introduce core funcionality from Ray Serve
    with a focus on how it helps to address the challenges of online inference as
    outlined above. To follow along with the code samples in this section, you’ll
    need the following Python packages installed locally:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将逐步介绍Ray Serve的核心功能，重点是它如何帮助解决上述在线推理的挑战。要跟随本节中的代码示例，您需要在本地安装以下Python包：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Running the examples assumes that you have the code saved locally in a file
    named `app.py` in the current working directory.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 运行示例假设你在当前工作目录中已经保存了名为`app.py`的文件。
- en: Architectural overview
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 架构概述
- en: Ray Serve is built on top of Ray, so it inherits a lot of benefits such as scalability,
    low overhead communication, an API well-suited to parallelism, and the ability
    to leverage shared memory via the object store. The core primitive in Ray Serve
    is a **deployment**, which you can think of as a managed group of Ray actors that
    can be addressed together and will handle requests load-balanced across them.
    Each actor in a deployment is called a **replica** in Ray Serve. Often, a deployment
    will map one-to-one with a machine learning model, but deployments can contain
    arbitrary Python code so they might also house business logic.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Ray Serve建立在Ray之上，因此它继承了许多优点，比如可扩展性、低开销的通信、适合并行的API，以及通过对象存储利用共享内存的能力。Ray Serve中的核心原语是**部署**，你可以把它看作是一组受管理的Ray
    actor，可以一起使用并处理通过负载平衡分布在它们之间的请求。部署中的每个actor称为Ray Serve中的**副本**。通常，一个部署将一对一地映射到一个机器学习模型，但部署可以包含任意的Python代码，因此它们也可以包含业务逻辑。
- en: Ray Serve enables exposing deployments over HTTP and defining the input parsing
    and output logic. However, one of the most important features of Ray Serve is
    that deployments can also call into each other directly using a native Python
    API, which will translate to direct actor calls between the replicas. This enables
    flexible, high performance composition of models and business logic; you’ll see
    this in action later in the section.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Ray Serve使部署可以通过HTTP公开，并定义输入解析和输出逻辑。但Ray Serve最重要的功能之一是，部署也可以直接使用本机Python API相互调用，这将转换为副本之间的直接actor调用。这使得模型和业务逻辑的灵活高性能组合成为可能；您将在本节后面看到这一点的实际应用。
- en: Under the hood, the deployments making up a Ray Serve application are managed
    by a centralized **controller** actor. This is a detached actor that is managed
    by Ray and will be restarted upon failure. The controller is in charge of creating
    and updating replica actors, broadcasting updates to other actors in the system,
    and performing health checking and failure recovery. If a replica or an entire
    Ray node crashes for any reason, the controller will detect the failures and ensure
    that the actors are recovered and can continue serving traffic.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，组成Ray Serve应用程序的部署由一个集中的**控制器**actor管理。这是一个由Ray管理的独立actor，将在失败时重新启动。控制器负责创建和更新副本actor，向系统中的其他actor广播更新，并执行健康检查和故障恢复。如果由于任何原因副本或整个Ray节点崩溃，控制器将检测到失败，并确保actor被恢复并可以继续提供服务。
- en: Defining a basic HTTP endpoint
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义基本的HTTP端点
- en: 'This section will introduce Ray Serve by defining a simple HTTP endpoint wrapping
    a single ML model. The model we’ll deploy is a sentiment classifier: given a text
    input, it will predict if the output had a positive or negative sentiment. We’ll
    be using a pretrained sentiment classifier from the [Hugging Face](https://huggingface.co/)
    `transformers` library, which provides a simple Python API for pretrained models
    that will abstract away the details of the model and allow us to focus on the
    serving logic. To deploy this model using Ray Serve, we need to define a Python
    class and turn it into a Serve **deployment** using the `@serve.deployment` decorator.
    The decorator allows us to pass a number of useful options to configure the deployment;
    we will explore some of those options later in the chapter.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将通过定义一个简单的HTTP端点来介绍Ray Serve。我们将部署的模型是一个情感分类器：给定一个文本输入，它将预测输出是否具有积极或消极的情感。我们将使用[Hugging
    Face](https://huggingface.co/) `transformers`库中的预训练情感分类器，该库提供了一个简单的Python API，用于预训练模型，它将隐藏模型的细节，使我们可以专注于服务逻辑。要使用Ray
    Serve部署此模型，我们需要定义一个Python类，并使用`@serve.deployment`装饰器将其转换为Serve的**部署**。该装饰器允许我们传递多个有用的选项来配置部署；我们将在本章后面探讨其中的一些选项。
- en: Example 8-1\.
  id: totrans-29
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 8-1。
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'There are a few important points to note here. First, we instantiate our model
    in the constructor of the class. This model may be very large, so downloading
    it and loading it into memory can be slow (10s of seconds or longer). In Ray Serve,
    the code in the constructor will only be run once in each replica on startup and
    any properties can be cached for future use. Second, we define the logic to handle
    a request in the `__call__` method. This takes a `Starlette` HTTP request as input
    and can return any JSON-serializable output. In this case, we’ll return a single
    string from the output of our model: `"POSITIVE"` or `"NEGATIVE"`.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有几个重要的要点需要注意。首先，在类的构造函数中实例化我们的模型。这个模型可能非常庞大，因此下载和加载到内存中可能会很慢（可能需要 10 秒或更长时间）。在
    Ray Serve 中，构造函数中的代码将仅在每个副本在启动时运行一次，并且任何属性都可以被缓存以供将来使用。其次，我们在 `__call__` 方法中定义处理请求的逻辑。这个方法以
    `Starlette` 的 HTTP 请求作为输入，并可以返回任何可 JSON 序列化的输出。在本例中，我们将从模型的输出中返回一个字符串："POSITIVE"
    或 "NEGATIVE"。
- en: Once a deployment is defined, we use the `.bind()` API to instantiate a copy
    of it. This is where we can pass optional arguments to the constructor to configure
    the deployment (such as a remote path to download model weights from). Note that
    this doesn’t actually run the deployment, but just packages it up with its arguments
    (this will be more important later when we combine multiple models together).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦一个部署被定义，我们使用 `.bind()` API 来实例化它的一个副本。这是我们可以传递给构造函数的可选参数来配置部署（例如远程路径以下载模型权重）。请注意，这实际上并没有运行部署，而只是将其与其参数打包在一起（当我们将多个模型组合在一起时，这将变得更加重要）。
- en: Example 8-2\.
  id: totrans-33
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 8-2\.
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can run the bound deployment using the `serve.run` Python API or corresponding
    `serve run` CLI command. Assuming you save the above code in a file called `app.py`,
    you can run it locally with the following command:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `serve.run` Python API 或相应的 `serve run` CLI 命令来运行绑定的部署。假设你将以上代码保存在一个名为
    `app.py` 的文件中，你可以用以下命令在本地运行它：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This will instantiate a single replica of our deployment and host it behind
    a local HTTP server. To test it, we can use the Python `requests` package:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这将实例化我们部署的单个副本，并将其托管在本地 HTTP 服务器后面。要测试它，我们可以使用 Python 的 `requests` 包：
- en: Example 8-3\.
  id: totrans-38
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 8-3\.
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Testing the sentiment classifier on a sample input text of `"Hello friend!"`,
    it correctly classifies the text as positive!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在样本输入文本 `"Hello friend!"` 上测试情感分类器，它正确地将文本分类为正面！
- en: 'This example is effectively the “hello world” of Ray Serve: we deployed a single
    model behind a basic HTTP endpoint. Note, however, that we had to manually parse
    the input HTTP request and feed it into our model. For this basic example it was
    just a single line of code, but real world applications often take a more complex
    schema as input and hand-writing HTTP logic can be tedious and error prone. To
    enable writing more expressive HTTP APIs, Serve integrates with the [FastAPI](https://fastapi.tiangolo.com/)
    Python framework.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例实际上是 Ray Serve 的 "hello world"：我们在一个基本的 HTTP 端点后部署了一个单一模型。然而，请注意，我们不得不手动解析输入的
    HTTP 请求并将其馈送到我们的模型中。对于这个基本示例来说，这只是一行代码，但现实世界的应用程序通常需要更复杂的输入模式，并且手动编写 HTTP 逻辑可能会很繁琐且容易出错。为了能够编写更具表现力的
    HTTP API，Serve 与 [FastAPI](https://fastapi.tiangolo.com/) Python 框架集成。
- en: A Serve deployment can wrap a `FastAPI` app, making use of its expressive APIs
    for parsing inputs and configuring HTTP behavior. In the following example, we
    rely on `FastAPI` to handle parsing the `input_text` query parameter for us, allowing
    to remove the boilerplate parsing code.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Serve 部署可以包装一个 `FastAPI` 应用程序，利用其表达性的 API 来解析输入并配置 HTTP 行为。在下面的示例中，我们依赖于 `FastAPI`
    来处理 `input_text` 查询参数，从而允许我们删除样板解析代码。
- en: Example 8-4\.
  id: totrans-43
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 8-4\.
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The modified deployment should have exactly the same behavior on the example
    above (try it out using `serve run`!), but will gracefully handle invalid inputs.
    These may look like small benefits for this simple example, but for more complex
    APIs this can make a world of difference. We won’t delve deeper into the details
    of FastAPI here, but for more details on its features and syntax check out their
    excellent [documentation](https://fastapi.tiangolo.com/).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 修改后的部署应该在上述示例中表现完全相同（试着使用 `serve run` 运行它！），但会优雅地处理无效输入。对于这个简单的示例来说，这些可能看起来像是小小的好处，但对于更复杂的
    API 来说，这可能产生天壤之别。我们在这里不会深入探讨 FastAPI 的细节，但如果想了解其功能和语法的更多细节，请查看他们出色的[文档](https://fastapi.tiangolo.com/)。
- en: Scaling and resource allocation
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展和资源分配
- en: 'As mentioned above, machine learning models are often compute hungry. Therefore,
    it’s important to be able to allocate the correct amount of resources to your
    ML application to handle request load while minimizing cost. Ray Serve allows
    you to adjust the resources dedicated to a deployment in two ways: by tuning the
    number of **replicas** of the deployment and tuning the resources allocated to
    each replica. By default, a deployment consists of a single replica that uses
    a single CPU but these parameters can be adjusted in the `@serve.deployment` decorator
    (or using the corresponding `deployment.options` API).'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，机器学习模型通常需要大量计算资源。因此，重要的是能够为您的 ML 应用程序分配正确数量的资源，以处理请求负载并最大程度地减少成本。Ray Serve
    允许您通过调整部署的资源以两种方式来调整：通过调整部署的**副本数**和调整分配给每个副本的资源。默认情况下，部署由一个使用单个 CPU 的副本组成，但这些参数可以在`@serve.deployment`装饰器中（或使用相应的`deployment.options`
    API）进行调整。
- en: Let’s modify the `SentimentClassifier` example from above to scale out to multiple
    replicas and adjust the resource allocation so that each replica uses 2 CPUs instead
    of 1 (in practice, you would want to profile and understand your model in order
    to set this paramter correctly). We’ll also add a print statement to log the process
    ID of the process handling each request to show that the requests are now load
    balanced across two replicas.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们修改上面的`SentimentClassifier`示例，以扩展到多个副本，并调整资源分配，使每个副本使用 2 个 CPU 而不是 1 个（在实践中，您将希望分析和了解您的模型以正确设置此参数）。我们还将添加一个打印语句来记录处理每个请求的进程
    ID，以显示请求现在跨两个副本进行负载平衡。
- en: Example 8-5\.
  id: totrans-49
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 8-5\.
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Running this new version of our classifier with `serve run app:scaled_deployment`
    and querying it using `requests` as we did above, you should see that there are
    now two copies of the model handling requests! We could easily scale up to tens
    or hundreds of replicas just by tweaking `num_replicas` in the same way: Ray enables
    scaling to hundreds of machines and thousands of processes in a single cluster.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 运行我们分类器的新版本时，使用`serve run app:scaled_deployment`，并像上面一样使用`requests`进行查询，您会看到现在有两个模型副本处理请求！我们可以通过简单调整`num_replicas`来轻松地扩展到数十个或数百个副本：Ray
    可以在单个集群中扩展到数百台机器和数千个进程。
- en: 'In this example we scaled out to a static number of replicas with each replica
    consuming two full CPUs, but Serve also supports more expressive resource allocation
    policies: - Enabling a deployment to use GPUs simpliy requires setting `num_gpus`
    instead of `num_cpus`. Serve supports the same resource types as Ray core, so
    deployments can also use TPUs or other custom resources. - Resources can be **fractional**,
    allowing replicas to be efficiently bin-packed. For example, if a single replica
    doesn’t saturate a full GPU you can allocate `num_gpus=0.5` to it and multiplex
    with another model. - For applications with varying request load, a deployment
    can be configured to dynamically autoscale the number of replicas based on the
    number of requests currently in flight.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们扩展到了一个静态副本数量，每个副本使用两个完整的 CPU，但 Serve 还支持更具表现力的资源分配策略： - 启用部署使用 GPU
    只需设置`num_gpus`而不是`num_cpus`。Serve 支持与 Ray 核心相同的资源类型，因此部署也可以使用 TPU 或其他自定义资源。 -
    资源可以是**分数**的，允许副本被高效地装箱。例如，如果单个副本不饱和一个完整的 GPU，则可以为其分配`num_gpus=0.5`，并与另一个模型进行复用。
    - 对于请求负载变化的应用程序，可以配置部署根据当前正在进行的请求数量动态调整副本的数量。
- en: For more details about resource allocation options, refer to the latest Ray
    Serve documentation.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 有关资源分配选项的更多详细信息，请参阅最新的 Ray Serve 文档。
- en: Request batching
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 请求批处理
- en: 'Many machine learning models can be efficiently **vectorized**, meaning that
    multiple computations can be run in parallel more efficiently than running them
    sequentially. This is especially beneficial when running models on GPUs which
    are purpose-built for efficiently performing many computations in parallel. In
    the context of online inference this offers a path for optimization: serving multiple
    requests (possibly from different sources) in parallel can drastically improve
    the throughput of the system (and therefore save cost).'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习模型可以被有效地**向量化**，这意味着可以更有效地并行运行多个计算，而不是顺序运行它们。当在 GPU 上运行模型时，这尤其有益，因为 GPU
    专为高效地并行执行许多计算而构建。在在线推断的背景下，这为优化提供了一条路径：并行服务多个请求（可能来自不同的来源）可以显着提高系统的吞吐量（从而节省成本）。
- en: 'There are two high-level strategies to take advantage of request batching:
    **client-side** batching and **server-side** batching. In client-side batching,
    the server accepts multiple inputs in a single request and clients include logic
    to send them in batches instead of one at a time. This is mostly useful in situations
    where a single client is frequently sending many inference requests. Server-side
    batching, in contrast, enables the server to batch multiple requests without requiring
    any modification on the client. This can also be used to batch requests across
    multiple clients, which enables efficient batching even in situations with many
    clients that each sends relatively few requests.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 利用请求批处理的两种高级策略：**客户端**批处理和**服务器端**批处理。在客户端批处理中，服务器接受单个请求中的多个输入，并且客户端包含逻辑以批量发送而不是逐个发送。这在单个客户端频繁发送多个推理请求的情况下非常有用。相比之下，服务器端批处理使服务器能够批处理多个请求，而无需客户端进行任何修改。这也可用于跨多个客户端批处理请求，这使得即使在每个客户端每次发送相对较少请求的情况下，也能实现高效批处理。
- en: Ray Serve offers a built-in utility for server-side batching, the `@serve.batch`
    decorator, that requires just a few code changes. This batching support uses Python’s
    `asyncio` capabilities to enqueue multiple requests into a single function call.
    The function should take in a list of inputs and return the corresponding list
    of outputs.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Ray Serve 提供了一个内置的实用程序用于服务器端批处理，即`@serve.batch`装饰器，只需进行少量代码更改即可。此批处理支持使用 Python
    的`asyncio`能力将多个请求排队到单个函数调用中。该函数应接受一个输入列表并返回相应的输出列表。
- en: Once again, let’s revisit the sentiment classifier from earlier and this time
    modify it to perform server-side batching. The underlying Hugging Face `pipeline`
    supports vectorized inference, all we need to do is pass a list of inputs and
    it will return the corresponding list of outputs. We’ll split out the call to
    the classifier into a new method, `classify_batched`, that will take a list of
    input texts as input, perform inference across them, and return the outputs in
    a formatted list. `classify_batched` will use the `@serve.batch` decorator to
    automatically perform batching. The behavior can be configured using the `max_batch_size`
    and `batch_timeout_wait_s` parameters, here we’ll set the max batch size to 10
    and wait for up to 100ms.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 再次回顾之前的情感分类器，并将其修改为执行服务器端批处理。底层的 Hugging Face `pipeline` 支持矢量化推理，我们只需传递一个输入文本列表，它将返回相应的输出列表。我们将调用分类器的部分拆分为一个新方法`classify_batched`，它将接受一个输入文本列表作为输入，在它们之间执行推理，并以格式化列表返回输出。`classify_batched`将使用`@serve.batch`装饰器自动执行批处理。可以使用`max_batch_size`和`batch_timeout_wait_s`参数配置行为，这里我们将最大批处理大小设置为
    10，并等待最多 100 毫秒。
- en: Example 8-6\.
  id: totrans-59
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 8-6。
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Notice that both the `classify` and `classify_batched` methods now use Python’s
    `async` and `await` syntax, meaning that many of these calls can run concurrently
    in the same process. To test this behavior, we’ll use the `serve.run` Python API
    to send requests using the Python-native handle to our deployment.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，现在`classify`和`classify_batched`方法都使用了 Python 的`async`和`await`语法，这意味着在同一个进程中可以并发运行许多这些调用。为了测试这种行为，我们将使用`serve.run`
    Python API使用本地 Python 句柄发送请求到我们的部署中。
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The handle returned by `serve.run` can be used to send multiple requests in
    parallel: here, we send 10 requests in parallel and wait for them all to return.
    Without batching, each request would be handled sequentially, but because we enabled
    batching we should see the requests handled all at once (evidenced by batch size
    printed in the `classify_batched` method). Running on a CPU, this might be marginally
    faster than running sequentially, but running the same handler on a GPU we would
    observe a significant speedup for the batched version.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 通过`serve.run`返回的句柄可用于并行发送多个请求：在这里，我们并行发送 10 个请求并等待它们全部返回。如果没有批处理，每个请求将按顺序处理，但因为我们启用了批处理，我们应该看到所有请求一次处理（在`classify_batched`方法中打印的批处理大小证明了这一点）。在
    CPU 上运行时，这可能比顺序运行稍快，但在 GPU 上运行相同处理程序时，批处理版本将显著加快速度。
- en: Multi-model inference graphs
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多模型推理图表。
- en: 'Up until now, we’ve been deploying and querying a single Serve deployment wrapping
    one ML model. As described earlier, machine learning models are not often useful
    in isolation: many applications require multiple models to be composed together
    and for business logic to be intertwined with machine learning. The real power
    of Ray Serve is in its ability to compose multiple models along with regular Python
    logic into a single application. This is possible by instantiating many different
    deployments and passing reference between them. Each of these deployments can
    use all of the features we’ve discussed up until this point: they can be independently
    scaled, perform request batching, and use flexible resource allocations.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在部署和查询一个 Serve 部署，包装一个 ML 模型。如前所述，单独使用机器学习模型通常是不够有用的：许多应用程序需要将多个模型组合在一起，并将业务逻辑与机器学习交织在一起。Ray
    Serve 的真正强大之处在于它能够将多个模型与常规 Python 逻辑组合成一个单一的应用程序。通过实例化许多不同的部署，并在它们之间传递引用，这是可能的。这些部署可以使用我们到目前为止讨论过的所有功能：它们可以独立缩放，执行请求批处理，并使用灵活的资源分配。
- en: This section provides illustrative examples of common multi-model serving patterns
    but doesn’t actually contain any ML models in order to focus on the core capabilities
    that Serve provides. Later in the chapter we will explore an end-to-end multi-model
    inference graph containing ML models.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了常见的多模型服务模式的示例，但实际上不包含任何 ML 模型，以便专注于 Serve 提供的核心功能。稍后在本章中，我们将探讨一个端到端的多模型推理图，其中包含
    ML 模型。
- en: 'Core feature: binding multiple deployments'
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 核心功能：绑定多个部署
- en: 'All types of multi-model inference graphs in Ray Serve center around the ability
    to pass a reference to one deployment into the constructor of another. In order
    to do this, we use another feature of the `.bind()` API: a bound deployment can
    be passed to another call to `.bind()` and this will resolve to a “handle” to
    the deployment at runtime. This enables deployments to be deployed and instantiated
    independently and then call each other at runtime. Below is the most basic example
    of a multi-deployment Serve application.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Ray Serve 中的所有类型的多模型推理图都围绕着将一个部署的引用传递给另一个的能力。为了做到这一点，我们使用 `.bind()` API 的另一个特性：一个绑定的部署可以传递给另一个
    `.bind()` 调用，并且这将在运行时解析为对部署的“句柄”。这使得部署可以独立部署和实例化，然后在运行时互相调用。以下是一个多部署 Serve 应用程序的最基本示例。
- en: Example 8-7\.
  id: totrans-69
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 8-7。
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In this example, the downstream model is passed into the “driver” deployment.
    Then at runtime the driver deployment calls into the downstream model. The driver
    could take any number of models passed in, and the downstream model could even
    take other downstream models of its own.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，下游模型被传递到“驱动器”部署中。然后在运行时，驱动器部署调用下游模型。驱动器可以接收任意数量的传入模型，并且下游模型甚至可以接收其自己的其他下游模型。
- en: 'Pattern 1: Pipelining'
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '模式 1: 管道化'
- en: The first common multi-model pattern among machine learning applications is
    “pipelining:” calling multiple models in sequence, where the input of one model
    depends on the output of the previous. Image processing, for example, often consists
    of a pipeline consistenting of multiple stages of transformations such as cropping,
    segmentation, and object recognition or optical character recognition (OCR). Each
    of these models may have different properties with some of them being lightweight
    transformations that can run on a CPU and others being heavyweight deep learning
    models that run on a CPU.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习应用程序中的第一个常见多模型模式是“管道化”：依次调用多个模型，其中一个模型的输入取决于前一个模型的输出。例如，图像处理通常由多个转换阶段的管道组成，如裁剪、分割、物体识别或光学字符识别（OCR）。每个模型可能具有不同的属性，其中一些是可以在
    CPU 上运行的轻量级转换，而其他一些是在 GPU 上运行的重型深度学习模型。
- en: Such pipelines can easily be expressed using Serve’s API. Each stage of the
    pipeline is defined as an independent deployment and each deployment is passed
    into a top-level “pipeline driver.” In the example below, we pass two deployments
    into a top-level driver and the driver calls them in sequence. Note that there
    could be many requests to the driver happening concurrently, therefore it is possible
    to efficiently saturate all stages of the pipeline.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的管道可以很容易地使用 Serve 的 API 表达。管道的每个阶段被定义为独立的部署，并且每个部署被传递到一个顶层的“管道驱动器”中。在下面的例子中，我们将两个部署传递到一个顶层驱动器中，驱动器按顺序调用它们。请注意，可能会有许多请求同时发送到驱动器，因此可以有效地饱和管道的所有阶段。
- en: Example 8-8\.
  id: totrans-75
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 8-8。
- en: '[PRE10]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'To test this example, you can once again use the `serve run` API. Sending a
    test request to the pipeline returns `"''input|val1|val2''"` as output: each downstream
    “model” appended its own value to construct the final result. In practice, each
    of these deployments could be wrapping its own ML model and a single request may
    flow across many physical nodes in a cluster.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 要测试此示例，可以再次使用`serve run` API。向管道发送测试请求将`"'input|val1|val2'"`作为输出返回：每个下游“模型”都添加了自己的值来构建最终结果。在实践中，每个部署可能会封装自己的ML模型，单个请求可能会在集群中的多个物理节点之间流动。
- en: 'Pattern 2: Broadcasting'
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模式 2：广播
- en: 'In addition to sequentially chaining models together, it’s often useful to
    perform inference on multiple models in parallel. This could be to perform “ensembling,”
    or combining the results of multiple independent models into a single result,
    or in a situation where different models may perform better on different inputs.
    Often the results of the models need to be combined in some way into a final result:
    either simply concatenated together or maybe a single result chosen from the lot.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 除了顺序链接模型之外，同时对多个模型进行推断通常也很有用。这可以是为了执行“集成学习”，即将多个独立模型的结果合并为一个结果，或者在不同输入上不同模型表现更好的情况下。通常需要将模型的结果以某种方式结合成最终结果：简单地连接在一起或者可能从中选择一个单一结果。
- en: 'This is expressed very similarly to the pipelining example: a number of downstream
    models are passed into a top-level “driver.” In this case, it’s important that
    we call the models in parallel: waiting for the result of each before calling
    the next would dramatically increase the overall latency of the system.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这与流水线示例非常相似：许多下游模型传递到顶级“驱动程序”。在这种情况下，重要的是我们并行调用模型：在调用下一个模型之前等待每个模型的结果将显著增加系统的总体延迟。
- en: Example 8-9\.
  id: totrans-81
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 8-9\.
- en: '[PRE11]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Testing this endpoint after running it once again with `serve run` returns `'["val1",
    "val2"]'`, the combined output of the two models called in parallel.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 测试此端点在再次运行`serve run`后返回`'["val1", "val2"]'`，这是并行调用两个模型的组合输出。
- en: 'Pattern 3: Conditional logic'
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模式 3：条件逻辑
- en: Finally, while many machine learning applications fit roughly into one of the
    above patterns, often having static control flow can be very limiting. Take, for
    instance, the example of building a service to extract license plate numbers from
    user-uploaded images. In this case, we’ll likely need to build an image processing
    pipeline as discussed above, but we also don’t simply want to feed any image into
    the pipeline blindly. If the user uploads something other than a car or with an
    image that is low quality, we likely want to short circuit, avoid calling into
    the heavy-weight and expensive pipeline, and provide a useful error message. Similarly,
    in a product recommendation use case we may want to select a downstream model
    based on user input or the result of an intermediate model. Each of these examples
    requires embedding custom logic alongside our ML models.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，尽管许多机器学习应用程序大致符合上述模式之一，但静态控制流往往会非常限制。例如，构建一个从用户上传的图像中提取车牌号码的服务。在这种情况下，我们可能需要构建如上讨论的图像处理流水线，但我们也不只是盲目地将任何图像输入到流水线中。如果用户上传的是除了汽车或质量低下的图像，我们可能希望进行短路，避免调用重量级和昂贵的流水线，并提供有用的错误消息。类似地，在产品推荐用例中，我们可能希望根据用户输入或中间模型的结果选择下游模型。每个示例都需要在我们的ML模型旁嵌入自定义逻辑。
- en: We can accomplish this trivially using Serve’s multi-model API because our computation
    graph is defined as ordinary Python logic rather than as a statically-defined
    graph. For instance, in the example below, we use a simple random number generator
    (RNG) to decide which of two downstream models to call into. In a real-world example,
    the RNG could be replaced with business logic, a database query, or the result
    of an intermediate model.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以轻松地使用Serve的多模型API来实现这一点，因为我们的计算图是作为普通Python逻辑而不是作为静态定义的图形来定义的。例如，在下面的示例中，我们使用一个简单的随机数生成器（RNG）来决定调用哪个下游模型。在实际示例中，RNG可以替换为业务逻辑、数据库查询或中间模型的结果。
- en: Example 8-10\.
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 8-10\.
- en: '[PRE12]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Each call to this endpoint returns either `"val1"` or `"val2"` with 50/50 probability.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对此端点的每次调用都以50/50的概率返回`"val1"`或`"val2"`。
- en: Deploying on Kubernetes
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Kubernetes上部署
- en: 'TODO: the Kubernetes deployment story for Ray Serve is currently being reworked,
    will update this section once it’s finalized.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: TODO：Ray Serve的Kubernetes部署故事目前正在重新制定，完成后将更新本节内容。
- en: 'End-to-end Example: Building an NLP-powered API'
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 端到端示例：构建基于 NLP 的 API。
- en: In this section, we’ll use Ray Serve to build an end-to-end natural language
    processing (NLP) pipeline hosted for online inference. Our goal will be to provide
    a Wikipedia summarization endpoint that will leverage multiple NLP models and
    some custom logic to provide a succinct summary of the most relevant Wikipedia
    page for a given search term.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将使用 Ray Serve 构建一个端到端的自然语言处理（NLP）流水线，用于在线推断。我们的目标是提供一个维基百科摘要端点，利用多个
    NLP 模型和一些自定义逻辑来提供给定搜索项最相关维基百科页面的简明摘要。
- en: 'This task will bring together many of the concepts and features discussed above:
    - We’ll be combining custom business logic along with multiple machine learning
    models. - The inference graph will consist of all three multi-model patterns discussed
    above: pipelining, broadcasting, and conditional logic. - Each model will be hosted
    as a separate Serve deployment, so they can be independently scaled and given
    their own resource allocation. - One of the models will leverage vectorized computation
    via batching. - The API will be defined using Ray Serve’s `FastAPI` for input
    parsing and defining our output schema.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 此任务将汇集上述讨论的许多概念和特性：
- en: 'Our online inference pipeline will be structured as follows: - The user will
    provide a keyword search term. - We’ll fetch the content for the most relevant
    Wikipedia article for the search term. - A sentiment analysis model will be applied
    to the article. Anything with a “negative” sentiment will be rejected and we’ll
    return early. - The article content will be broadcast to summarizer and named
    entity recognition models. - We’ll return a composed result based on the summarizer
    and named entity recognition outputs.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的在线推断流水线将被结构化如下：
- en: This pipeline will be exposed over HTTP and return the results in a structured
    format. By the end of this section, we’ll have the pipeline running end-to-end
    locally and ready to scale up on a cluster. Let’s get started!
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这个流水线将通过 HTTP 公开，并以结构化格式返回结果。通过本节结束时，我们将在本地运行端到端流水线，并准备在集群上进行扩展。让我们开始吧！
- en: 'Step 0: Install dependencies'
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 0：安装依赖项。
- en: Before we dive into the code, you’ll need the following Python packages installed
    locally in order to follow along:.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入代码之前，您需要在本地安装以下 Python 包，以便跟随操作。
- en: '[PRE13]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Additionally, in this section we’ll assume that all of the code samples are
    available locally in a file called `app.py` so we can run the deployments using
    `serve run` from the same directory.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，在本节中，我们假设所有代码示例都在名为`app.py`的文件中本地可用，以便我们可以从相同目录使用`serve run`运行部署。
- en: 'Step 1: Fetching content & preprocessing'
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 1：获取内容与预处理。
- en: The first step is to fetch the most relevant page from Wikipedia given a user-provided
    search term. For this, we will leverage the `wikipedia` package on PyPI to do
    the heavy lifting. We’ll first search for the term, then select the top result
    and return its page content. If no results are found, we’ll return `None` — this
    edge case will be handled later when we define the API.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是根据用户提供的搜索项从维基百科获取最相关的页面。为此，我们将利用 PyPI 上的`wikipedia`包来进行繁重的工作。我们首先搜索该术语，然后选择顶部结果并返回其页面内容。如果找不到结果，我们将返回`None`，这种边缘情况将在我们定义
    API 时处理。
- en: Example 8-11\.
  id: totrans-103
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 8-11。
- en: '[PRE14]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Step 2: NLP models'
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 2：NLP 模型。
- en: Next, we need to define the ML models that will do the heavy lifting of our
    API. As we did in the introduction section, we’ll be using the [Hugging Face](https://huggingface.co/)
    `transformers` library as it provides convenient APIs to pretrained state-of-the-art
    ML models, so we can focus on the serving logic.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要定义将在 API 中负责重型工作的 ML 模型。与介绍部分一样，我们将使用[Hugging Face](https://huggingface.co/)的`transformers`库，因为它提供了预训练的最先进
    ML 模型的便捷 API，这样我们可以专注于服务逻辑。
- en: The first model we’ll use is a sentiment classifier, the same one we used in
    the examples above. The deployment for this model will take advantage of vectorized
    computations using Serve’s batching API.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的第一个模型是情感分类器，与我们在上面示例中使用的相同。此模型的部署将利用 Serve 的批处理 API 进行向量化计算。
- en: Example 8-12\.
  id: totrans-108
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 8-12\.
- en: '[PRE15]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We’ll also use a text summarization model to provide a succinct summary for
    the selected article. This model takes an optional “max_length” argument to cap
    the length of the summary. Because we know this is the most computationally expensive
    of the models, we set `num_replicas=2` — that way, if we have many requests coming
    in at the same time, it can keep up with the throughput of the other models. In
    practice, we may need more replicas to keep up with the input load, but we could
    only know that from profiling and monitoring.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将使用文本摘要模型为所选文章提供简洁的摘要。此模型接受一个可选的“max_length”参数来限制摘要的长度。因为我们知道这是模型中计算成本最高的，所以我们设置
    `num_replicas=2` — 这样，如果我们同时有多个请求进来，它可以跟上其他模型的吞吐量。实际上，我们可能需要更多的副本来跟上输入负载，但这只能通过分析和监控来确定。
- en: Example 8-13\.
  id: totrans-111
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 8-13\.
- en: '[PRE16]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The final model in our pipeline will be a named entity recognition model: this
    will attempt extract named entities from the text. Each result will have a confidence
    score, so we can set a threshold to only accept results above a certain threshold.
    We may also want to cap the total number of entities returned. The request handler
    for this deployment calls the model, then uses some basic business logic to enforce
    the provided confidence threshold and limit on the number of entities.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们管道中的最终模型将是一个命名实体识别模型：它将尝试从文本中提取命名实体。每个结果都有一个置信度分数，因此我们可以设置一个阈值，只接受超过某个阈值的结果。我们还可能希望限制返回的实体总数。该部署的请求处理程序调用模型，然后使用一些基本的业务逻辑来执行所提供的置信度阈值和实体数量的限制。
- en: Example 8-14\.
  id: totrans-114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 8-14\.
- en: '[PRE17]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Step 3: HTTP handling and driver logic'
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第三步：HTTP 处理和驱动逻辑
- en: With the input preprocessing and ML models defined, we’re ready to define the
    HTTP API and driver logic. First, we define the schema of the response that we’ll
    return from the API using [Pydantic](https://pydantic-docs.helpmanual.io/). The
    response includes whether or not the request was successful and a status message
    in addition to our summary and named entities. This will allow us to return a
    helpful response in error conditions such as when no result is found or the sentiment
    analysis comes back as negative.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 随着输入预处理和 ML 模型的定义，我们准备定义 HTTP API 和驱动逻辑。首先，我们使用 [Pydantic](https://pydantic-docs.helpmanual.io/)
    定义了从 API 返回的响应模式的架构。响应包括请求是否成功以及状态消息，另外还有我们的摘要和命名实体。这将允许我们在错误条件下返回有用的响应，例如当未找到结果或情感分析结果为负面时。
- en: Example 8-15\.
  id: totrans-118
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 8-15\.
- en: '[PRE18]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Next, we need to define the actual control flow logic that will run in the driver
    deployment. The driver will not do any of the actual heavy lifting itself, but
    instead call into our three downstream model deployments and interpret their results.
    It will also house the FastAPI app definition, parsing the input and returning
    the correct `Response` model based on the results of the pipeline.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要定义实际的控制流逻辑，这些逻辑将在驱动程序部署中运行。驱动程序本身不会执行任何实际的重型工作，而是调用我们的三个下游模型部署并解释它们的结果。它还将承载
    FastAPI 应用程序定义，解析输入并根据管道的结果返回正确的`Response`模型。
- en: Example 8-16\.
  id: totrans-121
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 8-16\.
- en: '[PRE19]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'In the body of the main handler, we first fetch the page content using our
    `fetch_wikipedia_page` logic (if no result is found, an error is returned). Then,
    we call into the sentiment analysis model. If this returns negative, we terminate
    early and return an error to avoid calling the other expensive ML models.. Finally,
    we broadcast the article contents to both the summary and named entity recognition
    models in parallel. The results of the two models are stitched together into the
    final response and we return success. Remember that we may have many calls to
    this handler running concurrently: the calls to the downstream models don’t block
    the driver and it could coordinate calls to many replicas of the heavyweight models.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在主处理程序的主体中，我们首先使用我们的 `fetch_wikipedia_page` 逻辑获取页面内容（如果未找到结果，则返回错误）。然后，我们调用情感分析模型。如果返回负面结果，我们提前终止并返回错误，以避免调用其他昂贵的
    ML 模型。最后，我们并行广播文章内容到摘要和命名实体识别模型。两个模型的结果被拼接到最终响应中，我们返回成功。请记住，我们可能有许多对此处理程序的调用同时运行：对下游模型的调用不会阻塞驱动程序，并且它可以协调对重型模型的多个副本的调用。
- en: 'Step 4: Putting it all together'
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第四步：将所有内容整合在一起
- en: At this point, all of the core logic is defined. All that’s left is to bind
    the graph of deployments together and run it!
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 到这一步，所有核心逻辑都已经定义好了。现在剩下的就是将部署的图形绑定在一起并运行它！
- en: Example 8-17\.
  id: totrans-126
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 8-17\.
- en: '[PRE20]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'First, we need to instantiate each of the deployments with any relevant input
    arguments. For example, here we pass a threshold and limit for the entity recognition
    model. The most important piece is we pass a reference to each of the three models
    into the driver so it can coordinate the computation. Now that we’ve defined the
    full NLP pipeline, we can run it using `serve run`:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要为每个部署实例化，使用任何相关的输入参数。例如，在这里，我们为实体识别模型传递了阈值和限制。最重要的部分是我们将三个模型的引用传递给驱动程序，以便它可以协调计算。现在我们已经定义了完整的自然语言处理流水线，我们可以使用
    `serve run` 来运行它：
- en: '[PRE21]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This will deploy each of the four deployments locally and make the driver available
    at `http://localhost:8000/`. We can query the pipeline using the `requests` to
    see it in action. First, let’s try querying for an entry on Ray Serve.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在本地部署每个四个部署，并使驱动程序在 `http://localhost:8000/` 上可用。我们可以使用 `requests` 查询流水线，看看它如何工作。首先，让我们尝试查询
    Ray Serve 上的一个条目。
- en: Example 8-18\.
  id: totrans-131
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 8-18\.
- en: '[PRE22]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Unfortunately, this page doesn’t exist yet! The first chunk of validation business
    logic kicks in and returns a “No pages found” message. Let’s try finding looking
    for something more common:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这个页面还不存在！验证业务逻辑的第一块代码生效并返回了“未找到页面”的消息。让我们尝试寻找一些更常见的内容：
- en: Example 8-19\.
  id: totrans-134
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 8-19\.
- en: '[PRE23]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Maybe we were just interested in learning about history, but this article was
    a bit too negative for our sentiment classifier. Let’s try something more neutral
    this time — what about science?
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 也许我们只是对了解历史感兴趣，但是这篇文章对我们的情感分类器来说有点太消极了。这次让我们试试更中立一点的东西——科学怎么样？
- en: Example 8-20\.
  id: totrans-137
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 8-20\.
- en: '[PRE24]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This example successfully ran through the full pipeline: the API responded
    with a cogent summary of the article and a list of relevant named entities.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例成功地通过了完整的流水线：API 返回了文章的简明摘要和相关命名实体的列表。
- en: To recap, in this section we were able to build an online natural language processing
    API using Ray Serve. This inference graph consisted of multiple machine learning
    models in addition to custom business logic and dynamic control flow. Each model
    can be independently scaled and have its own resource allocation, and we can exploit
    vectorized computations using server-side batching. Now that we were able to test
    the API locally, the next step would be to deploy to production. Ray Serve makes
    it easy to deploy on Kubernetes or other cloud provider offerings using the Ray
    cluster launcher, and we could easily scale up to handle many users by tweaking
    the resource allocations for our deployments.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，在这一部分，我们能够使用 Ray Serve 构建一个在线自然语言处理 API。这个推理图包含了多个机器学习模型以及自定义业务逻辑和动态控制流。每个模型可以独立扩展并拥有自己的资源分配，我们可以利用服务器端批处理来进行向量化计算。现在我们已经能够在本地测试
    API，下一步将是部署到生产环境。Ray Serve 通过使用 Ray 集群启动器，可以轻松部署到 Kubernetes 或其他云提供商提供的服务，并且通过调整部署的资源分配，可以轻松扩展以处理多个用户。
- en: Summary
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: About the Author
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于作者
- en: '**Max Pumperla** is a data science professor and software engineer located
    in Hamburg, Germany. He’s an active open source contributor, maintainer of several
    Python packages, author of machine learning books and speaker at international
    conferences. As head of product research at Pathmind Inc. he’s developing reinforcement
    learning solutions for industrial applications at scale using Ray. Pathmind works
    closely with the AnyScale team and is a power user of Ray’s RLlib, Tune and Serve
    libraries. Max has been a core developer of DL4J at Skymind, helped grow and extend
    the Keras ecosystem, and is a Hyperopt maintainer.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**马克斯·普姆佩拉** 是一位数据科学教授和软件工程师，位于德国汉堡。他是一位活跃的开源贡献者，维护了几个Python包，撰写了机器学习书籍，并在国际会议上发表演讲。作为
    Pathmind 公司产品研究负责人，他正在使用 Ray 开发规模化的工业应用强化学习解决方案。Pathmind 与 AnyScale 团队密切合作，并且是
    Ray 的 RLlib、Tune 和 Serve 库的高级用户。马克斯曾是 Skymind 公司的 DL4J 核心开发者，帮助扩展和发展 Keras 生态系统，并担任
    Hyperopt 维护者。'
- en: '**Edward Oakes**'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '**爱德华·奥克斯**'
- en: '**Richard Liaw**'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '**理查德·李奥**'
