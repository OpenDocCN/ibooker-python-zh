<html><head></head><body><section data-pdf-bookmark="Chapter 2. The Legalities and Ethics of Web Scraping" data-type="chapter" epub:type="chapter"><div class="chapter" id="c-2">&#13;
<h1><span class="label">Chapter 2. </span>The Legalities and Ethics of Web Scraping</h1>&#13;
&#13;
<p>In 2010, software engineer Pete Warden built a web crawler to gather data from Facebook. He collected data from approximately 200 million Facebook users—names, location information, friends, and interests. Of course, Facebook noticed and sent him cease and desist letters, which he obeyed. When asked why he complied with the cease and desist, he said: “Big data? Cheap. Lawyers? Not so cheap.”</p>&#13;
&#13;
<p>In this chapter, you’ll look at US laws (and some international ones) that are relevant to web scraping and learn how to analyze the legality and ethics of a given web scraping situation.</p>&#13;
&#13;
<p>Before you read the following section, consider the obvious: I am a software engineer, not a lawyer. Do not interpret anything you read here or in any other chapter of the book as professional legal advice or act on it accordingly. Although I believe I’m able to discuss the legalities and ethics of web scraping knowledgeably, you should consult a lawyer (not a software engineer) before undertaking any legally ambiguous web scraping projects.</p>&#13;
&#13;
<p>The goal of this chapter is to provide you with a framework for being able to understand and discuss various aspects of web scraping legalities, such as intellectual property, unauthorized computer access, and server usage, but this should not be a substitute for actual legal advice.</p>&#13;
&#13;
<section data-pdf-bookmark="Trademarks, Copyrights, Patents, Oh My!" data-type="sect1"><div class="sect1" id="id13">&#13;
<h1>Trademarks, Copyrights, Patents, Oh My!</h1>&#13;
&#13;
<p>It’s time for a crash course in intellectual property! There are three basic types of intellectual property: trademarks (indicated by a <span class="large_symbol">™</span> or <span class="large_symbol">®</span> symbol), copyrights (the ubiquitous ©), and patents (sometimes indicated by text noting that the invention is patent protected or a patent number but often by nothing at all).</p>&#13;
&#13;
<p>Patents are used to <a contenteditable="false" data-primary="intellectual property" data-secondary="patents" data-type="indexterm" id="id296"/><a contenteditable="false" data-primary="patents" data-type="indexterm" id="id297"/><a contenteditable="false" data-primary="legal issues" data-secondary="intellectual property" data-type="indexterm" id="lgstlppy"/>declare ownership over inventions only. You cannot patent images, text, or any information itself. Although some patents, such as software patents, are less tangible than what we think of as “inventions,” keep in mind that it is the <em>thing</em> (or technique) that is patented—not the data that comprises the software. Unless you are either building things from scraped diagrams, or someone patents a method of web scraping, you are unlikely to inadvertently infringe on a patent by scraping the web.</p>&#13;
&#13;
<p>Trademarks also are unlikely <a contenteditable="false" data-primary="intellectual property" data-secondary="trademarks" data-type="indexterm" id="id298"/><a contenteditable="false" data-primary="trademarks" data-type="indexterm" id="id299"/>to be an issue but still something that must be considered. According to the US Patent and Trademark Office:</p>&#13;
&#13;
<blockquote>&#13;
<p>A <em>trademark</em> is a word, phrase, symbol, and/or design that identifies and distinguishes the source of the goods of one party from those of others. A <em>service mark</em> is a word, phrase, symbol, and/or design that identifies and distinguishes the source of a service rather than goods. The term “trademark” is often used to refer to both trademarks and service marks.</p>&#13;
</blockquote>&#13;
&#13;
<p>In addition to the words and symbols that come to mind when you think of trademarks, other descriptive attributes can be trademarked. This includes, for example, the shape of a container (like Coca-Cola bottles) or even a color (most notably, the pink color of Owens Corning’s Pink Panther fiberglass insulation).</p>&#13;
&#13;
<p>Unlike with patents, the ownership of a trademark depends heavily on the context in which it is used. For example, if I wish to publish a blog post with an accompanying picture of the Coca-Cola logo, I could do that, as long as I wasn’t implying that my blog post was sponsored by, or published by, Coca-Cola. If I wanted to manufacture a new soft drink with the same Coca-Cola logo displayed on the packaging, that would clearly be a trademark infringement. Similarly, although I could package my new soft drink in Pink Panther pink, I could not use that same color to create a home insulation product.</p>&#13;
&#13;
<p>This brings us to the <a contenteditable="false" data-primary="intellectual property" data-secondary="fair use" data-type="indexterm" id="id300"/><a contenteditable="false" data-primary="fair use, intellectual property and" data-type="indexterm" id="id301"/>topic of “fair use,” which is often discussed in the context of copyright law but also applies to trademarks. Storing or displaying a trademark as a reference to the brand it represents is fine. Using a trademark in a way that might mislead the consumer is not. The concept of “fair use” does not apply to patents, however. For example, a patented invention in one industry cannot be applied to another industry without an agreement with the patent holder.</p>&#13;
&#13;
<section data-pdf-bookmark="Copyright Law" data-type="sect2"><div class="sect2" id="id14">&#13;
<h2>Copyright Law</h2>&#13;
&#13;
<p>Both trademarks and patents have <a contenteditable="false" data-primary="copyright law" data-type="indexterm" id="id302"/><a contenteditable="false" data-primary="legal issues" data-secondary="copyright law" data-type="indexterm" id="id303"/>something in common in that they have to be formally registered in order to be recognized. Contrary to popular belief, this is not true with copyrighted material. What makes images, text, music, etc., copyrighted? It’s not the All Rights Reserved warning at the bottom of the page or anything special about “published” versus “unpublished” material. Every piece of material you create is automatically subject to copyright law as soon as you bring it into existence.</p>&#13;
&#13;
<p>The Berne Convention for the Protection of Literary and Artistic Works, named after Berne, Switzerland, where <a contenteditable="false" data-primary="Berne Convention for the Protection of Literary and Artistic Works" data-type="indexterm" id="id304"/>it was first adopted in 1886, is the international standard for copyright. This convention says, in essence, that all member countries must recognize the copyright protection of the works of citizens of other member countries as if they were citizens of their own country. In practice, this means that, as a US citizen, you can be held accountable in the United States for <a contenteditable="false" data-primary="legal issues" data-secondary="intellectual property" data-startref="lgstlppy" data-type="indexterm" id="id305"/>violating the copyright of material written by someone in, say, France (and vice versa).</p>&#13;
&#13;
<div data-type="note" epub:type="note">&#13;
<h1>Copyright Registration</h1>&#13;
&#13;
<p>While it’s true that copyright <a contenteditable="false" data-primary="copyright law" data-type="indexterm" id="cprgw"/>protections apply automatically and do not require any sort of registration, it is also possible to formally register a copyright with the US government. This is often done for valuable creative works, such as major motion pictures, in order to make any litigation easier later on and create a strong paper trail about who owns the work. However, do not let the existence of this copyright registration confuse you—all creative works, unless specifically part of the public domain, are copyrighted!</p>&#13;
</div>&#13;
&#13;
<p>Obviously, copyright is more of a concern for web scrapers than trademarks or patents. If I scrape content from someone’s blog and publish it on my own blog, I could very well be opening myself up to a lawsuit. Fortunately, I have several layers of protection that might make my blog-scraping project defensible, depending on how it functions.</p>&#13;
&#13;
<p>First, copyright protection extends to creative works only. It does not cover statistics or facts. Fortunately, much of what web scrapers are after <em>are</em> statistics and facts.</p>&#13;
&#13;
<p>A web scraper that gathers poetry from around the web and displays that poetry on your own website might be violating copyright law; however, a web scraper that gathers information on the frequency of poetry postings over time is not. The poetry, in its raw form, is a creative work. The average word count of poems published on a website by month is factual data and not a creative work.</p>&#13;
&#13;
<p>Content that is posted verbatim (as opposed to aggregated/calculated content from raw scraped data) might not be violating copyright law if that data is prices, names of company executives, or some other factual piece of information.</p>&#13;
&#13;
<p>Even copyrighted content can be used directly, within <a contenteditable="false" data-primary="DMCA (Digital Millennium Copyright Act) of 1988" data-type="indexterm" id="id306"/>reason, under the Digital Millennium Copyright Act of 1988. The DMCA outlines some rules for the automated handling of copyrighted material. The DMCA is long, with many specific rules governing everything from ebooks to telephones. However, two main points may be of particular relevance to web scraping:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>Under the “safe harbor” protection, if you scrape material from a source that you are led to believe contains only copyright-free material, but a user has submitted copyright material to, you are protected as long as you removed the copyrighted material when notified.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>You cannot circumvent security measures (such as password protection) in order to gather content.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>In addition, the DMCA also acknowledges that fair use under 17 U.S. Code § 107 applies, and that take-down notices may not be issued according to the safe harbor protection if the use of the copyrighted material falls under fair use.</p>&#13;
&#13;
<p>In short, you should never directly publish copyrighted material without permission from the original author or copyright holder. If you are storing copyrighted material that you have free access to in your own nonpublic database for the purposes of analysis, that is fine. If you are publishing that database to your website for viewing or download, that is not fine. If you are analyzing that database and publishing statistics about word counts, a list of authors by prolificacy, or some other meta-analysis of the data, that is fine. If you are accompanying that meta-analysis with a few select quotes, or brief samples of data to make your point, that is likely also fine, but you might want to examine the fair-use clause in the US Code to make sure.</p>&#13;
&#13;
<section data-pdf-bookmark="Copyright and artificial intelligence" data-type="sect3"><div class="sect3" id="id15">&#13;
<h3>Copyright and artificial intelligence</h3>&#13;
&#13;
<p>Generative artificial intelligence, or AI <a contenteditable="false" data-primary="AI (artificial intelligence)" data-secondary="copyright law and" data-type="indexterm" id="aicyrg"/><a contenteditable="false" data-primary="artificial intelligence (AI)" data-see="AI (artificial intelligence)" data-type="indexterm" id="id307"/>programs that generate new “creative” works based on a corpus of existing creative works, present unique challenges for copyright law.</p>&#13;
&#13;
<p>If the output of the generative AI program resembles an existing work, there may be a copyright issue. Many cases have been used as precedent to guide what the word “resembles” means here, but, according to the Congressional Research Service:<sup><a data-type="noteref" href="ch02.html#id308" id="id308-marker">1</a></sup></p>&#13;
&#13;
<blockquote>&#13;
<p>The substantial similarity test is difficult to define and varies across U.S. courts. Courts have variously described the test as requiring, for example, that the works have “a substantially similar total concept and feel” or “overall look and feel” or that “the ordinary reasonable person would fail to differentiate between the two works.”</p>&#13;
</blockquote>&#13;
&#13;
<p>The problem with modern complex algorithms is that it can be  impossible to automatically determine if your AI has produced an exciting and novel mash-up or something more...directly derivative. The AI may have no way of labeling its output as “substantially similar” to a particular input, or even identifying which of the inputs it used to generate its creation at all! The first indication that anything is wrong at all may come in the form of a cease and desist letter or a court summons.</p>&#13;
&#13;
<p>Beyond the issues of copyright infringement over the output of generative AI, upcoming court cases are testing whether the training process itself might infringe on a copyright holder’s rights.</p>&#13;
&#13;
<p>To train these systems, it is almost always necessary to download, store, and reproduce the copyrighted work. While it might not seem like a big deal to download a copyrighted image or text, this isn’t much different from downloading a copyrighted movie—and you wouldn’t download a movie, would you?</p>&#13;
&#13;
<p>Some claim that this constitutes fair use, and they are not publishing or using the content in a way that would impact its market.</p>&#13;
&#13;
<p>As of this writing, OpenAI is arguing before the United States Patent and Trademark Office that its use of large volumes of copyrighted material constitutes fair use.<sup><a data-type="noteref" href="ch02.html#id309" id="id309-marker">2</a></sup> While this argument is primarily in the context of AI generative algorithms, I suspect that its outcome will be applicable to web scrapers <a contenteditable="false" data-primary="copyright law" data-startref="cprgw" data-type="indexterm" id="id310"/><a contenteditable="false" data-primary="AI (artificial intelligence)" data-secondary="copyright law and" data-startref="aicyrg" data-type="indexterm" id="id311"/>built for a variety of purposes.</p>&#13;
</div></section>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Trespass to Chattels" data-type="sect1"><div class="sect1" id="id16">&#13;
<h1>Trespass to Chattels</h1>&#13;
&#13;
<p><em>Trespass to chattels</em> is fundamentally <a contenteditable="false" data-primary="legal issues" data-secondary="trespass to chattels" data-type="indexterm" id="lgssctt"/><a contenteditable="false" data-primary="trespass to chattels" data-type="indexterm" id="tspscttl"/><a contenteditable="false" data-primary="chattels" data-type="indexterm" id="id312"/>different from what we think of as “trespassing laws” in that it applies not to real estate or land but to movable property, or <em>chattels</em> in legal parlance. It applies when your property is interfered with in some way that does not allow you to access or use it.</p>&#13;
&#13;
<p>In this era of cloud computing, it’s tempting not to think of web servers as real, tangible resources. However, not only do servers consist of expensive components, but they also need to be stored, monitored, cooled, cleaned, and supplied with vast amounts of electricity. By some estimates, 10% of global electricity usage is consumed by computers.<sup><a data-type="noteref" href="ch02.html#id313" id="id313-marker">3</a></sup> If a survey of your own electronics doesn’t convince you, consider Google’s vast server farms, all of which need to be connected to large power stations.</p>&#13;
&#13;
<p>Although servers are expensive resources, they’re interesting from a legal perspective in that webmasters generally <em>want</em> people to consume their resources (i.e., access their websites); they just don’t want them to consume their resources <em>too much.</em> Checking out a website via your browser is fine; launching a full-scale Distributed Denial of Service (DDOS) attack against it obviously is not.</p>&#13;
&#13;
<p>Three criteria need to be met for a web scraper to violate trespass to chattels:</p>&#13;
&#13;
<dl>&#13;
	<dt>Lack of consent</dt>&#13;
	<dd>Because web servers are open to everyone, they are generally “giving consent” to web scrapers as well. However, many websites’ Terms of Service agreements specifically prohibit the use of scrapers. In addition, any cease and desist notices delivered to you may revoke this consent.</dd>&#13;
	<dt>Actual harm</dt>&#13;
	<dd>Servers are costly. In addition to server costs, if your scrapers take a website down, or limit its ability to serve other users, this can add to the “harm” you cause.</dd>&#13;
	<dt>Intentionality</dt>&#13;
	<dd>If you’re writing the code, you know what it does! Arguing a lack of intention would likely not go well when defending your web scraper.</dd>&#13;
</dl>&#13;
&#13;
<p>You must meet all three of these criteria for trespass to chattels to apply. However, if you are violating a Terms of Service agreement, but not causing actual harm, don’t think that you’re immune from legal action. You might very well be violating copyright law, the DMCA, the Computer Fraud and Abuse Act (more on that later in this chapter), or one of the other myriad of laws that apply to web scrapers.</p>&#13;
&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id314">&#13;
<h1>Throttling Your Bots</h1>&#13;
&#13;
<p>Back in the olden days, web servers <a contenteditable="false" data-primary="bots" data-secondary="throttling" data-type="indexterm" id="id315"/>were far more powerful than personal computers. In fact, part of the definition of <em>server</em> was <em>big computer</em>. Now, the tables have turned somewhat. My personal computer, for instance, has a 3.5 GHz processor and 32 GB of RAM. An AWS medium instance, in contrast, has 4 GB of RAM and about 3 GHz of processing capacity.</p>&#13;
&#13;
<p>With a decent internet connection and a dedicated machine, even a single personal computer can place a heavy load on many websites, even crippling them or taking them down completely. Unless there’s a medical emergency and the only cure is aggregating all the data from Joe Schmo’s website in two seconds flat, there’s really no reason to hammer a site.</p>&#13;
&#13;
<p>A watched bot never completes. Sometimes it’s better to leave crawlers running overnight than in the middle of the afternoon or evening for a few reasons:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>If you have about 8 hours, even at the glacial pace of 2 seconds per page, you can crawl over 14,000 pages. When time is less of an issue, you’re not tempted to push the speed of your crawlers.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Assuming the target audience of the website is in your general location (adjust accordingly for remote target audiences), the website’s traffic load is probably far lower during the night, meaning that your crawling will not be compounding peak traffic hour congestion.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>You save time by sleeping, instead of constantly checking your logs for new information. Think of how excited you’ll be to wake up in the morning to brand-new data!</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>Consider the following scenarios:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>You have a web crawler that traverses Joe Schmo’s website, aggregating some or all of its data.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>You have a web crawler that traverses hundreds of small websites, aggregating some or all of their data.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>You have a web crawler that traverses a very large site, such as Wikipedia.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>In the first scenario, it’s best to leave the bot running slowly and during the night.</p>&#13;
&#13;
<p>In the second scenario, it’s best to crawl each website in a round-robin fashion, rather than crawling them slowly, one at a time. Depending on how many websites you’re crawling, this means that you can collect data as fast as your internet connection and machine can manage, yet the load is reasonable for each individual remote server. You can accomplish this programmatically, either using multiple threads (where each individual thread crawls a single site and pauses its own execution), or using Python lists to keep track of sites.</p>&#13;
&#13;
<p>In the third scenario, the load your internet connection and home machine can place on a site like Wikipedia is unlikely to be noticed or cared much about. However, if you’re using a distributed network of machines, this is obviously a different matter. Use caution, and <a contenteditable="false" data-primary="legal issues" data-secondary="trespass to chattels" data-startref="lgssctt" data-type="indexterm" id="id316"/><a contenteditable="false" data-primary="trespass to chattels" data-startref="tspscttl" data-type="indexterm" id="id317"/>ask a company representative whenever possible.</p>&#13;
</div></aside>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="The Computer Fraud and Abuse Act" data-type="sect1"><div class="sect1" id="id17">&#13;
<h1>The Computer Fraud and Abuse Act</h1>&#13;
&#13;
<p>In the early 1980s, computers started <a contenteditable="false" data-primary="legal issues" data-secondary="CFAA (Computer Fraud and Abuse  Act)" data-type="indexterm" id="lglcfa"/><a contenteditable="false" data-primary="CFAA (Computer Fraud and Abuse Act)" data-type="indexterm" id="cfcfb"/><a contenteditable="false" data-primary="Computer Fraud and Abuse Act (CFAA)" data-type="indexterm" id="cpfbfa"/>moving out of academia and into the business world. For the first time, viruses and worms were seen as more than an inconvenience (or even a fun hobby) and as a serious criminal matter that could cause monetary damages. In 1983, <a contenteditable="false" data-primary="War Games movie" data-type="indexterm" id="id318"/>the movie <em>War Games</em>, starring Matthew Broderick, also brought this issue to the public eye and to the eye of President Ronald Reagan.<sup><a data-type="noteref" href="ch02.html#id319" id="id319-marker">4</a></sup> In response, the Computer Fraud and Abuse Act (CFAA) was created in 1986.</p>&#13;
&#13;
<p>Although you might think that the CFAA applies to only a stereotypical version of a malicious hacker unleashing viruses, the act has strong implications for web scrapers as well. Imagine a scraper that scans the web looking for login forms with easy-to-guess passwords, or collects government secrets accidentally left in a hidden but public location. All of these activities are illegal (and rightly so) under the CFAA.</p>&#13;
&#13;
<p>The act defines seven main criminal offenses, which can be summarized as follows:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>The knowing unauthorized access of computers owned by the US government and obtaining information from those computers.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>The knowing unauthorized access of a computer, obtaining financial information.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>The knowing unauthorized access of a computer owned by the US government, affecting the use of that computer by the government.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Knowingly accessing any protected computer with the attempt to defraud.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Knowingly accessing a computer without authorization and causing damage to that computer.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Sharing or trafficking passwords or authorization information for computers used by the US government or computers that affect interstate or foreign <span class="keep-together">commerce.</span></p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Attempts to extort money or “anything of value” by causing damage, or threatening to cause damage, to any protected computer.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>In short: stay away from protected computers, do not access computers (including web servers) that you are not <a contenteditable="false" data-primary="legal issues" data-secondary="CFAA (Computer Fraud and Abuse  Act)" data-startref="lglcfa" data-type="indexterm" id="id320"/><a contenteditable="false" data-primary="CFAA (Computer Fraud and Abuse Act)" data-startref="cfcfb" data-type="indexterm" id="id321"/><a contenteditable="false" data-primary="Computer Fraud and Abuse Act (CFAA)" data-startref="cpfbfa" data-type="indexterm" id="id322"/>given access to, and especially, stay away from government or financial computers.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="robots.txt and Terms of Service" data-type="sect1"><div class="sect1" id="id18">&#13;
<h1>robots.txt and Terms of Service</h1>&#13;
&#13;
<p class="p1">A website’s terms of service and <em>robots.txt</em> files are in interesting territory, legally speaking. If <a contenteditable="false" data-primary="TOS (Terms of Service)" data-type="indexterm" id="tostos"/><a contenteditable="false" data-primary="robots.txt files" data-type="indexterm" id="rbtxtf"/>a website is publicly accessible, the webmaster’s right to declare what software can and cannot access it is debatable. Saying that “it is fine if you use your browser to view this site, but not if you use a program you wrote to view it” is tricky.</p>&#13;
&#13;
<p class="p1">Most sites have a link to their Terms of Service (TOS) in the footer on every page. The TOS contains more than just the rules for web crawlers and automated access; it often has information about what kind of information the website collects, what it does with it, and usually a legal disclaimer that the services provided by the website come without any express or implied warranty.</p>&#13;
&#13;
<p class="p1">If you are interested in <a contenteditable="false" data-primary="SEO (search engine optimization)" data-secondary="robots.txt file" data-type="indexterm" id="id323"/>search engine optimization (SEO) or search engine technology, you’ve probably heard of the <em>robots.txt</em> file. If you go to just about any large website and look for its <em>robots.txt</em> file, you will find it in the root web folder: <em>http://website.com/robots.txt</em>.</p>&#13;
&#13;
<p class="p1">The syntax for <em>robots.txt</em> files was developed in 1994 during the initial boom of web search engine technology. It was about this time that search engines scouring the entire internet, such as AltaVista and DogPile, started competing in earnest with simple lists of sites organized by subject, such as the one curated by Yahoo! This growth of search across the internet meant an explosion not only in the number of web crawlers but also in the availability of information collected by those web crawlers to the average citizen.</p>&#13;
&#13;
<p class="p1">While we might take this sort of availability for granted today, some webmasters were shocked when information they published deep in the file structure of their website became available on the front page of search results in major search engines. In response, the syntax for <em>robots.txt</em> files, called the Robots Exclusion Protocol, <a contenteditable="false" data-primary="Robots Exclusion Protocol" data-type="indexterm" id="id324"/>was developed.</p>&#13;
&#13;
<p class="p1">Unlike the terms of service, which often talks about web crawlers in broad terms and in very human language, <em>robots.txt</em> can be parsed and used by automated programs extremely easily. Although it might seem like the perfect system to solve the problem of unwanted bots once and for all, keep in mind that:</p>&#13;
&#13;
<ul>&#13;
	<li class="p1">&#13;
	<p>There is no official governing body for the syntax of <em>robots.txt</em>. It is a commonly used and generally well-followed convention, but there is nothing to stop anyone from creating their own version of a <em>robots.txt</em> file (apart from the fact that no bot will recognize or obey it until it gets popular). That being said, it is a widely accepted convention, mostly because it is relatively straightforward, and there is no incentive for companies to invent their own standard or try to improve on it.</p>&#13;
	</li>&#13;
	<li class="p1">&#13;
	<p>There is no way to legally or technically enforce a <em>robots.txt</em> file. It is merely a sign that says “Please don’t go to these parts of the site.” Many web scraping libraries are available that obey <em>robots.txt</em>—although this is usually a default setting that can be overridden. Library defaults aside, writing a web crawler that obeys <em>robots.txt</em> is actually more technically challenging than writing one that ignores it altogether. After all, you need to read, parse, and apply the contents of <em>robots.txt</em> to your code logic.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p class="p1">The Robot Exclusion Protocol syntax is fairly straightforward. As in Python (and many other languages), comments begin with a <code>#</code> symbol, end with a newline, and can be used anywhere in the file.</p>&#13;
&#13;
<p class="p1">The first line of the file, apart from any comments, is started with <code>User-agent:</code>, which specifies the user to which of the following rules apply. This is followed by a set of rules, either <code>Allow:</code> or <code>Disallow:</code>, depending on whether the bot is allowed on that section of the site. An asterisk (*) indicates a wildcard and can be used to describe either a <code>User-agent</code> or a URL.</p>&#13;
&#13;
<p class="p1">If a rule follows a rule that it seems to contradict, the last rule takes precedence. For example:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="c1">#Welcome to my robots.txt file!</code>&#13;
<code class="n">User</code><code class="o">-</code><code class="n">agent</code><code class="p">:</code> <code class="o">*</code>&#13;
<code class="n">Disallow</code><code class="p">:</code> <code class="o">*</code>&#13;
&#13;
<code class="n">User</code><code class="o">-</code><code class="n">agent</code><code class="p">:</code> <code class="n">Googlebot</code>&#13;
<code class="n">Allow</code><code class="p">:</code> <code class="o">*</code>&#13;
<code class="n">Disallow</code><code class="p">:</code> <code class="o">/</code><code class="n">private</code></pre>&#13;
&#13;
<p class="p1">In this case, all bots are disallowed from anywhere on the site, except for the Googlebot, which is allowed anywhere except for the <em>/private</em> directory.</p>&#13;
&#13;
<p class="p1">The <em>robots.txt</em> file of <a contenteditable="false" data-primary="Twitter, robots.txt file" data-type="indexterm" id="id325"/>Twitter (also branded as “X”) has explicit instructions for the bots of Google, Yahoo!, Yandex (a popular Russian search engine), Microsoft, and other bots or search engines not covered by any of the preceding categories. The Google section (which looks identical to the permissions allowed to all other categories of bots) looks like this:</p>&#13;
&#13;
<pre data-code-langauage="python" data-type="programlisting">&#13;
#Google Search Engine Robot&#13;
User-agent: Googlebot&#13;
Allow: /?_escaped_fragment_&#13;
&#13;
Allow: /?lang=&#13;
Allow: /hashtag/*?src=&#13;
Allow: /search?q=%23&#13;
Disallow: /search/realtime&#13;
Disallow: /search/users&#13;
Disallow: /search/*/grid&#13;
&#13;
Disallow: /*?&#13;
Disallow: /*/followers&#13;
Disallow: /*/following</pre>&#13;
&#13;
<p class="p1">Notice that Twitter restricts access to the portions of its site for which it has an API in place. Because Twitter has a well-regulated API (and one that it can make money off of by licensing), it is in the company’s best interest to disallow any “home-brewed APIs” that gather information by independently crawling its site.</p>&#13;
&#13;
<p class="p2">Although a file telling your crawler where it can’t go might seem restrictive at first, it can be a blessing in disguise for web crawler development. If you find a <em>robots.txt</em> file that disallows crawling in a particular section of the site, the webmaster is saying, essentially, that they are fine with crawlers in all other sections of the site. After all, if they weren’t fine with it, they would have restricted access when they were writing <em>robots.txt</em> in the first place.</p>&#13;
&#13;
<p class="pagebreak-before">For example, the section of Wikipedia’s <em>robots.txt</em> file that applies to general web scrapers (as opposed to search engines) is wonderfully permissive. It even goes as far as containing human-readable text to welcome bots (that’s us!) and blocks access to only a few pages, such as the login page, search page, and “random article” page:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
#&#13;
# Friendly, low-speed bots are welcome viewing article pages, but not &#13;
# dynamically generated pages please.&#13;
#&#13;
# Inktomi's "Slurp" can read a minimum delay between hits; if your bot supports&#13;
# such a thing using the 'Crawl-delay' or another instruction, please let us &#13;
# know.&#13;
#&#13;
# There is a special exception for API mobileview to allow dynamic mobile web &amp;&#13;
# app views to load section content.&#13;
# These views aren't HTTP-cached but use parser cache aggressively and don't &#13;
# expose special: pages etc.&#13;
#&#13;
User-agent: *&#13;
Allow: /w/api.php?action=mobileview&amp;&#13;
Disallow: /w/&#13;
Disallow: /trap/&#13;
Disallow: /wiki/Especial:Search&#13;
Disallow: /wiki/Especial%3ASearch&#13;
Disallow: /wiki/Special:Collection&#13;
Disallow: /wiki/Spezial:Sammlung&#13;
Disallow: /wiki/Special:Random&#13;
Disallow: /wiki/Special%3ARandom&#13;
Disallow: /wiki/Special:Search&#13;
Disallow: /wiki/Special%3ASearch&#13;
Disallow: /wiki/Spesial:Search&#13;
Disallow: /wiki/Spesial%3ASearch&#13;
Disallow: /wiki/Spezial:Search&#13;
Disallow: /wiki/Spezial%3ASearch&#13;
Disallow: /wiki/Specjalna:Search&#13;
Disallow: /wiki/Specjalna%3ASearch&#13;
Disallow: /wiki/Speciaal:Search&#13;
Disallow: /wiki/Speciaal%3ASearch&#13;
Disallow: /wiki/Speciaal:Random&#13;
Disallow: /wiki/Speciaal%3ARandom&#13;
Disallow: /wiki/Speciel:Search&#13;
Disallow: /wiki/Speciel%3ASearch&#13;
Disallow: /wiki/Speciale:Search&#13;
Disallow: /wiki/Speciale%3ASearch&#13;
Disallow: /wiki/Istimewa:Search&#13;
Disallow: /wiki/Istimewa%3ASearch&#13;
Disallow: /wiki/Toiminnot:Search&#13;
Disallow: /wiki/Toiminnot%3ASearch&#13;
</pre>&#13;
&#13;
<p class="p1">Whether you choose to write web crawlers <a contenteditable="false" data-primary="TOS (Terms of Service)" data-startref="tostos" data-type="indexterm" id="id326"/><a contenteditable="false" data-primary="robots.txt files" data-startref="rbtxtf" data-type="indexterm" id="id327"/>that obey <em>robots.txt</em> is up to you, but I highly recommend it, particularly if you have crawlers that indiscriminately crawl the web.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Three Web Scrapers" data-type="sect1"><div class="sect1" id="id242">&#13;
<h1>Three Web Scrapers</h1>&#13;
&#13;
<p>Because web scraping is such a limitless field, there are a staggering number of ways to land yourself in legal hot water. This section presents three cases that touched on some form of law that generally applies to web scrapers, and how it was used in that case.</p>&#13;
&#13;
<section data-pdf-bookmark="eBay v. Bidder’s Edge and Trespass to Chattels" data-type="sect2"><div class="sect2" id="id19">&#13;
<h2>eBay v. Bidder’s Edge and Trespass to Chattels</h2>&#13;
&#13;
<p>In 1997, the Beanie Baby market was booming, the <a contenteditable="false" data-primary="Bidder's Edge" data-type="indexterm" id="bddedg"/><a contenteditable="false" data-primary="eBay v. Bidder's Edge" data-type="indexterm" id="ebybedg"/><a contenteditable="false" data-primary="trespass to chattels" data-type="indexterm" id="tpctbg"/><a contenteditable="false" data-primary="legal issues" data-secondary="eBay v. Bidder's Edge" data-type="indexterm" id="lgssbd"/>tech sector was bubbling, and online auction houses were the hot new thing on the internet. A company called Bidder’s Edge formed and created a new kind of meta-auction site. Rather than force you to go from auction site to auction site, comparing prices, it would aggregate data from all current auctions for a specific product (say, a hot new Furby doll or a copy of <em>Spice World</em>) and point you to the site that had the lowest price.</p>&#13;
&#13;
<p>Bidder’s Edge accomplished this with an army of web scrapers, constantly making requests to the web servers of the various auction sites to get price and product information. Of all the auction sites, eBay was the largest, and Bidder’s Edge hit eBay’s servers about 100,000 times a day. Even by today’s standards, this is a lot of traffic. According to eBay, this was 1.53% of its total internet traffic at the time, and it certainly wasn’t happy about it.</p>&#13;
&#13;
<p>eBay sent Bidder’s Edge a cease and desist letter, coupled with an offer to license its data. However, negotiations for this licensing failed, and Bidder’s Edge continued to crawl eBay’s site.</p>&#13;
&#13;
<p>eBay tried blocking IP addresses used by Bidder’s Edge, blocking 169 IP addresses—although Bidder’s Edge was able to get around this by using proxy servers (servers that forward requests on behalf of another machine but using the proxy server’s own IP address). As I’m sure you can imagine, this was a frustrating and unsustainable solution for both parties—Bidder’s Edge was constantly trying to find new proxy servers and buy new IP addresses while old ones were blocked, and eBay was forced to maintain large firewall lists (and adding computationally expensive IP address-comparing overhead to each packet check).</p>&#13;
&#13;
<p>Finally, in December 1999, eBay sued Bidder’s Edge under trespass to chattels.</p>&#13;
&#13;
<p>Because eBay’s servers were real, tangible resources that it owned, and it didn’t appreciate Bidder’s Edge’s abnormal use of them, trespass to chattels seemed like the ideal law to use. In fact, in modern times, trespass to chattels goes hand in hand with web-scraping lawsuits and is most often thought of as an IT law.</p>&#13;
&#13;
<p class="pagebreak-before">The courts ruled that for eBay to win its case using trespass to chattels, eBay had to show two things:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>Bidder’s Edge knew it was explicitly disallowed from using eBay’s resources.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>eBay suffered financial loss as a result of Bidder’s Edge’s actions.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>Given the record of eBay’s cease and desist letters, coupled with IT records showing server usage and actual costs associated with the servers, this was relatively easy for eBay to do. Of course, no large court battles end easily: countersuits were filed, many lawyers were paid, and the matter was eventually settled out of court for an undisclosed sum in March 2001.</p>&#13;
&#13;
<p>So does this mean that any unauthorized use of another person’s server is automatically a violation of trespass to chattels? Not necessarily. Bidder’s Edge was an extreme case; it was using so many of eBay’s resources that the company had to buy additional servers, pay more for electricity, and perhaps hire additional personnel. Although the 1.53% increase might not seem like a lot, in large companies, it can add up to a significant amount.</p>&#13;
&#13;
<p>In 2003, the California Supreme Court ruled on another case, Intel Corp versus Hamidi, in which a former Intel employee (Hamidi) sent emails Intel didn’t like, across Intel’s servers, to Intel employees. The court said:</p>&#13;
&#13;
<blockquote>&#13;
<p>Intel’s claim fails not because e-mail transmitted through the internet enjoys unique immunity, but because the trespass to chattels tort—unlike the causes of action just mentioned—may not, in California, be proved without evidence of an injury to the plaintiff’s personal property or legal interest therein.</p>&#13;
</blockquote>&#13;
&#13;
<p>Essentially, Intel had failed to prove that the costs of transmitting the six emails sent by Hamidi to all employees (each one, interestingly enough, with an option to be removed from Hamidi’s mailing list—at least he was polite!) contributed to any financial injury for Intel. It <a contenteditable="false" data-primary="Bidder's Edge" data-startref="bddedg" data-type="indexterm" id="id328"/><a contenteditable="false" data-primary="trespass to chattels" data-startref="tpctbg" data-type="indexterm" id="id329"/><a contenteditable="false" data-primary="legal issues" data-secondary="eBay v. Bidder's Edge" data-startref="lgssbd" data-type="indexterm" id="id330"/>did not deprive Intel of any property or use of its property.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="United States v. Auernheimer and the Computer Fraud and Abuse Act" data-type="sect2"><div class="sect2" id="id20">&#13;
<h2>United States v. Auernheimer and the Computer Fraud and Abuse Act</h2>&#13;
&#13;
<p>If information is readily accessible on <a contenteditable="false" data-primary="Auernheimer, Andrew" data-type="indexterm" id="auaw"/><a contenteditable="false" data-primary="legal issues" data-secondary="United States v. Auernheimer" data-type="indexterm" id="usvanh"/>the internet to a human using a web browser, it’s unlikely that accessing the same exact information in an automated fashion would land you in hot water with the Feds. However, as easy as it can be for a sufficiently curious person to find a small security leak, that small security leak can quickly become a much larger and much more dangerous one when automated scrapers enter the picture.</p>&#13;
&#13;
<p class="pagebreak-before">In 2010, Andrew Auernheimer and Daniel Spitler noticed a nice feature of iPads: when you visited AT&amp;T’s website on them, AT&amp;T would redirect you to a URL containing your iPad’s unique ID number:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
https://dcp2.att.com/OEPClient/openPage?ICCID=&lt;idNumber&gt;&amp;IMEI=</pre>&#13;
&#13;
<p>This page would contain a login form, with the email address of the user whose ID number was in the URL. This allowed users to gain access to their accounts simply by entering their password.</p>&#13;
&#13;
<p>Although there were a large number of potential iPad ID numbers, it was possible, with a web scraper, to iterate through the possible numbers, gathering email addresses along the way. By providing users with this convenient login feature, AT&amp;T, in essence, made its customer email addresses public to the web.</p>&#13;
&#13;
<p>Auernheimer and Spitler created a scraper that collected 114,000 of these email addresses, among them the private email addresses of celebrities, CEOs, and government officials. Auernheimer (but not Spitler) then sent the list, and information about how it was obtained, to Gawker Media, which published the story (but not the list) under the headline: “Apple’s Worst Security Breach: 114,000 iPad Owners Exposed.”</p>&#13;
&#13;
<p>In June 2011, Auernheimer’s home was raided by the FBI in connection with the email address collection, although they ended up arresting him on drug charges. In November 2012, he was found guilty of identity fraud and conspiracy to access a computer without authorization and later sentenced to 41 months in federal prison and ordered to pay $73,000 in restitution.</p>&#13;
&#13;
<p>His case caught the attention of civil rights lawyer Orin Kerr, who joined his legal team and appealed the case to the Third Circuit Court of Appeals. On April 11, 2014 (these legal processes can take quite a while), they made the argument:</p>&#13;
&#13;
<blockquote>&#13;
<p>Auernheimer’s conviction on Count 1 must be overturned because visiting a publicly available website is not unauthorized access under the Computer Fraud and Abuse Act, 18 U.S.C. § 1030(a)(2)(C). AT&amp;T chose not to employ passwords or any other protective measures to control access to the e-mail addresses of its customers. It is irrelevant that AT&amp;T subjectively wished that outsiders would not stumble across the data or that Auernheimer hyperbolically characterized the access as a “theft.” The company configured its servers to make the information available to everyone and thereby authorized the general public to view the information. Accessing the e-mail addresses through AT&amp;T’s public website was authorized under the CFAA and therefore was not a crime.</p>&#13;
</blockquote>&#13;
&#13;
<p>Although Auernheimer’s conviction was only overturned on appeal due to lack of venue, the Third Circuit Court did seem amenable to this argument in a footnote they wrote in their decision:</p>&#13;
&#13;
<blockquote>&#13;
<p class="pagebreak-before">Although we need not resolve whether Auernheimer’s conduct involved such a breach, no evidence was advanced at trial that the account slurper ever breached any password gate or other code-based barrier. The account slurper simply accessed the publicly facing portion of the login screen and scraped information that AT&amp;T unintentionally published.</p>&#13;
</blockquote>&#13;
&#13;
<p>While Auernheimer ultimately was not convicted under the Computer Fraud and Abuse Act, he had his house raided by the FBI, spent many thousands of dollars in legal fees, and spent three years in and out of courtrooms and prisons.</p>&#13;
&#13;
<p>As web scrapers, what lessons can we take away from this to avoid similar situations? Perhaps a good start is: don’t be a jerk.</p>&#13;
&#13;
<p>Scraping any sort of sensitive information, whether it’s personal data (in this case, email addresses), trade secrets, or government secrets, is probably not something you want to do without having a lawyer on speed dial. Even if it’s publicly available, think: “Would the average computer user be able to easily access this information if they wanted to see it?” or “Is this something the company wants users to see?”</p>&#13;
&#13;
<p>I have on many occasions called companies to report security vulnerabilities in their web applications. This line works wonders: “Hi, I’m a security professional who discovered a potential vulnerability on your website. Could you direct me to someone so that I can report it and get the issue resolved?” In addition to the immediate satisfaction of recognition for your (white hat) hacking genius, you might be able to get free subscriptions, cash rewards, and other goodies out of it!</p>&#13;
&#13;
<p>In addition, Auernheimer’s release of the information to Gawker Media (before notifying AT&amp;T) and his showboating around the exploit of the vulnerability also made him an especially attractive target for AT&amp;T’s lawyers.</p>&#13;
&#13;
<p>If you find security vulnerabilities in a site, the best thing to do is to alert the owners of the site, not the media. You might be tempted to write up a blog post and announce it to the world, especially if a fix to the problem is not put in place immediately. However, you need to remember that it is the company’s responsibility, not yours. The best thing you can do is take your web scrapers (and, if applicable, your business) <a contenteditable="false" data-primary="Auernheimer, Andrew" data-startref="auhaw" data-type="indexterm" id="id331"/><a contenteditable="false" data-primary="Spitler, Daniel" data-type="indexterm" id="dspitl"/><a contenteditable="false" data-primary="Auernheimer, Andrew" data-startref="auaw" data-type="indexterm" id="id332"/><a contenteditable="false" data-primary="legal issues" data-secondary="United States v. Auernheimer" data-startref="usvanh" data-type="indexterm" id="id333"/>away from the site!</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Field v. Google: Copyright and robots.txt" data-type="sect2"><div class="sect2" id="id21">&#13;
<h2>Field v. Google: Copyright and robots.txt</h2>&#13;
&#13;
<p>Blake Field, an attorney, filed a lawsuit <a contenteditable="false" data-primary="Field v. Google lawsuit" data-type="indexterm" id="fgkggw"/><a contenteditable="false" data-primary="legal issues" data-secondary="Field v. Google" data-type="indexterm" id="lgsflkg"/>against Google on the basis that its site-caching feature violated copyright law by displaying a copy of his book after he had removed it from his website. Copyright law allows the creator of an original creative work to have control over the distribution of that work. Field’s argument was that Google’s caching (after he had removed it from his website) removed his ability to control its distribution.</p>&#13;
&#13;
<div data-type="note" epub:type="note">&#13;
<h1>The Google Web Cache</h1>&#13;
&#13;
<p>When Google web scrapers (also known as <em>Googlebots</em>) crawl websites, they make a copy of the site <a contenteditable="false" data-primary="Googlebots" data-type="indexterm" id="id334"/>and host it on the internet. Anyone can access this cache, using the URL format:</p>&#13;
&#13;
<ul class="simplelist"><li><em>http://webcache.googleusercontent.com/search?q=cache:http://pythonscraping.com</em></li></ul>&#13;
&#13;
<p>If a website you are searching for, or scraping, is unavailable, you might want to check there to see if a usable copy exists!</p>&#13;
</div>&#13;
&#13;
<p>Knowing about Google’s caching feature and not taking action did not help Field’s case. After all, he could have prevented the Googlebots from caching his website simply by adding the <em>robots.txt</em> file, with simple directives about which pages should and should not be scraped.</p>&#13;
&#13;
<p>More important, the court found that the DMCA Safe Harbor provision allowed Google to legally cache and display <a contenteditable="false" data-primary="Field v. Google lawsuit" data-startref="fgkggw" data-type="indexterm" id="id335"/><a contenteditable="false" data-primary="legal issues" data-secondary="Field v. Google" data-startref="lgsflkg" data-type="indexterm" id="id336"/>sites such as Field’s: “[a] service provider shall not be liable for monetary relief...for infringement of copyright by reason of the intermediate and temporary storage of material on a system or network controlled or operated by or for the service provider.”</p>&#13;
</div></section>&#13;
</div></section>&#13;
<div data-type="footnotes"><p data-type="footnote" id="id308"><sup><a href="ch02.html#id308-marker">1</a></sup> For the full analysis see <a href="https://crsreports.congress.gov/product/pdf/LSB/LSB10922">“Generative Artificial Intelligence and Copyright Law”</a>, Legal Sidebar, Congressional Research Service. 29 September 2023.</p><p data-type="footnote" id="id309"><sup><a href="ch02.html#id309-marker">2</a></sup> See <a href="https://www.uspto.gov/sites/default/files/documents/OpenAI_RFC-84-FR-58141.pdf">“Comment Regarding Request for Comments on Intellectual Property Protection for Artificial Intelligence Innovation.”</a> Docket No. PTO-C-2019-0038, U.S. Patents and Trademark Office.</p><p data-type="footnote" id="id313"><sup><a href="ch02.html#id313-marker">3</a></sup> Bryan Walsh, <a href="http://ti.me/2IFOF3F">“The Surprisingly Large Energy Footprint of the Digital Economy [UPDATE]”</a>, TIME.com, August 14, 2013.</p><p data-type="footnote" id="id319"><sup><a href="ch02.html#id319-marker">4</a></sup> See “‘WarGames’ and Cybersecurity’s Debt to a Hollywood Hack,” <a href="https://oreil.ly/nBCMT"><em>https://oreil.ly/nBCMT</em></a>, and “Disloyal Computer Use and the Computer Fraud and Abuse Act: Narrowing the Scope,” <a href="https://oreil.ly/6TWJq"><em>https://oreil.ly/6TWJq</em></a>.</p></div></div></section></body></html>