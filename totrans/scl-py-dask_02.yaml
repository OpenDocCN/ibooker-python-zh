- en: Chapter 2\. Getting Started with Dask
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are so happy that you’ve decided to explore whether Dask is the system for
    you by trying it out. In this chapter, we will focus on getting started with Dask
    in its local mode. Using this, we’ll explore a few more straightforward parallel
    computing tasks (including everyone’s favorite, word count).^([1](ch02.xhtml#id330))
  prefs: []
  type: TYPE_NORMAL
- en: Installing Dask Locally
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Installing Dask locally is reasonably straightforward. If you want to begin
    running on multiple machines, doing so is often easier when you start with a conda
    environment (or virtualenv). This lets you figure out what packages you depend
    on by running `pip freeze` to make sure they’re on all of the workers when it’s
    time to scale.
  prefs: []
  type: TYPE_NORMAL
- en: While you can just run `pip install -U dask`, we prefer using a conda environment
    since it’s easier to match the version of Python to that on a cluster, which allows
    us to connect a local machine to the cluster directly.^([2](ch02.xhtml#id333))
    If you don’t already have conda on your machine, [Miniforge](https://oreil.ly/qVDa7)
    is a good and quick way to get conda installed across multiple platforms. The
    installation of Dask into a new conda environment is shown in [Example 2-1](#install_conda_env_with_dask).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-1\. Installing Dask into a new conda environment
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here we install a specific version of Dask rather than just the latest version.
    If you’re planning to connect to a cluster later on, it will be useful to pick
    the same version of Dask as is installed on the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You don’t have to install Dask locally. There is a [BinderHub example with Dask](https://oreil.ly/EK5n5)
    and distributed options, including [one from the creators of Dask](https://oreil.ly/3UEq-),
    that you can use to run Dask, as well as other providers such as [SaturnCloud](https://oreil.ly/_6SyV).
    That being said, we recommend having Dask installed locally even if you end up
    using one of these services.
  prefs: []
  type: TYPE_NORMAL
- en: Hello Worlds
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you have Dask installed locally, it’s time to try the versions of “Hello
    World” available through its various APIs. There are many different options for
    starting Dask. For now, you should use LocalCluster, as shown in [Example 2-2](#make_dask_client).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-2\. Using LocalCluster to start Dask
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Task Hello World
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the core building blocks of Dask is `dask.delayed`, which allows you
    to run functions in parallel. If you are running Dask on multiple machines, these
    functions can also be distributed (or spread out) on the different machines. When
    you wrap a function with `dask.delayed` and call it, you get back a “delayed”
    object representing the desired computation. When you created a delayed object,
    Dask is just making a note of what you might want it to do. As with a lazy teenager,
    you need to be explicit. You can force Dask to start computing the value with
    `dask.submit`, which produces a “future.” You can use `dask.compute` both to start
    computing the delayed objects and futures and to return their values.^([3](ch02.xhtml#id344))
  prefs: []
  type: TYPE_NORMAL
- en: Sleepy task
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An easy way to see the performance difference is by writing an intentionally
    slow function, like `slow_task`, which calls `sleep`. Then you can compare the
    performance of Dask to “regular” Python by mapping the function over a few elements
    with and without `dask.delayed`, as shown in [Example 2-3](#sleepy_task_ch02_1688747609671).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-3\. Sleepy task
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: When we run this example, we get `In sequence 20.01662155520171, in parallel
    6.259156636893749`, which shows that Dask can run some of the tasks in parallel,
    but not all of them.^([4](ch02.xhtml#id350))
  prefs: []
  type: TYPE_NORMAL
- en: Nested tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the neat things about `dask.delayed` is that you can launch tasks inside
    of other tasks.^([5](ch02.xhtml#id351)) A straightforward real-world example of
    this is a web crawler, with which, when you visit a web page, you want to fetch
    all of the links from that page, as shown in [Example 2-4](#web_crawler_ch02_1688747981454).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-4\. Web crawler
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In practice, some central co-ordination is still involved behind the scenes
    (including the scheduler), but the freedom to write your code in this nested way
    is quite powerful.
  prefs: []
  type: TYPE_NORMAL
- en: We cover other kinds of task dependencies in [“Task Dependencies”](ch03.xhtml#task_deps).
  prefs: []
  type: TYPE_NORMAL
- en: Distributed Collections
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to the low-level task APIs, Dask also has distributed collections.
    These collections enable you to work with data that would be too large to fit
    on a single machine and to naturally distribute work on it, which is called *data
    parallelism*. Dask has both an unordered collection called a *bag*, and an ordered
    collection called an *array*. Dask arrays aim to implement some of the ndarray
    interface, whereas bags focus more on functional programming (e.g., things like
    `map` and `filter`). You can load Dask collections from files, take local collections
    and distribute them, or take the results of `dask.delayed` tasks and turn them
    into a collection.
  prefs: []
  type: TYPE_NORMAL
- en: In distributed collections, Dask splits the data up using partitions. Partitions
    are used to decrease the scheduling cost compared to operating on individual rows,
    which is covered in more detail in [“Partitioning/Chunking Collections”](ch03.xhtml#basic_partitioning).
  prefs: []
  type: TYPE_NORMAL
- en: Dask arrays
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dask arrays allow you to go beyond what can fit in memory, or even on disk,
    on a single computer. Many of the standard NumPy operations are supported out
    of the box, including aggregates such as average and standard deviation. The `from_array`
    function in Dask arrays converts a local array-like collection into a distributed
    collection. [Example 2-5](#ex_dask_array) shows how to create a distributed array
    from a local one and then compute the average.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-5\. Creating a distributed array and computing the average
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As with all distributed collections, what is expensive on a Dask array is not
    the same as what is expensive on a local array. In the next chapter you’ll learn
    a bit more about how Dask arrays are implemented and hopefully gain a better intuition
    around their performance.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a distributed collection from a local collection uses the two fundamental
    building blocks of distributed computing, called the *scatter-gather pattern*.
    While the originating dataset must be from a local computer, fitting into a single
    machine, this already expands the number of processors you have at your disposal,
    as well as the intermediate memory you can utilize, enabling you to better exploit
    modern cloud infrastructure and scale. A practical use case would be a distributed
    web crawler, where the list of seed URLs to crawl might be a small dataset, but
    the memory you need to hold while crawling might be an order of magnitude larger,
    requiring distributed computing.
  prefs: []
  type: TYPE_NORMAL
- en: Dask bags and a word count
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dask bags implement more of the functional programming interfaces than Dask
    arrays. The “Hello World” of big data is word count, which is easier to implement
    with functional programming interfaces. Since you’ve already made a crawler function,
    you can turn its output into a Dask bag using the `from_delayed` function in [Example 2-6](#make_bag_of_crawler).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-6\. Turning the crawler function’s output into a Dask bag
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now that you have a Dask bag collection, you can build everyone’s favorite word
    count example on top of it. The first step is to turn your bag of text into a
    bag of words, which you do by using `map` (see [Example 2-7](#make_a_bag_of_words)).
    Once you have the bag of words, you can either use Dask’s built-in `frequency`
    method (see [Example 2-8](#wc_freq)) or write your own `frequency` method using
    functional transformations (see [Example 2-9](#wc_func)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-7\. Turning a bag of text into a bag of words
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Example 2-8\. Using Dask’s built-in `frequency` method
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Example 2-9\. Using functional transformations to write a custom `frequency`
    method
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: On Dask bags, `foldby`, `frequency`, and many other reductions return a single
    partition bag, meaning the data after reduction needs to fit in a single computer.
    Dask DataFrames handle reductions differently and don’t have that same restriction.
  prefs: []
  type: TYPE_NORMAL
- en: Dask DataFrame (Pandas/What People Wish Big Data Was)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pandas is one of the most popular Python data libraries, and Dask has a DataFrame
    library that implements much of the pandas API. Thanks to Python’s duck-typing,
    you can often use Dask’s distributed DataFrame library in place of pandas. Not
    all of the API will work exactly the same, and some parts are not implemented,
    so be sure you have good test coverage.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Your intuition around what’s slow and fast with pandas does not carry over.
    We will explore this more in [“Dask DataFrames”](ch03.xhtml#dask_df).
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate how you can use Dask DataFrame, we’ll rework Examples [2-6](#make_bag_of_crawler)
    through [2-8](#wc_freq) to use it. As with Dask’s other collections, you can create
    DataFrames from local collections, futures, or distributed files. Since you’ve
    already made a crawler function, you can turn its output into a Dask bag using
    the `from_delayed` function from [Example 2-6](#make_bag_of_crawler). Instead
    of using `map` and `foldby`, you can use pandas APIs such as `explode` and `value_counts`,
    as shown in [Example 2-10](#wc_dataframe).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-10\. DataFrame word count
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter you got Dask working on your local machine, as well as had a
    tour of the different “Hello World” (or getting started) examples with most of
    Dask’s different built-in libraries. Subsequent chapters will dive into these
    different tools in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you’ve got Dask working on your local machine, you might want to jump
    on over to [Chapter 12](ch12.xhtml#ch12) and look at the different deployment
    mechanisms. For the most part, you can run the examples in local mode, albeit
    sometimes a little slower or at a smaller scale. However, the next chapter will
    look at the core concepts of Dask, and one of the upcoming examples emphasizes
    the benefits of having Dask running on multiple machines and is also generally
    easier to explore on a cluster. If you don’t have a cluster available, you may
    wish to set up a simulated one using something like [MicroK8s](https://microk8s.io).
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch02.xhtml#id330-marker)) Word count may be a somewhat tired example,
    but it is an important example, since it covers both work that can be done with
    minimal co-ordination (splitting up the text into words) and work requiring co-ordination
    between computers (summing the words).
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch02.xhtml#id333-marker)) There are downsides to deploying your Dask application
    in this way, as discussed in [Chapter 12](ch12.xhtml#ch12), but it can be an excellent
    debugging technique.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch02.xhtml#id344-marker)) Provided they fit in memory.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch02.xhtml#id350-marker)) When we run this on a cluster, we get worse
    performance, as there is overhead to distributing a task to a remote computer
    compared to the small delay.
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch02.xhtml#id351-marker)) This is very different from Apache Spark, where
    only the driver/head node can launch tasks.
  prefs: []
  type: TYPE_NORMAL
