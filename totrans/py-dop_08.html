<html><head></head><body><section data-pdf-bookmark="Chapter 8. Pytest for DevOps" data-type="chapter" epub:type="chapter"><div class="chapter" id="pytest_for_devops">&#13;
<h1><span class="label">Chapter 8. </span>Pytest for DevOps</h1>&#13;
&#13;
&#13;
<p><a data-primary="pytest" data-type="indexterm" id="ix_ch08-asciidoc0"/>Continuous integration, continuous delivery, deployments, and any pipeline workflow in general with some thought put into it will be filled with validation. This <em>validation</em> can happen at every step of the way and when achieving important objectives.</p>&#13;
&#13;
<p>For example, if in the middle of a long list of steps to produce a deployment, a <code>curl</code> command is called to get an all-important file, do you think the build should continue if it fails? Probably not! <code>curl</code> has a flag that can be used to produce a nonzero exit status (<code>--fail</code>) if an HTTP error happens. That simple flag usage is a form of validation: ensure that the request succeeded, otherwise fail the build step. The <em>key word</em> is to <em>ensure</em> that something succeeded, and that is at the core of this chapter: validation and testing strategies that can help you build better infrastructure.</p>&#13;
&#13;
<p>Thinking about validation becomes all the more satisfying when Python gets in the mix, harnessing testing frameworks like <code>pytest</code> to handle the verification of systems.</p>&#13;
&#13;
<p>This chapter reviews some of the basics associated with testing in Python using the phenomenal <code>pytest</code> framework, then dives into some advanced features of the framework, and finally goes into detail about the <em>TestInfra</em> project, a plug-in to <code>pytest</code> that can do system verification.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Testing Superpowers with pytest" data-type="sect1"><div class="sect1" id="idm46691327548616">&#13;
<h1>Testing Superpowers with pytest</h1>&#13;
&#13;
<p><a data-primary="pytest" data-secondary="testing superpowers with" data-type="indexterm" id="idm46691327547160"/>We can’t say enough good things about the <code>pytest</code> framework. Created by Holger Krekel, it is now maintained by quite a few people that do an incredible job at producing a high-quality piece of software that is usually part of our everyday work. As a full-featured framework, it is tough to narrow down the scope enough to provide a useful introduction without repeating the project’s complete documentation.</p>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>The <code>pytest</code> project has lots of information, examples, and feature details <a href="https://oreil.ly/PSAu2">in its documentation</a> that are worth reviewing. There are always new things to learn as the project continues to provide new releases and different ways to improve <span class="keep-together">testing.</span></p>&#13;
</div>&#13;
&#13;
<p>When Alfredo was first introduced to the framework, he was struggling to write tests, and found it cumbersome to adhere to Python’s built-in way of testing with <code>unittest</code> (this chapter goes through the differences later). It took him a couple of minutes to get hooked into <code>pytest</code>’s magical reporting. It wasn’t forcing him to move away from how he had written his tests, and it worked right out of the box with no modifications! This flexibility shows throughout the project, and even when things might not be possible today, you can extend its functionality via plug-ins or its configuration files.</p>&#13;
&#13;
<p>By understanding how to write more straightforward test cases, and by taking advantage of the command-line tool, reporting engine, plug-in extensibility, and framework utilities, you will want to write more tests that will undoubtedly be better all around.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Getting Started with pytest" data-type="sect1"><div class="sect1" id="idm46691327539544">&#13;
<h1>Getting Started with pytest</h1>&#13;
&#13;
<p><a data-primary="pytest" data-secondary="getting started" data-type="indexterm" id="ix_ch08-asciidoc1"/>In its simplest form, <code>pytest</code> is a command-line tool that discovers Python tests and executes them. It doesn’t force a user to understand its internals, which makes it easy to get started with. This section demonstrates some of the most basic features, from writing tests to laying out files (so that they get automatically discovered), and finally, looking at the main differences between it and Python’s built-in testing framework, <code>unittest</code>.</p>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>Most<a data-primary="IDEs (Integrated Development Environments)" data-type="indexterm" id="idm46691327534392"/><a data-primary="Integrated Development Environments (IDEs)" data-type="indexterm" id="idm46691327533624"/> <em>Integrated Development Environments</em> (IDEs), such as PyCharm and Visual Studio Code, have built-in support for running <code>pytest</code>. If using a text editor like Vim, there is support via the <a href="https://oreil.ly/HowKu"><code>pytest.vim</code></a> plug-in. Using <code>pytest</code> from the editor saves time and makes debugging failures easier, but be aware that not every option or plug-in is supported.</p>&#13;
</div>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Testing with pytest" data-type="sect2"><div class="sect2" id="idm46691327530376">&#13;
<h2>Testing with pytest</h2>&#13;
&#13;
<p><a data-primary="pytest" data-secondary="testing with" data-type="indexterm" id="ix_ch08-asciidoc2"/>Make sure that you have <code>pytest</code> installed and available in the command line:</p>&#13;
&#13;
<pre data-type="programlisting">$ python3 -m venv testing&#13;
$ source testing/bin/activate</pre>&#13;
&#13;
<p class="pagebreak-before">Create a file called <em>test_basic.py</em>; it should look like this:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">test_simple</code><code class="p">():</code>&#13;
    <code class="k">assert</code> <code class="bp">True</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">test_fails</code><code class="p">():</code>&#13;
    <code class="k">assert</code> <code class="bp">False</code></pre>&#13;
&#13;
<p>If <code>pytest</code> runs without any arguments, it should show a pass and a failure:</p>&#13;
&#13;
<pre data-type="programlisting">$ (testing) pytest&#13;
============================= test session starts =============================&#13;
platform linux -- Python 3.6.8, pytest-4.4.1, py-1.8.0, pluggy-0.9.0&#13;
rootdir: /home/alfredo/python/testing&#13;
collected 2 items&#13;
&#13;
test_basic.py .F                                                        [100%]&#13;
&#13;
================================== FAILURES ===================================&#13;
_________________________________ test_fails __________________________________&#13;
&#13;
    def test_fails():&#13;
&gt;       assert False&#13;
E       assert False&#13;
&#13;
test_basic.py:6: AssertionError&#13;
===================== 1 failed, 1 passed in 0.02 seconds ======================</pre>&#13;
&#13;
<p>The output is beneficial from the start; it displays how many tests were collected, how many passed, and which one failed—including its line number.</p>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>The default output from <code>pytest</code> is handy, but it might be too verbose. You can control the amount of output with configuration, reducing it with the <code>-q</code> flag.</p>&#13;
</div>&#13;
&#13;
<p>There was no need to create a class to include the tests; functions were discovered and ran correctly. A test suite can have a mix of both, and the framework works fine in such an environment.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Layouts and conventions" data-type="sect3"><div class="sect3" id="idm46691327506360">&#13;
<h3>Layouts and conventions</h3>&#13;
&#13;
<p><a data-primary="pytest" data-secondary="layouts and conventions" data-type="indexterm" id="idm46691327504920"/>When testing in Python, there are a few conventions that <code>pytest</code> follows implicitly. Most of these conventions are about naming and structure. For example, try renaming the <em>test_basic.py</em> file to <em>basic.py</em> and run <code>pytest</code> to see what happens:</p>&#13;
&#13;
<pre data-type="programlisting">$ (testing) pytest -q&#13;
&#13;
no tests ran in 0.00 seconds</pre>&#13;
&#13;
<p>No tests ran because of the convention of prefixing test files with <em>test_</em>. If you rename the file back to <em>test_basic.py</em>, it should be automatically discovered and tests should run.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Layouts and conventions are helpful for automatic test discovery. It is possible to configure the framework to use other naming conventions or to directly test a file that has a unique name. However, it is useful to follow through with basic expectations to avoid confusion when tests don’t run.</p>&#13;
</div>&#13;
&#13;
<p>These are conventions that will allow the tool to discover tests:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>The testing directory needs to be named <em>tests</em>.</p>&#13;
</li>&#13;
<li>&#13;
<p>Test files need to be prefixed with <em>test</em>; for example, <em>test_basic.py</em>, or suffixed with <em>test.py</em>.</p>&#13;
</li>&#13;
<li>&#13;
<p>Test functions need to be prefixed with <code>test_</code>; for example, <code>def test</code><span class="keep-together"><code>simple():</code></span>.</p>&#13;
</li>&#13;
<li>&#13;
<p>Test classes need to be prefixed with <code>Test</code>; for example, <code>class TestSimple</code>.</p>&#13;
</li>&#13;
<li>&#13;
<p>Test methods follow the same conventions as functions, prefixed with <code>test_</code>; for example, <code>def test_method(self):</code>.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Because prefixing with <code>test_</code> is a requirement for automatic discovery and execution of tests, it allows introducing helper functions and other nontest code with different names, so that they get excluded automatically.<a data-startref="ix_ch08-asciidoc2" data-type="indexterm" id="idm46691327467400"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Differences with unittest" data-type="sect2"><div class="sect2" id="idm46691327529752">&#13;
<h2>Differences with unittest</h2>&#13;
&#13;
<p><a data-primary="pytest" data-secondary="unittest versus" data-type="indexterm" id="ix_ch08-asciidoc3"/><a data-primary="unittest, pytest versus" data-type="indexterm" id="ix_ch08-asciidoc4"/>Python already comes with a set of utilities and helpers for testing, and they are part of the <code>unittest</code> module. It is useful to understand how <code>pytest</code> is different and why it is highly recommended.</p>&#13;
&#13;
<p>The <code>unittest</code> module forces the use of classes and class inheritance. For an experienced developer who understands object-oriented programming and class inheritance, this shouldn’t be a problem, but for beginners, <em>it is an obstacle</em>. Using classes and inheritance shouldn’t be a requisite for writing basic tests!</p>&#13;
&#13;
<p>Part of forcing users to inherit from <code>unittest.TestCase</code> is that you are required to understand (and remember) most of the assertion methods that are used to verify results. With <code>pytest</code>, there is a single assertion helper that can do it all: <code>assert</code>.</p>&#13;
&#13;
<p>These are a few of the assert methods that can be used when writing tests with <span class="keep-together"><code>unittest</code></span>. Some of them are easy to grasp, while others are very confusing:</p>&#13;
&#13;
<ul class="pagebreak-before">&#13;
<li>&#13;
<p><code>self.assertEqual(a, b)</code></p>&#13;
</li>&#13;
<li>&#13;
<p><code>self.assertNotEqual(a, b)</code></p>&#13;
</li>&#13;
<li>&#13;
<p><code>self.assertTrue(x)</code></p>&#13;
</li>&#13;
<li>&#13;
<p><code>self.assertFalse(x)</code></p>&#13;
</li>&#13;
<li>&#13;
<p><code>self.assertIs(a, b)</code></p>&#13;
</li>&#13;
<li>&#13;
<p><code>self.assertIsNot(a, b)</code></p>&#13;
</li>&#13;
<li>&#13;
<p><code>self.assertIsNone(x)</code></p>&#13;
</li>&#13;
<li>&#13;
<p><code>self.assertIsNotNone(x)</code></p>&#13;
</li>&#13;
<li>&#13;
<p><code>self.assertIn(a, b)</code></p>&#13;
</li>&#13;
<li>&#13;
<p><code>self.assertNotIn(a, b)</code></p>&#13;
</li>&#13;
<li>&#13;
<p><code>self.assertIsInstance(a, b)</code></p>&#13;
</li>&#13;
<li>&#13;
<p><code>self.assertNotIsInstance(a, b)</code></p>&#13;
</li>&#13;
<li>&#13;
<p><code>self.assertRaises(exc, fun, *args, **kwds)</code></p>&#13;
</li>&#13;
<li>&#13;
<p><code>self.assertRaisesRegex(exc, r, fun, *args, **kwds)</code></p>&#13;
</li>&#13;
<li>&#13;
<p><code>self.assertWarns(warn, fun, *args, **kwds)</code></p>&#13;
</li>&#13;
<li>&#13;
<p><code>self.assertWarnsRegex(warn, r, fun, *args, **kwds)</code></p>&#13;
</li>&#13;
<li>&#13;
<p><code>self.assertLogs(logger, level)</code></p>&#13;
</li>&#13;
<li>&#13;
<p><code>self.assertMultiLineEqual(a, b)</code></p>&#13;
</li>&#13;
<li>&#13;
<p><code>self.assertSequenceEqual(a, b)</code></p>&#13;
</li>&#13;
<li>&#13;
<p><code>self.assertListEqual(a, b)</code></p>&#13;
</li>&#13;
<li>&#13;
<p><code>self.assertTupleEqual(a, b)</code></p>&#13;
</li>&#13;
<li>&#13;
<p><code>self.assertSetEqual(a, b)</code></p>&#13;
</li>&#13;
<li>&#13;
<p><code>self.assertDictEqual(a, b)</code></p>&#13;
</li>&#13;
<li>&#13;
<p><code>self.assertAlmostEqual(a, b)</code></p>&#13;
</li>&#13;
<li>&#13;
<p><code>self.assertNotAlmostEqual(a, b)</code></p>&#13;
</li>&#13;
<li>&#13;
<p><code>self.assertGreater(a, b)</code></p>&#13;
</li>&#13;
<li>&#13;
<p><code>self.assertGreaterEqual(a, b)</code></p>&#13;
</li>&#13;
<li>&#13;
<p><code>self.assertLess(a, b)</code></p>&#13;
</li>&#13;
<li>&#13;
<p><code>self.assertLessEqual(a, b)</code></p>&#13;
</li>&#13;
<li>&#13;
<p><code>self.assertRegex(s, r)</code></p>&#13;
</li>&#13;
<li>&#13;
<p><code>self.assertNotRegex(s, r)</code></p>&#13;
</li>&#13;
<li>&#13;
<p><code>self.assertCountEqual(a, b)</code></p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p><code>pytest</code> allows you to use <code>assert</code> exclusively and does not force you to use any of the above. Moreover, it <em>does allow</em> you to write tests using <code>unittest</code>, and it even executes them. We strongly advise against doing that and suggest you concentrate on just using plain asserts.</p>&#13;
&#13;
<p>Not only is it easier to use plain asserts, but <code>pytest</code> also provides a rich comparison engine on failures (more on this in the next section)<a data-startref="ix_ch08-asciidoc4" data-type="indexterm" id="idm46691327354808"/><a data-startref="ix_ch08-asciidoc3" data-type="indexterm" id="idm46691327354104"/>.<a data-startref="ix_ch08-asciidoc1" data-type="indexterm" id="idm46691327353304"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="pytest Features" data-type="sect1"><div class="sect1" id="idm46691327352472">&#13;
<h1>pytest Features</h1>&#13;
&#13;
<p><a data-primary="pytest" data-secondary="features" data-type="indexterm" id="ix_ch08-asciidoc5"/>Aside from making it easier to write tests and execute them, the framework provides lots of extensible options, such as hooks. Hooks allow you to interact with the framework internals at different points in the runtime. If you want to alter the collection of tests, for example, a hook for the collection engine can be added. Another useful example is if you want to implement a nicer report when a test fails.</p>&#13;
&#13;
<p>While developing an <a data-primary="HTTP" data-secondary="pytest and" data-type="indexterm" id="idm46691327348968"/>HTTP API, we found that sometimes the failures in the tests that used HTTP requests against the application weren’t beneficial: an assertion failure would be reported because the expected response (an HTTP 200) was an HTTP 500 error. We wanted to know more about the request: to what URL endpoint? If it was a POST request, did it have data? What did it look like? These are things that were already available in the HTTP response object, so we wrote a hook to poke inside this object and include all these items as part of the failure report.</p>&#13;
&#13;
<p>Hooks are an advanced feature of <code>pytest</code> that you might not need at all, but it is useful to understand that the framework can be flexible enough to accommodate different requirements. The next sections cover how to extend the framework, why using <code>assert</code> is so valuable, how to parametrize tests to reduce repetition, how to make helpers with <code>fixtures</code>, and how to use the built-in ones.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="conftest.py" data-type="sect2"><div class="sect2" id="idm46691327345128">&#13;
<h2>conftest.py</h2>&#13;
&#13;
<p><a data-primary="conftest.py" data-type="indexterm" id="idm46691327343752"/><a data-primary="pytest" data-secondary="conftest.py" data-type="indexterm" id="idm46691327342824"/>Most software lets you extend functionality via plug-ins (web browsers call them <em>extensions</em>, for example); similarly, <code>pytest</code> has a rich API for developing plug-ins. The complete API is not covered here, but its simpler approach is: the <em>conftest.py</em> file. In this file, the tool can be extended <em>just like a plug-in can</em>. There is no need to fully understand how to create a separate plug-in, package it, and install it. If a <em>conftest.py</em> file is present, the framework will load it and consume any specific directives in it. It all happens automatically!</p>&#13;
&#13;
<p>Usually, you will find that a <em>conftest.py</em> file is used to hold hooks, fixtures, and helpers for those fixtures. Those <em>fixtures</em> can then be used within tests if declared as arguments (that process is described later in the fixture section).</p>&#13;
&#13;
<p>It makes sense to add fixtures and helpers to this file when more than one test module will use it. If there is only a single test file, or if only one file is going to make use of a fixture or hook, there is no need to create or use a <em>conftest.py</em> file. Fixtures and helpers can be defined within the same file as the test and behave the same.</p>&#13;
&#13;
<p>The only condition for loading a <em>conftest.py</em> file is to be present in the <em>tests</em> directory and match the name correctly. Also, although this name is configurable, we advise against changing it and encourage you to follow the default naming conventions to avoid potential issues.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Amazing assert" data-type="sect2"><div class="sect2" id="idm46691327335128">&#13;
<h2>The Amazing assert</h2>&#13;
&#13;
<p><a data-primary="assert statement" data-type="indexterm" id="idm46691327333752"/><a data-primary="pytest" data-secondary="assert statement" data-type="indexterm" id="idm46691327333048"/>When we have to describe how great the <code>pytest</code> tooling is, we start by describing the important uses of the <code>assert</code> statement. Behind the scenes, the framework is inspecting objects and providing a rich comparison engine to better describe errors. This is usually met with resistance because a bare <code>assert</code> in Python is terrible at describing errors. Compare two long strings as an example:</p>&#13;
&#13;
<pre data-code-language="python3" data-type="programlisting"><code class="o">&gt;&gt;&gt;</code> <code class="k">assert</code> <code class="s2">"using assert for errors"</code> <code class="o">==</code> <code class="s2">"using asert for errors"</code>&#13;
<code class="n">Traceback</code> <code class="p">(</code><code class="n">most</code> <code class="n">recent</code> <code class="n">call</code> <code class="n">last</code><code class="p">):</code>&#13;
  <code class="n">File</code> <code class="s2">"&lt;stdin&gt;"</code><code class="p">,</code> <code class="n">line</code> <code class="mi">1</code><code class="p">,</code> <code class="ow">in</code> <code class="o">&lt;</code><code class="n">module</code><code class="o">&gt;</code>&#13;
<code class="ne">AssertionError</code></pre>&#13;
&#13;
<p>Where is the difference? It is hard to tell without spending some time looking at those two long lines closely. This will cause people to recommend against it. A small test shows how <code>pytest</code> augments when reporting the failure:</p>&#13;
&#13;
<pre data-type="programlisting">$ (testing) pytest test_long_lines.py&#13;
============================= test session starts =============================&#13;
platform linux -- Python 3.6.8, pytest-4.4.1, py-1.8.0, pluggy-0.9.0&#13;
collected 1 item&#13;
&#13;
test_long_lines.py F                                                    [100%]&#13;
&#13;
================================== FAILURES ===================================&#13;
_______________________________ test_long_lines _______________________________&#13;
&#13;
    def test_long_lines():&#13;
&gt;      assert "using assert for errors" == "using asert for errors"&#13;
E      AssertionError: assert '...rt for errors' == '...rt for errors'&#13;
E        - using assert for errors&#13;
E        ?        -&#13;
E        + using asert for errors&#13;
&#13;
test_long_lines.py:2: AssertionError&#13;
========================== 1 failed in 0.04 seconds ===========================</pre>&#13;
&#13;
<p class="pagebreak-before">Can you tell where the error is? This is <em>tremendously easier</em>. Not only does it tell you it fails, but it points to exactly <em>where</em> the failure is. The example is a simple assert with a long string, but the framework handles other data structures like lists and dictionaries without a problem. Have you ever compared very long lists in tests? It is impossible to easily tell what items are different. Here is a small snippet with long lists:</p>&#13;
&#13;
<pre data-type="programlisting">    assert ['a', 'very', 'long', 'list', 'of', 'items'] == [&#13;
            'a', 'very', 'long', 'list', 'items']&#13;
E   AssertionError: assert [...'of', 'items'] == [...ist', 'items']&#13;
E     At index 4 diff: 'of' != 'items'&#13;
E     Left contains more items, first extra item: 'items'&#13;
E     Use -v to get the full diff</pre>&#13;
&#13;
<p>After informing the user that the test failed, it points exactly to the index number (index four or fifth item), and finally, it says that one list has one extra item. Without this level of introspection, debugging failures would take a very long time. The bonus in reporting is that, by default, it omits very long items when making comparisons, so that only the relevant portion shows in the output. After all, what you want is to know not only that the lists (or any other data structure) are different but <em>exactly where they</em> are different.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Parametrization" data-type="sect2"><div class="sect2" id="idm46691327316408">&#13;
<h2>Parametrization</h2>&#13;
&#13;
<p><a data-primary="parametrization" data-type="indexterm" id="idm46691327302312"/><a data-primary="pytest" data-secondary="parametrization" data-type="indexterm" id="idm46691327301608"/>Parametrization is one of the features that can take a while to understand because it doesn’t exist in the <code>unittest</code> module and is a feature unique to the pytest framework. It can be clear once you find yourself writing very similar tests that had minor changes in the inputs but are testing the same thing. Take, for example, this class that is testing a function that returns <code>True</code> if a string is implying a truthful value. The <a data-primary="string_to_bool() function" data-type="indexterm" id="idm46691327299272"/><code>string_to_bool</code> is the function under test:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">my_module</code> <code class="kn">import</code> <code class="n">string_to_bool</code>&#13;
&#13;
<code class="k">class</code> <code class="nc">TestStringToBool</code><code class="p">(</code><code class="nb">object</code><code class="p">):</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">test_it_detects_lowercase_yes</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>&#13;
        <code class="k">assert</code> <code class="n">string_to_bool</code><code class="p">(</code><code class="s1">'yes'</code><code class="p">)</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">test_it_detects_odd_case_yes</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>&#13;
        <code class="k">assert</code> <code class="n">string_to_bool</code><code class="p">(</code><code class="s1">'YeS'</code><code class="p">)</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">test_it_detects_uppercase_yes</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>&#13;
        <code class="k">assert</code> <code class="n">string_to_bool</code><code class="p">(</code><code class="s1">'YES'</code><code class="p">)</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">test_it_detects_positive_str_integers</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>&#13;
        <code class="k">assert</code> <code class="n">string_to_bool</code><code class="p">(</code><code class="s1">'1'</code><code class="p">)</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">test_it_detects_true</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>&#13;
        <code class="k">assert</code> <code class="n">string_to_bool</code><code class="p">(</code><code class="s1">'true'</code><code class="p">)</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">test_it_detects_true_with_trailing_spaces</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>&#13;
        <code class="k">assert</code> <code class="n">string_to_bool</code><code class="p">(</code><code class="s1">'true '</code><code class="p">)</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">test_it_detects_true_with_leading_spaces</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>&#13;
        <code class="k">assert</code> <code class="n">string_to_bool</code><code class="p">(</code><code class="s1">' true'</code><code class="p">)</code></pre>&#13;
&#13;
<p>See how all these tests are evaluating the same result from similar inputs? This is where parametrization shines because it can group all these values and pass them to the test; it can effectively reduce them to a single test:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">pytest</code>&#13;
<code class="kn">from</code> <code class="nn">my_module</code> <code class="kn">import</code> <code class="n">string_to_bool</code>&#13;
&#13;
<code class="n">true_values</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'yes'</code><code class="p">,</code> <code class="s1">'1'</code><code class="p">,</code> <code class="s1">'Yes'</code><code class="p">,</code> <code class="s1">'TRUE'</code><code class="p">,</code> <code class="s1">'TruE'</code><code class="p">,</code> <code class="s1">'True'</code><code class="p">,</code> <code class="s1">'true'</code><code class="p">]</code>&#13;
&#13;
<code class="k">class</code> <code class="nc">TestStrToBool</code><code class="p">(</code><code class="nb">object</code><code class="p">):</code>&#13;
&#13;
    <code class="nd">@pytest.mark.parametrize</code><code class="p">(</code><code class="s1">'value'</code><code class="p">,</code> <code class="n">true_values</code><code class="p">)</code>&#13;
    <code class="k">def</code> <code class="nf">test_it_detects_truish_strings</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">value</code><code class="p">)</code>&#13;
        <code class="k">assert</code> <code class="n">string_to_bool</code><code class="p">(</code><code class="n">value</code><code class="p">)</code></pre>&#13;
&#13;
<p>There are a couple of things happening here. First <code>pytest</code> is imported (the framework) to use the <code>pytest.mark.parametrize</code> module, then <code>true_values</code> is defined as a (list) variable of all the values to use that should evaluate the same, and finally, it replaces all the test methods to a single one. The test method uses the <code>parametrize</code> decorator, which defines two arguments. The first is a string, <code><em>value</em></code>, and the second is the name of the list defined previously. This can look a bit odd, but it is telling the framework that <code><em>value</em></code> is the name to use for the argument in the test method. That is where the <code>value</code> argument comes from!</p>&#13;
&#13;
<p>If the verbosity is increased when running, the output will show exactly what value was passed in. It almost looks like the single test got cloned into every single iteration passed in:</p>&#13;
&#13;
<pre data-type="programlisting">test_long_lines.py::TestLongLines::test_detects_truish_strings[yes] PASSED&#13;
test_long_lines.py::TestLongLines::test_detects_truish_strings[1] PASSED&#13;
test_long_lines.py::TestLongLines::test_detects_truish_strings[Yes] PASSED&#13;
test_long_lines.py::TestLongLines::test_detects_truish_strings[TRUE] PASSED&#13;
test_long_lines.py::TestLongLines::test_detects_truish_strings[TruE] PASSED&#13;
test_long_lines.py::TestLongLines::test_detects_truish_strings[True] PASSED&#13;
test_long_lines.py::TestLongLines::test_detects_truish_strings[true] PASSED</pre>&#13;
&#13;
<p>The output includes the values used in each iteration of the <em>single test</em> in brackets. It is reducing the very verbose test class into a single test method, thanks to <code>parametrize</code>. The next time you find yourself writing tests that seem very similar and that assert the same outcome with different inputs, you will know that you can make it simpler with the <code>parametrize</code> decorator.<a data-startref="ix_ch08-asciidoc5" data-type="indexterm" id="idm46691327154728"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Fixtures" data-type="sect1"><div class="sect1" id="idm46691327153864">&#13;
<h1>Fixtures</h1>&#13;
&#13;
<p><a data-primary="pytest" data-secondary="fixtures" data-type="indexterm" id="ix_ch08-asciidoc6"/>We think of <a href="https://oreil.ly/gPoM5"><code>pytest</code> fixtures</a> like little helpers that can get injected into a test. Regardless of whether you are writing a single test function or a bunch of test methods, fixtures can be used in the same way. If they aren’t going to be shared among other test files, it is fine to define them in the same test file; otherwise they can go into the <em>conftest.py</em> file. Fixtures, just like helper functions, can be almost anything you need for a test, from simple data structures that get pre-created to more complex ones like setting a database for a web application.</p>&#13;
&#13;
<p>These helpers can also have a defined <em>scope</em>. They can have specific code that cleans up for every test method, class, and module, or even allows setting them up once for the whole test session. By defining them in a test method (or test function), you are effectively getting the fixture injected at runtime. If this sounds a bit confusing, it will become clear through examples in the next few sections.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Getting Started" data-type="sect2"><div class="sect2" id="idm46691327147784">&#13;
<h2>Getting Started</h2>&#13;
&#13;
<p>Fixtures are so easy to define and use that they are often abused. We know we’ve created a few that could have been simple helper methods! As we’ve mentioned already, there are many different use cases for fixtures—from simple data structures to more complex ones, such as setting up whole databases for a single test.</p>&#13;
&#13;
<p>Recently, Alfredo had to test a small application that parses the contents of a particular file called a <em>keyring file</em>. It has some structure similar to an INI file, with some values that have to be unique and follow a specific format. The file structure can be very tedious to recreate on every test, so a fixture was created to help. This is how the keyring file looks:</p>&#13;
&#13;
<pre data-type="programlisting">[mon.]&#13;
    key = AQBvaBFZAAAAABAA9VHgwCg3rWn8fMaX8KL01A==&#13;
    caps mon = "allow *"</pre>&#13;
&#13;
<p>The fixture is a function that returns the contents of the keyring file. Let’s create a new file called test_keyring.py with the contents of the fixture, and a small test function that verifies the default key:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">pytest</code>&#13;
<code class="kn">import</code> <code class="nn">random</code>&#13;
&#13;
<code class="nd">@pytest.fixture</code>&#13;
<code class="k">def</code> <code class="nf">mon_keyring</code><code class="p">():</code>&#13;
    <code class="k">def</code> <code class="nf">make_keyring</code><code class="p">(</code><code class="n">default</code><code class="o">=</code><code class="bp">False</code><code class="p">):</code>&#13;
        <code class="k">if</code> <code class="n">default</code><code class="p">:</code>&#13;
            <code class="n">key</code> <code class="o">=</code> <code class="s2">"AQBvaBFZAAAAABAA9VHgwCg3rWn8fMaX8KL01A=="</code>&#13;
        <code class="k">else</code><code class="p">:</code>&#13;
            <code class="n">key</code> <code class="o">=</code> <code class="s2">"</code><code class="si">%032x</code><code class="s2">=="</code> <code class="o">%</code> <code class="n">random</code><code class="o">.</code><code class="n">getrandbits</code><code class="p">(</code><code class="mi">128</code><code class="p">)</code>&#13;
&#13;
        <code class="k">return</code> <code class="s2">"""</code>&#13;
<code class="s2">    [mon.]</code>&#13;
<code class="s2">        key = </code><code class="si">%s</code><code class="s2"/>&#13;
<code class="s2">            caps mon = "allow *"</code>&#13;
<code class="s2">        """</code> <code class="o">%</code> <code class="n">key</code>&#13;
    <code class="k">return</code> <code class="n">make_keyring</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">test_default_key</code><code class="p">(</code><code class="n">mon_keyring</code><code class="p">):</code>&#13;
    <code class="n">contents</code> <code class="o">=</code> <code class="n">mon_keyring</code><code class="p">(</code><code class="n">default</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>&#13;
    <code class="k">assert</code> <code class="s2">"AQBvaBFZAAAAABAA9VHgwCg3rWn8fMaX8KL01A=="</code> <code class="ow">in</code> <code class="n">contents</code></pre>&#13;
&#13;
<p>The fixture is using a nested function that does the heavy lifting, allows using a <em>default</em> key value, and returns the nested function in case the caller wants to have a randomized key. Inside the test, it receives the fixture by declaring it part of the argument of the test function (<code>mon_keyring</code> in this case), and is calling the fixture with <code>default=True</code> so that the default key is used, and then verifying it is generated as expected.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>In a real-world scenario, the generated contents would be passed to the parser, ensuring expected behavior after parsing and that no errors happen.</p>&#13;
</div>&#13;
&#13;
<p>The production code that used this fixture eventually grew to do other kinds of testing, and at some point, the test wanted to verify that the parser could handle files in different conditions. The fixture was returning a string, so it needed extending. Existing tests already made use of the <code>mon_keyring</code> fixture, so to extend the functionality without altering the current fixture, a new one was created that used a feature from the framework. Fixtures can <em>request</em> other fixtures! You define the required fixture as an argument (like a test function or test method would), so the framework injects it when it gets executed.</p>&#13;
&#13;
<p>This is how the new fixture that creates (and returns) the file looks:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="nd">@pytest.fixture</code>&#13;
<code class="k">def</code> <code class="nf">keyring_file</code><code class="p">(</code><code class="n">mon_keyring</code><code class="p">,</code> <code class="n">tmpdir</code><code class="p">):</code>&#13;
    <code class="k">def</code> <code class="nf">generate_file</code><code class="p">(</code><code class="n">default</code><code class="o">=</code><code class="bp">False</code><code class="p">):</code>&#13;
        <code class="n">keyring</code> <code class="o">=</code> <code class="n">tmpdir</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="s1">'keyring'</code><code class="p">)</code>&#13;
        <code class="n">keyring</code><code class="o">.</code><code class="n">write_text</code><code class="p">(</code><code class="n">mon_keyring</code><code class="p">(</code><code class="n">default</code><code class="o">=</code><code class="n">default</code><code class="p">))</code>&#13;
        <code class="k">return</code> <code class="n">keyring</code><code class="o">.</code><code class="n">strpath</code>&#13;
    <code class="k">return</code> <code class="n">generate_file</code></pre>&#13;
&#13;
<p>Going line by line, the <code>pytest.fixture</code> decorator tells the framework that this function is a fixture, then the fixture is defined, asking for <em>two fixtures</em> as arguments: <code>mon_keyring</code> and <code>tmpdir</code>. The first is the one created previously in the  <em>test_keyring.py</em> file earlier, and the second one is a built-in fixture from the framework (more on built-in fixtures in the next section). The <code>tmpdir</code> fixture allows you to use a <span class="keep-together">temporary</span> directory that gets removed after the test completes, then the <em>keyring</em> file is created, and the text generated by the <code>mon_keyring</code> fixture is written, passing the <code>default</code> argument. Finally, it returns the absolute path of the new file created so that the test can use it.</p>&#13;
&#13;
<p>This is how the test function would use it:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">test_keyring_file_contents</code><code class="p">(</code><code class="n">keyring_file</code><code class="p">):</code>&#13;
    <code class="n">keyring_path</code> <code class="o">=</code> <code class="n">keyring_file</code><code class="p">(</code><code class="n">default</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>&#13;
    <code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="n">keyring_path</code><code class="p">)</code> <code class="k">as</code> <code class="n">fp</code><code class="p">:</code>&#13;
        <code class="n">contents</code> <code class="o">=</code> <code class="n">fp</code><code class="o">.</code><code class="n">read</code><code class="p">()</code>&#13;
    <code class="k">assert</code> <code class="s2">"AQBvaBFZAAAAABAA9VHgwCg3rWn8fMaX8KL01A=="</code> <code class="ow">in</code> <code class="n">contents</code></pre>&#13;
&#13;
<p>You should now have a good idea of what fixtures are, where can you define them, and how to consume them in tests. The next section goes through a few of the most useful built-in fixtures that are part of the framework.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Built-in Fixtures" data-type="sect2"><div class="sect2" id="idm46691327147160">&#13;
<h2>Built-in Fixtures</h2>&#13;
&#13;
<p><a data-primary="pytest" data-secondary="built-in fixtures" data-type="indexterm" id="ix_ch08-asciidoc7"/>The previous section briefly touched on one of the many built-in fixtures that <code>pytest</code> has to offer: the <code>tmpdir</code> fixture. The framework provides a few more fixtures. To verify the full list of available fixtures, run the following command:</p>&#13;
&#13;
<pre data-type="programlisting">$ (testing) pytest  -q --fixtures</pre>&#13;
&#13;
<p>There are two fixtures that we use a lot: <a data-primary="monkeypatch" data-type="indexterm" id="idm46691326911336"/><code>monkeypatch</code> and <a data-primary="capsys" data-type="indexterm" id="idm46691326825144"/><code>capsys</code>, and they are in the list produced when the above command is run. This is the brief description you will see in the terminal:</p>&#13;
&#13;
<pre data-type="programlisting">capsys&#13;
    enables capturing of writes to sys.stdout/sys.stderr and makes&#13;
    captured output available via ``capsys.readouterr()`` method calls&#13;
    which return a ``(out, err)`` tuple.&#13;
monkeypatch&#13;
    The returned ``monkeypatch`` funcarg provides these&#13;
    helper methods to modify objects, dictionaries or os.environ::&#13;
&#13;
    monkeypatch.setattr(obj, name, value, raising=True)&#13;
    monkeypatch.delattr(obj, name, raising=True)&#13;
    monkeypatch.setitem(mapping, name, value)&#13;
    monkeypatch.delitem(obj, name, raising=True)&#13;
    monkeypatch.setenv(name, value, prepend=False)&#13;
    monkeypatch.delenv(name, value, raising=True)&#13;
    monkeypatch.syspath_prepend(path)&#13;
    monkeypatch.chdir(path)&#13;
&#13;
    All modifications will be undone after the requesting&#13;
    test function has finished. The ``raising``&#13;
    parameter determines if a KeyError or AttributeError&#13;
    will be raised if the set/deletion operation has no target.</pre>&#13;
&#13;
<p><code>capsys</code> captures any <code>stdout</code> or <code>stderr</code> produced in a test. Have you ever tried to verify some command output or logging in a unit test? It is challenging to get right and is something that requires a separate plug-in or library to <em>patch</em> Python’s internals and then inspect its contents.</p>&#13;
&#13;
<p>These are two test functions that verify the output produced on <code>stderr</code> and <code>stdout</code>, respectively:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">sys</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">stderr_logging</code><code class="p">():</code>&#13;
    <code class="n">sys</code><code class="o">.</code><code class="n">stderr</code><code class="o">.</code><code class="n">write</code><code class="p">(</code><code class="s1">'stderr output being produced'</code><code class="p">)</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">stdout_logging</code><code class="p">():</code>&#13;
    <code class="n">sys</code><code class="o">.</code><code class="n">stdout</code><code class="o">.</code><code class="n">write</code><code class="p">(</code><code class="s1">'stdout output being produced'</code><code class="p">)</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">test_verify_stderr</code><code class="p">(</code><code class="n">capsys</code><code class="p">):</code>&#13;
    <code class="n">stderr_logging</code><code class="p">()</code>&#13;
    <code class="n">out</code><code class="p">,</code> <code class="n">err</code> <code class="o">=</code> <code class="n">capsys</code><code class="o">.</code><code class="n">readouterr</code><code class="p">()</code>&#13;
    <code class="k">assert</code> <code class="n">out</code> <code class="o">==</code> <code class="s1">''</code>&#13;
    <code class="k">assert</code> <code class="n">err</code> <code class="o">==</code> <code class="s1">'stderr output being produced'</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">test_verify_stdout</code><code class="p">(</code><code class="n">capsys</code><code class="p">):</code>&#13;
    <code class="n">stdout_logging</code><code class="p">()</code>&#13;
    <code class="n">out</code><code class="p">,</code> <code class="n">err</code> <code class="o">=</code> <code class="n">capsys</code><code class="o">.</code><code class="n">readouterr</code><code class="p">()</code>&#13;
    <code class="k">assert</code> <code class="n">out</code> <code class="o">==</code> <code class="s1">'stdout output being produced'</code>&#13;
    <code class="k">assert</code> <code class="n">err</code> <code class="o">==</code> <code class="s1">''</code></pre>&#13;
&#13;
<p>The <code>capsys</code> fixture handles all the patching, setup, and helpers to retrieve the <code>stderr</code> and <code>stdout</code> produced in the test. The content is reset for every test, which ensures that the variables populate with the correct output.</p>&#13;
&#13;
<p><code>monkeypatch</code> is probably the fixture that we use the most. When testing, there are situations where the code under test is out of our control, and <em>patching</em> needs to happen to override a module or function to have a specific behavior. There are quite a few <em>patching</em> and <em>mocking</em> libraries (<em>mocks</em> are helpers to set behavior on patched objects) available for Python, but <code>monkeypatch</code> is good enough that you might not need to install a separate library to help out.</p>&#13;
&#13;
<p>The following function runs a system command to capture details from a device, then parses the output, and returns a property (the <code>ID_PART_ENTRY_TYPE</code> as reported by <code>blkid</code>):</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">subprocess</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">get_part_entry_type</code><code class="p">(</code><code class="n">device</code><code class="p">):</code>&#13;
    <code class="sd">"""</code>&#13;
<code class="sd">    Parses the ``ID_PART_ENTRY_TYPE`` from the "low level" (bypasses the cache)</code>&#13;
<code class="sd">    output that uses the ``udev`` type of output.</code>&#13;
<code class="sd">    """</code>&#13;
    <code class="n">stdout</code> <code class="o">=</code> <code class="n">subprocess</code><code class="o">.</code><code class="n">check_output</code><code class="p">([</code><code class="s1">'blkid'</code><code class="p">,</code> <code class="s1">'-p'</code><code class="p">,</code> <code class="s1">'-o'</code><code class="p">,</code> <code class="s1">'udev'</code><code class="p">,</code> <code class="n">device</code><code class="p">])</code>&#13;
    <code class="k">for</code> <code class="n">line</code> <code class="ow">in</code> <code class="n">stdout</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="s1">'</code><code class="se">\n</code><code class="s1">'</code><code class="p">):</code>&#13;
        <code class="k">if</code> <code class="s1">'ID_PART_ENTRY_TYPE='</code> <code class="ow">in</code> <code class="n">line</code><code class="p">:</code>&#13;
            <code class="k">return</code> <code class="n">line</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="s1">'='</code><code class="p">)[</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">strip</code><code class="p">()</code>&#13;
    <code class="k">return</code> <code class="s1">''</code></pre>&#13;
&#13;
<p>To test it, set the desired behavior on the <code>check_output</code> attribute of the <code>subprocess</code> module. This is how the test function looks using the <code>monkeypatch</code> fixture:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">test_parses_id_entry_type</code><code class="p">(</code><code class="n">monkeypatch</code><code class="p">):</code>&#13;
    <code class="n">monkeypatch</code><code class="o">.</code><code class="n">setattr</code><code class="p">(</code>&#13;
        <code class="s1">'subprocess.check_output'</code><code class="p">,</code>&#13;
        <code class="k">lambda</code> <code class="n">cmd</code><code class="p">:</code> <code class="s1">'</code><code class="se">\n</code><code class="s1">ID_PART_ENTRY_TYPE=aaaaa'</code><code class="p">)</code>&#13;
    <code class="k">assert</code> <code class="n">get_part_entry_type</code><code class="p">(</code><code class="s1">'/dev/sda'</code><code class="p">)</code> <code class="o">==</code> <code class="s1">'aaaa'</code></pre>&#13;
&#13;
<p>The <a data-primary="setattr" data-type="indexterm" id="idm46691326584152"/><code>setattr</code> call <em>sets the attribute</em> on the patched callable (<code>check_output</code> in this case). It <em>patches</em> it with a lambda function that returns the one interesting line. Since the <code>subprocess.check_output</code> function is not under our direct control, and the <code>get_part_entry_type</code> function doesn’t allow any other way to inject the values, patching is the only way.</p>&#13;
&#13;
<p>We tend to favor using other techniques like injecting values (known as <em>dependency injection</em>) before attempting to patch, but sometimes there is no other way. Providing a library that can patch and handle all the cleanup on testing is one more reason <code>pytest</code> is a joy to work with<a data-startref="ix_ch08-asciidoc7" data-type="indexterm" id="idm46691326579752"/>.<a data-startref="ix_ch08-asciidoc6" data-type="indexterm" id="idm46691326578888"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Infrastructure Testing" data-type="sect1"><div class="sect1" id="idm46691326915640">&#13;
<h1>Infrastructure Testing</h1>&#13;
&#13;
<p><a data-primary="infrastructure testing" data-type="indexterm" id="ix_ch08-asciidoc8"/><a data-primary="pytest" data-secondary="infrastructure testing" data-type="indexterm" id="ix_ch08-asciidoc9"/>This section explains how to do infrastructure testing and validation with the <a href="https://oreil.ly/e7Afx">Testinfra project</a>. It is a <code>pytest</code> plug-in for infrastructure that relies heavily on fixtures and  allows you to write Python tests as if testing code.</p>&#13;
&#13;
<p>The previous sections went into some detail on <code>pytest</code> usage and examples, and this chapter started with the idea of verification at a system level. The way we explain infrastructure testing is by asking a question: <em>How can you tell that the deployment was successful?</em> Most of the time, this means some manual checks, such as loading a website or looking at processes, which is insufficient; it is error-prone and can get tedious if the system is significant.</p>&#13;
&#13;
<p>Although you can initially get introduced to <code>pytest</code> as a tool to write and run Python unit tests, it can be advantageous to repurpose it for infrastructure testing. A few years ago Alfredo was tasked to produce an installer that exposed its features over an HTTP API. This installer was to create a <a href="https://ceph.com">Ceph cluster</a>, involving many machines. During the QA portion of launching the API, it was common to get reports where the cluster wouldn’t work as expected, so he would get the credentials to log in to these machines and inspect them. There is a multiplier effect once you have to debug a distributed system comprising several machines: multiple <span class="keep-together">configuration</span> files, different hard drives, network setups, anything and everything can be different even if they appear to be similar.</p>&#13;
&#13;
<p>Every time Alfredo had to debug these systems, he had an ever-growing list of things to check. Is the configuration the same on all servers? Are the permissions as expected? Does a specific user exist? He would eventually forget something and spend time trying to figure out what he was missing. It was an unsustainable process. <em>What if I could write simple test cases against the cluster?</em> Alfredo wrote a few simple tests to verify the items on the list to execute them against the machines making up the cluster. Before he knew it, he had a good set of tests that took a few seconds to run that would identify all kinds of issues.</p>&#13;
&#13;
<p>That was an incredible eye-opener for improving the delivery process. He could even execute these (functional) tests while developing the installer and catch things that weren’t quite right. If the QA team caught any issues, he could run the same tests against their setup. Sometimes tests caught environmental issues: a drive was <em>dirty</em> and caused the deployment to fail; a configuration file from a different cluster was left behind and caused issues. Automation, granular tests, and the ability to run them often made the work better and alleviated the amount of work the QA team had to put up with.</p>&#13;
&#13;
<p>The TestInfra project has all kinds of fixtures to test a system efficiently, and it includes a complete set of backends to connect to servers; regardless of their deployment type: Ansible, Docker, SSH, and Kubernetes are some of the supported connections. By supporting many different connection backends, you can execute the same set of tests regardless of infrastructure changes.</p>&#13;
&#13;
<p>The next sections go through different backends and get into examples of a production project.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="What Is System Validation?" data-type="sect2"><div class="sect2" id="idm46691326598520">&#13;
<h2>What Is System Validation?</h2>&#13;
&#13;
<p><a data-primary="infrastructure testing" data-secondary="system validation" data-type="indexterm" id="idm46691326597112"/><a data-primary="pytest" data-secondary="system validation" data-type="indexterm" id="idm46691326596136"/><a data-primary="system validation" data-type="indexterm" id="idm46691326595192"/>System validation can happen at different levels (with monitoring and alert systems) and at different stages in the life cycle of an application, such as during pre-deployment, at runtime, or during deployment. An application that Alfredo recently put into production needed to handle client connections gracefully without any disruption, even when restarted. To sustain traffic, the application is load balanced: when the system is under heavy loads, new connections get sent to other servers with a lighter load.</p>&#13;
&#13;
<p>When a new release gets deployed, the application <em>has to be restarted</em>. Restarting means that clients experience an odd behavior at best, or a very broken experience at the worst. To avoid this, the restart process waits for all client connections to terminate, the system refuses new connections, allowing it to finish work from existing <span class="keep-together">clients,</span> and the rest of the system picks up the work. When no connections are active, the deployment continues and stops services to get the newer code in.</p>&#13;
&#13;
<p>There is validation at every step of the way: before the deployment to tell the balancer to stop sending new clients and later, verifying that no new clients are active. If that workflow converts to a test, the title could be something like: <code>make sure that no clients are currently running</code>. Once the new code is in, another validation step checks whether the balancer has acknowledged that the server is ready to produce work once again. Another test here could be: <code>balancer has server as active</code>. Finally, it makes sure that the server is receiving new client connections—yet another test to write!</p>&#13;
&#13;
<p>Throughout these steps, verification is in place, and tests can be written to verify this type of workflow.</p>&#13;
&#13;
<p>System validation can also be tied to monitoring the overall health of a server (or servers in a clustered environment) or be part of the continuous integration while developing the application and testing functionally. The basics of validation apply to these situations and anything else that might benefit from status verification. It shouldn’t be used exclusively for testing, although that is a good start!</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Introduction to Testinfra" data-type="sect2"><div class="sect2" id="idm46691326495720">&#13;
<h2>Introduction to Testinfra</h2>&#13;
&#13;
<p><a data-primary="infrastructure testing" data-secondary="Testinfra" data-type="indexterm" id="idm46691326494472"/><a data-primary="pytest" data-secondary="Testinfra" data-type="indexterm" id="idm46691326493624"/><a data-primary="Testinfra" data-type="indexterm" id="idm46691326492680"/>Writing unit tests against infrastructure is a powerful concept, and having used Testinfra for over a year, we can say that it has improved the quality of production applications we’ve had to deliver. The following sections go into specifics, such as connecting to different nodes and executing validation tests, and explore what type of fixtures are available.</p>&#13;
&#13;
<p>To create a new virtual environment, install <code>pytest</code>:</p>&#13;
&#13;
<pre data-type="programlisting">$ python3 -m venv validation&#13;
$ source testing/bin/activate&#13;
(validation) $ pip install pytest</pre>&#13;
&#13;
<p>Install <code>testinfra</code>, ensuring that version <code>2.1.0</code> is used:</p>&#13;
&#13;
<pre data-type="programlisting">(validation) $ pip install "testinfra==2.1.0"</pre>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p><code>pytest</code> fixtures provide all the test functionality offered by the Testinfra project. To take advantage of this section, you will need to know how they work.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Connecting to Remote Nodes" data-type="sect2"><div class="sect2" id="idm46691326485928">&#13;
<h2>Connecting to Remote Nodes</h2>&#13;
&#13;
<p><a data-primary="infrastructure testing" data-secondary="connecting to remote nodes" data-type="indexterm" id="ix_ch08-asciidoc10"/><a data-primary="pytest" data-secondary="connecting to remote nodes" data-type="indexterm" id="ix_ch08-asciidoc11"/>Because different backend connection types exist, when the connection is not specified directly, Testinfra defaults to certain ones. It is better to be explicit about the connection type and define it in the command line.</p>&#13;
&#13;
<p>These are all the connection types that Testinfra supports:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>local</p>&#13;
</li>&#13;
<li>&#13;
<p>Paramiko (an SSH implementation in Python)</p>&#13;
</li>&#13;
<li>&#13;
<p>Docker</p>&#13;
</li>&#13;
<li>&#13;
<p>SSH</p>&#13;
</li>&#13;
<li>&#13;
<p>Salt</p>&#13;
</li>&#13;
<li>&#13;
<p>Ansible</p>&#13;
</li>&#13;
<li>&#13;
<p>Kubernetes (via kubectl)</p>&#13;
</li>&#13;
<li>&#13;
<p>WinRM</p>&#13;
</li>&#13;
<li>&#13;
<p>LXC</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>A <code>testinfra</code> section appears in the help menu with some context on the flags that are provided. This is a neat feature from <code>pytest</code> and its integration with Testinfra. The help for both projects comes from the same command:</p>&#13;
&#13;
<pre data-type="programlisting">(validation) $ pytest --help&#13;
...&#13;
&#13;
testinfra:&#13;
  --connection=CONNECTION&#13;
                        Remote connection backend (paramiko, ssh, safe-ssh,&#13;
                        salt, docker, ansible)&#13;
  --hosts=HOSTS         Hosts list (comma separated)&#13;
  --ssh-config=SSH_CONFIG&#13;
                        SSH config file&#13;
  --ssh-identity-file=SSH_IDENTITY_FILE&#13;
                        SSH identify file&#13;
  --sudo                Use sudo&#13;
  --sudo-user=SUDO_USER&#13;
                        sudo user&#13;
  --ansible-inventory=ANSIBLE_INVENTORY&#13;
                        Ansible inventory file&#13;
  --nagios              Nagios plugin</pre>&#13;
&#13;
<p>There are two servers up and running. To demonstrate the connection options, let’s check if they are running CentOS 7 by poking inside the <em>/etc/os-release</em> file. This is how the test function looks (saved as <code>test_remote.py</code>):</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">test_release_file</code><code class="p">(</code><code class="n">host</code><code class="p">):</code>&#13;
    <code class="n">release_file</code> <code class="o">=</code> <code class="n">host</code><code class="o">.</code><code class="n">file</code><code class="p">(</code><code class="s2">"/etc/os-release"</code><code class="p">)</code>&#13;
    <code class="k">assert</code> <code class="n">release_file</code><code class="o">.</code><code class="n">contains</code><code class="p">(</code><code class="s1">'CentOS'</code><code class="p">)</code>&#13;
    <code class="k">assert</code> <code class="n">release_file</code><code class="o">.</code><code class="n">contains</code><code class="p">(</code><code class="s1">'VERSION="7 (Core)"'</code><code class="p">)</code></pre>&#13;
&#13;
<p>It is a single test function that accepts the <code>host</code> fixture, which runs against all the nodes specified.</p>&#13;
&#13;
<p>The <code>--hosts</code> flag accepts a list of hosts with a connection scheme (SSH would use <code><em>ssh://hostname</em></code> for example), and some other variations using globbing are allowed. If testing against more than a couple of remote servers at a time, passing them on the command line becomes cumbersome. This is how it would look to test against two servers using SSH:</p>&#13;
&#13;
<pre data-type="programlisting">(validation) $ pytest -v --hosts='ssh://node1,ssh://node2' test_remote.py&#13;
============================= test session starts =============================&#13;
platform linux -- Python 3.6.8, pytest-4.4.1, py-1.8.0, pluggy-0.9.0&#13;
cachedir: .pytest_cache&#13;
rootdir: /home/alfredo/python/python-devops/samples/chapter16&#13;
plugins: testinfra-3.0.0, xdist-1.28.0, forked-1.0.2&#13;
collected 2 items&#13;
&#13;
test_remote.py::test_release_file[ssh://node1] PASSED                   [ 50%]&#13;
test_remote.py::test_release_file[ssh://node2] PASSED                   [100%]&#13;
&#13;
========================== 2 passed in 3.82 seconds ===========================</pre>&#13;
&#13;
<p>The increased verbosity (with the <code>-v</code> flag) shows that Testinfra is executing the one test function in the two remote servers specified in the invocation.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>When setting up the hosts, it is important to have a passwordless connection. There shouldn’t be any password prompts, and if using SSH, a key-based configuration should be used.</p>&#13;
</div>&#13;
&#13;
<p>When automating these types of tests (as part of a job in a CI system, for example), you can benefit from generating the hosts, determining how they connect, and any other special directives. Testinfra can consume an SSH configuration file to determine what hosts to connect to. For the previous test run, <a href="https://www.vagrantup.com">Vagrant</a> was used, which created these servers with special keys and connection settings. Vagrant can generate an ad-hoc SSH config file for the servers it has created:</p>&#13;
&#13;
<pre data-type="programlisting">(validation) $ vagrant ssh-config&#13;
&#13;
Host node1&#13;
  HostName 127.0.0.1&#13;
  User vagrant&#13;
  Port 2200&#13;
  UserKnownHostsFile /dev/null&#13;
  StrictHostKeyChecking no&#13;
  PasswordAuthentication no&#13;
  IdentityFile /home/alfredo/.vagrant.d/insecure_private_key&#13;
  IdentitiesOnly yes&#13;
  LogLevel FATAL&#13;
&#13;
Host node2&#13;
  HostName 127.0.0.1&#13;
  User vagrant&#13;
  Port 2222&#13;
  UserKnownHostsFile /dev/null&#13;
  StrictHostKeyChecking no&#13;
  PasswordAuthentication no&#13;
  IdentityFile /home/alfredo/.vagrant.d/insecure_private_key&#13;
  IdentitiesOnly yes&#13;
  LogLevel FATAL</pre>&#13;
&#13;
<p>Exporting the contents of the output to a file and then passing that to Testinfra offers greater flexibility if using more than one host:</p>&#13;
&#13;
<pre data-type="programlisting">(validation) $ vagrant ssh-config &gt; ssh-config&#13;
(validation) $ pytest --hosts=default --ssh-config=ssh-config test_remote.py</pre>&#13;
&#13;
<p>Using <code>--hosts=default</code> avoids having to specify them directly in the command line, and the engine feeds from the SSH configuration. Even without Vagrant, the SSH configuration tip is still useful if connecting to many hosts with specific directives.</p>&#13;
&#13;
<p><a href="https://www.ansible.com">Ansible</a> is another option if the nodes are local, SSH, or Docker containers. The test setup can benefit from using an inventory of hosts (much like the SSH config), which can group the hosts into different sections. The host groups can also be specified so that you can single out hosts to test against, instead of executing against all.</p>&#13;
&#13;
<p>For <code>node1</code> and <code>node2</code> used in the previous example, this is how the inventory file is defined (and saved as <code>hosts</code>):</p>&#13;
&#13;
<pre data-type="programlisting">[all]&#13;
node1&#13;
node2</pre>&#13;
&#13;
<p>If executing against all of them, the command changes to:</p>&#13;
&#13;
<pre data-type="programlisting">$ pytest --connection=ansible --ansible-inventory=hosts test_remote.py</pre>&#13;
&#13;
<p>If defining other hosts in the inventory that need an exclusion, a group can be specified as well. Assuming that both nodes are web servers and are in the <code>nginx</code> group, this command would run the tests on only that one group:</p>&#13;
&#13;
<pre data-type="programlisting">$ pytest --hosts='ansible://nginx' --connection=ansible \&#13;
  --ansible-inventory=hosts test_remote.py</pre>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>A lot of system commands require superuser privileges. To allow escalation of privileges, Testinfra allows specifying <code>--sudo</code> or <code>--sudo-user</code>. The <code>--sudo</code> flag makes the engine use <code>sudo</code> when executing the commands, while the <code>--sudo-user</code> command allows running with higher privileges as a different user.The fixture can be used directly as well.<a data-startref="ix_ch08-asciidoc11" data-type="indexterm" id="idm46691326425144"/><a data-startref="ix_ch08-asciidoc10" data-type="indexterm" id="idm46691326424440"/></p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Features and Special Fixtures" data-type="sect2"><div class="sect2" id="idm46691326485016">&#13;
<h2>Features and Special Fixtures</h2>&#13;
&#13;
<p><a data-primary="infrastructure testing" data-secondary="features and special fixtures" data-type="indexterm" id="idm46691326422744"/><a data-primary="pytest" data-secondary="features" data-type="indexterm" id="idm46691326421576"/><a data-primary="pytest" data-secondary="fixtures" data-type="indexterm" id="idm46691326420632"/>So far, the <code>host</code> fixture is the only one used in examples to check for a file and its contents. However, this is deceptive. The <code>host</code> fixture is an <em>all-included</em> fixture; it contains all the other powerful fixtures that Testinfra provides. This means that the example has already used the <code>host.file</code>, which has lots of extras packed in it. It is also possible to use the fixture directly:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">1</code><code class="p">]:</code> <code class="kn">import</code> <code class="nn">testinfra</code>&#13;
&#13;
<code class="n">In</code> <code class="p">[</code><code class="mi">2</code><code class="p">]:</code> <code class="n">host</code> <code class="o">=</code> <code class="n">testinfra</code><code class="o">.</code><code class="n">get_host</code><code class="p">(</code><code class="s1">'local://'</code><code class="p">)</code>&#13;
&#13;
<code class="n">In</code> <code class="p">[</code><code class="mi">3</code><code class="p">]:</code> <code class="n">node_file</code> <code class="o">=</code> <code class="n">host</code><code class="o">.</code><code class="n">file</code><code class="p">(</code><code class="s1">'/tmp'</code><code class="p">)</code>&#13;
&#13;
<code class="n">In</code> <code class="p">[</code><code class="mi">4</code><code class="p">]:</code> <code class="n">node_file</code><code class="o">.</code><code class="n">is_directory</code>&#13;
<code class="n">Out</code><code class="p">[</code><code class="mi">4</code><code class="p">]:</code> <code class="bp">True</code>&#13;
&#13;
<code class="n">In</code> <code class="p">[</code><code class="mi">5</code><code class="p">]:</code> <code class="n">node_file</code><code class="o">.</code><code class="n">user</code>&#13;
<code class="n">Out</code><code class="p">[</code><code class="mi">5</code><code class="p">]:</code> <code class="s1">'root'</code></pre>&#13;
&#13;
<p>The all-in-one <code>host</code> fixture makes use of the extensive API from Testinfra, which loads everything for each host it connects to. The idea is to write a single test that gets executed against different nodes, all accessible from the same <code>host</code> fixture.</p>&#13;
&#13;
<p>These are a <a href="https://oreil.ly/2_J-o">couple dozen</a> attributes available. These are some of the most used ones:</p>&#13;
<dl>&#13;
<dt><code>host.ansible</code></dt>&#13;
<dd>&#13;
<p><a data-primary="host.ansible" data-type="indexterm" id="idm46691326328328"/>Provides full access to any of the Ansible properties at runtime, such as hosts, inventory, and vars</p>&#13;
</dd>&#13;
<dt><code>host.addr</code></dt>&#13;
<dd>&#13;
<p><a data-primary="host.addr" data-type="indexterm" id="idm46691326326104"/>Network utilities, like checks for IPV4 and IPV6, is host reachable, is host <span class="keep-together">resolvable</span></p>&#13;
</dd>&#13;
<dt><code>host.docker</code></dt>&#13;
<dd>&#13;
<p><a data-primary="host.docker" data-type="indexterm" id="idm46691326323432"/>Proxy to the Docker API, allows interacting with containers, and checks if they are running</p>&#13;
</dd>&#13;
<dt><code>host.interface</code></dt>&#13;
<dd>&#13;
<p><a data-primary="host.interface" data-type="indexterm" id="idm46691326321208"/>Helpers for inspecting addresses from a given interface</p>&#13;
</dd>&#13;
<dt><code>host.iptables</code></dt>&#13;
<dd>&#13;
<p><a data-primary="host.iptables" data-type="indexterm" id="idm46691326319032"/>Helpers for verifying firewall rules as seen by <code>host.iptables</code></p>&#13;
</dd>&#13;
<dt><code>host.mount_point</code></dt>&#13;
<dd>&#13;
<p><a data-primary="host.mount_point" data-type="indexterm" id="idm46691326316568"/>Check mounts, filesystem types as they exist in paths, and mount options</p>&#13;
</dd>&#13;
<dt><code>host.package</code></dt>&#13;
<dd>&#13;
<p><a data-primary="host.package" data-type="indexterm" id="idm46691326314456"/><em>Very useful</em> to query if a package is installed and at what version</p>&#13;
</dd>&#13;
<dt><code>host.process</code></dt>&#13;
<dd>&#13;
<p><a data-primary="host.process" data-type="indexterm" id="idm46691326311992"/>Check for running processes</p>&#13;
</dd>&#13;
<dt><code>host.sudo</code></dt>&#13;
<dd>&#13;
<p><a data-primary="host.sudo" data-type="indexterm" id="idm46691326309880"/>Allows you to execute commands with <code>host.sudo</code> or as a different user</p>&#13;
</dd>&#13;
<dt><code>host.system_info</code></dt>&#13;
<dd>&#13;
<p><a data-primary="host.system_info" data-type="indexterm" id="idm46691326307320"/>All kinds of system metadata, such as distribution version, release, and codename</p>&#13;
</dd>&#13;
<dt><code>host.check_output</code></dt>&#13;
<dd>&#13;
<p><a data-primary="host.check_output" data-type="indexterm" id="idm46691326305208"/>Runs a system command, checks its output if runs successfully, and can be used in combination with <code>host.sudo</code></p>&#13;
</dd>&#13;
<dt><code>host.run</code></dt>&#13;
<dd>&#13;
<p><a data-primary="host.run" data-type="indexterm" id="idm46691326302696"/>Runs a command, allows you to check the return code, <code>host.stderr</code>, and <code>host.stdout</code></p>&#13;
</dd>&#13;
<dt><code>host.run_expect</code></dt>&#13;
<dd>&#13;
<p><a data-primary="host.run_expect" data-type="indexterm" id="idm46691326299784"/>Verifies that the return code is as expected<a data-startref="ix_ch08-asciidoc9" data-type="indexterm" id="idm46691326298888"/><a data-startref="ix_ch08-asciidoc8" data-type="indexterm" id="idm46691326298216"/></p>&#13;
</dd>&#13;
</dl>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Examples" data-type="sect1"><div class="sect1" id="idm46691326577592">&#13;
<h1>Examples</h1>&#13;
&#13;
<p><a data-primary="pytest" data-secondary="examples" data-type="indexterm" id="ix_ch08-asciidoc12"/>A frictionless way to start developing system validation tests is to do so while creating the actual deployment. Somewhat similar to <em>Test Driven Development</em> (TDD), any progress warrants a new test. In this section, a web server needs to be installed and configured to run on port 80 to serve a static landing page. While making progress, tests will be added. Part of writing tests is understanding failures, so a few problems will be introduced to help us figure out what to fix.</p>&#13;
&#13;
<p>With a <em>vanilla</em> Ubuntu server, start by installing the <a data-primary="Nginx package" data-type="indexterm" id="idm46691326292968"/>Nginx package:</p>&#13;
&#13;
<pre data-type="programlisting">$ apt install nginx</pre>&#13;
&#13;
<p>Create a new test file called <em>test_webserver.py</em> for adding new tests after making progress. After Nginx installs, let’s create another test:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">test_nginx_is_installed</code><code class="p">(</code><code class="n">host</code><code class="p">):</code>&#13;
    <code class="k">assert</code> <code class="n">host</code><code class="o">.</code><code class="n">package</code><code class="p">(</code><code class="s1">'nginx'</code><code class="p">)</code><code class="o">.</code><code class="n">is_installed</code></pre>&#13;
&#13;
<p>Reduce the verbosity in <code>pytest</code> output with the <code>-q</code> flag to concentrate on failures. The remote server is called <code>node4</code> and SSH is used to connect to it. This is the command to run the first test:</p>&#13;
&#13;
<pre data-type="programlisting">(validate) $ pytest -q --hosts='ssh://node4' test_webserver.py&#13;
.&#13;
1 passed in 1.44 seconds</pre>&#13;
&#13;
<p>Progress! The web server needs to be up and running, so a new test is added to verify that behavior:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">test_nginx_is_running</code><code class="p">(</code><code class="n">host</code><code class="p">):</code>&#13;
    <code class="k">assert</code> <code class="n">host</code><code class="o">.</code><code class="n">service</code><code class="p">(</code><code class="s1">'nginx'</code><code class="p">)</code><code class="o">.</code><code class="n">is_running</code></pre>&#13;
&#13;
<p>Running again <em>should</em> work once again:</p>&#13;
&#13;
<pre data-type="programlisting">(validate) $ pytest -q --hosts='ssh://node4' test_webserver.py&#13;
.F&#13;
================================== FAILURES ===================================&#13;
_____________________ test_nginx_is_running[ssh://node4] ______________________&#13;
&#13;
host = &lt;testinfra.host.Host object at 0x7f629bf1d668&gt;&#13;
&#13;
    def test_nginx_is_running(host):&#13;
&gt;       assert host.service('nginx').is_running&#13;
E       AssertionError: assert False&#13;
E        +  where False = &lt;service nginx&gt;.is_running&#13;
E        +    where &lt;service nginx&gt; = &lt;class 'SystemdService'&gt;('nginx')&#13;
&#13;
test_webserver.py:7: AssertionError&#13;
1 failed, 1 passed in 2.45 seconds</pre>&#13;
&#13;
<p>Some Linux distributions do not allow packages to start the services when they get installed. Moreover, the test has caught the Nginx service not running, as reported by <code>systemd</code> (the default unit service). Starting Nginx manually and running the test should make everything pass once again:</p>&#13;
&#13;
<pre data-type="programlisting">(validate) $ systemctl start nginx&#13;
(validate) $ pytest -q --hosts='ssh://node4' test_webserver.py&#13;
..&#13;
2 passed in 2.38 seconds</pre>&#13;
&#13;
<p>As mentioned at the beginning of this section, the web server should be serving a static landing page on port 80. Adding another test (in <em>test_webserver.py</em>) to verify the port is the next step:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">test_nginx_listens_on_port_80</code><code class="p">(</code><code class="n">host</code><code class="p">):</code>&#13;
    <code class="k">assert</code> <code class="n">host</code><code class="o">.</code><code class="n">socket</code><code class="p">(</code><code class="s2">"tcp://0.0.0.0:80"</code><code class="p">)</code><code class="o">.</code><code class="n">is_listening</code></pre>&#13;
&#13;
<p>This test is more involved and needs attention to some details. It opts to check for TCP connections on port <code>80</code> on <em>any IP in the server</em>. While this is fine for this test, if the server has multiple interfaces and is configured to bind to a specific address, then a new test would have to be added. Adding another test that checks if port <code>80</code> is listening on a given address might seem like overkill, but if you think about the reporting, it helps explain what is going on:</p>&#13;
<ol>&#13;
<li>&#13;
<p>Test <code>nginx</code> listens on port <code>80</code> : PASS</p>&#13;
</li>&#13;
<li>&#13;
<p>Test <code>nginx</code> listens on address <code>192.168.0.2</code> and port <code>80</code>: FAIL</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
&#13;
<p>The above tells us that Nginx is binding to port <code>80</code>, <em>just not to the right interface</em>. An extra test is an excellent way to provide granularity (at the expense of extra verbosity).</p>&#13;
&#13;
<p>Run the newly added test again:</p>&#13;
&#13;
<pre data-type="programlisting">(validate) $ pytest -q --hosts='ssh://node4' test_webserver.py&#13;
..F&#13;
================================== FAILURES ===================================&#13;
_________________ test_nginx_listens_on_port_80[ssh://node4] __________________&#13;
&#13;
host = &lt;testinfra.host.Host object at 0x7fbaa64f26a0&gt;&#13;
&#13;
    def test_nginx_listens_on_port_80(host):&#13;
&gt;       assert host.socket("tcp://0.0.0.0:80").is_listening&#13;
E       AssertionError: assert False&#13;
E        +  where False = &lt;socket tcp://0.0.0.0:80&gt;.is_listening&#13;
E        +    where &lt;socket tcp://0.0.0.0:80&gt; = &lt;class 'LinuxSocketSS'&gt;&#13;
&#13;
test_webserver.py:11: AssertionError&#13;
1 failed, 2 passed in 2.98 seconds</pre>&#13;
&#13;
<p>No address has anything listening on port <code>80</code>. Looking at the configuration for Nginx reveals that it is set to listen on port <code>8080</code> using  a directive in the default site that configures the port:</p>&#13;
&#13;
<pre data-type="programlisting">(validate) $ grep "listen 8080" /etc/nginx/sites-available/default&#13;
    listen 8080 default_server;</pre>&#13;
&#13;
<p>After changing it back to port <code>80</code> and restarting the <code>nginx</code> service, the tests pass again:</p>&#13;
&#13;
<pre data-type="programlisting">(validate) $ grep "listen 80" /etc/nginx/sites-available/default&#13;
    listen 80 default_server;&#13;
(validate) $ systemctl restart nginx&#13;
(validate) $ pytest -q --hosts='ssh://node4' test_webserver.py&#13;
...&#13;
3 passed in 2.92 seconds</pre>&#13;
&#13;
<p>Since there isn’t a built-in fixture to handle HTTP requests to an address, the final test uses the <code>wget</code> utility to retrieve the contents of the running website and make assertions on the output to ensure that the static site renders:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">test_get_content_from_site</code><code class="p">(</code><code class="n">host</code><code class="p">):</code>&#13;
    <code class="n">output</code> <code class="o">=</code> <code class="n">host</code><code class="o">.</code><code class="n">check_output</code><code class="p">(</code><code class="s1">'wget -qO- 0.0.0.0:80'</code><code class="p">)</code>&#13;
    <code class="k">assert</code> <code class="s1">'Welcome to nginx'</code> <code class="ow">in</code> <code class="n">output</code></pre>&#13;
&#13;
<p>Running <em>test_webserver.py</em> once more verifies that all our assumptions are correct:</p>&#13;
&#13;
<pre data-type="programlisting">(validate) $ pytest -q --hosts='ssh://node4' test_webserver.py&#13;
....&#13;
4 passed in 3.29 seconds</pre>&#13;
&#13;
<p>Understanding the concepts of testing in Python, and repurposing those for system validation, is incredibly powerful. Automating test runs while developing applications or even writing and running tests on existing infrastructure are both excellent ways to simplify day-to-day operations that can become error-prone. pytest and Testinfra are great projects that can help you get started, and make it easy when extending is needed. Testing is a <em>level up</em> on skills.<a data-startref="ix_ch08-asciidoc12" data-type="indexterm" id="idm46691326103128"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Testing Jupyter Notebooks with pytest" data-type="sect1"><div class="sect1" id="idm46691326296504">&#13;
<h1>Testing Jupyter Notebooks with pytest</h1>&#13;
&#13;
<p><a data-primary="Jupyter Notebooks" data-type="indexterm" id="idm46691326101656"/><a data-primary="pytest" data-secondary="testing Jupyter Notebooks" data-type="indexterm" id="idm46691326100952"/>One easy way to introduce big problems into your company is to forget about applying software engineering best practices when it comes to data science and machine learning.  One way to fix this is to use the <code>nbval</code> plug-in for pytest that allows you to test your notebooks.  Take a look at this <code>Makefile</code>:</p>&#13;
&#13;
<pre data-code-language="make" data-type="programlisting"><code class="nf">setup</code><code class="o">:</code>&#13;
    python3 -m venv ~/.myrepo&#13;
&#13;
<code class="nf">install</code><code class="o">:</code>&#13;
    pip install -r requirements.txt&#13;
&#13;
<code class="nf">test</code><code class="o">:</code>&#13;
    python -m py<code class="nb">test</code> -vv --cov=myrepolib tests/*.py&#13;
    python -m py<code class="nb">test</code> --nbval notebook.ipynb&#13;
&#13;
<code class="nf">lint</code><code class="o">:</code>&#13;
    pylint --disable=R,C myrepolib cli web&#13;
&#13;
<code class="nf">all</code><code class="o">:</code> <code class="n">install</code> <code class="n">lint</code> <code class="n">test</code></pre>&#13;
&#13;
<p>The key item is the <code>--nbval</code> flag that also allows the notebook in the repo to be tested by the build server.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Exercises" data-type="sect1"><div class="sect1" id="idm46691326055864">&#13;
<h1>Exercises</h1>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Name at least three conventions needed so that <code>pytest</code> can discover a test.</p>&#13;
</li>&#13;
<li>&#13;
<p>What is the <em>conftest.py</em> file for?</p>&#13;
</li>&#13;
<li>&#13;
<p>Explain parametrization of tests.</p>&#13;
</li>&#13;
<li>&#13;
<p>What is a fixture and how can it be used in tests? Is it convenient? Why?</p>&#13;
</li>&#13;
<li>&#13;
<p>Explain how to use the <code>monkeypatch</code> fixture.</p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Case Study Question" data-type="sect1"><div class="sect1" id="idm46691326048488">&#13;
<h1>Case Study Question</h1>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Create a test module to use <code>testinfra</code> to connect to a remote server. Test that Nginx is installed, is running with <code>systemd</code>, and the server is binding to port 80. When all tests pass, try to make them fail by configuring Nginx to listen on a different port.<a data-startref="ix_ch08-asciidoc0" data-type="indexterm" id="idm46691326045416"/></p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>