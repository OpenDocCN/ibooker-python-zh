<html><head></head><body><section data-pdf-bookmark="Appendix B. Installing and Deploying Ray" data-type="appendix" epub:type="appendix"><div class="appendix" id="appB">
<h1><span class="label">Appendix B. </span>Installing and Deploying Ray</h1>


<p>The power of Ray is in its support for various deployment models, ranging from a single-node deployment—​allowing you to experiment with Ray locally—​to clusters containing thousands of machines. The same code developed on the local Ray installation can run on the entire spectrum of Ray’s installations. In this appendix, we will show some of the installation options that we evaluated while writing this book.</p>






<section data-pdf-bookmark="Installing Ray Locally" data-type="sect1"><div class="sect1" id="idm45354756770160">
<h1>Installing Ray Locally</h1>

<p>The <a data-primary="Ray" data-secondary="installing" data-tertiary="with pip command" data-tertiary-sortas="pip command" data-type="indexterm" id="idm45354756768832"/><a data-primary="installing" data-secondary="Ray" data-tertiary="with pip command" data-tertiary-sortas="pip command" data-type="indexterm" id="idm45354756767280"/><a data-primary="pip command, installing Ray" data-type="indexterm" id="idm45354756690576"/>simplest Ray installation is done locally with <code>pip</code>. Use the following command:</p>

<pre data-type="programlisting">pip install -U ray</pre>

<p>This command installs all the code required to run local Ray programs or launch programs on a Ray cluster (see <a data-type="xref" href="#sec-using-ray-clusters">“Using Ray Clusters”</a>). The command installs the latest official release. In addition, it is possible to install Ray from <a href="https://oreil.ly/2VzQD">daily releases</a> or a <a href="https://oreil.ly/f9k7H">specific commit</a>. It is also possible to install Ray inside the <a href="https://oreil.ly/1TsIZ">Conda environment</a>. Finally, you can build Ray from the source by following the instructions in the <a href="https://oreil.ly/rjane">Ray documentation</a>.</p>
</div></section>






<section data-pdf-bookmark="Using Ray Docker Images" data-type="sect1"><div class="sect1" id="idm45354756684624">
<h1>Using Ray Docker Images</h1>

<p>In <a data-primary="Ray" data-secondary="installing" data-tertiary="with Docker images" data-tertiary-sortas="Docker images" data-type="indexterm" id="ray-install-docker"/><a data-primary="installing" data-secondary="Ray" data-tertiary="with Docker images" data-tertiary-sortas="Docker images" data-type="indexterm" id="install-ray-docker"/><a data-primary="Docker images, installing Ray" data-type="indexterm" id="docker-install-ray"/>addition to natively installing on your local machine, Ray provides an option for running the provided <a href="https://oreil.ly/zrvoq">Docker image</a>. The Ray project provides a wealth of <a href="https://oreil.ly/0qv77">Docker images</a> built for various versions of Python and hardware options. These images can be used to execute Ray’s code by starting a corresponding Ray image:</p>

<pre data-type="programlisting">docker run --rm --shm-size=&lt;<em>shm-size</em>&gt; -t -i &lt;<em>image name</em>&gt;</pre>

<p>Here <code>&lt;<em>shm-size</em>&gt;</code> is the memory that Ray uses internally for its object store. A good estimate for this value is to use roughly 30% of your available memory; <code>&lt;<em>image name</em>&gt;</code> is the name of the image used.</p>

<p>Once this command is executed, you will get back a command-line prompt and can enter <a data-primary="Ray" data-secondary="installing" data-startref="ray-install-docker" data-tertiary="with Docker images" data-tertiary-sortas="Docker images" data-type="indexterm" id="idm45354756672960"/><a data-primary="installing" data-secondary="Ray" data-startref="install-ray-docker" data-tertiary="with Docker images" data-tertiary-sortas="Docker images" data-type="indexterm" id="idm45354756671168"/><a data-primary="Docker images, installing Ray" data-startref="docker-install-ray" data-type="indexterm" id="idm45354756669600"/>any Ray code.</p>
</div></section>






<section data-pdf-bookmark="Using Ray Clusters" data-type="sect1"><div class="sect1" id="sec-using-ray-clusters">
<h1>Using Ray Clusters</h1>

<p>Although<a data-primary="Ray" data-secondary="installing" data-tertiary="on clusters" data-tertiary-sortas="clusters" data-type="indexterm" id="ray-install-cluster"/><a data-primary="installing" data-secondary="Ray" data-tertiary="on clusters" data-tertiary-sortas="clusters" data-type="indexterm" id="install-ray-cluster"/><a data-primary="clusters" data-secondary="installing Ray" data-type="indexterm" id="cluster-install-ray"/><a data-primary="Ray" data-secondary="clusters" data-tertiary="installing on" data-type="indexterm" id="ray-cluster-install"/> a local Ray installation is extremely useful for experimenting and initial debugging, the real power of Ray is its ability to run and scale on clusters of machines.</p>

<p>Ray <em>cluster nodes</em> are<a data-primary="clusters" data-secondary="cluster nodes" data-type="indexterm" id="idm45354756659568"/> logical nodes based on Docker images. Docker images provided by the Ray project contain all the code required for running logical nodes, but not necessarily all the code required to run user applications. The issue here is that the user’s code might need specific Python libraries, which are not part of Ray’s Docker images.</p>

<p>To overcome this problem, Ray allows the installation of specific libraries to the nodes as part of the cluster installation, which is great for initial testing but can significantly impact the node’s creation performance. As a result, in production installs, it is typically recommended to use custom images derived from Ray-provided ones and add required libraries.</p>

<p>Ray provides two main options for installation: installation directly on the hardware nodes or cloud provider’s VMs and installation on Kubernetes. Here we will discuss Ray’s installation on cloud providers and Kubernetes. For information on Ray’s installation on <a data-primary="hardware nodes, installing Ray" data-type="indexterm" id="idm45354756657632"/>hardware nodes, refer to the <a href="https://oreil.ly/3hYV0">Ray documentation</a>.</p>

<p>The<a data-primary="Ray" data-secondary="installing" data-tertiary="on cloud providers" data-tertiary-sortas="cloud providers" data-type="indexterm" id="ray-install-cloud"/><a data-primary="installing" data-secondary="Ray" data-tertiary="on cloud providers" data-tertiary-sortas="cloud providers" data-type="indexterm" id="install-ray-cloud"/><a data-primary="cloud providers, installing Ray" data-type="indexterm" id="cloud-install"/> official <a href="https://oreil.ly/mrThY">documentation</a> describes Ray’s installation on several cloud providers, including AWS, Azure, Google Cloud, Alibaba, and custom clouds. Here we will discuss installation on AWS (as it is the most popular) and IBM Cloud (as one of the coauthors works at IBM, which takes a unique approach).<sup><a data-type="noteref" href="app02.html#idm45354756650320" id="idm45354756650320-marker">1</a></sup></p>








<section data-pdf-bookmark="Installing Ray on AWS" data-type="sect2"><div class="sect2" id="idm45354756649632">
<h2>Installing Ray on AWS</h2>

<p>AWS <a data-primary="AWS (Amazon Web Services), installing Ray" data-type="indexterm" id="aws-install"/><a data-primary="Amazon Web Services (AWS), installing Ray" data-type="indexterm" id="aws-install2"/>cloud installation leverages the Boto3 AWS SDK for Python and requires configuring your AWS credentials in the <em>~/.aws/credentials</em> file.<sup><a data-type="noteref" href="app02.html#idm45354756645408" id="idm45354756645408-marker">2</a></sup></p>

<p>Once the credentials are created and Boto3 is installed, you can use the <a href="https://oreil.ly/zkodJ"><em>ray-aws.yaml</em> file</a>, which was adapted from the <a href="https://oreil.ly/h0UnW">Ray GitHub repository</a>, to install Ray on AWS via the following command:</p>

<pre data-type="programlisting">ray up &lt;<em>your location</em>&gt;/ray-aws.yaml</pre>

<p>This command creates the cluster. It also provides a set of useful commands that you can use:</p>

<pre data-type="programlisting">Monitor autoscaling with
    ray exec ~/Projects/Platform-Infrastructure/middleware\
    /ray/install/ray-aws.yaml\
    'tail -n 100 -f /tmp/ray/session_latest/logs/monitor*'
Connect to a terminal on the cluster head:
    ray attach ~/Projects/Platform-Infrastructure/middleware\
    /ray/install/ray-aws.yaml
Get a remote shell to the cluster manually:
    ssh -o IdentitiesOnly=yes\
    -i /Users/boris/Downloads/id.rsa.ray-boris root@52.118.80.225</pre>

<p>Note that the IP addresses that you’ll see will be different from those shown here. When the cluster is created, it uses a firewall that allows only a Secure Shell (SSH) connection to the cluster. If you want to access the cluster’s dashboard, you need to open port 8265, and for gRPC access, use port 10001. To do this, find your node in the Amazon Elastic Compute Cloud (EC2) dashboard, click the Security tab, choose the security group, and modify the inbound rules. <a data-type="xref" href="#fig-appb-1">Figure B-1</a> shows a new rule allowing any instance port access from anywhere. For more information on inbound rule configuration, refer to the <a href="https://oreil.ly/MRfib">AWS documentation</a>.</p>

<figure><div class="figure" id="fig-appb-1">
<img alt="spwr ab01" src="assets/spwr_ab01.png"/>
<h6><span class="label">Figure B-1. </span>Instances view in the AWS console</h6>
</div></figure>

<p>As requested by your YAML file, you can see only a head, and the worker nodes will be created to satisfy the execution requirements of submitted jobs. To verify that the cluster is running correctly, you can use the code in <a href="https://oreil.ly/OzOQN"><em>localPython.py</em> on GitHub</a>, which verifies that it can connect to the cluster and its nodes.</p>

<p>An alternative <a data-primary="Ray" data-secondary="installing" data-tertiary="directly on VMs" data-type="indexterm" id="idm45354756633632"/><a data-primary="installing" data-secondary="Ray" data-tertiary="directly on VMs" data-type="indexterm" id="idm45354756632352"/>approach to using Docker images for installation is <a href="https://oreil.ly/k733p">installing Ray directly on a VM</a>. The advantage of this approach is the ability to easily add additional software to the VM, which can be useful in real life. An obvious use case is managing Python libraries. You can do this with Docker-based installation, but you will then need to build Docker images for each library configuration. In the VM-based approach, there is no need to create and manage Docker images; just do appropriate <code>pip</code> installs. Additionally, you can install applications on VMs to leverage them in the Ray execution (see <a data-type="xref" href="ch12.html#sec-wrapping-custom-programs">“Wrapping Custom Programs with Ray”</a>).</p>
<div data-type="tip"><h6>Tip</h6>
<p>Installing Ray on a VM requires a lot of setup commands, and as a result, it can take a significant amount of time for the Ray node to start. A recommended approach is to start the Ray cluster once, create a new image, and then use this image and remove additional setup <a data-primary="AWS (Amazon Web Services), installing Ray" data-startref="aws-install" data-type="indexterm" id="idm45354756627824"/><a data-primary="Amazon Web Services (AWS), installing Ray" data-startref="aws-install2" data-type="indexterm" id="idm45354756626752"/>commands.</p>
</div>
</div></section>








<section data-pdf-bookmark="Installing Ray on IBM Cloud" data-type="sect2"><div class="sect2" id="idm45354756649040">
<h2>Installing Ray on IBM Cloud</h2>

<p>IBM Cloud <a data-primary="IBM Cloud, installing Ray" data-type="indexterm" id="ibm-cloud-install"/>installation is based on the <a href="https://oreil.ly/tIF6Y">Gen2 connector</a> that enables the Ray cluster to be deployed on IBM’s Gen2 cloud infrastructure. As with Ray on AWS, you’ll start with creating the cluster specification in a YAML file. You can use Lithopscloud to do this interactively if you don’t want to manually create the YAML file. You install Lithopscloud with <code>pip</code> as normal:</p>

<pre data-type="programlisting">pip3 install lithopscloud</pre>

<p>To use Lithopscloud, you first need to either create an <a href="https://oreil.ly/ZO9Nv">API key</a> or reuse the existing one. With your API key, you can run <code>lithopscloud -o cluster.yaml</code> to generate a <em>cluster.yaml</em> file. Once you start Lithopscloud, 
<span class="keep-together">follow the questions</span> to generate a file (you’ll need to use the up and down arrows to 
<span class="keep-together">make your selections).</span> You can find an example of the generated file on <a href="https://oreil.ly/rQNOx">GitHub</a>.</p>

<p>The limitation of the autogenerated file is that it uses the same image type for both head and worker nodes, which is not always ideal. You often may want to provide different types for these nodes. To do this, you can modify the autogenerated <a href="https://oreil.ly/LqpIl"><em>cluster.yaml</em> file</a> as follows:</p>

<pre data-code-language="yaml" data-type="programlisting"><code class="nt">available_node_types</code><code class="p">:</code><code class="w"/>
<code class="w"> </code><code class="nt">ray_head_default</code><code class="p">:</code><code class="w"/>
<code class="w">   </code><code class="nt">max_workers</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">0</code><code class="w"/>
<code class="w">   </code><code class="nt">min_workers</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">0</code><code class="w"/>
<code class="w">   </code><code class="nt">node_config</code><code class="p">:</code><code class="w"/>
<code class="w">     </code><code class="nt">boot_volume_capacity</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">100</code><code class="w"/>
<code class="w">     </code><code class="nt">image_id</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">r006-dd164da8-c4d9-46ba-87c4-03c614f0532c</code><code class="w"/>
<code class="w">     </code><code class="nt">instance_profile_name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">bx2-4x16</code><code class="w"/>
<code class="w">     </code><code class="nt">key_id</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">r006-d6d823da-5c41-4e92-a6b6-6e98dcc90c8e</code><code class="w"/>
<code class="w">     </code><code class="nt">resource_group_id</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">5f6b028dc4ef41b9b8189bbfb90f2a79</code><code class="w"/>
<code class="w">     </code><code class="nt">security_group_id</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">r006-c8e44f9c-7159-4041-a7ab-cf63cdb0dca7</code><code class="w"/>
<code class="w">     </code><code class="nt">subnet_id</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">0737-213b5b33-cee3-41d0-8d25-95aef8e86470</code><code class="w"/>
<code class="w">     </code><code class="nt">volume_tier_name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">general-purpose</code><code class="w"/>
<code class="w">     </code><code class="nt">vpc_id</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">r006-50485f78-a76f-4401-a742-ce0a748b46f9</code><code class="w"/>
<code class="w">   </code><code class="nt">resources</code><code class="p">:</code><code class="w"/>
<code class="w">     </code><code class="nt">CPU</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">4</code><code class="w"/>
<code class="w"> </code><code class="nt">ray_worker_default</code><code class="p">:</code><code class="w"/>
<code class="w">   </code><code class="nt">max_workers</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">10</code><code class="w"/>
<code class="w">   </code><code class="nt">min_workers</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">0</code><code class="w"/>
<code class="w">   </code><code class="nt">node_config</code><code class="p">:</code><code class="w"/>
<code class="w">     </code><code class="nt">boot_volume_capacity</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">100</code><code class="w"/>
<code class="w">     </code><code class="nt">image_id</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">r006-dd164da8-c4d9-46ba-87c4-03c614f0532c</code><code class="w"/>
<code class="w">     </code><code class="nt">instance_profile_name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">bx2-8x32</code><code class="w"/>
<code class="w">     </code><code class="nt">key_id</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">r006-d6d823da-5c41-4e92-a6b6-6e98dcc90c8e</code><code class="w"/>
<code class="w">     </code><code class="nt">resource_group_id</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">5f6b028dc4ef41b9b8189bbfb90f2a79</code><code class="w"/>
<code class="w">     </code><code class="nt">security_group_id</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">r006-c8e44f9c-7159-4041-a7ab-cf63cdb0dca7</code><code class="w"/>
<code class="w">     </code><code class="nt">subnet_id</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">0737-213b5b33-cee3-41d0-8d25-95aef8e86470</code><code class="w"/>
<code class="w">     </code><code class="nt">volume_tier_name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">general-purpose</code><code class="w"/>
<code class="w">     </code><code class="nt">vpc_id</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">r006-50485f78-a76f-4401-a742-ce0a748b46f9</code><code class="w"/>
<code class="w">   </code><code class="nt">resources</code><code class="p">:</code><code class="w"/>
<code class="w">     </code><code class="nt">CPU</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">8</code><code class="w"/></pre>

<p>Here you define two types of nodes: the default head node and default worker node (you can define multiple worker node types with a max number of workers per time). Therefore, you can now have a relatively small head node (running all the time) and much larger worker nodes that will be created just in time.</p>
<div data-type="tip"><h6>Tip</h6>
<p>If you take a look at the generated YAML file, you will notice that it has a lot of setup commands, and as a result, it can take a significant amount of time for the Ray node to start. A recommended approach is to start the Ray cluster once, create a new image, and then use this image and remove the setup commands.</p>
</div>

<p>Once the YAML file is generated, you can install Gen2-connector to be able to use it. Run <code>pip3 install gen2-connector</code>. You can then create your cluster by running <code>ray up cluster.yaml</code>.</p>

<p>Similar to installing Ray on AWS, this installation displays a list of useful commands:</p>

<pre data-type="programlisting">Monitor autoscaling with
    ray exec /Users/boris/Downloads/cluster.yaml \
    'tail -n 100 -f /tmp/ray/session_latest/logs/monitor*'
Connect to a terminal on the cluster head:
    ray attach ~/Downloads/cluster.yaml
Get a remote shell to the cluster manually:
    ssh -o IdentitiesOnly=yes -i ~/Downloads/id.rsa.ray-boris root@52.118.80.225</pre>

<p>To be able to access the cluster, be sure to open the required ports following <a href="https://oreil.ly/8oTDR">IBM Cloud documentation</a> (<a data-type="xref" href="#fig-appB-2">Figure B-2</a>).</p>

<figure><div class="figure" id="fig-appB-2">
<img alt="spwr ab02" src="assets/spwr_ab02.png"/>
<h6><span class="label">Figure B-2. </span>IBM Cloud console displaying firewall rules</h6>
</div></figure>

<p>As requested by your YAML file, you can see only a head; the worker nodes will be created to satisfy the execution requirements of submitted jobs. To verify that the cluster is running correctly, <a data-primary="Ray" data-secondary="installing" data-startref="ray-install-cloud" data-tertiary="on cloud providers" data-tertiary-sortas="cloud providers" data-type="indexterm" id="idm45354756533216"/><a data-primary="installing" data-secondary="Ray" data-startref="install-ray-cloud" data-tertiary="on cloud providers" data-tertiary-sortas="cloud providers" data-type="indexterm" id="idm45354756531424"/><a data-primary="cloud providers, installing Ray" data-startref="cloud-install" data-type="indexterm" id="idm45354756529664"/><a data-primary="IBM Cloud, installing Ray" data-startref="ibm-cloud-install" data-type="indexterm" id="idm45354756528752"/>execute the <a href="https://oreil.ly/rl5SL"><em>localPython.py</em> script</a>.</p>
</div></section>








<section data-pdf-bookmark="Installing Ray on Kubernetes" data-type="sect2"><div class="sect2" id="idm45354756526480">
<h2>Installing Ray on Kubernetes</h2>

<p>When <a data-primary="Ray" data-secondary="installing" data-tertiary="on Kubernetes" data-tertiary-sortas="Kubernetes" data-type="indexterm" id="ray-install-kubernetes"/><a data-primary="installing" data-secondary="Ray" data-tertiary="on Kubernetes" data-tertiary-sortas="Kubernetes" data-type="indexterm" id="install-ray-kubernetes"/><a data-primary="Kubernetes, installing Ray" data-type="indexterm" id="kubernetes-install"/>it comes to the actual cluster’s installation on Kubernetes, Ray provides two basic mechanisms:</p>
<dl>
<dt>Cluster launcher</dt>
<dd>
<p>Similar to installation using VMs, this makes it simple to deploy a Ray cluster on any cloud. It will provision a new instance or machine using the cloud provider’s SDK, execute shell commands to set up Ray with the provided options, and initialize the cluster.</p>
</dd>
<dt>Ray Kubernetes operator</dt>
<dd>
<p>This facilitates deploying Ray on an existing Kubernetes cluster. The operator defines a <a href="https://oreil.ly/RTWR9">custom resource</a> called a <code>RayCluster</code>, which describes the desired state of the Ray cluster, and a <a href="https://oreil.ly/ADp7y">custom controller</a>, the Ray Operator, which processes RayCluster resources and manages the Ray cluster.</p>
</dd>
</dl>
<div data-type="tip"><h6>Tip</h6>
<p>When you install Ray on a Kubernetes cluster by using both the cluster launcher and operator, Ray uses Kubernetes capabilities to create a new Ray node in the form of Kubernetes pod. Although the Ray autoscaler works the same way, it effectively “steals” resources from the Kubernetes cluster. Therefore, your Kubernetes cluster has to either be large enough to support all of Ray’s resource requirements or provide its own autoscaling mechanism. In addition, because Ray’s nodes are in this case implemented as underlying Kubernetes pods, the Kubernetes resource manager can delete these pods at any time to obtain additional <a data-primary="Ray" data-secondary="installing" data-startref="ray-install-kubernetes" data-tertiary="on Kubernetes" data-tertiary-sortas="Kubernetes" data-type="indexterm" id="idm45354756364256"/><a data-primary="installing" data-secondary="Ray" data-startref="install-ray-kubernetes" data-tertiary="on Kubernetes" data-tertiary-sortas="Kubernetes" data-type="indexterm" id="idm45354756362496"/><a data-primary="Kubernetes, installing Ray" data-startref="kubernetes-install" data-type="indexterm" id="idm45354756360736"/>resources.</p>
</div>
</div></section>








<section data-pdf-bookmark="Installing Ray on a kind Cluster" data-type="sect2"><div class="sect2" id="idm45354756359264">
<h2>Installing Ray on a kind Cluster</h2>

<p>To<a data-primary="Ray" data-secondary="installing" data-tertiary="on kind clusters" data-tertiary-sortas="kind clusters" data-type="indexterm" id="idm45354756357968"/><a data-primary="installing" data-secondary="Ray" data-tertiary="on kind clusters" data-tertiary-sortas="kind clusters" data-type="indexterm" id="idm45354756356416"/><a data-primary="kind (Kubernetes in Docker) clusters, installing Ray" data-type="indexterm" id="idm45354756354928"/> demonstrate both approaches, let’s start by installing and accessing the Ray cluster on a <a href="https://oreil.ly/qvuAi">kind (Kubernetes in Docker) cluster</a>. This popular tool runs local Kubernetes clusters by using Docker container “nodes” and is often used for local development. To do this, you need to create a cluster first by running the following command:</p>

<pre data-type="programlisting">kind create cluster</pre>

<p>This creates a cluster with a default configuration. To modify the configuration, refer to the <a href="https://oreil.ly/Rvq54">configuration documentation</a>. Once the cluster is up and running, you can use either <code>ray up</code> or the Kubernetes operator to create a Ray cluster.</p>
</div></section>








<section data-pdf-bookmark="Using ray up" data-type="sect2"><div class="sect2" id="idm45354756350624">
<h2>Using ray up</h2>

<p>To<a data-primary="Ray" data-secondary="installing" data-tertiary="with ray up command" data-tertiary-sortas="ray up command" data-type="indexterm" id="ray-install-rayup"/><a data-primary="installing" data-secondary="Ray" data-tertiary="with ray up command" data-tertiary-sortas="ray up command" data-type="indexterm" id="install-ray-rayup"/><a data-primary="ray up command" data-type="indexterm" id="rayup"/> create a Ray cluster by using <code>ray up</code>, you must specify the resource requirements in a YAML file, such as <a href="https://oreil.ly/YGOp5"><em>raycluster.yaml</em></a>, which was adapted from the Ray Kubernetes autoscaler defaults in the <a href="https://oreil.ly/m2mm2">Ray GitHub repository</a>. This file contains all the information required to create the Ray cluster:</p>

<ul>
<li>
<p>General information about the cluster name and autoscaling parameters.</p>
</li>
<li>
<p>Information about the cluster provider (Kubernetes, in our case), including provider-specific information required for the creation of Ray cluster’s nodes.</p>
</li>
<li>
<p>Node-specific information (CPU/memory, etc). This also includes a list of node startup commands, including Python libraries required for the installation.</p>
</li>
</ul>

<p>With this file in place, a command to create a cluster looks like this:</p>

<pre data-type="programlisting">ray up &lt;<em>your location</em>&gt;/raycluster.yaml</pre>

<p class="pagebreak-before">Once the cluster creation completes, you can see that several pods are running:</p>

<pre data-type="programlisting">&gt; get pods -n ray
NAME                   READY   STATUS    RESTARTS   AGE
ray-ray-head-88978     1/1     Running   0          2m15s
ray-ray-worker-czqlx   1/1     Running   0          23s
ray-ray-worker-lcdmm   1/1     Running   0          23s</pre>

<p>As requested by our YAML file, you can see one head and two worker 
<span class="keep-together">nodes. To verify</span> that the cluster is running correctly, you can use the following <a href="https://oreil.ly/swESN">job</a>:</p>

<pre data-type="programlisting">kubectl create -f &lt;your location&gt;/jobexample.yaml -n ray</pre>

<p>The execution results in something similar to this:</p>

<pre data-type="programlisting">&gt; kubectl logs ray-test-job-bx4xj-4nfbl -n ray
--2021-09-28 15:18:59--  https://raw.githubusercontent.com/scalingpythonml/...
Resolving raw.githubusercontent.com (raw.githubusercontent.com) ...
Connecting to raw.githubusercontent.com (raw.githubusercontent.com) ...
Length: 1750 (1.7K) [text/plain]
Saving to: ‘servicePython.py’

     0K .                                                     100% 9.97M=0s

2021-09-28 15:18:59 (9.97 MB/s) - ‘servicePython.py’ saved [1750/1750]

Connecting to Ray at service ray-ray-head, port 10001
Iteration 0
Counter({('ray-ray-head-88978', 'ray-ray-head-88978'): 30, ...
Iteration 1
……………………………….
Success!</pre>

<p>Once your job is up, you can additionally port-forward the <code>ray-ray-head</code> service by running the following:<sup><a data-type="noteref" href="app02.html#idm45354756330752" id="idm45354756330752-marker">3</a></sup></p>

<pre data-type="programlisting">kubectl port-forward -n ray service/ray-ray-head 10001</pre>

<p>Then, connect to it from your local machine by using the <a href="https://oreil.ly/RV8yx"><em>localPython.py</em> testing script from the book’s example files</a>. Execution of this code produces the same results as shown previously.</p>

<p>Additionally, you can port-forward ray service to port 8265 to look at the Ray dashboard:</p>

<pre class="pagebreak-after" data-type="programlisting">kubectl port-forward -n ray service/ray-ray-head 8265</pre>

<p>Once this is done, you can take a look at the Ray dashboard (<a data-type="xref" href="#fig-appB-3">Figure B-3</a>).</p>

<figure><div class="figure" id="fig-appB-3">
<img alt="spwr ab03" src="assets/spwr_ab03.png"/>
<h6><span class="label">Figure B-3. </span>Ray dashboard</h6>
</div></figure>

<p>You <a data-primary="ray down command" data-type="indexterm" id="idm45354756322592"/><a data-primary="Ray" data-secondary="installing" data-startref="ray-install-rayup" data-tertiary="with ray up command" data-tertiary-sortas="ray up command" data-type="indexterm" id="idm45354756321856"/><a data-primary="installing" data-secondary="Ray" data-startref="install-ray-rayup" data-tertiary="with ray up command" data-tertiary-sortas="ray up command" data-type="indexterm" id="idm45354756320096"/><a data-primary="ray up command" data-startref="rayup" data-type="indexterm" id="idm45354756318336"/>can uninstall the Ray cluster by using the following command:<sup><a data-type="noteref" href="app02.html#idm45354756317264" id="idm45354756317264-marker">4</a></sup></p>

<pre data-type="programlisting">ray down &lt;<em>your location</em>&gt;/raycluster.yaml</pre>
</div></section>








<section data-pdf-bookmark="Using the Ray Kubernetes Operator" data-type="sect2"><div class="sect2" id="idm45354756349680">
<h2>Using the Ray Kubernetes Operator</h2>

<p>For <a data-primary="Ray" data-secondary="installing" data-tertiary="with Ray Kubernetes operator" data-tertiary-sortas="Ray Kubernetes operator" data-type="indexterm" id="ray-install-operator"/><a data-primary="installing" data-secondary="Ray" data-tertiary="with Ray Kubernetes operator" data-tertiary-sortas="Ray Kubernetes operator" data-type="indexterm" id="install-ray-operator"/>deployment to the Kubernetes cluster, we can also use the Ray operator, which is a recommended approach. To simplify usage of the operator, Ray provides a <a href="https://oreil.ly/4jjfI">Helm chart</a> available as part of the Ray GitHub repository. Here, instead of the Helm chart, we are using several YAML files to deploy Ray to make installation a bit simpler.</p>

<p>Our deployment is split into three files: <a href="https://oreil.ly/wORyi"><em>operatorcrd.yaml</em></a>, containing all the commands for CustomResourceDefinition (CRD) creation; <a href="https://oreil.ly/RyjD7">
<span class="keep-together"><em>operator.yaml</em></span></a>, containing all the commands for operator creation; and <a href="https://oreil.ly/Ibqbn"><em>rayoperatorcluster.yaml</em></a>, containing all the commands for cluster creation. It is assumed in these files that the operator is created in the namespace <em>ray</em>.</p>

<p>To install the operator itself, we need to execute these two commands:</p>

<pre class="pagebreak-after" data-type="programlisting">kubectl apply -f &lt;<em>your location</em>&gt;/operatorcrd.yaml
kubectl apply -f &lt;<em>your location</em>&gt;/operator.yaml</pre>

<p>Once this is done, use the following command to ensure that the operator pod is running:</p>

<pre data-type="programlisting">&gt; kubectl get pods -n ray
NAME                            READY   STATUS    RESTARTS   AGE
ray-operator-6c9954cddf-cjn9c   1/1     Running   0          110s</pre>

<p>Once the operator is up and running, you can start the cluster itself by using the following command:<sup><a data-type="noteref" href="app02.html#idm45354756301680" id="idm45354756301680-marker">5</a></sup></p>

<pre data-type="programlisting">kubectl apply -f &lt;<em>your location</em>&gt;/rayoperatorcluster.yaml -n ray</pre>

<p>Here the content of <em>rayoperatorcluster.yaml</em> is similar to the content of <em>raycluster.yaml</em> but formatted slightly differently. Once the cluster is up and running, you can use the same validation code as described previously <a data-primary="Ray" data-secondary="installing" data-startref="ray-install-operator" data-tertiary="with Ray Kubernetes operator" data-tertiary-sortas="Ray Kubernetes operator" data-type="indexterm" id="idm45354756298640"/><a data-primary="installing" data-secondary="Ray" data-startref="install-ray-operator" data-tertiary="with Ray Kubernetes operator" data-tertiary-sortas="Ray Kubernetes operator" data-type="indexterm" id="idm45354756296784"/>for <code>ray up</code>.</p>
</div></section>








<section data-pdf-bookmark="Installing Ray on OpenShift" data-type="sect2"><div class="sect2" id="idm45354756315296">
<h2>Installing Ray on OpenShift</h2>

<p>OpenShift is<a data-primary="Ray" data-secondary="installing" data-tertiary="on OpenShift" data-tertiary-sortas="OpenShift" data-type="indexterm" id="idm45354756292816"/><a data-primary="installing" data-secondary="Ray" data-tertiary="on OpenShift" data-tertiary-sortas="OpenShift" data-type="indexterm" id="idm45354756291264"/><a data-primary="OpenShift, installing Ray" data-type="indexterm" id="idm45354756289776"/> a type of Kubernetes cluster, so theoretically the Kubernetes operator can be used to install Ray on an OpenShift cluster. Unfortunately, this installation is a little bit more involved.</p>

<p>If you have ever used OpenShift, you know that by default all of the pods in OpenShift run in <a href="https://oreil.ly/ZkcDY">restrictive mode</a>. This mode denies access to all host features and requires pods to be run with a unique identifier (UID) and Security-Enhanced Linux (SELinux) context that are allocated to the namespace.</p>

<p>Unfortunately, this does not quite work for the Ray operator, designed to run as user 1000. To enable this, you need to introduce several changes to the files that you used for installing on the kind (and any other plain Kubernetes cluster):</p>

<ul>
<li>
<p>Add the <code>ray-operator-serviceaccount</code> service account, which is used by the operator, to <code>anyuid</code> mode. This allows users to run with any nonroot UID:</p>

<pre data-type="programlisting">oc adm policy add-scc-to-user anyuid -z ray-operator-serviceaccount</pre>
</li>
<li>
<p>Modify <a href="https://oreil.ly/eYIht"><em>operator.yaml</em></a> to ensure that the operator pod is running as user 1000.</p>
</li>
</ul>

<p>Additionally, a <a href="https://oreil.ly/R2r8x">testing job</a> has to be modified slightly to run as user 1000. This requires the creation of a <code>ray-node-serviceaccount</code> service account used for running a job and adding this service account to <code>anyuid</code> mode, which allows users to run with any nonroot<a data-primary="Ray" data-secondary="installing" data-startref="ray-install-cluster" data-tertiary="on clusters" data-tertiary-sortas="clusters" data-type="indexterm" id="idm45354756280144"/><a data-primary="installing" data-secondary="Ray" data-startref="install-ray-cluster" data-tertiary="on clusters" data-tertiary-sortas="clusters" data-type="indexterm" id="idm45354756278352"/><a data-primary="clusters" data-secondary="installing Ray" data-startref="cluster-install-ray" data-type="indexterm" id="idm45354756276592"/><a data-primary="Ray" data-secondary="clusters" data-startref="ray-cluster-install" data-tertiary="installing on" data-type="indexterm" id="idm45354756275376"/> UID.</p>
</div></section>
</div></section>






<section data-pdf-bookmark="Conclusion" data-type="sect1"><div class="sect1" id="idm45354756667904">
<h1>Conclusion</h1>

<p>Ray provides a wealth of deployment options. When using Ray to solve a specific problem, you will need to decide which option is
is most suitable for your specific situation.</p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="idm45354756650320"><sup><a href="app02.html#idm45354756650320-marker">1</a></sup> In the interest of transparency: Boris currently works at IBM, and Holden used to work at IBM. Holden has also worked for Google, Microsoft, and Amazon.</p><p data-type="footnote" id="idm45354756645408"><sup><a href="app02.html#idm45354756645408-marker">2</a></sup> See the <a href="https://oreil.ly/5A6jE">“Boto3 Docs 1.24.95” documentation</a> for information on setting up Boto3 configuration.</p><p data-type="footnote" id="idm45354756330752"><sup><a href="app02.html#idm45354756330752-marker">3</a></sup> Theoretically, you can also create an ingress to connect to the Ray cluster. Unfortunately, in the case of the NGINX ingress controller, it will not work. The issue here is that the Ray client is using unsecure gRPC, while the NGINX ingress controller supports only secure gRPC calls. When using the Ray cluster on a specific cloud, check whether an ingress supports unsecure gRPC before exposing Ray’s head service as an ingress.</p><p data-type="footnote" id="idm45354756317264"><sup><a href="app02.html#idm45354756317264-marker">4</a></sup> This command deletes pods, and it leaves behind the service created as part of a cluster. You have to manually delete a service for a complete cleanup.</p><p data-type="footnote" id="idm45354756301680"><sup><a href="app02.html#idm45354756301680-marker">5</a></sup> Although documentation mentions a cluster-wide deploy operator, it works only for a namespace where the operator is deployed.</p></div></div></section></body></html>