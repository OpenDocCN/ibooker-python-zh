["```py\nIn [1]: %load_ext memory_profiler  # load the %memit magic function\nIn [2]: %memit [0] * int(1e8)\npeak memory: 806.33 MiB, increment: 762.77 MiB\n```", "```py\n# we use a new IPython shell so we have a clean memory\nIn [1]: %load_ext memory_profiler\nIn [2]: %memit # show how much RAM this process is consuming right now\npeak memory: 43.39 MiB, increment: 0.11 MiB\nIn [3]: %memit [n for n in range(int(1e8))]\npeak memory: 3850.29 MiB, increment: 3806.59 MiB\nIn [4]: %memit\npeak memory: 44.79 MiB, increment: 0.00 MiB\n```", "```py\nIn [5]: %memit [n for n in range(int(1e8))]\npeak memory: 3855.78 MiB, increment: 3810.96 MiB\n```", "```py\nIn [1]: %load_ext memory_profiler\nIn [2]: import array\nIn [3]: %memit array.array('l', range(int(1e8)))\npeak memory: 837.88 MiB, increment: 761.39 MiB\nIn [4]: arr = array.array('l')\nIn [5]: arr.itemsize\nOut[5]: 8\n```", "```py\nIn [5]: array.array? # IPython magic, similar to help(array)\nInit signature: array.array(self, /, *args, **kwargs)\nDocstring:\narray(typecode [, initializer]) -> array\n\nReturn a new array whose items are restricted by typecode, and\ninitialized from the optional initializer value, which must be a list,\nstring, or iterable over elements of the appropriate type.\n\nArrays represent basic values and behave very much like lists, except\nthe type of objects stored in them is constrained. The type is specified\nat object creation time by using a type code, which is a single character.\nThe following type codes are defined:\n\n    Type code   C Type             Minimum size in bytes\n    'b'         signed integer     1\n    'B'         unsigned integer   1\n    'u'         Unicode character  2 (see note)\n    'h'         signed integer     2\n    'H'         unsigned integer   2\n    'i'         signed integer     2\n    'I'         unsigned integer   2\n    'l'         signed integer     4\n    'L'         unsigned integer   4\n    'q'         signed integer     8 (see note)\n    'Q'         unsigned integer   8 (see note)\n    'f'         floating point     4\n    'd'         floating point     8\n```", "```py\nIn [1]: %load_ext memory_profiler\nIn [2]: import numpy as np\n# NOTE that zeros have lazy allocation so misreport the memory used!\nIn [3]: %memit arr=np.zeros(int(1e8), np.complex128)\npeak memory: 58.37 MiB, increment: 0.00 MiB\nIn [4]: %memit arr=np.ones(int(1e8), np.complex128)\npeak memory: 1584.41 MiB, increment: 1525.89 MiB\nIn [5]: f\"{arr.size:,}\"\nOut[5]: '100,000,000'\nIn [6]: f\"{arr.nbytes:,}\"\nOut[6]: '1,600,000,000'\nIn [7]: arr.nbytes/arr.size\nOut[7]: 16.0\nIn [8]: arr.itemsize\nOut[8]: 16\n```", "```py\nIn [1]: import ipython_memory_usage.ipython_memory_usage as imu; import numpy as np\nIn [2]: %ipython_memory_usage_start\nOut[2]: 'memory profile enabled'\nIn [3]: nbr_items = 200_000_000\nIn [4]: yp = np.random.uniform(low=0.0000001, size=nbr_items)\nIn [4] used 1526.0508 MiB RAM in 2.18s, peaked 0.00 MiB above current,\n       total RAM usage 1610.05 MiB\nIn [5]: yt = np.ones(shape=nbr_items)\nIn [5] used 1525.8516 MiB RAM in 0.44s, peaked 0.00 MiB above current,\n       total RAM usage 3135.90 MiB\n\nIn [6]: answer = -(yt * np.log(yp) + ((1-yt) * (np.log(1-yp))))\nIn [6] used 1525.8594 MiB RAM in 18.63s, peaked 4565.70 MiB above current,\n       total RAM usage 4661.76 MiB\n\nIn [7]: del answer\nIn [7] used -1525.8242 MiB RAM in 0.11s, peaked 0.00 MiB above current,\n       total RAM usage 3135.93 MiB\n```", "```py\nIn [8]: import numexpr\nIn [8] used 0.0430 MiB RAM in 0.12s, peaked 0.00 MiB above current,\n       total RAM usage 3135.95 MiB\nIn [9]: answer = numexpr.evaluate(\"-(yt * log(yp) + ((1-yt) * (log(1-yp))))\")\nIn [9] used 1525.8281 MiB RAM in 2.67s, peaked 0.00 MiB above current,\n       total RAM usage 4661.78 MiB\n```", "```py\nIn [2] df = pd.DataFrame({'yp': np.random.uniform(low=0.0000001, size=nbr_items),\n       'yt': np.ones(nbr_items)})\nIn [3]: answer_eval = df.eval(\"-(yt * log(yp) + ((1-yt) * (log(1-yp))))\")\nIn [3] used 3052.1953 MiB RAM in 5.26s, peaked 3045.77 MiB above current,\n       total RAM usage 6185.45 MiB\n```", "```py\nIn [2] df = pd.DataFrame({'yp': np.random.uniform(low=0.0000001, size=nbr_items),\n       'yt': np.ones(nbr_items)})\nIn [3]: answer_eval = df.eval(\"-(yt * log(yp) + ((1-yt) * (log(1-yp))))\")\nIn [3] used 3052.5625 MiB RAM in 34.88s, peaked 7620.15 MiB above current,\n       total RAM usage 6185.24 MiB\n```", "```py\nIn [1]: sys.getsizeof(0)\nOut[1]: 24\nIn [2]: sys.getsizeof(1)\nOut[2]: 28\nIn [3]: sys.getsizeof((2**30)-1)\nOut[3]: 28\nIn [4]: sys.getsizeof((2**30))\nOut[4]: 32\n```", "```py\nIn [5]: sys.getsizeof(b\"\")\nOut[5]: 33\nIn [6]: sys.getsizeof(b\"a\")\nOut[6]: 34\nIn [7]: sys.getsizeof(b\"ab\")\nOut[7]: 35\nIn [8]: sys.getsizeof(b\"abc\")\nOut[8]: 36\n```", "```py\n# goes up in 8-byte steps rather than the 24+ we might expect!\nIn [9]: sys.getsizeof([])\nOut[9]: 64\nIn [10]: sys.getsizeof([1])\nOut[10]: 72\nIn [11]: sys.getsizeof([1, 2])\nOut[11]: 80\n```", "```py\nIn [12]: sys.getsizeof([b\"\"])\nOut[12]: 72\nIn [13]: sys.getsizeof([b\"abcdefghijklm\"])\nOut[13]: 72\nIn [14]: sys.getsizeof([b\"a\", b\"b\"])\nOut[14]: 80\n```", "```py\nIn [1]: from pympler.asizeof import asizeof\nIn [2]: asizeof([x for x in range(int(1e7))])\nOut[2]: 401528048\nIn [3]: %memit [x for x in range(int(1e7))]\npeak memory: 401.91 MiB, increment: 326.77 MiB\n```", "```py\nIn [1]: %load_ext memory_profiler\nIn [2]: type(b\"b\")\nOut[2]: bytes\nIn [3]: %memit b\"a\" * int(1e8)\npeak memory: 121.55 MiB, increment: 78.17 MiB\nIn [4]: type(\"u\")\nOut[4]: str\nIn [5]: %memit \"u\" * int(1e8)\npeak memory: 122.43 MiB, increment: 78.49 MiB\nIn [6]: %memit \"Σ\" * int(1e8)\npeak memory: 316.40 MiB, increment: 176.17 MiB\n```", "```py\nfaddishness\n'melanesians'\nKharálampos\nPizzaInACup™\nurl=\"http://en.wikipedia.org/wiki?curid=363886\"\nVIIIa),\nSuperbagnères.\n```", "```py\nprint(\"RAM at start {:0.1f}MiB\".format(memory_profiler.memory_usage()[0]))\nt1 = time.time()\nwords = [w for w in text_example.readers]\nprint(\"Loading {} words\".format(len(words)))\nt2 = time.time()\nprint(\"RAM after creating list {:0.1f}MiB, took {:0.1f}s\" \\\n      .format(memory_profiler.memory_usage()[0], t2 - t1))\n```", "```py\nassert 'Zwiebel' in words\ntime_cost = sum(timeit.repeat(stmt=\"'Zwiebel' in words\",\n                              setup=\"from __main__ import words\",\n                              number=1,\n                              repeat=10000))\nprint(\"Summed time to look up word {:0.4f}s\".format(time_cost))\n```", "```py\n$ python text_example_list.py\nRAM at start 36.6MiB\nLoading 499056 words\nRAM after creating list 70.9MiB, took 1.0s\nSummed time to look up word 53.5657s\n```", "```py\n    print(\"RAM at start {:0.1f}MiB\".format(memory_profiler.memory_usage()[0]))\n    t1 = time.time()\n    words = [w for w in text_example.readers]\n    print(\"Loading {} words\".format(len(words)))\n    t2 = time.time()\n    print(\"RAM after creating list {:0.1f}MiB, took {:0.1f}s\" \\\n          .format(memory_profiler.memory_usage()[0], t2 - t1))\n    print(\"The list contains {} words\".format(len(words)))\n    words.sort()\n    t3 = time.time()\n    print(\"Sorting list took {:0.1f}s\".format(t3 - t2))\n```", "```py\nimport bisect\n...\ndef index(a, x):\n    'Locate the leftmost value exactly equal to x'\n    i = bisect.bisect_left(a, x)\n    if i != len(a) and a[i] == x:\n        return i\n    raise ValueError\n...\n    time_cost = sum(timeit.repeat(stmt=\"index(words, 'Zwiebel')\",\n                                  setup=\"from __main__ import words, index\",\n                                  number=1,\n                                  repeat=10000))\n```", "```py\n$ python text_example_list_bisect.py \nRAM at start 36.6MiB\nLoading 11595290 words\nRAM after creating list 871.9MiB, took 20.6s\nThe list contains 11595290 words\nSorting list took 0.6s\nSummed time to look up word 0.0109s\n\n```", "```py\n    words_set = set(text_example.readers)\n```", "```py\n$ python text_example_set.py\nRAM at start 36.6MiB\nRAM after creating set 1295.3MiB, took 24.0s\nThe set contains 11595290 words\nSummed time to look up word 0.0023s\n\n```", "```py\nimport dawg\n...\n    words_dawg = dawg.DAWG(text_example.readers)\n```", "```py\n$ python text_example_dawg.py\nRAM at start 38.1MiB\nRAM after creating dawg 200.8MiB, took 31.6s\nSummed time to look up word 0.0044s\n\n```", "```py\n$ python text_example_dawg_load_only.py\nRAM at start 38.4MiB\nRAM after load 109.0MiB\nSummed time to look up word 0.0051s\n\n```", "```py\nimport marisa_trie\n...\n    words_trie = marisa_trie.Trie(text_example.readers)\n```", "```py\n$ python text_example_trie.py\nRAM at start 38.3MiB\nRAM after creating trie 419.9MiB, took 35.1s\nThe trie contains 11595290 words\nSummed time to look up word 0.0148s\n\n```", "```py\n$ python text_example_trie_load_only.py\nRAM at start 38.5MiB\nRAM after loading trie from disk 76.7MiB, took 0.0s\nThe trie contains 11595290 words\nSummed time to look up word 0.0092s\n\n```", "```py\nIn [2]: from sklearn.feature_extraction import DictVectorizer\n   ...:\n   ...: dv = DictVectorizer()\n   ...: # frequency counts for [\"there is a cat\", \"there is a cat and a dog\"]\n   ...: token_dict = [{'there': 1, 'is': 1, 'a': 1, 'cat': 1},\n   ...:               {'there': 1, 'is': 1, 'a': 2, 'cat': 1, 'and': 1, 'dog': 1}]\n\nIn [3]: dv.fit(token_dict)\n   ...:\n   ...: print(\"Vocabulary:\")\n   ...: pprint(dv.vocabulary_)\n\nVocabulary:\n{'a': 0, 'and': 1, 'cat': 2, 'dog': 3, 'is': 4, 'there': 5}\n\nIn [4]: X = dv.transform(token_dict)\n```", "```py\nIn [5]: print(\"Reversing the transform:\")\n   ...: pprint(dv.inverse_transform(X))\n\nReversing the transform:\n[{'a': 1, 'cat': 1, 'is': 1, 'there': 1},\n {'a': 2, 'and': 1, 'cat': 1, 'dog': 1, 'is': 1, 'there': 1}]\n```", "```py\nIn [6]: from sklearn.feature_extraction import FeatureHasher\n   ...:\n   ...: fh = FeatureHasher(n_features=10, alternate_sign=False)\n   ...: fh.fit(token_dict)\n   ...: X = fh.transform(token_dict)\n   ...: pprint(X.toarray().astype(np.int_))\n   ...:\narray([[1, 0, 0, 0, 0, 0, 0, 2, 0, 1],\n       [2, 0, 0, 1, 0, 1, 0, 2, 0, 1]])\n\nIn [7]: extra_token_dict = [{'there': 1, 'is': 1}, ]\n   ...: X = fh.transform(extra_token_dict)\n   ...: print(X.toarray().astype(np.int_))\n   ...:\n[[0 0 0 0 0 0 0 2 0 0]]\n```", "```py\nLoading 20 newsgroups training data\n18846 documents - 35.855MB\n\nDictVectorizer on frequency dicts\nDictVectorizer has shape (14134, 4335793) with 78,872,376 bytes\n and 9,859,047 non-zero items in 42.15 seconds\nVocabulary has 4,335,793 tokens\nLogisticRegression score 0.89 in 1179.33 seconds\n\nFeatureHasher on frequency dicts\nFeatureHasher has shape (14134, 1048576) with 78,787,936 bytes\n and 9,848,492 non-zero items in 21.59 seconds\nLogisticRegression score 0.89 in 903.35 seconds\n```", "```py\n>>> from scipy import sparse\n>>> A_sparse = sparse.random(2048, 2048, 0.05).tocsr()\n>>> A_sparse\n<2048x2048 sparse matrix of type '<class 'numpy.float64'>'\n        with 209715 stored elements in Compressed Sparse Row format>\n>>> %timeit A_sparse * A_sparse\n150 ms ± 1.71 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n>>> A_dense = A_sparse.todense()\n>>> type(A_dense)\nnumpy.matrix\n>>> %timeit A_dense * A_dense\n571 ms ± 14.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n```", "```py\n\"\"\"Approximate Morris counter supporting many counters\"\"\"\nimport math\nimport random\nimport array\n\nSMALLEST_UNSIGNED_INTEGER = 'B' # unsigned char, typically 1 byte\n\nclass MorrisCounter(object):\n    \"\"\"Approximate counter, stores exponent and counts approximately 2^exponent\n\n https://en.wikipedia.org/wiki/Approximate_counting_algorithm\"\"\"\n    def __init__(self, type_code=SMALLEST_UNSIGNED_INTEGER, nbr_counters=1):\n        self.exponents = array.array(type_code, [0] * nbr_counters)\n\n    def __len__(self):\n        return len(self.exponents)\n\n    def add_counter(self):\n        \"\"\"Add a new zeroed counter\"\"\"\n        self.exponents.append(0)\n\n    def get(self, counter=0):\n        \"\"\"Calculate approximate value represented by counter\"\"\"\n        return math.pow(2, self.exponents[counter])\n\n    def add(self, counter=0):\n        \"\"\"Probabilistically add 1 to counter\"\"\"\n        value = self.get(counter)\n        probability = 1.0 / value\n        if random.uniform(0, 1) < probability:\n            self.exponents[counter] += 1\n\nif __name__ == \"__main__\":\n    mc = MorrisCounter()\n    print(\"MorrisCounter has {} counters\".format(len(mc)))\n    for n in range(10):\n        print(\"Iteration %d, MorrisCounter has: %d\" % (n, mc.get()))\n        mc.add()\n\n    for n in range(990):\n        mc.add()\n    print(\"Iteration 1000, MorrisCounter has: %d\" % (mc.get()))\n```", "```py\n>>> mc = MorrisCounter()\n>>> mc.get()\n1.0\n\n>>> mc.add()\n>>> mc.get()\n2.0\n\n>>> mc.add()\n>>> mc.get()\n4.0\n\n>>> mc.add()\n>>> mc.get()\n4.0\n```", "```py\nimport mmh3\nfrom blist import sortedset\n\nclass KMinValues:\n    def __init__(self, num_hashes):\n        self.num_hashes = num_hashes\n        self.data = sortedset()\n\n    def add(self, item):\n        item_hash = mmh3.hash(item)\n        self.data.add(item_hash)\n        if len(self.data) > self.num_hashes:\n            self.data.pop()\n\n    def __len__(self):\n        if len(self.data) <= 2:\n            return 0\n        length = (self.num_hashes - 1) * (2 ** 32 - 1) /\n                 (self.data[-2] + 2 ** 31 - 1)\n        return int(length)\n```", "```py\n>>> from countmemaybe import KMinValues\n\n>>> kmv1 = KMinValues(k=1024)\n\n>>> kmv2 = KMinValues(k=1024)\n\n>>> for i in range(0,50000): ![1](Images/1.png)\n    kmv1.add(str(i))\n   ...:\n\n>>> for i in range(25000, 75000): ![2](Images/2.png)\n    kmv2.add(str(i))\n   ...:\n\n>>> print(len(kmv1))\n50416\n\n>>> print(len(kmv2))\n52439\n\n>>> print(kmv1.cardinality_intersection(kmv2))\n25900.2862992\n\n>>> print(kmv1.cardinality_union(kmv2))\n75346.2874158\n```", "```py\ndef multi_hash(key, num_hashes):\n    hash1, hash2 = hashfunction(key)\n    for i in range(num_hashes):\n        yield (hash1 + i * hash2) % (2^32 - 1)\n```", "```py\nimport math\n\nimport bitarray\nimport mmh3\n\nclass BloomFilter:\n    def __init__(self, capacity, error=0.005):\n        \"\"\"\n Initialize a Bloom filter with given capacity and false positive rate\n \"\"\"\n        self.capacity = capacity\n        self.error = error\n        self.num_bits = int((-capacity * math.log(error)) // math.log(2) ** 2 + 1)\n        self.num_hashes = int((self.num_bits * math.log(2)) // capacity + 1)\n        self.data = bitarray.bitarray(self.num_bits)\n\n    def _indexes(self, key):\n        h1, h2 = mmh3.hash64(key)\n        for i in range(self.num_hashes):\n            yield (h1 + i * h2) % self.num_bits\n\n    def add(self, key):\n        for index in self._indexes(key):\n            self.data[index] = True\n\n    def __contains__(self, key):\n        return all(self.data[index] for index in self._indexes(key))\n\n    def __len__(self):\n        bit_off_num = self.data.count(True)\n        bit_off_percent = 1.0 - bit_off_num / self.num_bits\n        length = -1.0 * self.num_bits * math.log(bit_off_percent) / self.num_hashes\n        return int(length)\n\n    @staticmethod\n    def union(bloom_a, bloom_b):\n        assert bloom_a.capacity == bloom_b.capacity, \"Capacities must be equal\"\n        assert bloom_a.error == bloom_b.error, \"Error rates must be equal\"\n\n        bloom_union = BloomFilter(bloom_a.capacity, bloom_a.error)\n        bloom_union.data = bloom_a.data | bloom_b.data\n        return bloom_union\n```", "```py\nfrom bloomfilter import BloomFilter\n\nclass ScalingBloomFilter:\n    def __init__(self, capacity, error=0.005, max_fill=0.8,\n                 error_tightening_ratio=0.5):\n        self.capacity = capacity\n        self.base_error = error\n        self.max_fill = max_fill\n        self.items_until_scale = int(capacity * max_fill)\n        self.error_tightening_ratio = error_tightening_ratio\n        self.bloom_filters = []\n        self.current_bloom = None\n        self._add_bloom()\n\n    def _add_bloom(self):\n        new_error = self.base_error * self.error_tightening_ratio ** len(\n            self.bloom_filters\n        )\n        new_bloom = BloomFilter(self.capacity, new_error)\n        self.bloom_filters.append(new_bloom)\n        self.current_bloom = new_bloom\n        return new_bloom\n\n    def add(self, key):\n        if key in self:\n            return True\n        self.current_bloom.add(key)\n        self.items_until_scale -= 1\n        if self.items_until_scale == 0:\n            bloom_size = len(self.current_bloom)\n            bloom_max_capacity = int(self.current_bloom.capacity * self.max_fill)\n\n            # We may have been adding many duplicate values into the Bloom, so\n            # we need to check if we actually need to scale or if we still have\n            # space\n            if bloom_size >= bloom_max_capacity:\n                self._add_bloom()\n                self.items_until_scale = bloom_max_capacity\n            else:\n                self.items_until_scale = int(bloom_max_capacity - bloom_size)\n        return False\n\n    def __contains__(self, key):\n        return any(key in bloom for bloom in self.bloom_filters)\n\n    def __len__(self):\n        return int(sum(len(bloom) for bloom in self.bloom_filters))\n```", "```py\n>>> bloom = BloomFilter(100)\n\n>>> for i in range(50):\n   ....:     bloom.add(str(i))\n   ....:\n\n>>> \"20\" in bloom\nTrue\n\n>>> \"25\" in bloom\nTrue\n\n>>> \"51\" in bloom\nFalse\n\n>>> num_false_positives = 0\n\n>>> num_true_negatives = 0\n\n>>> # None of the following numbers should be in the Bloom.\n>>> # If one is found in the Bloom, it is a false positive.\n>>> for i in range(51,10000):\n   ....:     if str(i) in bloom:\n   ....:         num_false_positives += 1\n   ....:     else:\n   ....:         num_true_negatives += 1\n   ....:\n\n>>> num_false_positives\n54\n\n>>> num_true_negatives\n9895\n\n>>> false_positive_rate = num_false_positives / float(10000 - 51)\n\n>>> false_positive_rate\n0.005427681173987335\n\n>>> bloom.error\n0.005\n```", "```py\n>>> bloom_a = BloomFilter(200)\n\n>>> bloom_b = BloomFilter(200)\n\n>>> for i in range(50):\n   ...:     bloom_a.add(str(i))\n   ...:\n\n>>> for i in range(25,75):\n   ...:     bloom_b.add(str(i))\n   ...:\n\n>>> bloom = BloomFilter.union(bloom_a, bloom_b)\n\n>>> \"51\" in bloom_a ![1](Images/1.png)\nOut[9]: False\n\n>>> \"24\" in bloom_b ![2](Images/2.png)\nOut[10]: False\n\n>>> \"55\" in bloom ![3](Images/3.png)\nOut[11]: True\n\n>>> \"25\" in bloom\nOut[12]: True\n```", "```py\nimport mmh3\n\ndef trailing_zeros(number):\n    \"\"\"\n Returns the index of the first bit set to 1 from the right side of a 32-bit\n integer\n >>> trailing_zeros(0)\n 32\n >>> trailing_zeros(0b1000)\n 3\n >>> trailing_zeros(0b10000000)\n 7\n \"\"\"\n    if not number:\n        return 32\n    index = 0\n    while (number >> index) & 1 == 0:\n        index += 1\n    return index\n\nclass LogLogRegister:\n    counter = 0\n    def add(self, item):\n        item_hash = mmh3.hash(str(item))\n        return self._add(item_hash)\n\n    def _add(self, item_hash):\n        bit_index = trailing_zeros(item_hash)\n        if bit_index > self.counter:\n            self.counter = bit_index\n\n    def __len__(self):\n        return 2**self.counter\n```", "```py\nimport mmh3\n\nfrom llregister import LLRegister\n\nclass LL:\n    def __init__(self, p):\n        self.p = p\n        self.num_registers = 2 ** p\n        self.registers = [LLRegister() for i in range(int(2 ** p))]\n        self.alpha = 0.7213 / (1.0 + 1.079 / self.num_registers)\n\n    def add(self, item):\n        item_hash = mmh3.hash(str(item))\n        register_index = item_hash & (self.num_registers - 1)\n        register_hash = item_hash >> self.p\n        self.registers[register_index]._add(register_hash)\n\n    def __len__(self):\n        register_sum = sum(h.counter for h in self.registers)\n        length = (self.num_registers * self.alpha *\n                  2 ** (register_sum / self.num_registers))\n        return int(length)\n```", "```py\nimport math\n\nfrom ll import LL\n\nclass HyperLogLog(LL):\n    def __len__(self):\n        indicator = sum(2 ** -m.counter for m in self.registers)\n        E = self.alpha * (self.num_registers ** 2) / indicator\n\n        if E <= 5.0 / 2.0 * self.num_registers:\n            V = sum(1 for m in self.registers if m.counter == 0)\n            if V != 0:\n                Estar = (self.num_registers *\n                         math.log(self.num_registers / (1.0 * V), 2))\n\n            else:\n                Estar = E\n        else:\n            if E <= 2 ** 32 / 30.0:\n                Estar = E\n            else:\n                Estar = -2 ** 32 * math.log(1 - E / 2 ** 32, 2)\n        return int(Estar)\n\nif __name__ == \"__main__\":\n    import mmh3\n\n    hll = HyperLogLog(8)\n    for i in range(100000):\n        hll.add(mmh3.hash(str(i)))\n    print(len(hll))\n```"]