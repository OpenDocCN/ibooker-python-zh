["```py\nworker_template = make_pod_spec(image='holdenk/dask:latest',\n                                memory_limit='8G', memory_request='8G',\n                                cpu_limit=1, cpu_request=1)\nworker_template.spec.containers[0].resources.limits[\"gpu\"] = 1\nworker_template.spec.containers[0].resources.requests[\"gpu\"] = 1\nworker_template.spec.containers[0].args[0] = \"dask-cuda-worker --resources 'GPU=1'\"\nworker_template.spec.containers[0].env.append(\"NVIDIA_VISIBLE_DEVICES=ALL\")\n# Or append --resources \"GPU=2\"\n```", "```py\nfrom dask_cuda import LocalCUDACluster\nfrom dask.distributed import Client\n#NOTE: The resources= flag is important; by default the \n# LocalCUDACluster *does not* label any resources, which can make\n# porting your code to a cluster where some workers have GPUs and \n# some don't painful.\ncluster = LocalCUDACluster(resources={\"GPU\": 1})\nclient = Client(cluster)\n```", "```py\n# Use the Dask base image; for arm64, though, we have to use custom built\n# FROM ghcr.io/dask/dask\nFROM holdenk/dask:latest\n\n# arm64 channel\nRUN conda config --add channels rpi\n# Numba and conda-forge channels\nRUN conda config --add channels numba\nRUN conda config --add channels conda-forge\n# Some CUDA-specific stuff\nRUN conda config --add channels rapidsai\n# Accelerator libraries often involve a lot of native code, so it's\n# faster to install with conda\nRUN conda install numba -y\n# GPU support (NV)\nRUN conda install cudatoolkit -y\n# GPU support (AMD)\nRUN conda install roctools -y || echo \"No roc tools on $(uname -a)\"\n# A lot of GPU acceleration libraries are in the rapidsai channel\n# These are not installable with pip\nRUN conda install cudf -y\n```", "```py\n#/bin/bash\nset -ex\n\ndocker buildx build -t holdenk/dask-extended  --platform \\\n  linux/arm64,linux/amd64 --push . -f Dockerfile\ndocker buildx build -t holdenk/dask-extended-notebook  --platform \\\n  linux/arm64,linux/amd64 --push . -f NotebookDockerfile\n```", "```py\nfuture = client.submit(how_many_gpus, 1, resources={'GPU': 1})\n```", "```py\nwith dask.annotate(resources={'GPU': 1}):\n    future = client.submit(how_many_gpus, 1)\n```", "```py\n# Works in local mode, but not distributed\n@dask.delayed\n@guvectorize(['void(float64[:], intp[:], float64[:])'],\n             '(n),()->(n)')\ndef delayed_move_mean(a, window_arr, out):\n    window_width = window_arr[0]\n    asum = 0.0\n    count = 0\n    for i in range(window_width):\n        asum += a[i]\n        count += 1\n        out[i] = asum / count\n    for i in range(window_width, len(a)):\n        asum += a[i] - a[i - window_width]\n        out[i] = asum / count\n\narr = np.arange(20, dtype=np.float64).reshape(2, 10)\nprint(arr)\nprint(dask.compute(delayed_move_mean(arr, 3)))\n```", "```py\n@guvectorize(['void(float64[:], intp[:], float64[:])'],\n             '(n),()->(n)')\ndef move_mean(a, window_arr, out):\n    window_width = window_arr[0]\n    asum = 0.0\n    count = 0\n    for i in range(window_width):\n        asum += a[i]\n        count += 1\n        out[i] = asum / count\n    for i in range(window_width, len(a)):\n        asum += a[i] - a[i - window_width]\n        out[i] = asum / count\n\narr = np.arange(20, dtype=np.float64).reshape(2, 10)\nprint(arr)\nprint(move_mean(arr, 3))\n\ndef wrapped_move_mean(*args):\n    return move_mean(*args)\n\na = dask.delayed(wrapped_move_mean)(arr, 3)\n```"]