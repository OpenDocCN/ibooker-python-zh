- en: Chapter 10\. Clusters and Job Queues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A *cluster* is commonly recognized to be a collection of computers working together
    to solve a common task. It could be viewed from the outside as a larger single
    system.
  prefs: []
  type: TYPE_NORMAL
- en: In the 1990s, the notion of using a cluster of commodity PCs on a local area
    network for clustered processing—known as a [Beowulf cluster](https://oreil.ly/2aNvw)—became
    popular. [Google](https://oreil.ly/V83g1) later gave the practice a boost by using
    clusters of commodity PCs in its own data centers, particularly for running MapReduce
    tasks. At the other end of the scale, the [TOP500 project](https://oreil.ly/rHOQO)
    ranks the most powerful computer systems each year; these typically have a clustered
    design, and the fastest machines all use Linux.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Web Services (AWS) is commonly used both for engineering production clusters
    in the cloud and for building on-demand clusters for short-lived tasks like machine
    learning. With AWS you can rent tiny to huge machines with 10s of CPUs and up
    to 768 GB of RAM for $1 to $15 an hour. Multiple GPUs can be rented at extra cost.
    Look at [“Using IPython Parallel to Support Research”](#clustering-ipython) and
    the ElastiCluster package if you’d like to explore AWS or other providers for
    ad hoc clusters on compute-heavy or RAM-heavy tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Different computing tasks require different configurations, sizes, and capabilities
    in a cluster. We’ll define some common scenarios in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '*Before* you move to a clustered solution, do make sure that you have done
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Profiled your system so you understand the bottlenecks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploited compiler solutions like Numba and Cython
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploited multiple cores on a single machine (possibly a big machine with many
    cores) with Joblib or `multiprocessing`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploited techniques for using less RAM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keeping your system to one machine will make your life easier (even if the “one
    machine” is a really beefy computer with lots of RAM and many CPUs). Move to a
    cluster if you really need a *lot* of CPUs or the ability to process data from
    disks in parallel, or if you have production needs like high resiliency and rapid
    speed of response. Most research scenarios do not need resilience or scalability
    and are limited to few people, so the simplest solution is often the most sensible.
  prefs: []
  type: TYPE_NORMAL
- en: A benefit of staying on one *large* machine is that a tool like Dask can quickly
    parallelize your Pandas or plain Python code with no networking complications.
    Dask can also control a cluster of machines to parallelize Pandas, NumPy, and
    pure Python problems. Swifter automatically parallelizes some multicore single-machine
    cases by piggybacking on Dask. We introduce both Dask and Swifter later in this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Benefits of Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most obvious benefit of a cluster is that you can easily scale computing
    requirements—if you need to process more data or to get an answer faster, you
    just add more machines (or *nodes*).
  prefs: []
  type: TYPE_NORMAL
- en: By adding machines, you can also improve reliability. Each machine’s components
    have a certain likelihood of failing, but with a good design, the failure of a
    number of components will not stop the operation of the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Clusters are also used to create systems that scale dynamically. A common use
    case is to cluster a set of servers that process web requests or associated data
    (e.g., resizing user photos, transcoding video, or transcribing speech) and to
    activate more servers as demand increases at certain times of the day.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic scaling is a very cost-effective way of dealing with nonuniform usage
    patterns, as long as the machine activation time is fast enough to deal with the
    speed of changing demand.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Consider the effort versus the reward of building a cluster. Whilst the parallelization
    gains of a cluster can feel attractive, do consider the costs associated with
    constructing and maintaining a cluster. They fit well for long-running processes
    in a production environment or for well-defined and oft-repeated R&D tasks. They
    are less attractive for variable and short-lived R&D tasks.
  prefs: []
  type: TYPE_NORMAL
- en: A subtler benefit of clustering is that clusters can be separated geographically
    but still centrally controlled. If one geographic area suffers an outage (due
    to a flood or power loss, for example), the other cluster can continue to work,
    perhaps with more processing units being added to handle the demand. Clusters
    also allow you to run heterogeneous software environments (e.g., different versions
    of operating systems and processing software), which *might* improve the robustness
    of the overall system—note, though, that this is definitely an expert-level topic!
  prefs: []
  type: TYPE_NORMAL
- en: Drawbacks of Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Moving to a clustered solution requires a change in thinking. This is an evolution
    of the change in thinking required when you move from serial to parallel code,
    as we introduced back in [Chapter 9](ch09_split_000.xhtml#multiprocessing). Suddenly
    you have to consider what happens when you have more than one machine—you have
    latency between machines, you need to know if your other machines are working,
    and you need to keep all the machines running the same version of your software.
    System administration is probably your biggest challenge.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, you normally have to think hard about the algorithms you are implementing
    and what happens once you have all these additional moving parts that may need
    to stay in sync. This additional planning can impose a heavy mental tax; it is
    likely to distract you from your core task, and once a system grows large enough,
    you’ll probably need to add a dedicated engineer to your team.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We’ve tried to focus on using one machine efficiently in this book because we
    believe that life is easier if you’re dealing with only one computer rather than
    a collection (though we confess it can be *way* more fun to play with a cluster—until
    it breaks). If you can scale vertically (by buying more RAM or more CPUs), it
    is worth investigating this approach in favor of clustering. Of course, your processing
    needs may exceed what’s possible with vertical scaling, or the robustness of a
    cluster may be more important than having a single machine. If you’re a single
    person working on this task, though, bear in mind also that running a cluster
    will suck up some of your time.
  prefs: []
  type: TYPE_NORMAL
- en: When designing a clustered solution, you’ll need to remember that each machine’s
    configuration might be different (each machine will have a different load and
    different local data). How will you get all the right data onto the machine that’s
    processing your job? Does the latency involved in moving the job and the data
    amount to a problem? Do your jobs need to communicate partial results to one another?
    What happens if a process fails or a machine dies or some hardware wipes itself
    when several jobs are running? Failures can be introduced if you don’t consider
    these questions.
  prefs: []
  type: TYPE_NORMAL
- en: You should also consider that failures *can be acceptable*. For example, you
    probably don’t need 99.999% reliability when you’re running a content-based web
    service—if on occasion a job fails (e.g., a picture doesn’t get resized quickly
    enough) and the user is required to reload a page, that’s something that everyone
    is already used to. It might not be the solution you want to give to the user,
    but accepting a little bit of failure typically reduces your engineering and management
    costs by a worthwhile margin. On the flip side, if a high-frequency trading system
    experiences failures, the cost of bad stock market trades could be considerable!
  prefs: []
  type: TYPE_NORMAL
- en: Maintaining a fixed infrastructure can become expensive. Machines are relatively
    cheap to purchase, but they have an awful habit of going wrong—automatic software
    upgrades can glitch, network cards fail, disks have write errors, power supplies
    can give spikey power that disrupts data, cosmic rays can flip a bit in a RAM
    module. The more computers you have, the more time will be lost to dealing with
    these issues. Sooner or later you’ll want to bring in a system engineer who can
    deal with these problems, so add another $100,000 to the budget. Using a cloud-based
    cluster can mitigate a lot of these problems (it costs more, but you don’t have
    to deal with the hardware maintenance), and some cloud providers also offer a
    [spot-priced market](http://bit.ly/spot-instances) for cheap but temporary computing
    resources.
  prefs: []
  type: TYPE_NORMAL
- en: An insidious problem with a cluster that grows organically over time is that
    it’s possible no one has documented how to restart it safely if everything gets
    turned off. If you don’t have a documented restart plan, you should assume you’ll
    have to write one at the worst possible time (one of your authors has been involved
    in debugging this sort of problem on Christmas Eve—this is not the Christmas present
    you want!). At this point you’ll also learn just how long it can take each part
    of a system to get up to speed—it might take minutes for each part of a cluster
    to boot and to start to process jobs, so if you have 10 parts that operate in
    succession, it might take an hour to get the whole system running from cold. The
    consequence is that you might have an hour’s worth of backlogged data. Do you
    then have the necessary capacity to deal with this backlog in a timely fashion?
  prefs: []
  type: TYPE_NORMAL
- en: Slack behavior can be a cause of expensive mistakes, and complex and hard-to-anticipate
    behavior can cause unexpected and expensive outcomes. Let’s look at two high-profile
    cluster failures and see what lessons we can learn.
  prefs: []
  type: TYPE_NORMAL
- en: $462 Million Wall Street Loss Through Poor Cluster Upgrade Strategy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In 2012, the high-frequency trading firm [Knight Capital lost $462 million](http://bit.ly/Wall_Street_crash)
    after a bug was introduced during a software upgrade in a cluster. The software
    made orders for more shares than customers had requested.
  prefs: []
  type: TYPE_NORMAL
- en: In the trading software, an older flag was repurposed for a new function. The
    upgrade was rolled out to seven of the eight live machines, but the eighth machine
    used older code to handle the flag, which resulted in the wrong trades being made.
    The Securities and Exchange Commission (SEC) noted that Knight Capital didn’t
    have a second technician review the upgrade and in fact had no established process
    for reviewing such an upgrade.
  prefs: []
  type: TYPE_NORMAL
- en: The underlying mistake seems to have had two causes. The first was that the
    software development process hadn’t removed an obsolete feature, so the stale
    code stayed around. The second was that no manual review process was in place
    to confirm that the upgrade was completed successfully.
  prefs: []
  type: TYPE_NORMAL
- en: 'Technical debt adds a cost that eventually has to be paid—preferably by taking
    time when not under pressure to remove the debt. Always use unit tests, both when
    building and when refactoring code. The lack of a written checklist to run through
    during system upgrades, along with a second pair of eyes, could cost you an expensive
    failure. There’s a reason that airplane pilots have to work through a takeoff
    checklist: it means that nobody ever skips the important steps, no matter how
    many times they might have done them before!'
  prefs: []
  type: TYPE_NORMAL
- en: Skype’s 24-Hour Global Outage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Skype suffered a [24-hour planetwide failure](http://bit.ly/Skype_outage) in
    2010. Behind the scenes, Skype is supported by a peer-to-peer network. An overload
    in one part of the system (used to process offline instant messages) caused delayed
    responses from Windows clients; some versions of the Windows client didn’t properly
    handle the delayed responses and crashed. In all, approximately 40% of the live
    clients crashed, including 25% of the public supernodes. Supernodes are critical
    to routing data in the network.
  prefs: []
  type: TYPE_NORMAL
- en: With 25% of the routing offline (it came back on, but slowly), the network overall
    was under great strain. The crashed Windows client nodes were also restarting
    and attempting to rejoin the network, adding a new volume of traffic on the already
    overloaded system. The supernodes have a back-off procedure if they experience
    too much load, so they started to shut down in response to the waves of traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Skype became largely unavailable for 24 hours. The recovery process involved
    first setting up hundreds of new “mega-supernodes” configured to deal with the
    increased traffic, and then following up with thousands more. Over the coming
    days, the network recovered.
  prefs: []
  type: TYPE_NORMAL
- en: This incident caused a lot of embarrassment for Skype; clearly, it also changed
    its focus to damage limitation for several tense days. Customers were forced to
    look for alternative solutions for voice calls, which was likely a marketing boon
    for competitors.
  prefs: []
  type: TYPE_NORMAL
- en: Given the complexity of the network and the escalation of failures that occurred,
    this failure likely would have been hard both to predict and to plan for. The
    reason that *all* of the nodes on the network didn’t fail was due to different
    versions of the software and different platforms—there’s a reliability benefit
    to having a heterogeneous network rather than a homogeneous system.
  prefs: []
  type: TYPE_NORMAL
- en: Common Cluster Designs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is common to start with a local ad hoc cluster of reasonably equivalent machines.
    You might wonder if you can add old computers to an ad hoc network, but typically
    older CPUs eat a lot of power and run very slowly, so they don’t contribute nearly
    as much as you might hope compared to one new, high-specification machine. An
    in-office cluster requires someone who can maintain it. A cluster on [Amazon’s
    EC2](http://aws.amazon.com/ec2) or [Microsoft’s Azure](http://azure.microsoft.com/en-us),
    or one run by an academic institution, offloads the hardware support to the provider’s
    team.
  prefs: []
  type: TYPE_NORMAL
- en: If you have well-understood processing requirements, it might make sense to
    design a custom cluster—perhaps one that uses an InfiniBand high-speed interconnect
    in place of gigabit Ethernet, or one that uses a particular configuration of RAID
    drives that support your read, write, or resiliency requirements. You might want
    to combine CPUs and GPUs on some machines, or just default to CPUs.
  prefs: []
  type: TYPE_NORMAL
- en: You might want a massively decentralized processing cluster, like the ones used
    by projects such as *SETI@home* and *Folding@home* through [the Berkeley Open
    Infrastructure for Network Computing (BOINC) system](https://oreil.ly/jNCA9).
    They share a centralized coordination system, but the computing nodes join and
    leave the project in an ad hoc fashion.
  prefs: []
  type: TYPE_NORMAL
- en: On top of the hardware design, you can run different software architectures.
    Queues of work are the most common and easiest to understand. Typically, jobs
    are put onto a queue and consumed by a processor. The result of the processing
    might go onto another queue for further processing, or it might be used as a final
    result (e.g., being added into a database). Message-passing systems are slightly
    different—messages get put onto a message bus and are then consumed by other machines.
    The messages might time out and get deleted, and they might be consumed by multiple
    machines. In a more complex system, processes talk to each other using interprocess
    communication—this can be considered an expert-level configuration, as there are
    lots of ways that you can set it up badly, which will result in you losing your
    sanity. Go down the IPC route only if you really know that you need it.
  prefs: []
  type: TYPE_NORMAL
- en: How to Start a Clustered Solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The easiest way to start a clustered system is to begin with one machine that
    will run both the job server and a job processor (just one job processor for one
    CPU). If your tasks are CPU-bound, run one job processor per CPU; if your tasks
    are I/O-bound, run several per CPU. If they’re RAM-bound, be careful that you
    don’t run out of RAM. Get your single-machine solution working with one processor
    and then add more. Make your code fail in unpredictable ways (e.g., do a `1/0`
    in your code, use `kill -9` *`<pid>`* on your worker, pull the power plug from
    the socket so the whole machine dies) to check if your system is robust.
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, you’ll want to do heavier testing than this—a unit test suite full
    of coding errors and artificial exceptions is good. Ian likes to throw in unexpected
    events, like having a processor run a set of jobs while an external process is
    systematically killing important processes and confirming that these all get restarted
    cleanly by whatever monitoring process is being used.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have one running job processor, add a second. Check that you’re not
    using too much RAM. Do you process jobs twice as fast as before?
  prefs: []
  type: TYPE_NORMAL
- en: Now introduce a second machine, with just one job processor on that new machine
    and no job processors on the coordinating machine. Does it process jobs as fast
    as when you had the processor on the coordinating machine? If not, why not? Is
    latency a problem? Do you have different configurations? Maybe you have different
    machine hardware, like CPUs, RAM, and cache sizes?
  prefs: []
  type: TYPE_NORMAL
- en: Now add another nine computers and test to see if you’re processing jobs 10
    times faster than before. If not, why not? Are network collisions now occurring
    that slow down your overall processing rate?
  prefs: []
  type: TYPE_NORMAL
- en: To reliably start the cluster’s components when the machine boots, we tend to
    use either a `cron` job, [Circus](https://oreil.ly/MCUOQ), or [`supervisord`](http://supervisord.org).
    Circus and `supervisord` are both Python-based and have been around for years.
    `cron` is old but very reliable if you’re just starting scripts like a monitoring
    process that can start subprocesses as required.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have a reliable cluster, you might want to introduce a random-killer
    tool like Netflix’s [Chaos Monkey](https://oreil.ly/sL5nG), which deliberately
    kills parts of your system to test them for resiliency. Your processes and your
    hardware will die eventually, and it doesn’t hurt to know that you’re likely to
    survive at least the errors you predict might happen.
  prefs: []
  type: TYPE_NORMAL
- en: Ways to Avoid Pain When Using Clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In one particularly painful experience Ian encountered, a series of queues in
    a clustered system ground to a halt. Later queues were not being consumed, so
    they filled up. Some of the machines ran out of RAM, so their processes died.
    Earlier queues were being processed but couldn’t pass their results to the next
    queue, so they crashed. In the end the first queue was being filled but not consumed,
    so it crashed. After that, we were paying for data from a supplier that ultimately
    was discarded. You must sketch out some notes to consider the various ways your
    cluster will die and what will happen when (not *if*) it does. Will you lose data
    (and is this a problem)? Will you have a large backlog that’s too painful to process?
  prefs: []
  type: TYPE_NORMAL
- en: Having a system that’s easy to debug *probably* beats having a faster system.
    Engineering time and the cost of downtime are *probably* your largest expenses
    (this isn’t true if you’re running a missile defense program, but it is probably
    true for a start-up). Rather than shaving a few bytes by using a low-level compressed
    binary protocol, consider using human-readable text in JSON when passing messages.
    It does add an overhead for sending the messages and decoding them, but when you’re
    left with a partial database after a core computer has caught fire, you’ll be
    glad that you can read the important messages quickly as you work to bring the
    system back online.
  prefs: []
  type: TYPE_NORMAL
- en: Make sure it is cheap in time and money to deploy updates to the system—both
    operating system updates and new versions of your software. Every time anything
    changes in the cluster, you risk the system responding in odd ways if it is in
    a schizophrenic state. Make sure you use a deployment system like [Fabric](http://www.fabfile.org),
    [Salt](https://oreil.ly/esyVt), [Chef](http://www.getchef.com), or [Puppet](http://puppetlabs.com),
    or a system image like a Debian *.deb*, a RedHat *.rpm*, or an [Amazon Machine
    Image](https://oreil.ly/5eLt4). Being able to robustly deploy an update that upgrades
    an entire cluster (with a report on any problems found) massively reduces stress
    during difficult times.
  prefs: []
  type: TYPE_NORMAL
- en: Positive reporting is useful. Every day, send an email to someone detailing
    the performance of the cluster. If that email doesn’t turn up, that’s a useful
    clue that something’s happened. You’ll probably want other early warning systems
    that’ll notify you faster too; [Pingdom](https://www.pingdom.com) and [Server
    Density](https://www.serverdensity.com) are particularly useful here. A “dead
    man’s switch” that reacts to the absence of an event (e.g., [Dead Man’s Switch](http://www.deadmansswitch.net))
    is another useful backup.
  prefs: []
  type: TYPE_NORMAL
- en: Reporting to the team on the health of the cluster is very useful. This might
    be an admin page inside a web application, or a separate report. [Ganglia](http://ganglia.sourceforge.net)
    is great for this. Ian has seen a *Star Trek* LCARS-like interface running on
    a spare PC in an office that plays the “red alert” sound when problems are detected—that’s
    particularly effective at getting the attention of an entire office. We’ve even
    seen Arduinos driving analog instruments like old-fashioned boiler pressure gauges
    (they make a nice sound when the needle moves!) showing system load. This kind
    of reporting is important so that everyone understands the difference between
    “normal” and “this might ruin our Friday night!”
  prefs: []
  type: TYPE_NORMAL
- en: Two Clustering Solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section we introduce IPython Parallel and NSQ.
  prefs: []
  type: TYPE_NORMAL
- en: IPython clusters are easy to use on one machine with multiple cores. Since many
    researchers use IPython as their shell or work through Jupyter Notebooks, it is
    natural to also use it for parallel job control. Building a cluster requires a
    little bit of system administration knowledge. A huge win with IPython Parallel
    is that you can use remote clusters (Amazon’s AWS and EC2, for example) just as
    easily as local clusters.
  prefs: []
  type: TYPE_NORMAL
- en: NSQ is a production-ready queuing system. It has persistence (so if machines
    die, jobs can be picked up again by another machine) and strong mechanisms for
    scalability. With this greater power comes a slightly greater need for system
    administration and engineering skills. However, NSQ shines in its simplicity and
    ease of use. While many queuing systems exist (such as the popular [Kafka](https://kafka.apache.org)),
    none have such as low a barrier for entry as NSQ.
  prefs: []
  type: TYPE_NORMAL
- en: Using IPython Parallel to Support Research
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The IPython clustering support comes via the [IPython Parallel](https://oreil.ly/SAV5i)
    project. IPython becomes an interface to local and remote processing engines where
    data can be pushed among the engines and jobs can be pushed to remote machines.
    Remote debugging is possible, and the message passing interface (MPI) is optionally
    supported. This same ZeroMQ communication mechanism powers the Jupyter Notebook
    interface.
  prefs: []
  type: TYPE_NORMAL
- en: This is great for a research setting—you can push jobs to machines in a local
    cluster, interact and debug if there’s a problem, push data to machines, and collect
    results back, all interactively. Note also that PyPy runs IPython and IPython
    Parallel. The combination might be very powerful (if you don’t use `numpy`).
  prefs: []
  type: TYPE_NORMAL
- en: Behind the scenes, ZeroMQ is used as the messaging middleware—be aware that
    ZeroMQ provides no security by design. If you’re building a cluster on a local
    network, you can avoid SSH authentication. If you need security, SSH is fully
    supported, but it makes configuration a little more involved—start on a local
    trusted network and build out as you learn how each component works.
  prefs: []
  type: TYPE_NORMAL
- en: The project is split into four components. An *engine* is an extension of the
    IPython kernel; it is a synchronous Python interpreter that runs your code. You’ll
    run a set of engines to enable parallel computing. A *controller* provides an
    interface to the engines; it is responsible for work distribution and supplies
    a *direct* interface and a *load-balanced* interface that provides a work scheduler.
    A *hub* keeps track of engines, schedulers, and clients. *Schedulers* hide the
    synchronous nature of the engines and provide an asynchronous interface.
  prefs: []
  type: TYPE_NORMAL
- en: On the laptop, we start four engines using `ipcluster start -n 4`. In [Example 10-1](#cluster-ipython-firsttest),
    we start IPython and check that a local `Client` can see our four local engines.
    We can address all four engines using `c[:]`, and we apply a function to each
    engine—`apply_sync` takes a callable, so we supply a zero-argument `lambda` that
    will return a string. Each of our four local engines will run one of these functions,
    returning the same result.
  prefs: []
  type: TYPE_NORMAL
- en: Example 10-1\. Testing that we can see the local engines in IPython
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The engines we’ve constructed are now in an empty state. If we import modules
    locally, they won’t be imported into the remote engines.
  prefs: []
  type: TYPE_NORMAL
- en: A clean way to import both locally and remotely is to use the `sync_imports`
    context manager. In [Example 10-2](#cluster-ipython-secondtest), we’ll `import
    os` on both the local IPython and the four connected engines and then call `apply_sync`
    again on the four engines to fetch their PIDs.
  prefs: []
  type: TYPE_NORMAL
- en: If we didn’t do the remote imports, we’d get a `NameError`, as the remote engines
    wouldn’t know about the `os` module. We can also use `execute` to run any Python
    command remotely on the engines.
  prefs: []
  type: TYPE_NORMAL
- en: Example 10-2\. Importing modules into our remote engines
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'You’ll want to push data to the engines. The `push` command shown in [Example 10-3](#cluster-ipython-pushdata)
    lets you send a dictionary of items that are added to the global namespace of
    each engine. There’s a corresponding `pull` to retrieve items: you give it keys,
    and it’ll return the corresponding values from each of the engines.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 10-3\. Pushing shared data to the engines
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In [Example 10-4](#cluster-ipython-pi), we use these four engines to estimate
    pi. This time we use the `@require` decorator to import the `random` module in
    the engines. We use a direct view to send our work out to the engines; this blocks
    until all the results come back. Then we estimate pi as we did in [Example 9-1](ch09_split_000.xhtml#code-pi-lists-calculation).
  prefs: []
  type: TYPE_NORMAL
- en: Example 10-4\. Estimating pi using our local cluster
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In [Example 10-5](#cluster-ipython-pi-2) we run this on our four local engines.
    As in [Figure 9-5](ch09_split_000.xhtml#FIG-pi_monte_carlo_lists_4_processes),
    this takes approximately 20 seconds on the laptop.
  prefs: []
  type: TYPE_NORMAL
- en: Example 10-5\. Estimating pi using our local cluster in IPython
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: IPython Parallel offers much more than what’s shown here. Asynchronous jobs
    and mappings over larger input ranges are, of course, possible. MPI is supported,
    which can provide efficient data sharing. The Joblib library introduced in [“Replacing
    multiprocessing with Joblib”](ch09_split_000.xhtml#replacing-multiprocessing-with-joblib)
    can use IPython Parallel as a backend along with Dask (which we introduce in [“Parallel
    Pandas with Dask”](#clustering-dask)).
  prefs: []
  type: TYPE_NORMAL
- en: One particularly powerful feature of IPython Parallel is that it allows you
    to use larger clustering environments, including supercomputers and cloud services
    like Amazon’s EC2\. The [ElastiCluster project](https://elasticluster.readthedocs.io)
    has support for common parallel environments such as IPython and for deployment
    targets, including AWS, Azure, and OpenStack.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel Pandas with Dask
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dask aims to provide a suite of parallelization solutions that scales from a
    single core on a laptop to multicore machines to thousands of cores in a cluster.
    Think of it as “Apache Spark lite.” If you don’t need all of Apache Spark’s functionality
    (which includes replicated writes and multimachine failover) and you don’t want
    to support a second computation and storage environment, then Dask may provide
    the parallelized and bigger-than-RAM solution you’re after.
  prefs: []
  type: TYPE_NORMAL
- en: 'A task graph is constructed for the lazy evaluation of a number of computation
    scenarios, including pure Python, scientific Python, and machine learning with
    small, medium, and big datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: Bag
  prefs: []
  type: TYPE_NORMAL
- en: '`bag` enables parallelized computation on unstructured and semistructured data,
    including text files, JSON or user-defined objects. `map`, `filter`, and `groupby`
    are supported on generic Python objects, including lists and sets.'
  prefs: []
  type: TYPE_NORMAL
- en: Array
  prefs: []
  type: TYPE_NORMAL
- en: '`array` enables distributed and larger-than-RAM `numpy` operations. Many common
    operations are supported, including some linear algebra functions. Operations
    that are inefficient across cores (sorting, for example, and many linear algebra
    operations) are not supported. Threads are used, as NumPy has good thread support,
    so data doesn’t have to be copied during parallelized operations.'
  prefs: []
  type: TYPE_NORMAL
- en: Distributed DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: '`dataframe` enables distributed and larger-than-RAM `Pandas` operations; behind
    the scenes, Pandas is used to represent partial DataFrames that have been partitioned
    using their index. Operations are lazily computed using `.compute()` and otherwise
    look very similar to their Pandas counterparts. Supported functions include `groupby-aggregate`,
    `groupby-apply`, `value_counts`, `drop_duplicates`, and `merge`. By default, threads
    are used, but as Pandas is more GIL-bound than NumPy, you may want to look at
    the Process or Distributed scheduler options.'
  prefs: []
  type: TYPE_NORMAL
- en: Delayed
  prefs: []
  type: TYPE_NORMAL
- en: '`delayed` extends the idea we introduced with Joblib in [“Replacing multiprocessing
    with Joblib”](ch09_split_000.xhtml#replacing-multiprocessing-with-joblib) to parallelize
    *chains* of arbitrary Python functions in a lazy fashion. A `visualize()` function
    will draw the task graph to assist in diagnosing issues.'
  prefs: []
  type: TYPE_NORMAL
- en: Futures
  prefs: []
  type: TYPE_NORMAL
- en: The `Client` interface enables immediate execution and evolution of tasks, unlike
    `delayed`, which is lazy and doesn’t allow operations like adding or destroying
    tasks. The `Future` interface includes `Queue` and `Lock` to support task collaboration.
  prefs: []
  type: TYPE_NORMAL
- en: Dask-ML
  prefs: []
  type: TYPE_NORMAL
- en: A scikit-learn-like interface is provided for scalable machine learning. Dask-ML
    provides cluster support to some scikit-learn algorithms, and it reimplements
    some algorithms (e.g., the `linear_model` set) using Dask to enable learning on
    big data. It closes some of the gap to the Apache Spark distributed machine learning
    toolkit. It also provides support for XGBoost and TensorFlow to be used in a Dask
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'For Pandas users, Dask can help in two use cases: larger-than-RAM datasets
    and a desire for multicore parallelization.'
  prefs: []
  type: TYPE_NORMAL
- en: If your dataset is larger than Pandas can fit into RAM, Dask can split the dataset
    by rows into a set of partitioned DataFrames called a *Distributed DataFrame*.
    These DataFrames are split by their index; a subset of operations can be performed
    across each partition. As an example, if you have a set of multi-GB CSV files
    and want to calculate `value_counts` across all the files, Dask will perform partial
    `value_counts` on each DataFrame (one per file) and then combine the results into
    a single set of counts.
  prefs: []
  type: TYPE_NORMAL
- en: A second use case is to take advantage of the multiple cores on your laptop
    (and just as easily across a cluster); we’ll examine this use case here. Recall
    that in [Example 6-24](ch06_split_001.xhtml#pandas_ols_functions), we calculated
    the slope of the line across rows of values in a DataFrame with various approaches.
    Let’s use the two fastest approaches and parallelize them with Dask.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can use Dask (and Swifter, discussed in the next section) to parallelize
    any side-effect-free function that you’d usually use in an `apply` call. Ian has
    done this for numeric calculations and for calculating text metrics on multiple
    columns of text in a large DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: With Dask, we have to specify the number of *partitions* to make from our DataFrame;
    a good rule of thumb is to use at least as many partitions as cores so that each
    core can be used. In [Example 10-6](#calculate-ols-dask), we ask for eight partitions.
    We use `dd.from_pandas` to convert our regular Pandas DataFrame into a Dask Distributed
    DataFrame split into eight equal-sized sections.
  prefs: []
  type: TYPE_NORMAL
- en: We call our familiar `ddf.apply` on the Distributed DataFrame, specifying our
    function `ols_lstsq` and the optional expected return type via the `meta` argument.
    Dask requires us to specify when we should apply the computation with the `compute()`
    call; here, we specify the use of `processes` rather than the default `threads`
    to spread our work over multiple cores, avoiding Python’s GIL.
  prefs: []
  type: TYPE_NORMAL
- en: Example 10-6\. Calculating line slopes with multiple cores using Dask
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Running `ols_lstsq_raw` in [Example 10-7](#calculate-ols-dask-raw) with the
    same eight partitions (on four cores with four hyperthreads) in Ian’s laptop,
    we go from the previous single-threaded `apply` result of 6.8 seconds to 1.5 seconds—almost
    a 5× speedup.
  prefs: []
  type: TYPE_NORMAL
- en: Example 10-7\. Calculating line slopes with multiple cores using Dask
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Running `ols_lstsq_raw` with the same eight partitions takes us from the previous
    single-threaded `apply` result of 5.3 seconds with `raw=True` to 1.2 seconds—also
    almost a 5× speedup.
  prefs: []
  type: TYPE_NORMAL
- en: If we also use the compiled Numba function from [“Numba to Compile NumPy for
    Pandas”](ch07.xhtml#compiling-numba-for-pandas) with `raw=True`, our runtime drops
    from 0.58 seconds to 0.3 seconds—a further 2× speedup. Functions compiled with
    Numba using NumPy arrays on Pandas DataFrames work well with Dask for very little
    effort.
  prefs: []
  type: TYPE_NORMAL
- en: Parallelized apply with Swifter on Dask
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Swifter](https://oreil.ly/1SOcL) builds on Dask to provide three parallelized
    options with very simple calls—`apply`, `resample`, and `rolling`. Behind the
    scenes, it takes a subsample of your DataFrame and attempts to vectorize your
    function call. If that works, Swifter will apply it; if it works but it is slow,
    Swifter will run it on multiple cores using Dask.'
  prefs: []
  type: TYPE_NORMAL
- en: Since Swifter uses heuristics to determine how to run your code, it could run
    slower than if you didn’t use it at all—but the “cost” of trying it is one line
    of effort. It is well worth evaluating.
  prefs: []
  type: TYPE_NORMAL
- en: Swifter makes its own decisions about how many cores to use with Dask and how
    many rows to sample for its evaluation; as a result, in [Example 10-8](#calculate-ols-swiftly)
    we see the call to `df.swifter...apply()` looks just like a regular call to `df.apply`.
    In this case, we’ve disabled the progress bar; the progress bar works fine in
    a Jupyter Notebook using the excellent `tqdm` library.
  prefs: []
  type: TYPE_NORMAL
- en: Example 10-8\. Calculating line slopes with multiple cores using Dask
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Swifter with `ols_lstsq_raw` and no partitioning choices takes our previous
    single-threaded result of 5.3 seconds down to 1.6 seconds. For this particular
    function and dataset, this is not as fast as the slightly longer Dask solution
    we’ve just looked at, but it does offer a 3× speedup for only one line of code.
    For different functions and datasets, you’ll see different results; it is definitely
    worth an experiment to see whether you can achieve a very easy win.
  prefs: []
  type: TYPE_NORMAL
- en: Vaex for bigger-than-RAM DataFrames
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Vaex](https://vaex.io) is an intriguing new library that provides a Pandas
    DataFrame–like structure that has built-in support for larger-than-RAM computations.
    It neatly combines the features of Pandas and Dask into a single package.'
  prefs: []
  type: TYPE_NORMAL
- en: Vaex uses lazy computation to compute column results just-in-time; it’ll compute
    on only the subset of rows required by the user. If, for example, you ask for
    a sum on a billion rows between two columns and you ask for only a *sample* of
    those rows as the result, Vaex will touch only the data for that sample and will
    not compute the sum for all of the nonsampled rows. For interactive work and visualization-driven
    investigation, this can be very efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Pandas’s support of strings comes from CPython; it is GIL-bound, and the string
    objects are larger objects that are scattered in memory and do not support vectorized
    operations. Vaex uses its own custom string library, which enables significantly
    faster string-based operations with a similar Pandas-like interface.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re working on string-heavy DataFrames or larger-than-RAM datasets, Vaex
    is an obvious choice for evaluation. If you commonly work on subsets of a DataFrame,
    the implicit lazy evaluation may make your workflow simpler than adding Dask to
    Pandas DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: NSQ for Robust Production Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a production environment, you will need a solution that is more robust than
    the other solutions we’ve talked about so far. This is because during the everyday
    operation of your cluster, nodes may become unavailable, code may crash, networks
    may go down, or one of the other thousands of problems that can happen may happen.
    The problem is that all the previous systems have had one computer where commands
    are issued, and a limited and static number of computers that read the commands
    and execute them. We would instead like a system that can have multiple actors
    communicating via a message bus—this would allow us to have an arbitrary and constantly
    changing number of message creators and consumers.
  prefs: []
  type: TYPE_NORMAL
- en: 'One simple solution to these problems is [NSQ](https://github.com/nsqio/nsq),
    a highly performant distributed messaging platform. While it is written in GO,
    it is completely data format and language agnostic. As a result, there are libraries
    in many languages, and the basic interface into NSQ is a REST API that requires
    only the ability to make HTTP calls. Furthermore, we can send messages in any
    format we want: JSON, Pickle, `msgpack`, and so on. Most importantly, however,
    it provides fundamental guarantees regarding message delivery, and it does all
    of this using two simple design patterns: queues and pub/subs.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We picked NSQ to discuss because it is simple to use and generally performant.
    Most importantly for our purposes, it clearly highlights the considerations you
    must make when thinking about queuing and message passing in a cluster. However,
    other solutions such as ZeroMQ, Amazon’s SQS, Celery, or even Redis may be better
    suited for your application.
  prefs: []
  type: TYPE_NORMAL
- en: Queues
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A *queue* is a type of buffer for messages. Whenever you want to send a message
    to another part of your processing pipeline, you send it to the queue, and it’ll
    wait there until a worker is available. A queue is most useful in distributed
    processing when an imbalance exists between production and consumption. When this
    imbalance occurs, we can simply scale horizontally by adding more data consumers
    until the message production rate and the consumption rate are equal. In addition,
    if the computers responsible for consuming messages go down, the messages are
    not lost but are simply queued until a consumer is available, thus giving us message
    delivery guarantees.
  prefs: []
  type: TYPE_NORMAL
- en: For example, let’s say we would like to process new recommendations for a user
    every time that user rates a new item on our site. If we didn’t have a queue,
    the “rate” action would directly call the “recalculate-recommendations” action,
    regardless of how busy the servers dealing with recommendations were. If all of
    a sudden thousands of users decided to rate something, our recommendation servers
    could get so swamped with requests that they could start timing out, dropping
    messages, and generally becoming unresponsive!
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, with a queue, the recommendation servers ask for more tasks
    when they are ready. A new “rate” action would put a new task on the queue, and
    when a recommendation server becomes ready to do more work, it would grab the
    task from the queue and process it. In this setup, if more users than normal start
    rating items, our queue would fill up and act as a buffer for the recommendation
    servers—their workload would be unaffected, and they could still process messages
    until the queue was empty.
  prefs: []
  type: TYPE_NORMAL
- en: One potential problem with this is that if a queue becomes completely overwhelmed
    with work, it will be storing quite a lot of messages. NSQ solves this by having
    multiple storage backends—when there aren’t many messages, they are stored in
    memory, and as more messages start coming in, the messages get put onto disk.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Generally, when working with queued systems, it is a good idea to try to have
    the downstream systems (i.e., the recommendation systems in the preceding example)
    be at 60% capacity with a normal workload. This is a good compromise between allocating
    too many resources for a problem and giving your servers enough extra power for
    when the amount of work increases beyond normal levels.
  prefs: []
  type: TYPE_NORMAL
- en: Pub/sub
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A *pub/sub* (short for *publisher/subscriber*), on the other hand, describes
    who gets what messages. A data publisher can push data out of a particular topic,
    and data subscribers can subscribe to different feeds of data. Whenever the publisher
    puts out a piece of information, it gets sent to all the subscribers—each gets
    an identical copy of the original information. You can think of this like a newspaper:
    many people can subscribe to a particular newspaper, and whenever a new edition
    of the newspaper comes out, every subscriber gets an identical copy of it. In
    addition, the producer of the newspaper doesn’t need to know all the people its
    papers are being sent to. As a result, publishers and subscribers are decoupled
    from each other, which allows our system to be more robust as our network changes
    while still in production.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, NSQ adds the notion of a *data consumer*; that is, multiple processes
    can be connected to the same data subscription. Whenever a new piece of data comes
    out, every subscriber gets a copy of the data; however, only one consumer of each
    subscription sees that data. In the newspaper analogy, you can think of this as
    having multiple people in the same household who read the newspaper. The publisher
    will deliver one paper to the house, since that house has only one subscription,
    and whoever in the house gets to it first gets to read that data. Each subscriber’s
    consumers do the same processing to a message when they see it; however, they
    can potentially be on multiple computers and thus add more processing power to
    the entire pool.
  prefs: []
  type: TYPE_NORMAL
- en: We can see a depiction of this pub/sub/consumer paradigm in [Figure 10-1](#clustering_nsq_overview).
    If a new message gets published on the “clicks” topic, all the subscribers (or,
    in NSQ parlance, *channels*—i.e., “metrics,” “spam_analysis,” and “archive”) will
    get a copy. Each subscriber is composed of one or more consumers, which represent
    actual processes that react to the messages. In the case of the “metrics” subscriber,
    only one consumer will see the new message. The next message will go to another
    consumer, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '![hpp2 1001](Images/hpp2_1001.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-1\. NSQ’s pub/sub-like topology
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The benefit of spreading the messages among a potentially large pool of consumers
    is essentially automatic load balancing. If a message takes quite a long time
    to process, that consumer will not signal to NSQ that it is ready for more messages
    until it’s done, and thus the other consumers will get the majority of future
    messages (until that original consumer is ready to process again). In addition,
    it allows existing consumers to disconnect (whether by choice or because of failure)
    and new consumers to connect to the cluster while still maintaining processing
    power within a particular subscription group. For example, if we find that “metrics”
    takes quite a while to process and often is not keeping up with demand, we can
    simply add more processes to the consumer pool for that subscription group, giving
    us more processing power. On the other hand, if we see that most of our processes
    are idle (i.e., not getting any messages), we can easily remove consumers from
    this subscription pool.
  prefs: []
  type: TYPE_NORMAL
- en: It is also important to note that anything can publish data. A consumer doesn’t
    simply need to be a consumer—it can consume data from one topic and then publish
    it to another topic. In fact, this chain is an important workflow when it comes
    to this paradigm for distributed computing. Consumers will read from a topic of
    data, transform the data in some way, and then publish the data onto a new topic
    that other consumers can further transform. In this way, different topics represent
    different data, subscription groups represent different transformations on the
    data, and consumers are the actual workers who transform individual messages.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, this system provides an incredible redundancy. There can be many
    `nsqd` processes that each consumer connects to, and there can be many consumers
    connected to a particular subscription. This makes it so that no single point
    of failure exists, and your system will be robust even if several machines disappear.
    We can see in [Figure 10-2](#clustering_nsq_spof) that even if one of the computers
    in the diagram goes down, the system is still able to deliver and process messages.
    In addition, since NSQ saves pending messages to disk when shutting down, unless
    the hardware loss is catastrophic, your data will most likely still be intact
    and be delivered. Last, if a consumer is shut down before responding to a particular
    message, NSQ will resend that message to another consumer. This means that even
    as consumers get shut down, we know that all the messages in a topic will be responded
    to at least once.^([1](ch10.xhtml#idm46122403968536))
  prefs: []
  type: TYPE_NORMAL
- en: '![hpp2 1002](Images/hpp2_1002.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-2\. NSQ connection topology
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Distributed Prime Calculation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Code that uses NSQ is generally asynchronous (see [Chapter 8](ch08.xhtml#chapter-concurrency)
    for a full explanation), although it doesn’t necessarily have to be.^([2](ch10.xhtml#idm46122403960472))
    In the following example, we will create a pool of workers that read from a topic
    called *numbers* where the messages are simply JSON blobs with numbers in them.
    The consumers will read this topic, find out if the numbers are primes, and then
    write to another topic, depending on whether the numbers were prime. This will
    give us two new topics, *prime* and *non_prime*, that other consumers can connect
    to in order to do more calculations.^([3](ch10.xhtml#idm46122403957848))
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '`pynsq` (last release Nov 11, 2018) depends on a very outdated release of `tornado`
    (4.5.3, from Jan 6, 2018). This is a good example use case for Docker (discussed
    in [“Docker”](#docker)).'
  prefs: []
  type: TYPE_NORMAL
- en: As we’ve said before, there are many benefits to doing CPU-bound work like this.
    First, we have all the guarantees of robustness, which may or may not be useful
    for this project. More importantly, however, we get automatic load balancing.
    That means that if one consumer gets a number that takes a particularly long time
    to process, the other consumers will pick up the slack.
  prefs: []
  type: TYPE_NORMAL
- en: We create a consumer by creating an `nsq.Reader` object with the topic and subscription
    group specified (as can be seen at the end of [Example 10-9](#clustering_nsq_example)).
    We also must specify the location of the running `nsqd` instance (or the `nsqlookupd`
    instance, which we will not get into in this section). In addition, we specify
    a *handler*, which is simply a function that gets called for each message from
    the topic. To create a producer, we create an `nsq.Writer` object and specify
    the location of one or more `nsqd` instances to write to. This gives us the ability
    to write to `nsq`, simply by specifying the topic name and the message.^([4](ch10.xhtml#idm46122403943144))
  prefs: []
  type: TYPE_NORMAL
- en: Example 10-9\. Distributed prime calculation with NSQ
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_clusters_and_job_queues_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: We must signal to NSQ when we are done with a message. This will make sure the
    message is not redelivered to another reader in case of failure.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can handle messages asynchronously by enabling `message.enable_async()` in
    the message handler after the message is received. However, note that NSQ uses
    the older callback mechanisms with `tornado`’s IOLoop (discussed in [“tornado”](ch08.xhtml#tornado)).
  prefs: []
  type: TYPE_NORMAL
- en: To set up the NSQ ecosystem, start an instance of `nsqd` on our local machine:^([5](ch10.xhtml#idm46122402448536))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Now we can start as many instances of our Python code ([Example 10-9](#clustering_nsq_example))
    as we want. In fact, we can have these instances running on other computers as
    long as the reference to the `nsqd_tcp_address` in the instantiation of the `nsq.Reader`
    is still valid. These consumers will connect to `nsqd` and wait for messages to
    be published on the *numbers* topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data can be published to the *numbers* topic in many ways. We will use command-line
    tools to do this, since knowing how to poke and prod a system goes a long way
    in understanding how to properly deal with it. We can simply use the HTTP interface
    to publish messages to the topic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'As this command starts running, we are publishing messages with different numbers
    in them to the *numbers* topic. At the same time, all of our producers will start
    outputting status messages indicating that they have seen and processed messages.
    In addition, these numbers are being published to either the *prime* or the *non_prime*
    topic. This allows us to have other data consumers that connect to either of these
    topics to get a filtered subset of our original data. For example, an application
    that requires only the prime numbers can simply connect to the *prime* topic and
    constantly have new primes for its calculation. We can see the status of our calculation
    by using the `stats` HTTP endpoint for `nsqd`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We can see here that the *numbers* topic has one subscription group, *worker_group_a*,
    with one consumer. In addition, the subscription group has a large depth of 1,926
    messages, which means that we are putting messages into NSQ faster than we can
    process them. This would be an indication to add more consumers so that we have
    more processing power to get through more messages. Furthermore, we can see that
    this particular consumer has been connected for 15 seconds, has processed 1,083
    messages, and currently has 1 message in flight. This status endpoint gives quite
    a good deal of information for debugging your NSQ setup! Last, we see the *prime*
    and *non_prime* topics, which have no subscribers or consumers. This means that
    the messages will be stored until a subscriber comes requesting the data.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In production systems, you can use the even more powerful tool `nsqadmin`, which
    provides a web interface with very detailed overviews of all topics/subscribers
    and consumers. In addition, it allows you to easily pause and delete subscribers
    and topics.
  prefs: []
  type: TYPE_NORMAL
- en: 'To actually see the messages, we would create a new consumer for the *prime*
    (or *non_prime*) topic that simply archives the results to a file or database.
    Alternatively, we can use the `nsq_tail` tool to take a peek at the data and see
    what it contains:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Other Clustering Tools to Look At
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Job processing systems using queues have existed since the start of the computer
    science industry, back when computers were very slow and lots of jobs needed to
    be processed. As a result, there are *many* libraries for queues, and many of
    these can be used in a cluster configuration. We strongly suggest that you pick
    a mature library with an active community behind it and supporting the same feature
    set that you’ll need without too many additional features.
  prefs: []
  type: TYPE_NORMAL
- en: 'The more features a library has, the more ways you’ll find to misconfigure
    it and waste time on debugging. Simplicity is *generally* the right aim when dealing
    with clustered solutions. Here are a few of the more commonly used clustering
    solutions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[ZeroMQ](https://zeromq.org) is a low-level and performant messaging library
    that enables you to send messages between nodes. It natively supports pub/sub
    paradigms and can also communicate over multiple types of transports (TCP, UDP,
    WebSocket, etc). It is quite low level and doesn’t provide many useful abstractions,
    which can make its use a bit difficult. That being said, it’s in use in Jupyter,
    Auth0, Spotify, and many more places!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Celery](http://www.celeryproject.org) (BSD license) is a widely used asynchronous
    task queue using a distributed messaging architecture, written in Python. It supports
    Python, PyPy, and Jython. It typically it uses RabbitMQ as the message broker,
    but it also supports Redis, MongoDB, and other storage systems. It is often used
    in web development projects. Andrew Godwin discusses Celery in [“Task Queues at
    Lanyrd.com (2014)”](ch12.xhtml#lessons-from-field-andrew).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Airflow](https://airflow.apache.org) and [Luigi](https://github.com/spotify/luigi)
    use directed acyclic graphs to chain dependent jobs into sequences that run reliably,
    with monitoring and reporting services. They’re widely used in industry for data
    science tasks, and we recommend reviewing these before you embark on a custom
    solution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Amazon’s Simple Queue Service (SQS)](http://aws.amazon.com/sqs) is a job processing
    system integrated into AWS. Job consumers and producers can live inside AWS or
    can be external, so SQS is easy to start with and supports easy migration into
    the cloud. Library support exists for many languages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Docker](https://docker.com) is a tool of general importance in the Python
    ecosystem. However, the problems it solves are particularly important when dealing
    with a large team or a cluster. In particular, Docker helps to create reproducible
    environments in which to run your code, share/control runtime environments, easily
    share runnable code between team members, and deploy code to a cluster of nodes
    based on resource needs.'
  prefs: []
  type: TYPE_NORMAL
- en: Docker’s Performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One common misconception about Docker is that it substantially slows down the
    runtime performance of the applications it is running. While this can be true
    in some cases, it generally is not. Furthermore, most of the performance degradations
    can almost always be removed with some easy configuration changes.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of CPU and memory access, Docker (and all other container-based solutions)
    will *not* lead to any performance degradations. This is because Docker simply
    creates a special namespace within the host operating system where the code can
    run normally, albeit with separate constraints from other running programs. Essentially,
    Docker code accesses the CPU and memory in the same way that every other program
    on the computer does; however, it can have a separate set of configuration values
    to fine-tune resource limits.^([6](ch10.xhtml#idm46122402901784))
  prefs: []
  type: TYPE_NORMAL
- en: This is because Docker is an instance of OS-level virtualization, as opposed
    to hardware virtualization such as VMware or VirtualBox. With hardware virtualization,
    software runs on “fake” hardware that introduces overhead accessing all resources.
    On the other hand, OS virtualization uses the native hardware but runs on a “fake”
    operating system. Thanks to the `cgroups` Linux feature, this “fake” operating
    system can be tightly coupled to the running operating system, which gives the
    possibility of running with almost no overhead.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '`cgroups` is specifically a feature in the Linux kernel. As a result, performance
    implications discussed here are restricted to Linux systems. In fact, to run Docker
    on macOS or Windows, we first must run the Linux kernel in a hardware-virtualized
    environment. Docker Machine, the application helping streamline this process,
    uses VirtualBox to accomplish this. As a result, you will see performance overhead
    from the hardware-virtualized portion of the process. This overhead will be greatly
    reduced when running on a Linux system, where hardware virtualization is not needed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, let’s create a simple Docker container to run the 2D diffusion
    code from [Example 6-17](ch06_split_001.xhtml#matrix_numpy_memory2). As a baseline,
    we can run the code on the host system’s Python to get a benchmark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: To create our Docker container, we have to make a directory that contains the
    Python file *diffusion_numpy_memory2.py*, a `pip` requirements file for dependencies,
    and a *Dockerfile*, as shown in [Example 10-10](#simple-docker).
  prefs: []
  type: TYPE_NORMAL
- en: Example 10-10\. Simple Docker container
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The *Dockerfile* starts by stating what container we’d like to use as our base.
    These base containers can be a wide selection of Linux-based operating systems
    or a higher-level service. The Python Foundation provides [official containers
    for all major Python versions](https://hub.docker.com/_/python), which makes selecting
    the Python version you’d like to use incredibly simple. Next, we define the location
    of our working directory (the selection of `/usr/src/app` is arbitrary), copy
    our requirements file into it, and begin setting up our environment as we normally
    would on our local machine, using `RUN` commands.
  prefs: []
  type: TYPE_NORMAL
- en: One major difference between setting up your development environment normally
    and on Docker are the `COPY` commands. They copy files from the local directory
    into the container. For example, the *requirements.txt* file is copied into the
    container so that it is there for the `pip install` command. Finally, at the end
    of the *Dockerfile*, we copy all the files from the current directory into the
    container and tell Docker to run `python ./diffusion_numpy_memory2.py` when the
    container starts.
  prefs: []
  type: TYPE_NORMAL
- en: Caution
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the *Dockerfile* in [Example 10-10](#simple-docker), beginners often wonder
    why we first copy only the requirements file and then later copy the entire directory
    into the container. When building a container, Docker tries hard to cache each
    step in the build process. To determine whether the cache is still valid, the
    contents of the files being copied back and forth are checked. By first copying
    only the requirements file and *then* moving the rest of the directory, we will
    have to run `pip install` only if the requirements file changes. If only the Python
    source has changed, a new build will use cached build steps and skip straight
    to the second `COPY` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we are ready to build and run the container, which can be named and tagged.
    Container names generally take the format `*<username>*/*<project-name>*`,^([7](ch10.xhtml#idm46122403724904))
    while the optional tag generally is either descriptive of the current version
    of the code or simply the tag `latest` (this is the default and will be applied
    automatically if no tag is specified). To help with versioning, it is general
    convention to always tag the most recent build with `latest` (which will get overwritten
    when a new build is made) as well as a descriptive tag so that we can easily find
    this version again in the future:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that at its core, Docker is not slower than running on the host
    machine in any meaningful way when the task relies mainly on CPU/memory. However,
    as with anything, there is no free lunch, and at times Docker performance suffers.
    While a full discussion of optimizing Docker containers is outside the scope of
    this book, we offer the following list of considerations for when you are creating
    Docker containers for high performance code:'
  prefs: []
  type: TYPE_NORMAL
- en: Be wary of copying too much data into a Docker container or even having too
    much data in the same directory as a Docker build. If the `build context`, as
    advertised by the first line of the `docker build` command, is too large, performance
    can suffer (remedy this with a *.dockerignore* file).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker uses various filesystem tricks to layer filesystems on top of each other.
    This helps the caching of builds but can be slower than dealing with the host
    filesystem. Use host-level mounts when you need to access data quickly, and consider
    using `volumes` set as read-only to choose a volume driver that is fitting for
    your infrastructure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker creates a virtual network for all your containers to live behind. This
    can be great for keeping most of your services hidden behind a gateway, but it
    also adds slight network overhead. For most use cases, this overhead is negligible,
    but it can be mitigated by changing the network driver.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPUs and other host-level devices can be accessed using special runtime drivers
    for Docker. For example, `nvidia-docker` allows Docker environments to easily
    use connected NVIDIA GPUs. In general, devices can be made available with the
    `--device` runtime flag.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As always, it is important to profile your Docker containers to know what the
    issues are and whether there are some easy wins in terms of efficiency. The `docker
    stats` command provides a good high-level view to help understand the current
    runtime performance of your containers.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages of Docker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far it has seemed that Docker is simply adding a whole new host of issues
    to contend with in terms of performance. However, the gains to reproducibility
    and reliability of runtime environments far surpass any extra complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Locally, having access to all our previously run Docker containers allows us
    to quickly rerun and retest previous versions of our code without having to worry
    about changes to the runtime environment, such as dependencies and system packages
    ([Example 10-11](#docker_tags_runtime_env) shows a list of containers we can run
    with a simple `docker_run` command). This makes it incredibly easy to constantly
    be testing for performance regressions that otherwise would be difficult to reproduce.
  prefs: []
  type: TYPE_NORMAL
- en: Example 10-11\. Docker tags to keep track of previous runtime environments
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Many more benefits come with the use of a [container registry](https://oreil.ly/BaJhI),
    which allows the storage and sharing of Docker images with the simple `docker`
    `pull` and `docker` `push` commands, in a similar way to `git`. This lets us put
    all our containers in a publicly available location, allowing team members to
    pull in changes or new versions and letting them immediately run the code.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This book is a great example of the benefits of sharing a Docker container for
    standardizing a runtime environment. To convert this book from `asciidoc`, the
    markup language it was written in, into PDF, a Docker container was shared between
    us so we could reliably and reproducibly build book artifacts. This standardization
    saved us countless hours that were spent in the first edition as one of us would
    have a build issue that the other couldn’t reproduce or help debug.
  prefs: []
  type: TYPE_NORMAL
- en: Running `docker pull highperformance/diffusion2d:latest` is much easier than
    having to clone a repository and doing all the associated setup that may be necessary
    to run a project. This is particularly true for research code, which may have
    some very fragile system dependencies. Having all of this inside an easily pullable
    Docker container means all of these setup steps can be skipped and the code can
    be run easily. As a result, code can be shared more easily, and a coding team
    can work more effectively together.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, in conjunction with [`kubernetes`](https://kubernetes.io) and other
    similar technologies, Dockerizing your code helps with actually running it with
    the resources it needs. Kubernetes allows you to create a cluster of nodes, each
    labeled with resources it may have, and to orchestrate running containers on the
    nodes. It will take care of making sure the correct number of instances are being
    run, and thanks to the Docker virtualization, the code will be run in the same
    environment that you saved it to. One of the biggest pains of working with a cluster
    is making sure that the cluster nodes have the correct runtime environment as
    your workstation, and using Docker virtualization completely resolves this.^([8](ch10.xhtml#idm46122403689192))
  prefs: []
  type: TYPE_NORMAL
- en: Wrap-Up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far in the book, we’ve looked at profiling to understand slow parts of your
    code, compiling and using `numpy` to make your code run faster, and various approaches
    to multiple processes and computers. In addition, we surveyed container virtualization
    to manage code environments and help in cluster deployment. In the penultimate
    chapter, we’ll look at ways of using less RAM through different data structures
    and probabilistic approaches. These lessons could help you keep all your data
    on one machine, avoiding the need to run a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch10.xhtml#idm46122403968536-marker)) This can be quite advantageous when
    we’re working in AWS, where we can have our `nsqd` processes running on a reserved
    instance and our consumers working on a cluster of spot instances.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch10.xhtml#idm46122403960472-marker)) This asynchronicity comes from NSQ’s
    protocol for sending messages to consumers being push-based. This makes it so
    our code can have an asynchronous read from our connection to NSQ happen in the
    background and wake up when a message is found.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch10.xhtml#idm46122403957848-marker)) This sort of chaining of data analysis
    is called *pipelining* and can be an effective way to perform multiple types of
    analysis on the same data efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch10.xhtml#idm46122403943144-marker)) You can also easily publish a message
    manually with an HTTP call; however, this `nsq.Writer` object simplifies much
    of the error handling.
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch10.xhtml#idm46122402448536-marker)) For this example, we installed NSQ
    straight onto our system by unpacking the provided binaries into our `PATH` environment
    variable. Alternatively, you can use Docker, discussed in [“Docker”](#docker),
    to easily run the latest versions.
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch10.xhtml#idm46122402901784-marker)) This fine-tuning can, for example,
    be used to adjust the amount of memory a process has access to, or which CPUs
    or even how much of the CPU it can use.
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch10.xhtml#idm46122403724904-marker)) The `*username*` portion of the
    container name is useful when also pushing built containers to a repository.
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch10.xhtml#idm46122403689192-marker)) A great tutorial to get started
    with Docker and Kubernetes can be found at [*https://oreil.ly/l9jXD*](https://oreil.ly/l9jXD).
  prefs: []
  type: TYPE_NORMAL
