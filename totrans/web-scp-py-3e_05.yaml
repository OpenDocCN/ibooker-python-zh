- en: Chapter 4\. Writing Your First Web Scraper
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you start web scraping, you start to appreciate all the little things that
    browsers do for you. The web, without its layers of HTML formatting, CSS styling,
    JavaScript execution, and image rendering, can look a little intimidating at first. In
    this chapter, we’ll begin to look at how to format and interpret this bare data
    without the help of a web browser.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter starts with the basics of sending a `GET` request (a request to
    fetch, or “get,” the content of a web page) to a web server for a specific page,
    reading the HTML output from that page, and doing some simple data extraction
    in order to isolate the content you are looking for.
  prefs: []
  type: TYPE_NORMAL
- en: Installing and Using Jupyter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code for this course can be found at [*https://github.com/REMitchell/python-scraping*](https://github.com/REMitchell/python-scraping).
    In most cases, code samples are in the form of Jupyter Notebook files, with an
    *.ipynb* extension.
  prefs: []
  type: TYPE_NORMAL
- en: If you haven’t used them already, Jupyter Notebooks are an excellent way to
    organize and work with many small but related pieces of Python code, as shown
    in [Figure 4-1](#fig0401).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/wsp3_0401.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-1\. A Jupyter Notebook running in the browser
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Each piece of code is contained in a box called a *cell*. The code within each
    cell can be run by typing Shift + Enter, or by clicking the Run button at the
    top of the page.
  prefs: []
  type: TYPE_NORMAL
- en: Project Jupyter began as a spin-off project from the IPython (Interactive Python)
    project in 2014\. These notebooks were designed to run Python code in the browser
    in an accessible and interactive way that would lend itself to teaching and presenting.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install Jupyter Notebooks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'After installation, you should have access to the `jupyter` command, which
    will allow you to start the web server. Navigate to the directory containing the
    downloaded exercise files for this book, and run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This will start the web server on port 8888\. If you have a web browser running,
    a new tab should open automatically. If it doesn’t, copy the URL shown in the
    terminal, with the provided token, to your web browser.
  prefs: []
  type: TYPE_NORMAL
- en: Connecting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the first section of this book, we took a deep dive into how the internet
    sends packets of data across wires from a browser to a web server and back again.
    When you open a browser, type in `**google.com**`, and hit Enter, that’s exactly
    what’s happening—data, in the form of an HTTP request, is being transferred from
    your computer, and Google’s web server is responding with an HTML file that represents
    the data at the root of *google.com*.
  prefs: []
  type: TYPE_NORMAL
- en: But where, in this exchange of packets and frames, does the web browser actually
    come into play? Absolutely nowhere. In fact, ARPANET (the first public packet-switched
    network) predated the first web browser, Nexus, by at least 20 years.
  prefs: []
  type: TYPE_NORMAL
- en: 'Yes, the web browser is a useful application for creating these packets of
    information, telling your operating system to send them off and interpreting the
    data you get back as pretty pictures, sounds, videos, and text. However, a web
    browser is just code, and code can be taken apart, broken into its basic components,
    rewritten, reused, and made to do anything you want. A web browser can tell the
    processor to send data to the application that handles your wireless (or wired)
    interface, but you can do the same thing in Python with just three lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'To run this, you can use the [IPython notebook](https://github.com/REMitchell/python-scraping/blob/master/Chapter01_BeginningToScrape.ipynb)
    for [Chapter 1](ch01.html#c-1) in the GitHub repository, or you can save it locally
    as *scrapetest.py* and run it in your terminal by using this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that if you also have Python 2.x installed on your machine and are running
    both versions of Python side by side, you may need to explicitly call Python 3.x
    by running the command this way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This command outputs the complete HTML code for *page1* located at the URL *http://pythonscraping.com/pages/page1.html*.
    More accurately, this outputs the HTML file *page1.html*, found in the directory
    *<web root>/pages*, on the server located at the domain name [*http://pythonscraping.com*](http://pythonscraping.com).
  prefs: []
  type: TYPE_NORMAL
- en: Why is it important to start thinking of these addresses as “files” rather than
    “pages”? Most modern web pages have many resource files associated with them.
    These could be image files, JavaScript files, CSS files, or any other content
    that the page you are requesting is linked to. When a web browser hits a tag such
    as `<img src="cute​Kit⁠ten.jpg">`, the browser knows that it needs to make another
    request to the server to get the data at the location *cuteKitten.jpg* in order
    to fully render the page for the user.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, your Python script doesn’t have the logic to go back and request
    multiple files (yet); it can read only the single HTML file that you’ve directly
    requested.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'means what it looks like it means: it looks at the Python module request (found
    within the *urllib* library) and imports only the function `urlopen`.'
  prefs: []
  type: TYPE_NORMAL
- en: '*urllib* is a standard Python library (meaning you don’t have to install anything
    extra to run this example) and contains functions for requesting data across the
    web, handling cookies, and even changing metadata such as headers and your user
    agent. We will be using urllib extensively throughout the book, so I recommend
    you read the [Python documentation for the library](https://docs.python.org/3/library/urllib.html).'
  prefs: []
  type: TYPE_NORMAL
- en: '`urlopen` is used to open a remote object across a network and read it. Because
    it is a fairly generic function (it can read HTML files, image files, or any other
    file stream with ease), we will be using it quite frequently throughout the book.'
  prefs: []
  type: TYPE_NORMAL
- en: An Introduction to BeautifulSoup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Beautiful Soup, so rich and green,
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Waiting in a hot tureen!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Who for such dainties would not stoop?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Soup of the evening, beautiful Soup!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The *BeautifulSoup* library was named after a Lewis Carroll poem of the same
    name in *Alice’s Adventures in Wonderland*. In the story, this poem is sung by
    a character called the Mock Turtle (itself a pun on the popular Victorian dish
    Mock Turtle Soup made not of turtle but of cow).
  prefs: []
  type: TYPE_NORMAL
- en: Like its Wonderland namesake, BeautifulSoup tries to make sense of the nonsensical;
    it helps format and organize the messy web by fixing bad HTML and presenting us
    with easily traversable Python objects representing XML structures.
  prefs: []
  type: TYPE_NORMAL
- en: Installing BeautifulSoup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Because the BeautifulSoup library is not a default Python library, it must be
    installed. If you’re already experienced at installing Python libraries, please
    use your favorite installer and skip ahead to the next section, [“Running BeautifulSoup”](#runningBsoup).
  prefs: []
  type: TYPE_NORMAL
- en: For those who have not installed Python libraries (or need a refresher), this
    general method will be used for installing multiple libraries throughout the book,
    so you may want to reference this section in the future.
  prefs: []
  type: TYPE_NORMAL
- en: We will be using the BeautifulSoup 4 library (also known as BS4) throughout
    this book. The complete documentation, as well as installation instructions, for
    BeautifulSoup 4 can be found at [Crummy.com](https://www.crummy.com/software/BeautifulSoup/bs4/doc/).
  prefs: []
  type: TYPE_NORMAL
- en: If you’ve spent much time writing Python, you’ve probably used the package installer
    for Python ([pip](https://pypi.org/project/pip/)). If you haven’t, I highly recommend
    that you install pip in order to install BeautifulSoup and other Python packages
    used throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on the Python installer you used, pip may already be installed on
    your computer. To check, try:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This command should result in the pip help text being printed to your terminal.
    If the command isn’t recognized, you may need to install pip. Pip can be installed
    in a variety of ways, such as with `apt-get` on Linux or `brew` on macOS. Regardless
    of your operating system, you can also download the pip bootstrap file at [*https://bootstrap.pypa.io/get-pip.py*](https://bootstrap.pypa.io/get-pip.py),
    save this file as *get-pip.py*, and run it with Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, note that if you have both Python 2.x and 3.x installed on your machine,
    you might need to call `python3` explicitly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, use pip to install BeautifulSoup:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'If you have two versions of Python, along with two versions of pip, you may
    need to call `pip3` to install the Python 3.x versions of packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'And that’s it! BeautifulSoup will now be recognized as a Python library on
    your machine. You can test this by opening a Python terminal and importing it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The import should complete without errors.
  prefs: []
  type: TYPE_NORMAL
- en: Running BeautifulSoup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The most commonly used object in the BeautifulSoup library is, appropriately,
    the `BeautifulSoup` object. Let’s take a look at it in action, modifying the example
    found in the beginning of this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Note that this returns only the first instance of the `h1` tag found on the
    page. By convention, only one `h1` tag should be used on a single page, but conventions
    are often broken on the web, so you should be aware that this will retrieve only
    the first instance of the tag, and not necessarily the one that you’re looking
    for.
  prefs: []
  type: TYPE_NORMAL
- en: 'As in previous web scraping examples, you are importing the `urlopen` function
    and calling `html.read()` to get the HTML content of the page. In addition to
    the text string, BeautifulSoup can use the file object directly returned by `urlopen`,
    without needing to call `.read()` first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This HTML content is then transformed into a `BeautifulSoup` object with the
    following structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '**html** → *<html><head>...</head><body>...</body></html>*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**head** → *<head><title>A Useful Page<title></head>*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**title** → *<title>A Useful Page</t**itle>*'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**body** → *<body><h1>An Int...</h1><div>Lorem ip...</div></body>*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**h1** → *<h1>An Interesting Title</h1>*'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**div** → *<div>Lorem Ipsum dolor...</div>*'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note that the `h1` tag that you extract from the page is nested two layers
    deep into your `BeautifulSoup` object structure (`html` → `body` → `h1`). However,
    when you actually fetch it from the object, you call the `h1` tag directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'In fact, any of the following function calls would produce the same output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'When you create a `BeautifulSoup` object, two arguments are passed in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The first is the HTML string that the object is based on, and the second specifies
    the parser that you want BeautifulSoup to use to create that object. In the majority
    of cases, it makes no difference which parser you choose.
  prefs: []
  type: TYPE_NORMAL
- en: '`html.parser` is a parser that is included with Python 3 and requires no extra
    installations to use. Except where required, we will use this parser throughout
    the book.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another popular parser is [`lxml`](http://lxml.de/parsing.html). This can be
    installed through pip:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '`lxml` can be used with BeautifulSoup by changing the parser string provided:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '`lxml` has some advantages over `html.parser` in that it is generally better
    at parsing “messy” or malformed HTML code. It is forgiving and fixes problems
    like unclosed tags, tags that are improperly nested, and missing head or body
    tags.'
  prefs: []
  type: TYPE_NORMAL
- en: '`lxml` is also somewhat faster than `html.parser`, although speed is not necessarily
    an advantage in web scraping, given that the speed of the network itself will
    almost always be your largest bottleneck.'
  prefs: []
  type: TYPE_NORMAL
- en: Avoid Over-Optimizing Web Scraping Code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Elegant algorithms are lovely to behold, but when it comes to web scraping,
    they may not have a practical impact. A few microseconds of processing time will
    likely be dwarfed by the—sometimes *actual*—seconds of network latency that a
    network request takes.
  prefs: []
  type: TYPE_NORMAL
- en: Good web scraping code generally focuses on robust and easily readable implementations,
    rather than clever processing optimizations.
  prefs: []
  type: TYPE_NORMAL
- en: One of the disadvantages of `lxml` is that it needs to be installed separately
    and depends on third-party C libraries to function. This can cause problems for
    portability and ease of use, compared to `html.parser`.
  prefs: []
  type: TYPE_NORMAL
- en: Another popular HTML parser is `html5lib`. Like `lxml`, `html5lib` is an extremely
    forgiving parser that takes even more initiative with correcting broken HTML.
    It also depends on an external dependency and is slower than both `lxml` and `html.parser`.
    Despite this, it may be a good choice if you are working with messy or handwritten
    HTML sites.
  prefs: []
  type: TYPE_NORMAL
- en: 'It can be used by installing and passing the string `html5lib` to the BeautifulSoup
    object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: I hope this small taste of BeautifulSoup has given you an idea of the power
    and simplicity of this library. Virtually any information can be extracted from
    any HTML (or XML) file, as long as it has an identifying tag surrounding it or
    near it. [Chapter 5](ch05.html#c-5) delves more deeply into more-complex BeautifulSoup
    function calls and presents regular expressions and how they can be used with
    BeautifulSoup in order to extract information from websites.
  prefs: []
  type: TYPE_NORMAL
- en: Connecting Reliably and Handling Exceptions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The web is messy. Data is poorly formatted, websites go down, and closing tags
    go missing. One of the most frustrating experiences in web scraping is to go to
    sleep with a scraper running, dreaming of all the data you’ll have in your database
    the next day—only to find that the scraper hit an error on some unexpected data
    format and stopped execution shortly after you stopped looking at the screen.
  prefs: []
  type: TYPE_NORMAL
- en: In situations like these, you might be tempted to curse the name of the developer
    who created the website (and the oddly formatted data), but the person you should
    really be kicking is yourself for not anticipating the exception in the first
    place!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the first line of our scraper, after the import statements, and
    figure out how to handle any exceptions this might throw:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Two main things can go wrong in this line:'
  prefs: []
  type: TYPE_NORMAL
- en: The page is not found on the server (or there was an error in retrieving it).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The server is not found at all.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the first situation, an HTTP error will be returned. This HTTP error may
    be “404 Page Not Found,” “500 Internal Server Error,” and so forth. In all of
    these cases, the `urlopen` function will throw the generic exception `HTTPError`.
    You can handle this exception in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: If an HTTP error code is returned, the program now prints the error and does
    not execute the rest of the program under the `else` statement.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the server is not found at all (if, for example, *http://www.pythonscraping.com*
    is down, or the URL is mistyped), `urlopen` will throw an `URLError`. This indicates
    that no server could be reached at all, and, because the remote server is responsible
    for returning HTTP status codes, an `HTTPError` cannot be thrown, and the more
    serious `URLError` must be caught. You can add a check to see whether this is
    the case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Of course, if the page is retrieved successfully from the server, there is still
    the issue of the content on the page not being quite what you expected. Every
    time you access a tag in a `BeautifulSoup` object, it’s smart to add a check to
    make sure the tag actually exists. If you attempt to access a tag that does not
    exist, BeautifulSoup will return a `None` object. The problem is, attempting to
    access a tag on a `None` object itself will result in an `AttributeError` being
    thrown.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following line (where `nonExistentTag` is a made-up tag, not the name of
    a real BeautifulSoup function):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'returns a `None` object. This object is perfectly reasonable to handle and
    check for. The trouble comes if you don’t check for it but instead go on and try
    to call another function on the `None` object, as illustrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns an exception:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'So how can you guard against these two situations? The easiest way is to explicitly
    check for both situations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This checking and handling of every error does seem laborious at first, but
    it’s easy to add a little reorganization to this code to make it less difficult
    to write (and, more important, much less difficult to read). This code, for example,
    is our same scraper written in a slightly different way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: In this example, you’re creating a function `getTitle`, which returns either
    the title of the page, or a `None` object if there was a problem retrieving it.
    Inside `getTitle`, you check for an `HTTPError`, as in the previous example, and
    encapsulate two of the BeautifulSoup lines inside one `try` statement. An `AttributeError`
    might be thrown from either of these lines (if the server did not exist, `html`
    would be a `None` object, and `html.read()` would throw an `AttributeError`).
    You could, in fact, encompass as many lines as you want inside one `try` statement
    or call another function entirely, which can throw an `AttributeError` at any
    point.
  prefs: []
  type: TYPE_NORMAL
- en: When writing scrapers, it’s important to think about the overall pattern of
    your code in order to handle exceptions and make it readable at the same time.
    You’ll also likely want to heavily reuse code. Having generic functions such as
    `getSiteHTML` and `getTitle` (complete with thorough exception handling) makes
    it easy to quickly—and reliably—scrape the web.
  prefs: []
  type: TYPE_NORMAL
