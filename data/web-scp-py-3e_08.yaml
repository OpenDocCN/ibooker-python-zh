- en: Chapter 7\. Web Crawling Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第七章。网络爬虫模型
- en: Writing clean, scalable code is difficult enough when you have control over
    your data and your inputs. Writing code for web crawlers, which may need to scrape
    and store a variety of data from diverse sets of websites that the programmer
    has no control over, often presents unique organizational challenges.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当你控制数据和输入时，编写干净、可扩展的代码已经足够困难了。编写网页爬虫的代码，可能需要从程序员无法控制的各种网站中抓取和存储各种数据，通常会带来独特的组织挑战。
- en: You may be asked to collect news articles or blog posts from a variety of websites,
    each with different templates and layouts. One website’s `h1` tag contains the
    title of the article, another’s `h1` tag contains the title of the website itself,
    and the article title is in `<span id="title">`.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会被要求从各种网站上收集新闻文章或博客文章，每个网站都有不同的模板和布局。一个网站的 `h1` 标签包含文章的标题，另一个网站的 `h1` 标签包含网站本身的标题，而文章标题在
    `<span id="title">` 中。
- en: You may need flexible control over which websites are scraped and how they’re
    scraped, and a way to quickly add new websites or modify existing ones, as fast
    as possible without writing multiple lines of code.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能需要灵活地控制哪些网站被抓取以及它们如何被抓取，并且需要一种快速添加新网站或修改现有网站的方法，而不需要编写多行代码。
- en: You may be asked to scrape product prices from different websites, with the
    ultimate aim of comparing prices for the same product. Perhaps these prices are
    in different currencies, and perhaps you’ll also need to combine this with external
    data from some other nonweb source.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能被要求从不同网站上抓取产品价格，最终目标是比较相同产品的价格。也许这些价格是以不同的货币计价的，也许你还需要将其与某些其他非网络来源的外部数据结合起来。
- en: Although the applications of web crawlers are nearly endless, large scalable
    crawlers tend to fall into one of several patterns. By learning these patterns
    and recognizing the situations they apply to, you can vastly improve the maintainability
    and robustness of your web crawlers.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然网络爬虫的应用几乎是无穷无尽的，但是大型可扩展的爬虫往往会落入几种模式之一。通过学习这些模式，并识别它们适用的情况，你可以极大地提高网络爬虫的可维护性和健壮性。
- en: This chapter focuses primarily on web crawlers that collect a limited number
    of “types” of data (such as restaurant reviews, news articles, company profiles)
    from a variety of websites and store these data types as Python objects that read
    and write from a database.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章主要关注收集有限数量“类型”数据的网络爬虫（例如餐馆评论、新闻文章、公司简介）从各种网站收集这些数据类型，并将其存储为 Python 对象，从数据库中读写。
- en: Planning and Defining Objects
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 规划和定义对象
- en: 'One common trap of web scraping is defining the data that you want to collect
    based entirely on what’s available in front of your eyes. For instance, if you
    want to collect product data, you may first look at a clothing store and decide
    that each product you scrape needs to have the following fields:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 网页抓取的一个常见陷阱是完全基于眼前可见的内容定义你想要收集的数据。例如，如果你想收集产品数据，你可能首先看一下服装店，决定你要抓取的每个产品都需要有以下字段：
- en: Product name
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品名称
- en: Price
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 价格
- en: Description
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述
- en: Sizes
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尺码
- en: Colors
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 颜色
- en: Fabric type
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 织物类型
- en: Customer rating
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户评分
- en: 'Looking at another website, you find that it has SKUs (stock-keeping units,
    used to track and order items) listed on the page. You definitely want to collect
    that data as well, even if it doesn’t appear on the first site! You add this field:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当你查看另一个网站时，你发现它在页面上列出了 SKU（用于跟踪和订购商品的库存单位）你肯定也想收集这些数据，即使在第一个网站上看不到它！你添加了这个字段：
- en: Item SKU
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 商品 SKU
- en: 'Although clothing may be a great start, you also want to make sure you can
    extend this crawler to other types of products. You start perusing product sections
    of other websites and decide you also need to collect this information:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然服装可能是一个很好的起点，但你也希望确保你可以将这个爬虫扩展到其他类型的产品上。你开始浏览其他网站的产品部分，并决定你还需要收集以下信息：
- en: Hardcover/paperback
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精装/平装
- en: Matte/glossy print
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚光/亮光打印
- en: Number of customer reviews
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户评论数量
- en: Link to manufacturer
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 制造商链接
- en: Clearly, this is an unsustainable approach. Simply adding attributes to your
    product type every time you see a new piece of information on a website will lead
    to far too many fields to keep track of. Not only that, but every time you scrape
    a new website, you’ll be forced to perform a detailed analysis of the fields the
    website has and the fields you’ve accumulated so far, and potentially add new
    fields (modifying your Python object type and your database structure). This will
    result in a messy and difficult-to-read dataset that may lead to problems using
    it.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这种方法是不可持续的。每次在网站上看到新的信息时，简单地将属性添加到产品类型中将导致要跟踪的字段过多。不仅如此，每次抓取新的网站时，你都将被迫对网站具有的字段和到目前为止积累的字段进行详细分析，并可能添加新字段（修改你的Python对象类型和数据库结构）。这将导致一个混乱且难以阅读的数据集，可能会导致在使用它时出现问题。
- en: One of the best things you can do when deciding which data to collect is often
    to ignore the websites altogether. You don’t start a project that’s designed to
    be large and scalable by looking at a single website and saying, “What exists?”
    but by saying, “What do I need?” and then finding ways to seek the information
    that you need from there.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在决定收集哪些数据时，你经常最好的做法是忽略网站。你不会通过查看单个网站并说“存在什么？”来开始一个旨在大规模和可扩展的项目，而是通过说“我需要什么？”然后找到从那里获取所需信息的方法。
- en: 'Perhaps what you really want to do is compare product prices among multiple
    stores and track those product prices over time. In this case, you need enough
    information to uniquely identify the product, and that’s it:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 也许你真正想做的是比较多家商店的产品价格，并随着时间跟踪这些产品价格。在这种情况下，你需要足够的信息来唯一标识产品，就是这样：
- en: Product title
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品标题
- en: Manufacturer
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 制造商
- en: Product ID number (if available/relevant)
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品ID编号（如果可用/相关）
- en: It’s important to note that none of this information is specific to a particular
    store. For instance, product reviews, ratings, price, and even description are
    specific to the instance of that product at a particular store. That can be stored
    separately.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，所有这些信息都不特定于特定商店。例如，产品评论、评级、价格，甚至描述都特定于特定商店中的该产品的实例。这可以单独存储。
- en: 'Other information (colors the product comes in, what it’s made of) is specific
    to the product but may be sparse—it’s not applicable to every product. It’s important
    to take a step back and perform a checklist for each item you consider and ask
    yourself these questions:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 其他信息（产品的颜色、材质）是产品特定的，但可能稀疏——并非适用于每种产品。重要的是要退后一步，对你考虑的每个项目执行一个清单，并问自己这些问题：
- en: Will this information help with the project goals? Will it be a roadblock if
    I don’t have it, or is it just “nice to have” but won’t ultimately impact anything?
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些信息是否有助于项目目标？如果我没有它，是否会成为一个障碍，还是它只是“好有”但最终不会对任何事情产生影响？
- en: If it *might* help in the future, but I’m unsure, how difficult will it be to
    go back and collect the data at a later time?
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果*可能*将来有用，但我不确定，那么在以后收集这些数据会有多困难？
- en: Is this data redundant to data I’ve already collected?
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些数据是否与我已经收集的数据重复了？
- en: Does it make logical sense to store the data within this particular object?
    (As mentioned before, storing a description in a product doesn’t make sense if
    that description changes from site to site for the same product.)
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这个特定对象中存储数据是否有逻辑意义？（如前所述，如果一个产品的描述在不同网站上发生变化，则在产品中存储描述是没有意义的。）
- en: 'If you do decide that you need to collect the data, it’s important to ask a
    few more questions to then decide how to store and handle it in code:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你决定需要收集这些数据，重要的是要提出更多问题，然后决定如何在代码中存储和处理它：
- en: Is this data sparse or dense? Will it be relevant and populated in every listing,
    or just a handful out of the set?
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些数据是稀疏的还是密集的？它在每个列表中是否都是相关且填充的，还是只有一小部分是相关的？
- en: How large is the data?
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据有多大？
- en: Especially in the case of large data, will I need to regularly retrieve it every
    time I run my analysis, or only on occasion?
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尤其是在大数据的情况下，我需要每次运行分析时定期检索它，还是只在某些情况下检索它？
- en: How variable is this type of data? Will I regularly need to add new attributes,
    modify types (such as fabric patterns, which may be added frequently), or is it
    set in stone (shoe sizes)?
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种类型的数据变化多大？我是否经常需要添加新属性，修改类型（如可能经常添加的面料图案），还是它是固定的（鞋码）？
- en: 'Let’s say you plan to do some meta-analysis around product attributes and prices:
    for example, the number of pages a book has, or the type of fabric a piece of
    clothing is made of, and potentially other attributes in the future, correlated
    to price. You run through the questions and realize that this data is sparse (relatively
    few products have any one of the attributes), and that you may decide to add or
    remove attributes frequently. In this case, it may make sense to create a product
    type that looks like this:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您计划围绕产品属性和价格进行一些元分析：例如，一本书的页数，或一件衣服的面料类型，以及未来可能的其他属性，与价格相关。您仔细思考这些问题，并意识到这些数据是稀疏的（相对较少的产品具有任何一个属性），并且您可能经常决定添加或删除属性。在这种情况下，创建一个如下所示的产品类型可能是有意义的：
- en: Product title
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品标题
- en: Manufacturer
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 制造商
- en: Product ID number (if available/relevant)
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品ID编号（如适用/相关）
- en: Attributes (optional list or dictionary)
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 属性（可选列表或字典）
- en: 'And an attribute type that looks like this:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 以及以下类似的属性类型：
- en: Attribute name
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 属性名称
- en: Attribute value
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 属性值
- en: This allows you to flexibly add new product attributes over time, without requiring
    you to redesign your data schema or rewrite code. When deciding how to store these
    attributes in the database, you can write JSON to the `attribute` field, or store
    each attribute in a separate table with a product ID. See [Chapter 9](ch09.html#c-9)
    for more information about implementing these types of database models.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这使您能够随时间灵活添加新的产品属性，而无需重新设计数据模式或重写代码。在决定如何在数据库中存储这些属性时，您可以将JSON写入`attribute`字段，或者将每个属性存储在一个带有产品ID的单独表中。有关实现这些类型数据库模型的更多信息，请参见[第9章](ch09.html#c-9)。
- en: 'You can apply the preceding questions to the other information you’ll need
    to store as well. For keeping track of the prices found for each product, you’ll
    likely need the following:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将前述问题应用于您需要存储的其他信息。例如，要跟踪每个产品找到的价格，您可能需要以下信息：
- en: Product ID
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品ID
- en: Store ID
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 商店ID
- en: Price
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 价格
- en: Date/timestamp when price was found
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记录价格发现的日期/时间戳
- en: 'But what if you have a situation in which the product’s attributes actually
    modify the price of the product? For instance, stores might charge more for a
    large shirt than a small one, because the large shirt requires more labor or materials.
    In this case, you may consider splitting the single shirt product into separate
    product listings for each size (so that each shirt product can be priced independently)
    or creating a new item type to store information about instances of a product,
    containing these fields:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果产品的属性实际上会改变产品的价格怎么办？例如，商店可能会对大号衬衫收取更高的价格，因为制作大号衬衫需要更多的人工或材料。在这种情况下，您可以考虑将单个衬衫产品拆分为每种尺寸的独立产品列表（以便每件衬衫产品可以独立定价），或者创建一个新的项目类型来存储产品实例信息，包含以下字段：
- en: Product ID
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品ID
- en: Instance type (the size of the shirt, in this case)
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实例类型（本例中为衬衫尺寸）
- en: 'And each price would then look like this:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 每个价格看起来会像这样：
- en: Product instance ID
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品实例ID
- en: Store ID
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 商店ID
- en: Price
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 价格
- en: Date/Timestamp when price was found
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记录价格发现的日期/时间戳
- en: While the subject of “products and prices” may seem overly specific, the basic
    questions you need to ask yourself, and the logic used when designing your Python
    objects, apply in almost every situation.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然“产品和价格”主题可能显得过于具体，但是在设计Python对象时，您需要问自己的基本问题和逻辑几乎适用于每种情况。
- en: 'If you’re scraping news articles, you may want basic information such as:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在抓取新闻文章，您可能希望获取基本信息，例如：
- en: Title
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标题
- en: Author
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作者
- en: Date
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日期
- en: Content
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内容
- en: But say some articles contain a “revision date,” or “related articles, or a
    “number of social media shares.” Do you need these? Are they relevant to your
    project? How do you efficiently and flexibly store the number of social media
    shares when not all news sites use all forms of social media, and social media
    sites may grow or wane in popularity over time?
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果一些文章包含“修订日期”，或“相关文章”，或“社交媒体分享数量”呢？您是否需要这些信息？它们是否与您的项目相关？当不是所有新闻网站都使用所有形式的社交媒体，并且社交媒体站点可能随时间而增长或减少时，您如何高效灵活地存储社交媒体分享数量？
- en: It can be tempting, when faced with a new project, to dive in and start writing
    Python to scrape websites immediately. The data model, left as an afterthought,
    often becomes strongly influenced by the availability and format of the data on
    the first website you scrape.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当面对一个新项目时，很容易立即投入到编写Python代码以抓取网站的工作中。然而，数据模型往往被第一个抓取的网站的数据的可用性和格式所强烈影响，而被忽视。
- en: However, the data model is the underlying foundation of all the code that uses
    it. A poor decision in your model can easily lead to problems writing and maintaining
    code down the line, or difficulty in extracting and efficiently using the resulting
    data. Especially when dealing with a variety of websites—both known and unknown—it
    becomes vital to give serious thought and planning to what, exactly, you need
    to collect and how you need to store it.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，数据模型是所有使用它的代码的基础。在模型中做出不好的决定很容易导致以后编写和维护代码时出现问题，或者在提取和高效使用结果数据时出现困难。特别是在处理各种网站（已知和未知的）时，认真考虑和规划你需要收集什么，以及你如何存储它变得至关重要。
- en: Dealing with Different Website Layouts
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理不同的网页布局
- en: One of the most impressive feats of a search engine such as Google is that it
    manages to extract relevant and useful data from a variety of websites, having
    no up-front knowledge about the website structure itself. Although we, as humans,
    are able to immediately identify the title and main content of a page (barring
    instances of extremely poor web design), it is far more difficult to get a bot
    to do the same thing.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 象Google这样的搜索引擎最令人印象深刻的一个特性是，它能够从各种网站中提取相关且有用的数据，而不需要预先了解网站的结构。尽管我们作为人类能够立即识别页面的标题和主要内容（除了极端糟糕的网页设计情况），但让机器人做同样的事情要困难得多。
- en: Fortunately, in most cases of web crawling, you’re not looking to collect data
    from sites you’ve never seen before, but from a few, or a few dozen, websites
    that are pre-selected by a human. This means that you don’t need to use complicated
    algorithms or machine learning to detect which text on the page “looks most like
    a title” or which is probably the “main content.” You can determine what these
    elements are manually.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，在大多数网络爬虫的情况下，你不需要从你以前从未见过的网站收集数据，而是从少数或几十个由人类预先选择的网站收集数据。这意味着你不需要使用复杂的算法或机器学习来检测页面上“看起来最像标题”的文本或者哪些可能是“主要内容”。你可以手动确定这些元素是什么。
- en: The most obvious approach is to write a separate web crawler or page parser
    for each website. Each might take in a URL, string, or `BeautifulSoup` object,
    and return a Python object for the thing that was scraped.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 最明显的方法是为每个网站编写单独的网络爬虫或页面解析器。每个解析器可能接受一个URL、字符串或`BeautifulSoup`对象，并返回一个被爬取的Python对象。
- en: 'The following is an example of a `Content` class (representing a piece of content
    on a website, such as a news article) and two scraper functions that take in a
    `BeautifulSoup` object and return an instance of `Content`:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个`Content`类的示例（代表网站上的一篇内容，比如新闻文章），以及两个爬虫函数，它们接受一个`BeautifulSoup`对象并返回一个`Content`的实例：
- en: '[PRE0]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As you start to add scraper functions for additional news sites, you might
    notice a pattern forming. Every site’s parsing function does essentially the same
    thing:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 当你开始为额外的新闻网站添加爬虫函数时，你可能会注意到一种模式正在形成。每个网站的解析函数基本上都在做同样的事情：
- en: Selects the title element and extracts the text for the title
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择标题元素并提取标题文本
- en: Selects the main content of the article
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择文章的主要内容
- en: Selects other content items as needed
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据需要选择其他内容项
- en: Returns a `Content` object instantiated with the strings found previously
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 返回一个通过先前找到的字符串实例化的`Content`对象
- en: The only real site-dependent variables here are the CSS selectors used to obtain
    each piece of information. BeautifulSoup’s `find`  and `find_all` functions take
    in two arguments—a tag string and a dictionary of key/value attributes—so you
    can pass these arguments in as parameters that define the structure of the site
    itself and the location of the target data.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这里唯一真正依赖于网站的变量是用于获取每个信息片段的CSS选择器。BeautifulSoup的`find`和`find_all`函数接受两个参数——一个标签字符串和一个键/值属性字典——所以你可以将这些参数作为定义站点结构和目标数据位置的参数传递进去。
- en: 'To make things even more convenient, rather than dealing with all of these
    tag arguments and key/value pairs, you can use the BeautifulSoup `select` function
    with a single string CSS selector for each piece of information you want to collect
    and put all of these selectors in a dictionary object:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 更方便的是，你可以使用BeautifulSoup的`select`函数，用一个单一的字符串CSS选择器来获取每个想要收集的信息片段，并将所有这些选择器放在一个字典对象中：
- en: '[PRE1]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that the `Website` class does not store information collected from the
    individual pages themselves, but stores instructions about *how* to collect that
    data. It doesn’t store the title “My Page Title.” It simply stores the string
    tag `h1` that indicates where the titles can be found. This is why the class is
    called `Website` (the information here pertains to the entire website) and not
    `Content` (which contains information from just a single page).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`Website`类不存储从单个页面收集的信息，而是存储关于*如何*收集这些数据的说明。它不存储标题“我的页面标题”。它只是存储表示标题位置的字符串标签`h1`。这就是为什么这个类被称为`Website`（这里的信息涉及整个网站）而不是`Content`（它只包含来自单个页面的信息）的原因。
- en: 'As you write web scrapers, you may notice that you tend to do many of the same
    tasks over and over again. For instance: fetch the content of a page while checking
    for errors, get the contents of a tag, and fail gracefully if these are not found.
    Let’s add these to a `Crawler` class:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 当您编写网络爬虫时，您可能会注意到您经常一遍又一遍地执行许多相同的任务。例如：获取页面内容同时检查错误，获取标签内容，如果找不到则优雅地失败。让我们将这些添加到一个`Crawler`类中：
- en: '[PRE2]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Note that the `Crawler` class currently does not have any state. It’s simply
    a collection of static methods. It also seems poorly named—it doesn’t do any crawling
    at all! You can at least make it slightly more useful by adding a `getContent`
    method to it, which takes as an argument a `website` object and a URL, and returns
    a `Content` object:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`Crawler`类目前没有任何状态。它只是一组静态方法。它的命名似乎也很差劲——它根本不进行任何爬取！你至少可以通过向其添加一个`getContent`方法稍微增加其实用性，该方法接受一个`website`对象和一个URL作为参数，并返回一个`Content`对象：
- en: '[PRE3]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The following shows how these `Content`, `website`, and `Crawler` classes can
    be used together to scrape four different websites:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 以下显示了如何将这些`Content`、`website`和`Crawler`类一起使用来爬取四个不同的网站：
- en: '[PRE4]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: While this new method might not seem remarkably simpler than writing a new Python
    function for each new website at first glance, imagine what happens when you go
    from a system with 4 website sources to a system with 20 or 200 sources.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这种新方法乍看起来可能并不比为每个新网站编写新的Python函数更简单，但想象一下当你从一个拥有4个网站来源的系统转变为一个拥有20个或200个来源的系统时会发生什么。
- en: Each list of strings defining a new website is relatively easy to write. It
    doesn’t take up much space. It can be loaded from a database or a CSV file. It
    can be imported from a remote source or handed off to a nonprogrammer with a little
    frontend experience. This programmer can fill it out and add new websites to the
    scraper without ever having to look at a line of code.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个新网站的每个字符串列表都相对容易编写。它不占用太多空间。它可以从数据库或CSV文件中加载。它可以从远程源导入或交给具有一点前端经验的非程序员。这个程序员可以填写它并向爬虫添加新的网站，而无需查看一行代码。
- en: Of course, the downside is that you are giving up a certain amount of flexibility.
    In the first example, each website gets its own free-form function to select and
    parse HTML however necessary to get the end result. In the second example, each
    website needs to have a certain structure in which fields are guaranteed to exist,
    data must be clean coming out of the field, and each target field must have a
    unique and reliable CSS selector.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，缺点是你放弃了一定的灵活性。在第一个示例中，每个网站都有自己的自由形式函数来选择和解析HTML，以便获得最终结果。在第二个示例中，每个网站需要具有一定的结构，其中字段被保证存在，数据必须在字段出来时保持干净，每个目标字段必须具有唯一且可靠的CSS选择器。
- en: However, I believe that the power and relative flexibility of this approach
    more than makes up for its real or perceived shortcomings. The next section covers
    specific applications and expansions of this basic template so that you can, for
    example, deal with missing fields, collect different types of data, crawl through
    specific parts of a website, and store more-complex information about pages.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，我相信这种方法的力量和相对灵活性远远弥补了其实际或被认为的缺点。下一节将涵盖此基本模板的具体应用和扩展，以便您可以处理丢失的字段，收集不同类型的数据，浏览网站的特定部分，并存储关于页面的更复杂的信息。
- en: Structuring Crawlers
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结构化爬虫
- en: Creating flexible and modifiable website layout types doesn’t do much good if
    you still have to locate each link you want to scrape by hand. [Chapter 6](ch06.html#c-6)
    showed various automated methods of crawling through websites and finding new
    pages.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 创建灵活和可修改的网站布局类型如果仍然需要手动定位要爬取的每个链接，则不会有太大帮助。[第6章](ch06.html#c-6)展示了通过网站并找到新页面的各种自动化方法。
- en: This section shows how to incorporate these methods into a well-structured and
    expandable website crawler that can gather links and discover data in an automated
    way. I present just three basic web crawler structures here; they apply to the
    majority of situations that you will likely encounter when crawling sites in the
    wild, perhaps with a few modifications here and there. If you encounter an unusual
    situation with your own crawling problem, I hope that you will use these structures
    as inspiration to create an elegant and robust crawler design.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 本节展示了如何将这些方法整合到一个结构良好且可扩展的网站爬虫中，该爬虫可以自动收集链接并发现数据。我在这里仅介绍了三种基本的网页爬虫结构；它们适用于你在野外爬取网站时可能遇到的大多数情况，也许在某些情况下需要进行一些修改。
- en: Crawling Sites Through Search
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过搜索爬取网站
- en: 'One of the easiest ways to crawl a website is via the same method that humans
    use with the search bar. Although the process of searching a website for a keyword
    or topic and collecting a list of search results may seem like a task with a lot
    of variability from site to site, several key points make this surprisingly trivial:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 通过与搜索栏相同的方法，爬取网站是最简单的方法之一。尽管在网站上搜索关键词或主题并收集搜索结果列表的过程似乎因网站而异，但几个关键点使其出奇地简单：
- en: 'Most sites retrieve a list of search results for a particular topic by passing
    that topic as a string through a parameter in the URL. For example: `http://example.com?search=myTopic`.
    The first part of this URL can be saved as a property of the `Website` object,
    and the topic can simply be appended to it.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数网站通过在URL参数中将主题作为字符串传递来检索特定主题的搜索结果列表。例如：`http://example.com?search=myTopic`。这个URL的前半部分可以保存为`Website`对象的属性，主题只需简单附加即可。
- en: After searching, most sites present the resulting pages as an easily identifiable
    list of links, usually with a convenient surrounding tag such as `<span class="result">`,
    the exact format of which can also be stored as a property of the `Website` object.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索后，大多数网站将结果页面呈现为易于识别的链接列表，通常使用方便的包围标签，如`<span class="result">`，其确切格式也可以作为`Website`对象的属性存储。
- en: Each *result link* is either a relative URL (e.g., */articles/page.html*) or
    an absolute URL (e.g., *http://example.com/articles/page.html*). Whether you are
    expecting an absolute or relative URL, it can be stored as a property of the `Website`
    object.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个*结果链接*都可以是相对URL（例如，*/articles/page.html*）或绝对URL（例如，*http://example.com/articles/page.html*）。无论你期望绝对还是相对URL，它都可以作为`Website`对象的属性存储。
- en: After you’ve located and normalized the URLs on the search page, you’ve successfully
    reduced the problem to the example in the previous section—extracting data from
    a page, given a website format.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你定位并规范化搜索页面上的URL后，你已成功将问题简化为前一节示例中的情况——在给定网站格式的情况下提取页面数据。
- en: 'Let’s look at an implementation of this algorithm in code. The `Content` class
    is much the same as in previous examples. You are adding the URL property to keep
    track of where the content was found:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下这个算法在代码中的实现。`Content`类与之前的示例大致相同。你正在添加URL属性来跟踪内容的来源：
- en: '[PRE5]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `Website` class has a few new properties added to it. The `searchUrl` defines
    where you should go to get search results if you append the topic you are looking
    for. The `resultListing` defines the “box” that holds information about each result,
    and the `resultUrl` defines the tag inside this box that will give you the exact
    URL for the result. The `absoluteUrl` property is a boolean that tells you whether
    these search results are absolute or relative URLs:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`Website`类添加了一些新属性。`searchUrl`定义了你应该去哪里获取搜索结果，如果你附加你要查找的主题。`resultListing`定义了包含每个结果信息的“框”，`resultUrl`定义了这个框内部的标签，将为你提供结果的确切URL。`absoluteUrl`属性是一个布尔值，告诉你这些搜索结果是绝对URL还是相对URL。'
- en: '[PRE6]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The `Crawler` class has also expanded. It now has a `Website` object, as well
    as a dictionary of URLs pointing to `Content` objects, to keep track of what it
    has seen before. Note that the methods `getPage` and `safeGet` have not changed
    and are omitted here:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`Crawler`类也有所扩展。它现在有一个`Website`对象，以及一个指向`Content`对象的URL字典，用于跟踪它之前所见过的内容。请注意，`getPage`和`safeGet`方法没有更改，这里省略了它们：'
- en: '[PRE7]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You can call your Crawler like this:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以像这样调用你的爬虫：
- en: '[PRE8]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'As before, an array of data is created about each website: what the tags look
    like, what the URL is, and a name for tracking purposes. This data is then loaded
    into a list of `Website` objects and turned into `Crawler` objects.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 就像以前一样，关于每个网站的数据数组被创建：标签的外观、URL和用于跟踪目的的名称。然后将这些数据加载到`Website`对象的列表中，并转换为`Crawler`对象。
- en: 'Then it loops through each crawler in the `crawlers` list and crawls each particular
    site for each particular topic. Each time it successfully collects information
    about a page, it prints it to the console:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然后它会循环遍历`crawlers`列表中的每个爬虫，并为每个特定主题的每个特定站点爬取信息。每次成功收集有关页面的信息时，它都会将其打印到控制台：
- en: '[PRE9]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note that it loops through all topics and then loops through all websites in
    the inner loop. Why not do it the other way around, collecting all topics from
    one website and then all topics from the next website? Looping through all topics
    first is a way to more evenly distribute the load placed on any one web server.
    This is especially important if you have a list of hundreds of topics and dozens
    of websites. You’re not making tens of thousands of requests to one website at
    once; you’re making 10 requests, waiting a few minutes, making another 10 requests,
    waiting a few minutes, and so forth.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，它首先遍历所有主题，然后在内部循环中遍历所有网站。为什么不反过来做，先从一个网站收集所有主题，然后再从下一个网站收集所有主题呢？首先遍历所有主题可以更均匀地分配对任何一个Web服务器的负载。如果你有数百个主题和几十个网站的列表，这一点尤为重要。你不是一次性向一个网站发送成千上万个请求；你发送10个请求，等待几分钟，然后再发送10个请求，依此类推。
- en: Although the number of requests is ultimately the same either way, it’s generally
    better to distribute these requests over time as much as is reasonable. Paying
    attention to how your loops are structured is an easy way to do this.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管无论哪种方式，请求的数量最终是相同的，但是通常最好尽量合理地分布这些请求的时间。注意如何结构化你的循环是做到这一点的简单方法。
- en: Crawling Sites Through Links
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过链接爬取站点
- en: '[Chapter 6](ch06.html#c-6) covered some ways of identifying internal and external
    links on web pages and then using those links to crawl across the site. In this
    section, you’ll combine those same basic methods into a more flexible website
    crawler that can follow any link matching a specific URL pattern.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[第六章](ch06.html#c-6)介绍了在网页上识别内部和外部链接的一些方法，然后利用这些链接来跨站点爬取。在本节中，你将把这些基本方法结合起来，形成一个更灵活的网站爬虫，可以跟随任何匹配特定URL模式的链接。'
- en: This type of crawler works well for projects when you want to gather all the
    data from a site—not just data from a specific search result or page listing.
    It also works well when the site’s pages may be disorganized or widely dispersed.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 当你想要从一个站点收集所有数据时——而不仅仅是特定搜索结果或页面列表的数据时，这种类型的爬虫效果很好。当站点的页面可能不太有序或广泛分布时，它也能很好地工作。
- en: 'These types of crawlers don’t require a structured method of locating links,
    as in the previous section on crawling through search pages, so the attributes
    that describe the search page aren’t required in the `Website` object. However,
    because the crawler isn’t given specific instructions for the locations/positions
    of the links it’s looking for, you do need some rules to tell it what sorts of
    pages to select. You provide a `target​Pat⁠tern` (regular expression for the target
    URLs) and leave the boolean `absoluteUrl` variable to accomplish this:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这些类型的爬虫不需要像在前一节中爬取搜索页面那样定位链接的结构化方法，因此在`Website`对象中不需要描述搜索页面的属性。但是，由于爬虫没有为其正在查找的链接位置/位置提供具体指令，因此您需要一些规则来告诉它选择哪种类型的页面。您提供一个`target​Pat⁠tern`（目标URL的正则表达式），并留下布尔变量`absoluteUrl`来完成此操作：
- en: '[PRE10]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `Content` class is the same one used in the first crawler example.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '`Content`类与第一个爬虫示例中使用的相同。'
- en: 'The `Crawler` class is written to start from the home page of each site, locate
    internal links, and parse the content from each internal link found:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '`Crawler`类被设计为从每个站点的主页开始，定位内部链接，并解析每个找到的内部链接的内容：'
- en: '[PRE11]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As in the previous example, the `Website` object is a property of the `Crawler`
    object itself. This works well to store the visited pages (`visited`) in the crawler
    but means that a new crawler must be instantiated for each website rather than
    reusing the same one to crawl a list of websites.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 与先前的示例一样，`Website`对象是`Crawler`对象本身的属性。这样可以很好地存储爬虫中访问的页面(`visited`)，但意味着必须为每个网站实例化一个新的爬虫，而不是重复使用同一个爬虫来爬取网站列表。
- en: Whether you choose to make a crawler website-agnostic or choose to make the
    website an attribute of the crawler is a design decision that you must weigh in
    the context of your own specific needs. Either approach is generally fine.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你选择将爬虫设计成与网站无关还是将网站作为爬虫的属性，都是你必须在特定需求背景下权衡的设计决策。两种方法通常都可以接受。
- en: Another thing to note is that this crawler will get the pages from the home
    page, but it will not continue crawling after all those pages have been logged.
    You may want to write a crawler incorporating one of the patterns in this chapter
    and have it look for more targets on each page it visits. You can even follow
    all the URLs on each page (not just ones matching the target pattern) to look
    for URLs containing the target pattern.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要注意的是，这个爬虫将从主页获取页面，但在记录了所有这些页面后，它将不会继续爬取。你可能希望编写一个爬虫，采用本章中的模式之一，并让它在访问的每个页面上查找更多目标。甚至可以跟踪每个页面上的所有URL（不仅限于与目标模式匹配的URL），以寻找包含目标模式的URL。
- en: Crawling Multiple Page Types
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 爬取多种页面类型
- en: 'Unlike crawling through a predetermined set of pages, crawling through all
    internal links on a website can present a challenge in that you never know exactly
    what you’re getting. Fortunately, there are a few ways to identify the page type:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 与通过预定页面集合进行爬取不同，通过网站上所有内部链接进行爬取可能会带来挑战，因为你永远不知道确切的内容。幸运的是，有几种方法可以识别页面类型：
- en: By the URL
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 通过URL
- en: All blog posts on a website might contain a URL (*http://example.com/blog/title-of-post*,
    for example).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 网站上的所有博客文章可能都包含一个URL（例如*http://example.com/blog/title-of-post*）。
- en: By the presence or lack of certain fields on a site
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 通过网站上特定字段的存在或缺失
- en: If a page has a date but no author name, you might categorize it as a press
    release. If it has a title, main image, and price but no main content, it might
    be a product page.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个页面有日期但没有作者名字，你可能会将其归类为新闻稿。如果它有标题、主图像和价格但没有主要内容，它可能是一个产品页面。
- en: By the presence of certain tags on the page to identify the page
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 通过页面上特定的标签来识别页面
- en: You can take advantage of tags even if you’re not collecting the data within
    the tags. Your crawler might look for an element such as `<div id="related-products">`
    to identify the page as a product page, even though the crawler is not interested
    in the content of the related products.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你不收集标签内的数据，你也可以利用标签。例如，你的爬虫可能会查找诸如`<div id="related-products">`这样的元素来识别页面是否为产品页面，尽管爬虫并不关心相关产品的内容。
- en: To keep track of multiple page types, you need to have multiple types of page
    objects in Python. This can be done in two ways.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟踪多种页面类型，你需要在Python中拥有多种类型的页面对象。有两种方法可以做到这一点。
- en: 'If the pages are all similar (they all have basically the same types of content),
    you may want to add a `pageType` attribute to your existing web-page object:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果页面都很相似（它们基本上都具有相同类型的内容），你可能希望为现有的网页对象添加一个`pageType`属性：
- en: '[PRE12]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: If you’re storing these pages in an SQL-like database, this type of pattern
    indicates that all these pages would probably be stored in the same table and
    that an extra `pageType` column would be added.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将这些页面存储在类似SQL的数据库中，这种模式表明所有这些页面可能都存储在同一张表中，并且会添加一个额外的`pageType`列。
- en: 'If the pages/content you’re scraping are different enough from each other (they
    contain different types of fields), this may warrant creating new classes for
    each page type. Of course, some things will be common to all web pages—they will
    all have a URL and likely also a name or page title. This is an ideal situation
    in which to use subclasses:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你要抓取的页面/内容差异很大（它们包含不同类型的字段），这可能需要为每种页面类型创建新的类。当然，一些东西对所有网页都是通用的——它们都有一个URL，很可能还有一个名称或页面标题。这是使用子类的理想情况：
- en: '[PRE13]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This `Product` page extends the `Website` base class and adds the attributes
    `productNumber` and `price` that apply only to products; the `Article` class adds
    the attributes `body` and `date`, which don’t apply to products.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`Product`页面扩展了`Website`基类，并添加了仅适用于产品的`productNumber`和`price`属性；`Article`类添加了`body`和`date`属性，这些属性不适用于产品。
- en: You can use these two classes to scrape, for example, a store website that might
    contain blog posts or press releases in addition to products.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用这两个类来抓取，例如，一个商店网站可能除了产品之外还包含博客文章或新闻稿。
- en: Thinking About Web Crawler Models
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 思考网络爬虫模型
- en: Collecting information from the internet can be like drinking from a fire hose.
    There’s a lot of stuff out there, and it’s not always clear what you need or how
    you need it. The first step of any large web scraping project (and even some of
    the small ones) should be to answer these questions.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 从互联网收集信息有如饮水从火龙头喝水。那里有很多东西，而且并不总是清楚您需要什么或者您如何需要它。任何大型网络抓取项目（甚至某些小型项目）的第一步应该是回答这些问题。
- en: When collecting similar data across multiple domains or from multiple sources,
    your goal should almost always be to try to normalize it. Dealing with data with
    identical and comparable fields is much easier than dealing with data that is
    completely dependent on the format of its original source.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在收集多个领域或多个来源的类似数据时，几乎总是应该尝试对其进行规范化。处理具有相同和可比较字段的数据要比处理完全依赖于其原始来源格式的数据容易得多。
- en: In many cases, you should build scrapers under the assumption that more sources
    of data will be added to them in the future, with the goal to minimize the programming
    overhead required to add these new sources. Even if a website doesn’t appear to
    fit your model at first glance, there may be more subtle ways that it does conform.
    Being able to see these underlying patterns can save you time, money, and a lot
    of headaches in the long run.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，您应该在假设将来将添加更多数据源到抓取器的基础上构建抓取器，目标是最小化添加这些新来源所需的编程开销。即使一个网站乍一看似乎与您的模型不符，也可能有更微妙的方式使其符合。能够看到这些潜在模式可以在长远中为您节省时间、金钱和许多头疼的问题。
- en: The connections between pieces of data should also not be ignored. Are you looking
    for information that has properties such as “type,” “size,” or “topic” that span
    data sources? How do you store, retrieve, and conceptualize these attributes?
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 数据之间的关联也不应被忽视。您是否在寻找跨数据源具有“类型”、“大小”或“主题”等属性的信息？您如何存储、检索和概念化这些属性？
- en: Software architecture is a broad and important topic that can take an entire
    career to master. Fortunately, software architecture for web scraping is a much
    more finite and manageable set of skills that can be relatively easily acquired.
    As you continue to scrape data, you will find the same basic patterns occurring
    over and over. Creating a well-structured web scraper doesn’t require a lot of
    arcane knowledge, but it does require taking a moment to step back and think about
    your project.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 软件架构是一个广泛而重要的主题，可能需要整个职业生涯来掌握。幸运的是，用于网络抓取的软件架构是一组相对容易获取的有限且可管理的技能。随着您继续抓取数据，您会发现相同的基本模式一次又一次地出现。创建一个良好结构化的网络爬虫并不需要太多神秘的知识，但确实需要花时间退后一步，思考您的项目。
