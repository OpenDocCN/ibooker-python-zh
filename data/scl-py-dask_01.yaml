- en: Chapter 1\. What Is Dask?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 1 章 什么是 Dask？
- en: Dask is a framework for parallelized computing with Python that scales from
    multiple cores on one machine to data centers with thousands of machines. It has
    both low-level task APIs and higher-level data-focused APIs. The low-level task
    APIs power Dask’s integration with a wide variety of Python libraries. Having
    public APIs has allowed an ecosystem of tools to grow around Dask for various
    use cases.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 是一个用于 Python 的并行计算框架，从单机多核扩展到拥有数千台机器的数据中心。它既有低级任务 API，也有更高级的面向数据的 API。低级任务
    API 支持 Dask 与多种 Python 库的集成。公共 API 的存在使得围绕 Dask 发展了各种工具的生态系统。
- en: Continuum Analytics, now known as Anaconda Inc, started the open source, DARPA-funded
    [Blaze project](https://oreil.ly/FyqwQ), which has evolved into Dask. Continuum
    has participated in developing many essential libraries and even conferences in
    the Python data analytics space. Dask remains an open source project, with much
    of its development now being supported by [Coiled](https://oreil.ly/BMLuP).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Continuum Analytics，现在被称为 Anaconda Inc，启动了开源、DARPA 资助的 [Blaze 项目](https://oreil.ly/FyqwQ)，该项目演变为
    Dask。Continuum 参与开发了 Python 数据分析领域许多重要库甚至会议。Dask 仍然是一个开源项目，现在大部分开发得到 [Coiled](https://oreil.ly/BMLuP)
    的支持。
- en: Dask is unique in the distributed computing ecosystem, because it integrates
    popular data science, parallel, and scientific computing libraries. Dask’s integration
    of different libraries allows developers to reuse much of their existing knowledge
    at scale. They can also frequently reuse some of their code with minimal changes.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 在分布式计算生态系统中独具一格，因为它整合了流行的数据科学、并行和科学计算库。Dask 整合不同库的能力允许开发者在规模化时重复使用他们的现有知识。他们还可以最小程度地更改一些代码并频繁重复使用它们。
- en: Why Do You Need Dask?
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么需要使用 Dask？
- en: Dask simplifies scaling analytics, ML, and other code written in Python,^([1](ch01.xhtml#id288))
    allowing you to handle larger and more complex data and problems. Dask aims to
    fill the space where your existing tools, like pandas DataFrames, or your scikit-learn
    machine learning pipelines start to become too slow (or do not succeed). While
    the term “big data” is perhaps less in vogue now than a few years ago, the data
    size of the problems has not gotten smaller, and the complexity of the computation
    and models has not gotten simpler. Dask allows you to primarily use the existing
    interfaces that you are used to (such as pandas and multi-processing) while going
    beyond the scale of a single core or even a single machine.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 简化了用 Python 编写的分析、机器学习和其他代码的扩展，^([1](ch01.xhtml#id288)) 允许你处理更大更复杂的数据和问题。Dask
    的目标是填补现有工具（如 pandas DataFrames 或你的 scikit-learn 机器学习流水线）在处理速度变慢（或无法成功）时的空白。虽然“大数据”这个术语可能比几年前少流行一些，但问题的数据规模并没有减小，计算和模型的复杂性也没有变得更简单。Dask
    允许你主要使用你习惯的现有接口（如 pandas 和多进程），同时超越单个核心甚至单台机器的规模。
- en: Note
  id: totrans-6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: On the other hand, if all your data fits in memory on a laptop, and you can
    finish your analysis before you’ve had a chance to brew a cup of your favorite
    warm beverage, you probably don’t need Dask yet.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果你所有的数据都能在笔记本电脑的内存中处理，并且你能在你喝完一杯最喜欢的热饮之前完成分析，那么你可能还不需要使用 Dask。
- en: Where Does Dask Fit in the Ecosystem?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dask 在生态系统中的位置？
- en: 'Dask provides scalability to multiple, traditionally distinct tools. It is
    most often used to scale Python data libraries like pandas and NumPy. Dask extends
    existing tools for scaling, such as multi-processing, allowing them to exceed
    their current limits of single machines to multi-core and multi-machine. The following
    provides a quick look at the ecosystem evolution:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 提供了对多个传统上独立工具的可扩展性。它通常用于扩展 Python 数据库库，如 pandas 和 NumPy。Dask 扩展了现有的扩展工具，例如多进程，使它们能够超越单机的当前限制，扩展到多核和多机。以下是生态系统演变的简要概述：
- en: Early “big data” query
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 先“大数据”查询
- en: Apache Hadoop and Apache Hive
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Hadoop 和 Apache Hive
- en: Later “big data” query
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 后“大数据”查询
- en: Apache Flink and Apache Spark
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Flink 和 Apache Spark
- en: DataFrame-focused distributed tools
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 集中于 DataFrame 的分布式工具
- en: Koalas, Ray, and Dask
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Koalas、Ray 和 Dask
- en: 'From an abstraction point of view, Dask sits above the machines and cluster
    management tools, allowing you to focus on Python code instead of the intricacies
    of machine-to-machine communication:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 从抽象角度来看，Dask 位于机器和集群管理工具之上，使你能够专注于 Python 代码，而不是机器间通信的复杂性：
- en: Scalable data and ML tools
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展的数据和机器学习工具
- en: Hadoop, Hive, Flink, Spark, TensorFlow, Koalas, Ray, Dask, etc.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop、Hive、Flink、Spark、TensorFlow、Koalas、Ray、Dask 等
- en: Compute resources
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 计算资源
- en: Apache Hadoop YARN, Kubernetes, Amazon Web Services, Slurm Workload Manager,
    etc.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Hadoop YARN、Kubernetes、Amazon Web Services、Slurm Workload Manager 等。
- en: We say a problem is *compute-bound* if the limiting factor is not the amount
    of data but rather the work we are doing on the data. *Memory-bound* problems
    are problems in which the computation is not the limiting factor; rather, the
    ability to store all the data in memory is the limiting factor. Some problems
    can be both compute-bound and memory-bound, as is often the case for large deep-learning
    problems.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果限制因素不是数据量而是我们对数据的处理工作，则我们说问题是*计算密集型*。*内存限制*问题是指计算不是限制因素；相反，能否将所有数据存储在内存中是限制因素。某些问题既可以是计算密集型又可以是内存密集型，这在大型深度学习问题中经常发生。
- en: Multi-core (think multi-threading) processing can help with compute-bound problems
    (up to the limit of the number of cores in a machine). Generally, multi-core processing
    is unable to help with memory-bound problems, as all Central Processing Units
    (CPUs) have similar access to the memory.^([2](ch01.xhtml#id304))
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 多核心（考虑多线程）处理可以帮助解决计算密集型问题（在机器核心数限制内）。通常情况下，多核心处理无法帮助解决内存密集型问题，因为所有中央处理单元（CPU）对内存的访问方式相似。^([2](ch01.xhtml#id304))
- en: Accelerated processing, including the use of specialized instruction sets or
    specialized hardware like Tensor Processing Units or Graphics Processing Units,
    is generally useful only for compute-bound problems. Sometimes using accelerated
    processing introduces memory-bound problems, as the amount of memory available
    to the accelerated computation can be smaller than the “main” system memory.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 加速处理，包括使用专门的指令集或专用硬件如张量处理单元（TPU）或图形处理单元（GPU），通常仅对计算密集型问题有用。有时使用加速处理会引入内存限制问题，因为加速计算的内存可用量可能小于“主”系统内存。
- en: Multi-machine processing is important for both classes of problems. Since the
    number of cores you can get in a machine (affordably) is limited, even if a problem
    is “only” compute bound at certain scales, you will need to consider multi-machine
    processing. More commonly, memory-bound problems are a good fit for multi-machine
    scaling, as Dask can often split the data between the different machines.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这两类问题，多机处理都很重要。因为即使在某些规模上问题“仅”是计算密集型，您也需要考虑多机处理，因为在一台机器上您能（负担得起的话）获得的核心数量有限。更常见的是，内存限制问题非常适合多机扩展，因为
    Dask 常常能够将数据分割到不同的机器上。
- en: Dask has both multi-core and multi-machine scaling, allowing you to scale your
    Python code as you see fit.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 既支持多核心，也支持多机器扩展，允许您根据需要扩展 Python 代码。
- en: Much of Dask’s power comes from the tools and libraries built on top of it,
    which fit into their parts of the data processing ecosystem (such as BlazingSQL).
    Your background and interest will naturally shape how you first view Dask, so
    in the following subsections, we’ll briefly discuss how you can use Dask for different
    types of problems, as well as how it compares to some existing tools.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 的许多功能来自于建立在其之上的工具和库，这些工具和库适应其在数据处理生态系统中的各个部分（如 BlazingSQL）。您的背景和兴趣自然会影响您首次查看
    Dask 的方式，因此在接下来的小节中，我们将简要讨论您如何在不同类型的问题上使用 Dask，以及它与一些现有工具的比较。
- en: Big Data
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大数据
- en: Dask has better Python library integrations and lower overhead for tasks than
    many alternatives. Apache Spark (and its Python companion, PySpark) is one of
    the most popular tools for big data. Existing big data tools, such as PySpark,
    have more data sources and optimizers (like predicate push-down) but higher overhead
    per task. Dask’s lower overhead is due mainly to the rest of the Python big data
    ecosystem being built primarily on top of the JVM. These tools have advanced features
    such as query optimizers, but with the cost of copying data between the JVM and
    Python.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 拥有比许多替代方案更好的 Python 库集成和较低的任务开销。Apache Spark（及其 Python 伴侣 PySpark）是最流行的大数据工具之一。现有的大数据工具，如
    PySpark，具有更多的数据源和优化器（如谓词下推），但每个任务的开销更高。Dask 的较低开销主要归因于 Python 大数据生态系统的其他部分主要构建在
    JVM 之上。这些工具具有高级功能，如查询优化器，但以在 JVM 和 Python 之间复制数据为代价。
- en: Unlike many other traditional big data tools, such as Spark and Hadoop, Dask
    considers local mode a first-class citizen. The traditional big data ecosystem
    focuses on using the local mode for testing, but Dask focuses on good performance
    when running on a single node.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 与许多其他传统的大数据工具不同，如 Spark 和 Hadoop，Dask 将本地模式视为一等公民。传统的大数据生态系统侧重于在测试时使用本地模式，但
    Dask 专注于在单个节点上运行时的良好性能。
- en: Another significant cultural difference comes from packaging, with many projects
    in big data putting everything together (for example, Spark SQL, Spark Kubernetes,
    and so on are released together). Dask takes a more modular approach, with its
    components following their own development and release cadence. Dask’s approach
    can iterate faster, at the cost of occasional incompatibilities between libraries.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个显著的文化差异来自打包，许多大数据项目将所有内容整合在一起（例如，Spark SQL、Spark Kubernetes 等一起发布）。Dask 采用更模块化的方法，其组件遵循其自己的开发和发布节奏。Dask
    的这种方法可以更快地迭代，但有时会导致库之间的不兼容性。
- en: Data Science
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据科学
- en: One of the most popular Python libraries in the data science ecosystem is pandas.
    Apache Spark (and its Python companion, PySpark) is also one of the most popular
    tools for distributed data science. It has support for both Python and JVM languages.
    Spark’s first attempt at DataFrames more closely resembled SQL than what you may
    think of as DataFrames. While Spark has started to integrate pandas support with
    the [Koalas project](https://oreil.ly/VmU6O), Dask’s support of data science library
    APIs is best in class, in our opinion.^([3](ch01.xhtml#id310)) In addition to
    the pandas APIs, Dask supports scaling NumPy, scikit-learn, and other data science
    tools.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据科学生态系统中，最受欢迎的 Python 库之一是 pandas。Apache Spark（及其 Python 伴侣 PySpark）也是最受欢迎的分布式数据科学工具之一。它支持
    Python 和 JVM 语言。Spark 最初的 DataFrame 尝试更接近 SQL，而不是您可能认为的 DataFrame。虽然 Spark 已开始与
    [Koalas 项目](https://oreil.ly/VmU6O) 集成 pandas 支持，但我们认为 Dask 对数据科学库 API 的支持是最佳的。^([3](ch01.xhtml#id310))
    除了 pandas API，Dask 还支持 NumPy、scikit-learn 和其他数据科学工具的扩展。
- en: Note
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Dask can be extended to support data types besides NumPy and pandas, and this
    is how GPU support is implemented with [cuDF](https://oreil.ly/m-K8W).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 可以扩展以支持除了 NumPy 和 pandas 之外的数据类型，这正是如何通过 [cuDF](https://oreil.ly/m-K8W)
    实现 GPU 支持的。
- en: Parallel to Distributed Python
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 并行到分布式 Python
- en: '*Parallel computing* refers to running multiple operations at the same time,
    and *distributed computing* carries this on to multiple operations on multiple
    machines. Parallel Python encompasses a wide variety of tools ranging from multi-processing
    to Celery.^([4](ch01.xhtml#id315)) Dask gives you the ability to specify an arbitrary
    graph of dependencies and execute them in parallel. Under the hood, this execution
    can either be backed by a single machine (with threads or processes) or be distributed
    across multiple workers.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*并行计算* 指同时运行多个操作，*分布式计算* 将此扩展到多个机器上的多个操作。并行 Python 涵盖了从多进程到 Celery 等各种工具。^([4](ch01.xhtml#id315))
    Dask 允许您指定一个任意的依赖图，并并行执行它们。在内部，这种执行可以由单台机器（使用线程或进程）支持，也可以分布在多个工作节点上。'
- en: Note
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Many big data tools have similar low-level task APIs, but they are internal
    and are not exposed for our use or protected against failures.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 许多大数据工具具有类似的低级任务 API，但这些 API 是内部的，不会向我们公开使用，也没有受到故障保护。
- en: Dask Community Libraries
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dask 社区库
- en: Dask’s true power comes from the ecosystem built around it. Different libraries
    are built on top of Dask, giving you the ability to use multiple tools in the
    same framework. These community libraries are so powerful in part because of the
    combination of low-level and high-level APIs that are available for more than
    just first-party development.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 的真正力量来自于围绕它构建的生态系统。不同的库建立在 Dask 之上，使您能够在同一框架中使用多个工具。这些社区库之所以如此强大，部分原因在于低级和高级
    API 的结合，这些 API 不仅适用于第一方开发。
- en: Accelerated Python
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加速 Python
- en: You can accelerate Python in a few different ways, ranging from code generation
    (such as Numba) to libraries for special hardware such as NVidia’s CUDA (and wrappers
    like cuDF), AMD’s ROCm, and Intel’s MKL.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过几种不同的方式加速 Python，从代码生成（如 Numba）到针对特殊硬件的库，如 NVidia 的 CUDA（以及 cuDF 类似的包装器）、AMD
    的 ROCm 和 Intel 的 MKL。
- en: Dask itself is not a library for accelerated Python, but you can use it in conjunction
    with accelerated Python tools. For ease of use, some community projects integrate
    acceleration tools, such as cuDF and dask-cuda, with Dask. When using accelerated
    Python tools with Dask, you’ll need to be careful to structure your code to avoid
    serialization errors (see [“Serialization and Pickling”](ch03.xhtml#ser_pick_dtl)).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 本身并不是加速 Python 的库，但您可以与加速 Python 工具一起使用它。为了方便使用，一些社区项目将加速工具（如 cuDF 和 dask-cuda）与
    Dask 集成。当与 Dask 一起使用加速 Python 工具时，您需要小心地构造代码，以避免序列化错误（参见 [“序列化和 Pickling”](ch03.xhtml#ser_pick_dtl)）。
- en: Note
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Accelerated Python libraries tend to use more “native” memory structures, which
    are not as easily handled by pickle.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 加速 Python 库通常使用更“本地”的内存结构，这些结构不容易通过 pickle 处理。
- en: SQL engines
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SQL 引擎
- en: Dask itself does not have a SQL engine; however, [FugueSQL](https://oreil.ly/sBLQM),
    [Dask-SQL](https://oreil.ly/ZMVD1), and [BlazingSQL](https://oreil.ly/4gHru) use
    Dask to provide a distributed SQL engine.^([5](ch01.xhtml#id323)) Dask-SQL uses
    the popular Apache Calcite project, which powers many other SQL engines. BlazingSQL
    extends Dask DataFrames to support GPU operations. cuDF DataFrames have a slightly
    different representation. Apache Arrow makes it straightforward to convert a Dask
    DataFrame to cuDF and vice versa.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 本身没有 SQL 引擎；但是，[FugueSQL](https://oreil.ly/sBLQM)，[Dask-SQL](https://oreil.ly/ZMVD1)，和
    [BlazingSQL](https://oreil.ly/4gHru) 使用 Dask 提供分布式 SQL 引擎。^([5](ch01.xhtml#id323))
    Dask-SQL 使用流行的 Apache Calcite 项目，该项目支持许多其他 SQL 引擎。BlazingSQL 扩展了 Dask DataFrames
    以支持 GPU 操作。cuDF DataFrames 具有略有不同的表示形式。Apache Arrow 使得将 Dask DataFrame 转换为 cuDF
    及其相反变得简单直接。
- en: Dask allows these different SQL engines to scale both memory- and compute-wise,
    handling larger data sizes than fit in memory on a single computer and processing
    rows on multiple computers. Dask also powers the important aggregation step of
    combining the results from the different machines into a cohesive view of the
    data.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 允许这些不同的 SQL 引擎在内存和计算方面进行扩展，处理比单台计算机内存能容纳的更大数据量，并在多台计算机上处理行。Dask 还负责重要的聚合步骤，将不同机器的结果组合成数据的一致视图。
- en: Tip
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Dask-SQL can read data from parts of the Hadoop ecosystem that Dask cannot read
    from (e.g., Hive).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Dask-SQL 可以从 Dask 无法读取的 Hadoop 生态系统的部分读取数据（例如 Hive）。
- en: Workflow scheduling
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 工作流调度
- en: Most organizations have the need for some kind of scheduled work, from programs
    that run at specific times (such as those that calculate end-of-day or end-of-month
    financials) to programs that run in response to events. These events can be things
    like data becoming available (such as after the daily financials are run) or a
    new email coming in, or they can be user triggered. In the simplest case the scheduled
    work can be a single program, but it is often more complex than that.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数组织都需要某种形式的定期工作，从在特定时间运行的程序（例如计算每日或月末财务数据的程序）到响应事件运行的程序。这些事件可以是数据可用（例如每日财务数据运行后）或新邮件到达，或者可以是用户触发的。在最简单的情况下，定期工作可以是单个程序，但通常情况下比这更复杂。
- en: As mentioned previously, you can specify arbitrary graphs in Dask, and if you
    chose to, you could write your workflows using Dask itself. You can call system
    commands and parse their results, but just because you can do something doesn’t
    mean it will be fun or simple.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，您可以在 Dask 中指定任意图形，如果选择的话，可以使用 Dask 编写工作流程。您可以调用系统命令并解析其结果，但仅仅因为您可以做某事并不意味着它将是有趣或简单的。
- en: The household name^([6](ch01.xhtml#id327)) for workflow scheduling in the big
    data ecosystem is Apache Airflow. While Airflow has a wonderful collection of
    operators, making it easy to express complex task types easily, it is notoriously
    difficult to scale.^([7](ch01.xhtml#id328)) Dask can be used to run [Airflow tasks](https://oreil.ly/Vw54J).
    Alternatively, it can be used as a backend for other task scheduling systems like
    [Prefect](https://oreil.ly/9Xmvo). Prefect aims to bring Airflow-like functionality
    to Dask with a large predefined task library. Since Prefect used Dask as an execution
    backend from the start, it has a tighter integration and lower overhead than Airflow
    on Dask.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据生态系统中的工作流调度的家喻户晓的名字^([6](ch01.xhtml#id327)) 是 Apache Airflow。虽然 Airflow 拥有一套精彩的操作器集合，使得表达复杂任务类型变得容易，但它以难以扩展而著称。^([7](ch01.xhtml#id328))
    Dask 可以用于运行 [Airflow 任务](https://oreil.ly/Vw54J)。或者，它可以用作其他任务调度系统（如 [Prefect](https://oreil.ly/9Xmvo)）的后端。Prefect
    旨在将类似 Airflow 的功能带到 Dask，具有一个大型预定义的任务库。由于 Prefect 从开始就将 Dask 作为执行后端，因此它与 Dask
    的集成更紧密，开销更低。
- en: Note
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Few tools cover all of the same areas, with the most similar tool being Ray.
    Dask and Ray both expose Python APIs, with underlying extensions when needed.
    There is a [GitHub issue](https://oreil.ly/cPJpW) where the creators of both systems
    compare their similarities and differences. From a systems perspective, the biggest
    differences between Ray and Dask are handling state, fault tolerance, and centralized
    versus decentralized scheduling. Ray implements more of its logic in C++, which
    can have performance benefits but is also more difficult to read. From a user
    point of view, Dask has more of a data science focus, and Ray emphasizes distributed
    state and actor support. Dask can use Ray as a backend for scheduling.^([8](ch01.xhtml#id329))
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 少数工具涵盖了完全相同的领域，最相似的工具是 Ray。Dask 和 Ray 都暴露了 Python API，在需要时有底层扩展。有一个 [GitHub
    问题](https://oreil.ly/cPJpW)，其中两个系统的创作者比较了它们的相似之处和差异。从系统角度来看，Ray 和 Dask 之间的最大区别在于状态处理、容错性和集中式与分散式调度。Ray
    在 C++ 中实现了更多的逻辑，这可能会带来性能上的好处，但也更难阅读。从用户角度来看，Dask 更加注重数据科学，而 Ray 强调分布式状态和 actor
    支持。Dask 可以使用 Ray 作为调度的后端。
- en: What Dask Is Not
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dask 不是什么
- en: While Dask is many things, it is not a magic wand you wave over your code to
    make it faster. There are places where Dask has largely compatible drop-in APIs,
    but misusing them can result in slower execution. Dask is not a code rewriting
    or just-in-time (JIT) tool; instead, Dask allows you to scale these tools to run
    on clusters. Dask focuses on Python and may not be the right tool for scaling
    languages not tightly integrated with Python (such as Go). Dask does not have
    built-in catalog support (e.g., Hive or Iceberg), so reading and writing data
    from tables stored with the catalogs can pose a challenge.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Dask 是很多东西，但它不是你可以挥舞在代码上使其更快的魔术棒。Dask 在某些地方具有兼容的 API，但误用它们可能导致执行速度变慢。Dask
    不是代码重写或即时编译（JIT）工具；相反，Dask 允许你将这些工具扩展到集群上运行。Dask 着重于 Python，并且可能不适合与 Python 集成不紧密的语言（如
    Go）扩展。Dask 没有内置的目录支持（例如 Hive 或 Iceberg），因此从存储在目录中的表中读取和写入数据可能会带来挑战。
- en: Conclusion
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Dask is one of the possible options for scaling your analytical Python code.
    It covers various deployment options, from multiple cores on a single computer
    to data centers. Dask takes a modular approach compared to many other tools in
    similar spaces, which means that taking the time to understand the ecosystem and
    libraries around it is essential. The right choice to scale your software depends
    on your code and on the ecosystem, data consumers, and sources for your project.
    We hope we’ve convinced you that it’s worth the time to play with Dask a bit,
    which you do in the next chapter.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 是扩展你的分析 Python 代码的可能选项之一。它涵盖了从单台计算机上的多个核心到数据中心的各种部署选项。与许多类似领域的其他工具相比，Dask
    采用了模块化的方法，这意味着理解其周围的生态系统和库是至关重要的。选择正确的软件扩展取决于你的代码、生态系统、数据消费者以及项目的数据源。我们希望我们已经说服你，值得在下一章节中稍微尝试一下
    Dask。
- en: ^([1](ch01.xhtml#id288-marker)) Not *all* Python code, however; for example,
    Dask would be a bad choice for scaling a web server (very stateful from the web
    socket needs).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch01.xhtml#id288-marker)) 不是 *所有* Python 代码；例如，Dask 在扩展 Web 服务器（从 Web
    Socket 需求来看非常有状态）方面是一个不好的选择。
- en: ^([2](ch01.xhtml#id304-marker)) With the exception of non-uniform memory access
    (NUMA) systems.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch01.xhtml#id304-marker)) 除了非均匀内存访问（NUMA）系统。
- en: '^([3](ch01.xhtml#id310-marker)) Of course, opinions vary. See, for example,
    [“Single Node Processing — Spark, Dask, Pandas, Modin, Koalas Vol. 1”](https://oreil.ly/HBExc),
    [“Benchmark: Koalas (PySpark) and Dask”](https://oreil.ly/PNZPm), and [“Spark
    vs. Dask vs. Ray”](https://oreil.ly/eA28o).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch01.xhtml#id310-marker)) 当然，意见有所不同。例如，参见 [“单节点处理 — Spark、Dask、Pandas、Modin、Koalas
    Vol. 1”](https://oreil.ly/HBExc)，[“基准测试：Koalas（PySpark）和Dask”](https://oreil.ly/PNZPm)，以及
    [“Spark vs. Dask vs. Ray”](https://oreil.ly/eA28o)。
- en: ^([4](ch01.xhtml#id315-marker)) Celery, often used for background job management,
    is an asynchronous task queue that can also split up and distribute work. But
    it is at a lower level than Dask and does not have the same high-level conveniences
    as Dask.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch01.xhtml#id315-marker)) Celery，通常用于后台作业管理，是一个异步任务队列，也可以分割和分发工作。但它比 Dask
    低级，并且没有与 Dask 相同的高级便利性。
- en: ^([5](ch01.xhtml#id323-marker)) BlazingSQL is no longer maintained, though its
    concepts are interesting and may find life in another project.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch01.xhtml#id323-marker)) BlazingSQL 不再维护，尽管其概念很有趣，可能会在其他项目中找到用武之地。
- en: ^([6](ch01.xhtml#id327-marker)) Assuming a fairly nerdy household.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch01.xhtml#id327-marker)) 假设家庭比较书呆子。
- en: ^([7](ch01.xhtml#id328-marker)) With one thousand tasks per hour taking substantial
    tuning and manual consideration; see [“Scaling Airflow to 1000 Tasks/Hour”](https://oreil.ly/tVbSf).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch01.xhtml#id328-marker)) 每小时进行一千项任务，需要进行大量调整和手动考虑；参见[“将Airflow扩展到1000任务/小时”](https://oreil.ly/tVbSf)。
- en: ^([8](ch01.xhtml#id329-marker)) Or, flipping the perspective, Ray is capable
    of using Dask to provide data science functionality.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch01.xhtml#id329-marker)) 或者，换个角度看，Ray 能够利用 Dask 提供数据科学功能。
