- en: Chapter 9\. The multiprocessing Module
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CPython doesn’t use multiple CPUs by default. This is partly because Python
    was designed back in a single-core era, and partly because parallelizing can actually
    be quite difficult to do efficiently. Python gives us the tools to do it but leaves
    us to make our own choices. It is painful to see your multicore machine using
    just one CPU on a long-running process, though, so in this chapter we’ll review
    ways of using all the machine’s cores at once.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We just mentioned *CPython*—the common implementation that we all use. Nothing
    in the Python language stops it from using multicore systems. CPython’s implementation
    cannot efficiently use multiple cores, but future implementations may not be bound
    by this restriction.
  prefs: []
  type: TYPE_NORMAL
- en: We live in a multicore world—4 cores are common in laptops, and 32-core desktop
    configurations are available. If your job can be split to run on multiple CPUs
    *without* too much engineering effort, this is a wise direction to consider.
  prefs: []
  type: TYPE_NORMAL
- en: When Python is used to parallelize a problem over a set of CPUs, you can expect
    *up to* an *n*-times (*n*×) speedup with *n* cores. If you have a quad-core machine
    and you can use all four cores for your task, it might run in a quarter of the
    original runtime. You are unlikely to see a greater than 4× speedup; in practice,
    you’ll probably see gains of 3–4×.
  prefs: []
  type: TYPE_NORMAL
- en: Each additional process will increase the communication overhead and decrease
    the available RAM, so you rarely get a full *n*-times speedup. Depending on which
    problem you are solving, the communication overhead can even get so large that
    you can see very significant slowdowns. These sorts of problems are often where
    the complexity lies for any sort of parallel programming and normally require
    a change in algorithm. This is why parallel programming is often considered an
    art.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re not familiar with [Amdahl’s law](https://oreil.ly/GC2CK), it is worth
    doing some background reading. The law shows that if only a small part of your
    code can be parallelized, it doesn’t matter how many CPUs you throw at it; it
    still won’t run much faster overall. Even if a large fraction of your runtime
    could be parallelized, there’s a finite number of CPUs that can be used efficiently
    to make the overall process run faster before you get to a point of diminishing
    returns.
  prefs: []
  type: TYPE_NORMAL
- en: The `multiprocessing` module lets you use process- and thread-based parallel
    processing, share work over queues, and share data among processes. It is mostly
    focused on single-machine multicore parallelism (there are better options for
    multimachine parallelism). A very common use is to parallelize a task over a set
    of processes for a CPU-bound problem. You might also use OpenMP to parallelize
    an I/O-bound problem, but as we saw in [Chapter 8](ch08.xhtml#chapter-concurrency),
    there are better tools for this (e.g., the new `asyncio` module in Python 3 and
    `tornado`).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: OpenMP is a low-level interface to multiple cores—you might wonder whether to
    focus on it rather than `multiprocessing`. We introduced it with Cython back in
    [Chapter 7](ch07.xhtml#chapter-compiling), but we don’t cover it in this chapter.
    `multiprocessing` works at a higher level, sharing Python data structures, while
    OpenMP works with C primitive objects (e.g., integers and floats) once you’ve
    compiled to C. Using it makes sense only if you’re compiling your code; if you’re
    not compiling (e.g., if you’re using efficient `numpy` code and you want to run
    on many cores), then sticking with `multiprocessing` is probably the right approach.
  prefs: []
  type: TYPE_NORMAL
- en: To parallelize your task, you have to think a little differently from the normal
    way of writing a serial process. You must also accept that debugging a parallelized
    task is *harder*—often, it can be very frustrating. We’d recommend keeping the
    parallelism as simple as possible (even if you’re not squeezing every last drop
    of power from your machine) so that your development velocity is kept high.
  prefs: []
  type: TYPE_NORMAL
- en: One particularly difficult topic is the sharing of state in a parallel system—it
    feels like it should be easy, but it incurs lots of overhead and can be hard to
    get right. There are many use cases, each with different trade-offs, so there’s
    definitely no one solution for everyone. In [“Verifying Primes Using Interprocess
    Communication”](ch09_split_000.xhtml#multiprocessing-verifying-primes-using-inter-process-communication),
    we’ll go through state sharing with an eye on the synchronization costs. Avoiding
    shared state will make your life far easier.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, an algorithm can be analyzed to see how well it’ll perform in a parallel
    environment almost entirely by how much state must be shared. For example, if
    we can have multiple Python processes all solving the same problem without communicating
    with one another (a situation known as *embarrassingly parallel*), not much of
    a penalty will be incurred as we add more and more Python processes.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if each process needs to communicate with every other Python
    process, the communication overhead will slowly overwhelm the processing and slow
    things down. This means that as we add more and more Python processes, we can
    actually slow down our overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, sometimes some counterintuitive algorithmic changes must be made
    to efficiently solve a problem in parallel. For example, when solving the diffusion
    equation ([Chapter 6](ch06_split_000.xhtml#matrix_computation)) in parallel, each
    process actually does some redundant work that another process also does. This
    redundancy reduces the amount of communication required and speeds up the overall
    calculation!
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some typical jobs for the `multiprocessing` module:'
  prefs: []
  type: TYPE_NORMAL
- en: Parallelize a CPU-bound task with `Process` or `Pool` objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallelize an I/O-bound task in a `Pool` with threads using the (oddly named)
    `dummy` module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Share pickled work via a `Queue`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Share state between parallelized workers, including bytes, primitive datatypes,
    dictionaries, and lists
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you come from a language where threads are used for CPU-bound tasks (e.g.,
    C++ or Java), you should know that while threads in Python are OS-native (they’re
    not simulated—they are actual operating system threads), they are bound by the
    GIL, so only one thread may interact with Python objects at a time.
  prefs: []
  type: TYPE_NORMAL
- en: By using processes, we run a number of Python interpreters in parallel, each
    with a private memory space with its own GIL, and each runs in series (so there’s
    no competition for each GIL). This is the easiest way to speed up a CPU-bound
    task in Python. If we need to share state, we need to add some communication overhead;
    we’ll explore that in [“Verifying Primes Using Interprocess Communication”](ch09_split_000.xhtml#multiprocessing-verifying-primes-using-inter-process-communication).
  prefs: []
  type: TYPE_NORMAL
- en: If you work with `numpy` arrays, you might wonder if you can create a larger
    array (e.g., a large 2D matrix) and ask processes to work on segments of the array
    in parallel. You can, but it is hard to discover how by trial and error, so in
    [“Sharing numpy Data with multiprocessing”](ch09_split_001.xhtml#multiprocessing-sharing-numpy-data-with-multiprocessing)
    we’ll work through sharing a 25 GB `numpy` array across four CPUs. Rather than
    sending partial copies of the data (which would at least double the working size
    required in RAM and create a massive communication overhead), we share the underlying
    bytes of the array among the processes. This is an ideal approach to sharing a
    large array among local workers on one machine.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we also introduce the [Joblib](https://oreil.ly/RqQXD) library—this
    builds on the `multiprocessing` library and offers improved cross-platform compatibility,
    a simple API for parallelization, and convenient persistence of cached results.
    Joblib is designed for scientific use, and we urge you to check it out.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here, we discuss `multiprocessing` on *nix-based machines (this chapter is written
    using Ubuntu; the code should run unchanged on a Mac). Since Python 3.4, the quirks
    that appeared on Windows have been dealt with. Joblib has stronger cross-platform
    support than `multiprocessing`, and we recommend you review it ahead of `multiprocessing`.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we’ll hardcode the number of processes (`NUM_PROCESSES=4`) to
    match the four physical cores on Ian’s laptop. By default, `multiprocessing` will
    use as many cores as it can see (the machine presents eight—four CPUs and four
    hyperthreads). Normally you’d avoid hardcoding the number of processes to create
    unless you were specifically managing your resources.
  prefs: []
  type: TYPE_NORMAL
- en: An Overview of the multiprocessing Module
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `multiprocessing` module provides a low-level interface to process- and
    thread-based parallelism. Its main components are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Process`'
  prefs: []
  type: TYPE_NORMAL
- en: A forked copy of the current process; this creates a new process identifier,
    and the task runs as an independent child process in the operating system. You
    can start and query the state of the `Process` and provide it with a `target`
    method to run.
  prefs: []
  type: TYPE_NORMAL
- en: '`Pool`'
  prefs: []
  type: TYPE_NORMAL
- en: Wraps the `Process` or `threading.Thread` API into a convenient pool of workers
    that share a chunk of work and return an aggregated result.
  prefs: []
  type: TYPE_NORMAL
- en: '`Queue`'
  prefs: []
  type: TYPE_NORMAL
- en: A FIFO queue allowing multiple producers and consumers.
  prefs: []
  type: TYPE_NORMAL
- en: '`Pipe`'
  prefs: []
  type: TYPE_NORMAL
- en: A uni- or bidirectional communication channel between two processes.
  prefs: []
  type: TYPE_NORMAL
- en: '`Manager`'
  prefs: []
  type: TYPE_NORMAL
- en: A high-level managed interface to share Python objects between processes.
  prefs: []
  type: TYPE_NORMAL
- en: '`ctypes`'
  prefs: []
  type: TYPE_NORMAL
- en: Allows sharing of primitive datatypes (e.g., integers, floats, and bytes) between
    processes after they have forked.
  prefs: []
  type: TYPE_NORMAL
- en: Synchronization primitives
  prefs: []
  type: TYPE_NORMAL
- en: Locks and semaphores to synchronize control flow between processes.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In Python 3.2, the `concurrent.futures` module was introduced (via [PEP 3148](http://bit.ly/concurrent_add));
    this provides the core behavior of `multiprocessing`, with a simpler interface
    based on Java’s `java.util.concurrent`. It is available as a [backport to earlier
    versions of Python](https://oreil.ly/G9e5e). We expect `multiprocessing` to continue
    to be preferred for CPU-intensive work and won’t be surprised if `concurrent.futures`
    becomes more popular for I/O-bound tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In the rest of the chapter, we’ll introduce a set of examples to demonstrate
    common ways of using the `multiprocessing` module.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll estimate pi using a Monte Carlo approach with a `Pool` of processes or
    threads, using normal Python and `numpy`. This is a simple problem with well-understood
    complexity, so it parallelizes easily; we can also see an unexpected result from
    using threads with `numpy`. Next, we’ll search for primes using the same `Pool`
    approach; we’ll investigate the nonpredictable complexity of searching for primes
    and look at how we can efficiently (and inefficiently!) split the workload to
    best use our computing resources. We’ll finish the primes search by switching
    to queues, where we introduce `Process` objects in place of a `Pool` and use a
    list of work and poison pills to control the lifetime of workers.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll tackle interprocess communication (IPC) to validate a small set
    of possible primes. By splitting each number’s workload across multiple CPUs,
    we use IPC to end the search early if a factor is found so that we can significantly
    beat the speed of a single-CPU search process. We’ll cover shared Python objects,
    OS primitives, and a Redis server to investigate the complexity and capability
    trade-offs of each approach.
  prefs: []
  type: TYPE_NORMAL
- en: We can share a 25 GB `numpy` array across four CPUs to split a large workload
    *without* copying data. If you have large arrays with parallelizable operations,
    this technique should buy you a great speedup, since you have to allocate less
    space in RAM and copy less data. Finally, we’ll look at synchronizing access to
    a file and a variable (as a `Value`) between processes without corrupting data
    to illustrate how to correctly lock shared state.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: PyPy (discussed in [Chapter 7](ch07.xhtml#chapter-compiling)) has full support
    for the `multiprocessing` library, and the following CPython examples (though
    not the `numpy` examples, at the time of this writing) all run far quicker using
    PyPy. If you’re using only CPython code (no C extensions or more complex libraries)
    for parallel processing, PyPy might be a quick win for you.
  prefs: []
  type: TYPE_NORMAL
- en: Estimating Pi Using the Monte Carlo Method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can estimate pi by throwing thousands of imaginary darts into a “dartboard”
    represented by a unit circle. The relationship between the number of darts falling
    inside the circle’s edge and the number falling outside it will allow us to approximate
    pi.
  prefs: []
  type: TYPE_NORMAL
- en: This is an ideal first problem, as we can split the total workload evenly across
    a number of processes, each one running on a separate CPU. Each process will end
    at the same time since the workload for each is equal, so we can investigate the
    speedups available as we add new CPUs and hyperthreads to the problem.
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 9-1](ch09_split_000.xhtml#FIG-pi_monte_carlo_estimate), we throw
    10,000 darts into the unit square, and a percentage of them fall into the quarter
    of the unit circle that’s drawn. This estimate is rather bad—10,000 dart throws
    does not reliably give us a three-decimal-place result. If you ran your own code,
    you’d see this estimate vary between 3.0 and 3.2 on each run.
  prefs: []
  type: TYPE_NORMAL
- en: To be confident of the first three decimal places, we need to generate 10,000,000
    random dart throws.^([1](ch09_split_001.xhtml#idm46122410678856)) This is massively
    inefficient (and better methods for pi’s estimation exist), but it is rather convenient
    to demonstrate the benefits of parallelization using `multiprocessing`.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the Monte Carlo method, we use the [Pythagorean theorem](https://oreil.ly/toFkX)
    to test if a dart has landed inside our circle:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign x squared plus y squared less-than-or-equal-to 1
    squared equals 1 dollar-sign"><mrow><mrow><msup><mi>x</mi> <mn>2</mn></msup> <mo>+</mo>
    <msup><mi>y</mi> <mn>2</mn></msup></mrow> <mo>≤</mo> <msup><mn>1</mn> <mn>2</mn></msup>
    <mo>=</mo> <mn>1</mn></mrow></math>![Estimating Pi using the Monte Carlo method](Images/hpp2_0901.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9-1\. Estimating pi using the Monte Carlo method
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We’ll look at a loop version of this in [Example 9-1](ch09_split_000.xhtml#code-pi-lists-calculation).
    We’ll implement both a normal Python version and, later, a `numpy` version, and
    we’ll use both threads and processes to parallelize the problem.
  prefs: []
  type: TYPE_NORMAL
- en: Estimating Pi Using Processes and Threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is easier to understand a normal Python implementation, so we’ll start with
    that in this section, using float objects in a loop. We’ll parallelize this using
    processes to use all of our available CPUs, and we’ll visualize the state of the
    machine as we use more CPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Using Python Objects
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Python implementation is easy to follow, but it carries an overhead, as
    each Python float object has to be managed, referenced, and synchronized in turn.
    This overhead slows down our runtime, but it has bought us thinking time, as the
    implementation was quick to put together. By parallelizing this version, we get
    additional speedups for very little extra work.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 9-2](ch09_split_000.xhtml#FIG-pi_monte_carlo_threads_and_processes_lists)
    shows three implementations of the Python example:'
  prefs: []
  type: TYPE_NORMAL
- en: No use of `multiprocessing` (named “Serial”)—one `for` loop in the main process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using threads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![not set](Images/hpp2_0902.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-2\. Working in series, with threads, and with processes
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When we use more than one thread or process, we’re asking Python to calculate
    the same total number of dart throws and to divide the work evenly between workers.
    If we want 100,000,000 dart throws in total using our Python implementation and
    we use two workers, we’ll be asking both threads or both processes to generate
    50,000,000 dart throws per worker.
  prefs: []
  type: TYPE_NORMAL
- en: Using one thread takes approximately 71 seconds, with no speedup when using
    more threads. By using two or more processes, we make the runtime *shorter*. The
    cost of using no processes or threads (the series implementation) is the same
    as running with one process.
  prefs: []
  type: TYPE_NORMAL
- en: By using processes, we get a linear speedup when using two or four cores on
    Ian’s laptop. For the eight-worker case, we’re using Intel’s Hyper-Threading Technology—the
    laptop has only four physical cores, so we get barely any change in speedup by
    running eight processes.
  prefs: []
  type: TYPE_NORMAL
- en: '[Example 9-1](ch09_split_000.xhtml#code-pi-lists-calculation) shows the Python
    version of our pi estimator. If we’re using threads, each instruction is bound
    by the GIL, so although each thread could run on a separate CPU, it will execute
    only when no other threads are running. The process version is not bound by this
    restriction, as each forked process has a private Python interpreter running as
    a single thread—there’s no GIL contention, as no objects are shared. We use Python’s
    built-in random number generator, but see [“Random Numbers in Parallel Systems”](ch09_split_000.xhtml#multiprocessing-random-numbers)
    for some notes about the dangers of parallelized random number sequences.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-1\. Estimating pi using a loop in Python
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[Example 9-2](ch09_split_000.xhtml#code-pi-lists-main) shows the `__main__`
    block. Note that we build the `Pool` before we start the timer. Spawning threads
    is relatively instant; spawning processes involves a fork, and this takes a measurable
    fraction of a second. We ignore this overhead in [Figure 9-2](ch09_split_000.xhtml#FIG-pi_monte_carlo_threads_and_processes_lists),
    as this cost will be a tiny fraction of the overall execution time.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-2\. main for estimating pi using a loop
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We create a list containing `nbr_estimates` divided by the number of workers.
    This new argument will be sent to each worker. After execution, we’ll receive
    the same number of results back; we’ll sum these to estimate the number of darts
    in the unit circle.
  prefs: []
  type: TYPE_NORMAL
- en: We import the process-based `Pool` from `multiprocessing`. We also could have
    used `from multiprocessing.dummy import Pool` to get a threaded version. The “dummy”
    name is rather misleading (we confess to not understanding why it is named this
    way); it is simply a light wrapper around the `threading` module to present the
    same interface as the process-based `Pool`.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Each process we create consumes some RAM from the system. You can expect a forked
    process using the standard libraries to take on the order of 10–20 MB of RAM;
    if you’re using many libraries and lots of data, you might expect each forked
    copy to take hundreds of megabytes. On a system with a RAM constraint, this might
    be a significant issue—if you run out of RAM and the system reverts to using the
    disk’s swap space, any parallelization advantage will be massively lost to the
    slow paging of RAM back and forth to disk!
  prefs: []
  type: TYPE_NORMAL
- en: The following figures plot the average CPU utilization of Ian’s laptop’s four
    physical cores and their four associated hyperthreads (each hyperthread runs on
    unutilized silicon in a physical core). The data gathered for these figures *includes*
    the startup time of the first Python process and the cost of starting subprocesses.
    The CPU sampler records the entire state of the laptop, not just the CPU time
    used by this task.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the following diagrams are created using a different timing method
    with a slower sampling rate than [Figure 9-2](ch09_split_000.xhtml#FIG-pi_monte_carlo_threads_and_processes_lists),
    so the overall runtime is a little longer.
  prefs: []
  type: TYPE_NORMAL
- en: The execution behavior in [Figure 9-3](ch09_split_000.xhtml#FIG-pi_monte_carlo_lists_1_processes)
    with one process in the `Pool` (along with the parent process) shows some overhead
    in the first seconds as the `Pool` is created, and then a consistent close-to-100%
    CPU utilization throughout the run. With one process, we’re efficiently using
    one core.
  prefs: []
  type: TYPE_NORMAL
- en: '![Estimating pi using lists and 1 process](Images/hpp2_0903.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-3\. Estimating pi using Python objects and one process
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Next we’ll add a second process, effectively saying `Pool(processes=2)`. As
    you can see in [Figure 9-4](ch09_split_000.xhtml#FIG-pi_monte_carlo_lists_2_processes),
    adding a second process roughly halves the execution time to 37 seconds, and two
    CPUs are fully occupied. This is the best result we can expect—we’ve efficiently
    used all the new computing resources, and we’re not losing any speed to other
    overheads like communication, paging to disk, or contention with competing processes
    that want to use the same CPUs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Estimating Pi using lists and 2 processes](Images/hpp2_0904.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-4\. Estimating pi using Python objects and two processes
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Figure 9-5](ch09_split_000.xhtml#FIG-pi_monte_carlo_lists_4_processes) shows
    the results when using all four physical CPUs—now we are using all of the raw
    power of this laptop. Execution time is roughly a quarter that of the single-process
    version, at 19 seconds.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Estimating Pi using lists and 4 processes](Images/hpp2_0905.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-5\. Estimating pi using Python objects and four processes
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: By switching to eight processes, as seen in [Figure 9-6](ch09_split_000.xhtml#FIG-pi_monte_carlo_lists_8_processes),
    we cannot achieve more than a tiny speedup compared to the four-process version.
    That is because the four hyperthreads are able to squeeze only a little extra
    processing power out of the spare silicon on the CPUs, and the four CPUs are already
    maximally utilized.
  prefs: []
  type: TYPE_NORMAL
- en: '![Estimating Pi using lists and 8 processes](Images/hpp2_0906.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-6\. Estimating pi using Python objects and eight processes, with little
    additional gain
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: These diagrams show that we’re efficiently using more of the available CPU resources
    at each step, and that the hyperthread resources are a poor addition. The biggest
    problem when using hyperthreads is that CPython is using a lot of RAM—hyperthreading
    is not cache friendly, so the spare resources on each chip are very poorly utilized.
    As we’ll see in the next section, `numpy` makes better use of these resources.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In our experience, hyperthreading can give up to a 30% performance gain *if*
    there are enough spare computing resources. This works if, for example, you have
    a mix of floating-point and integer arithmetic rather than just the floating-point
    operations we have here. By mixing the resource requirements, the hyperthreads
    can schedule more of the CPU’s silicon to be working concurrently. Generally,
    we see hyperthreads as an added bonus and not a resource to be optimized against,
    as adding more CPUs is probably more economical than tuning your code (which adds
    a support overhead).
  prefs: []
  type: TYPE_NORMAL
- en: Now we’ll switch to using threads in one process, rather than multiple processes.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 9-7](ch09_split_000.xhtml#FIG-pi_monte_carlo_lists_4_threads) shows
    the results of running the same code that we used in [Figure 9-5](ch09_split_000.xhtml#FIG-pi_monte_carlo_lists_4_processes),
    but with threads in place of processes. Although a number of CPUs are being used,
    they each share the workload lightly. If each thread was running without the GIL,
    then we’d see 100% CPU utilization on the four CPUs. Instead, each CPU is partially
    utilized (because of the GIL).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Estimating Pi using lists and 4 threads](Images/hpp2_0907.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-7\. Estimating pi using Python objects and four threads
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Replacing multiprocessing with Joblib
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Joblib is an improvement on `multiprocessing` that enables lightweight pipelining
    with a focus on easy parallel computing and transparent disk-based caching of
    results. It focuses on NumPy arrays for scientific computing. It may offer a quick
    win for you if you’re
  prefs: []
  type: TYPE_NORMAL
- en: Using pure Python, with or without NumPy, to process a loop that could be embarrassingly
    parallel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calling expensive functions that have no side effects, where the output could
    be cached to disk between sessions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Able to share NumPy data between processes but don’t know how (and you haven’t
    yet read [“Sharing numpy Data with multiprocessing”](ch09_split_001.xhtml#multiprocessing-sharing-numpy-data-with-multiprocessing))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Joblib builds on the Loky library (itself an improvement over Python’s `concurrent.futures`)
    and uses `cloudpickle` to enable the pickling of functions defined in the interactive
    scope. This solves a couple of common issues that are encountered with the built-in
    `multiprocessing` library.
  prefs: []
  type: TYPE_NORMAL
- en: For parallel computing, we need the `Parallel` class and the `delayed` decorator.
    The `Parallel` class sets up a process pool, similar to the `multiprocessing`
    `pool` we used in the previous section. The `delayed` decorator wraps our target
    function so it can be applied to the instantiated `Parallel` object via an iterator.
  prefs: []
  type: TYPE_NORMAL
- en: The syntax is a little confusing to read—take a look at [Example 9-3](ch09_split_000.xhtml#code-pi-joblib).
    The call is written on one line; this includes our target function `estimate_nbr_points_in_quarter_circle`
    and the iterator `(delayed(...)(nbr_samples_per_worker) for sample_idx in range(nbr_parallel_blocks))`.
    Let’s break this down.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-3\. Using Joblib to parallelize pi estimation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '`Parallel` is a class; we can set parameters such as `n_jobs` to dictate how
    many processes will run, along with optional arguments like `verbose` for debugging
    information. Other arguments can set time-outs, change between threads or processes,
    change the backends (which can help speed up certain edge cases), and configure
    memory mapping.'
  prefs: []
  type: TYPE_NORMAL
- en: '`Parallel` has a `__call__` callable method that takes an iterable. We supply
    the iterable in the following round brackets `(... for sample_idx in range(...))`.
    The callable iterates over each `delayed(estimate_nbr_points_in_quarter_circle)`
    function, batching the execution of these functions to their arguments (in this
    case, `nbr_samples_per_worker`). Ian has found it helpful to build up a parallelized
    call one step at a time, starting from a function with no arguments and building
    up arguments as needed. This makes diagnosing missteps much easier.'
  prefs: []
  type: TYPE_NORMAL
- en: '`nbr_in_quarter_unit_circles` will be a list containing the count of positive
    cases for each call as before. [Example 9-4](ch09_split_000.xhtml#code-pi-joblib-output)
    shows the console output for eight parallel blocks; each process ID (PID) is freshly
    created, and a summary is printed in a progress bar at the end of the output.
    In total this takes 19 seconds, the same amount of time as when we created our
    own `Pool` in the previous section.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Avoid passing large structures; passing large pickled objects to each process
    may be expensive. Ian had a case with a prebuilt cache of Pandas DataFrames in
    a dictionary object; the cost of serializing these via the `Pickle` module negated
    the gains from parallelization, and the serial version actually worked faster
    overall. The solution in this case was to build the DataFrame cache using Python’s
    built-in [`shelve` module](https://oreil.ly/e9dJs), storing the dictionary to
    a file. A single DataFrame was loaded with `shelve` on each call to the target
    function; hardly anything had to be passed to the functions, and then the parallelized
    benefit of `Joblib` was clear.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-4\. Output of `Joblib` calls
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To simplify debugging, we can set `n_jobs=1`, and the parallelized code is dropped.
    You don’t have to modify your code any further, and you can drop a call to `breakpoint()`
    in your function to ease your debugging.
  prefs: []
  type: TYPE_NORMAL
- en: Intelligent caching of function call results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A useful feature in Joblib is the `Memory` cache; this is a decorator that caches
    function results based on the input arguments to a disk cache. This cache persists
    between Python sessions, so if you turn off your machine and then run the same
    code the next day, the cached results will be used.
  prefs: []
  type: TYPE_NORMAL
- en: For our pi estimation, this presents a small problem. We don’t pass in unique
    arguments to `estimate_nbr_points_in_quarter_circle`; for each call we pass in
    `nbr_estimates`, so the call signature is the same, but we’re after different
    results.
  prefs: []
  type: TYPE_NORMAL
- en: In this situation, once the first call has completed (taking around 19 seconds),
    any subsequent call with the same argument will get the cached result. This means
    that if we rerun our code a second time, it completes instantly, but it uses only
    one of the eight sample results as the result for each call—this obviously breaks
    our Monte Carlo sampling! If the last process to complete resulted in `9815738`
    points in the quarter circle, the cache for the function call would always answer
    this. Repeating the call eight times would generate `[9815738, 9815738, 9815738,
    9815738, 9815738, 9815738, 9815738, 9815738]` rather than eight unique estimates.
  prefs: []
  type: TYPE_NORMAL
- en: The solution in [Example 9-5](ch09_split_000.xhtml#code-pi-joblib-cache) is
    to pass in a second argument, `idx`, which takes on a value between 0 and `nbr_parallel_blocks-1`.
    This unique combination of arguments will let the cache store each positive count,
    so that on the second run we get the same result as on the first run, but without
    the wait.
  prefs: []
  type: TYPE_NORMAL
- en: This is configured using `Memory`, which takes a folder for persisting the function
    results. This persistence is kept between Python sessions; it is refreshed if
    you change the function that is being called, or if you empty the files in the
    cache folder.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this refresh applies only to a change to the function that’s been
    decorated (in this case, `estimate_nbr_points_in_quarter_circle_with_idx`), not
    to any sub-functions that are called from inside that function.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-5\. Caching results with Joblib
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In [Example 9-6](ch09_split_000.xhtml#code-pi-joblib-cache-output), we can see
    that while the first call costs 19 seconds, the second call takes only a fraction
    of a second and has the same estimated pi. In this run, the estimates were `[9817605,
    9821064, 9818420, 9817571, 9817688, 9819788, 9816377, 9816478]`.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-6\. The zero-cost second call to the code thanks to cached results
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Joblib wraps up a lot of `multiprocessing` functionality with a simple (if slightly
    hard to read) interface. Ian has moved to using Joblib in favor of `multiprocessing`;
    he recommends you try it too.
  prefs: []
  type: TYPE_NORMAL
- en: Random Numbers in Parallel Systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generating good random number sequences is a hard problem, and it is easy to
    get it wrong if you try to do it yourself. Getting a good sequence quickly in
    parallel is even harder—suddenly you have to worry about whether you’ll get repeating
    or correlated sequences in the parallel processes.
  prefs: []
  type: TYPE_NORMAL
- en: We used Python’s built-in random number generator in [Example 9-1](ch09_split_000.xhtml#code-pi-lists-calculation),
    and we’ll use the `numpy` random number generator in [Example 9-7](ch09_split_000.xhtml#code-pi-numpy-calculation)
    in the next section. In both cases, the random number generators are seeded in
    their forked process. For the Python `random` example, the seeding is handled
    internally by `multiprocessing`—if during a fork it sees that `random` is in the
    namespace, it will force a call to seed the generators in each of the new processes.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Set the numpy seed when parallelizing your function calls. In the forthcoming
    `numpy` example, we have to explicitly set the random number seed. If you forget
    to seed the random number sequence with `numpy`, each of your forked processes
    will generate an identical sequence of random numbers—it’ll appear to be working
    as you wanted it to, but behind the scenes each parallel process will evolve with
    identical results!
  prefs: []
  type: TYPE_NORMAL
- en: If you care about the quality of the random numbers used in the parallel processes,
    we urge you to research this topic. *Probably* the `numpy` and Python random number
    generators are good enough, but if significant outcomes depend on the quality
    of the random sequences (e.g., for medical or financial systems), then you must
    read up on this area.
  prefs: []
  type: TYPE_NORMAL
- en: In Python 3, the [Mersenne Twister algorithm](https://oreil.ly/yNINO) is used—it
    has a long period, so the sequence won’t repeat for a long time. It is heavily
    tested, as it is used in other languages, and it is thread-safe. It is probably
    not suitable for cryptographic purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Using numpy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we switch to using `numpy`. Our dart-throwing problem is ideal
    for `numpy` vectorized operations—we generate the same estimate 25 times faster
    than the previous Python examples.
  prefs: []
  type: TYPE_NORMAL
- en: The main reason that `numpy` is faster than pure Python when solving the same
    problem is that `numpy` is creating and manipulating the same object types at
    a very low level in contiguous blocks of RAM, rather than creating many higher-level
    Python objects that each require individual management and addressing.
  prefs: []
  type: TYPE_NORMAL
- en: As `numpy` is far more cache friendly, we’ll also get a small speed boost when
    using the four hyperthreads. We didn’t get this in the pure Python version, as
    caches aren’t used efficiently by larger Python objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [Figure 9-8](ch09_split_000.xhtml#FIG-pi_monte_carlo_threads_and_processes_numpy),
    we see three scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: No use of `multiprocessing` (named “Serial”)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using threads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The serial and single-worker versions execute at the same speed—there’s no overhead
    to using threads with `numpy` (and with only one worker, there’s also no gain).
  prefs: []
  type: TYPE_NORMAL
- en: When using multiple processes, we see a classic 100% utilization of each additional
    CPU. The result mirrors the plots shown in Figures [9-3](ch09_split_000.xhtml#FIG-pi_monte_carlo_lists_1_processes),
    [9-4](ch09_split_000.xhtml#FIG-pi_monte_carlo_lists_2_processes), [9-5](ch09_split_000.xhtml#FIG-pi_monte_carlo_lists_4_processes),
    and [9-6](ch09_split_000.xhtml#FIG-pi_monte_carlo_lists_8_processes), but the
    code is much faster using `numpy`.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, the threaded version runs *faster* with more threads. As discussed
    on the [SciPy wiki](https://oreil.ly/XXKNo), by working outside the GIL, `numpy`
    can achieve some level of additional speedup around threads.
  prefs: []
  type: TYPE_NORMAL
- en: '![Working in series, with threads, and with processes](Images/hpp2_0908.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-8\. Working in series, with threads, and with processes using numpy
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Using processes gives us a predictable speedup, just as it did in the pure Python
    example. A second CPU doubles the speed, and using four CPUs quadruples the speed.
  prefs: []
  type: TYPE_NORMAL
- en: '[Example 9-7](ch09_split_000.xhtml#code-pi-numpy-calculation) shows the vectorized
    form of our code. Note that the random number generator is seeded when this function
    is called. For the threaded version, this isn’t necessary, as each thread shares
    the same random number generator and they access it in series. For the process
    version, as each new process is a fork, all the forked versions will share the
    *same state*. This means the random number calls in each will return the same
    sequence!'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Remember to call `seed()` per process with `numpy` to ensure that each of the
    forked processes generates a unique sequence of random numbers, as a random source
    is used to set the seed for each call. Look back at [“Random Numbers in Parallel
    Systems”](ch09_split_000.xhtml#multiprocessing-random-numbers) for some notes
    about the dangers of parallelized random number sequences.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-7\. Estimating pi using `numpy`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: A short code analysis shows that the calls to `random` run a little slower on
    this machine when executed with multiple threads, and the call to `(xs * xs +
    ys * ys) <= 1` parallelizes well. Calls to the random number generator are GIL-bound,
    as the internal state variable is a Python object.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process to understand this was basic but reliable:'
  prefs: []
  type: TYPE_NORMAL
- en: Comment out all of the `numpy` lines, and run with *no* threads using the serial
    version. Run several times and record the execution times using `time.time()`
    in `__main__`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a line back (we added `xs = np.random.uniform(...)` first) and run several
    times, again recording completion times.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add the next line back (now adding `ys = ...`), run again, and record completion
    time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat, including the `nbr_trials_in_quarter_unit_circle = np.sum(...)` line.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat this process again, but this time with four threads. Repeat line by line.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare the difference in runtime at each step for no threads and four threads.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Because we’re running code in parallel, it becomes harder to use tools like
    `line_profiler` or `cProfile`. Recording the raw runtimes and observing the differences
    in behavior with different configurations takes patience but gives solid evidence
    from which to draw conclusions.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you want to understand the serial behavior of the `uniform` call, take a
    look at the [`mtrand` code](https://oreil.ly/HxHQD) in the `numpy` source and
    follow the call to `def uniform` in *mtrand.pyx*. This is a useful exercise if
    you haven’t looked at the `numpy` source code before.
  prefs: []
  type: TYPE_NORMAL
- en: The libraries used when building `numpy` are important for some of the parallelization
    opportunities. Depending on the underlying libraries used when building `numpy`
    (e.g., whether Intel’s Math Kernel Library or OpenBLAS were included or not),
    you’ll see different speedup behavior.
  prefs: []
  type: TYPE_NORMAL
- en: You can check your `numpy` configuration using `numpy.show_config()`. Stack
    Overflow has some [example timings](http://bit.ly/BLAS_benchmarking) if you’re
    curious about the possibilities. Only some `numpy` calls will benefit from parallelization
    by external libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Finding Prime Numbers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, we’ll look at testing for prime numbers over a large number range. This
    is a different problem from estimating pi, as the workload varies depending on
    your location in the number range, and each individual number’s check has an unpredictable
    complexity. We can create a serial routine that checks for primality and then
    pass sets of possible factors to each process for checking. This problem is embarrassingly
    parallel, which means there is no state that needs to be shared.
  prefs: []
  type: TYPE_NORMAL
- en: The `multiprocessing` module makes it easy to control the workload, so we shall
    investigate how we can tune the work queue to use (and misuse!) our computing
    resources, and we will explore an easy way to use our resources slightly more
    efficiently. This means we’ll be looking at *load balancing* to try to efficiently
    distribute our varying-complexity tasks to our fixed set of resources.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll use an algorithm that is slightly removed from the one earlier in the
    book (see [“Idealized Computing Versus the Python Virtual Machine”](ch01_split_000.xhtml#understanding-performance-idealized-computing));
    it exits early if we have an even number—see [Example 9-8](ch09_split_000.xhtml#code-primes-serial-generation-calculation).
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-8\. Finding prime numbers using Python
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: How much variety in the workload do we see when testing for a prime with this
    approach? [Figure 9-9](ch09_split_000.xhtml#FIG-primes-cost-to-check-primality)
    shows the increasing time cost to check for primality as the possibly prime `n`
    increases from `10,000` to `1,000,000`.
  prefs: []
  type: TYPE_NORMAL
- en: Most numbers are nonprime; they’re drawn with a dot. Some can be cheap to check
    for, while others require the checking of many factors. Primes are drawn with
    an `x` and form the thick darker band; they’re the most expensive to check for.
    The time cost of checking a number increases as `n` increases, as the range of
    possible factors to check increases with the square root of `n`. The sequence
    of primes is not predictable, so we can’t determine the expected cost of a range
    of numbers (we could estimate it, but we can’t be sure of its complexity).
  prefs: []
  type: TYPE_NORMAL
- en: For the figure, we test each `n` two hundred times and take the fastest result
    to remove jitter from the results. If we took only one result, we’d see wide variance
    in timing that would be caused by system load from other processes; by taking
    many readings and keeping the fastest, we get to see the expected best-case timing.
  prefs: []
  type: TYPE_NORMAL
- en: '![Time to check primality](Images/hpp2_0909.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-9\. Time required to check primality as `n` increases
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When we distribute work to a `Pool` of processes, we can specify how much work
    is passed to each worker. We could divide all of the work evenly and aim for one
    pass, or we could make many chunks of work and pass them out whenever a CPU is
    free. This is controlled using the `chunksize` parameter. Larger chunks of work
    mean less communication overhead, while smaller chunks of work mean more control
    over how resources are allocated.
  prefs: []
  type: TYPE_NORMAL
- en: For our prime finder, a single piece of work is a number `n` that is checked
    by `check_prime`. A `chunksize` of `10` would mean that each process handles a
    list of 10 integers, one list at a time.
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 9-10](ch09_split_000.xhtml#FIG-primes-pool-plot-chunksizetimes-type2),
    we can see the effect of varying the `chunksize` from `1` (every job is a single
    piece of work) to `64` (every job is a list of 64 numbers). Although having many
    tiny jobs gives us the greatest flexibility, it also imposes the greatest communication
    overhead. All four CPUs will be utilized efficiently, but the communication pipe
    will become a bottleneck as each job and result is passed through this single
    channel. If we double the `chunksize` to `2`, our task gets solved twice as quickly,
    as we have less contention on the communication pipe. We might naively assume
    that by increasing the `chunksize`, we will continue to improve the execution
    time. However, as you can see in the figure, we will again come to a point of
    diminishing returns.
  prefs: []
  type: TYPE_NORMAL
- en: '![notset](Images/hpp2_0910.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-10\. Choosing a sensible `chunksize` value
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can continue to increase the `chunksize` until we start to see a worsening
    of behavior. In [Figure 9-11](ch09_split_000.xhtml#FIG-primes-pool-plot-chunksizetimes-type1),
    we expand the range of chunksizes, making them not just tiny but also huge. At
    the larger end of the scale, the worst result shown is 1.08 seconds, where we’ve
    asked for `chunksize` to be `50000`—this means our 100,000 items are divided into
    two work chunks, leaving two CPUs idle for that entire pass. With a `chunksize`
    of `10000` items, we are creating ten chunks of work; this means that four chunks
    of work will run twice in parallel, followed by the two remaining chunks. This
    leaves two CPUs idle in the third round of work, which is an inefficient usage
    of resources.
  prefs: []
  type: TYPE_NORMAL
- en: An optimal solution in this case is to divide the total number of jobs by the
    number of CPUs. This is the default behavior in `multiprocessing`, shown as the
    “default” blue dot in the figure.
  prefs: []
  type: TYPE_NORMAL
- en: As a general rule, the default behavior is sensible; tune it only if you expect
    to see a real gain, and definitely confirm your hypothesis against the default
    behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the Monte Carlo pi problem, our prime testing calculation has varying
    complexity—sometimes a job exits quickly (an even number is detected the fastest),
    and sometimes the number is large and a prime (this takes a much longer time to
    check).
  prefs: []
  type: TYPE_NORMAL
- en: '![notset](Images/hpp2_0911.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-11\. Choosing a sensible `chunksize` value (continued)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: What happens if we randomize our job sequence? For this problem, we squeeze
    out a 2% performance gain, as you can see in [Figure 9-12](ch09_split_000.xhtml#FIG-primes-pool-plot-chunksizetimes-type1-shuffled).
    By randomizing, we reduce the likelihood of the final job in the sequence taking
    longer than the others, leaving all but one CPU active.
  prefs: []
  type: TYPE_NORMAL
- en: 'As our earlier example using a `chunksize` of `10000` demonstrated, misaligning
    the workload with the number of available resources leads to inefficiency. In
    that case, we created three rounds of work: the first two rounds used 100% of
    the resources, and the last round used only 50%.'
  prefs: []
  type: TYPE_NORMAL
- en: '![notset](Images/hpp2_0912.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-12\. Randomizing the job sequence
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Figure 9-13](ch09_split_000.xhtml#FIG-primes-pool-plot-chunksizetimes-by-nbrchunks-sawtoothpattern)
    shows the odd effect that occurs when we misalign the number of chunks of work
    against the number of processors. Mismatches will underutilize the available resources.
    The slowest overall runtime occurs when only one chunk of work is created: this
    leaves three unutilized. Two work chunks leave two CPUs unutilized, and so on;
    only when we have four work chunks are we using all of our resources. But if we
    add a fifth work chunk, we’re underutilizing our resources again—four CPUs will
    work on their chunks, and then one CPU will run to calculate the fifth chunk.'
  prefs: []
  type: TYPE_NORMAL
- en: As we increase the number of chunks of work, we see that the inefficiencies
    decrease—the difference in runtime between 29 and 32 work chunks is approximately
    0.03 seconds. The general rule is to make lots of small jobs for efficient resource
    utilization if your jobs have varying runtimes.
  prefs: []
  type: TYPE_NORMAL
- en: '![notset](Images/hpp2_0913.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-13\. The danger of choosing an inappropriate number of chunks
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here are some strategies for efficiently using `multiprocessing` for embarrassingly
    parallel problems:'
  prefs: []
  type: TYPE_NORMAL
- en: Split your jobs into independent units of work.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your workers take varying amounts of time, consider randomizing the sequence
    of work (another example would be for processing variable-sized files).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sorting your work queue so that the slowest jobs go first may be an equally
    useful strategy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the default `chunksize` unless you have verified reasons for adjusting it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Align the number of jobs with the number of physical CPUs. (Again, the default
    `chunksize` takes care of this for you, although it will use any hyperthreads
    by default, which may not offer any additional gain.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that by default `multiprocessing` will see hyperthreads as additional CPUs.
    This means that on Ian’s laptop, it will allocate eight processes when only four
    will really be running at 100% speed. The additional four processes could be taking
    up valuable RAM while barely offering any additional speed gain.
  prefs: []
  type: TYPE_NORMAL
- en: With a `Pool`, we can split up a chunk of predefined work up front among the
    available CPUs. This is less helpful if we have dynamic workloads, though, and
    particularly if we have workloads that arrive over time. For this sort of workload,
    we might want to use a `Queue`, introduced in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Queues of Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`multiprocessing.Queue` objects give us nonpersistent queues that can send
    any pickleable Python objects between processes. They carry an overhead, as each
    object must be pickled to be sent and then unpickled in the consumer (along with
    some locking operations). In the following example, we’ll see that this cost is
    not negligible. However, if your workers are processing larger jobs, the communication
    overhead is probably acceptable.'
  prefs: []
  type: TYPE_NORMAL
- en: Working with the queues is fairly easy. In this example, we’ll check for primes
    by consuming a list of candidate numbers and posting confirmed primes back to
    a `definite_primes_queue`. We’ll run this with one, two, four, and eight processes
    and confirm that the latter three approaches all take longer than just running
    a single process that checks the same range.
  prefs: []
  type: TYPE_NORMAL
- en: A `Queue` gives us the ability to perform lots of interprocess communication
    using native Python objects. This can be useful if you’re passing around objects
    with lots of state. Since the `Queue` lacks persistence, though, you probably
    don’t want to use queues for jobs that might require robustness in the face of
    failure (e.g., if you lose power or a hard drive gets corrupted).
  prefs: []
  type: TYPE_NORMAL
- en: '[Example 9-9](ch09_split_000.xhtml#code-queues-of-work-fullwork-check-prime)
    shows the `check_prime` function. We’re already familiar with the basic primality
    test. We run in an infinite loop, blocking (waiting until work is available) on
    `possible_primes_queue.get()` to consume an item from the queue. Only one process
    can get an item at a time, as the `Queue` object takes care of synchronizing the
    accesses. If there’s no work in the queue, the `.get()` blocks until a task is
    available. When primes are found, they are `put` back on the `definite_primes_queue`
    for consumption by the parent process.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-9\. Using two queues for interprocess communication (IPC)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We define two flags: one is fed by the parent process as a poison pill to indicate
    that no more work is available, while the second is fed by the worker to confirm
    that it has seen the poison pill and has closed itself down. The first poison
    pill is also known as a [*sentinel*](https://oreil.ly/mfR2s), as it guarantees
    the termination of the processing loop.'
  prefs: []
  type: TYPE_NORMAL
- en: When dealing with queues of work and remote workers, it can be helpful to use
    flags like these to record that the poison pills were sent and to check that responses
    were sent from the children in a sensible time window, indicating that they are
    shutting down. We don’t handle that process here, but adding some timekeeping
    is a fairly simple addition to the code. The receipt of these flags can be logged
    or printed during debugging.
  prefs: []
  type: TYPE_NORMAL
- en: The `Queue` objects are created out of a `Manager` in [Example 9-10](ch09_split_000.xhtml#code-queues-of-work-fullwork-main1).
    We’ll use the familiar process of building a list of `Process` objects that each
    contain a forked process. The two queues are sent as arguments, and `multiprocessing`
    handles their synchronization. Having started the new processes, we hand a list
    of jobs to the `possible_primes_queue` and end with one poison pill per process.
    The jobs will be consumed in FIFO order, leaving the poison pills for last. In
    `check_prime` we use a blocking `.get()`, as the new processes will have to wait
    for work to appear in the queue. Since we use flags, we could add some work, deal
    with the results, and then iterate by adding more work, and signal the end of
    life of the workers by adding the poison pills later.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-10\. Building two queues for IPC
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: To consume the results, we start another infinite loop in [Example 9-11](ch09_split_000.xhtml#code-queues-of-work-fullwork-main2),
    using a blocking `.get()` on the `definite_primes_queue`. If the `finished-processing`
    flag is found, we take a count of the number of processes that have signaled their
    exit. If not, we have a new prime, and we add this to the `primes` list. We exit
    the infinite loop when all of our processes have signaled their exit.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-11\. Using two queues for IPC
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: There is quite an overhead to using a `Queue`, due to the pickling and synchronization.
    As you can see in [Figure 9-14](ch09_split_000.xhtml#FIG-queues-of-work-fullwork),
    using a `Queue`-less single-process solution is significantly faster than using
    two or more processes. The reason in this case is because our workload is very
    light—the communication cost dominates the overall time for this task. With `Queue`s,
    two processes complete this example a little faster than one process, while four
    and eight processes are both slower.
  prefs: []
  type: TYPE_NORMAL
- en: '![notset](Images/hpp2_0914.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-14\. Cost of using Queue objects
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If your task has a long completion time (at least a sizable fraction of a second)
    with a small amount of communication, a `Queue` approach might be the right answer.
    You will have to verify whether the communication cost makes this approach useful
    enough.
  prefs: []
  type: TYPE_NORMAL
- en: You might wonder what happens if we remove the redundant half of the job queue
    (all the even numbers—these are rejected very quickly in `check_prime`). Halving
    the size of the input queue halves our execution time in each case, but it still
    doesn’t beat the single-process non-`Queue` example! This helps to illustrate
    that the communication cost is the dominating factor in this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronously adding jobs to the Queue
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'By adding a `Thread` into the main process, we can feed jobs asynchronously
    into the `possible_primes_queue`. In [Example 9-12](ch09_split_000.xhtml#code-queues-of-work-fullwork-jobs-feeder-thread),
    we define a `feed_new_jobs` function: it performs the same job as the job setup
    routine that we had in `__main__` before, but it does it in a separate thread.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-12\. Asynchronous job-feeding function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Now, in [Example 9-13](ch09_split_000.xhtml#code-queues-of-work-fullwork-main),
    our `__main__` will set up the `Thread` using the `possible_primes_queue` and
    then move on to the result-collection phase *before* any work has been issued.
    The asynchronous job feeder could consume work from external sources (e.g., from
    a database or I/O-bound communication) while the `__main__` thread handles each
    processed result. This means that the input sequence and output sequence do not
    need to be created in advance; they can both be handled on the fly.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-13\. Using a thread to set up an asynchronous job feeder
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: If you want robust asynchronous systems, you should almost certainly look to
    using `asyncio` or an external library such as `tornado`. For a full discussion
    of these approaches, check out [Chapter 8](ch08.xhtml#chapter-concurrency). The
    examples we’ve looked at here will get you started, but pragmatically they are
    more useful for very simple systems and education than for production systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Be *very aware* that asynchronous systems require a special level of patience—you
    will end up tearing out your hair while you are debugging. We suggest the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Applying the “Keep It Simple, Stupid” principle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoiding asynchronous self-contained systems (like our example) if possible,
    as they will grow in complexity and quickly become hard to maintain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using mature libraries like `gevent` (described in the previous chapter) that
    give you tried-and-tested approaches to dealing with certain problem sets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Furthermore, we strongly suggest using an external queue system that gives you
    external visibility on the state of the queues (e.g., NSQ, discussed in [“NSQ
    for Robust Production Clustering”](ch10.xhtml#nsq); ZeroMQ; or Celery). This requires
    more thought but is likely to save you time because of increased debug efficiency
    and better system visibility for production systems.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Consider using a task graph for resilience. Data science tasks requiring long-running
    queues are frequently served well by specifying pipelines of work in acyclic graphs.
    Two strong libraries are [Airflow](https://airflow.apache.org) and [Luigi](https://oreil.ly/rBfGh).
    These are very frequently used in industrial settings and enable arbitrary task
    chaining, online monitoring, and flexible scaling.
  prefs: []
  type: TYPE_NORMAL
- en: Verifying Primes Using Interprocess Communication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Prime numbers are numbers that have no factor other than themselves and 1\.
    It stands to reason that the most common factor is 2 (every even number cannot
    be a prime). After that, the low prime numbers (e.g., 3, 5, 7) become common factors
    of larger nonprimes (e.g., 9, 15, and 21, respectively).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say that we are given a large number and are asked to verify if it is
    prime. We will probably have a large space of factors to search. [Figure 9-15](ch09_split_000.xhtml#FIG-verifying-primes-count-of-factors-of-nonprimes)
    shows the frequency of each factor for nonprimes up to 10,000,000\. Low factors
    are far more likely to occur than high factors, but there’s no predictable pattern.
  prefs: []
  type: TYPE_NORMAL
- en: '![notset](Images/hpp2_0915.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-15\. The frequency of factors of nonprimes
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let’s define a new problem—suppose we have a *small* set of numbers, and our
    task is to efficiently use our CPU resources to figure out if each number is a
    prime, one number at a time. Possibly we’ll have just one large number to test.
    It no longer makes sense to use one CPU to do the check; we want to coordinate
    the work across many CPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this section we’ll look at some larger numbers, one with 15 digits and
    four with 18 digits:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Small nonprime: 112,272,535,095,295'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Large nonprime 1: 100,109,100,129,100,369'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Large nonprime 2: 100,109,100,129,101,027'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prime 1: 100,109,100,129,100,151'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prime 2: 100,109,100,129,162,907'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By using a smaller nonprime and some larger nonprimes, we get to verify that
    our chosen process not only is faster at checking for primes but also is not getting
    slower at checking nonprimes. We’ll assume that we don’t know the size or type
    of numbers that we’re being given, so we want the fastest possible result for
    all our use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you own the previous edition of the book, you might be surprised to see that
    these runtimes with CPython 3.7 are *slightly slower* than the CPython 2.7 runtimes
    in the last edition, which ran on a slower laptop. The code here is one edge case
    where Python 3.*x* is currently slower than CPython 2.7\. This code depends on
    integer operations; CPython 2.7 had system integers mixed with “long” integers
    (which can store arbitrarily sized numbers but at a cost in speed). CPython 3.*x*
    uses only “long” integers for all operations. This implementation is optimized
    but is still slower in some cases compared to the old (and more complicated) implementation.
  prefs: []
  type: TYPE_NORMAL
- en: We never have to worry about which “kind” of integer is being used, and in CPython
    3.7 we take a small speed hit as a consequence. This is a microbenchmark that
    is incredibly unlikely to affect your own code, as CPython 3.*x* is faster than
    CPython 2.*x* in so many other ways. Our recommendation is not to worry about
    this, unless you depend on integer operations for most of your execution time—and
    in that case, we’d strongly suggest you look at PyPy, which doesn’t suffer from
    this slowdown.
  prefs: []
  type: TYPE_NORMAL
- en: Cooperation comes at a cost—the cost of synchronizing data and checking the
    shared data can be quite high. We’ll work through several approaches here that
    can be used in different ways for task coordination.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we’re *not* covering the somewhat specialized message passing interface
    (MPI) here; we’re looking at batteries-included modules and Redis (which is very
    common). If you want to use MPI, we assume you already know what you’re doing.
    The [MPI4PY project](http://bit.ly/MPI4PY_proj) would be a good place to start.
    It is an ideal technology if you want to control latency when lots of processes
    are collaborating, whether you have one or many machines.
  prefs: []
  type: TYPE_NORMAL
- en: For the following runs, each test is performed 20 times, and the minimum time
    is taken to show the fastest speed that is possible for that method. In these
    examples we’re using various techniques to share a flag (often as 1 byte). We
    could use a basic object like a `Lock`, but then we’d be able to share only 1
    bit of state. We’re choosing to show you how to share a primitive type so that
    more expressive state sharing is possible (even though we don’t need a more expressive
    state for this example).
  prefs: []
  type: TYPE_NORMAL
- en: We must emphasize that sharing state tends to make things *complicated*—you
    can easily end up in another hair-pulling state. Be careful and try to keep things
    as simple as they can be. It might be the case that less efficient resource usage
    is trumped by developer time spent on other challenges.
  prefs: []
  type: TYPE_NORMAL
- en: First we’ll discuss the results and then we’ll work through the code.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 9-16](ch09_split_001.xhtml#FIG-validating-primes-slower-results) shows
    the first approaches to trying to use interprocess communication to test for primality
    faster. The benchmark is the serial version, which does not use any interprocess
    communication; each attempt to speed up our code must at least be faster than
    this.'
  prefs: []
  type: TYPE_NORMAL
- en: '![notset](Images/hpp2_0916.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-16\. The slower ways to use IPC to validate primality
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The Less Naive Pool version has a predictable (and good) speed. It is good enough
    to be rather hard to beat. Don’t overlook the obvious in your search for high-speed
    solutions—sometimes a dumb and good-enough solution is all you need.
  prefs: []
  type: TYPE_NORMAL
- en: The approach for the Less Naive Pool solution is to take our number under test,
    divide its possible-factor range evenly among the available CPUs, and then push
    the work out to each CPU. If any CPU finds a factor, it will exit early, but it
    won’t communicate this fact; the other CPUs will continue to work through their
    part of the range. This means for an 18-digit number (our four larger examples),
    the search time is the same whether it is prime or nonprime.
  prefs: []
  type: TYPE_NORMAL
- en: The Redis and `Manager` solutions are slower when it comes to testing a larger
    number of factors for primality because of the communication overhead. They use
    a shared flag to indicate that a factor has been found and the search should be
    called off.
  prefs: []
  type: TYPE_NORMAL
- en: Redis lets you share state not just with other Python processes but also with
    other tools and other machines, and even to expose that state over a web-browser
    interface (which might be useful for remote monitoring). The `Manager` is a part
    of `multiprocessing`; it provides a high-level synchronized set of Python objects
    (including primitives, the `list`, and the `dict`).
  prefs: []
  type: TYPE_NORMAL
- en: For the larger nonprime cases, although there is a cost to checking the shared
    flag, this is dwarfed by the savings in search time gained by signaling early
    that a factor has been found.
  prefs: []
  type: TYPE_NORMAL
- en: For the prime cases, though, there is no way to exit early, as no factor will
    be found, so the cost of checking the shared flag will become the dominating cost.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A little bit of thought is often enough. Here we explore various IPC-based solutions
    to making the prime-validation task faster. In terms of “minutes of typing” versus
    “gains made,” the first step—introducing naive parallel processing—gave us the
    largest win for the smallest effort. Subsequent gains took a lot of extra experimentation.
    Always think about the ultimate run-time, especially for ad hoc tasks. Sometimes
    it is just easier to let a loop run all weekend for a one-off task than to optimize
    the code so it runs quicker.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 9-17](ch09_split_001.xhtml#FIG-validating-primes-faster-results) shows
    that we can get a considerably faster result with a bit of effort. The Less Naive
    Pool result is still our benchmark, but the `RawValue` and MMap (memory map) results
    are much faster than the previous Redis and `Manager` results. The real magic
    comes from taking the fastest solution and performing some less-obvious code manipulations
    to make a near-optimal MMap solution—this final version is faster than the Less
    Naive Pool solution for nonprimes and almost as fast for primes.'
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we’ll work through various ways of using IPC in Python
    to solve our cooperative search problem. We hope you’ll see that IPC is fairly
    easy but generally comes with a cost.
  prefs: []
  type: TYPE_NORMAL
- en: '![notset](Images/hpp2_0917.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-17\. The faster ways to use IPC to validate primality
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Serial Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ll start with the same serial factor-checking code that we used before, shown
    again in [Example 9-14](ch09_split_001.xhtml#code-verifying-primes-serial). As
    noted earlier, for any nonprime with a large factor, we could more efficiently
    search the space of factors in parallel. Still, a serial sweep will give us a
    sensible baseline to work from.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-14\. Serial verification
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Naive Pool Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Naive Pool solution works with a `multiprocessing.Pool`, similar to what
    we saw in [“Finding Prime Numbers”](ch09_split_000.xhtml#multiprocessing-finding-prime-numbers)
    and [“Estimating Pi Using Processes and Threads”](ch09_split_000.xhtml#multiprocessing-estimating-pi)
    with four forked processes. We have a number to test for primality, and we divide
    the range of possible factors into four tuples of subranges and send these into
    the `Pool`.
  prefs: []
  type: TYPE_NORMAL
- en: In [Example 9-15](ch09_split_001.xhtml#code-verifying-primes-pool1-1), we use
    a new method, `create_range.create` (which we won’t show—it’s quite boring), that
    splits the work space into equal-sized regions. Each item in `ranges_to_check`
    is a pair of lower and upper bounds to search between. For the first 18-digit
    nonprime (100,109,100,129,100,369), with four processes we’ll have the factor
    ranges `ranges_to_check == [(3, 79_100_057), (79_100_057, 158_200_111), (158_200_111,
    237_300_165), (237_300_165, 316_400_222)]` (where 316,400,222 is the square root
    of 100,109,100,129,100,369 plus 1). In `__main__` we first establish a `Pool`;
    `check_prime` then splits the `ranges_to_check` for each possibly prime number
    `n` via a `map`. If the result is `False`, we have found a factor and we do not
    have a prime.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-15\. Naive Pool solution
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We modify the previous `check_prime` in [Example 9-16](ch09_split_001.xhtml#code-verifying-primes-pool1-2)
    to take a lower and upper bound for the range to check. There’s no value in passing
    a complete list of possible factors to check, so we save time and memory by passing
    just two numbers that define our range.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-16\. `check_prime_in_range`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: For the “small nonprime” case, the verification time via the `Pool` is 0.1 seconds,
    a significantly longer time than the original 0.000002 seconds in the Serial solution.
    Despite this one worse result, the overall result is a speedup across the board.
    We could perhaps accept that one slower result isn’t a problem—but what if we
    might get lots of smaller nonprimes to check? It turns out we can avoid this slowdown;
    we’ll see that next with the Less Naive Pool solution.
  prefs: []
  type: TYPE_NORMAL
- en: A Less Naive Pool Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The previous solution was inefficient at validating the smaller nonprime. For
    any smaller (fewer than 18 digits) nonprime, it is likely to be slower than the
    serial method, because of the overhead of sending out partitioned work and not
    knowing if a very small factor (which is a more likely factor) will be found.
    If a small factor is found, the process will still have to wait for the other
    larger factor searches to complete.
  prefs: []
  type: TYPE_NORMAL
- en: We could start to signal between the processes that a small factor has been
    found, but since this happens so frequently, it will add a lot of communication
    overhead. The solution presented in [Example 9-17](ch09_split_001.xhtml#code-verifying-primes-pool2-1)
    is a more pragmatic approach—a serial check is performed quickly for likely small
    factors, and if none are found, then a parallel search is started. Combining a
    serial precheck before launching a relatively more expensive parallel operation
    is a common approach to avoiding some of the costs of parallel computing.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-17\. Improving the Naive Pool solution for the small-nonprime case
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The speed of this solution is equal to or better than that of the original serial
    search for each of our test numbers. This is our new benchmark.
  prefs: []
  type: TYPE_NORMAL
- en: Importantly, this `Pool` approach gives us an optimal case for the prime-checking
    situation. If we have a prime, there’s no way to exit early; we have to manually
    check all possible factors before we can exit.
  prefs: []
  type: TYPE_NORMAL
- en: 'There’s no faster way to check though these factors: any approach that adds
    complexity will have more instructions, so the check-all-factors case will cause
    the most instructions to be executed. See the various `mmap` solutions covered
    in [“Using mmap as a Flag”](ch09_split_001.xhtml#multiprocessing-mmap) for a discussion
    on how to get as close to this current result for primes as possible.'
  prefs: []
  type: TYPE_NORMAL
- en: Using Manager.Value as a Flag
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `multiprocessing.Manager()` lets us share higher-level Python objects between
    processes as managed shared objects; the lower-level objects are wrapped in proxy
    objects. The wrapping and safety have a speed cost but also offer great flexibility.
    You can share both lower-level objects (e.g., integers and floats) and lists and
    dictionaries.
  prefs: []
  type: TYPE_NORMAL
- en: In [Example 9-18](ch09_split_001.xhtml#code-verifying-primes-managervalue1),
    we create a `Manager` and then create a 1-byte (character) `manager.Value(b"c",
    FLAG_CLEAR)` flag. You could create any of the `ctypes` primitives (which are
    the same as the `array.array` primitives) if you wanted to share strings or numbers.
  prefs: []
  type: TYPE_NORMAL
- en: Note that `FLAG_CLEAR` and `FLAG_SET` are assigned a byte (`b'0'` and `b'1'`,
    respectively). We chose to use the leading `b` to be very explicit (it might default
    to a Unicode or string object if left as an implicit string, depending on your
    environment and Python version).
  prefs: []
  type: TYPE_NORMAL
- en: Now we can flag across all of our processes that a factor has been found, so
    the search can be called off early. The difficulty is balancing the cost of reading
    the flag against the speed savings that is possible. Because the flag is synchronized,
    we don’t want to check it too frequently—this adds more overhead.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-18\. Passing a `Manager.Value` object as a flag
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '`check_prime_in_range` will now be aware of the shared flag, and the routine
    will be checking to see if a prime has been spotted by another process. Even though
    we’ve yet to begin the parallel search, we must clear the flag as shown in [Example 9-19](ch09_split_001.xhtml#code-verifying-primes-managervalue2)
    before we start the serial check. Having completed the serial check, if we haven’t
    found a factor, we know that the flag must still be false.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-19\. Clearing the flag with a `Manager.Value`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: How frequently should we check the shared flag? Each check has a cost, both
    because we’re adding more instructions to our tight inner loop and because checking
    requires a lock to be made on the shared variable, which adds more cost. The solution
    we’ve chosen is to check the flag every one thousand iterations. Every time we
    check, we look to see if `value.value` has been set to `FLAG_SET`, and if so,
    we exit the search. If in the search the process finds a factor, then it sets
    `value.value = FLAG_SET` and exits (see [Example 9-20](ch09_split_001.xhtml#code-verifying-primes-managervalue3)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-20\. Passing a `Manager.Value` object as a flag
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The thousand-iteration check in this code is performed using a `check_every`
    local counter. It turns out that this approach, although readable, is suboptimal
    for speed. By the end of this section, we’ll replace it with a less readable but
    significantly faster approach.
  prefs: []
  type: TYPE_NORMAL
- en: You might be curious about the total number of times we check for the shared
    flag. In the case of the two large primes, with four processes we check for the
    flag 316,405 times (we check it this many times in all of the following examples).
    Since each check has an overhead due to locking, this cost really adds up.
  prefs: []
  type: TYPE_NORMAL
- en: Using Redis as a Flag
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Redis* is a key/value in-memory storage engine. It provides its own locking
    and each operation is atomic, so we don’t have to worry about using locks from
    inside Python (or from any other interfacing language).'
  prefs: []
  type: TYPE_NORMAL
- en: By using Redis, we make the data storage language-agnostic—any language or tool
    with an interface to Redis can share data in a compatible way. You could share
    data between Python, Ruby, C++, and PHP equally easily. You can share data on
    the local machine or over a network; to share to other machines, all you need
    to do is change the Redis default of sharing only on `localhost`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Redis lets you store the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Lists of strings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sets of strings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sorted sets of strings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hashes of strings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Redis stores everything in RAM and snapshots to disk (optionally using journaling)
    and supports master/slave replication to a cluster of instances. One possibility
    with Redis is to use it to share a workload across a cluster, where other machines
    read and write state and Redis acts as a fast centralized data repository.
  prefs: []
  type: TYPE_NORMAL
- en: We can read and write a flag as a text string (all values in Redis are strings)
    in just the same way as we have been using Python flags previously. We create
    a `StrictRedis` interface as a global object, which talks to the external Redis
    server. We could create a new connection inside `check_prime_in_range`, but this
    is slower and can exhaust the limited number of Redis handles that are available.
  prefs: []
  type: TYPE_NORMAL
- en: We talk to the Redis server using a dictionary-like access. We can set a value
    using `rds[SOME_KEY] = SOME_VALUE` and read the string back using `rds[SOME_KEY]`.
  prefs: []
  type: TYPE_NORMAL
- en: '[Example 9-21](ch09_split_001.xhtml#code-verifying-primes-redis1) is very similar
    to the previous `Manager` example—we’re using Redis as a substitute for the local
    `Manager`. It comes with a similar access cost. You should note that Redis supports
    other (more complex) data structures; it is a powerful storage engine that we’re
    using just to share a flag for this example. We encourage you to familiarize yourself
    with its features.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-21\. Using an external Redis server for our flag
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: To confirm that the data is stored outside these Python instances, we can invoke
    `redis-cli` at the command line, as in [Example 9-22](ch09_split_001.xhtml#code-08-redis-cli),
    and get the value stored in the key `redis_primes_flag`. You’ll note that the
    returned item is a string (not an integer). All values returned from Redis are
    strings, so if you want to manipulate them in Python, you’ll have to convert them
    to an appropriate datatype first.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-22\. `redis-cli`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: One powerful argument in favor of the use of Redis for data sharing is that
    it lives outside the Python world—non-Python developers on your team will understand
    it, and many tools exist for it. They’ll be able to look at its state while reading
    (but not necessarily running and debugging) your code and follow what’s happening.
    From a team-velocity perspective, this might be a big win for you, despite the
    communication overhead of using Redis. While Redis is an additional dependency
    on your project, you should note that it is a very commonly deployed tool, and
    one that is well debugged and well understood. Consider it a powerful tool to
    add to your armory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Redis has many configuration options. By default it uses a TCP interface (that’s
    what we’re using), although the benchmark documentation notes that sockets might
    be much faster. It also states that while TCP/IP lets you share data over a network
    between different types of OS, other configuration options are likely to be faster
    (but also are likely to limit your communication options):'
  prefs: []
  type: TYPE_NORMAL
- en: When the server and client benchmark programs run on the same box, both the
    TCP/IP loopback and unix domain sockets can be used. It depends on the platform,
    but unix domain sockets can achieve around 50% more throughput than the TCP/IP
    loopback (on Linux for instance). The default behavior of redis-benchmark is to
    use the TCP/IP loopback. The performance benefit of unix domain sockets compared
    to TCP/IP loopback tends to decrease when pipelining is heavily used (i.e., long
    pipelines).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Redis documentation](http://redis.io/topics/benchmarks)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Redis is widely used in industry and is mature and well trusted. If you’re not
    familiar with the tool, we strongly suggest you take a look at it; it has a place
    in your high performance toolkit.
  prefs: []
  type: TYPE_NORMAL
- en: Using RawValue as a Flag
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`multiprocessing.RawValue` is a thin wrapper around a `ctypes` block of bytes.
    It lacks synchronization primitives, so there’s little to get in our way in our
    search for the fastest way to set a flag between processes. It will be almost
    as fast as the following `mmap` example (it is slower only because a few more
    instructions get in the way).'
  prefs: []
  type: TYPE_NORMAL
- en: Again, we could use any `ctypes` primitive; there’s also a `RawArray` option
    for sharing an array of primitive objects (which will behave similarly to `array.array`).
    `RawValue` avoids any locking—it is faster to use, but you don’t get atomic operations.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, if you avoid the synchronization that Python provides during IPC,
    you’ll come unstuck (once again, back to that pulling-your-hair-out situation).
    *However*, in this problem it doesn’t matter if one or more processes set the
    flag at the same time—the flag gets switched in only one direction, and every
    other time it is read, it is just to learn if the search can be called off.
  prefs: []
  type: TYPE_NORMAL
- en: Because we never reset the state of the flag during the parallel search, we
    don’t need synchronization. Be aware that this may not apply to your problem.
    If you avoid synchronization, please make sure you are doing it for the right
    reasons.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to do things like update a shared counter, look at the documentation
    for the `Value` and use a context manager with `value.get_lock()`, as the implicit
    locking on a `Value` doesn’t allow for atomic operations.
  prefs: []
  type: TYPE_NORMAL
- en: This example looks very similar to the previous `Manager` example. The only
    difference is that in [Example 9-23](ch09_split_001.xhtml#code-verifying-primes-value1)
    we create the `RawValue` as a one-character (byte) flag.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-23\. Creating and passing a `RawValue`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The flexibility to use managed and raw values is a benefit of the clean design
    for data sharing in `multiprocessing`.
  prefs: []
  type: TYPE_NORMAL
- en: Using mmap as a Flag
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, we get to the fastest way of sharing bytes. [Example 9-24](ch09_split_001.xhtml#code-verifying-primes-mmap1)
    shows a memory-mapped (shared memory) solution using the `mmap` module. The bytes
    in a shared memory block are not synchronized, and they come with very little
    overhead. They act like a file—in this case, they are a block of memory with a
    file-like interface. We have to `seek` to a location and read or write sequentially.
    Typically, `mmap` is used to give a short (memory-mapped) view into a larger file,
    but in our case, rather than specifying a file number as the first argument, we
    instead pass `-1` to indicate that we want an anonymous block of memory. We could
    also specify whether we want read-only or write-only access (we want both, which
    is the default).
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-24\. Using a shared memory flag via `mmap`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '`mmap` supports a number of methods that can be used to move around in the
    file that it represents (including `find`, `readline`, and `write`). We are using
    it in the most basic way—we `seek` to the start of the memory block before each
    read or write, and since we’re sharing just 1 byte, we use `read_byte` and `write_byte`
    to be explicit.'
  prefs: []
  type: TYPE_NORMAL
- en: There is no Python overhead for locking and no interpretation of the data; we’re
    dealing with bytes directly with the operating system, so this is our fastest
    communication method.
  prefs: []
  type: TYPE_NORMAL
- en: Using mmap as a Flag Redux
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the previous `mmap` result was the best overall, we couldn’t help but
    think that we should be able to get back to the Naive Pool result for the most
    expensive case of having primes. The goal is to accept that there is no early
    exit from the inner loop and to minimize the cost of anything extraneous.
  prefs: []
  type: TYPE_NORMAL
- en: This section presents a slightly more complex solution. The same changes can
    be made to the other flag-based approaches we’ve seen, although this `mmap` result
    will still be fastest.
  prefs: []
  type: TYPE_NORMAL
- en: In our previous examples, we’ve used `CHECK_EVERY`. This means we have the `check_next`
    local variable to track, decrement, and use in Boolean tests—and each operation
    adds a bit of extra time to every iteration. In the case of validating a large
    prime, this extra management overhead occurs over 300,000 times.
  prefs: []
  type: TYPE_NORMAL
- en: The first optimization, shown in [Example 9-25](ch09_split_001.xhtml#code-verifying-primes-mmap2-1),
    is to realize that we can replace the decremented counter with a look-ahead value,
    and then we only have to do a Boolean comparison on the inner loop. This removes
    a decrement, which, because of Python’s interpreted style, is quite slow. This
    optimization works in this test in CPython 3.7, but it is unlikely to offer any
    benefit in a smarter compiler (e.g., PyPy or Cython). This step saved 0.1 seconds
    when checking one of our large primes.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-25\. Starting to optimize away our expensive logic
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: We can also entirely replace the logic that the counter represents, as shown
    in [Example 9-26](ch09_split_001.xhtml#code-verifying-primes-mmap3-1), by unrolling
    our loop into a two-stage process. First, the outer loop covers the expected range,
    but in steps, on `CHECK_EVERY`. Second, a new inner loop replaces the `check_every`
    logic—it checks the local range of factors and then finishes. This is equivalent
    to the `if not check_every:` test. We follow this with the previous `sh_mem` logic
    to check the early-exit flag.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-26\. Optimizing away our expensive logic
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The speed impact is dramatic. Our nonprime case improves even further, but more
    importantly, our prime-checking case is nearly as fast as the Less Naive Pool
    version (it is now just 0.1 seconds slower). Given that we’re doing a lot of extra
    work with interprocess communication, this is an interesting result. Do note,
    though, that it is specific to CPython and unlikely to offer any gains when run
    through a compiler.
  prefs: []
  type: TYPE_NORMAL
- en: In the last edition of the book we went even further with a final example that
    used loop unrolling and local references to global objects and eked out a further
    performance gain at the expense of readability. This example in Python 3 yields
    a minor slowdown, so we’ve removed it. We’re happy about this—fewer hoops needed
    jumping through to get the most performant example, and the preceding code is
    more likely to be supported correctly in a team than one that makes implementation-specific
    code changes.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: These examples work just fine with PyPy, where they run around seven times faster
    than in CPython. Sometimes the better solution will be to investigate other runtimes
    rather than to go down rabbit holes with CPython.
  prefs: []
  type: TYPE_NORMAL
- en: Sharing numpy Data with multiprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When working with large `numpy` arrays, you’re bound to wonder if you can share
    the data for read and write access, without a copy, between processes. It is possible,
    though a little fiddly. We’d like to acknowledge Stack Overflow user *pv* for
    the inspiration for this demo.^([2](ch09_split_001.xhtml#idm46122406714360))
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Do not use this method to re-create the behaviors of BLAS, MKL, Accelerate,
    and ATLAS. These libraries all have multithreading support in their primitives,
    and they likely are better-debugged than any new routine that you create. They
    can require some configuration to enable multithreading support, but it would
    be wise to see if these libraries can give you free speedups before you invest
    time (and lose time to debugging!) writing your own.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sharing a large matrix between processes has several benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: Only one copy means no wasted RAM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No time is wasted copying large blocks of RAM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You gain the possibility of sharing partial results between the processes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thinking back to the pi estimation demo using `numpy` in [“Using numpy”](ch09_split_000.xhtml#multiprocessing-estimating-pi-using-numpy),
    we had the problem that the random number generation was a serial process. Here,
    we can imagine forking processes that share one large array, each one using a
    differently seeded random number generator to fill in a section of the array with
    random numbers, and therefore completing the generation of a large random block
    faster than is possible with a single process.
  prefs: []
  type: TYPE_NORMAL
- en: To verify this, we modified the forthcoming demo to create a large random matrix
    (10,000 × 320,000 elements) as a serial process and by splitting the matrix into
    four segments where `random` is called in parallel (in both cases, one row at
    a time). The serial process took 53 seconds, and the parallel version took 29
    seconds. Refer back to [“Random Numbers in Parallel Systems”](ch09_split_000.xhtml#multiprocessing-random-numbers)
    to understand some of the dangers of parallelized random number generation.
  prefs: []
  type: TYPE_NORMAL
- en: For the rest of this section, we’ll use a simplified demo that illustrates the
    point while remaining easy to verify.
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 9-18](ch09_split_001.xhtml#FIG-numpy-shared-multiprocessing-htop),
    you can see the output from `htop` on Ian’s laptop. It shows four child processes
    of the parent (with PID 27628), where all five processes are sharing a single
    10,000-by-320,000-element `numpy` array of doubles. One copy of this array costs
    25.6 GB, and the laptop has only 32 GB—you can see in `htop` by the process meters
    that the `Mem` reading shows a maximum of 31.1 GB RAM.
  prefs: []
  type: TYPE_NORMAL
- en: '![notset](Images/hpp2_0918.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-18\. `htop` showing RAM and swap usage
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To understand this demo, we’ll first walk through the console output, and then
    we’ll look at the code. In [Example 9-27](ch09_split_001.xhtml#code-sharing-numpy-array-using-multiprocessing-run-setup),
    we start the parent process: it allocates a 25.6 GB double array of dimensions
    10,000 × 320,000, filled with the value zero. The 10,000 rows will be passed out
    as indices to the worker function, and the worker will operate on each column
    of 320,000 items in turn. Having allocated the array, we fill it with the answer
    to life, the universe, and everything (`42`!). We can test in the worker function
    that we’re receiving this modified array and not a filled-with-0s version to confirm
    that this code is behaving as expected.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-27\. Setting up the shared array
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In [Example 9-28](ch09_split_001.xhtml#code-sharing-numpy-array-using-multiprocessing-run-worker-fn),
    we’ve started four processes working on this shared array. No copy of the array
    was made; each process is looking at the same large block of memory, and each
    process has a different set of indices to work from. Every few thousand lines,
    the worker outputs the current index and its PID, so we can observe its behavior.
    The worker’s job is trivial—it will check that the current element is still set
    to the default (so we know that no other process has modified it already), and
    then it will overwrite this value with the current PID. Once the workers have
    completed, we return to the parent process and print the array again. This time,
    we see that it is filled with PIDs rather than `42`.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-28\. Running `worker_fn` on the shared array
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Finally, in [Example 9-29](ch09_split_001.xhtml#code-sharing-numpy-array-using-multiprocessing-run-verify)
    we use a `Counter` to confirm the frequency of each PID in the array. As the work
    was evenly divided, we expect to see each of the four PIDs represented an equal
    number of times. In our 3,200,000,000-element array, we see four sets of 800,000,000
    PIDs. The table output is presented using [PrettyTable](https://oreil.ly/tXL3a).
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-29\. Verifying the result on the shared array
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Having completed, the program now exits, and the array is deleted.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can take a peek inside each process under Linux by using `ps` and `pmap`.
    [Example 9-30](ch09_split_001.xhtml#code-sharing-numpy-array-using-multiprocessing-run-pmap)
    shows the result of calling `ps`. Breaking apart this command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ps` tells us about the process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-A` lists all processes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-o pid,size,vsize,cmd` outputs the PID, size information, and the command
    name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`grep` is used to filter all other results and leave only the lines for our
    demo.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The parent process (PID 27628) and its four forked children are shown in the
    output. The result is similar to what we saw in `htop`. We can use `pmap` to look
    at the memory map of each process, requesting extended output with `-x`. We `grep`
    for the pattern `s-` to list blocks of memory that are marked as being shared.
    In the parent process and the child processes, we see a 25,000,000 KB (25.6 GB)
    block that is shared between them.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-30\. Using `pmap` and `ps` to investigate the operating system’s view
    of the processes
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: We’ll use a `multprocessing.Array` to allocate a shared block of memory as a
    1D array and then instantiate a `numpy` array from this object and reshape it
    to a 2D array. Now we have a `numpy`-wrapped block of memory that can be shared
    between processes and addressed as though it were a normal `numpy` array. `numpy`
    is not managing the RAM; `multiprocessing.Array` is managing it.
  prefs: []
  type: TYPE_NORMAL
- en: In [Example 9-31](ch09_split_001.xhtml#code-sharing-numpy-array-using-multiprocessing-worker-fn),
    you can see that each forked process has access to a global `main_nparray`. While
    the forked process has a copy of the `numpy` object, the underlying bytes that
    the object accesses are stored as shared memory. Our `worker_fn` will overwrite
    a chosen row (via `idx`) with the current process identifier.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-31\. `worker_fn` for sharing `numpy` arrays using `multiprocessing`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'In our `__main__` in [Example 9-32](ch09_split_001.xhtml#code-sharing-numpy-array-using-multiprocessing-main1),
    we’ll work through three major stages:'
  prefs: []
  type: TYPE_NORMAL
- en: Build a shared `multiprocessing.Array` and convert it into a `numpy` array.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a default value into the array, and spawn four processes to work on the
    array in parallel.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Verify the array’s contents after the processes return.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Typically, you’d set up a `numpy` array and work on it in a single process,
    probably doing something like `arr = np.array((100, 5), dtype=np.float_)`. This
    is fine in a single process, but you can’t share this data across processes for
    both reading and writing.
  prefs: []
  type: TYPE_NORMAL
- en: The trick is to make a shared block of bytes. One way is to create a `multiprocessing.Array`.
    By default the `Array` is wrapped in a lock to prevent concurrent edits, but we
    don’t need this lock as we’ll be careful about our access patterns. To communicate
    this clearly to other team members, it is worth being explicit and setting `lock=False`.
  prefs: []
  type: TYPE_NORMAL
- en: If you don’t set `lock=False`, you’ll have an object rather than a reference
    to the bytes, and you’ll need to call `.get_obj()` to get to the bytes. By calling
    `.get_obj()`, you bypass the lock, so there’s no value in not being explicit about
    this in the first place.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we take this block of shareable bytes and wrap a `numpy` array around
    them using `frombuffer`. The `dtype` is optional, but since we’re passing bytes
    around, it is always sensible to be explicit. We `reshape` so we can address the
    bytes as a 2D array. By default the array values are set to `0`. [Example 9-32](ch09_split_001.xhtml#code-sharing-numpy-array-using-multiprocessing-main1)
    shows our `__main__` in full.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-32\. `__main__` to set up `numpy` arrays for sharing
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: To confirm that our processes are operating on the same block of data that we
    started with, we set each item to a new `DEFAULT_VALUE` (we again use `42`, the
    answer to life, the universe, and everything)—you’ll see that at the top of [Example 9-33](ch09_split_001.xhtml#code-sharing-numpy-array-using-multiprocessing-main2).
    Next, we build a `Pool` of processes (four in this case) and then send batches
    of row indices via the call to `map`.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-33\. `__main__` for sharing `numpy` arrays using `multiprocessing`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Once we’ve completed the parallel processing, we return to the parent process
    to verify the result ([Example 9-34](ch09_split_001.xhtml#code-sharing-numpy-array-using-multiprocessing-main3)).
    The verification step runs through a flattened view on the array (note that the
    view does *not* make a copy; it just creates a 1D iterable view on the 2D array),
    counting the frequency of each PID. Finally, we perform some `assert` checks to
    make sure we have the expected counts.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-34\. `__main__` to verify the shared result
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: We’ve just created a 1D array of bytes, converted it into a 2D array, shared
    the array among four processes, and allowed them to process concurrently on the
    same block of memory. This recipe will help you parallelize over many cores. Be
    careful with concurrent access to the *same* data points, though—you’ll have to
    use the locks in `multiprocessing` if you want to avoid synchronization problems,
    and this will slow down your code.
  prefs: []
  type: TYPE_NORMAL
- en: Synchronizing File and Variable Access
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the following examples, we’ll look at multiple processes sharing and manipulating
    a state—in this case, four processes incrementing a shared counter a set number
    of times. Without a synchronization process, the counting is incorrect. If you’re
    sharing data in a coherent way you’ll always need a method to synchronize the
    reading and writing of data, or you’ll end up with errors.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, the synchronization methods are specific to the OS you’re using,
    and they’re often specific to the language you use. Here, we look at file-based
    synchronization using a Python library and sharing an integer object between Python
    processes.
  prefs: []
  type: TYPE_NORMAL
- en: File Locking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reading and writing to a file will be the slowest example of data sharing in
    this section.
  prefs: []
  type: TYPE_NORMAL
- en: You can see our first `work` function in [Example 9-35](ch09_split_001.xhtml#code-08-syncronization-filelocking-nolock-1-work).
    The function iterates over a local counter. In each iteration it opens a file
    and reads the existing value, increments it by one, and then writes the new value
    over the old one. On the first iteration the file will be empty or won’t exist,
    so it will catch an exception and assume the value should be zero.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The examples given here are simplified—in practice it is safer to use a context
    manager to open a file using `with open(*filename*, "r") as f:`. If an exception
    is raised inside the context, the file `f` will correctly be closed.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-35\. `work` function without a lock
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Let’s run this example with one process. You can see the output in [Example 9-36](ch09_split_001.xhtml#code-08-syncronization-filelocking-nolock-1-console1).
    `work` is called one thousand times, and as expected it counts correctly without
    losing any data. On the first read, it sees an empty file. This raises the `invalid
    literal for int()` error for `int()` (as `int()` is called on an empty string).
    This error occurs only once; afterward, we always have a valid value to read and
    convert into an integer.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-36\. Timing of file-based counting without a lock and with one process
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Now we’ll run the same `work` function with four concurrent processes. We don’t
    have any locking code, so we’ll expect some odd results.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Before you look at the following code, what *two* types of error can you expect
    to see when two processes simultaneously read from or write to the same file?
    Think about the two main states of the code (the start of execution for each process
    and the normal running state of each process).
  prefs: []
  type: TYPE_NORMAL
- en: Take a look at [Example 9-37](ch09_split_001.xhtml#code-08-syncronization-filelocking-nolock-1-console2)
    to see the problems. First, when each process starts, the file is empty, so each
    tries to start counting from zero. Second, as one process writes, the other can
    read a partially written result that can’t be parsed. This causes an exception,
    and a zero will be written back. This, in turn, causes our counter to keep getting
    reset! Can you see how `\n` and two values have been written by two concurrent
    processes to the same open file, causing an invalid entry to be read by a third
    process?
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-37\. Timing of file-based counting without a lock and with four processes
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[Example 9-38](ch09_split_001.xhtml#code-08-syncronization-filelocking-nolock-1-main)
    shows the `multiprocessing` code that calls `work` with four processes. Note that
    rather than using a `map`, we’re building a list of `Process` objects. Although
    we don’t use the functionality here, the `Process` object gives us the power to
    introspect the state of each `Process`. We encourage you to [read the documentation](https://oreil.ly/B4_G7)
    to learn about why you might want to use a `Process`.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-38\. `run_workers` setting up four processes
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Using the [`fasteners` module](https://oreil.ly/n8ZlV), we can introduce a synchronization
    method so only one process gets to write at a time and the others each await their
    turn. The overall process therefore runs more slowly, but it doesn’t make mistakes.
    You can see the correct output in [Example 9-39](ch09_split_001.xhtml#code-08-syncronization-filelocking-lock-1-console).
    Be aware that the locking mechanism is specific to Python, so other processes
    that are looking at this file will *not* care about the “locked” nature of this
    file.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-39\. Timing of file-based counting with a lock and four processes
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Using `fasteners` adds a single line of code in [Example 9-40](ch09_split_001.xhtml#code-08-syncronization-filelocking-lock-1)
    with the `@fasteners.interprocess_locked` decorator; the filename can be anything,
    but using a similar name as the file you want to lock probably makes debugging
    from the command line easier. Note that we haven’t had to change the inner function;
    the decorator gets the lock on each call, and it will wait until it can get the
    lock before the call into `work` proceeds.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-40\. `work` function with a lock
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Locking a Value
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `multiprocessing` module offers several options for sharing Python objects
    between processes. We can share primitive objects with a low communication overhead,
    and we can also share higher-level Python objects (e.g., dictionaries and lists)
    using a `Manager` (but note that the synchronization cost will significantly slow
    down the data sharing).
  prefs: []
  type: TYPE_NORMAL
- en: Here, we’ll use a [`multiprocessing.Value` object](https://oreil.ly/nGKnY) to
    share an integer between processes. While a `Value` has a lock, the lock doesn’t
    do quite what you might expect—it prevents simultaneous reads or writes but does
    *not* provide an atomic increment. [Example 9-41](ch09_split_001.xhtml#code-08-syncronization-valuelocking-nolock-1-console)
    illustrates this. You can see that we end up with an incorrect count; this is
    similar to the file-based unsynchronized example we looked at earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-41\. No locking leads to an incorrect count
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: No corruption occurs to the data, but we do miss some of the updates. This approach
    might be suitable if you’re writing to a `Value` from one process and consuming
    (but not modifying) that `Value` in other processes.
  prefs: []
  type: TYPE_NORMAL
- en: The code to share the `Value` is shown in [Example 9-42](ch09_split_001.xhtml#code-08-syncronization-valuelocking-nolock-1).
    We have to specify a datatype and an initialization value—using `Value("i", 0)`,
    we request a signed integer with a default value of `0`. This is passed as a regular
    argument to our `Process` object, which takes care of sharing the same block of
    bytes between processes behind the scenes. To access the primitive object held
    by our `Value`, we use `.value`. Note that we’re asking for an in-place addition—we’d
    expect this to be an atomic operation, but that’s not supported by `Value`, so
    our final count is lower than expected.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-42\. The counting code without a `Lock`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: You can see the correctly synchronized count in [Example 9-43](ch09_split_001.xhtml#code-08-syncronization-valuelocking-lock-1-console)
    using a `multiprocessing.Lock`.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-43\. Using a `Lock` to synchronize writes to a `Value`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: In [Example 9-44](ch09_split_001.xhtml#code-08-syncronization-valuelocking-lock-1),
    we’ve used a context manager (`with Lock`) to acquire the lock.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-44\. Acquiring a `Lock` using a context manager
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: If we avoid the context manager and directly wrap our increment with `acquire`
    and `release`, we can go a little faster, but the code is less readable compared
    to using the context manager. We suggest sticking to the context manager to improve
    readability. The snippet in [Example 9-45](ch09_split_001.xhtml#code-08-syncronization-valuelocking-lock-1-inline-lock)
    shows how to `acquire` and `release` the `Lock` object.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-45\. Inline locking rather than using a context manager
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Since a `Lock` doesn’t give us the level of granularity that we’re after, the
    basic locking that it provides wastes a bit of time unnecessarily. We can replace
    the `Value` with a [`RawValue`](https://oreil.ly/MYjtB), as in [Example 9-46](ch09_split_001.xhtml#code-08-syncronization-valuelocking-lock-rawvalue-1-console),
    and achieve an incremental speedup. If you’re interested in seeing the bytecode
    behind this change, read [Eli Bendersky’s blog post](http://bit.ly/shared_counter)
    on the subject.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-46\. Console output showing the faster `RawValue` and `Lock` approach
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: To use a `RawValue`, just swap it for a `Value`, as shown in [Example 9-47](ch09_split_001.xhtml#code-08-syncronization-valuelocking-lock-rawvalue-1).
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-47\. Example of using a <code>RawValue</code> integer
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: We could also use a `RawArray` in place of a `multiprocessing.Array` if we were
    sharing an array of primitive objects.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve looked at various ways of dividing up work on a single machine between
    multiple processes, along with sharing a flag and synchronizing data sharing between
    these processes. Remember, though, that sharing data can lead to headaches—try
    to avoid it if possible. Making a machine deal with all the edge cases of state
    sharing is hard; the first time you have to debug the interactions of multiple
    processes, you’ll realize why the accepted wisdom is to avoid this situation if
    possible.
  prefs: []
  type: TYPE_NORMAL
- en: Do consider writing code that runs a bit slower but is more likely to be understood
    by your team. Using an external tool like Redis to share state leads to a system
    that can be inspected at runtime by people *other* than the developers—this is
    a powerful way to enable your team to keep on top of what’s happening in your
    parallel systems.
  prefs: []
  type: TYPE_NORMAL
- en: Definitely bear in mind that tweaked performant Python code is less likely to
    be understood by more junior members of your team—they’ll either be scared of
    it or break it. Avoid this problem (and accept a sacrifice in speed) to keep team
    velocity high.
  prefs: []
  type: TYPE_NORMAL
- en: Wrap-Up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ve covered a lot in this chapter. First, we looked at two embarrassingly
    parallel problems, one with predictable complexity and the other with nonpredictable
    complexity. We’ll use these examples again on multiple machines when we discuss
    clustering in [Chapter 10](ch10.xhtml#clustering).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we looked at `Queue` support in `multiprocessing` and its overheads. In
    general, we recommend using an external queue library so that the state of the
    queue is more transparent. Preferably, you should use an easy-to-read job format
    so that it is easy to debug, rather than pickled data.
  prefs: []
  type: TYPE_NORMAL
- en: The IPC discussion should have impressed upon you how difficult it is to use
    IPC efficiently, and that it can make sense just to use a naive parallel solution
    (without IPC). Buying a faster computer with more cores might be a far more pragmatic
    solution than trying to use IPC to exploit an existing machine.
  prefs: []
  type: TYPE_NORMAL
- en: Sharing `numpy` matrices in parallel without making copies is important for
    only a small set of problems, but when it counts, it’ll really count. It takes
    a few extra lines of code and requires some sanity checking to make sure that
    you’re really not copying the data between processes.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we looked at using file and memory locks to avoid corrupting data—this
    is a source of subtle and hard-to-track errors, and this section showed you some
    robust and lightweight solutions.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter we’ll look at clustering using Python. With a cluster, we
    can move beyond single-machine parallelism and utilize the CPUs on a group of
    machines. This introduces a new world of debugging pain—not only can your code
    have errors, but the other machines can also have errors (either from bad configuration
    or from failing hardware). We’ll show how to parallelize the pi estimation demo
    using the Parallel Python module and how to run research code inside IPython using
    an IPython cluster.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch09_split_000.xhtml#idm46122410678856-marker)) See [Brett Foster’s PowerPoint
    presentation](https://oreil.ly/DdIuv) on using the Monte Carlo method to estimate
    pi.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch09_split_001.xhtml#idm46122406714360-marker)) See the [Stack Overflow
    topic](http://bit.ly/Python_multiprocessing).
  prefs: []
  type: TYPE_NORMAL
