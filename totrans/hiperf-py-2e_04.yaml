- en: Chapter 4\. Dictionaries and Sets
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章：字典和集合
- en: 'Sets and dictionaries are ideal data structures to be used when your data has
    no intrinsic order (except for insertion order) but does have a unique object
    that can be used to reference it (the reference object is normally a string, but
    it can be any hashable type). This reference object is called the *key*, while
    the data is the *value*. Dictionaries and sets are almost identical, except that
    sets do not actually contain values: a set is simply a collection of unique keys.
    As the name implies, sets are very useful for doing set operations.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据没有内在顺序（除了插入顺序）但具有可用于引用数据的唯一对象时（引用对象通常是字符串，但可以是任何可散列类型），字典和集合是理想的数据结构。这个引用对象称为*键*，而数据是*值*。字典和集合几乎是相同的，唯一的区别在于集合不包含实际的值：集合只是唯一键的集合。正如其名称所示，集合非常适合执行集合操作。
- en: Note
  id: totrans-2
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: A *hashable* type is one that implements both the `__hash__` magic function
    and either `__eq__` or `__cmp__`. All native types in Python already implement
    these, and any user classes have default values. See [“Hash Functions and Entropy”](#SEC-dict-set-hash-and-entropy)
    for more details.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*可散列*类型是指同时实现了`__hash__`魔术函数和`__eq__`或`__cmp__`的类型。Python中所有原生类型已经实现了这些方法，任何用户类都有默认值。更多详情请参见[“哈希函数和熵”](#SEC-dict-set-hash-and-entropy)。'
- en: While we saw in the previous chapter that we are restricted to, at best, `O(log
    n)` lookup time on lists/tuples with no intrinsic order (through a search operation),
    dictionaries and sets give us `O(1)` lookups based on the arbitrary index. In
    addition, like lists/tuples, dictionaries and sets have `O(1)` insertion time.^([1](ch04.xhtml#idm46122426063384))
    As we will see in [“How Do Dictionaries and Sets Work?”](#dict_set_how_work),
    this speed is accomplished through the use of an open address hash table as the
    underlying data structure.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们看到在没有内在顺序的列表/元组（通过搜索操作）上，我们最多只能达到`O(log n)`的查找时间。然而，字典和集合基于任意索引可以实现`O(1)`的查找时间。此外，与列表/元组类似，字典和集合的插入时间也是`O(1)`。^[1](ch04.xhtml#idm46122426063384)
    正如我们将在[“字典和集合的工作原理”](#dict_set_how_work)中看到的，这种速度是通过使用开放地址哈希表作为底层数据结构实现的。
- en: However, there is a cost to using dictionaries and sets. First, they generally
    take up a larger footprint in memory. Also, although the complexity for insertions/lookups
    is `O(1)`, the actual speed depends greatly on the hashing function that is in
    use. If the hash function is slow to evaluate, any operations on dictionaries
    or sets will be similarly slow.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用字典和集合是有代价的。首先，它们通常在内存中占用更大的空间。此外，虽然插入/查找的复杂度是`O(1)`，但实际速度很大程度上取决于正在使用的哈希函数的评估速度。如果哈希函数评估速度慢，那么字典或集合上的任何操作都会变得很慢。
- en: Let’s look at an example. Say we want to store contact information for everyone
    in the phone book. We would like to store this in a form that will make it simple
    to answer the question “What is John Doe’s phone number?” in the future. With
    lists, we would store the phone numbers and names sequentially and scan through
    the entire list to find the phone number we required, as shown in [Example 4-1](#dict_set_phonebook_list).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个例子。假设我们想为电话簿中的每个人存储联系信息。我们希望以一种简单的方式存储这些信息，以便将来可以简单地回答“John Doe的电话号码是多少？”这样的问题。使用列表，我们将电话号码和姓名按顺序存储，并扫描整个列表以找到所需的电话号码，如[示例 4-1](#dict_set_phonebook_list)所示。
- en: Example 4-1\. Phone book lookup with a list
  id: totrans-7
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-1：使用列表进行电话簿查找
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: We could also do this by sorting the list and using the `bisect` module (from
    [Example 3-4](ch03.xhtml#list_bisect_example)) in order to get `O(log n)` performance.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以通过对列表进行排序并使用`bisect`模块（来自[示例 3-4](ch03.xhtml#list_bisect_example)）来实现`O(log
    n)`的性能。
- en: With a dictionary, however, we can simply have the “index” be the names and
    the “values” be the phone numbers, as shown in [Example 4-2](#dict_set_phonebook_dict).
    This allows us to simply look up the value we need and get a direct reference
    to it, instead of having to read every value in our dataset.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用字典，我们可以简单地将“索引”作为姓名，将“值”作为电话号码，如[示例 4-2](#dict_set_phonebook_dict)所示。这使得我们可以简单地查找我们需要的值，并直接引用它，而不必读取数据集中的每个值。
- en: Example 4-2\. Phone book lookup with a dictionary
  id: totrans-12
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-2：使用字典进行电话簿查找
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: For large phone books, the difference between the `O(1)` lookup of the dictionary
    and the `O(n)` time for linear search over the list (or, at best, the `O(log n)`
    complexity with the bisect module) is quite substantial.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大型电话簿，字典的`O(1)`查找与线性搜索列表的`O(n)`（或者最好情况下，使用二分模块的`O(log n)`复杂度）之间的差异是非常显著的。
- en: Tip
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Create a script that times the performance of the list-`bisect` method versus
    a dictionary for finding a number in a phone book. How does the timing scale as
    the size of the phone book grows?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个脚本，用于比较列表-`bisect`方法和字典在电话簿中查找数字的性能。随着电话簿大小的增长，时间如何扩展？
- en: If, on the other hand, we wanted to answer the question “How many unique first
    names are there in my phone book?” we could use the power of sets. Recall that
    a set is simply a collection of *unique* keys—this is the exact property we would
    like to enforce in our data. This is in stark contrast to a list-based approach,
    where that property needs to be enforced separately from the data structure by
    comparing all names with all other names. [Example 4-3](#alt_dup_layout) illustrates.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果我们想回答“我的电话簿中有多少个独特的名字？”，我们可以利用集合的威力。回想一下，集合就是一组*独特*的键的集合——这正是我们希望在数据中实施的属性。这与基于列表的方法形成鲜明对比，后者需要通过将所有名字与其他所有名字进行比较来单独执行此属性。[示例 4-3](#alt_dup_layout)进行了说明。
- en: Example 4-3\. Finding unique names with lists and sets
  id: totrans-18
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-3\. 使用列表和集合查找唯一名字
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[![1](Images/1.png)](#co_dictionaries_and_sets_CO1-1_new), [![3](Images/3.png)](#co_dictionaries_and_sets_CO1-3)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_dictionaries_and_sets_CO1-1_new), [![3](Images/3.png)](#co_dictionaries_and_sets_CO1-3)'
- en: We must go over all the items in our phone book, and thus this loop costs `O(n)`.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须检查电话簿中的所有条目，因此此循环的成本为`O(n)`。
- en: '[![2](Images/2.png)](#co_dictionaries_and_sets_CO1-2)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_dictionaries_and_sets_CO1-2)'
- en: Here, we must check the current name against all the unique names we have already
    seen. If it is a new unique name, we add it to our list of unique names. We then
    continue through the list, performing this step for every item in the phone book.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们必须检查当前名字是否已经存在于我们已经看到的所有唯一名字中。如果是一个新的唯一名字，我们就将其添加到我们的唯一名字列表中。然后我们继续通过列表，对电话簿中的每个项目执行此步骤。
- en: '[![4](Images/4.png)](#co_dictionaries_and_sets_CO1-4)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_dictionaries_and_sets_CO1-4)'
- en: For the set method, instead of iterating over all unique names we have already
    seen, we can simply add the current name to our set of unique names. Because sets
    guarantee the uniqueness of the keys they contain, if you try to add an item that
    is already in the set, that item simply won’t be added. Furthermore, this operation
    costs `O(1)`.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于集合方法，我们不需要遍历已经见过的所有唯一名字，而是可以简单地将当前名字添加到我们的唯一名字集合中。因为集合保证它们包含的键的唯一性，如果您尝试添加已经在集合中的项，该项就不会被添加。此外，此操作的成本为`O(1)`。
- en: The list algorithm’s inner loop iterates over `unique_names`, which starts out
    as empty and then grows, in the worst case, when all names are unique, to be the
    size of `phonebook`. This can be seen as performing a [linear search](ch03.xhtml#list_linear_search)
    for each name in the phone book over a list that is constantly growing. Thus,
    the complete algorithm performs as `O(n^2)`.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 列表算法的内部循环遍历`unique_names`，其开始为空，并在最坏的情况下（所有名字都是唯一的情况下）增长到与`phonebook`大小相同。这可以被看作是在一个不断增长的列表上执行每个名字的[线性搜索](ch03.xhtml#list_linear_search)。因此，完整算法的表现为`O(n^2)`。
- en: On the other hand, the set algorithm has no inner loop; the `set.add` operation
    is an `O(1)` process that completes in a fixed number of operations regardless
    of how large the phone book is (there are some minor caveats to this, which we
    will cover while discussing the implementation of dictionaries and sets). Thus,
    the only nonconstant contribution to the complexity of this algorithm is the loop
    over the phone book, making this algorithm perform in `O(n)`.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，集合算法没有内部循环；`set.add`操作是一个`O(1)`的过程，无论电话簿有多大，它都会在固定数量的操作内完成（这其中有一些小的注意事项，我们在讨论字典和集合的实现时会涉及到）。因此，这个算法的复杂性的唯一非常数贡献是对电话簿的循环遍历，使得这个算法的表现为`O(n)`。
- en: 'When timing these two algorithms using a `phonebook` with 10,000 entries and
    7,412 unique first names, we see how drastic the difference between `O(n)` and
    `O(n^2)` can be:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用具有10,000个条目和7,412个独特名字的`phonebook`来计时这两种算法时，我们可以看到`O(n)`和`O(n^2)`之间的巨大差异：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In other words, the set algorithm gave us a 252× speedup! In addition, as the
    size of the `phonebook` grows, the speed gains increase (we get a 557× speedup
    with a `phonebook` with 100,000 entries and 15,574 unique first names).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，集合算法使我们获得了252倍的加速！此外，随着`phonebook`的大小增长，速度增益也会增加（当`phonebook`有100,000条条目和15,574个独特的名字时，我们获得了557倍的加速）。
- en: How Do Dictionaries and Sets Work?
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 字典和集合是如何工作的？
- en: Dictionaries and sets use *hash tables* to achieve their `O(1)` lookups and
    insertions. This efficiency is the result of a very clever usage of a [hash function](#SEC-dict-set-hash-and-entropy)
    to turn an arbitrary key (i.e., a string or object) into an index for a list.
    The hash function and list can later be used to determine where any particular
    piece of data is right away, without a search. By turning the data’s key into
    something that can be used like a list index, we can get the same performance
    as with a list. In addition, instead of having to refer to data by a numerical
    index, which itself implies some ordering to the data, we can refer to it by this
    arbitrary key.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 字典和集合使用*哈希表*实现其 `O(1)` 的查找和插入。这种效率是通过非常巧妙地使用[哈希函数](#SEC-dict-set-hash-and-entropy)将任意键（例如字符串或对象）转换为列表的索引来实现的。哈希函数和列表稍后可以用于立即确定任何特定数据的位置，而无需搜索。通过将数据的键转换为可以像列表索引一样使用的东西，我们可以获得与列表相同的性能。此外，我们不必通过数字索引来引用数据，这本身就意味着数据的某种排序，而是可以通过这个任意键来引用它。
- en: Inserting and Retrieving
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 插入和检索
- en: To create a hash table from scratch, we start with some allocated memory, similar
    to what we started with for arrays. For an array, if we want to insert data, we
    simply find the smallest unused bucket and insert our data there (and resize if
    necessary). For hash tables, we must first figure out the placement of the data
    in this contiguous chunk of memory.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 要从头开始创建哈希表，我们首先需要一些分配的内存，类似于我们为数组所做的。对于数组，如果我们要插入数据，我们只需找到最小的未使用存储桶并在那里插入我们的数据（如果需要，进行调整大小）。对于哈希表，我们必须首先确定在这个连续的内存块中数据的位置。
- en: 'The placement of the new data is contingent on two properties of the data we
    are inserting: the hashed value of the key and how the value compares to other
    objects. This is because when we insert data, the key is first hashed and masked
    so that it turns into an effective index in an array.^([2](ch04.xhtml#idm46122425480024))
    The mask makes sure that the hash value, which can take the value of any integer,
    fits within the allocated number of buckets. So if we have allocated 8 blocks
    of memory and our hash value is `28975`, we consider the bucket at index `28975
    & 0b111 = 7`. If, however, our dictionary has grown to require 512 blocks of memory,
    the mask becomes `0b111111111` (and in this case, we would consider the bucket
    at index `28975 & 0b11111111`).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 插入新数据的位置取决于我们要插入的数据的两个属性：键的哈希值以及值与其他对象的比较。这是因为在插入数据时，首先对键进行哈希处理并进行掩码操作，使其变成数组中的有效索引。^([2](ch04.xhtml#idm46122425480024))
    掩码确保哈希值（可以是任何整数）适应于分配的存储桶数量。因此，如果我们分配了8个存储块而我们的哈希值是 `28975`，我们会考虑索引为 `28975 &
    0b111 = 7` 的存储桶。但是，如果我们的字典已经增长到需要512个存储块，掩码则变为 `0b111111111`（在这种情况下，我们将考虑索引为 `28975
    & 0b11111111` 的存储桶）。
- en: Now we must check if this bucket is already in use. If it is empty, we can insert
    the key and the value into this block of memory. We store the key so that we can
    make sure we are retrieving the correct value on lookups. If it is in use and
    the value of the bucket is equal to the value we wish to insert (a comparison
    done with the `cmp` built-in), then the key/value pair is already in the hash
    table and we can return. However, if the values don’t match, we must find a new
    place to put the data.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们必须检查这个存储桶是否已经被使用。如果为空，我们可以将键和值插入到这个内存块中。我们存储键以确保在查找时获取正确的值。如果已经被使用并且存储桶的值等于我们希望插入的值（使用
    `cmp` 内置函数进行比较），则键/值对已经存在于哈希表中，我们可以直接返回。然而，如果值不匹配，我们必须找一个新的位置来存放数据。
- en: As an extra optimization, Python first appends the key/value data into a standard
    array and then stores only the *index* into this array in the hash table. This
    allows us to reduce the amount of memory used by 30–95%.^([3](ch04.xhtml#idm46122425473400))
    In addition, this gives us the interesting property that we keep a record of the
    order which new items were added into the dictionary (which, since Python 3.7,
    is a guarantee that all dictionaries give).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 作为额外的优化，Python 首先将键/值数据追加到标准数组中，然后仅在哈希表中存储这个数组的*索引*。这样可以减少内存使用量，达到30%至95%的减少。^([3](ch04.xhtml#idm46122425473400))
    此外，这使我们保留了一个关于新项添加到字典中顺序的记录（自Python 3.7起，所有字典都有此保证）。
- en: To find the new index, we compute it using a simple linear function, a method
    called *probing*. Python’s probing mechanism adds a contribution from the higher-order
    bits of the original hash (recall that for a table of length 8 we considered only
    the last three bits of the hash for the initial index, through the use of a mask
    value of `mask = 0b111 = bin(8 - 1)`). Using these higher-order bits gives each
    hash a different sequence of next possible hashes, which helps to avoid future
    collisions.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到新的索引，我们使用一个简单的线性函数计算它，这称为*探测*方法。Python 的探测机制从原始哈希的高阶位添加贡献（回想一下，对于长度为 8 的表，我们仅考虑了初始索引的哈希的最后三位，通过使用掩码值
    `mask = 0b111 = bin(8 - 1)`）。使用这些高阶位为每个哈希提供了不同的下一个可能哈希序列，有助于避免未来的碰撞。
- en: 'There is a lot of freedom when picking the algorithm to generate a new index;
    however, it is quite important that the scheme visits every possible index in
    order to evenly distribute the data in the table. How well distributed the data
    is throughout the hash table is called the *load factor* and is related to the
    [entropy](#SEC-dict-set-hash-and-entropy) of the hash function. The pseudocode
    in [Example 4-4](#dict_set_index_sequence) illustrates the calculation of hash
    indices used in CPython 3.7\. This also shows an interesting fact about hash tables:
    most of the storage space they have is empty!'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 选择生成新索引的算法时有很大的自由度；但是，很重要的是方案必须访问表中的每个可能索引，以便均匀分布数据。数据在哈希表中的分布均匀程度称为*负载因子*，与哈希函数的[熵](#SEC-dict-set-hash-and-entropy)有关。[示例
    4-4](#dict_set_index_sequence) 中的伪代码展示了在 CPython 3.7 中使用的哈希索引计算。这也展示了有关哈希表的一个有趣事实：它们大部分的存储空间是空的！
- en: Example 4-4\. Dictionary lookup sequence
  id: totrans-40
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-4\. 字典查找序列
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[![1](Images/1.png)](#co_dictionaries_and_sets_CO1-1)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_dictionaries_and_sets_CO1-1)'
- en: '`hash` returns an integer, while the actual C code in CPython uses an unsigned
    integer. Because of that, this pseudocode doesn’t replicate exactly the behavior
    in CPython; however, it is a good approximation.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '`hash` 返回一个整数，而在实际的 CPython 代码中使用的是无符号整数。因此，这段伪代码并不能完全复制 CPython 中的行为；但是，它是一个很好的近似。'
- en: This probing is a modification of the naive method of *linear probing*. In linear
    probing, we simply yield the values `i = (i * 5 + perturb + 1) & mask`, where
    `i` is initialized to the hash value of the key.^([4](ch04.xhtml#idm46122425606376))
    An important thing to note is that linear probing deals only with the last several
    bits of the hash and disregards the rest (i.e., for a dictionary with eight elements,
    we look only at the last three bits since at that point the mask is `0x111`).
    This means that if hashing two items gives the same last three binary digits,
    we will not only have a collision, but also the sequence of probed indices will
    be the same. The perturbed scheme that Python uses will start taking into consideration
    more bits from the items’ hashes to resolve this problem.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这种探测方法是*线性探测*的一种修改版。在线性探测中，我们简单地生成值 `i = (i * 5 + perturb + 1) & mask`，其中 `i`
    被初始化为键的哈希值的。^([4](ch04.xhtml#idm46122425606376)) 需要注意的一点是，线性探测仅处理哈希的最后几位，并且忽略其余部分（即对于包含八个元素的字典，我们仅查看最后三位，此时掩码为
    `0x111`）。这意味着，如果对两个项目进行哈希处理得到相同的最后三位二进制数，不仅会发生碰撞，而且探测索引的序列也将相同。Python 使用的扰动方案将开始考虑来自项目哈希更多位以解决此问题。
- en: 'A similar procedure is done when we are performing lookups on a specific key:
    the given key is transformed into an index, and that index is examined. If the
    key in that index matches (recall that we also store the original key when doing
    insert operations), then we can return that value. If it doesn’t, we keep creating
    new indices using the same scheme, until we either find the data or hit an empty
    bucket. If we hit an empty bucket, we can conclude that the data does not exist
    in the table.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在特定键上执行查找时，也会执行类似的过程：给定键被转换为一个索引，然后检查该索引。如果该索引中的键匹配（回想一下，在执行插入操作时，我们还存储了原始键），那么我们可以返回该值。如果不匹配，我们将使用相同的方案创建新的索引，直到我们找到数据或者遇到一个空桶。如果遇到空桶，我们可以得出结论，表中不存在该数据。
- en: '[Figure 4-1](#FIG-hash-set-theory-example1) illustrates the process of adding
    data into a hash table. Here, we chose to create a hash function that simply uses
    the first letter of the input. We accomplish this by using Python’s `ord` function
    on the first letter of the input to get the integer representation of that letter
    (recall that hash functions must return integers). As we’ll see in [“Hash Functions
    and Entropy”](#SEC-dict-set-hash-and-entropy), Python provides hashing functions
    for most of its types, so typically you won’t have to provide one yourself except
    in extreme situations.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-1](#FIG-hash-set-theory-example1)展示了向哈希表添加数据的过程。在这里，我们选择创建一个哈希函数，简单地使用输入的第一个字母。我们通过使用Python的`ord`函数获取输入的第一个字母的整数表示来实现这一点（请回想哈希函数必须返回整数）。正如我们将在[“哈希函数和熵”](#SEC-dict-set-hash-and-entropy)中看到的，Python为其大多数类型提供了哈希函数，因此通常您不必自己提供哈希函数，除非在极端情况下。'
- en: '![hpp2 0401](Images/hpp2_0401.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![hpp2 0401](Images/hpp2_0401.png)'
- en: Figure 4-1\. The resulting hash table from inserting with collisions
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-1\. 插入带有冲突的哈希表的结果
- en: Insertion of the key `Barcelona` causes a collision, and a new index is calculated
    using the scheme in [Example 4-4](#dict_set_index_sequence). This dictionary can
    also be created in Python using the code in [Example 4-5](#dict_set_naive_dict).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 插入关键字`Barcelona`引发了冲突，并使用[示例 4-4](#dict_set_index_sequence)中的方案计算了一个新索引。此字典也可以使用[示例 4-5](#dict_set_naive_dict)中的代码在Python中创建。
- en: Example 4-5\. Custom hashing function
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-5\. 自定义哈希函数
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In this case, `Barcelona` and `Rome` cause the hash collision ([Figure 4-1](#FIG-hash-set-theory-example1)
    shows the outcome of this insertion). We see this because, for a dictionary with
    four elements, we have a mask value of `0b111`. As a result, `Barcelona` and `Rome`
    will try to use the same index:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，`Barcelona`和`Rome`导致了哈希冲突（[图 4-1](#FIG-hash-set-theory-example1)显示了这种插入的结果）。这是因为，对于一个具有四个元素的字典，我们的掩码值为`0b111`。因此，`Barcelona`和`Rome`将尝试使用相同的索引：
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Deletion
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 删除
- en: When a value is deleted from a hash table, we cannot simply write a `NULL` to
    that bucket of memory. This is because we have used `NULL`s as a sentinel value
    while probing for hash collisions. As a result, we must write a special value
    that signifies that the bucket is empty, but there still may be values after it
    to consider when resolving a hash collision. So if “Rome” was deleted from the
    dictionary, subsequent lookups for “Barcelona” will first see this sentinel value
    where “Rome” used to be and instead of stopping, continue to check the next indices
    given by the `index_sequence`. These empty slots can be written to in the future
    and are removed when the hash table is resized.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 当从哈希表中删除一个值时，我们不能简单地向内存的那个桶写入`NULL`。这是因为我们在探测哈希冲突时使用了`NULL`作为哨兵值。因此，我们必须写入一个特殊值，表示该桶为空，但在解决哈希冲突时仍可能存在后续值。因此，如果从字典中删除了“Rome”，那么对“Barcelona”的后续查找首先将看到曾经是“Rome”位置的哨兵值，而不是停止，并继续检查`index_sequence`给出的下一个索引。这些空槽可以在将来写入，并在调整哈希表大小时移除。
- en: Resizing
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调整大小
- en: As more items are inserted into the hash table, the table itself must be resized
    to accommodate them. It can be shown that a table that is no more than two-thirds
    full will have optimal space savings while still having a good bound on the number
    of collisions to expect. Thus, when a table reaches this critical point, it is
    grown. To do this, a larger table is allocated (i.e., more buckets in memory are
    reserved), the mask is adjusted to fit the new table, and all elements of the
    old table are reinserted into the new one. This requires recomputing indices,
    since the changed mask will change the resulting index. As a result, resizing
    large hash tables can be quite expensive! However, since we do this resizing operation
    only when the table is too small, as opposed to doing it on every insert, the
    amortized cost of an insert is still `O(1)`.^([5](ch04.xhtml#idm46122425201480))
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 当更多条目插入哈希表时，表本身必须调整大小以容纳它们。可以证明，只有不超过三分之二满的表在保持最佳空间节省的同时仍具有预期碰撞数的良好界限。因此，当表达到这一关键点时，它会被扩展。为此，分配一个更大的表（即，内存中保留更多桶），调整掩码以适应新表，并重新将旧表的所有元素重新插入新表中。这需要重新计算索引，因为更改的掩码会改变结果索引。因此，调整大小大的哈希表可能非常昂贵！但是，由于只有在表太小时才执行此调整操作，而不是在每次插入时执行，因此插入的摊销成本仍为`O(1)`。^([5](ch04.xhtml#idm46122425201480))
- en: 'By default, the smallest size of a dictionary or set is 8 (that is, if you
    are storing only three values, Python will still allocate eight elements), and
    it will resize by 3× if the dictionary is more than two-thirds full. So once the
    sixth item is being inserted into the originally empty dictionary, it will be
    resized to hold 18 elements. At this point, once the 13th element is inserted
    into the object, it will be resized to 39, then 81, and so on, always increasing
    the size by 3× (we will explain how to calculate a dictionary’s size in [“Hash
    Functions and Entropy”](#SEC-dict-set-hash-and-entropy)). This gives the following
    possible sizes:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，字典或集合的最小大小为8（即使您只存储三个值，Python也将分配八个元素），如果字典超过三分之二的容量，它将按3倍调整大小。因此，一旦第六个项目被插入到原始空字典中，它将调整大小以容纳18个元素。在这一点上，一旦第13个元素被插入对象中，它将调整大小为39，然后81，依此类推，始终以3倍增加大小（我们将解释如何计算字典大小在[“哈希函数和熵”](#SEC-dict-set-hash-and-entropy)）。这给出了以下可能的大小：
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: It is important to note that resizing can happen to make a hash table larger
    *or* smaller. That is, if sufficiently many elements of a hash table are deleted,
    the table can be scaled down in size. However, *resizing happens only during an
    insert*.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，调整大小可能使哈希表变大*或者*变小。也就是说，如果哈希表中删除了足够多的元素，表的大小可以缩小。但是，*调整大小只会在插入时发生*。
- en: Hash Functions and Entropy
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 哈希函数和熵
- en: Objects in Python are generally hashable, since they already have built-in `__hash__`
    and `__cmp__` functions associated with them. For numerical types (`int` and `float`),
    the hash is based simply on the bit value of the number they represent. Tuples
    and strings have a hash value that is based on their contents. Lists, on the other
    hand, do not support hashing because their values can change. Since a list’s values
    can change, so could the hash that represents the list, which would change the
    relative placement of that key in the hash table.^([6](ch04.xhtml#idm46122425187416))
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，对象通常是可哈希的，因为它们已经具有与它们相关联的内置`__hash__`和`__cmp__`函数。对于数值类型（`int`和`float`），哈希基于它们表示的数字的位值。元组和字符串的哈希值基于它们的内容。另一方面，列表不支持哈希化，因为它们的值可以改变。由于列表的值可以改变，表示列表的哈希也可能改变，这将改变哈希表中该键的相对位置。^([6](ch04.xhtml#idm46122425187416))
- en: User-defined classes also have default hash and comparison functions. The default
    `__hash__` function simply returns the object’s placement in memory as given by
    the built-in `id` function. Similarly, the `__cmp__` operator compares the numerical
    value of the object’s placement in memory.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 用户定义的类也有默认的哈希和比较函数。默认的`__hash__`函数简单地返回对象由内置`id`函数给出的内存位置。类似地，`__cmp__`运算符比较对象的内存位置的数值。
- en: 'This is generally acceptable, since two instances of a class are generally
    different and should not collide in a hash table. However, in some cases we would
    like to use `set` or `dict` objects to disambiguate between items. Take the following
    class definition:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，这是可以接受的，因为类的两个实例通常是不同的，不应在哈希表中发生碰撞。但是，在某些情况下，我们希望使用`set`或`dict`对象来消除项目之间的歧义。考虑以下类定义：
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'If we were to instantiate multiple `Point` objects with the same values for
    `x` and `y`, they would all be independent objects in memory and thus have different
    placements in memory, which would give them all different hash values. This means
    that putting them all into a `set` would result in all of them having individual
    entries:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用相同的`x`和`y`值实例化多个`Point`对象，它们将在内存中都是独立的对象，因此具有不同的内存位置，这将使它们具有不同的哈希值。这意味着将它们全部放入一个`set`中会导致它们都有独立的条目：
- en: '[PRE9]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can remedy this by forming a custom hash function that is based on the actual
    contents of the object as opposed to the object’s placement in memory. The hash
    function can be any function as long as it consistently gives the same result
    for the same object (there are also considerations regarding the entropy of the
    hashing function, which we will discuss later.) The following redefinition of
    the `Point` class will yield the results we expect:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过形成一个基于对象实际内容而不是内存中对象位置的自定义哈希函数来解决这个问题。哈希函数可以是任何函数，只要对于同一对象始终给出相同的结果（还有关于哈希函数熵的考虑，我们稍后会讨论）。重新定义`Point`类如下将产生我们期望的结果：
- en: '[PRE10]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This allows us to create entries in a set or dictionary indexed by the properties
    of the `Point` object rather than the memory address of the instantiated object:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们能够创建一个由`Point`对象的属性而不是实例化对象的内存地址索引的集合或字典的条目：
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As alluded to when we discussed hash collisions, a custom-selected hash function
    should be careful to evenly distribute hash values in order to avoid collisions.
    Having many collisions will degrade the performance of a hash table: if most keys
    have collisions, we need to constantly “probe” the other values, effectively walking
    a potentially large portion of the dictionary to find the key in question. In
    the worst case, when all keys in a dictionary collide, the performance of lookups
    in the dictionary is `O(n)` and thus the same as if we were searching through
    a list.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们讨论哈希碰撞时所提到的，一个自定义选择的哈希函数应该小心地均匀分布哈希值，以避免碰撞。有许多碰撞会降低哈希表的性能：如果大多数键都发生碰撞，我们需要不断地“探测”其他值，实际上是遍历字典的一个可能很大的部分以找到所需的键。在最坏的情况下，当字典中的所有键都发生碰撞时，字典中查找的性能是`O(n)`，因此与我们搜索列表时的性能相同。
- en: If we know that we are storing 5,000 values in a dictionary and we need to create
    a hashing function for the object we wish to use as a key, we must be aware that
    the dictionary will be stored in a hash table of size 16,384^([7](ch04.xhtml#idm46122424855864))
    and thus only the last 14 bits of our hash are being used to create an index (for
    a hash table of this size, the mask is `bin(16_384 - 1) = 0b11111111111111`).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们知道我们要在字典中存储 5,000 个值，并且我们需要为我们希望用作键的对象创建一个哈希函数，我们必须意识到该字典将存储在大小为 16,384^([7](ch04.xhtml#idm46122424855864))
    的哈希表中，因此我们的哈希的最后 14 位被用来创建索引（对于这个大小的哈希表，掩码是`bin(16_384 - 1) = 0b11111111111111`）。
- en: This idea of “how well distributed my hash function is” is called the *entropy*
    of the hash function. Entropy is defined as
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: “我的哈希函数分布得有多好”这个想法被称为哈希函数的*熵*。熵的定义为
- en: <math display="block" alttext="upper S equals minus sigma-summation Underscript
    i Endscripts p left-parenthesis i right-parenthesis dot log left-parenthesis p
    left-parenthesis i right-parenthesis right-parenthesis"><mrow><mi>S</mi> <mo>=</mo>
    <mo>–</mo> <munder><mo>∑</mo> <mi>i</mi></munder> <mi>p</mi> <mrow><mo>(</mo>
    <mi>i</mi> <mo>)</mo></mrow> <mo>·</mo> <mo form="prefix">log</mo> <mfenced separators=""
    open="(" close=")"><mi>p</mi> <mo>(</mo> <mi>i</mi> <mo>)</mo></mfenced></mrow></math>
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block" alttext="upper S equals minus sigma-summation Underscript
    i Endscripts p left-parenthesis i right-parenthesis dot log left-parenthesis p
    left-parenthesis i right-parenthesis right-parenthesis"><mrow><mi>S</mi> <mo>=</mo>
    <mo>–</mo> <munder><mo>∑</mo> <mi>i</mi></munder> <mi>p</mi> <mrow><mo>(</mo>
    <mi>i</mi> <mo>)</mo></mrow> <mo>·</mo> <mo form="prefix">log</mo> <mfenced separators=""
    open="(" close=")"><mi>p</mi> <mo>(</mo> <mi>i</mi> <mo>)</mo></mfenced></mrow></math>
- en: where `p(i)` is the probability that the hash function gives hash `i`. It is
    maximized when every hash value has equal probability of being chosen. A hash
    function that maximizes entropy is called an *ideal* hash function since it guarantees
    the minimal number of collisions.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这里`p(i)`是哈希函数给出哈希值`i`的概率。当每个哈希值被选择的概率相等时，它被最大化。最大化熵的哈希函数被称为*理想*哈希函数，因为它保证了最小数量的碰撞。
- en: For an infinitely large dictionary, the hash function used for integers is ideal.
    This is because the hash value for an integer is simply the integer itself! For
    an infinitely large dictionary, the mask value is infinite, and thus we consider
    all bits in the hash value. Therefore, given any two numbers, we can guarantee
    that their hash values will not be the same.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个无限大的字典，用于整数的哈希函数是理想的。这是因为整数的哈希值就是整数本身！对于一个无限大的字典，掩码值是无限的，因此我们考虑哈希值中的所有位。因此，给定任意两个数字，我们可以保证它们的哈希值不会相同。
- en: However, if we made this dictionary finite, we could no longer have this guarantee.
    For example, for a dictionary with four elements, the mask we use is `0b111`.
    Thus the hash value for the number `5` is `5 & 0b111 = 5`, and the hash value
    for `501` is `501 & 0b111 = 5`, and so their entries will collide.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们将这个字典设为有限的，我们就不能再有这个保证了。例如，对于一个有四个元素的字典，我们使用的掩码是`0b111`。因此数字`5`的哈希值是`5
    & 0b111 = 5`，而`501`的哈希值是`501 & 0b111 = 5`，它们的条目会发生碰撞。
- en: Note
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: To find the mask for a dictionary with an arbitrary number of elements, `N`,
    we first find the minimum number of buckets that dictionary must have to still
    be two-thirds full (`N * (2 / 3 + 1)`). Then we find the smallest dictionary size
    that will hold this number of elements (8; 32; 128; 512; 2,048; etc.) and find
    the number of bits necessary to hold this number. For example, if `N=1039`, then
    we must have at least 1,731 buckets, which means we need a dictionary with 2,048
    buckets. Thus the mask is `bin(2048 - 1) = 0b11111111111`.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到一个具有任意元素数`N`的字典的掩码，我们首先找到该字典必须拥有的最小桶数，以使其仍然是三分之二满（`N * (2 / 3 + 1)`）。然后我们找到能容纳这么多元素的最小字典大小（8；32；128；512；2,048；等等），并找到保存这个数量所需的位数。例如，如果`N=1039`，那么我们必须至少有
    1,731 个桶，这意味着我们需要一个有 2,048 个桶的字典。因此掩码是`bin(2048 - 1) = 0b11111111111`。
- en: There is no single best hash function to use when using a finite dictionary.
    However, knowing up front what range of values will be used and how large the
    dictionary will be helps in making a good selection. For example, if we are storing
    all 676 combinations of two lowercase letters as keys in a dictionary (*aa*, *ab*,
    *ac*, etc.), a good hashing function would be the one shown in [Example 4-6](#example_dict_set_twoletter_hash_function).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 使用有限字典时，没有单一的最佳哈希函数可用。然而，提前知道将使用的值范围和字典的大小有助于做出良好的选择。例如，如果我们将所有两个小写字母的676种组合存储为字典中的键（*aa*,
    *ab*, *ac*等），那么一个良好的哈希函数将是[示例 4-6](#example_dict_set_twoletter_hash_function)中显示的那种。
- en: Example 4-6\. Optimal two-letter hashing function
  id: totrans-82
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-6\. 最佳的两字母哈希函数
- en: '[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This gives no hash collisions for any combination of two lowercase letters,
    considering a mask of `0b1111111111` (a dictionary of 676 values will be held
    in a hash table of length 2,048, which has a mask of `bin(2048 - 1) = 0b11111111111`).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这对任何两个小写字母组合都不会发生哈希碰撞，考虑到`0b1111111111`掩码（一个包含676个值的字典将存储在长度为2048的哈希表中，其掩码为`bin(2048
    - 1) = 0b11111111111`）。
- en: '[Example 4-7](#example_dict_set_twoletter_hash_objects) very explicitly shows
    the ramifications of having a bad hashing function for a user-defined class—here,
    the cost of a bad hash function (in fact, it is the worst possible hash function!)
    is a 41.8× slowdown of lookups.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 4-7](#example_dict_set_twoletter_hash_objects)非常明确地展示了为用户定义的类使用不良哈希函数的后果——在这里，使用不良哈希函数的代价（事实上，这是可能的最差的哈希函数！）是查找减慢了41.8倍。'
- en: Example 4-7\. Timing differences between good and bad hashing functions
  id: totrans-86
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-7\. 良好和不良哈希函数之间的时间差异
- en: '[PRE13]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Dictionaries and Namespaces
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 字典和命名空间
- en: Doing a lookup on a dictionary is fast; however, doing it unnecessarily will
    slow down your code, just as any extraneous lines will. One area where this surfaces
    is in Python’s namespace management, which heavily uses dictionaries to do its
    lookups.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在字典上进行查找速度很快；然而，不必要地这样做会减慢您的代码，就像任何多余的行一样。一个显现这种情况的地方是Python的命名空间管理，在这里大量使用字典来进行查找。
- en: Whenever a variable, function, or module is invoked in Python, there is a hierarchy
    that determines where it looks for these objects. First, Python looks inside the
    `locals()` array, which has entries for all local variables. Python works hard
    to make local variable lookups fast, and this is the only part of the chain that
    doesn’t require a dictionary lookup. If it doesn’t exist there, the `globals()`
    dictionary is searched. Finally, if the object isn’t found there, the `__builtin__`
    object is searched. It is important to note that while `locals()` and `globals()`
    are explicitly dictionaries and `__builtin__` is technically a module object,
    when searching `__builtin__` for a given property, we are just doing a dictionary
    lookup inside *its* `locals()` map (this is the case for all module objects and
    class objects!).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 每当在Python中调用变量、函数或模块时，都有一个层次结构确定它查找这些对象的位置。首先，Python查找`locals()`数组，该数组包含所有局部变量的条目。Python努力使局部变量查找快速化，这是链中唯一不需要字典查找的部分。如果在那里找不到，就会搜索`globals()`字典。最后，如果对象在那里找不到，就会搜索`__builtin__`对象。需要注意的是，虽然`locals()`和`globals()`显式是字典，而`__builtin__`在技术上是模块对象，但当在`__builtin__`中搜索特定属性时，我们只是在它的`locals()`映射中进行字典查找（这对所有模块对象和类对象都适用！）。
- en: To make this clearer, let’s look at a simple example of calling functions that
    are defined in different scopes ([Example 4-8](#dict_set_namespace_code)). We
    can disassemble the functions with the `dis` module ([Example 4-9](#dict_set_namespace_dis))
    to get a better understanding of how these namespace lookups are happening (see
    [“Using the dis Module to Examine CPython Bytecode”](ch02.xhtml#profiling-dis)).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这更清晰，让我们看一个简单的例子，调用在不同范围内定义的函数（[示例 4-8](#dict_set_namespace_code)）。我们可以使用`dis`模块（[示例 4-9](#dict_set_namespace_dis)）来解开函数，以更好地理解这些命名空间查找是如何进行的（参见[“使用
    dis 模块来检查 CPython 字节码”](ch02.xhtml#profiling-dis)）。
- en: Example 4-8\. Namespace lookups
  id: totrans-92
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-8\. 命名空间查找
- en: '[PRE14]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Example 4-9\. Namespace lookups disassembled
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-9\. 命名空间查找拆解
- en: '[PRE15]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The first function, `test1`, makes the call to `sin` by explicitly looking
    at the math library. This is also evident in the bytecode that is produced: first
    a reference to the `math` module must be loaded, and then we do an attribute lookup
    on this module until we finally have a reference to the `sin` function. This is
    done through two dictionary lookups: one to find the `math` module and one to
    find the `sin` function within the module.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个函数，`test1`，通过显式查看数学库进行 `sin` 的调用。这也可以从生成的字节码中看出：首先必须加载对 `math` 模块的引用，然后我们对此模块进行属性查找，直到最终获得对
    `sin` 函数的引用。这通过两次字典查找完成：一次是找到 `math` 模块，另一次是在模块中找到 `sin` 函数。
- en: On the other hand, `test2` explicitly imports the `sin` function from the `math`
    module, and the function is then directly accessible within the global namespace.
    This means we can avoid the lookup of the `math` module and the subsequent attribute
    lookup. However, we still must find the `sin` function within the global namespace.
    This is yet another reason to be explicit about what functions you are importing
    from a module. This practice not only makes code more readable, because the reader
    knows exactly what functionality is required from external sources, but it also
    simplifies changing the implementation of specific functions and generally speeds
    up code!
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，`test2`从 `math` 模块显式导入 `sin` 函数，然后函数直接在全局命名空间中可访问。这意味着我们可以避免对 `math` 模块和随后的属性查找。然而，我们仍然必须在全局命名空间中找到
    `sin` 函数。这是另一个理由明确指定从模块导入哪些函数。这种做法不仅使代码更易读，因为读者知道确切需要从外部源中获取哪些功能，而且简化了特定函数实现的更改，并且通常加快了代码！
- en: Finally, `test3` defines the `sin` function as a keyword argument, with its
    default value being a reference to the `sin` function within the `math` module.
    While we still do need to find a reference to this function within the module,
    this is necessary only when the `test3` function is first defined. After this,
    the reference to the `sin` function is stored within the function definition as
    a local variable in the form of a default keyword argument. As mentioned previously,
    local variables do not need a dictionary lookup to be found; they are stored in
    a very slim array that has very fast lookup times. Because of this, finding the
    function is quite fast!
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`test3`将 `sin` 函数定义为关键字参数，其默认值是对 `math` 模块内 `sin` 函数的引用。虽然我们仍然需要在模块内查找对此函数的引用，但这只在首次定义
    `test3` 函数时需要。之后，对 `sin` 函数的引用将作为默认关键字参数的形式存储在函数定义中的局部变量中。如前所述，局部变量无需进行字典查找即可找到；它们存储在一个非常紧凑的数组中，具有非常快的查找时间。因此，查找函数是非常快的！
- en: While these effects are an interesting result of the way namespaces in Python
    are managed, `test3` is definitely not “Pythonic.” Luckily, these extra dictionary
    lookups start to degrade performance only when they are called a lot (i.e., in
    the innermost block of a very fast loop, such as in the Julia set example). With
    this in mind, a more readable solution would be to set a local variable with the
    global reference before the loop is started. We’ll still have to do the global
    lookup once whenever the function is called, but all the calls to that function
    in the loop will be made faster. This speaks to the fact that even minute slowdowns
    in code can be amplified if that code is being run millions of times. Even though
    a dictionary lookup itself may take only several hundred nanoseconds, if we are
    looping millions of times over this lookup, those nanoseconds can quickly add
    up.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些效果是 Python 命名空间管理方式的有趣结果，但`test3`显然不是“Pythonic”。幸运的是，只有在大量调用时（例如在非常快的循环的最内部块中，比如
    Julia 集的示例中），这些额外的字典查找才会开始降低性能。考虑到这一点，一个更可读的解决方案是在循环开始前用全局引用设置一个局部变量。我们仍然需要在每次调用函数时做一次全局查找，但在循环中对该函数的所有调用将变得更快。这表明，即使是代码中微小的减速也可能在代码被执行数百万次时被放大。即使字典查找本身可能只需要几百纳秒，如果我们在这个查找上进行数百万次的循环，这些纳秒很快就会累加起来。
- en: Note
  id: totrans-100
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'A message about microbenchmarks: it may seem confusing that in [Example 4-8](#dict_set_namespace_code)
    we add in extra work with the `for` loop and the modification to the `res` variable.
    Originally, each of these functions simply had the relevant `return sin(x)` line
    and nothing else. As a result, we were also getting nanosecond runtimes and results
    that did not make any sense!'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 有关微基准测试的消息：在[示例 4-8](#dict_set_namespace_code)中，我们添加了额外的工作量，包括`for`循环和对`res`变量的修改，这可能看起来令人困惑。最初，这些函数中每个都只有相关的`return
    sin(x)`行，没有其他内容。因此，我们也得到了纳秒级运行时间和毫无意义的结果！
- en: When we added a bigger workload within each function, as done through the loop
    and the modification of the `res` variable, we started seeing the results we expected.
    With a bigger workload inside the function, we can be more sure that we are not
    measuring overhead from the benchmarking/timing process. In general, when you
    are running benchmarks and have a difference in timing in the nanoseconds, it’s
    important to sit back for a second and think through whether the experiment you
    are running is valid or whether you are measuring noise or unrelated timings as
    a result of instrumentation.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在每个函数内部增加更大的工作负载时，就像通过循环和对`res`变量的修改完成的那样，我们开始看到了我们预期的结果。通过在函数内部增加更大的工作负载，我们可以更加确定我们不是在测量基准/计时过程中的开销。一般来说，当你运行基准测试并且在纳秒级别上有时间差异时，重要的是停下来思考一秒钟，思考你正在运行的实验是否有效，或者你是否在测量噪声或由仪器测量造成的无关时间。
- en: Wrap-Up
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Dictionaries and sets provide a fantastic way to store data that can be indexed
    by a key. The way this key is used, through the hashing function, can greatly
    affect the resulting performance of the data structure. Furthermore, understanding
    how dictionaries work gives you a better understanding not only of how to organize
    your data but also of how to organize your code, since dictionaries are an intrinsic
    part of Python’s internal functionality.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 字典和集合提供了一种绝佳的方式来存储可以通过键索引的数据。通过哈希函数使用这个键的方式，可以极大地影响数据结构的性能。此外，了解字典的工作原理不仅可以帮助你更好地组织数据，还可以帮助你组织代码，因为字典是Python内部功能的一个固有部分。
- en: In the next chapter we will explore generators, which allow us to provide data
    to code with more control over ordering and without having to store full datasets
    in memory beforehand. This lets us sidestep many of the possible hurdles that
    we might encounter when using any of Python’s intrinsic data structures.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨生成器，这些生成器允许我们为代码提供数据，并且可以更好地控制顺序，而无需预先将完整数据集存储在内存中。这使我们能够避开在使用Python内置数据结构时可能遇到的许多障碍。
- en: ^([1](ch04.xhtml#idm46122426063384-marker)) As we will discuss in [“Hash Functions
    and Entropy”](#SEC-dict-set-hash-and-entropy), dictionaries and sets are very
    dependent on their hash functions. If the hash function for a particular datatype
    is not `O(1)`, any dictionary or set containing that type will no longer have
    its `O(1)` guarantee.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch04.xhtml#idm46122426063384-marker)) 正如我们将在[“哈希函数和熵”](#SEC-dict-set-hash-and-entropy)中讨论的那样，字典和集合非常依赖它们的哈希函数。如果特定数据类型的哈希函数不是`O(1)`，那么包含该类型的任何字典或集合将不再具有`O(1)`的保证。
- en: ^([2](ch04.xhtml#idm46122425480024-marker)) A *mask* is a binary number that
    truncates the value of a number. So `0b1111101 & 0b111 = 0b101 = 5` represents
    the operation of `0b111` masking the number `0b1111101`. This operation can also
    be thought of as taking a certain number of the least-significant digits of a
    number.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch04.xhtml#idm46122425480024-marker)) *掩码*是一种二进制数，用于截断数字的值。因此，`0b1111101
    & 0b111 = 0b101 = 5`代表了`0b111`掩码操作`0b1111101`数的操作。这个操作也可以理解为取一个数字的最低有效位的某些位数。
- en: ^([3](ch04.xhtml#idm46122425473400-marker)) The discussion that led to this
    improvement can be found at [*https://oreil.ly/Pq7Lm*](https://oreil.ly/Pq7Lm).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch04.xhtml#idm46122425473400-marker)) 导致这一改进的讨论可以在[*https://oreil.ly/Pq7Lm*](https://oreil.ly/Pq7Lm)找到。
- en: ^([4](ch04.xhtml#idm46122425606376-marker)) The value of `5` comes from the
    properties of a linear congruential generator (LCG), which is used in generating
    random numbers.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch04.xhtml#idm46122425606376-marker)) 数值`5`来自于线性同余生成器（LCG）的特性，该生成器用于生成随机数。
- en: ^([5](ch04.xhtml#idm46122425201480-marker)) Amortized analysis looks at the
    average complexity of an algorithm. This means that some inserts will be much
    more expensive, but on average, inserts will be `O(1)`.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch04.xhtml#idm46122425201480-marker)) 摊还分析关注的是算法的平均复杂度。这意味着一些插入操作可能会更昂贵，但平均而言，插入操作的复杂度是`O(1)`。
- en: ^([6](ch04.xhtml#idm46122425187416-marker)) More information about this can
    be found at [*https://oreil.ly/g4I5-*](https://oreil.ly/g4I5-).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch04.xhtml#idm46122425187416-marker)) 关于这个的更多信息可以在[*https://oreil.ly/g4I5-*](https://oreil.ly/g4I5-)找到。
- en: ^([7](ch04.xhtml#idm46122424855864-marker)) 5,000 values need a dictionary that
    has at least 8,333 buckets. The first available size that can fit this many elements
    is 16,384.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch04.xhtml#idm46122424855864-marker)) 5000个数值需要一个至少有8,333个桶的字典。能容纳这么多元素的第一个可用尺寸是16,384。
