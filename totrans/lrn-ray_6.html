<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 7. Data Processing with Ray" data-type="chapter" epub:type="chapter"><div class="chapter" id="chapter_07">
<h1><span class="label">Chapter 7. </span>Data Processing with Ray</h1>
<p class="byline">Edward Oakes</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm44990021837216">
<h5>A Note for Early Release Readers</h5>
<p>With Early Release ebooks, you get books in their earliest form—the author’s raw and unedited content as they write—so you can take advantage of these technologies long before the official release of these titles.</p>
</div></aside>
<p>In the previous chapter, you learned how to scale machine learning training using Ray Train.
Of course, the key component to applying machine learning in practice is data.
In this chapter, we’ll explore the core set of data processing capabilities on Ray: Ray Data.</p>
<p>While not meant to be a replacement for more general data processing systems such as Apache Spark or Apache Hadoop, Ray Data offers basic data processing capabilities and a standard way to load, transform, and pass data to different parts of a Ray application.
This enables an ecosystem of libraries on Ray to all speak the same language so users can mix and match functionality in a framework-agnostic way to meet their needs.</p>
<p>The central component of the Ray Data ecosystem is the aptly-named “Datasets,” which offers the core abstractions for loading, transforming, and passing references to data in a Ray cluster.
Datasets are the “glue” that enable different libraries to interoperate on top of Ray.
You’ll see this in action later in the chapter, as we show how you can do dataframe processing using the full expressiveness of the Dask API using Dask on Ray and transform the result into a Dataset, as well as how a Dataset can be used to efficiently do distributed deep learning training at scale using Ray Train and Ray Tune.</p>
<p>The main benefits of Datasets are:</p>
<dl>
<dt>Flexibility</dt>
<dd>
<p>Datasets support a wide range of data formats, work seamlessly with library integrations like Dask on Ray, and can be passed between Ray tasks and actors without copying data.</p>
</dd>
<dt>Performance for ML workloads</dt>
<dd>
<p>Datasets offers important features like accelerator support, pipelining, and global random shuffles that accelerate ML training and inference workloads.</p>
</dd>
</dl>
<p>This chapter is intended to get you familiar with the core concepts for doing data processing on Ray and to help you understand how to accomplish common patterns and why you would choose to use different pieces to accomplish a task.
The chapter assumes a basic familiarity with data processing concepts such as map, filter, groupby, and partitions, but it’s not to be a tutorial on data science in general nor a deep dive into the internals of how these operations are implemented.
Readers with a limited data processing / data science background should not have a problem following along.</p>
<p>We’ll start by giving a basic introduction to the core building block: Ray Datasets.
This will cover the architecture, basics of the API, and an example how Datasets can be used to making building complex data-intensive applications easy.
Then, we’ll briefly cover external library integrations on Ray, focusing on Dask on Ray.
Finally, we’ll bring it all together by building a scalable end-to-end machine learning pipeline in a single Python script.</p>
<p>The notebook for this chapter <a href="https://github.com/maxpumperla/learning_ray/blob/main/notebooks/ch_07_data_processing.ipynb">is also available online</a>.</p>
<section data-pdf-bookmark="Ray Datasets" data-type="sect1"><div class="sect1" id="idm44990021797136">
<h1>Ray Datasets</h1>
<section data-pdf-bookmark="An Overview of Datasets" data-type="sect2"><div class="sect2" id="idm44990021795888">
<h2>An Overview of Datasets</h2>
<p>The main goal of Datasets is to support a scalable, flexible abstraction for data processing on Ray.
Datasets are intended to be the standard way to read, write, and transfer data across the full ecosystem of Ray libraries.
One of the most powerful uses of Datasets is acting as the data ingest and preprocessing layer for machine learning workloads, allowing you to efficiently scale up training using Ray Train and Ray Tune.
This will be explored in more detail in the last section of the chapter.</p>
<p>If you’ve worked with other distributed data processing APIs such as Apache Spark’s RDDs in the past, you will find the Datasets API very familiar.
The core of the API leans on functional programming and offers standard functionality such as reading/writing many different data sources, performing basic transformations like map, filter, and sort, as well as some simple aggregations such as groupby.</p>
<p>Under the hood, Datasets implements Distributed <a href="https://arrow.apache.org/">Apache Arrow</a>.
Apache Arrow is a unified columnar data format for data processing libraries and applications, so integrating with it means that Datasets get interoperability with many of the most popular processing libraries such as NumPy and pandas out of the box.</p>
<p>A Dataset consists of a list of Ray object references, each of which points at a “block” of data.
These blocks are either Arrow tables or Python lists (for data that isn’t supported by the Arrow format) in Ray’s shared memory object store, and compute over the data such as for map or filter operations happens in Ray tasks (and sometimes actors).</p>
<p>Because Datasets relies on the core Ray primitives of tasks and objects in the shared memory object store, it inherits key benefits of Ray: scalability to hundreds of nodes, efficient memory usage due to sharing memory across processes on the same node, and object spilling + recovery to gracefully handle failures.
Additionally, because Datasets are just lists of object references, they can also be passed between tasks and actors efficiently without needing to make a copy of the data, which is crucial for making data-intensive applications and libraries scalable.</p>
</div></section>
<section data-pdf-bookmark="Datasets Basics" data-type="sect2"><div class="sect2" id="idm44990021791904">
<h2>Datasets Basics</h2>
<p>This section will give a basic overview of Ray Datasets, covering how to get started reading, writing, and transforming datasets.
This is not meant to be a comprehensive reference, but rather to introduce you to the basic concepts so we can build up to some interesting examples of what makes Ray Datasets powerful in later sections.
For up-to-date information on what’s supported and exact syntax, see the <a href="https://docs.ray.io/en/latest/data/dataset.xhtml">Datasets documentation</a>.</p>
<p>To follow along with the examples in this section, please make sure you have Ray Data installed locally:</p>
<pre data-type="programlisting">pip install "ray[data]"==1.9.0</pre>
<section data-pdf-bookmark="Creating a dataset" data-type="sect3"><div class="sect3" id="idm44990021788352">
<h3>Creating a dataset</h3>
<p>First, let’s create a simple Dataset and perform some basic operations on it:</p>
<div data-type="example">
<h5><span class="label">Example 7-1. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">ray</code>

<code class="c1"># Create a dataset containing integers in the range [0, 10000).</code>
<code class="n">ds</code> <code class="o">=</code> <code class="n">ray</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">range</code><code class="p">(</code><code class="mi">10000</code><code class="p">)</code>

<code class="c1"># Basic operations: show the size of the dataset, get a few samples, print the schema.</code>
<code class="nb">print</code><code class="p">(</code><code class="n">ds</code><code class="o">.</code><code class="n">count</code><code class="p">())</code>  <code class="c1"># -&gt; 10000</code>
<code class="nb">print</code><code class="p">(</code><code class="n">ds</code><code class="o">.</code><code class="n">take</code><code class="p">(</code><code class="mi">5</code><code class="p">))</code>  <code class="c1"># -&gt; [0, 1, 2, 3, 4]</code>
<code class="nb">print</code><code class="p">(</code><code class="n">ds</code><code class="o">.</code><code class="n">schema</code><code class="p">())</code>  <code class="c1"># -&gt; &lt;class 'int'&gt;</code></pre></div>
<p>Here we created a Dataset containing the numbers from 0 to 10000, then printed some basic information about it: the total number of records, a few samples, and the schema (we will discuss this in more detail later).</p>
</div></section>
<section data-pdf-bookmark="Reading from and writing to storage" data-type="sect3"><div class="sect3" id="idm44990021733696">
<h3>Reading from and writing to storage</h3>
<p>Of course, for real workloads you’ll often want to read from and write to persistent storage to load your data and write the results.
Writing and reading Datasets is simple; for example, to write a Dataset to a CSV file and then load it back into memory, we just need to use the builtin write_csv and read_csv utilities:</p>
<div data-type="example">
<h5><span class="label">Example 7-2. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Save the dataset to a local file and load it back.</code>
<code class="n">ray</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">range</code><code class="p">(</code><code class="mi">10000</code><code class="p">)</code><code class="o">.</code><code class="n">write_csv</code><code class="p">(</code><code class="s2">"local_dir"</code><code class="p">)</code>
<code class="n">ds</code> <code class="o">=</code> <code class="n">ray</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code><code class="s2">"local_dir"</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">ds</code><code class="o">.</code><code class="n">count</code><code class="p">())</code></pre></div>
<p>Datasets supports a number of common serialization formats such as CSV, JSON, and Parquet and can read from or write to local disk as well as remote storage like HDFS or AWS S3.</p>
<p>In the example above, we provided just a local file path (<code>"local_dir"</code>) so the Dataset was written to a directory on the local machine.
If we wanted to write to and read from S3 instead, we would instead provide a path like <code>"s3://my_bucket/"</code> and Datasets would automatically handle efficiently reading/writing remote storage, parallelizing the requests across many tasks to improve throughput.</p>
<p>Note that Datasets also supports custom datasources that you can use to write to any external data storage system that isn’t supported out-of-the-box.</p>
</div></section>
<section data-pdf-bookmark="Built-in transformations" data-type="sect3"><div class="sect3" id="idm44990021754704">
<h3>Built-in transformations</h3>
<p>Now that we understand the basic APIs around how to create and inspect Datasets, let’s take a look at some of the builtin operations we can do on them.
The code sample below shows three basic operations that Datasets support:
- First, we <code>union</code> two Datasets together. The result is a new Dataset that contains all of the records of both.
- Then, we <code>filter</code> the elements of a Dataset to only include even integers by providing a custom filter function.
- Finally, we <code>sort</code> the Dataset.</p>
<div data-type="example">
<h5><span class="label">Example 7-3. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Basic transformations: join two datasets, filter, and sort.</code>
<code class="n">ds1</code> <code class="o">=</code> <code class="n">ray</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">range</code><code class="p">(</code><code class="mi">10000</code><code class="p">)</code>
<code class="n">ds2</code> <code class="o">=</code> <code class="n">ray</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">range</code><code class="p">(</code><code class="mi">10000</code><code class="p">)</code>
<code class="n">ds3</code> <code class="o">=</code> <code class="n">ds1</code><code class="o">.</code><code class="n">union</code><code class="p">(</code><code class="n">ds2</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">ds3</code><code class="o">.</code><code class="n">count</code><code class="p">())</code>  <code class="c1"># -&gt; 20000</code>

<code class="c1"># Filter the combined dataset to only the even elements.</code>
<code class="n">ds3</code> <code class="o">=</code> <code class="n">ds3</code><code class="o">.</code><code class="n">filter</code><code class="p">(</code><code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="n">x</code> <code class="o">%</code> <code class="mi">2</code> <code class="o">==</code> <code class="mi">0</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">ds3</code><code class="o">.</code><code class="n">count</code><code class="p">())</code>  <code class="c1"># -&gt; 10000</code>
<code class="nb">print</code><code class="p">(</code><code class="n">ds3</code><code class="o">.</code><code class="n">take</code><code class="p">(</code><code class="mi">5</code><code class="p">))</code>  <code class="c1"># -&gt; [0, 2, 4, 6, 8]</code>

<code class="c1"># Sort the filtered dataset.</code>
<code class="n">ds3</code> <code class="o">=</code> <code class="n">ds3</code><code class="o">.</code><code class="n">sort</code><code class="p">()</code>
<code class="nb">print</code><code class="p">(</code><code class="n">ds3</code><code class="o">.</code><code class="n">take</code><code class="p">(</code><code class="mi">5</code><code class="p">))</code>  <code class="c1"># -&gt; [0, 0, 2, 2, 4]</code></pre></div>
<p>In addition to the operations above, Datasets also support common aggregations you might expect such as <code>groupby</code>, <code>sum</code>, <code>min</code>, etc.
You can also pass a user-defined function for custom aggregations.</p>
</div></section>
<section data-pdf-bookmark="Blocks and repartitioning" data-type="sect3"><div class="sect3" id="idm44990021556928">
<h3>Blocks and repartitioning</h3>
<p>One of the important things to keep in mind when using Datasets is the concept of <em>blocks</em> discussed earlier in the section.
Blocks are the underlying chunks of data that make up a Dataset; operations are applied to the underlying data one block at a time.
If the number of blocks in a Dataset is too high, each block will be small and there will be a lot of overhead for each operation.
If the number of blocks is too small, operations won’t be able to be parallelized as efficiently.</p>
<p>If we take a peek under the hood from the example above, we can see that the initial datasets we created each had 200 blocks by default and when we combined them, the resulting Dataset had 400 blocks.
In this case, we may want to repartition the Dataset to bring it back to the original 200 blocks that we started with:</p>
<div data-type="example">
<h5><span class="label">Example 7-4. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="n">ds1</code> <code class="o">=</code> <code class="n">ray</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">range</code><code class="p">(</code><code class="mi">10000</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">ds1</code><code class="o">.</code><code class="n">num_blocks</code><code class="p">())</code>  <code class="c1"># -&gt; 200</code>
<code class="n">ds2</code> <code class="o">=</code> <code class="n">ray</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">range</code><code class="p">(</code><code class="mi">10000</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">ds2</code><code class="o">.</code><code class="n">num_blocks</code><code class="p">())</code>  <code class="c1"># -&gt; 200</code>
<code class="n">ds3</code> <code class="o">=</code> <code class="n">ds1</code><code class="o">.</code><code class="n">union</code><code class="p">(</code><code class="n">ds2</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">ds3</code><code class="o">.</code><code class="n">num_blocks</code><code class="p">())</code>  <code class="c1"># -&gt; 400</code>

<code class="nb">print</code><code class="p">(</code><code class="n">ds3</code><code class="o">.</code><code class="n">repartition</code><code class="p">(</code><code class="mi">200</code><code class="p">)</code><code class="o">.</code><code class="n">num_blocks</code><code class="p">())</code>  <code class="c1"># -&gt; 200</code></pre></div>
<p>Blocks also control the number of files that are created when we write a Dataset to storage (so if you want all of the data to be coalesced into a single output file, you should call <code>.repartition(1)</code> before writing it).</p>
</div></section>
<section data-pdf-bookmark="Schemas and data formats" data-type="sect3"><div class="sect3" id="idm44990021483632">
<h3>Schemas and data formats</h3>
<p>Up until this point, we’ve been operating on simple Datasets made up only of integers.
However, for more complex data processing we often want to have a schema, allowing us to more easily comprehend the data and enforce types on each column.</p>
<p>Given that datasets are meant to be the point of interoperation for applications and libraries on Ray, they are designed to be agnostic to a specific datatype and offer flexibility to read, write, and convert between many popular data formats.
Datasets by supporting Arrow’s columnar format, which enables converting between different types of structured data such as Python dictionaries, DataFrames, and serialized parquet files.</p>
<p>The simplest way to create a Dataset with a schema is to create it from a list of Python dictionaries:</p>
<div data-type="example">
<h5><span class="label">Example 7-5. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="n">ds</code> <code class="o">=</code> <code class="n">ray</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">from_items</code><code class="p">([{</code><code class="s2">"id"</code><code class="p">:</code> <code class="s2">"abc"</code><code class="p">,</code> <code class="s2">"value"</code><code class="p">:</code> <code class="mi">1</code><code class="p">},</code> <code class="p">{</code><code class="s2">"id"</code><code class="p">:</code> <code class="s2">"def"</code><code class="p">,</code> <code class="s2">"value"</code><code class="p">:</code> <code class="mi">2</code><code class="p">}])</code>
<code class="nb">print</code><code class="p">(</code><code class="n">ds</code><code class="o">.</code><code class="n">schema</code><code class="p">())</code>  <code class="c1"># -&gt; id: string, value: int64</code></pre></div>
<p>In this case, the schema was inferred from the keys in the dictionaries we passed in.
We can also convert to/from data types from popular libraries such as pandas:</p>
<div data-type="example">
<h5><span class="label">Example 7-6. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="n">pandas_df</code> <code class="o">=</code> <code class="n">ds</code><code class="o">.</code><code class="n">to_pandas</code><code class="p">()</code>  <code class="c1"># pandas_df will inherit the schema from our Dataset.</code></pre></div>
<p>Here we went from a Dataset to a pandas DataFrame, but this also works in reverse: if you create a Dataset from a dataframe, it will automatically inherit the schema from the DataFrame.</p>
</div></section>
</div></section>
<section data-pdf-bookmark="Computing Over Datasets" data-type="sect2"><div class="sect2" id="idm44990021352784">
<h2>Computing Over Datasets</h2>
<p>In the section above, we introduced some of the functionality that comes built in with Ray Datasets such as filtering, sorting, and unioning.
However, one of the most powerful parts of Datasets is that they allow you to harness the flexible compute model of Ray and perform computations efficiently over large amounts of data.</p>
<p>The primary way to perform a custom transformation on a Dataset is using <code>.map()</code>.
This allows you to pass a custom function that will be applied to the records of a Dataset.
A basic example might be to square the records of a Dataset:</p>
<div data-type="example">
<h5><span class="label">Example 7-7. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="n">ds</code> <code class="o">=</code> <code class="n">ray</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">range</code><code class="p">(</code><code class="mi">10000</code><code class="p">)</code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="n">x</code> <code class="o">**</code> <code class="mi">2</code><code class="p">)</code>
<code class="n">ds</code><code class="o">.</code><code class="n">take</code><code class="p">(</code><code class="mi">5</code><code class="p">)</code>  <code class="c1"># -&gt; [0, 1, 4, 9, 16]</code></pre></div>
<p>In this example, we passed a simple lambda function and the data we operated on was integers, but we could pass any function here and operate on structured data that supports the Arrow format.</p>
<p>We can also choose to map batches of data instead of individual records using <code>.map_batches()</code>.
There are some types of computations that are much more efficient when they’re <em>vectorized</em>, meaning that they use an algorithm or implementation that is more efficient operating on a set of items instead of one at a time.</p>
<p>Revisiting our simple example of squaring the values in the Dataset, we can rewrite it to be performed in batches and use the <code>numpy.square</code> optimized implementation instead of the naive Python implemenation:</p>
<div data-type="example">
<h5><span class="label">Example 7-8. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>


<code class="n">ds</code> <code class="o">=</code> <code class="n">ray</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">range</code><code class="p">(</code><code class="mi">10000</code><code class="p">)</code><code class="o">.</code><code class="n">map_batches</code><code class="p">(</code><code class="k">lambda</code> <code class="n">batch</code><code class="p">:</code> <code class="n">np</code><code class="o">.</code><code class="n">square</code><code class="p">(</code><code class="n">batch</code><code class="p">)</code><code class="o">.</code><code class="n">tolist</code><code class="p">())</code>
<code class="n">ds</code><code class="o">.</code><code class="n">take</code><code class="p">(</code><code class="mi">5</code><code class="p">)</code>  <code class="c1"># -&gt; [0, 1, 4, 9, 16]</code></pre></div>
<p>Vectorized computations are especially useful on GPUs when performing deep learning training or inference.
However, generally performing computations on GPUs also has significant fixed cost due to needing to load model weights or other data into the GPU RAM.
For this purpose, Datasets supports mapping data using Ray actors.
Ray actors are long lived and can hold state, as opposed to stateless Ray tasks, so we can cache expensive operations costs by running them in the actor’s constructor (such as loading a model onto a GPU).</p>
<p>To perform batch inference using Datasets, we need to pass a class instead of a function, specify that this computation should run using actors, and use <code>.map_batches()</code> so we can perform vectorized inference.
If we want this to run on a GPU, we would also pass <code>num_gpus=1</code>, which specifies that the actors running the map function each require a GPU.
Datasets will automatically autoscale a group of actors to perform the map operation.</p>
<div data-type="example">
<h5><span class="label">Example 7-9. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">load_model</code><code class="p">():</code>
    <code class="c1"># Return a dummy model just for this example.</code>
    <code class="c1"># In reality, this would likely load some model weights onto a GPU.</code>
    <code class="k">class</code> <code class="nc">DummyModel</code><code class="p">:</code>
        <code class="k">def</code> <code class="fm">__call__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">batch</code><code class="p">):</code>
            <code class="k">return</code> <code class="n">batch</code>

    <code class="k">return</code> <code class="n">DummyModel</code><code class="p">()</code>

<code class="k">class</code> <code class="nc">MLModel</code><code class="p">:</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
        <code class="c1"># load_model() will only run once per actor that's started.</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">_model</code> <code class="o">=</code> <code class="n">load_model</code><code class="p">()</code>

    <code class="k">def</code> <code class="fm">__call__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">batch</code><code class="p">):</code>
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">_model</code><code class="p">(</code><code class="n">batch</code><code class="p">)</code>


<code class="n">ds</code><code class="o">.</code><code class="n">map_batches</code><code class="p">(</code><code class="n">MLModel</code><code class="p">,</code> <code class="n">compute</code><code class="o">=</code><code class="s2">"actors"</code><code class="p">)</code></pre></div>
</div></section>
<section data-pdf-bookmark="Dataset Pipelines" data-type="sect2"><div class="sect2" id="idm44990021352528">
<h2>Dataset Pipelines</h2>
<p>By default, Dataset operations are blocking, meaning they run synchronously from start to finish and there is only a single operation happening at a time.
This pattern can be very inefficient for some workloads, however.
For example, consider the following set of Dataset transformations that might be used to do batch inference for a machine learning model:</p>
<div data-type="example">
<h5><span class="label">Example 7-10. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="n">ds</code> <code class="o">=</code> <code class="n">ray</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">read_parquet</code><code class="p">(</code><code class="s2">"s3://my_bucket/input_data"</code><code class="p">)</code>\
        <code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="n">cpu_intensive_preprocessing</code><code class="p">)</code>\
        <code class="o">.</code><code class="n">map_batches</code><code class="p">(</code><code class="n">gpu_intensive_inference</code><code class="p">,</code> <code class="n">compute</code><code class="o">=</code><code class="s2">"actors"</code><code class="p">,</code> <code class="n">num_gpus</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>\
        <code class="o">.</code><code class="n">repartition</code><code class="p">(</code><code class="mi">10</code><code class="p">)</code>\
        <code class="o">.</code><code class="n">write_parquet</code><code class="p">(</code><code class="s2">"s3://my_bucket/output_predictions"</code><code class="p">)</code></pre></div>
<p>There are five stages to this pipeline, and each of them stresses different parts of the system:
- Reading from remote storage requires ingress bandwidth to the cluster and may be limited by the throughput of the storage system.
- Preprocessing the inputs requires CPU resources.
- Vectorized inference on the model requires GPU resources.
- Repartitioning requires network bandwidth within the cluster.
- Writing to remote storage requires egress bandwidth from the cluster and may be limited by the throughput of storage once again.</p>
<figure><div class="figure" id="fig_pipeline_1">
<img alt="Inefficient Dataset Computation" height="300" src="assets/data_pipeline_1.png"/>
<h6><span class="label">Figure 7-1. </span>A naive Dataset computation, leading to idle resources between stages</h6>
</div></figure>
<p>In this scenario, it would likely be much more efficient to instead <em>pipeline</em> the the stages and allow them to overlap.
This means that as soon as some data has been read from storage, it is fed into the preprocessing stage, then to the inference stage, and so on.</p>
<figure><div class="figure" id="fig_pipeline_2">
<img alt="Optimized DatasetPipeline" height="300" src="assets/data_pipeline_2.png"/>
<h6><span class="label">Figure 7-2. </span>An optimized DatasetPipeline that enables overlapping compute between stages and reduces idle resources</h6>
</div></figure>
<p>This pipelining will improve the overall resource usage of the end-to-end workload, improving throughput and therefore decreasing the cost it takes to run the computation (fewer idle resources is better!).</p>
<p>Datasets can be converted to <a href="https://docs.ray.io/en/latest/data/dataset-pipeline.xhtml">DatasetPipelines</a> using <code>ds.window()</code>, enabling the pipelining behavior that we want in this scenario.
A window specifies the number of blocks that will be passed through a stage in the pipeline before being passed to the next stage in the pipeline.
This behavior can be tuned using the <code>blocks_per_window</code> parameter, which defaults to 10.</p>
<p>Let’s rewrite the inefficient pseudocode above to use a DatasetPipeline instead:</p>
<div data-type="example">
<h5><span class="label">Example 7-11. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="n">ds</code> <code class="o">=</code> <code class="n">ray</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">read_parquet</code><code class="p">(</code><code class="s2">"s3://my_bucket/input_data"</code><code class="p">)</code>\
        <code class="o">.</code><code class="n">window</code><code class="p">(</code><code class="n">blocks_per_window</code><code class="o">=</code><code class="mi">5</code><code class="p">)</code>\
        <code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="n">cpu_intensive_preprocessing</code><code class="p">)</code>\
        <code class="o">.</code><code class="n">map_batches</code><code class="p">(</code><code class="n">gpu_intensive_inference</code><code class="p">,</code> <code class="n">compute</code><code class="o">=</code><code class="s2">"actors"</code><code class="p">,</code> <code class="n">num_gpus</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>\
        <code class="o">.</code><code class="n">repartition</code><code class="p">(</code><code class="mi">10</code><code class="p">)</code>\
        <code class="o">.</code><code class="n">write_parquet</code><code class="p">(</code><code class="s2">"s3://my_bucket/output_predictions"</code><code class="p">)</code></pre></div>
<p>The only modification made was the addition of a <code>.window()</code> call after <code>read_parquet</code> and before the preprocessing stage.
Now the Dataset has been converted to a DatasetPipeline and its stages will proceed in parallel in 5-block windows, decreasing idle resources and improving efficiency.</p>
<p>DatasetPipelines can also be created using <code>ds.repeat()</code> to repeat stages in a pipeline a finite or infinite number of times.
This will be explored further in the next section, where we’ll use it for a training workload.
Of course, pipelining can be equally beneficial for performance for training in addition to inference.</p>
</div></section>
<section data-pdf-bookmark="Example: Parallel SGD from Scratch" data-type="sect2"><div class="sect2" id="idm44990021196048">
<h2>Example: Parallel SGD from Scratch</h2>
<p>One of the key benefits of Datasets is that they can be passed between tasks and actors.
In this section, we’ll explore how this can be used to write efficient implementations of complex distributed workloads like distributed hyperparameter tuning and machine learning training.</p>
<p>As discussed in <a data-type="xref" href="ch05.xhtml#chapter_05">Chapter 5</a>, a common pattern in machine learning training is to explore a range of “hyperparameters” to find the ones that result in the best model.
We may want to run across a wide range of hyperparameters, and doing this naively could be very expensive.
Datasets allows us to easily share the same in-memory data across a range of parallel training runs in a single Python script: we can load and preprocess the data once, then pass a reference to it to many downstream actors who can read the data from shared memory.</p>
<p>Additionally, sometimes when working with very large data sets it can be infeasible to load the full training data into memory in a single process or on a single machine.
It’s common in distributed training to instead shard the data across many different workers to train in parallel and combining the results either synchronously or asynchronously using a parameter server.
There are some important considerations that can make this tricky:
1. Many distributed training algorithms take a <em>synchronous</em> approach, requiring the workers to synchronize their weights after each training epoch. This means there needs to be some coordination between the workers to maintain consistency beween which batch of data they are operating on.
2. It’s important that each worker gets a random sample of the data during each epoch. A global random shuffle has been shown to perform better than local shuffle or no shuffle.</p>
<p>Let’s walk through an example of how we can implement this type of pattern using Ray Datasets.
In the example, we will training multiple copies of an SDG classifier using different hyperparameters across different workers in parallel.
This isn’t exactly the same, but it is a similar pattern that focuses on the flexibility and power of Ray Datasets for ML training workloads.</p>
<p>We’ll be training a scikit-learn <code>SGDClassifier</code> on a generated binary classification dataset and the hyperparameter we’ll tune is the regularization term (alpha value).
The actual details of the ML task and model aren’t too important to this example, you could replace the model and data with any number of examples.
The main thing to focus on here is how we orchestrate the data loading and computation using Datasets.</p>
<p>First, let’s define our <code>TrainingWorker</code> that will train a copy of the classifier on the data:</p>
<div data-type="example">
<h5><span class="label">Example 7-12. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>
<code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">SGDClassifier</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>

<code class="nd">@ray</code><code class="o">.</code><code class="n">remote</code>
<code class="k">class</code> <code class="nc">TrainingWorker</code><code class="p">:</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">alpha</code><code class="p">:</code> <code class="nb">float</code><code class="p">):</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">_model</code> <code class="o">=</code> <code class="n">SGDClassifier</code><code class="p">(</code><code class="n">alpha</code><code class="o">=</code><code class="n">alpha</code><code class="p">)</code>

    <code class="k">def</code> <code class="nf">train</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">train_shard</code><code class="p">:</code> <code class="n">ray</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">Dataset</code><code class="p">):</code>
        <code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">train_shard</code><code class="o">.</code><code class="n">iter_epochs</code><code class="p">()):</code>
            <code class="n">X</code><code class="p">,</code> <code class="n">Y</code> <code class="o">=</code> <code class="nb">zip</code><code class="p">(</code><code class="o">*</code><code class="nb">list</code><code class="p">(</code><code class="n">epoch</code><code class="o">.</code><code class="n">iter_rows</code><code class="p">()))</code>
            <code class="bp">self</code><code class="o">.</code><code class="n">_model</code><code class="o">.</code><code class="n">partial_fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">Y</code><code class="p">,</code> <code class="n">classes</code><code class="o">=</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">])</code>

        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">_model</code>

    <code class="k">def</code> <code class="nf">test</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">X_test</code><code class="p">:</code> <code class="n">np</code><code class="o">.</code><code class="n">ndarray</code><code class="p">,</code> <code class="n">Y_test</code><code class="p">:</code> <code class="n">np</code><code class="o">.</code><code class="n">ndarray</code><code class="p">):</code>
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">_model</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">Y_test</code><code class="p">)</code></pre></div>
<p>There are a few important things to note about the <code>TrainingWorker</code>:
- It’s a simple wrapper around the <code>SGDClassifier</code> and instantiates it with a given alpha value.
- The main training function happens in the <code>train</code> method. For each epoch, it trains the classifier on the data available.
- We also have a <code>test</code> method that can be used to run the trained model against a testing set.</p>
<p>Now, let’s instantiate a number of `TrainingWorker`s with different hyperparameters (alpha values):</p>
<div data-type="example">
<h5><span class="label">Example 7-13. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="n">ALPHA_VALS</code> <code class="o">=</code> <code class="p">[</code><code class="mf">0.00008</code><code class="p">,</code> <code class="mf">0.00009</code><code class="p">,</code> <code class="mf">0.0001</code><code class="p">,</code> <code class="mf">0.00011</code><code class="p">,</code> <code class="mf">0.00012</code><code class="p">]</code>

<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Starting </code><code class="si">{</code><code class="nb">len</code><code class="p">(</code><code class="n">ALPHA_VALS</code><code class="p">)</code><code class="si">}</code><code class="s2"> training workers."</code><code class="p">)</code>
<code class="n">workers</code> <code class="o">=</code> <code class="p">[</code><code class="n">TrainingWorker</code><code class="o">.</code><code class="n">remote</code><code class="p">(</code><code class="n">alpha</code><code class="p">)</code> <code class="k">for</code> <code class="n">alpha</code> <code class="ow">in</code> <code class="n">ALPHA_VALS</code><code class="p">]</code></pre></div>
<p>Next, we generate training and validation data and convert the training data to a Dataset.
Here, we’re using <code>.repeat()</code> to create a DatasetPipeline.
This defines the number of epochs that our training will run for.
In each epoch, the subsequent operations will be applied to the Dataset and the workers will be able to iterate over the resulting data.
We also shuffle the data randomly and shard it to be passed to the training workers, each getting an equal chunk.</p>
<div data-type="example">
<h5><span class="label">Example 7-14. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Generate training &amp; validation data for a classification problem.</code>
<code class="n">X_train</code><code class="p">,</code> <code class="n">X_test</code><code class="p">,</code> <code class="n">Y_train</code><code class="p">,</code> <code class="n">Y_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code><code class="o">*</code><code class="n">datasets</code><code class="o">.</code><code class="n">make_classification</code><code class="p">())</code>

<code class="c1"># Create a dataset pipeline out of the training data. The data will be randomly</code>
<code class="c1"># shuffled and split across the workers for 10 iterations.</code>
<code class="n">train_ds</code> <code class="o">=</code> <code class="n">ray</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">from_items</code><code class="p">(</code><code class="nb">list</code><code class="p">(</code><code class="nb">zip</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">Y_train</code><code class="p">)))</code>
<code class="n">shards</code> <code class="o">=</code> <code class="n">train_ds</code><code class="o">.</code><code class="n">repeat</code><code class="p">(</code><code class="mi">10</code><code class="p">)</code>\
                 <code class="o">.</code><code class="n">random_shuffle_each_window</code><code class="p">()</code>\
                 <code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">workers</code><code class="p">),</code> <code class="n">locality_hints</code><code class="o">=</code><code class="n">workers</code><code class="p">)</code></pre></div>
<p>To run the training on the workers, we invoke their <code>train</code> method and pass in one shard of the DatasetPipeline to each.
We then block waiting for training to complete across all of the workers.
To summarize what happens during this phase:
- Each epoch, each worker gets a random shard of the data.
- The worker trains its local model on the shard of data assigned to it.
- Once a worker has finished training on the current shard, it blocks until the other workers have finished.
- The above repeats for the remaining epochs (in this case, 10 total).</p>
<div data-type="example">
<h5><span class="label">Example 7-15. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Wait for training to complete on all of the workers.</code>
<code class="n">ray</code><code class="o">.</code><code class="n">get</code><code class="p">([</code><code class="n">worker</code><code class="o">.</code><code class="n">train</code><code class="o">.</code><code class="n">remote</code><code class="p">(</code><code class="n">shard</code><code class="p">)</code> <code class="k">for</code> <code class="n">worker</code><code class="p">,</code> <code class="n">shard</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">workers</code><code class="p">,</code> <code class="n">shards</code><code class="p">)])</code></pre></div>
<p>Finally, we can test out the trained models from each worker on some test data to determine which alpha value produced the most accurate model.</p>
<div data-type="example">
<h5><span class="label">Example 7-16. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Get validation results from each worker.</code>
<code class="nb">print</code><code class="p">(</code><code class="n">ray</code><code class="o">.</code><code class="n">get</code><code class="p">([</code><code class="n">worker</code><code class="o">.</code><code class="n">test</code><code class="o">.</code><code class="n">remote</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">Y_test</code><code class="p">)</code> <code class="k">for</code> <code class="n">worker</code> <code class="ow">in</code> <code class="n">workers</code><code class="p">]))</code></pre></div>
<p>While this not be a real-world ML task and in reality you should likely reach for Ray Tune or Ray Train, this example conveys the power of Ray Datasets, especially for machine learning workloads.
In just a few 10s of lines of Python code, we were able to implement a complex distributed hyperparameter tuning and training workflow that could easily be scaled up to 10s or 100s of machines and is agnostic to any framework or specific ML task.</p>
</div></section>
</div></section>
<section data-pdf-bookmark="External Library Integrations" data-type="sect1"><div class="sect1" id="idm44990020505136">
<h1>External Library Integrations</h1>
<section data-pdf-bookmark="Overview" data-type="sect2"><div class="sect2" id="idm44990020504256">
<h2>Overview</h2>
<p>While Ray Datasets supports a number of common data processing functionalities out of the box, as mentioned above it’s not a replacement for full data processing systems.
Instead, it’s more focused on performing “last mile” processing such as basic data loading, cleaning, and featurization before ML training or inference.</p>
<figure><div class="figure" id="fig_positioning_1">
<img alt="Ray for last-mile data processing" height="300" src="assets/data_positioning_1.svg"/>
<h6><span class="label">Figure 7-3. </span>A typical workflow using Ray for machine learning: use external systems for primary data processing and ETL, use Ray Datasets for last-mile preprocessing</h6>
</div></figure>
<p>However, there are a number of other more fully-featured DataFrame and relational data processing systems that integrate with Ray:
- Dask on Ray
- RayDP (Spark on Ray)
- Modin (Pandas on Ray)
- Mars on Ray</p>
<p>These are all standalone data processing libraries that you may be familiar with outside the context of Ray.
Each of these tools has an integration with Ray core that enables more expressive data processing than comes with the built-in Datasets while still using Ray’s deployment tooling, scalable scheduling, and shared memory object store for exchanging data.</p>
<figure><div class="figure" id="fig_positioning_2">
<img alt="Ray with ecosystem integrations" height="300" src="assets/data_positioning_1.svg"/>
<h6><span class="label">Figure 7-4. </span>The benefit of Ray data ecosystem integrations, enabling more expressive data processing on Ray. These libraries integrate with Ray Datasets to feed into downstream libraries such as Ray Train.</h6>
</div></figure>
<p>For the purposes of this book, we’ll explore Dask on Ray in slightly more depth to give you a feel for what these integrations look like.
If you’re interested in the details of a specific integration, please see the latest <a href="https://docs.ray.io/en/latest/data/dask-on-ray.xhtml">Ray documentation</a> for up-to-date information.</p>
</div></section>
<section data-pdf-bookmark="Dask on Ray" data-type="sect2"><div class="sect2" id="idm44990020496160">
<h2>Dask on Ray</h2>
<p>To follow along with the examples in this section, please install Ray and Dask:</p>
<pre data-type="programlisting">pip install ray["data"]==1.9.0 dask</pre>
<p>[Dask](<a href="https://dask.org/"><em class="hyperlink">https://dask.org/</em></a>) is a Python library for parallel computing that is specifically target at scaling analytics and scientific computing workloads to a cluster.
One of the most popular features of Dask is <a href="https://docs.dask.org/en/stable/dataframe.xhtml">Dask DataFrames</a>, which offers a subset of the pandas DataFrame API that can be scaled to a cluster of machines in cases where processing in memory on a single node is not feasible.
DataFrames work by creating a <em>task graph</em> that is submitted to a scheduler for execution.
The most typical way to execute Dask DataFrames operations is using the Dask Distributed scheduler, but there is also a pluggable API that allows other schedulers to execute these task graphs as well.</p>
<p>Ray comes packaged with a Dask scheduler backend, allowing Dask DataFrame task graphs to be executed as Ray tasks and therefore make use of the Ray scheduler and shared memory object store.
This doesn’t require modifying the core DataFrames code at all; instead, in order to run using Ray all you need to do is first connect to a running Ray cluster (or run Ray locally) and then enable the Ray scheduler backend:</p>
<div data-type="example">
<h5><span class="label">Example 7-17. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">ray</code>
<code class="kn">from</code> <code class="nn">ray.util.dask</code> <code class="kn">import</code> <code class="n">enable_dask_on_ray</code>

<code class="n">ray</code><code class="o">.</code><code class="n">init</code><code class="p">()</code>  <code class="c1"># Start or connect to Ray.</code>
<code class="n">enable_dask_on_ray</code><code class="p">()</code>  <code class="c1"># Enable the Ray scheduler backend for Dask.</code></pre></div>
<p>Now we can run regular Dask DataFrames code and have it scaled across the Ray cluster.
For example, we might want to do some time series analysis using standard DataFrame operations like filter, groupby, and computing the standard deviation (example taken from Dask documentation).</p>
<div data-type="example">
<h5><span class="label">Example 7-18. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">dask</code>

<code class="n">df</code> <code class="o">=</code> <code class="n">dask</code><code class="o">.</code><code class="n">datasets</code><code class="o">.</code><code class="n">timeseries</code><code class="p">()</code>
<code class="n">df</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="n">df</code><code class="o">.</code><code class="n">y</code> <code class="o">&gt;</code> <code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">groupby</code><code class="p">(</code><code class="s2">"name"</code><code class="p">)</code><code class="o">.</code><code class="n">x</code><code class="o">.</code><code class="n">std</code><code class="p">()</code>
<code class="n">df</code><code class="o">.</code><code class="n">compute</code><code class="p">()</code>  <code class="c1"># Trigger the task graph to be evaluated.</code></pre></div>
<p>If you’re used to pandas or other DataFrame libraries, you might wonder why we need to call <code>df.compute()</code>.
This is because Dask is <em>lazy</em> by default, and will only compute results on demand, allowing it to optimize the task graph that will be executed across the cluster.</p>
<p>One of the most powerful aspects of the is that it integrates very nicely with Ray Datasets.
We can convert a Ray Dataset to a Dask DataFrame and vice versa using built-in utilities:</p>
<div data-type="example">
<h5><span class="label">Example 7-19. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">ray</code>
<code class="n">ds</code> <code class="o">=</code> <code class="n">ray</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">range</code><code class="p">(</code><code class="mi">10000</code><code class="p">)</code>

<code class="c1"># Convert the Dataset to a Dask DataFrame.</code>
<code class="n">df</code> <code class="o">=</code> <code class="n">ds</code><code class="o">.</code><code class="n">to_dask</code><code class="p">()</code>
<code class="nb">print</code><code class="p">(</code><code class="n">df</code><code class="o">.</code><code class="n">std</code><code class="p">()</code><code class="o">.</code><code class="n">compute</code><code class="p">())</code>  <code class="c1"># -&gt; 2886.89568</code>

<code class="c1"># Convert the Dask DataFrame back to a Dataset.</code>
<code class="n">ds</code> <code class="o">=</code> <code class="n">ray</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">from_dask</code><code class="p">(</code><code class="n">df</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">ds</code><code class="o">.</code><code class="n">std</code><code class="p">())</code>  <code class="c1"># -&gt; 2886.89568</code></pre></div>
<p>This simple example might not look too impressive because we’re able to compute the standard deviation using either Dask DataFrames or Ray Datasets.
However, as you’ll see in the next section when we build an end-to-end ML pipeline, this enables really powerful workflows.
For example, we can use the full expressiveness of DataFrames to do our featurization and preprocessing, then pass the data directly into downstream operations such as distributed training or inference while keeping everything in memory.
This highlights how Datasets enables a wide range of use cases on top of Ray, and how integrations like Dask on Ray make the ecosystem even more powerful.</p>
</div></section>
</div></section>
<section data-pdf-bookmark="Building an ML Pipeline" data-type="sect1"><div class="sect1" id="idm44990020495696">
<h1>Building an ML Pipeline</h1>
<p>Although we were able to we build a simple distributed training application from scratch in the previous section, there were many edge cases, opportunities for performance optimization, and usability features that we would want to address to build a real-world application.
As you’ve learned in the previous chapters about Ray RLlib, Ray Tune, and Ray Train, Ray has an ecosystem of libraries that enable us to build production-ready ML applications.
In this section, we’ll explore how to use Datasets as the “glue layer” to build an ML pipeline end-to-end.</p>
<section data-pdf-bookmark="Background" data-type="sect2"><div class="sect2" id="idm44990020338496">
<h2>Background</h2>
<p>To successfully productionize a machine learning model, one first needs to collect and catalog data using standard ETL processes.
However, that’s not the end of the story: in order to train a model, we also often need to do featurization of the data before feeding into our training process, and how we feed the data into training can make a big impact on cost and performance.
After training a model, we’ll also want to run inference across many different datasets — that’s the whole point of training the model after all!
This end-to-end process is summarized in the figure below.</p>
<p>Though this might look like just a chain of steps, in practice the data processing workflow for machine learning is an iterative process of experimentation to define the right set of features and train a high-performing model on them.
Efficiently loading, transforming, and feeding the data into training and inference is also crucial for performance, which translates directly to cost for compute-intensive models.
Often times, implementing these ML pipelines means stitching together multiple different systems and materializing intermediate results to remote storage between the stages.
This has two major downsides:
1. First, it requires orchestrating many different systems and programs for a single workflow. This can be a lot to handle for any ML practictioner, so many people reach to workflow orchestration systems like <a href="https://airflow.apache.org/">Apache Airflow</a>. While Airflow has some great benefits, it’s also a lot of complexity to introduce (especially in development).
2. Second, running our ML workflow across multiple different systems means we need to read from and write to storage between each stage. This incurs significant overhead and cost due to data transfer and serialization.</p>
<p>In contrast, using Ray we are able to build a complete machine learning pipeline as a single application that can be run as a single Python script.
The ecosystem of built-in and third party libraries make it possible to mix-and-match the right functionality for a gien use case and build scalable, production-ready pipelines.
Ray Datasets acts as the glue layer, enabling us to efficiently load, preprocess, and compute over the data while avoiding expensive serialization costs and keeping intermediate data in shared memory.</p>
</div></section>
<section data-pdf-bookmark="End-to-End Example: Predicting Big Tips in Nyc Taxi Rides" data-type="sect2"><div class="sect2" id="idm44990020335424">
<h2>End-to-End Example: Predicting Big Tips in Nyc Taxi Rides</h2>
<p>This section walks through a practical, end-to-end example of building a deep learning pipeline using Ray.
We will build a binary classification model to predict if a taxi ride will result in a big tip (&gt;20% of the fare) using the public <a href="https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page">New York City Taxi and Limousine Commission (TLC) Trip Record Data</a>.
Our workflow will closely resemble that of a typical ML practitioner:
- First, we will load the data, do some basic preprocessing, and compute features we’ll use in our model.
- Then, we will define a neural network and train it using distributed data-parallel training.
- Finally, we will apply the trained neural network to a fresh batch of data.</p>
<p>The example will use Dask on Ray and train a PyTorch neural network, but note that nothing here is specific to either of those libraries, Ray Datasets and Ray Train can be used with a wide range of popular machine learning tools.
To follow along with the example code in this section, please install Ray, PyTorch, and Dask:</p>
<pre data-type="programlisting">pip install ray["data"]==1.9.0 torch dask</pre>
<p>In the examples below, we’ll be loading the data from local disk to make it easy to run the examples on your machine.
You can download the data to your local machine from the <a href="https://registry.opendata.aws/nyc-tlc-trip-records-pds/">AWS Registry of Open Data</a> using the AWS CLI:</p>
<pre data-type="programlisting">pip install awscli==1.22.1
aws s3 cp --no-sign-request "s3://nyc-tlc/trip data/" ./nyc_tlc_data/</pre>
<p>If you’d like to try loading the data directly from cloud storage, simply replace the local paths in the examples with the corresponding S3 URL.</p>
<section data-pdf-bookmark="Loading, preprocessing, and featurizing with dask on Ray" data-type="sect3"><div class="sect3" id="idm44990020329392">
<h3>Loading, preprocessing, and featurizing with dask on Ray</h3>
<p>The first step in training our model is to load and preprocess it.
To do this, we’ll be using Dask on Ray, which as discussed above gives us a convenient DataFrames API and the ability to scale up the preprocessing across a cluster and efficiently pass it into our training and inference operations.</p>
<p>Below is our code for preprocessing and featurization:</p>
<div data-type="example">
<h5><span class="label">Example 7-20. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">ray</code>
<code class="kn">from</code> <code class="nn">ray.util.dask</code> <code class="kn">import</code> <code class="n">enable_dask_on_ray</code>

<code class="kn">import</code> <code class="nn">dask.dataframe</code> <code class="k">as</code> <code class="nn">dd</code>

<code class="n">LABEL_COLUMN</code> <code class="o">=</code> <code class="s2">"is_big_tip"</code>

<code class="n">enable_dask_on_ray</code><code class="p">()</code>


<code class="k">def</code> <code class="nf">load_dataset</code><code class="p">(</code><code class="n">path</code><code class="p">:</code> <code class="nb">str</code><code class="p">,</code> <code class="o">*</code><code class="p">,</code> <code class="n">include_label</code><code class="o">=</code><code class="kc">True</code><code class="p">):</code>
    <code class="c1"># Load the data and drop unused columns.</code>
    <code class="n">df</code> <code class="o">=</code> <code class="n">dd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code><code class="n">path</code><code class="p">,</code> <code class="n">assume_missing</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
                     <code class="n">usecols</code><code class="o">=</code><code class="p">[</code><code class="s2">"tpep_pickup_datetime"</code><code class="p">,</code> <code class="s2">"tpep_dropoff_datetime"</code><code class="p">,</code>
                              <code class="s2">"passenger_count"</code><code class="p">,</code> <code class="s2">"trip_distance"</code><code class="p">,</code> <code class="s2">"fare_amount"</code><code class="p">,</code>
                              <code class="s2">"tip_amount"</code><code class="p">])</code>

    <code class="c1"># Basic cleaning, drop nulls and outliers.</code>
    <code class="n">df</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="n">dropna</code><code class="p">()</code>
    <code class="n">df</code> <code class="o">=</code> <code class="n">df</code><code class="p">[(</code><code class="n">df</code><code class="p">[</code><code class="s2">"passenger_count"</code><code class="p">]</code> <code class="o">&lt;=</code> <code class="mi">4</code><code class="p">)</code> <code class="o">&amp;</code>
            <code class="p">(</code><code class="n">df</code><code class="p">[</code><code class="s2">"trip_distance"</code><code class="p">]</code> <code class="o">&lt;</code> <code class="mi">100</code><code class="p">)</code> <code class="o">&amp;</code>
            <code class="p">(</code><code class="n">df</code><code class="p">[</code><code class="s2">"fare_amount"</code><code class="p">]</code> <code class="o">&lt;</code> <code class="mi">1000</code><code class="p">)]</code>

    <code class="c1"># Convert datetime strings to datetime objects.</code>
    <code class="n">df</code><code class="p">[</code><code class="s2">"tpep_pickup_datetime"</code><code class="p">]</code> <code class="o">=</code> <code class="n">dd</code><code class="o">.</code><code class="n">to_datetime</code><code class="p">(</code><code class="n">df</code><code class="p">[</code><code class="s2">"tpep_pickup_datetime"</code><code class="p">])</code>
    <code class="n">df</code><code class="p">[</code><code class="s2">"tpep_dropoff_datetime"</code><code class="p">]</code> <code class="o">=</code> <code class="n">dd</code><code class="o">.</code><code class="n">to_datetime</code><code class="p">(</code><code class="n">df</code><code class="p">[</code><code class="s2">"tpep_dropoff_datetime"</code><code class="p">])</code>

    <code class="c1"># Add three new features: trip duration, hour the trip started, and day of the week.</code>
    <code class="n">df</code><code class="p">[</code><code class="s2">"trip_duration"</code><code class="p">]</code> <code class="o">=</code> <code class="p">(</code><code class="n">df</code><code class="p">[</code><code class="s2">"tpep_dropoff_datetime"</code><code class="p">]</code> <code class="o">-</code>
                           <code class="n">df</code><code class="p">[</code><code class="s2">"tpep_pickup_datetime"</code><code class="p">])</code><code class="o">.</code><code class="n">dt</code><code class="o">.</code><code class="n">seconds</code>
    <code class="n">df</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="n">df</code><code class="p">[</code><code class="s2">"trip_duration"</code><code class="p">]</code> <code class="o">&lt;</code> <code class="mi">4</code> <code class="o">*</code> <code class="mi">60</code> <code class="o">*</code> <code class="mi">60</code><code class="p">]</code> <code class="c1"># 4 hours.</code>
    <code class="n">df</code><code class="p">[</code><code class="s2">"hour"</code><code class="p">]</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="s2">"tpep_pickup_datetime"</code><code class="p">]</code><code class="o">.</code><code class="n">dt</code><code class="o">.</code><code class="n">hour</code>
    <code class="n">df</code><code class="p">[</code><code class="s2">"day_of_week"</code><code class="p">]</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="s2">"tpep_pickup_datetime"</code><code class="p">]</code><code class="o">.</code><code class="n">dt</code><code class="o">.</code><code class="n">weekday</code>

    <code class="k">if</code> <code class="n">include_label</code><code class="p">:</code>
        <code class="c1"># Calculate label column: if tip was more or less than 20% of the fare.</code>
        <code class="n">df</code><code class="p">[</code><code class="n">LABEL_COLUMN</code><code class="p">]</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="s2">"tip_amount"</code><code class="p">]</code> <code class="o">&gt;</code> <code class="mf">0.2</code> <code class="o">*</code> <code class="n">df</code><code class="p">[</code><code class="s2">"fare_amount"</code><code class="p">]</code>

    <code class="c1"># Drop unused columns.</code>
    <code class="n">df</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code>
        <code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s2">"tpep_pickup_datetime"</code><code class="p">,</code> <code class="s2">"tpep_dropoff_datetime"</code><code class="p">,</code> <code class="s2">"tip_amount"</code><code class="p">]</code>
    <code class="p">)</code>

    <code class="k">return</code> <code class="n">ray</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">from_dask</code><code class="p">(</code><code class="n">df</code><code class="p">)</code></pre></div>
<p>This involves basic data loading and cleaning (dropping nulls and outliers) as well as transforming some columns into a format that can be used as features in our machine learning model.
For instance, we transform the pickup and dropoff datetimes, which are provided as a string, into three numerical features: <code>trip_duration</code>, <code>hour</code>, and <code>day_of_week</code>.
This is made easy by Dask’s built-in support for <a href="https://docs.python.org/3/library/datetime.xhtml">Python datetime utilites</a>.
If this data is going to be used for training, we also need to compute the label column (whether the tip was more or less than 20% of the fare amount).</p>
<p>Finally, once we’ve computed our preprocessed Dask DataFrame, we transform it into a Ray Dataset so we can pass it into our training and inference processes later.</p>
</div></section>
<section data-pdf-bookmark="Defining a PyTorch model" data-type="sect3"><div class="sect3" id="idm44990019996768">
<h3>Defining a PyTorch model</h3>
<p>Now that we’ve cleaned and prepared the data, we need to define a model architecture that we’ll use for the model.
In practice, this would likely be an iterative process and involve researching the state of the art for similar problems.
For the sake of our example, we’ll keep things simple and use use a basic PyTorch neural network.
The neural network has three linear transformations starting with the dimension of our feature vector and then outputs a value between 0 and 1 using a Sigmoid activation function.
This output value will be rounded to produce the binary prediction of if the ride will result in a big tip or not.</p>
<div data-type="example">
<h5><span class="label">Example 7-21. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">import</code> <code class="nn">torch.nn</code> <code class="k">as</code> <code class="nn">nn</code>
<code class="kn">import</code> <code class="nn">torch.nn.functional</code> <code class="k">as</code> <code class="nn">F</code>

<code class="n">NUM_FEATURES</code> <code class="o">=</code> <code class="mi">6</code>


<code class="k">class</code> <code class="nc">FarePredictor</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">()</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>

        <code class="bp">self</code><code class="o">.</code><code class="n">fc1</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">NUM_FEATURES</code><code class="p">,</code> <code class="mi">256</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">fc2</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">256</code><code class="p">,</code> <code class="mi">16</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">fc3</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">16</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>

        <code class="bp">self</code><code class="o">.</code><code class="n">bn1</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">BatchNorm1d</code><code class="p">(</code><code class="mi">256</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">bn2</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">BatchNorm1d</code><code class="p">(</code><code class="mi">16</code><code class="p">)</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="o">*</code><code class="n">x</code><code class="p">):</code>
        <code class="n">x</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">cat</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">dim</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
        <code class="n">x</code> <code class="o">=</code> <code class="n">F</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">fc1</code><code class="p">(</code><code class="n">x</code><code class="p">))</code>
        <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">bn1</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
        <code class="n">x</code> <code class="o">=</code> <code class="n">F</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">fc2</code><code class="p">(</code><code class="n">x</code><code class="p">))</code>
        <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">bn2</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
        <code class="n">x</code> <code class="o">=</code> <code class="n">F</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">fc3</code><code class="p">(</code><code class="n">x</code><code class="p">))</code>

        <code class="k">return</code> <code class="n">x</code></pre></div>
</div></section>
<section data-pdf-bookmark="Distributed training with Ray Train" data-type="sect3"><div class="sect3" id="idm44990020178320">
<h3>Distributed training with Ray Train</h3>
<p>Now that we’ve defined the neural network architecture, we need a way to efficiently train this on our data.
This data set is very large (hundreds of gigabytes in total), so our best bet is probably to perform distributed data-parallel training.
We will use Ray Train, which you learned about in <a data-type="xref" href="ch06.xhtml#chapter_06">Chapter 6</a>, to define a scalable training process that will use <a href="https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.xhtml">PyTorch DataParallel</a> under the hood.</p>
<p>The first thing we need to do is define the logic that will happen to train on a batch of data on each worker in each epoch.
This will take in a local shard of the full dataset, run it through the local copy of the model, and perform backpropagation.</p>
<div data-type="example">
<h5><span class="label">Example 7-22. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">ray.train</code> <code class="k">as</code> <code class="nn">train</code>


<code class="k">def</code> <code class="nf">train_epoch</code><code class="p">(</code><code class="n">iterable_dataset</code><code class="p">,</code> <code class="n">model</code><code class="p">,</code> <code class="n">loss_fn</code><code class="p">,</code> <code class="n">optimizer</code><code class="p">,</code> <code class="n">device</code><code class="p">):</code>
    <code class="n">model</code><code class="o">.</code><code class="n">train</code><code class="p">()</code>
    <code class="k">for</code> <code class="n">X</code><code class="p">,</code> <code class="n">y</code> <code class="ow">in</code> <code class="n">iterable_dataset</code><code class="p">:</code>
        <code class="n">X</code> <code class="o">=</code> <code class="n">X</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code>
        <code class="n">y</code> <code class="o">=</code> <code class="n">y</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code>

        <code class="c1"># Compute prediction error.</code>
        <code class="n">pred</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">round</code><code class="p">(</code><code class="n">model</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">float</code><code class="p">()))</code>
        <code class="n">loss</code> <code class="o">=</code> <code class="n">loss_fn</code><code class="p">(</code><code class="n">pred</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>

        <code class="c1"># Backpropagation.</code>
        <code class="n">optimizer</code><code class="o">.</code><code class="n">zero_grad</code><code class="p">()</code>
        <code class="n">loss</code><code class="o">.</code><code class="n">backward</code><code class="p">()</code>
        <code class="n">optimizer</code><code class="o">.</code><code class="n">step</code><code class="p">()</code></pre></div>
<p>Next, we also need to define the logic for each worker to validate its current copy of the model in each epoch.
This will run a local batch of data through the model, compare the predictions to the actual label values, and compute the subsequent loss using a provided loss function.</p>
<div data-type="example">
<h5><span class="label">Example 7-23. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">validate_epoch</code><code class="p">(</code><code class="n">iterable_dataset</code><code class="p">,</code> <code class="n">model</code><code class="p">,</code> <code class="n">loss_fn</code><code class="p">,</code> <code class="n">device</code><code class="p">):</code>
    <code class="n">num_batches</code> <code class="o">=</code> <code class="mi">0</code>
    <code class="n">model</code><code class="o">.</code><code class="n">eval</code><code class="p">()</code>
    <code class="n">loss</code> <code class="o">=</code> <code class="mi">0</code>
    <code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
        <code class="k">for</code> <code class="n">X</code><code class="p">,</code> <code class="n">y</code> <code class="ow">in</code> <code class="n">iterable_dataset</code><code class="p">:</code>
            <code class="n">X</code> <code class="o">=</code> <code class="n">X</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code>
            <code class="n">y</code> <code class="o">=</code> <code class="n">y</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code>
            <code class="n">num_batches</code> <code class="o">+=</code> <code class="mi">1</code>
            <code class="n">pred</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">round</code><code class="p">(</code><code class="n">model</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">float</code><code class="p">()))</code>
            <code class="n">loss</code> <code class="o">+=</code> <code class="n">loss_fn</code><code class="p">(</code><code class="n">pred</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code><code class="o">.</code><code class="n">item</code><code class="p">()</code>
    <code class="n">loss</code> <code class="o">/=</code> <code class="n">num_batches</code>
    <code class="n">result</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"loss"</code><code class="p">:</code> <code class="n">loss</code><code class="p">}</code>
    <code class="k">return</code> <code class="n">result</code></pre></div>
<p>Finally, we define the core training logic.
This will take in a variety of configuration options (such as a batch size and other model hyperparameters), instantiate the model, loss function, and optimizer, and then run the core training loop.
In each epoch, each worker will get its shard of the training and validation datasets, convert it to a local <a href="https://pytorch.org/docs/stable/data.xhtml">PyTorch Dataset</a>, and run the validation and training code we defined above.
After each epoch, the worker will using Ray Train utilities to report the result and save the current model weights for use later.</p>
<div data-type="example">
<h5><span class="label">Example 7-24. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">train_func</code><code class="p">(</code><code class="n">config</code><code class="p">):</code>
    <code class="n">batch_size</code> <code class="o">=</code> <code class="n">config</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s2">"batch_size"</code><code class="p">,</code> <code class="mi">32</code><code class="p">)</code>
    <code class="n">lr</code> <code class="o">=</code> <code class="n">config</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s2">"lr"</code><code class="p">,</code> <code class="mf">1e-2</code><code class="p">)</code>
    <code class="n">epochs</code> <code class="o">=</code> <code class="n">config</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s2">"epochs"</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code>

    <code class="n">train_dataset_pipeline_shard</code> <code class="o">=</code> <code class="n">train</code><code class="o">.</code><code class="n">get_dataset_shard</code><code class="p">(</code><code class="s2">"train"</code><code class="p">)</code>
    <code class="n">validation_dataset_pipeline_shard</code> <code class="o">=</code> <code class="n">train</code><code class="o">.</code><code class="n">get_dataset_shard</code><code class="p">(</code><code class="s2">"validation"</code><code class="p">)</code>

    <code class="n">model</code> <code class="o">=</code> <code class="n">train</code><code class="o">.</code><code class="n">torch</code><code class="o">.</code><code class="n">prepare_model</code><code class="p">(</code><code class="n">FarePredictor</code><code class="p">())</code>

    <code class="n">loss_fn</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">SmoothL1Loss</code><code class="p">()</code>
    <code class="n">optimizer</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">optim</code><code class="o">.</code><code class="n">Adam</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">parameters</code><code class="p">(),</code> <code class="n">lr</code><code class="o">=</code><code class="n">lr</code><code class="p">)</code>

    <code class="n">train_dataset_iterator</code> <code class="o">=</code> <code class="n">train_dataset_pipeline_shard</code><code class="o">.</code><code class="n">iter_epochs</code><code class="p">()</code>
    <code class="n">validation_dataset_iterator</code> <code class="o">=</code> \
        <code class="n">validation_dataset_pipeline_shard</code><code class="o">.</code><code class="n">iter_epochs</code><code class="p">()</code>

    <code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">epochs</code><code class="p">):</code>
        <code class="n">train_dataset</code> <code class="o">=</code> <code class="nb">next</code><code class="p">(</code><code class="n">train_dataset_iterator</code><code class="p">)</code>
        <code class="n">validation_dataset</code> <code class="o">=</code> <code class="nb">next</code><code class="p">(</code><code class="n">validation_dataset_iterator</code><code class="p">)</code>

        <code class="n">train_torch_dataset</code> <code class="o">=</code> <code class="n">train_dataset</code><code class="o">.</code><code class="n">to_torch</code><code class="p">(</code>
            <code class="n">label_column</code><code class="o">=</code><code class="n">LABEL_COLUMN</code><code class="p">,</code>
            <code class="n">batch_size</code><code class="o">=</code><code class="n">batch_size</code><code class="p">,</code>
        <code class="p">)</code>
        <code class="n">validation_torch_dataset</code> <code class="o">=</code> <code class="n">validation_dataset</code><code class="o">.</code><code class="n">to_torch</code><code class="p">(</code>
            <code class="n">label_column</code><code class="o">=</code><code class="n">LABEL_COLUMN</code><code class="p">,</code>
            <code class="n">batch_size</code><code class="o">=</code><code class="n">batch_size</code><code class="p">)</code>

        <code class="n">device</code> <code class="o">=</code> <code class="n">train</code><code class="o">.</code><code class="n">torch</code><code class="o">.</code><code class="n">get_device</code><code class="p">()</code>

        <code class="n">train_epoch</code><code class="p">(</code><code class="n">train_torch_dataset</code><code class="p">,</code> <code class="n">model</code><code class="p">,</code> <code class="n">loss_fn</code><code class="p">,</code> <code class="n">optimizer</code><code class="p">,</code> <code class="n">device</code><code class="p">)</code>
        <code class="n">result</code> <code class="o">=</code> <code class="n">validate_epoch</code><code class="p">(</code><code class="n">validation_torch_dataset</code><code class="p">,</code> <code class="n">model</code><code class="p">,</code> <code class="n">loss_fn</code><code class="p">,</code>
                                <code class="n">device</code><code class="p">)</code>
        <code class="n">train</code><code class="o">.</code><code class="n">report</code><code class="p">(</code><code class="o">**</code><code class="n">result</code><code class="p">)</code>
        <code class="n">train</code><code class="o">.</code><code class="n">save_checkpoint</code><code class="p">(</code><code class="n">epoch</code><code class="o">=</code><code class="n">epoch</code><code class="p">,</code> <code class="n">model_weights</code><code class="o">=</code><code class="n">model</code><code class="o">.</code><code class="n">module</code><code class="o">.</code><code class="n">state_dict</code><code class="p">())</code></pre></div>
<p>Now that the full training process has been defined, we need to load the training and validation data to feed into our training workers.
Here, we call the <code>load_dataset</code> function we defined earlier that will do preprocessing and featurization, then split the dataset into a training and validation Dataset<sup><a data-type="noteref" href="ch07.xhtml#idm44990019385840" id="idm44990019385840-marker">1</a></sup>.
Finally, we want to convert both Datasets into <a href="https://docs.ray.io/en/latest/data/dataset-pipeline.xhtml">Dataset Pipelines</a> for efficiency and make sure that the training dataset is globally shuffled between all of the workers in each epoch.</p>
<div data-type="example">
<h5><span class="label">Example 7-25. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">get_training_datasets</code><code class="p">(</code><code class="o">*</code><code class="p">,</code> <code class="n">test_pct</code><code class="o">=</code><code class="mf">0.8</code><code class="p">):</code>
    <code class="n">ds</code> <code class="o">=</code> <code class="n">load_dataset</code><code class="p">(</code><code class="s2">"nyc_tlc_data/yellow_tripdata_2020-01.csv"</code><code class="p">)</code>
    <code class="n">ds</code><code class="p">,</code> <code class="n">_</code> <code class="o">=</code> <code class="n">ds</code><code class="o">.</code><code class="n">split_at_indices</code><code class="p">([</code><code class="nb">int</code><code class="p">(</code><code class="mf">0.01</code> <code class="o">*</code> <code class="n">ds</code><code class="o">.</code><code class="n">count</code><code class="p">())])</code>
    <code class="n">train_ds</code><code class="p">,</code> <code class="n">test_ds</code> <code class="o">=</code> <code class="n">ds</code><code class="o">.</code><code class="n">split_at_indices</code><code class="p">([</code><code class="nb">int</code><code class="p">(</code><code class="n">test_pct</code> <code class="o">*</code> <code class="n">ds</code><code class="o">.</code><code class="n">count</code><code class="p">())])</code>
    <code class="n">train_ds_pipeline</code> <code class="o">=</code> <code class="n">train_ds</code><code class="o">.</code><code class="n">repeat</code><code class="p">()</code><code class="o">.</code><code class="n">random_shuffle_each_window</code><code class="p">()</code>
    <code class="n">test_ds_pipeline</code> <code class="o">=</code> <code class="n">test_ds</code><code class="o">.</code><code class="n">repeat</code><code class="p">()</code>
    <code class="k">return</code> <code class="p">{</code><code class="s2">"train"</code><code class="p">:</code> <code class="n">train_ds_pipeline</code><code class="p">,</code> <code class="s2">"validation"</code><code class="p">:</code> <code class="n">test_ds_pipeline</code><code class="p">}</code></pre></div>
<p>Everything is set, and it’s time to run our distributed training process!
All that’s left is to create a Trainer, pass in our Datasets, and let the training scale up and run across the configured number of workers.
After training has completed, we fetch the latest model weights using the checkpoint API.</p>
<div data-type="example">
<h5><span class="label">Example 7-26. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="n">trainer</code> <code class="o">=</code> <code class="n">train</code><code class="o">.</code><code class="n">Trainer</code><code class="p">(</code><code class="s2">"torch"</code><code class="p">,</code> <code class="n">num_workers</code><code class="o">=</code><code class="mi">4</code><code class="p">)</code>
<code class="n">config</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"lr"</code><code class="p">:</code> <code class="mf">1e-2</code><code class="p">,</code> <code class="s2">"epochs"</code><code class="p">:</code> <code class="mi">3</code><code class="p">,</code> <code class="s2">"batch_size"</code><code class="p">:</code> <code class="mi">64</code><code class="p">}</code>
<code class="n">trainer</code><code class="o">.</code><code class="n">start</code><code class="p">()</code>
<code class="n">trainer</code><code class="o">.</code><code class="n">run</code><code class="p">(</code><code class="n">train_func</code><code class="p">,</code> <code class="n">config</code><code class="p">,</code> <code class="n">dataset</code><code class="o">=</code><code class="n">get_training_datasets</code><code class="p">())</code>
<code class="n">model_weights</code> <code class="o">=</code> <code class="n">trainer</code><code class="o">.</code><code class="n">latest_checkpoint</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s2">"model_weights"</code><code class="p">)</code>
<code class="n">trainer</code><code class="o">.</code><code class="n">shutdown</code><code class="p">()</code></pre></div>
</div></section>
<section data-pdf-bookmark="Distributed batch inference with Ray datasets" data-type="sect3"><div class="sect3" id="idm44990020177728">
<h3>Distributed batch inference with Ray datasets</h3>
<p>Once we’ve trained a model and gotten the best accuracy that we can, the next step is to actually apply it in practice.
Sometimes this means powering a low-latency service, which we’ll explore in <a data-type="xref" href="ch08.xhtml#chapter_08">Chapter 8</a>, but often the task is to apply the model across batches of data as they come in.</p>
<p>Let’s use the trained model weights from the training process above and apply them across a new batch of data (in this case, it’ll just be another chunk of the same public data set).
To do this, first we need to load, preprocess, and featurize the data in the same way we did for training.
Then we will load our model and <code>map</code> it across the whole data set.
As discussed in the section above, Datasets allows us to do this efficiently with Ray Actors, even using GPUs just by changing one parameter.
We simply wrap our trained model weights in a class that will load them and configure a model for inference, then call <code>map_batches</code> and pass in the inference model class:</p>
<div data-type="example">
<h5><span class="label">Example 7-27. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="k">class</code> <code class="nc">InferenceWrapper</code><code class="p">:</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">_model</code> <code class="o">=</code> <code class="n">FarePredictor</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">_model</code><code class="o">.</code><code class="n">load_state_dict</code><code class="p">(</code><code class="n">model_weights</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">_model</code><code class="o">.</code><code class="n">eval</code><code class="p">()</code>

    <code class="k">def</code> <code class="fm">__call__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">df</code><code class="p">):</code>
        <code class="n">tensor</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">as_tensor</code><code class="p">(</code><code class="n">df</code><code class="o">.</code><code class="n">to_numpy</code><code class="p">(),</code> <code class="n">dtype</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">float32</code><code class="p">)</code>
        <code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
            <code class="n">predictions</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">round</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">_model</code><code class="p">(</code><code class="n">tensor</code><code class="p">))</code>
        <code class="n">df</code><code class="p">[</code><code class="n">LABEL_COLUMN</code><code class="p">]</code> <code class="o">=</code> <code class="n">predictions</code><code class="o">.</code><code class="n">numpy</code><code class="p">()</code>
        <code class="k">return</code> <code class="n">df</code>


<code class="n">ds</code> <code class="o">=</code> <code class="n">load_dataset</code><code class="p">(</code><code class="s2">"nyc_tlc_data/yellow_tripdata_2021-01.csv"</code><code class="p">,</code> <code class="n">include_label</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code>
<code class="n">ds</code><code class="o">.</code><code class="n">map_batches</code><code class="p">(</code><code class="n">InferenceWrapper</code><code class="p">,</code> <code class="n">compute</code><code class="o">=</code><code class="s2">"actors"</code><code class="p">)</code><code class="o">.</code><code class="n">write_csv</code><code class="p">(</code><code class="s2">"output"</code><code class="p">)</code></pre></div>
</div></section>
</div></section>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="idm44990019385840"><sup><a href="ch07.xhtml#idm44990019385840-marker">1</a></sup> The code only loads a subset of the data for testing, to test at scale use all partitions of the data when calling <code>load_dataset</code> and increase <code>num_workers</code> when training the model.</p></div></div></section></div></body></html>