<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 1. An Overview of Ray" data-type="chapter" epub:type="chapter"><div class="chapter" id="chapter_01">
<h1><span class="label">Chapter 1. </span>An Overview of Ray</h1>
<blockquote>
<p>A distributed system is one in which the failure of a computer you didn’t even know
existed can render your own computer unusable.</p>
<p data-type="attribution">Leslie Lamport</p>
</blockquote>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm44990045363168">
<h5>A Note for Early Release Readers</h5>
<p>With Early Release ebooks, you get books in their earliest form—the author’s raw and unedited content as they write—so you can take advantage of these technologies long before the official release of these titles.</p>
</div></aside>
<p>One of the reasons we need efficient distributed computing is that we’re collecting ever more data with a large variety at increasing speeds.
The storage systems, data processing and analytics engines that have emerged in the last decade are crucially important to the success of many companies.
Interestingly, most “big data” technologies are built for and operated by (data) engineers, that are in charge of data collection and processing tasks.
The rationale is to free up data scientists to do what they’re best at.
As a data science practitioner you might want to focus on training complex machine learning models, running efficient hyperparameter selection, building entirely new and custom models or simulations, or serving your models to showcase them.
At the same time you simply might <em>have to</em> scale them to a compute cluster.
To do that, the distributed system of your choice needs to support all of these fine-grained “big compute” tasks, potentially on specialized hardware.
Ideally, it also fits into the big data tool chain you’re using and is fast enough to meet your latency requirements.
In other words, distributed computing has to be powerful and flexible enough for complex data science workloads — and Ray can help you with that.</p>
<p>Python is likely the most popular language for data science today, and it’s certainly the one I find the most useful for my daily work.
By now it’s over 30 years old, but has a still growing and active community.
The rich <a href="https://pydata.org/">PyData ecosystem</a> is an essential part of a data scientist’s toolbox.
How can you make sure to scale out your workloads while still leveraging the tools you need?
That’s a difficult problem, especially since communities can’t be forced to just toss their toolbox, or programming language.
That means distributed computing tools for data science have to be built for their existing community.</p>
<section data-pdf-bookmark="What Is Ray?" data-type="sect1"><div class="sect1" id="idm44990045348496">
<h1>What Is Ray?</h1>
<p>What I like about Ray is that it checks all the above boxes.
It’s a flexible distributed computing framework build for the Python data science community.
Ray is easy to get started and keeps simple things simple.
Its core API is as lean as it gets and helps you reason effectively about the distributed programs you want to write.
You can efficiently parallelize Python programs on your laptop, and run the code you tested locally on a cluster practically without any changes.
Its high-level libraries are easy to configure and can seamlessly be used together.
Some of them, like Ray’s reinforcement learning library, would have a bright future as standalone projects, distributed or not.
While Ray’s core is built in C++, it’s been a Python-first framework since day one, integrates with many important data science tools, and can count on a growing ecosystem.</p>
<p>Distributed Python is not new, and Ray is not the first framework in this space (nor will it be the last), but it is special in what it has to offer.
Ray is particularly strong when you combine several of its modules and have custom, machine learning heavy workloads that would be difficult to implement otherwise.
It makes distributed computing easy enough to run your complex workloads flexibly by leveraging the Python tools you know and want to use.
In other words, by <em>learning Ray</em> you get to know <em>flexible distributed Python for data science</em>.</p>
<p>In this chapter you’ll get a first glimpse at what Ray can do for you.
We will discuss the three layers that make up Ray, namely its core engine, its high-level libraries and its ecosystem.
Throughout the chapter we’ll show you first code examples to give you a feel for Ray,
but we defer any in-depth treatment of Ray’s APIs and components to later chapters.
You can view this chapter as an overview of the whole book as well.</p>
</div></section>
<section data-pdf-bookmark="What Led to Ray?" data-type="sect1"><div class="sect1" id="idm44990045369760">
<h1>What Led to Ray?</h1>
<p>Programming distributed systems is hard.
It requires specific knowledge and experience you might not have.
Ideally, such systems get out of your way and provide abstractions to let you focus on your job.
But in practice “all non-trivial abstractions, to some degree, are leaky” (<a href="https://www.joelonsoftware.com/2002/11/11/the-law-of-leaky-abstractions/">Spolsky</a>), and getting clusters of computers to do what you want is undoubtedly difficult.
Many software systems require resources that far exceed what single servers can do.
Even if one server was enough, modern systems need to be failsafe and provide features like high availability.
That means your applications might have to run on multiple machines, or even datacenters, just to make sure they’re running reliably.</p>
<p>Even if you’re not too familiar with machine learning (ML) or more generally artificial intelligence (AI) as such, you must have heard of recent breakthroughs in the field.
To name just two, systems like <a href="https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology">Deepmind’s AlphaFold</a> for solving the protein folding problem, or <a href="https://openai.com/blog/openai-codex/">OpenAI’s Codex</a> that’s helping
software developers with the tedious parts of their job, have made the news lately.
You might also have heard that ML systems generally require large amounts of data to be trained.
OpenAI has shown exponential growth in compute needed to train AI models in their paper <a href="https://openai.com/blog/ai-and-compute/">“AI and Compute”</a>.
The operations needed for AI systems in their study is measured in petaflops (thousands of trillion operations per second), and has been <em>doubling every 3.4 months</em> since 2012.</p>
<p>Compare this to Moore’s Law<sup><a data-type="noteref" href="ch01.xhtml#idm44990045517696" id="idm44990045517696-marker">1</a></sup>, which states that the number of transistors in computers would double every two years.
Even if you’re bullish on Moore’s law, you can see how there’s a clear need for distributed computing in ML.
You should also understand that many tasks in ML can be naturally decomposed to run in parallel.
So, why not speed things up if you can?</p>
<p>Distributed computing is generally perceived as hard.
But why is that?
Shouldn’t it be realistic to find good abstractions to run your code on clusters, without having to constantly think about individual machines and how they interoperate?
What if we specifically focused on AI workloads?</p>
<p>Researchers at <a href="https://rise.cs.berkeley.edu/">RISELab</a> at UC Berkeley created Ray to address these questions.
None of the tools existing at the time met their needs.
They were looking for easy ways to speed up their workloads by distributing them to compute clusters.
The workloads they had in mind were quite flexible in nature and didn’t fit into the analytics engines available.
At the same time, RISELab wanted to build a system that took care of how the work was distributed.
With reasonable default behaviors in place, researchers should be able to focus on their work.
And ideally they should have access to all their favorite tools in Python.
For this reason, Ray was built with an emphasis on high-performance and heterogeneous workloads.
<a href="https://www.anyscale.com/">Anyscale</a>, the company behind Ray, is building a managed Ray Platform and offers hosted solutions for your Ray applications.
Let’s have a look at an example of what kinds of applications Ray was designed for.</p>
<section data-pdf-bookmark="Flexible Workloads in Python and Reinforcement Learning" data-type="sect2"><div class="sect2" id="idm44990044261984">
<h2>Flexible Workloads in Python and Reinforcement Learning</h2>
<p>One of my favorite apps on my phone can automatically classify or “label” individual plants in our garden.
It works by simply showing it a picture of the plant in question.
That’s immensely helpful, as I’m terrible at distinguishing them all.
(I’m not bragging about the size of my garden, I’m just bad at it.)
In the last couple of years we’ve seen a surge of impressive applications like that.</p>
<p>Ultimately, the promise of AI is to build intelligent agents that go far beyond classifying objects.
Imagine an AI application that not only knows your plants, but can take care of to them, too.
Such an application would have to</p>
<ul>
<li>
<p>Operate in dynamic environments (like the change of seasons)</p>
</li>
<li>
<p>React to changes in the environment (like a heavy storm or pests attacking your plants)</p>
</li>
<li>
<p>Take sequences of actions (like watering and fertilizing plants)</p>
</li>
<li>
<p>Accomplish long-term goals (like prioritizing plant health)</p>
</li>
</ul>
<p>By observing its environment such an AI would also learn to explore the possible actions it could take and come up with better solutions over time.
If you feel like this example is artificial or too far out, it’s not difficult to come up with examples on your own that share all the above requirements.
Think of managing and optimizing a supply chain, strategically restocking a warehouse considering fluctuating demands, or orchestrating the processing steps in an assembly line.
Another famous example of what you could expect from an AI would be Stephen Wozniak’s famous “Coffee Test”.
If you’re invited to a friend’s house, you can navigate to the kitchen, spot the coffee machine and all necessary ingredients, figure out how to brew a cup of coffee, and sit down to enjoy it.
A machine should be able to do the same, except the last part might be a bit of a stretch.
What other examples can you think of?</p>
<p>You can frame all the above requirements naturally in a subfield of machine learning called reinforcement learning (RL).
We’ve dedicated all of <a data-type="xref" href="ch04.xhtml#chapter_04">Chapter 4</a> to RL.
For now, it’s enough to understand that it’s about agents interacting with their environment by observing it and emitting actions.
In RL, agents evaluate their environments by attributing a reward (e.g., how healthy is my plant on a scale from 1 to 10).
The term “reinforcement” comes from the fact that agents will hopefully learn to seek out behaviour that leads to good outcomes (high reward), and shy away from punishing situations (low or negative reward).
The interaction of agents with their environment is usually modeled by creating a computer simulation of it.
These simulations can become complicated quite quickly, as you might imagine from the examples we’ve given.</p>
<p>We don’t have gardening robots like the one I’ve sketched yet.
And we don’t know which AI paradigm will get us there.<sup><a data-type="noteref" href="ch01.xhtml#idm44990045384464" id="idm44990045384464-marker">2</a></sup>
What I do know is that the world is full of complex, dynamic and interesting examples that we need to tackle.
For that we need computational frameworks that help us do that, and Ray was built to do exactly that.
RISELab created Ray to build and run complex AI applications at scale, and reinforcement learning has been an integral part of Ray from the start.</p>
</div></section>
<section data-pdf-bookmark="Three Layers: Core, Libraries and Ecosystem" data-type="sect2"><div class="sect2" id="idm44990045277936">
<h2>Three Layers: Core, Libraries and Ecosystem</h2>
<p>Now that you know why Ray was built and what its creators had in mind, let’s look at the three layers of Ray.</p>
<ul>
<li>
<p>A low-level, distributed computing framework for Python with a concise core API.<sup><a data-type="noteref" href="ch01.xhtml#idm44990045356496" id="idm44990045356496-marker">3</a></sup></p>
</li>
<li>
<p>A set of high-level libraries for data science built and maintained by the creators of Ray.</p>
</li>
<li>
<p>A growing ecosystem of integrations and partnerships with other notable projects.</p>
</li>
</ul>
<p>There’s a lot to unpack here, and we’ll look into each of these layers individually in the remainder of this chapter.
You can imagine Ray’s core engine with its API at the center of things, on which everything else builds.
Ray’s data science libraries build on top of it.
In practice, most data scientists will use these higher level libraries directly and won’t often need to resort to the core API.
The growing number of third-party integrations for Ray is another great entrypoint for experienced practitioners.
Let’s look into each one of the layers one by one.</p>
</div></section>
</div></section>
<section data-pdf-bookmark="A Distributed Computing Framework" data-type="sect1"><div class="sect1" id="idm44990045369392">
<h1>A Distributed Computing Framework</h1>
<p>At its core, Ray is a distributed computing framework.
We’ll  provide you with just the basic terminology here, and talk about Ray’s architecture in depth in <a data-type="xref" href="ch02.xhtml#chapter_02">Chapter 2</a>.
In short, Ray sets up and manages clusters of computers so that you can run distributed tasks on them.
A ray cluster consists of nodes that are connected to each other via a network.
You program against the so-called <em>driver</em>, the program root, which lives on the <em>head node</em>.
The driver can run <em>jobs</em>, that is a collection of tasks, that are run on the nodes in the cluster.
Specifically, the individual tasks of a job are run on <em>worker</em> processes on <em>worker nodes</em>.
Figure <a data-type="xref" href="#fig_simple_cluster">Figure 1-1</a> illustrates the basic structure of a Ray cluster.</p>
<figure><div class="figure" id="fig_simple_cluster">
<img alt="Ray cluster schematics" height="336" src="assets/simple_cluster.png" width="662"/>
<h6><span class="label">Figure 1-1. </span>The basic components of a Ray cluster</h6>
</div></figure>
<p>What’s interesting is that a Ray cluster can also be a <em>local cluster</em>, i.e. a cluster consisting just of your own computer.
In this case, there’s just one node, namely the head node, which has the driver process and some worker processes.
The default number of worker processes is the number of CPUs available on your machine.</p>
<p>With that knowledge at hand, it’s time to get your hands dirty and run your first local Ray cluster.
Installing Ray<sup><a data-type="noteref" href="ch01.xhtml#idm44990042817344" id="idm44990042817344-marker">4</a></sup>
on any of the major operating systems should work seamlessly using <code>pip</code>:</p>
<pre data-type="programlisting">pip install "ray[rllib, serve, tune]"==1.9.0</pre>
<p>With a simple <code>pip install ray</code> you would have installed just the very basics of Ray.
Since we want to explore some advanced features, we installed the “extras” <code>rllib</code>, <code>serve</code> and <code>tune</code>, which we’ll discuss in a bit.
Depending on your system configuration you may not need the quotation marks in the above installation command.</p>
<p>Next, go ahead and start a Python session.
You could use the <code>ipython</code> interpreter, which I find to be the most suitable environment for following along simple examples.
If you don’t feel like typing in the commands yourself, you can also jump into the <a href="https://github.com/maxpumperla/learning_ray/blob/main/notebooks/ch_01_overview.ipynb">jupyter notebook for this chapter</a> and run the code there.
The choice is up to you, but in any case please remember to use Python version <code>3.7</code> or later.
In your Python session you can now easily import and initialize Ray as follows:</p>
<div data-type="example">
<h5><span class="label">Example 1-1. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">ray</code>
<code class="n">ray</code><code class="o">.</code><code class="n">init</code><code class="p">()</code></pre></div>
<p>With those two lines of code you’ve started a Ray cluster on your local machine.
This cluster can utilize all the cores available on your computer as workers.
In this case you didn’t provide any arguments to the <code>init</code> function.
If you wanted to run Ray on a “real” cluster, you’d have to pass more arguments to <code>init</code>.
The rest of your code would stay the same.</p>
<p>After running this code you should see output of the following form (we use ellipses to remove the clutter):</p>
<pre data-type="programlisting">... INFO services.py:1263 -- View the Ray dashboard at http://127.0.0.1:8265
{'node_ip_address': '192.168.1.41',
 'raylet_ip_address': '192.168.1.41',
 'redis_address': '192.168.1.41:6379',
 'object_store_address': '.../sockets/plasma_store',
 'raylet_socket_name': '.../sockets/raylet',
 'webui_url': '127.0.0.1:8265',
 'session_dir': '...',
 'metrics_export_port': 61794,
 'node_id': '...'}</pre>
<p>This indicates that your Ray cluster is up and running.
As you can see from the first line of the output, Ray comes with its own, pre-packaged dashboard.
In all likelihood you can check it out at <a href="http://127.0.0.1:8265"><em class="hyperlink">http://127.0.0.1:8265</em></a>, unless your output shows a different port.
If you want you can take your time to explore the dashboard for a little.
For instance, you should see all your CPU cores listed and the total utilization of your (trivial) Ray application.
We’ll come back to the dashboard in later chapters.</p>
<p>We’re not quite ready to dive into all the details of a Ray cluster here.
To jump ahead just a little, you might see the <code>raylet_ip_address</code>, which is a reference to a so-called <em>Raylet</em>, which is responsible for scheduling tasks on your worker nodes.
Each Raylet has a store for distributed objects, which is hinted at by the <code>object_store_address</code> above.
Once tasks are scheduled, they get executed by worker processes.
In <a data-type="xref" href="ch02.xhtml#chapter_02">Chapter 2</a> you’ll get a much better understanding of all these components and how they make up a Ray cluster.</p>
<p>Before moving on, we should also briefly mention that the Ray core API is very accessible and easy to use.
But since it is also a rather low-level interface, it takes time to build interesting examples with it.
<a data-type="xref" href="ch02.xhtml#chapter_02">Chapter 2</a> has an extensive first example to get you started with the Ray core API, and in <a data-type="xref" href="ch03.xhtml#chapter_03">Chapter 3</a> you’ll see how to build a more interesting Ray application for reinforcement learning.</p>
<p>Right now your Ray cluster doesn’t do much, but that’s about to change.
After giving you a quick introduction to the data science workflow in the following section, you’ll run your first concrete Ray examples.</p>
</div></section>
<section data-pdf-bookmark="A Suite of Data Science Libraries" data-type="sect1"><div class="sect1" id="section_data_science">
<h1>A Suite of Data Science Libraries</h1>
<p>Moving on to the second layer of Ray, in this section we’ll introduce all the data science libraries that Ray comes with.
To do so, let’s first take a bird’s eye view on what it means to do data science.
Once you understand this context, it’s much easier to place Ray’s higher-level libraries and see how they can be useful to you.
If you have a good idea of the data science process, you can safely skip ahead to section <a data-type="xref" href="#section_data_processing">“Data Processing with Ray Data”</a>.</p>
<section data-pdf-bookmark="Machine Learning and the Data Science Workflow" data-type="sect2"><div class="sect2" id="idm44990043108352">
<h2>Machine Learning and the Data Science Workflow</h2>
<p>The somewhat elusive term “data science” (DS) evolved quite a bit in recent years, and you can find many definitions of varying usefulness online.<sup><a data-type="noteref" href="ch01.xhtml#idm44990042948624" id="idm44990042948624-marker">5</a></sup>
To me, it’s <em>the practice of gaining insights and building real-world applications by leveraging data</em>.
That’s quite a broad definition, and you don’t have to agree with me.
My point is that data science is an inherently practical and applied field that centers around building and understanding things, which makes fairly little sense in a <em>purely</em> academic context.
In that sense, describing practitioners of this field as “data scientists” is about as bad of a misnomer as describing hackers as “computer scientists”.<sup><a data-type="noteref" href="ch01.xhtml#idm44990044482384" id="idm44990044482384-marker">6</a></sup></p>
<p>Since you are familiar with Python and hopefully bring a certain craftsmanship attitude with you, we can approach the Ray’s data science libraries from a very pragmatic angle.
Doing data science in practice is an iterative process that goes something like this:</p>
<dl>
<dt>Requirements engineering</dt>
<dd>
<p>You talk to stakeholders to identify the problems you need to solve and clarify the requirements for this project.</p>
</dd>
<dt>Data collection</dt>
<dd>
<p>Then you source, collect and inspect the data.</p>
</dd>
<dt>Data processing</dt>
<dd>
<p>Afterwards you process the data such that you can tackle the problem.</p>
</dd>
<dt>Model building</dt>
<dd>
<p>You then move on to build a model (in the broadest sense) using the data. That could be a dashboard with important metrics, a visualisation, or a machine learning model, among many other things.</p>
</dd>
<dt>Model evaluation</dt>
<dd>
<p>The next step is to evaluate your model against the requirements in the first step.</p>
</dd>
<dt>Deployment</dt>
<dd>
<p>If all goes well (it likely doesn’t), you deploy your solution in a production environment. You should understand this as an ongoing process that needs to be monitored, not as a one-off step.</p>
</dd>
</dl>
<p>Otherwise, you need to circle back and start from the top. The most likely outcome is that you need to improve your solution in various ways, even after initial deployment.</p>
<p>Machine learning is not necessarily part of this process, but you can see how building smart applications or gaining insights might benefit from ML.
Building a face detection app into your social media platform, for better or worse, might be one example of that. When the data science process just described explicitly involves building machine learning models, you can further specify some steps:</p>
<dl>
<dt>Data processing</dt>
<dd>
<p>To train machine learning models, you need data in a format that is understood by your ML model. The process of transforming and selecting what data should be fed into your model is often called <em>feature engineering</em>. This step can be messy. You’ll benefit a lot if you can rely on common tools to do the job.</p>
</dd>
<dt>Model training</dt>
<dd>
<p>In ML you need to train your algorithms on data that got processed in the last step. This includes selecting the right algorithm for the job, and it helps if you can choose from a wide variety.</p>
</dd>
<dt>Hyperparameter tuning</dt>
<dd>
<p>Machine learning models have parameters that are tuned in the model training step. Most ML models also have another set of parameters, called <em>hyperparameters</em> that can be modified prior to training. These parameters can heavily influence the performance of your resulting ML model and need to be tuned properly. There are good tools to help automate that process.</p>
</dd>
<dt>Model serving</dt>
<dd>
<p>Trained models need to be deployed. To serve a model means to make it available to whoever needs access by whatever means necessary. In prototypes, you often use simple HTTP servers, but there are many specialised software packages for ML model serving.</p>
</dd>
</dl>
<p>This list is by no means exhaustive.
Don’t worry if you’ve never gone through these steps or struggle with the terminology, we’ll come back to this in much more detail in later chapters.
If you want to understand more about the holistic view of the data science process when building machine learning applications, the book <a href="https://www.oreilly.com/library/view/building-machine-learning/9781492045106/">Building Machine Learning Powered Applications</a> is dedicated to it entirely.</p>
<p>Figure <a data-type="xref" href="#fig_ds_experimentation">Figure 1-2</a> gives an overview of the steps we just discussed:</p>
<figure><div class="figure" id="fig_ds_experimentation">
<img alt="Data science experimentation workflow" height="400" src="assets/ds_workflow.png"/>
<h6><span class="label">Figure 1-2. </span>An overview of the data science experimentation workflow using machine learning</h6>
</div></figure>
<p>At this point you might be wondering how any of this relates to Ray.
The good news is that Ray has a dedicated library for each of the four ML-specific tasks above, covering data processing, model training, hyperparameter tuning and model serving.
And the way Ray is designed, all these libraries are <em>distributed by construction</em>.
Let’s walk through each of them one-by-one.</p>
</div></section>
<section data-pdf-bookmark="Data Processing with Ray Data" data-type="sect2"><div class="sect2" id="section_data_processing">
<h2>Data Processing with Ray Data</h2>
<p>The first high-level library of Ray we talk about is called “Ray Data”.
This library contains a data structure aptly called <code>Dataset</code>, a multitude of connectors for loading data from various formats and systems, an API for transforming such datasets, a way to build data processing pipelines with them, and many integrations with other data processing frameworks.
The <code>Dataset</code> abstraction builds on the powerful <a href="https://arrow.apache.org/">Arrow framework</a>.</p>
<p>To use Ray Data, you need to install Arrow for Python, for instance by running <code>pip install pyarrow</code>.
We’ll now discuss a simple example that creates a distributed <code>Dataset</code> on your local Ray cluster from a Python data structure. Specifically, you’ll create a dataset from a Python dictionary containing a string <code>name</code> and an integer-valued <code>data</code> for <code>10000</code> entries:</p>
<div data-type="example">
<h5><span class="label">Example 1-2. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code><code> </code><code class="nn">ray</code><code>
</code><code>
</code><code class="n">items</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="p">{</code><code class="s2">"</code><code class="s2">name</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="nb">str</code><code class="p">(</code><code class="n">i</code><code class="p">)</code><code class="p">,</code><code> </code><code class="s2">"</code><code class="s2">data</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="n">i</code><code class="p">}</code><code> </code><code class="k">for</code><code> </code><code class="n">i</code><code> </code><code class="ow">in</code><code> </code><code class="nb">range</code><code class="p">(</code><code class="mi">10000</code><code class="p">)</code><code class="p">]</code><code>
</code><code class="n">ds</code><code> </code><code class="o">=</code><code> </code><code class="n">ray</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">from_items</code><code class="p">(</code><code class="n">items</code><code class="p">)</code><code>   </code><a class="co" href="#callout_an_overview_of_ray_CO1-1" id="co_an_overview_of_ray_CO1-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code>
</code><code class="n">ds</code><code class="o">.</code><code class="n">show</code><code class="p">(</code><code class="mi">5</code><code class="p">)</code><code>  </code><a class="co" href="#callout_an_overview_of_ray_CO1-2" id="co_an_overview_of_ray_CO1-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_an_overview_of_ray_CO1-1" id="callout_an_overview_of_ray_CO1-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>Creating a <code>Dataset</code> by using <code>from_items</code> from the <code>ray.data</code> module.</p></dd>
<dt><a class="co" href="#co_an_overview_of_ray_CO1-2" id="callout_an_overview_of_ray_CO1-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a></dt>
<dd><p>Printing the first 10 items of the <code>Dataset</code>.</p></dd>
</dl></div>
<p>To <code>show</code> a <code>Dataset</code> means to print some of its values. You should see precisely <code>5</code> so-called <code>ArrowRow</code> elements on your command line, like this:</p>
<pre data-code-language="shell" data-type="programlisting">ArrowRow<code class="o">({</code><code class="s1">'name'</code>: <code class="s1">'0'</code>, <code class="s1">'data'</code>: <code class="m">0</code><code class="o">})</code>
ArrowRow<code class="o">({</code><code class="s1">'name'</code>: <code class="s1">'1'</code>, <code class="s1">'data'</code>: <code class="m">1</code><code class="o">})</code>
ArrowRow<code class="o">({</code><code class="s1">'name'</code>: <code class="s1">'2'</code>, <code class="s1">'data'</code>: <code class="m">2</code><code class="o">})</code>
ArrowRow<code class="o">({</code><code class="s1">'name'</code>: <code class="s1">'3'</code>, <code class="s1">'data'</code>: <code class="m">3</code><code class="o">})</code>
ArrowRow<code class="o">({</code><code class="s1">'name'</code>: <code class="s1">'4'</code>, <code class="s1">'data'</code>: <code class="m">4</code><code class="o">})</code></pre>
<p>Great, now you have some distributed rows, but what can you do with that data?
The <code>Dataset</code> API bets heavily on functional programming, as it is very well suited for data transformations.
Even though Python 3 made a point of hiding some of its functional programming capabilities, you’re probably familiar with functionality such as <code>map</code>, <code>filter</code> and others.
If not, it’s easy enough to pick up.
<code>map</code> takes each element of your dataset and transforms is into something else, in parallel.
<code>filter</code> removes data points according to a boolean filter function.
And the slightly more elaborate <code>flat_map</code> first maps values similarly to <code>map</code>, but then also “flattens” the result.
For instance, if <code>map</code> would produce a list of lists, <code>flat_map</code> would flatten out the nested lists and give you just a list.
Equipped with these three functional API calls, let’s see how easily you can transform your dataset <code>ds</code>:</p>
<div data-type="example" id="ray_data_transform">
<h5><span class="label">Example 1-3. </span>Transforming a <code>Dataset</code> with common functional programming routines</h5>
<pre data-code-language="python" data-type="programlisting"><code class="n">squares</code><code> </code><code class="o">=</code><code> </code><code class="n">ds</code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="k">lambda</code><code> </code><code class="n">x</code><code class="p">:</code><code> </code><code class="n">x</code><code class="p">[</code><code class="s2">"</code><code class="s2">data</code><code class="s2">"</code><code class="p">]</code><code> </code><code class="o">*</code><code class="o">*</code><code> </code><code class="mi">2</code><code class="p">)</code><code>  </code><a class="co" href="#callout_an_overview_of_ray_CO2-1" id="co_an_overview_of_ray_CO2-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code>
</code><code>
</code><code class="n">evens</code><code> </code><code class="o">=</code><code> </code><code class="n">squares</code><code class="o">.</code><code class="n">filter</code><code class="p">(</code><code class="k">lambda</code><code> </code><code class="n">x</code><code class="p">:</code><code> </code><code class="n">x</code><code> </code><code class="o">%</code><code> </code><code class="mi">2</code><code> </code><code class="o">==</code><code> </code><code class="mi">0</code><code class="p">)</code><code>  </code><a class="co" href="#callout_an_overview_of_ray_CO2-2" id="co_an_overview_of_ray_CO2-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a><code>
</code><code class="n">evens</code><code class="o">.</code><code class="n">count</code><code class="p">(</code><code class="p">)</code><code>
</code><code>
</code><code class="n">cubes</code><code> </code><code class="o">=</code><code> </code><code class="n">evens</code><code class="o">.</code><code class="n">flat_map</code><code class="p">(</code><code class="k">lambda</code><code> </code><code class="n">x</code><code class="p">:</code><code> </code><code class="p">[</code><code class="n">x</code><code class="p">,</code><code> </code><code class="n">x</code><code class="o">*</code><code class="o">*</code><code class="mi">3</code><code class="p">]</code><code class="p">)</code><code>  </code><a class="co" href="#callout_an_overview_of_ray_CO2-3" id="co_an_overview_of_ray_CO2-3"><img alt="3" height="12" src="assets/3.png" width="12"/></a><code>
</code><code class="n">sample</code><code> </code><code class="o">=</code><code> </code><code class="n">cubes</code><code class="o">.</code><code class="n">take</code><code class="p">(</code><code class="mi">10</code><code class="p">)</code><code>  </code><a class="co" href="#callout_an_overview_of_ray_CO2-4" id="co_an_overview_of_ray_CO2-4"><img alt="4" height="12" src="assets/4.png" width="12"/></a><code>
</code><code class="nb">print</code><code class="p">(</code><code class="n">sample</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_an_overview_of_ray_CO2-1" id="callout_an_overview_of_ray_CO2-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>We <code>map</code> each row of <code>ds</code> to only keep the square value of its <code>data</code> entry.</p></dd>
<dt><a class="co" href="#co_an_overview_of_ray_CO2-2" id="callout_an_overview_of_ray_CO2-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a></dt>
<dd><p>Then we <code>filter</code> the <code>squares</code> to only keep even numbers (a total of 5000 elements).</p></dd>
<dt><a class="co" href="#co_an_overview_of_ray_CO2-3" id="callout_an_overview_of_ray_CO2-3"><img alt="3" height="12" src="assets/3.png" width="12"/></a></dt>
<dd><p>We then use <code>flat_map</code> to augment the remaining values with their respective cubes.</p></dd>
<dt><a class="co" href="#co_an_overview_of_ray_CO2-4" id="callout_an_overview_of_ray_CO2-4"><img alt="4" height="12" src="assets/4.png" width="12"/></a></dt>
<dd><p>To <code>take</code> a total of <code>10</code> values means to leave Ray and return a Python list with these values that we can print.</p></dd>
</dl></div>
<p>The drawback of <code>Dataset</code> transformations is that each step gets executed synchronously.
In example <a data-type="xref" href="#ray_data_transform">Example 1-3</a> this is a non-issue, but for complex tasks that e.g. mix reading files and processing data, you want an execution that can overlap individual tasks.
<code>DatasetPipeline</code> does exactly that.
Let’s rewrite the last example into a pipeline.</p>
<div data-type="example">
<h5><span class="label">Example 1-4. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="n">pipe</code><code> </code><code class="o">=</code><code> </code><code class="n">ds</code><code class="o">.</code><code class="n">window</code><code class="p">(</code><code class="p">)</code><code>  </code><a class="co" href="#callout_an_overview_of_ray_CO3-1" id="co_an_overview_of_ray_CO3-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code>
</code><code class="n">result</code><code> </code><code class="o">=</code><code> </code><code class="n">pipe</code><code>\
</code><code>    </code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="k">lambda</code><code> </code><code class="n">x</code><code class="p">:</code><code> </code><code class="n">x</code><code class="p">[</code><code class="s2">"</code><code class="s2">data</code><code class="s2">"</code><code class="p">]</code><code> </code><code class="o">*</code><code class="o">*</code><code> </code><code class="mi">2</code><code class="p">)</code><code>\
</code><code>    </code><code class="o">.</code><code class="n">filter</code><code class="p">(</code><code class="k">lambda</code><code> </code><code class="n">x</code><code class="p">:</code><code> </code><code class="n">x</code><code> </code><code class="o">%</code><code> </code><code class="mi">2</code><code> </code><code class="o">==</code><code> </code><code class="mi">0</code><code class="p">)</code><code>\
</code><code>    </code><code class="o">.</code><code class="n">flat_map</code><code class="p">(</code><code class="k">lambda</code><code> </code><code class="n">x</code><code class="p">:</code><code> </code><code class="p">[</code><code class="n">x</code><code class="p">,</code><code> </code><code class="n">x</code><code class="o">*</code><code class="o">*</code><code class="mi">3</code><code class="p">]</code><code class="p">)</code><code>  </code><a class="co" href="#callout_an_overview_of_ray_CO3-2" id="co_an_overview_of_ray_CO3-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a><code>
</code><code class="n">result</code><code class="o">.</code><code class="n">show</code><code class="p">(</code><code class="mi">10</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_an_overview_of_ray_CO3-1" id="callout_an_overview_of_ray_CO3-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>You can turn a <code>Dataset</code> into a pipeline by calling <code>.window()</code> on it.</p></dd>
<dt><a class="co" href="#co_an_overview_of_ray_CO3-2" id="callout_an_overview_of_ray_CO3-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a></dt>
<dd><p>Pipeline steps can be chained to yield the same result as before.</p></dd>
</dl></div>
<p>There’s a lot more to be said about Ray Data, especially its integration with notable data processing systems, but we’ll have to defer an in-depth discussion until <a data-type="xref" href="ch07.xhtml#chapter_07">Chapter 7</a>.</p>
</div></section>
<section data-pdf-bookmark="Model Training" data-type="sect2"><div class="sect2" id="idm44990043423824">
<h2>Model Training</h2>
<p>Moving on to the next set of libraries, let’s look at the distributed training capabilities of Ray.
For that, you have access to two libraries.
One is dedicated to reinforcement learning specifically, the other one has a different scope and is aimed primarily at supervised learning tasks.</p>
<section data-pdf-bookmark="Reinforcement learning with Ray RLlib" data-type="sect3"><div class="sect3" id="idm44990034747008">
<h3>Reinforcement learning with Ray RLlib</h3>
<p>Let’s start with <em>Ray RLlib</em> for reinforcement learning.
This library is powered by the modern ML frameworks TensorFlow and PyTorch, and you can choose which one to use.
Both frameworks seem to converge more and more conceptually, so you can pick the one you like most without losing much in the process.
Throughout the book we use TensorFlow for consistency.
Go ahead and install it with <code>pip install tensorflow</code> right now.</p>
<p>One of the easiest ways to run examples with RLlib is to use the command line tool <code>rllib</code>, which we’ve already implicitly installed earlier with <code>pip</code>.
Once you run more complex examples in <a data-type="xref" href="ch04.xhtml#chapter_04">Chapter 4</a>, you will mostly rely on its Python API, but for now we just want to get a first taste of running RL experiments.</p>
<p>We’ll look at a fairly classical control problem of balancing a pendulum.
Imagine you have a pendulum like the one in figure <a data-type="xref" href="#fig_pendulum">Figure 1-3</a>, fixed at as single point and subject to gravity.
You can manipulate that pendulum by giving it a push from the left or the right.
If you assert just the right amount of force, the pendulum might remain in an upright position.
That’s our goal - and the question is whether we can teach a reinforcement learning algorithm to do so for us.</p>
<figure><div class="figure" id="fig_pendulum">
<img alt="Pendulum" height="300" src="assets/pendulum.png"/>
<h6><span class="label">Figure 1-3. </span>Controlling a simple pendulum by asserting force to the left or the right</h6>
</div></figure>
<p>Specifically, we want to train a reinforcement learning agent that can push to the left or right,
thereby acting on its environment (manipulating the pendulum) to reach the “upright position” goal for which it will be rewarded.
To tackle this problem with Ray RLlib, store the following content in a file called <code>pendulum.yml</code>.</p>
<div data-type="example">
<h5><span class="label">Example 1-5. </span></h5>
<pre data-code-language="yaml" data-type="programlisting"><code class="c1"># pendulum.yml</code><code class="w">
</code><code class="nt">pendulumppo</code><code class="p">:</code><code class="w">
</code><code class="w">    </code><code class="nt">env</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Pendulum-v1</code><code class="w">  </code><a class="co" href="#callout_an_overview_of_ray_CO4-1" id="co_an_overview_of_ray_CO4-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code class="w">
</code><code class="w">    </code><code class="nt">run</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">PPO</code><code class="w">  </code><a class="co" href="#callout_an_overview_of_ray_CO4-2" id="co_an_overview_of_ray_CO4-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a><code class="w">
</code><code class="w">    </code><code class="nt">checkpoint_freq</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">5</code><code class="w">  </code><a class="co" href="#callout_an_overview_of_ray_CO4-3" id="co_an_overview_of_ray_CO4-3"><img alt="3" height="12" src="assets/3.png" width="12"/></a><code class="w">
</code><code class="w">    </code><code class="nt">stop</code><code class="p">:</code><code class="w">
</code><code class="w">        </code><code class="nt">episode_reward_mean</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">800</code><code class="w">  </code><a class="co" href="#callout_an_overview_of_ray_CO4-4" id="co_an_overview_of_ray_CO4-4"><img alt="4" height="12" src="assets/4.png" width="12"/></a><code class="w">
</code><code class="w">    </code><code class="nt">config</code><code class="p">:</code><code class="w">
</code><code class="w">        </code><code class="nt">lambda</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">0.1</code><code class="w">  </code><a class="co" href="#callout_an_overview_of_ray_CO4-5" id="co_an_overview_of_ray_CO4-5"><img alt="5" height="12" src="assets/5.png" width="12"/></a><code class="w">
</code><code class="w">        </code><code class="nt">gamma</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">0.95</code><code class="w">
</code><code class="w">        </code><code class="nt">lr</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">0.0003</code><code class="w">
</code><code class="w">        </code><code class="nt">num_sgd_iter</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">6</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_an_overview_of_ray_CO4-1" id="callout_an_overview_of_ray_CO4-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>The <code>Pendulum-v1</code> environment simulates the pendulum problem we just described.</p></dd>
<dt><a class="co" href="#co_an_overview_of_ray_CO4-2" id="callout_an_overview_of_ray_CO4-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a></dt>
<dd><p>We use a powerful RL algorithm called Proximal Policy Optimization, or PPO.</p></dd>
<dt><a class="co" href="#co_an_overview_of_ray_CO4-3" id="callout_an_overview_of_ray_CO4-3"><img alt="3" height="12" src="assets/3.png" width="12"/></a></dt>
<dd><p>After every five “training iterations” we checkpoint a model.</p></dd>
<dt><a class="co" href="#co_an_overview_of_ray_CO4-4" id="callout_an_overview_of_ray_CO4-4"><img alt="4" height="12" src="assets/4.png" width="12"/></a></dt>
<dd><p>Once we reach a reward of <code>-800</code> , we stop the experiment.</p></dd>
<dt><a class="co" href="#co_an_overview_of_ray_CO4-5" id="callout_an_overview_of_ray_CO4-5"><img alt="5" height="12" src="assets/5.png" width="12"/></a></dt>
<dd><p>The PPO needs some RL-specific configuration to make it work for this problem.</p></dd>
</dl></div>
<p>The details of this configuration file don’t matter much at this point, don’t get distracted by them.
The important part is that you specify the built-in <code>Pendulum-v1</code> environment and sufficient RL-specific configuration to ensure the training procedure works.
The configuration is a simplified version of one of Ray’s <a href="https://github.com/ray-project/ray/tree/master/rllib/tuned_examples">tuned examples</a>.
We chose this one because it doesn’t require any special hardware and finishes in a matter of minutes.
If your computer is powerful enough, you can try to run the tuned example as well, which should yield much better results.
To train this pendulum example you can now simply run:</p>
<pre data-code-language="shell" data-type="programlisting">rllib train -f pendulum.yml</pre>
<p>If you want, you can check the output of this Ray program and see how the different metrics evolve during the training procedure.
In case you don’t want to create this file on your own, and want to run an experiment which gives you much better results, you can also run this:</p>
<pre data-code-language="shell" data-type="programlisting">curl https://raw.githubusercontent.com/maxpumperla/learning_ray/main/notebooks/pendulum.yml -o pendulum.yml
rllib train -f pendulum.yml</pre>
<p>In any case, assuming the training program finished, we can now check how well it worked.
To visualize the trained pendulum you need to install one more Python library with <code>pip install pyglet</code>.
The only other thing you need to figure out is where Ray stored your training progress.
When you run <code>rllib train</code> for an experiment, Ray will create a unique experiment ID for you and stores results in a sub-folder of <code>~/ray-results</code> by default.
For the training configuration we used, you should see a folder with results that looks like <code>~/ray_results/pendulum-ppo/PPO_Pendulum-v1_&lt;experiment_id&gt;</code>.
During the training procedure intermediate model checkpoints get generated in the same folder.
For instance, I have a folder on my machine called:</p>
<pre data-type="programlisting"> ~/ray_results/pendulum-ppo/PPO_Pendulum-v1_20cbf_00000_0_2021-09-24_15-20-03/checkpoint_000029/checkpoint-29</pre>
<p>Once you figured out the experiment ID and chose a checkpoint ID (as a rule of thumb the larger the ID, the better the results), you can evaluate the training performance of your pendulum training run like this:</p>
<pre data-code-language="shell" data-type="programlisting">rllib evaluate <code class="se">\</code>
  ~/ray_results/pendulum-ppo/PPO_Pendulum-v1_&lt;experiment_id&gt;/checkpoint_0000&lt;cp-id&gt;/checkpoint-&lt;cp-id&gt; <code class="se">\</code>
  --run PPO --env Pendulum-v1 --steps <code class="m">2000</code></pre>
<p>You should see an animation of a pendulum controlled by an agent that looks like figure <a data-type="xref" href="#fig_pendulum">Figure 1-3</a>.
Since we opted for a quick training procedure instead of maximizing performance, you should see the agent struggle with the pendulum exercise.
We could have done much better, and if you’re interested to scan Ray’s tuned examples for the <code>Pendulum-v1</code> environment, you’ll find an abundance of solutions to this exercise.
The point of this example was to show you how simple it can be to train and evaluate reinforcement learning tasks with RLlib, using just two command line calls to <code>rllib</code>.</p>
</div></section>
<section data-pdf-bookmark="Distributed training with Ray Train" data-type="sect3"><div class="sect3" id="idm44990034746384">
<h3>Distributed training with Ray Train</h3>
<p>Ray RLlib is dedicated to reinforcement learning, but what do you do if you need to train models for other types of machine learning, like supervised learning?
You can use another Ray library for distributed training in this case, called <em>Ray Train</em>.
At this point, we don’t have built up enough knowledge of frameworks such as <code>TensorFlow</code> to give you a concrete and informative example for Ray Train.
We’ll discuss all of that in <a data-type="xref" href="ch06.xhtml#chapter_06">Chapter 6</a>, when it’s time to.
But we can at least roughly sketch what a distributed training “wrapper” for an ML model would look like, which is simple enough conceptually:</p>
<div data-type="example">
<h5><span class="label">Example 1-6. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code><code> </code><code class="nn">ray</code><code class="nn">.</code><code class="nn">train</code><code> </code><code class="kn">import</code><code> </code><code class="n">Trainer</code><code>
</code><code>
</code><code>
</code><code class="k">def</code><code> </code><code class="nf">training_function</code><code class="p">(</code><code class="p">)</code><code class="p">:</code><code>  </code><a class="co" href="#callout_an_overview_of_ray_CO5-1" id="co_an_overview_of_ray_CO5-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code>
</code><code>    </code><code class="k">pass</code><code>
</code><code>
</code><code>
</code><code class="n">trainer</code><code> </code><code class="o">=</code><code> </code><code class="n">Trainer</code><code class="p">(</code><code class="n">backend</code><code class="o">=</code><code class="s2">"</code><code class="s2">tensorflow</code><code class="s2">"</code><code class="p">,</code><code> </code><code class="n">num_workers</code><code class="o">=</code><code class="mi">4</code><code class="p">)</code><code>  </code><a class="co" href="#callout_an_overview_of_ray_CO5-2" id="co_an_overview_of_ray_CO5-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a><code>
</code><code class="n">trainer</code><code class="o">.</code><code class="n">start</code><code class="p">(</code><code class="p">)</code><code>
</code><code>
</code><code class="n">results</code><code> </code><code class="o">=</code><code> </code><code class="n">trainer</code><code class="o">.</code><code class="n">run</code><code class="p">(</code><code class="n">training_function</code><code class="p">)</code><code>  </code><a class="co" href="#callout_an_overview_of_ray_CO5-3" id="co_an_overview_of_ray_CO5-3"><img alt="3" height="12" src="assets/3.png" width="12"/></a><code>
</code><code class="n">trainer</code><code class="o">.</code><code class="n">shutdown</code><code class="p">(</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_an_overview_of_ray_CO5-1" id="callout_an_overview_of_ray_CO5-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>First, define your ML model training function. We simply pass here.</p></dd>
<dt><a class="co" href="#co_an_overview_of_ray_CO5-2" id="callout_an_overview_of_ray_CO5-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a></dt>
<dd><p>Then initialize a <code>Trainer</code> instance with TensorFlow as the backend.</p></dd>
<dt><a class="co" href="#co_an_overview_of_ray_CO5-3" id="callout_an_overview_of_ray_CO5-3"><img alt="3" height="12" src="assets/3.png" width="12"/></a></dt>
<dd><p>Lastly, scale out your training function on a Ray cluster.</p></dd>
</dl></div>
<p>If you’re interested in distributed training, you could jump ahead to <a data-type="xref" href="ch06.xhtml#chapter_06">Chapter 6</a>.</p>
</div></section>
</div></section>
<section data-pdf-bookmark="Hyperparameter Tuning" data-type="sect2"><div class="sect2" id="idm44990034398544">
<h2>Hyperparameter Tuning</h2>
<p>Naming things is hard, but the Ray team hit the spot with <em>Ray Tune</em>, which you can use to tune all sorts of parameters.
Specifically, it was built to find good hyperparameters for machine learning models.
The typical setup is as follows:</p>
<ul>
<li>
<p>You want to run an extremely computationally expensive training function. In ML it’s not uncommon to run training procedures that take days, if not weeks, but let’s say you’re dealing with just a couple of minutes.</p>
</li>
<li>
<p>As result of training, you compute a so-called objective function. Usually you either want to maximize your gains or minimize your losses in terms of performance of your experiment.</p>
</li>
<li>
<p>The tricky bit is that your training function might depend on certain parameters, hyperparameters, that influence the value of your objective function.</p>
</li>
<li>
<p>You may have a hunch what individual hyperparameters should be, but tuning them all can be difficult. Even if you can restrict these parameters to a sensible range, it’s usually prohibitive to test a wide range of combinations. Your training function is simply too expensive.</p>
</li>
</ul>
<p>What can you do to efficiently sample hyperparameters and get “good enough” results on your objective?
The field concerned with solving this problem is called <em>hyperparameter optimization</em> (HPO), and Ray Tune has an enormous suite of algorithms for tackling it.
Let’s look at a first example of Ray Tune used for the situation we just explained.
The focus is yet again on Ray and its API, and not on a specific ML task (which we simply simulate for now).</p>
<div data-type="example" id="ray_tune">
<h5><span class="label">Example 1-7. </span>Minimizing an objective for an expensive training function with Ray Tune</h5>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code><code> </code><code class="nn">ray</code><code> </code><code class="kn">import</code><code> </code><code class="n">tune</code><code>
</code><code class="kn">import</code><code> </code><code class="nn">math</code><code>
</code><code class="kn">import</code><code> </code><code class="nn">time</code><code>
</code><code>
</code><code>
</code><code class="k">def</code><code> </code><code class="nf">training_function</code><code class="p">(</code><code class="n">config</code><code class="p">)</code><code class="p">:</code><code>  </code><a class="co" href="#callout_an_overview_of_ray_CO6-1" id="co_an_overview_of_ray_CO6-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code>
</code><code>    </code><code class="n">x</code><code class="p">,</code><code> </code><code class="n">y</code><code> </code><code class="o">=</code><code> </code><code class="n">config</code><code class="p">[</code><code class="s2">"</code><code class="s2">x</code><code class="s2">"</code><code class="p">]</code><code class="p">,</code><code> </code><code class="n">config</code><code class="p">[</code><code class="s2">"</code><code class="s2">y</code><code class="s2">"</code><code class="p">]</code><code>
</code><code>    </code><code class="n">time</code><code class="o">.</code><code class="n">sleep</code><code class="p">(</code><code class="mi">10</code><code class="p">)</code><code>
</code><code>    </code><code class="n">score</code><code> </code><code class="o">=</code><code> </code><code class="n">objective</code><code class="p">(</code><code class="n">x</code><code class="p">,</code><code> </code><code class="n">y</code><code class="p">)</code><code>
</code><code>    </code><code class="n">tune</code><code class="o">.</code><code class="n">report</code><code class="p">(</code><code class="n">score</code><code class="o">=</code><code class="n">score</code><code class="p">)</code><code>  </code><a class="co" href="#callout_an_overview_of_ray_CO6-2" id="co_an_overview_of_ray_CO6-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a><code>
</code><code>
</code><code>
</code><code class="k">def</code><code> </code><code class="nf">objective</code><code class="p">(</code><code class="n">x</code><code class="p">,</code><code> </code><code class="n">y</code><code class="p">)</code><code class="p">:</code><code>
</code><code>    </code><code class="k">return</code><code> </code><code class="n">math</code><code class="o">.</code><code class="n">sqrt</code><code class="p">(</code><code class="p">(</code><code class="n">x</code><code class="o">*</code><code class="o">*</code><code class="mi">2</code><code> </code><code class="o">+</code><code> </code><code class="n">y</code><code class="o">*</code><code class="o">*</code><code class="mi">2</code><code class="p">)</code><code class="o">/</code><code class="mi">2</code><code class="p">)</code><code>  </code><a class="co" href="#callout_an_overview_of_ray_CO6-3" id="co_an_overview_of_ray_CO6-3"><img alt="3" height="12" src="assets/3.png" width="12"/></a><code>
</code><code>
</code><code>
</code><code class="n">result</code><code> </code><code class="o">=</code><code> </code><code class="n">tune</code><code class="o">.</code><code class="n">run</code><code class="p">(</code><code>  </code><a class="co" href="#callout_an_overview_of_ray_CO6-4" id="co_an_overview_of_ray_CO6-4"><img alt="4" height="12" src="assets/4.png" width="12"/></a><code>
</code><code>    </code><code class="n">training_function</code><code class="p">,</code><code>
</code><code>    </code><code class="n">config</code><code class="o">=</code><code class="p">{</code><code>
</code><code>        </code><code class="s2">"</code><code class="s2">x</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="n">tune</code><code class="o">.</code><code class="n">grid_search</code><code class="p">(</code><code class="p">[</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code><code> </code><code class="o">-</code><code class="mf">.5</code><code class="p">,</code><code> </code><code class="mi">0</code><code class="p">,</code><code> </code><code class="mf">.5</code><code class="p">,</code><code> </code><code class="mi">1</code><code class="p">]</code><code class="p">)</code><code class="p">,</code><code>  </code><a class="co" href="#callout_an_overview_of_ray_CO6-5" id="co_an_overview_of_ray_CO6-5"><img alt="5" height="12" src="assets/5.png" width="12"/></a><code>
</code><code>        </code><code class="s2">"</code><code class="s2">y</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="n">tune</code><code class="o">.</code><code class="n">grid_search</code><code class="p">(</code><code class="p">[</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code><code> </code><code class="o">-</code><code class="mf">.5</code><code class="p">,</code><code> </code><code class="mi">0</code><code class="p">,</code><code> </code><code class="mf">.5</code><code class="p">,</code><code> </code><code class="mi">1</code><code class="p">]</code><code class="p">)</code><code>
</code><code>    </code><code class="p">}</code><code class="p">)</code><code>
</code><code>
</code><code class="nb">print</code><code class="p">(</code><code class="n">result</code><code class="o">.</code><code class="n">get_best_config</code><code class="p">(</code><code class="n">metric</code><code class="o">=</code><code class="s2">"</code><code class="s2">score</code><code class="s2">"</code><code class="p">,</code><code> </code><code class="n">mode</code><code class="o">=</code><code class="s2">"</code><code class="s2">min</code><code class="s2">"</code><code class="p">)</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_an_overview_of_ray_CO6-1" id="callout_an_overview_of_ray_CO6-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>We simulate an expensive training function that depends on two hyperparameters <code>x</code> and <code>y</code>, read from a <code>config</code>.</p></dd>
<dt><a class="co" href="#co_an_overview_of_ray_CO6-2" id="callout_an_overview_of_ray_CO6-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a></dt>
<dd><p>After sleeping for 5 seconds to simulate training and computing the objective we report back the score to <code>tune</code>.</p></dd>
<dt><a class="co" href="#co_an_overview_of_ray_CO6-3" id="callout_an_overview_of_ray_CO6-3"><img alt="3" height="12" src="assets/3.png" width="12"/></a></dt>
<dd><p>The objective computes the mean of the squares of <code>x</code> and <code>y</code> and returns the square root of this term. This type of objective is fairly common in ML.</p></dd>
<dt><a class="co" href="#co_an_overview_of_ray_CO6-4" id="callout_an_overview_of_ray_CO6-4"><img alt="4" height="12" src="assets/4.png" width="12"/></a></dt>
<dd><p>We then use <code>tune.run</code> to initialize hyperparameter optimization on our <code>training_function</code>.</p></dd>
<dt><a class="co" href="#co_an_overview_of_ray_CO6-5" id="callout_an_overview_of_ray_CO6-5"><img alt="5" height="12" src="assets/5.png" width="12"/></a></dt>
<dd><p>A key part is to provide a parameter space for <code>x</code> and <code>y</code> for <code>tune</code> to search over.</p></dd>
</dl></div>
<p>The Tune example in <a data-type="xref" href="#ray_tune">Example 1-7</a> finds the best possible choices of parameters <code>x</code> and <code>y</code> for a <code>training_function</code> with a given <code>objective</code> we want to minimize.
Even though the objective function might look a little intimidating at first, since we compute the sum of squares of <code>x</code> and <code>y</code>, all values will be non-negative.
That means the smallest value is obtained at <code>x=0</code> and <code>y=0</code> which evaluates the objective function to <code>0</code>.</p>
<p>We do a so-called <em>grid search</em> over all possible parameter combinations.
As we explicitly pass in five possible values for both <code>x</code> and <code>y</code> that’s a total of <code>25</code> combinations that get fed into the training function.
Since we instruct <code>training_function</code> to sleep for <code>10</code> seconds, testing all combinations of hyperparameters sequentially would take more than four minutes total.
Since Ray is smart about parallelizing this workload, on my laptop this whole experiment only takes about <code>35</code> seconds.
Now, imagine each training run would have taken several hours, and we’d have 20 instead of two hyperparameters.
That makes grid search infeasible, especially if you don’t have educated guesses on the parameter range.
In such situations you’ll have to use more elaborate HPO methods from Ray Tune, as discussed in <a data-type="xref" href="ch05.xhtml#chapter_05">Chapter 5</a>.</p>
</div></section>
<section data-pdf-bookmark="Model Serving" data-type="sect2"><div class="sect2" id="idm44990034117696">
<h2>Model Serving</h2>
<p>The last of Ray’s high-level libraries we’ll discuss specializes on model serving and is simply called <em>Ray Serve</em>.
To see an example of it in action, you need a trained ML model to serve.
Luckily, nowadays you can find many interesting models on the internet that have already been trained for you.
For instance, <em>Hugging Face</em> has a variety of models available for you to download directly in Python.
The model we’ll use is a language model called <em>GPT-2</em> that takes text as input and produces text to continue or complete the input.
For example, you can prompt a question and GPT-2 will try to complete it.</p>
<p>Serving such a model is a good way to make it accessible.
You may not now how to load and run a TensorFlow model on your computer, but you do now how to ask a question in plain English.
Model serving hides the implementation details of a solution and lets users focus on providing inputs and understanding outputs of a model.</p>
<p>To proceed, make sure to run <code>pip install transformers</code> to install the Hugging Face library that has the model we want to use.
With that we can now import and start an instance of Ray’s <code>serve</code> library, load and deploy a GPT-2 model and ask it for the meaning of life, like so:</p>
<div data-type="example">
<h5><span class="label">Example 1-8. </span></h5>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code><code> </code><code class="nn">ray</code><code> </code><code class="kn">import</code><code> </code><code class="n">serve</code><code>
</code><code class="kn">from</code><code> </code><code class="nn">transformers</code><code> </code><code class="kn">import</code><code> </code><code class="n">pipeline</code><code>
</code><code class="kn">import</code><code> </code><code class="nn">requests</code><code>
</code><code>
</code><code class="n">serve</code><code class="o">.</code><code class="n">start</code><code class="p">(</code><code class="p">)</code><code>  </code><a class="co" href="#callout_an_overview_of_ray_CO7-1" id="co_an_overview_of_ray_CO7-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code>
</code><code>
</code><code>
</code><code class="nd">@serve</code><code class="o">.</code><code class="n">deployment</code><code>  </code><a class="co" href="#callout_an_overview_of_ray_CO7-2" id="co_an_overview_of_ray_CO7-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a><code>
</code><code class="k">def</code><code> </code><code class="nf">model</code><code class="p">(</code><code class="n">request</code><code class="p">)</code><code class="p">:</code><code>
</code><code>    </code><code class="n">language_model</code><code> </code><code class="o">=</code><code> </code><code class="n">pipeline</code><code class="p">(</code><code class="s2">"</code><code class="s2">text-generation</code><code class="s2">"</code><code class="p">,</code><code> </code><code class="n">model</code><code class="o">=</code><code class="s2">"</code><code class="s2">gpt2</code><code class="s2">"</code><code class="p">)</code><code>  </code><a class="co" href="#callout_an_overview_of_ray_CO7-3" id="co_an_overview_of_ray_CO7-3"><img alt="3" height="12" src="assets/3.png" width="12"/></a><code>
</code><code>    </code><code class="n">query</code><code> </code><code class="o">=</code><code> </code><code class="n">request</code><code class="o">.</code><code class="n">query_params</code><code class="p">[</code><code class="s2">"</code><code class="s2">query</code><code class="s2">"</code><code class="p">]</code><code>
</code><code>    </code><code class="k">return</code><code> </code><code class="n">language_model</code><code class="p">(</code><code class="n">query</code><code class="p">,</code><code> </code><code class="n">max_length</code><code class="o">=</code><code class="mi">100</code><code class="p">)</code><code>  </code><a class="co" href="#callout_an_overview_of_ray_CO7-4" id="co_an_overview_of_ray_CO7-4"><img alt="4" height="12" src="assets/4.png" width="12"/></a><code>
</code><code>
</code><code>
</code><code class="n">model</code><code class="o">.</code><code class="n">deploy</code><code class="p">(</code><code class="p">)</code><code>  </code><a class="co" href="#callout_an_overview_of_ray_CO7-5" id="co_an_overview_of_ray_CO7-5"><img alt="5" height="12" src="assets/5.png" width="12"/></a><code>
</code><code>
</code><code class="n">query</code><code> </code><code class="o">=</code><code> </code><code class="s2">"</code><code class="s2">What</code><code class="s2">'</code><code class="s2">s the meaning of life?</code><code class="s2">"</code><code>
</code><code class="n">response</code><code> </code><code class="o">=</code><code> </code><code class="n">requests</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="sa">f</code><code class="s2">"</code><code class="s2">http://localhost:8000/model?query=</code><code class="si">{</code><code class="n">query</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code><code>  </code><a class="co" href="#callout_an_overview_of_ray_CO7-6" id="co_an_overview_of_ray_CO7-6"><img alt="6" height="12" src="assets/6.png" width="12"/></a><code>
</code><code class="nb">print</code><code class="p">(</code><code class="n">response</code><code class="o">.</code><code class="n">text</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_an_overview_of_ray_CO7-1" id="callout_an_overview_of_ray_CO7-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>We start <code>serve</code> locally.</p></dd>
<dt><a class="co" href="#co_an_overview_of_ray_CO7-2" id="callout_an_overview_of_ray_CO7-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a></dt>
<dd><p>The <code>@serve.deployment</code> decorator turns a function with a <code>request</code> parameter into a <code>serve</code> deployment.</p></dd>
<dt><a class="co" href="#co_an_overview_of_ray_CO7-3" id="callout_an_overview_of_ray_CO7-3"><img alt="3" height="12" src="assets/3.png" width="12"/></a></dt>
<dd><p>Loading <code>language_model</code> inside the <code>model</code> function for every request is inefficient, but it’s the quickest way to show you a deployment.</p></dd>
<dt><a class="co" href="#co_an_overview_of_ray_CO7-4" id="callout_an_overview_of_ray_CO7-4"><img alt="4" height="12" src="assets/4.png" width="12"/></a></dt>
<dd><p>We ask the model to give us at most <code>100</code> characters to continue our query.</p></dd>
<dt><a class="co" href="#co_an_overview_of_ray_CO7-5" id="callout_an_overview_of_ray_CO7-5"><img alt="5" height="12" src="assets/5.png" width="12"/></a></dt>
<dd><p>Then we formally deploy the model so that it can start receiving requests over HTTP.</p></dd>
<dt><a class="co" href="#co_an_overview_of_ray_CO7-6" id="callout_an_overview_of_ray_CO7-6"><img alt="6" height="12" src="assets/6.png" width="12"/></a></dt>
<dd><p>We use the indispensable <code>requests</code> library to get a response for any question you might have.</p></dd>
</dl></div>
<p>In [Link to Come] you will learn how to properly deploy models in various scenarios, but for now
I encourage you to play around with this example and test different queries.
Running the last two lines of code repeatedly will give you different answers practically every time.
Here’s a darkly poetic gem, raising more questions, that I queried on my machine and slightly censored for underaged readers:</p>
<pre data-type="programlisting">[{
    "generated_text": "What's the meaning of life?\n\n
     Is there one way or another of living?\n\n
     How does it feel to be trapped in a relationship?\n\n
     How can it be changed before it's too late?
     What did we call it in our time?\n\n
     Where do we fit within this world and what are we going to live for?\n\n
     My life as a person has been shaped by the love I've received from others."
}]</pre>
<p>This concludes our whirlwind tour of Ray’s data science libraries, the second of Ray’s layers.
Before we wrap up this chapter, let’s have a very brief look at the third layer, the growing ecosystem around Ray.</p>
</div></section>
</div></section>
<section data-pdf-bookmark="A Growing Ecosystem" data-type="sect1"><div class="sect1" id="idm44990033936384">
<h1>A Growing Ecosystem</h1>
<p>Ray’s high-level libraries are powerful and deserve a much deeper treatment throughout the book.
While their usefulness for the data science experimentation lifecycle is undeniable, I also don’t want to give off the impression that Ray is all you need from now on.
In fact, I believe the best and most successful frameworks are the ones that integrate well with existing solutions and ideas.
It’s better to focus on your core strengths and leverage other tools for what’s missing in your solution.
There’s usually no reason to re-invent the wheel.</p>
<section data-pdf-bookmark="How Ray Integrates and Extends" data-type="sect2"><div class="sect2" id="idm44990033935120">
<h2>How Ray Integrates and Extends</h2>
<p>To give you an example for how Ray integrates with other tools, consider that Ray Data is a relatively new addition to its libraries.
If you want to boil it down, and maybe oversimplify a little, Ray is a compute-first framework.
In contrast, distributed frameworks like Apache Spark<sup><a data-type="noteref" href="ch01.xhtml#idm44990033933984" id="idm44990033933984-marker">7</a></sup> or Dask can be considered data-first.
Pretty much anything you do with Spark starts with the definition of a distributed dataset and transformations thereof.
Dask bets on bringing common data structures like Pandas dataframes or Numpy arrays to a distributed setup.
Both are immensely powerful in their own regard, and we’ll give you a more detailed and fair comparison to Ray in [Link to Come].
The gist of it is that Ray Data does not attempt to replace these tools.
Instead, it integrates well with both.
As you’ll come to see, that’s a common theme with Ray.</p>
</div></section>
<section data-pdf-bookmark="Ray as Distributed Interface" data-type="sect2"><div class="sect2" id="idm44990033932368">
<h2>Ray as Distributed Interface</h2>
<p>One aspect of Ray that’s vastly understated in my eyes is that its libraries seamlessly integrate common tools as <em>backends</em>.
Ray often creates common interfaces, instead of trying to create new standards<sup><a data-type="noteref" href="ch01.xhtml#idm44990033930720" id="idm44990033930720-marker">8</a></sup>.
These interfaces allow you to run tasks in a distributed fashion, a property most of the respective backends don’t have, or not to the same extent.
For instance, Ray RLlib and Train are backed by the full power of TensorFlow and PyTorch.
Ray Tune supports algorithms from practically every notable HPO tool available, including Hyperopt, Optuna, Nevergrad, Ax, SigOpt and many others.
None of these tools are distributed by default, but Tune unifies them in a common interface.
Ray Serve can be used with frameworks such as FastAPI, and Ray Data is backed by Arrow and comes with many integrations to other frameworks, such as Spark and Dask.
Overall this seems to be a robust design pattern that can be used to extend current Ray projects or integrate new backends in the future.</p>
</div></section>
</div></section>
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm44990033929088">
<h1>Summary</h1>
<p>To sum up what we’ve discussed in this chapter, <a data-type="xref" href="#fig_ray_layers">Figure 1-4</a> gives you an overview of the three layers of Ray as we laid them out.
Ray’s core distributed execution engine sits at the center of the framework.
For practical data science workflows you can use Ray Data for data processing, Ray RLlib for reinforcement learning, Ray Train for distributed model training, Ray Tune for hyperparameter tuning and Ray Serve for model serving.
You’ve seen examples for each of these libraries and have an idea of what their APIs entail.
On top of that, Ray’s ecosystem has many extensions that we’ll look more into later on.
Maybe you can already spot a few tools you know and like in <a data-type="xref" href="#fig_ray_layers">Figure 1-4</a><sup><a data-type="noteref" href="ch01.xhtml#idm44990033926608" id="idm44990033926608-marker">9</a></sup>?</p>
<figure><div class="figure" id="fig_ray_layers">
<img alt="Ray layers" height="525" src="assets/ray_layers.png" width="573"/>
<h6><span class="label">Figure 1-4. </span>Ray in three layers: its core API, the libraries RLlib, Tune, Ray Train, Ray Serve, Ray Data, and some of the many third-party integrations</h6>
</div></figure>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="idm44990045517696"><sup><a href="ch01.xhtml#idm44990045517696-marker">1</a></sup> Moore’s Law held for a long time, but there might be signs that it’s slowing down. We’re not here to argue it, though. What’s important is not that our computers generally keep getting faster, but the relation to the amount of compute we need.</p><p data-type="footnote" id="idm44990045384464"><sup><a href="ch01.xhtml#idm44990045384464-marker">2</a></sup> For the experts among you, I don’t claim that RL is the answer. RL is just a paradigm that naturally fits into this discussion of AI goals.</p><p data-type="footnote" id="idm44990045356496"><sup><a href="ch01.xhtml#idm44990045356496-marker">3</a></sup> This is a Python book, so we’ll exclusively focus on it. But you should at least know that Ray also has a Java API, which at this point is less mature than its Python equivalent.</p><p data-type="footnote" id="idm44990042817344"><sup><a href="ch01.xhtml#idm44990042817344-marker">4</a></sup> We’re using Ray version <code>1.9.0</code> at this point, as it’s the latest version available as of this writing.</p><p data-type="footnote" id="idm44990042948624"><sup><a href="ch01.xhtml#idm44990042948624-marker">5</a></sup> I never liked the categorization of data science as an intersection of disciplines, like maths, coding and business. Ultimately, that doesn’t tell you what practitioners <em>do</em>. It doesn’t do a cook justice to tell them they sit at the intersection of agriculture, thermodynamics and human relations. It’s not wrong, but also not very helpful.</p><p data-type="footnote" id="idm44990044482384"><sup><a href="ch01.xhtml#idm44990044482384-marker">6</a></sup> As a fun exercise, I recommend reading Paul Graham’s famous <a href="http://www.paulgraham.com/hp.xhtml">“Hackers and Painters”</a> essay on this topic and replace “computer science” with “data science”. What would hacking 2.0 be?</p><p data-type="footnote" id="idm44990033933984"><sup><a href="ch01.xhtml#idm44990033933984-marker">7</a></sup> Spark has been created by another lab in Berkeley, AMPLab. The internet is full of blog posts claiming that Ray should therefore be seen as a replacement of Spark. It’s better to think of them as tools with different strengths that are both likely here to stay.</p><p data-type="footnote" id="idm44990033930720"><sup><a href="ch01.xhtml#idm44990033930720-marker">8</a></sup> Before the deep learning framework <a href="https://keras.io">Keras</a> became an official part of a corporate flagship, it started out as a convenient API specification for various lower-level frameworks such as Theano, CNTK, or TensorFlow. In that sense Ray RLlib has the chance to become Keras for RL. Ray Tune might just be Keras for HPO. The missing piece for more adoption is probably a more elegant API for both.</p><p data-type="footnote" id="idm44990033926608"><sup><a href="ch01.xhtml#idm44990033926608-marker">9</a></sup> Note that “Ray Train” has been called “raysgd” in older versions of Ray, and does not have a new logo yet.</p></div></div></section></div></body></html>