- en: Chapter 4\. Dask DataFrame
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pandas DataFrames, while popular, quickly run into memory constraints as data
    sizes grow, since they store the entirety of the data in memory. Pandas DataFrames
    have a robust API for all kinds of data manipulation and are frequently the starting
    point for many analytics and machine learning projects. While pandas itself does
    not have machine learning built in, data scientists often use it as part of data
    and feature preparation during the exploratory phase of new projects. As such,
    scaling pandas DataFrames to be able to handle large datasets is of vital importance
    to many data scientists. Most data scientists are already familiar with the pandas
    libraries, and Dask’s DataFrame implements much of the pandas API while adding
    the ability to scale.
  prefs: []
  type: TYPE_NORMAL
- en: Dask is one of the first to implement a usable subset of the pandas APIs, but
    other projects such as Spark have added their approaches. This chapter assumes
    you have a good understanding of the pandas DataFrame APIs; if not, you should
    check out [*Python for Data Analysis*](https://learning.oreilly.com/library/view/python-for-data/9781098104023).
  prefs: []
  type: TYPE_NORMAL
- en: You can often use Dask DataFrames as a replacement for pandas DataFrames with
    minor changes, thanks to duck-typing. However, this approach can have performance
    drawbacks, and some functions are not present. These drawbacks come from the distributed
    parallel nature of Dask, which adds communication costs for certain types of operations.
    In this chapter, you will learn how to minimize these performance drawbacks and
    work around any missing functionality.
  prefs: []
  type: TYPE_NORMAL
- en: Dask DataFrames require that your data and your computation are well suited
    to pandas DataFrames. Dask has bags for unstructured data, arrays for array-structured
    data, the Dask delayed interface for arbitrary functions, and actors for stateful
    operations. If even at a small scale you wouldn’t consider using pandas for your
    problem, Dask DataFrames are probably not the right solution.
  prefs: []
  type: TYPE_NORMAL
- en: How Dask DataFrames Are Built
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dask DataFrames build on top of pandas DataFrames. Each partition is stored
    as a pandas DataFrame.^([1](ch04.xhtml#id479)) Using pandas DataFrames for the
    partitions simplifies the implementation of much of the APIs. This is especially
    true for row-based operations, where Dask passes the function call down to each
    pandas DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the distributed components of Dask DataFrames use the three core building
    blocks `map_partitions`, `reduction`, and `rolling`. You mostly won’t need to
    call these functions directly; you will use higher-level APIs instead. But understanding
    these functions and how they work is important to understanding how Dask works.
    `shuffle` is a critical building block of distributed DataFrames for reorganizing
    your data. Unlike the other building blocks, you may use it directly more frequently,
    as Dask is unable to abstract away partitioning.
  prefs: []
  type: TYPE_NORMAL
- en: Loading and Writing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data analytics is only as valuable as the data it has access to, and our insights
    are helpful only if they result in action. Since not all of our data is in Dask,
    it’s essential to read and write data from the rest of the world. So far, the
    examples in this book have mainly used local collections, but you have many more
    options.
  prefs: []
  type: TYPE_NORMAL
- en: Dask supports reading and writing many standard file formats and filesystems.
    These formats include CSV, HDF, fixed-width, Parquet, and ORC. Dask supports many
    of the standard distributed filesystems, from HDFS to S3, and reading from regular
    filesystems.
  prefs: []
  type: TYPE_NORMAL
- en: Most important for Dask, distributed filesystems allow multiple computers to
    read and write to the same set of files. Distributed filesystems often store data
    on multiple computers, which allows for storing more data than a single computer
    can hold. Often, but not always, distributed filesystems are also fault tolerant
    (which they achieve through replication). Distributed filesystems can have important
    performance differences from what you are used to working with, so it’s important
    to skim the user documentation for the filesystems you are using. Some things
    to look for are block sizes (you often don’t want to write files smaller than
    these, as the rest is wasted space), latency, and consistency guarantees.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Reading from regular local files can be complicated in Dask, as the files need
    to exist on all workers. If a file exists only on the head node, consider copying
    it to a distributed filesystem like S3 or NFS, or load it locally and use Dask’s
    `client.scatter` function to distribute the data if it’s small enough. Sufficiently
    small files *may* be a sign that you don’t yet need Dask, unless the processing
    on them is complex or slow.
  prefs: []
  type: TYPE_NORMAL
- en: Formats
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dask’s DataFrame loading and writing functions start with `to_` or `read_` as
    the prefixes. Each format has its own configuration, but in general, the first
    positional argument is the location of the data to be read. The location can be
    a wildcard path of files (e.g., *s3://test-bucket/magic/**), a list of files,
    or a regular file location.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Wildcard paths work only with filesystems that support directory listing. For
    example, they do not work on HTTP.
  prefs: []
  type: TYPE_NORMAL
- en: When loading data, having the right number of partitions will speed up all of
    your operations. Sometimes it’s not possible to load the data with the right number
    of partitions, and in those cases you can repartition your data after the load.
    As discussed, more partitions allow for more parallelism but have a non-zero overhead.
    The different formats have slightly different ways to control this. HDF takes
    `chunksize`, indicating the number of rows per partition. Parquet also takes `split_row_groups`,
    which takes an integer of the desired logical partitioning out of the Parquet
    file, and Dask will split the whole set into those chunks, or less. If not given,
    the default behavior is for each partition to correspond to a Parquet file. The
    text-based formats (CSV, fixed-width, etc.) take a `blocksize` parameter with
    the same meaning as Parquet’s `chunksize` but a maximum value of 64 MB. You can
    verify this by loading a dataset and seeing the number of tasks and partitions
    increase with smaller target sizes, as in [Example 4-1](#ex_load_1kb).
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-1\. Dask DataFrame loading CSV with 1 KB chunks
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Loading CSV and JSON files can be more complicated than Parquet, and other self-describing
    data types don’t have any schema information encoded. Dask DataFrames need to
    know the types of the different columns to serialize the data correctly. By default,
    Dask will automatically look at the first few records and guess the data types
    for each column. This process is known as schema inference, and it can be quite
    slow.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, schema inference does not always work. For example, if you try
    to load the UK’s gender pay gap disparity data from *https​://gender-pay-gap​.ser⁠vice.gov.uk/viewing/download-data/2021*,
    when you access the data, as in [Example 4-2](#ex_load_uk_gender_pay_gap_infered),
    you will get an error of “Mismatched dtypes found in `pd.read​_csv`/`pd.read_table`.”
    When Dask’s column type inference is incorrect, you can override it (per column)
    by specifying the `dtype` parameter, as shown in [Example 4-3](#ex_load_uk_gender_pay_gap).
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-2\. Dask DataFrame loading CSV, depending entirely on inference
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Example 4-3\. Dask DataFrame loading CSV and specifying data type
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In theory, you can have Dask sample more records by specifying more bytes with
    the `sample` parameter, but this does not currently fix the problem. The current
    sampling code does not strictly respect the number of bytes requested.
  prefs: []
  type: TYPE_NORMAL
- en: Even when schema inference does not return an error, depending on it has a number
    of drawbacks. Schema inference involves sampling data, and its results are therefore
    both probabilistic and slow. When you can, you should use self-describing formats
    or otherwise avoid schema inference; your data loading will be faster and more
    reliable. Some common self-describing formats you may encounter include Parquet,
    Avro, and ORC.
  prefs: []
  type: TYPE_NORMAL
- en: Reading and writing from/to new file formats is a lot of work, especially if
    there are no existing Python libraries. If there are existing libraries, you might
    find it easier to read the raw data into a bag and parse it with a `map` function,
    which we will explore further in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Dask does not detect sorted data on load. Instead, if you have presorted data,
    add the `sorted=true` parameter when setting an index to take advantage of your
    already sorted data, a step you will learn about in the next section. If you specify
    this when the data is not sorted, however, you may get silent data corruption.
  prefs: []
  type: TYPE_NORMAL
- en: You can also connect Dask to databases or microservices. Relational databases
    are a fantastic tool and are often quite performant at both simple reads and writes.
    Often relational databases support distributed deployment whereby the data is
    split up on multiple nodes, and this is mostly used with large datasets. Relational
    databases tend to be great at handling transactions at scale, but running analytic
    capabilities on the same node can encounter issues. Dask can be used to efficiently
    read and compute over SQL databases.
  prefs: []
  type: TYPE_NORMAL
- en: You can use Dask’s built-in support for loading SQL databases using SQLAlchemy.
    For Dask to split up the query on multiple machines, you need to give it an index
    key. Often SQL databases will have a primary key or numerical index key that you
    can use for this purpose (e.g., `read_sql_table("customers", index_col="customer_id")`).
    An example of this is shown in [Example 4-4](#ex_read_SQL_Dataframe).
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-4\. Reading from and writing to SQL with Dask DataFrame
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: More advanced connections to databases or microservices are best made using
    the bag interface and writing your custom load code, which you will learn more
    about in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Filesystems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Loading data can be a substantial amount of work and a bottleneck, so Dask distributes
    this like most other tasks. If you are using Dask distributed, each worker must
    have access to the files to parallelize the loading. Rather than copying the file
    to each worker, network filesystems allow everyone to access the files. Dask’s
    file access layer uses the FSSPEC library (from the intake project) to access
    the different filesystems. Since FSSPEC supports a range of filesystems, it does
    not install the requirements for every supported filesystem. Use the code in [Example 4-5](#ex_supported_fs)
    to see which filesystems are supported and which ones need additional packages.
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-5\. Getting a list of FSSPEC-supported filesystems
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Many filesystems require some kind of configuration, be it endpoint or credentials.
    Often new filesystems, like MinIO, offer S3-compatible APIs but overload the endpoint
    and require some extra configuration to function. With Dask you specify the configuration
    parameters to the read/write function with `storage​_options`. Everyone’s configuration
    here will likely be a bit different.^([2](ch04.xhtml#id522)) Dask will use your
    `storage_options` dict as the keyword arguments to the underlying FSSPEC implementation.
    For example, my `storage_options` for MinIO are shown in [Example 4-6](#minio_storage_options).
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-6\. Configuring Dask to talk to MinIO
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Indexing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Indexing into a DataFrame is one of the powerful features of pandas, but it
    comes with some restrictions when moving into a distributed system like Dask.
    Since Dask does not track the size of each partition, positional indexing by row
    is not supported. You can use positional indexing for columns, as well as label
    indexing for columns or rows.
  prefs: []
  type: TYPE_NORMAL
- en: Indexing is frequently used to filter the data to have only the components you
    need. We did this for the San Francisco COVID-19 data by looking at just the case
    rates for people of all vaccine statuses, as shown in [Example 4-7](#index_covid_data).
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-7\. Dask DataFrame indexing
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: If you truly need positional indexing by row, you can implement your own by
    computing the size of each partition and using this to select the desired partition
    subsets. This is very inefficient, so Dask avoids implementing it directly; make
    an intentional choice before doing this.
  prefs: []
  type: TYPE_NORMAL
- en: Shuffles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned in the previous chapter, shuffles are expensive. The primary causes
    of the expensive nature of shuffles are the serialization overhead in moving data
    between processes and the comparative slowness of networks relative to reading
    data from memory. These costs scale as the amount of data being shuffled increases,
    so Dask has techniques to reduce the amount of data being shuffled. These techniques
    depend on certain data properties or the operation being performed.
  prefs: []
  type: TYPE_NORMAL
- en: Rolling Windows and map_overlap
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One situation that can trigger the need for a shuffle is a rolling window, where
    at the edges of a partition your function needs some records from its neighbors.
    Dask DataFrame has a special `map_overlap` function in which you can specify a
    *look-after* window (also called a *look-ahead* window) and *look-behind* window
    (also called a *look-back* window) of rows to transfer (either an integer or a
    time delta). The simplest example taking advantage of this is a rolling average,
    shown in [Example 4-8](#rolling_date_ex).
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-8\. Dask DataFrame rolling average
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Using `map_overlap` allows Dask to transfer only the data needed. For this implementation
    to work correctly, your minimum partition size needs to be larger than your largest
    window.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Dask’s rolling windows will not cross multiple partitions. If your DataFrame
    is partitioned so that the look-after or look-back is greater than the length
    of the neighbor’s partition, the results will either fail or be incorrect. Dask
    validates this for time delta look-after, but no such checks are performed for
    look-backs or integer look-after.
  prefs: []
  type: TYPE_NORMAL
- en: An effective but expensive technique for working around the single-partition
    look-ahead/look-behind of Dask is to `repartition` your Dask DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: Aggregations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Aggregations are another special case that can reduce the amount of data that
    needs to be transferred over the network. Aggregations are functions that combine
    records. If you are coming from a map/reduce or Spark background, `reduceByKey`
    is the classic aggregation. Aggregations can either be “by key” or be global across
    an entire DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: To aggregate by key, you first need to call `groupby` with the column(s) representing
    the key, or the keying function to aggregate on. For example, calling `df.groupby("PostCode")`
    groups your DataFrame by postal code, or calling `df.groupby(["PostCode", "SicCodes"])`
    uses a combination of columns for grouping. Function-wise, many of the same pandas
    aggregates are available, but the performance of aggregates in Dask are very different
    from local pandas DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you’re aggregating by partition key, Dask can compute the aggregation without
    needing a shuffle.
  prefs: []
  type: TYPE_NORMAL
- en: The first way to speed up your aggregations is to reduce the columns that you
    are aggregating on, since the fastest data to process is no data. Finally, when
    possible, doing multiple aggregations at the same time reduces the number of times
    the same data needs to be shuffled. Therefore, if you need to compute the average
    and the max, you should compute both at the same time (see [Example 4-9](#max_mean)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-9\. Dask DataFrame max and mean
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: For distributed systems like Dask, if an aggregation can be partially evaluated
    and then merged, you can potentially combine some records pre-shuffle. Not all
    partial aggregations are created equal. What matters with partial aggregations
    is that the amount of data is reduced when merging values with the same key compared
    to the original multiple values.
  prefs: []
  type: TYPE_NORMAL
- en: The most efficient aggregations take a sublinear amount of space regardless
    of the number of records. Some of these, such as sum, count, first, minimum, maximum,
    mean, and standard deviation, can take constant space. More complicated tasks,
    like quantiles and distinct counts, also have sublinear approximation options.
    These approximation options can be great, as exact answers can require linear
    growth in storage.^([3](ch04.xhtml#id539))
  prefs: []
  type: TYPE_NORMAL
- en: Some aggregation functions are not sublinear in growth, but tend to or might
    not grow too quickly. Counting the distinct values is in this group, but if all
    your values are unique there is no space saving.
  prefs: []
  type: TYPE_NORMAL
- en: To take advantage of efficient aggregations, you need to use a built-in aggregation
    from Dask, or write your own using Dask’s aggregation class. Whenever you can,
    use a built-in. Built-ins not only require less effort but also are often faster.
    Not all of the pandas aggregates are directly supported in Dask, so sometimes
    your only choice is to write your own aggregate.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you choose to write your own aggregate, you have three functions to define:
    `chunk` for handling each group-partition/chunk, `agg` to combine the results
    of `chunk` between partitions, and (optionally) `finalize` to take the result
    of `agg` and produce a final value.'
  prefs: []
  type: TYPE_NORMAL
- en: The fastest way to understand how to use partial aggregation is by looking at
    an example that uses all three functions. Using the weighted average in [Example 4-10](#weight_avg)
    can help you think of what is needed for each function. The first function needs
    to compute the weighted values and the weights. The `agg` function combines these
    by summing each side part of the tuple. Finally, the `finalize` function divides
    the total by the weights.
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-10\. Dask custom aggregate
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In some cases, such as with a pure summation, you don’t need to do any post-processing
    on `agg`’s output, so you can skip the `finalize` function.
  prefs: []
  type: TYPE_NORMAL
- en: Not all aggregations must be by key; you can also compute aggregations across
    all rows. Dask’s custom aggregation interface, however, is exposed only with by-key
    operations.
  prefs: []
  type: TYPE_NORMAL
- en: Dask’s built-in full DataFrame aggregations use a lower-level interface called
    `apply_contact_apply` for partial aggregations. Rather than learn two different
    APIs for partial aggregations, we prefer to do a static `groupby` by providing
    a constant grouping function. This way, we only have to know one interface for
    aggregations. You can use this to find the aggregate COVID-19 numbers across the
    DataFrame, as shown in [Example 4-11](#agg_entire).
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-11\. Aggregating across the entire DataFrame
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'When built-in aggregation exists, it will likely be better than anything we
    would write. Sometimes a partial aggregation is partially implemented, as in the
    case of Dask’s HyperLogLog: it is implemented only for full DataFrames. You can
    often translate simple aggregations using `apply_contact_apply` or `aca` by copying
    the `chunk` function, using the `combine` parameter for `agg`, and using the `aggregate`
    parameter for `finalize`. This is shown via porting Dask’s HyperLogLog implementation
    in [Example 4-12](#custom_agg_hyperloglog).'
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-12\. Wrapping Dask’s HyperLogLog in `dd.Aggregation`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Slow/inefficient aggregations (or those very likely to cause an out-of-memory
    exception) use storage proportional to the records being aggregated. Examples
    from this slow group include making a list and naively computing exact quantiles.^([4](ch04.xhtml#id545))
    With these slow aggregates, using Dask’s aggregation class has no benefit over
    the `apply` API, which you may wish to use for simplicity. For example, if you
    just wanted a list of employer IDs by postal code, rather than having to write
    three functions you could use a one-liner like `df.groupby("PostCode")["EmployerId"].apply(lambda
    g: list(g))`. Dask implements the `apply` function as a full shuffle, which is
    covered in the next section.'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Dask is unable to apply partial aggregations when you use the `apply` function.
  prefs: []
  type: TYPE_NORMAL
- en: Full Shuffles and Partitioning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If an operation seems to be slower inside of Dask than you would expect from
    working in local DataFrames, it might be because it requires a full shuffle. An
    example of this is sorting, which is inherently expensive in distributed systems
    because it most often requires a shuffle. Full shuffles are sometimes an unavoidable
    part of working in Dask. Counterintuitively, while full shuffles are themselves
    slow, you can use them to speed up future operations that are all happening on
    the same grouping key(s). As mentioned in the aggregation section, one of the
    ways a full shuffle is triggered is by using the `apply` method when partitioning
    is not aligned.
  prefs: []
  type: TYPE_NORMAL
- en: Partitioning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You will most commonly use full shuffles to repartition your data. It’s important
    to have the right partitioning when dealing with aggregations, rolling windows,
    or look-ups/indexing. As discussed in the rolling window section, Dask cannot
    do more than one partition’s worth of look-ahead or look-behind, so having the
    right partitioning is required to get the correct results. For most other operations,
    having incorrect partitioning will slow down your job.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dask has three primary methods for controlling the partitioning of a DataFrame:
    `set_index`, `repartition`, and `shuffle` (see [Table 4-1](#table_ch04_1687446786141)).
    You use `set_index` when changing the partitioning to a new key/index. `repartition`
    keeps the same key/index but changes the splits. `repartition` and `set_index`
    take similar parameters, with `repartition` not taking an index key name. In general,
    if you are not changing the column used for the index, you should use `repartition`.
    `shuffle` is a bit different since it does not produce a known partitioning scheme
    that operations like `groupby` can take advantage of.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 4-1\. Functions to control partitioning
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Changes index key | Sets number of partitions | Results in a known
    partitioning scheme | Ideal use case |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `set_index` | Yes | Yes | Yes | Changing the index key |'
  prefs: []
  type: TYPE_TB
- en: '| `repartition` | No | Yes | Yes | Increasing/decreasing number of partitions
    |'
  prefs: []
  type: TYPE_TB
- en: '| `shuffle` | No | Yes | No | Skewed distribution of key^([a](ch04.xhtml#id556))
    |'
  prefs: []
  type: TYPE_TB
- en: '| ^([a](ch04.xhtml#id556-marker)) Hashes the key for distribution, which can
    help randomly distribute skewed data *if* the keys are unique (but clustered).
    |'
  prefs: []
  type: TYPE_TB
- en: The first step in getting the right partitioning for your DataFrame is to decide
    whether you want an index. Indexes are useful when filtering data by an indexed
    value, indexing, grouping, and for almost any other by-key operation. One such
    by-key operation would be a `groupby`, in which the column being grouped on could
    be a good candidate for the key. If you use a rolling window over a column, that
    column must be the key, which makes choosing the key relatively easy. Once you’ve
    decided on an index, you can call `set_index` with the column name of the index
    (e.g., `set_index("PostCode")`). This will, under most circumstances, result in
    a shuffle, so it’s a good time to size your partitions.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you’re unsure what the current key used for partitioning is, you can check
    the `index` property to see the partitioning key.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you’ve chosen your key, the next question is how to size your partitions.
    The advice in [“Partitioning/Chunking Collections”](ch03.xhtml#basic_partitioning)
    generally applies here: shoot for enough partitions to keep each machine busy,
    but keep in mind the general sweet spot of 100 MB to 1 GB. Dask generally computes
    pretty even splits if you give it a target number of partitions.^([5](ch04.xhtml#id558))
    Thankfully, `set_index` will also take `npartitions`. To repartition the data
    by postal code, with 10 partitions, you would add `set_index("PostCode", npartitions=10)`;
    otherwise Dask will default to the number of input partitions.'
  prefs: []
  type: TYPE_NORMAL
- en: If you plan to use rolling windows, you will likely need to ensure that you
    have the right size (in terms of key range) covered in each partition. To do this
    as part of `set_index`, you would need to compute your own divisions to ensure
    each partition has the right range of records present. Divisions are specified
    as a list starting from the minimal value of the first partition up to the maximum
    value of the last partition. Each value in between is a “cut” point between the
    pandas DataFrames that make up the Dask DataFrame. To make a DataFrame with partitions
    `[0, 100) [100, 200), [200, 300), [300, 500)`, you would write `df.set_index("NumEmployees",
    divisions=[0, 100, 200, 300, 500])`. Similarly, for the date range to support
    a rolling window of up to seven days from around the start of the COVID-19 pandemic
    to today, see [Example 4-13](#set_index_with_rolling_window).
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-13\. Dask DataFrame rolling window with `set_index`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Dask, including for rolling time windows, assumes that your partition index
    is monotonically increasing.^([6](ch04.xhtml#id559))
  prefs: []
  type: TYPE_NORMAL
- en: So far, you’ve had to specify the number of partitions, or the specific divisions,
    but you might be wondering if Dask can just figure that out itself. Thankfully,
    Dask’s repartition function has the ability to pick divisions for a given target
    size, as shown in [Example 4-14](#repartition_ex). However, doing this has a non-trivial
    cost, as Dask must evaluate the DataFrame as well as the repartition itself.
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-14\. Dask DataFrame automatic partitioning
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Dask’s `set_index` has a similar `partition_size` parameter but, as of this
    writing, [works only to reduce the number of partitions](https://oreil.ly/3I9Gm).
  prefs: []
  type: TYPE_NORMAL
- en: As you saw at the start of this chapter, when writing a DataFrame, each partition
    is given its own file, but sometimes this can result in files that are too big
    or too small. Some tools can accept only one file as input, so you need to repartition
    everything into a single partition. At other times, the data storage system is
    optimized for a certain file size, like the HDFS default block size of 128 MB.
    The good news is that techniques such as `repartition` and `set_index` solve these
    problems for you.
  prefs: []
  type: TYPE_NORMAL
- en: Embarrassingly Parallel Operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dask’s `map_partitions` function applies a function to each of the partitions
    underlying pandas DataFrames, and the result is also a pandas DataFrame. Functions
    implemented with `map_partitions` are embarrassingly parallel since they don’t
    require any inter-worker transfer of data.^([7](ch04.xhtml#id566)) Dask implements
    `map` with `map_partitions`, as well as many row-wise operations. If you want
    to use a row-wise operation that you find missing, you can implement it yourself,
    as shown in [Example 4-15](#filna_ex).
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-15\. Dask DataFrame `fillna`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: You aren’t limited to calling pandas built-ins. Provided that your function
    takes and returns a DataFrame, you can do pretty much anything you want inside
    `map​_parti⁠tions`.
  prefs: []
  type: TYPE_NORMAL
- en: The full pandas API is too long to cover in this chapter, but if a function
    can operate on a row-by-row basis without any knowledge of the rows before or
    after, it may already be implemented in Dask DataFrames using `map_partitions`.
  prefs: []
  type: TYPE_NORMAL
- en: When using `map_partitions` on a DataFrame, you can change anything about each
    row, including the key that it is partitioned on. If you *are* changing the values
    in the partition key, you *must* either clear the partitioning information on
    the resulting DataFrame with `clear_divisions()` *or* specify the correct indexing
    with `set_index`, which you’ll learn more about in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Incorrect partitioning information can result in incorrect results, not just
    exceptions, as Dask may miss relevant data.
  prefs: []
  type: TYPE_NORMAL
- en: Working with Multiple DataFrames
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pandas and Dask have four common functions for combining DataFrames. At the
    root is the `concat` function, which allows you to join DataFrames on any axis.
    Concatenating DataFrames is generally slower in Dask since it involves inter-worker
    communication. The other three functions are `join`, `merge`, and `append`, all
    of which implement special cases for common situations on top of `concat` and
    have slightly different performance considerations. Having good divisions/partitioning,
    in terms of key selection and number of partitions, makes a huge difference when
    working on multiple DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: Dask’s `join` and `merge` functions take most of the standard pandas arguments
    along with an extra optional one, `npartitions`. `npartitions` specifies a target
    number of output partitions, but it is used only for hash joins (which you’ll
    learn about in [“Multi-DataFrame Internals”](#multidataframe_internals_ch04_1689595647992)).
    Both functions automatically repartition your input DataFrames if needed. This
    is great, as you might not know the partitioning, but since repartitioning can
    be slow, explicitly using the lower-level `concat` function when you don’t expect
    any partitioning changes to be needed can help catch performance problems early.
    Dask’s `join` can take more than two DataFrames at a time only when doing a *left*
    or *outer* join type.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Dask has special logic to speed up multi-DataFrame joins, so in most cases,
    rather than doing `a.join(b).join(c).join(d)​.join(e)`, you will benefit from
    doing `a.join([b, c, d, e])`. However, if you are performing a left join with
    a small dataset, then the first syntax may be more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: When you combine or `concat` DataFrames by row (similar to a SQL UNION), the
    performance depends on whether divisions of the DataFrames being combined are
    *well ordered*. We call the divisions of a series of DataFrames well ordered if
    all the divisions are known and the highest division of the previous DataFrame
    is below that of the lowest division of the next. If any input has an unknown
    division, Dask will produce an output without known partitioning. With all known
    partitions, Dask treats row-based concatenations as a metadata-only change and
    will not perform any shuffle. This requires that there is no overlap between the
    divisions. There is also an extra `interleave_partitions` parameter, which will
    change the join type for row-based combinations to one without the input partitioning
    restriction and result in a known partitioner. Dask DataFrames with known partitioners
    can support faster look-ups and operations by key.
  prefs: []
  type: TYPE_NORMAL
- en: Dask’s column-based `concat` (similar to a SQL JOIN) also has restrictions around
    the divisions/partitions of the DataFrames it is combining. Dask’s version of
    `concat` supports only inner or full outer join, not left or right. Column-based
    joins require that all inputs have known partitioners and also result in a DataFrame
    with known partitioning. Having a known partitioner can be useful for subsequent
    joins.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Don’t use Dask’s `concat` when operating by row on a DataFrame with unknown
    divisions, as it will likely return incorrect results.^([8](ch04.xhtml#id579))
  prefs: []
  type: TYPE_NORMAL
- en: Multi-DataFrame Internals
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Dask uses four techniques—​hash, broadcast, partitioned, and `stack_partitions`—to
    combine DataFrames, and each has very different performance. These four functions
    do not map 1:1 with the join functions you choose from. Rather, Dask chooses the
    technique based on the indexes, divisions, and requested join type (e.g., outer/left/inner).
    The three column-based join techniques are hash joins, broadcast joins, and partitioned
    joins. When doing row-based combinations (e.g., `append`), Dask has a special
    technique called `stack_partitions` that is extra fast. It’s important that you
    understand the performance of each of these techniques and the conditions that
    will cause Dask to pick each approach:'
  prefs: []
  type: TYPE_NORMAL
- en: Hash joins
  prefs: []
  type: TYPE_NORMAL
- en: The default that Dask uses when no other join technique is suitable. Hash joins
    shuffle the data for all the input DataFrames to partition on the target key.
    They use the hash values of keys, which results in a DataFrame that is not in
    any particular order. As such, the result of a hash join does not have any known
    divisions.
  prefs: []
  type: TYPE_NORMAL
- en: Broadcast joins
  prefs: []
  type: TYPE_NORMAL
- en: Ideal for joining large DataFrames with small DataFrames. In a broadcast join,
    Dask takes the smaller DataFrame and distributes it to all the workers. This means
    that the smaller DataFrame must be able to fit in memory. To tell Dask that a
    DataFrame is a good candidate for broadcasting, you make sure it is all stored
    in one partition, such as by calling `repartition(npartitions=1)`.
  prefs: []
  type: TYPE_NORMAL
- en: Partitioned joins
  prefs: []
  type: TYPE_NORMAL
- en: Occur when combining DataFrames along an index where the partitions/​divi⁠sions
    are known for all the DataFrames. Since the input partitions are known, Dask is
    able to align the partitions between the DataFrames, involving less data transfer,
    as each output partition has less than a full set of inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Since partitioned and broadcast joins are faster, doing some work to help Dask
    can be worth it. For example, concatenating several DataFrames with known and
    aligned partitions/divisions and one unaligned DataFrame will result in an expensive
    hash join. Instead, try to either set the index and partition on the remaining
    DataFrame or join the less expensive DataFrames first and then perform the expensive
    join after.
  prefs: []
  type: TYPE_NORMAL
- en: The fourth technique, `stack_partitions`, is different from the other options
    since it doesn’t involve any movement of data. Instead, the resulting DataFrame
    partitions list is a union of the upstream partitions from the input DataFrames.
    Dask uses `stack_partitions` for most row-based combinations except when all of
    the input DataFrame divisions are known, they are not well ordered, and you ask
    Dask to `interleave_partitions`. The `stack_partitions` technique is able to provide
    known partitioning in its output only when the input divisions are known and well
    ordered. If all of the divisions are known but not well ordered and you set `interleave​_parti⁠tions`,
    Dask will use a partitioned join instead. While this approach is comparatively
    inexpensive, it is not free, and it can result in an excessively large number
    of partitions, requiring you to repartition anyway.
  prefs: []
  type: TYPE_NORMAL
- en: Missing Functionality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Not all multi-DataFrame operations are implemented, like `compare`, which leads
    us into the next section about the limitations of Dask DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: What Does Not Work
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dask’s DataFrame implements most, but not all, of the pandas DataFrame API.
    Some of the pandas API is not implemented in Dask because of the development time
    involved. Other parts are not used to avoid exposing an API that would be unexpectedly
    slow.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes the API is just missing small parts, as both pandas and Dask are under
    active development. An example is the `split` function from [Example 2-10](ch02.xhtml#wc_dataframe).
    In local pandas, instead of doing `split().explode()`, you could have called `split(expand=true)`.
    Some of these missing parts can be excellent places for you to get involved and
    [contribute to the Dask project](https://oreil.ly/Txd_R) if you are interested.
  prefs: []
  type: TYPE_NORMAL
- en: Some libraries do not parallelize as well as others. In these cases, a common
    approach is to try to filter or aggregate the data down enough that it can be
    represented locally and then apply the local libraries to the data. For example,
    with graphing, it’s common to pre-aggregate the counts or take a random sample
    and graph the result.
  prefs: []
  type: TYPE_NORMAL
- en: While much of the pandas DataFrame API will work, before you swap in Dask DataFrame,
    it’s important to make sure you have good test coverage to catch the situations
    where it does not.
  prefs: []
  type: TYPE_NORMAL
- en: What’s Slower
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Usually, using Dask DataFrames will improve performance, but not always. Generally,
    smaller datasets will perform better in local pandas. As discussed, anything involving
    shuffles is generally slower in a distributed system than in a local one. Iterative
    algorithms can also produce large graphs of operations, which are slow to evaluate
    in Dask compared to traditional greedy evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Some problems are generally unsuitable for data-parallel computing. For example,
    writing out to a data store with a single lock that has more parallel writers
    will increase the lock contention and may make it slower than if a single thread
    was doing the writing. In these situations, you can sometimes repartition your
    data or write individual partitions to avoid lock contention.
  prefs: []
  type: TYPE_NORMAL
- en: Handling Recursive Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dask’s lazy evaluation, powered by its lineage graph, is normally beneficial,
    allowing it to combine steps automatically. However, when the graph gets too large,
    Dask can struggle to manage it, which often shows up as a slow driver process
    or notebook, and sometimes as an out-of-memory exception. Thankfully, you can
    work around this by writing out your DataFrame and reading it back in. Generally,
    Parquet is the best format for doing this as it is space-efficient and self-describing,
    so no schema inference is required.
  prefs: []
  type: TYPE_NORMAL
- en: Re-computed Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another challenge of lazy evaluation is if you want to reuse an element multiple
    times. For example, say you want to load a few DataFrames and then compute multiple
    pieces of information. You can ask Dask to keep a collection (including DataFrame,
    series, etc.) in memory by running `client.persist(collection)`. Not all re-computed
    data needs to be avoided; for example, if loading the DataFrames is fast enough,
    it might be fine not to persist them.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Like other functions in Dask, `persist()` does not modify the DataFrame—​and
    if you call functions on it you will still have your data re-computed. This is
    notably different from Apache Spark.
  prefs: []
  type: TYPE_NORMAL
- en: How Other Functions Are Different
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For performance reasons, various parts of Dask DataFrames behave a little differently
    than local DataFrames:'
  prefs: []
  type: TYPE_NORMAL
- en: '`reset_index`'
  prefs: []
  type: TYPE_NORMAL
- en: The index will start back over at zero on each partition.
  prefs: []
  type: TYPE_NORMAL
- en: '`kurtosis`'
  prefs: []
  type: TYPE_NORMAL
- en: This function does not filter out NaNs and uses SciPy defaults.
  prefs: []
  type: TYPE_NORMAL
- en: '`concat`'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of coercing category types, each category type is expanded to the union
    of all the categories it is concatenated with.
  prefs: []
  type: TYPE_NORMAL
- en: '`sort_values`'
  prefs: []
  type: TYPE_NORMAL
- en: Dask supports only single-column sorts.
  prefs: []
  type: TYPE_NORMAL
- en: Joining multiple DataFrames
  prefs: []
  type: TYPE_NORMAL
- en: When joining more than two DataFrames at the same time, the join type must be
    either outer or left.
  prefs: []
  type: TYPE_NORMAL
- en: When porting your code to use Dask DataFrames, you should be especially mindful
    anytime you use these functions, as they might not exactly work in the axis you
    intended. Work small first and test the correctness of the numbers, as issues
    can often be tricky to track down.
  prefs: []
  type: TYPE_NORMAL
- en: When porting existing pandas code to Dask, consider using the local single-machine
    version to produce test datasets to compare the results with, to ensure that all
    changes are intentional.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data Science with Dask DataFrame: Putting It Together'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dask DataFrame has already proven to be a popular framework for big data uses,
    so we wanted to highlight a common use case and considerations. Here, we use a
    canonical data science challenge dataset, the New York City yellow taxicab, and
    walk through what a data engineer working with this dataset might consider. In
    the subsequent chapters covering ML workloads, we will be using many of the DataFrame
    tools to build on.
  prefs: []
  type: TYPE_NORMAL
- en: Deciding to Use Dask
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As discussed earlier, Dask excels in data-parallel tasks. A particularly good
    fit is a dataset that may already be available in columnar format, like Parquet.
    We also assess where the data lives, such as in S3 or in other remote storage
    options. Many data scientists and engineers would probably have a dataset that
    cannot be contained on a single machine or cannot be stored locally due to compliance
    constraints. Dask’s design lends itself well to these use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our NYC taxi data fits all these criteria: the data is stored in S3 by the
    City of New York in Parquet format, and it is easily scalable up and down, as
    it is partitioned by dates. Additionally, we evaluate that the data is structured
    already, so we can use Dask DataFrame. Since Dask DataFrames and pandas DataFrames
    are similar, we can also use a lot of existing workflows for pandas. We can sample
    a few of these, do our exploratory data analysis in a smaller dev environment,
    and then scale it up to the full dataset, all with the same code. Note that for
    [Example 4-16](#ex_load_nyc_taxi), we use row groups to specify chunking behavior.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-16\. Dask DataFrame loading multiple Parquet files
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Exploratory Data Analysis with Dask
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step of data science often consists of exploratory data analysis (EDA),
    or understanding the dataset and plotting its shape. Here, we use Dask DataFrames
    to walk through the process and examine the common troubleshooting issues that
    arise from nuanced differences between pandas DataFrame and Dask DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Loading Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first time you load the data into your dev environment, you might encounter
    block size issues or schema issues. While Dask tries to infer both, at times it
    cannot. Block size issues will often show up when you call `.compute()` on trivial
    code and see one worker hitting the memory ceiling. In that case, some manual
    work would be involved in determining the right chunk size. Schema issues would
    show up as an error or a warning as you read the data, or in subtle ways later
    on, such as mismatching float32 and float64\. If you know the schema already,
    it’s a good idea to enforce that by specifying dtypes at reading.
  prefs: []
  type: TYPE_NORMAL
- en: As you further explore a dataset, you might encounter data printed by default
    in a format that you don’t like, for example, scientific notation. The control
    for that is through pandas, not Dask itself. Dask implicitly calls pandas, so
    you want to explicitly set your preferred format using pandas.
  prefs: []
  type: TYPE_NORMAL
- en: Summary statistics on the data work just like `.describe()` from pandas, along
    with specified percentiles or `.quantile()`. Remember to chain multiple computes
    together if you are running several of these, which will save compute time back
    and forth. Using Dask DataFrame `describe` is shown in [Example 4-17](#ex_describe_percentiles).
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-17\. Dask DataFrame describing percentiles with pretty formatting
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Plotting Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Plotting data is often an important step in getting to know your dataset. Plotting
    big data is a tricky subject. We as data engineers often get around that issue
    by first working with a smaller sampled dataset. For that, Dask would work alongside
    a Python plotting library such as matplotlib or seaborn, just like pandas. The
    advantage of Dask DataFrame is that we are now able to plot the entire dataset,
    if desired. We can use plotting frameworks along with Dask to plot the entire
    dataset. Here, Dask does the filtering, the aggregation on the distributed workers,
    and then collects down to one worker to give to a non-distributed library like
    matplotlib to render. Plotting a Dask DataFrame is shown in [Example 4-18](#ex_plot_distances).
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-18\. Dask DataFrame plotting trip distance
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note that if you’re used to the NumPy logic, you will have to think of the Dask
    DataFrame layer when plotting. For example, NumPy users would be familiar with
    `df[col].values` syntax for defining plotting variables. The `.values` mean a
    different action in Dask; what we pass is `df[col]` instead.
  prefs: []
  type: TYPE_NORMAL
- en: Inspecting Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pandas DataFrame users would be familiar with `.loc()` and `.iloc()` for inspecting
    data at a particular row or column. This logic translates to Dask DataFrame, with
    important differences in `.iloc()` behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: A sufficiently large Dask DataFrame will contain multiple pandas DataFrames.
    This changes the way we should think about numbering and addressing indices. For
    example, `.iloc()` (a way to access the positions by index) doesn’t work exactly
    the same for Dask, since each smaller DataFrame would have its own `.iloc()` value,
    and Dask does not track the size of each smaller DataFrame. In other words, a
    global index value is hard for Dask to figure out, since Dask will have to iteratively
    count through each DataFrame to get to an index. Users should check `.iloc()`
    on their DataFrame and ensure that indices return the correct values.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Be aware that calling methods like `.reset_index()` can reset indices in each
    of the smaller DataFrames, potentially returning multiple values when users call
    `.iloc()`.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you’ve learned how to understand what kinds of operations are
    slower than you might expect with Dask. You’ve also gained a number of techniques
    to deal with the performance differences between pandas DataFrames and Dask DataFrames.
    By understanding the situations in which Dask DataFrames performance may not meet
    your needs, you’ve also gained an understanding of what problems are not well
    suited to Dask. So that you can put this all together, you’ve also learned about
    Dask DataFrame IO options. From here you will go on to learn more about Dask’s
    other collections and then how to move beyond collections.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you have learned what may cause your Dask DataFrames to behave
    differently or more slowly than you might expect. This same understanding of how
    Dask DataFrames are implemented can help you decide whether distributed DataFrames
    are well suited to your problem. You’ve also seen how to get datasets larger than
    a single machine can handle into and out of Dask’s DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch04.xhtml#id479-marker)) See [“Partitioning/Chunking Collections”](ch03.xhtml#basic_partitioning)
    for a review of partitioning.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch04.xhtml#id522-marker)) [FSSPEC documentation](https://oreil.ly/ZfcRv)
    includes the specifics for configuring each of the backends.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch04.xhtml#id539-marker)) This can lead to out-of-memory exceptions while
    executing the aggregation. The linear growth in storage requires that (within
    a constant factor) all the data must be able to fit on a single process, which
    limits how effective Dask can be.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch04.xhtml#id545-marker)) Alternate algorithms for exact quantiles depend
    on more shuffles to reduce the space overhead.
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch04.xhtml#id558-marker)) Key-skew can make this impossible for a known
    partitioner.
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch04.xhtml#id559-marker)) Strictly increasing with no repeated values
    (e.g., 1, 4, 7 is monotonically increasing, but 1, 4, 4, 7 is not).
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch04.xhtml#id566-marker)) [Embarrassingly parallel problems](https://oreil.ly/30938)
    are ones in which the overhead of distributed computing and communication is low.
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch04.xhtml#id579-marker)) Dask assumes the indices are aligned when there
    are no indices present.
  prefs: []
  type: TYPE_NORMAL
