- en: 6 Testing and prompt engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Understanding the importance of testing Copilot code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using closed-box versus open-box testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Addressing errors by Copilot by modifying prompts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working through examples of testing Copilot-generated code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In chapter 3, we first started to see the importance of testing the code produced
    by Copilot. Testing is an essential skill for anyone writing software because
    it gives you confidence that the code is functioning properly. In this chapter,
    we’ll learn how to test our code thoroughly and how to help Copilot fix code that
    doesn’t work by modifying our prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Testing is an essential skill that you’ll need to learn how to do well on your
    own, so that you’re able to check that the code works correctly. Copilot can generate
    tests and has been improving in the quality of tests it produces, but we encourage
    you to hold off on using Copilot to generate tests just yet because you need to
    learn to do this well enough on your own to be able to verify that the tests Copilot
    produces are reasonable. This will be true of the next few chapters as well—problem
    decomposition, testing, and debugging are all skills that are essential to learn
    how to do on your own, before asking for Copilot’s help, as you need to know how
    to do it on your own to know if Copilot is doing something reasonable.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Why it’s crucial to test code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Back in chapter 3, we mentioned that you should test code to make sure it’s
    correct. Unfortunately, in our experience, beginning programmers seem to have
    an aversion to testing! Why? We think a couple of things are at play. The first
    is that there’s this well-documented problem nicknamed the *Superbug*, which is
    that humans, when first learning to code, think the computer can understand the
    intent of the code and respond accordingly [1]. Because they wrote the code and
    the code made sense to them, they find it hard to even imagine that the code might
    not work. The second problem is compounded on the first: if you think your code
    is right, testing can only bring you bad news. If you don’t test, you can’t find
    out if the code is wrong. It’s like the old saying about putting your head in
    the sand.'
  prefs: []
  type: TYPE_NORMAL
- en: Professional software engineers take a completely different approach than new
    programmers. They take testing extremely seriously because a mistake in the code
    can have significant consequences for their company. No one wants to be the person
    whose code causes the company to lose tons of revenue, lets hackers gain access
    to confidential user data, or has the self-driving car cause an accident. Given
    the cost of a mistake, it makes more sense to assume the code is wrong until proven
    otherwise. Only after testing it extensively should we trust that it’s working
    correctly. And, companies don’t just test the code once, they keep the tests in
    their system so every time someone changes code, tests are run not just on the
    changed code but also on any code that the changed code might affect (this is
    called regression testing).
  prefs: []
  type: TYPE_NORMAL
- en: Companies take this so seriously that they often write their tests *before*
    writing their code in a process called test-driven development (TDD). This ensures
    everyone agrees on what the code should or shouldn’t do. We don’t think you (as
    readers) need to take this approach for the programs you’re writing with this
    book, but we mention it here to convey how crucial it is to test. Thinking about
    testing before writing code can help you understand what the code should do and
    that will help you write better prompts. In fact, you can include test cases directly
    in your prompts!
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s remember what we know about Copilot: it makes mistakes. We shouldn’t
    assume anything about the correctness of any code given to us by Copilot. All
    this is to say that any code you’re given by Copilot should be tested before you
    trust it.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Closed-box and open-box testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are two ways that software engineers commonly test their code. The first
    is called closed-box testing, and this approach assumes you know nothing about
    how the code works. As such, this kind of testing involves varying the inputs
    and observing the outputs. We often see closed-box testing applied to functions
    or entire programs. The advantage of closed-box testing is that you don’t need
    to look at the code to perform the tests and can therefore focus simply on the
    desired behavior.
  prefs: []
  type: TYPE_NORMAL
- en: The second approach to testing is called open-box testing, and in this approach,
    we look at the code to see where the errors might occur. The advantage of open-box
    testing is that by looking at the particular structure of the code, we may see
    where the code is likely to fail and can design additional tests specific to that
    code. We’ll use both closed-box and open-box testing to come up with test cases
    that combine to strengthen our testing. A brief summary of closed-box and open-box
    testing appears in table 6.1\. In this section, let’s look at how we might test
    some functions using these approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Table 6.1 Brief overview of closed-box and open-box testing
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Closed-box testing | Open-box testing |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Requires understanding the function specification to test  | Requires both
    the function specification and the code that implements the function to test  |'
  prefs: []
  type: TYPE_TB
- en: '| Tests don’t require an understanding of what the code does.  | Tests should
    be tailored based on how the code was written.  |'
  prefs: []
  type: TYPE_TB
- en: '| Testers need not have technical expertise about the code they’re testing.  |
    Testers need to be able to understand the code sufficiently well to determine
    which tests may be more important.  |'
  prefs: []
  type: TYPE_TB
- en: '| Tests the function by varying inputs and checking against expected results  |
    Can test the function in the same way as closed-box testing but can also have
    more granular tests within a function  |'
  prefs: []
  type: TYPE_TB
- en: 6.2.1 Closed-box testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s imagine we’re trying to test a function that takes in a list of words
    (strings) and returns the longest word. To be more precise, the function signature
    would be
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The expected input is a list of words. The expected output is the word in that
    list with the most characters. In the event that multiple words are tied for the
    most characters, it should return the first word of that length.
  prefs: []
  type: TYPE_NORMAL
- en: Shorthand for expressing test cases
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: When writing tests for a function, the standard format is to write the function
    name and its input along with the desired outcome. For example, the call
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: means that if we call the function `longest_word` with the input list `['a',`
    `'bb',` `'ccc']`, then the value returned from the function should be `'ccc'`.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two categories for which we typically think about writing test cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Common use cases—*These cases include some standard inputs you could imagine
    the function receiving and the corresponding result.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Edge cases—*These cases are uncommon but possible cases that might break
    the code. These are inputs that might test some of the rules for the function
    in more depth or contain unexpected inputs (e.g., a list with all empty strings).**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Looking back at our `longest_word` function signature in the previous example,
    let’s think about some test cases we might use to test it. Later in the chapter,
    we’ll see how to actually run these test cases to determine whether our code is
    working correctly. Let’s start with *common use cases*. We would likely want to
    include a test with just a few words where one word is longer than the others:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s another test with more words with the longest word appearing elsewhere
    in the list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'And last, let’s have a test with just one word:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: If the program is working for these common uses, our next step would be to think
    about some *edge cases.* Let’s consider some edge cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Say we want to check whether the function conforms to our description by returning
    the first word when there are multiple words of the same length. This test may
    be considered a common case or an edge case, depending on whom you ask:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'What do we do if all the words in the list have no characters? A string with
    no characters is called an *empty string* and is written as just an empty pair
    of quotes. If all we have is a list of empty strings, then the longest word is
    just the empty string! So, a test with all empty strings should just give us back
    an empty string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The term *edge case* comes from the fact that errors often happen at the “edge”
    of execution, meaning either the first or last element. In many loops, mistakes
    can be made when the loop is starting (e.g., forgetting or mishandling the first
    element in the list) or at the end (e.g., forgetting the last element or going
    past the end of the list and trying to access an element that doesn’t exist).
    Especially when the code is likely to have loops processing many elements, you’ll
    want to watch the behavior at the start and end of the loop.
  prefs: []
  type: TYPE_NORMAL
- en: Incorrect input testing
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Another category of tests will test the function on how it responds when given
    incorrect input. We won’t talk about this much in our book because we’re assuming
    you’re correctly calling your own functions, but in production code, this kind
    of testing can be common. A few examples of calling this function with incorrect
    inputs might be to give the function a nonexisting list by using the value `None`
    instead of an actual list (e.g., `longest_word(None)`), to give the function an
    empty list (e.g., `longest_word([])`), to give the function a list with integers
    as input (e.g., `longest_word` `([1,2])`), or to provide a list of strings but
    have the strings contain spaces or more than single words (e.g., `longest_word(['hi`
    `there',` `'` `my` `',` `'friend'])`). It’s hard to say what the function should
    do when given incorrect input, and programmers need to decide whether they care
    about this in larger code bases, but we’ll ignore this category of tests in this
    book because we’ll assume you’ll call your own functions in ways that the functions
    are designed to handle.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.2 How do we know which test cases to use?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In chapter 3, we discussed that good testing involves capturing different categories
    of function calls. One way to find these categories is by using the types of parameters
    and varying their values.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if the function takes a string or list as a parameter, it may make
    sense to test the case when that string or list is empty, has one element, and
    has multiple elements. If we’re trying to test multiple elements, we might use
    four elements, for example. It likely also wouldn’t make sense to test with five
    or six elements or more because if our code works with four elements, it’s unlikely
    that something could suddenly go wrong when we increase to five. Sometimes, some
    of these test cases may not make sense for a given function; for example, it wouldn’t
    make sense to ask for the longest word in a list that didn’t have any words in
    it, so we wouldn’t test the empty list for our `longest_word` function.
  prefs: []
  type: TYPE_NORMAL
- en: As another example, if a function takes two numbers as parameters, it may make
    sense to test when one number is zero, both numbers are zero, one number is negative,
    both numbers are negative, and both numbers are positive.
  prefs: []
  type: TYPE_NORMAL
- en: Another way to find categories is to think about the specific task of the function.
    For example, for our `longest_word` function, it’s supposed to be finding the
    longest word, so we should test that it’s actually doing that in a typical case.
    And, if multiple words are the longest, it’s supposed to return the first of those,
    so we should have a test case where the list has multiple words that are the longest.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the categories to test is a mix of science and art. We’ve given you
    some rules of thumb here, but what counts as useful test cases often depends on
    the specific functionality being tested. As is so often the case, practicing your
    testing skill is the best way to improve your ability to write useful tests that
    ultimately help you make your code better.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.3 Open-box testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The big difference between open-box testing and closed-box testing is that open-box
    testing examines the code to see if there are additional kinds of test cases to
    check. In theory, closed-box testing may be sufficient to fully test the function,
    but open-box testing tends to give you more ideas about where the code might be
    failing. Let’s say we asked for Copilot to write our `longest_word` function and
    got back the code shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.1 Function to find the longest word (incorrect!)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '#1 >= is wrong. It should be >.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this example, we intentionally introduced an error in the code to help
    explain the role of open-box testing. Let’s say that when you were thinking through
    your test cases, you forgot to test what happens when there are two words in the
    list of `words` that both have the most characters. Well, reading through this
    code you might spot the following `if` statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'When reading the `if` statement, you might notice that it’s going to update
    the longest word in the list of words when the length of the most recent element
    is greater than *or equal* to the longest word we’ve seen so far. This is a mistake;
    it should be `>`, not `>=`, but suppose you aren’t sure. This would motivate you
    to write a test case like the one we described previously that has multiple words,
    more than one of which is the longest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This test will fail with the code in listing 6.1 as it would return `'dog'`
    rather than the correct answer of `'cat'`. The test failing is valuable information
    that the code in listing 6.1 is incorrect.
  prefs: []
  type: TYPE_NORMAL
- en: As we’ve said, open-box testing is useful because it leads to test cases that
    follow the structure of the code itself. For example, if our code is using a loop,
    we’ll find that loop when doing open-box testing. The loop in listing 6.1 is correct,
    but by seeing the loop in our code, we’ll be reminded to test the edge cases to
    make sure it’s properly handling the first element, the last element, and an empty
    list. In sum, knowing how the code is processing the input often offers insight
    into when the program might be misfunctioning.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 How to test your code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are a number of good ways to test your code that vary from quick tests
    you might perform just to check if your code is working for yourself to tests
    that are built into a company’s regression test suite. For production code, Python
    programmers typically use testing tools more powerful and full-featured than what
    we’re about to demonstrate in this chapter. The most common of those tools is
    pytest, which is a module that needs to be installed before it can be used. We
    feel that pytest is beyond what we need here to introduce the core ideas of testing.
    We’ll focus on more lightweight testing to help you gain confidence that the code
    from Copilot works properly. We can do that either by testing at the Python prompt
    or using a built-in Python module called doctest.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.1 Testing using the Python prompt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first way to test is in the Python prompt through the interactive window
    like we have in the previous chapters. The advantage of this testing is that it
    can be quick to run, and you can easily add more tests as a result of output from
    the previous test. The tests we’ve run so far are examples of testing with the
    Python prompt. For example,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In running that test, if you expected the result to be `'cat'`, you’d be pleased
    to see that result. However, if the test shows that your code was wrong, you now
    have the opportunity to go back to fix it.
  prefs: []
  type: TYPE_NORMAL
- en: After you fix the code, you’ll want to test the new code. Here is where you
    may go wrong with testing using the Python prompt alone. When you come back to
    test the code you just changed, you might be tempted to run *just* the test case
    that had failed previously. However, in fixing the code to correctly address the
    test case that had failed, you could have introduced an error that would cause
    the *previous* test cases that had already passed to now fail. What you really
    want then is a way to run not just your current test but all previous tests as
    well.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.2 Testing in your Python file (we won’t be doing it this way)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It would be tempting to then put all your test cases in your Python program
    (outside a function, so in the equivalent of a main function) so they can all
    run. This solution addresses the problem with Python prompts that we just described,
    but it introduces a new problem. What happens when you want your Python program
    to perform the main task for which it was designed rather than just run tests?
    You could delete all the tests, but the point was running them again if so desired.
    You could comment them out so you can run them in the future, but that’s not a
    very clean solution either. What we want then is a way to run all our tests on
    our functions when we want to but still have the ability to run the program. The
    way to do this is using a module called doctest.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.3 doctest
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The doctest module is built in to Python. The great thing about using doctest
    is that we simply add our test cases to the docstring that describes the function.
    This beefed-up docstring serves a dual purpose. First, we can use doctest to run
    all those test cases whenever we’d like. Second, it can sometimes help Copilot
    generate better code in the first place or fix already-written code that isn’t
    quite working. Let’s write that `longest_word` function with all the test cases
    included and ready to be executed with doctest (see listing 6.2).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.2 Using doctest to test the `longest_word` function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Shows the test cases for doctest'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Shows the correct code for the function'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Code (in main) that calls doctest to perform the test'
  prefs: []
  type: TYPE_NORMAL
- en: In this code, we see the docstring with our test cases provided as the prompt
    to Copilot. Copilot generated the correct code to implement this function. We
    then manually wrote the last two lines of the code to perform the testing. When
    run, we get the output in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.3 Doctest output from running our program in listing 6.2
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '#1 First test in longest_word passed'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Second test in longest_word passed'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Third test in longest_word passed'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Fourth test in longest_word passed'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Fifth test in longest_word passed'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 There are no tests in main (outside the function).'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 longest_word passed all tests.'
  prefs: []
  type: TYPE_NORMAL
- en: '#8 0 failed is what you hope to see.'
  prefs: []
  type: TYPE_NORMAL
- en: 'From this output, we can see that each test ran and each test passed. The reason
    these tests ran is because of the last two lines that we added in listing 6.2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In the first line, we import the doctest module. That’s the module that helps
    us test our code by automatically running the test cases when we run our program.
    In the second line, we’re calling the `testmod` function from the doctest module.
    That function call tells doctest to perform all the tests; the argument `verbose=True`
    tells doctest to give us the outcome for all tests, whether they pass or not.
    If we switch to `verbose=False`, it will only give us output if test cases fail
    (`verbose=False` is actually the default, so you can just call the function with
    no arguments, and it will default to not providing output unless one or more tests
    fail). This can be a nice feature as we can keep the doctest running and only
    see the output when tests fail.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, our code passed all the test cases. But let’s experience what
    happens when our code doesn’t pass.
  prefs: []
  type: TYPE_NORMAL
- en: If we find a word that’s the same length as our current longest word, we should
    ignore it because we always want to return the first longest word if there are
    multiple words tied for the longest. That’s why the correct thing to do is to
    use `>` in the `if` statement (finding a new longest word only if it’s truly longer
    than our current longest word) rather than `>=`.
  prefs: []
  type: TYPE_NORMAL
- en: We can break the code in listing 6.2 then by changing the `>` to `>=`, which
    will cause it to select the last word of the longest length rather than the first.
    Let’s change the following line from
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: to
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Now, the tests shouldn’t all pass. In addition, let’s change the last line to
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'By providing no arguments to the `testmod` function, `verbose` is now set to
    `False`. When we run the code, this is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Doctest conveniently tells us which test was run, what the expected output was,
    and what the function produced instead. This would catch the bug and allow us
    to go back to fix the error.
  prefs: []
  type: TYPE_NORMAL
- en: Test cases aren’t automatically run by Copilot
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We commonly hear the following question: Why doesn’t Copilot directly incorporate
    the test cases when generating code? For example, if we add test cases, it would
    be nice if Copilot could try to generate functions and only provide us with the
    code that would pass those test cases. Unfortunately, there are some technical
    challenges in doing this, and as of the time of writing, this feature isn’t yet
    included. So, if you add test cases, it just improves the prompt to Copilot but
    doesn’t guarantee that the Copilot code suggestion passes those tests.'
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we’ve seen how to run our tests with both the Python prompt and
    doctest. Now that we know how to test our code, let’s think about how this modifies
    our code design cycle.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Revisiting the cycle of designing functions with Copilot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In chapter 3, we gave you an early version of how to design functions in figure
    3.3\. At that point, we didn’t know as much about examining our code (which we
    learned in chapters 4 and 5) or as much about how to test our code as we do now.
    As such, let’s create a new version of this cycle (figure 6.1) to reflect our
    new understanding.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 The function design cycle with Copilot, augmented to include more
    about testing
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The figure is a bit more complex than before, but if we examine it closely,
    we can see much of the original process is retained. The things that have been
    added or changed include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: When we write the prompt, we may include doctests as part of that initial prompt
    to help Copilot in generating the code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having made our way through chapters 4 and 5, we’re in good shape to read the
    code to see whether it behaves properly, so we now have an additional step to
    address what happens when the initial code from Copilot looks wrong. If that occurs,
    we’ll use Ctrl-Enter to explore the Copilot suggestions to hopefully find a solution.
    If we can find such a solution, we’ll select it and move forward. If we can’t,
    we’ll need to revise our prompt to help Copilot generate improved suggestions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After finding code that looks like it could be correct, we’ll run doctest to
    see whether the code passes the doctests we included in the prompt (if we didn’t
    include any, it will pass automatically). If doctest passes, then we can continue
    adding tests and checking them until we’re happy that the code is functioning
    properly. If doctest fails, we’ll need to figure out how to modify the prompt
    to address the failed tests. Once the prompt is modified, it will hopefully help
    Copilot generate new code that may be capable of passing the tests that we’ve
    provided.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With this new workflow, we’re in a much better position not only to determine
    whether our code is working properly but also to fix the code if it isn’t already
    working. In the next chapters, we’ll give ourselves even more tools to help when
    the code isn’t working properly, but for now, let’s put this all together by solving
    a new problem and testing the Copilot solutions using the workflow we described
    here.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5 Full testing example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s try a problem that we might need to solve to help students decide where
    to sit in in-person classes. Although we’re talking about finding empty seats
    in a classroom, this problem is the same as trying to find the number of empty
    seats in a concert hall, theater, movie theater, or office layout.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we have the classroom layout as a two-dimensional list with rows of
    seats. Each seat either contains a space (`'` `'`) or a student (`'S'`) to represent
    an empty or occupied seat, respectively. We’re going to ask Copilot to create
    a function to determine which row we could add the most students to, and along
    the way, we’ll design the tests needed to check whether the code is working properly.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.1 Finding the most students we can add to a row
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We want to figure out the largest number of new students we could add in a single
    row in the classroom. (Equivalently, we’re looking for the largest number of empty
    seats in any row.) For this, let’s start with a somewhat ambiguous prompt and
    see how well Copilot does.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.4 Largest number of students we can add to a row
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '#1 This is a somewhat ambiguous part of the prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the sake of this example, our prompt wasn’t very good. “How many new students
    can sit in a row?” is a reasonable starting point to give to a human, but it’s
    not that specific, and it’s easy to imagine Copilot struggling to interpret what
    we mean. Copilot might get confused about whether we’re counting the number of
    students or the number of empty seats in a row, or it might return the total number
    of available seats in the classroom rather than in a row. It turns out that our
    prompt confused Copilot, and the code isn’t correct, but before we dive into the
    code, let’s think about what tests we should run. We’ve come up with the following
    set of test cases:'
  prefs: []
  type: TYPE_NORMAL
- en: A classroom with some number of consecutive empty seats and some number of nonconsecutive
    empty seats to make sure it isn’t just counting consecutive empty seats
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A classroom with no empty seats to make sure it returns 0 in that case
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A classroom with a row full of empty seats to make sure all are counted, including
    the first and last seats (edge case)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A classroom with multiple rows with the same number of empty seats to make sure
    it returns just one of those values (and not, perhaps, the sum of the number of
    empty seats across all of these rows)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start by adding the first test case and adding the doctest code to run
    the test, as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.5 Largest number of students we can add to a row
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Doctest for a common case. The \ is necessary in docstring test cases if
    you need to do a newline.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When we run this code, we get this output (we cleaned up the formatting of
    the classroom list manually to help with the readability of the answer):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Although we’d prefer the code to work, we appreciate that the first test case
    found an error. The row with the most empty seats is the third row with four seats
    available. But the code from Copilot is incorrectly telling us the answer is six.
    That’s pretty odd. Even without reading the code, you might hypothesize that it’s
    counting either the number of seats per row or the maximum number of students
    seated per row. Our test case had a full row of students in the second row, so
    it’s hard to tell. What we can do is change the classroom to be
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '#1 We removed the first student from the second row.'
  prefs: []
  type: TYPE_NORMAL
- en: So, the second row now has five students. When we run the code again, the test
    again fails with the code giving us an answer of five. It seems that the code
    isn’t just telling us the number of seats per row. It must be doing something
    related to where students are sitting. Our next step is to improve the prompt
    and determine whether we can get better code from Copilot, but for completeness,
    let’s first explain what the code was really doing in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.6 Walkthrough of the incorrect code from Copilot
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Starts with the max_students initialized to 0'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 For each row in the classroom'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Initializes the student counter to 0 for this row'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 For each seat in the row'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 If there is a student in the seat, increment the counter.'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 If there is an empty seat, reset the counter.'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Checks to see if the student counter is bigger than seen before and, if
    so, makes that the new maximum seen before'
  prefs: []
  type: TYPE_NORMAL
- en: You can see from the code description what is happening per line, but at a high
    level, this code is counting the number of consecutive students in each row. It
    does this by initializing a counter to 0 for each row and then incrementing that
    counter as long as it keeps seeing a student in a seat. It also resets the counter
    as soon as it sees an empty seat. The `if` statement at the end of the inner loop
    is a pretty standard way of keeping track of the largest of something seen before,
    and in this case, it’s keeping track of the largest number of consecutive students
    seen. That’s not at all what we wanted, and our poor prompt is partially to blame.
    The key piece, though, is that our test lets us know the code is incorrect. (If
    you spotted the error yourself in reading the code, that’s great too!)
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.2 Improving the prompt to find a better solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s rewrite the prompt, keep the test case, and see whether we can do better
    in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.7 Trying again to find the largest number of students
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The improved prompt says we specifically want the maximum number of '' ''
    characters in any given row.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 count is a list function that returns the number of the argument in the
    list.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Code to keep track of maximum seats'
  prefs: []
  type: TYPE_NORMAL
- en: To get this solution, we had to look through the possible Copilot solutions
    using Ctrl-Enter. Some of the solutions now counted the consecutive occurrences
    of `'` `'`, whereas others, like the one in listing 6.7, passed the doctest. Oddly,
    the first time we tried the improved prompt, the suggested solution was correct.
    This is another reminder of why nondeterminism in the Copilot output makes testing
    so important.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a minute and look at what made this second prompt better than the
    first. Both prompts had
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The part of the prompt that led to us receiving the wrong answer was
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The part of the prompt that yielded a correct answer was
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: You can never really know why a large language model (LLM) like Copilot produces
    the answer it does, but let’s remember that it’s trained to just make predictions
    of next words based on the words it’s been given and words that were in its training
    data (i.e., lots of code in GitHub).
  prefs: []
  type: TYPE_NORMAL
- en: The first prompt asks Copilot to make some inferences, some of which it does
    well, and some not so well. The prompt, in a sense, is asking Copilot to know
    what a row is in a list of lists. Thankfully, that’s really common in programming,
    so it had no problem there.
  prefs: []
  type: TYPE_NORMAL
- en: Then, the prompt asks Copilot to make the basic logical step of inferring that
    an empty seat is where a *new* student could sit. Here is where Copilot struggled.
    We suspect that because we’re asking about new students sitting in a row, it wasn’t
    able to make the jump to realize that “new” students would require figuring out
    how many students you can *add* or, in other words, how many empty seats there
    are. Instead, Copilot focused on the “students °.°.°. in a row” part of the prompt
    and started counting students in each row. It could have also used the function
    name (which, admittedly, could be better; i.e., `max_empty_seats_per_row`) to
    think it needs to count the maximum number of students. That’s not what we want,
    but we can understand how Copilot makes this mistake.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s talk about why, in response to our vague first prompt, Copilot decided
    to count *consecutive* students in a given row. Maybe counting consecutive students
    is a more common pattern in Copilot’s training data. Maybe it’s because “sit in
    a row” could be interpreted as “sit consecutively.” Or maybe it’s because when
    we were coding this example, we’d been working on another version of the problem
    that asked for consecutive empty seats, and Copilot remembered that conversation.
    We don’t know why Copilot gave us this answer, but we know that our prompt was
    too vague.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, our second prompt was more specific in a few ways. First, it clearly
    asks for the maximum. Second, it asks for the number of spaces, or empty seats,
    in a row. That takes away the need for Copilot to infer that an empty seat means
    a spot for a new student. We also used “total” and “given row” to try to get Copilot
    out of its current approach to counting consecutive values, but that didn’t quite
    do the trick. Consequently, we ended up having to sift through Copilot answers
    (using Ctrl-Enter) that were sometimes looking for consecutive empty seats and
    sometimes finding the count of empty seats.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.3 Testing the new solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Returning to our example, because the new code passes the current test, let’s
    give it more tests to ensure that it’s behaving correctly. In the next test, we’ll
    check that the code properly returns 0 when there are no empty seats in any rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The next test will ensure that the code properly counts all three empty seats
    in a single row (here, the second row) so there isn’t an edge case problem (e.g.,
    it fails to count the first or last element). Admittedly, looking at the code,
    we can see the `count` function is being used, and because that function is built
    in to Python, we should be fairly confident this test will pass. However, it’s
    still safer to test it to make sure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The last test checks to see that Copilot properly handles the case that two
    rows have the same number of empty seats:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: After adding these test cases, we again ran the full program, shown in the following
    listing, and all test cases passed.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.8 Full code and doctests for largest number of students
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we saw how to write a function to solve a problem from start
    to finish. Copilot gave us the wrong answer, partially because of a difficult-to-interpret
    prompt. We figured out that it gave us the wrong answer because the code failed
    on our first test. We then improved the prompt and used the code reading skills
    we learned in the previous two chapters to pick out a solution that looked correct
    for our needs. The new code passed our initial basic test, so we added more test
    cases to see whether the code worked in more situations. After seeing it pass
    those additional tests, we have more evidence that the code is correct. At this
    point, we’ve tested the common cases and edge cases, so we’re highly confident
    that our current code is correct. Regarding testing, this example showed us how
    tests can help us find mistakes *and* give us more confidence that the code will
    function properly.
  prefs: []
  type: TYPE_NORMAL
- en: '6.6 Another full testing example: Testing with files'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In most cases, you’ll be able to test your code by adding examples to the docstring
    like we did in the previous example. However, there are times when testing can
    be a bit more challenging. This is true when you need to test your code against
    some kind of external input. An example is when we need to test code that interacts
    with external websites, but this is more common in advanced code than the kind
    of code you’ll be creating within the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: An example that *is* within the scope of this book is working with files. How
    do you write test cases when your input is a file? Python does support doing this
    in a way internal to the docstring here, but for continuity with what we’ve already
    done, we’re not going to do it that way. Instead, we’ll use external files to
    test our code. Let’s see how to do that by revising our NFL quarterback (QB) example
    from chapter 2.
  prefs: []
  type: TYPE_NORMAL
- en: We could walk through an example with the entire file, but because our queries
    about quarterbacks were only for the first nine columns of the file, we’re going
    to strip off the remaining columns of the file to make things more readable. After
    stripping off the remaining columns, table 6.2 shows the first four rows of the
    file.
  prefs: []
  type: TYPE_NORMAL
- en: Table 6.2 The first four lines of an abridged version of the NFL dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| game_id | player_id | position | player | team | pass_cmp | pass_att | pass_yds
    | pass_td |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 201909050chi  | RodgAa00  | QB  | Aaron Rodgers  | GNB  | 18  | 30  | 203  |
    1  |'
  prefs: []
  type: TYPE_TB
- en: '| 201909050chi  | JoneAa00  | RB  | Aaron Jones  | GNB  | 0  | 0  | 0  | 0  |'
  prefs: []
  type: TYPE_TB
- en: '| 201909050chi  | ValdMa00  | WR  | Marquez Valdes-Scantling  | GNB  | 0  |
    0  | 0  | 0  |'
  prefs: []
  type: TYPE_TB
- en: We’ll assume that each row in the dataset has just these nine columns for the
    remainder of the example, but we hope it’s not a big stretch to imagine how to
    do this for the full dataset (you’d just need to add all the additional columns
    in each case).
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we want to make a function that takes in the filename of the dataset
    and the name of a player as input and then outputs the total number of passing
    yards that player achieved in the dataset. We’ll assume that the user will be
    providing the data as formatted in the NFL offensive stats file in chapter 2 and
    in table 6.2\. Before we write the prompt or function, how should we test this?
    Well, we have some options:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Find tests in the larger dataset—*A solution is to give the full dataset to
    the function and multiple player names as inputs. The challenge is figuring out
    whether we’re correct or not. We could open the file in software such as Google
    Sheets or Microsoft Excel and use spreadsheet features to figure out the answer
    for each player. For example, we could open the file as a sheet in Excel, sort
    by player, find a player, and use the sum function in Excel to add up all the
    passing yards for that player. This isn’t a bad solution at all, but it’s also
    a fair bit of work, and if you put enough time into finding the answer for testing,
    you might have already fulfilled your needs and no longer require the Python code!
    In other words, figuring out the answer for the test cases might just give you
    the answer you wanted in the first place, making the code less valuable. Another
    problem is in finding all the edge cases you might want to test: Will your dataset
    have all the edge cases you’d want to test to write a program that will work on
    other datasets later? Yet another drawback of this approach is determining what
    you do when the function is doing something considerably more complicated than
    just summing a value in a bunch of rows. There, figuring out the answers for some
    real test values might be a great deal of work.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Create artificial dataset(s) for testing—*Another solution is to create artificial
    datasets where you know the answer to a number of possible queries. Because the
    dataset is artificial, you can add edge cases to see how the code performs in
    those cases without having to find such rare examples in the real dataset. (Sometimes
    the real dataset won’t include those edge cases, but you still want to test them,
    so the code behaves properly if you get an updated or new dataset.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given the advantages to creating test cases in an artificial dataset, we’re
    going to proceed with that approach here.
  prefs: []
  type: TYPE_NORMAL
- en: 6.6.1 What tests should we run?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s think through the common cases and edge cases that we would want to test.
    For common cases, we’d want to have a few tests:'
  prefs: []
  type: TYPE_NORMAL
- en: '*A player appears multiple times in different rows of the dataset (nonconsecutively),
    including the last row.* This test makes sure the code iterates over all the players
    before returning a result (i.e., doesn’t make the false assumption that the data
    is sorted by player name).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A player appears in consecutive rows of the dataset.* This test makes sure
    there isn’t some kind of error where consecutive values are somehow skipped.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A player appears just once in the dataset.* This test makes sure that the
    sum behaves properly even when it’s just summing one value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A non-quarterback could appear in the dataset.* For this, we ensure the code
    is including all players, not just quarterbacks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A player has 0 total passing yards in a game.* This checks to make sure that
    the code behaves properly when players don’t have any passing yards. This is a
    common case to test because players can get hurt and miss a game due to the injury.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For edge cases, we’d want to test a couple more things:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The player isn’t in the dataset.* This is actually pretty interesting: What
    do we want the code to do in this case? A reasonable answer is to return that
    they passed for 0 yards. If we asked the dataset how many yards Lebron James (a
    basketball player, not a football player) passed for in the NFL from 2019 to 2022,
    0 is the right answer. However, this may not be the most elegant solution for
    production code. For example, if we ask for the passing yards for Aron Rodgers
    (misspelling Aaron Rodgers), we’d rather have the code tell us he’s not in the
    dataset than that he passed for 0 yards, which could really confuse us when he
    won the league MVP twice during this time frame. To signal that the name was missing,
    we might return a large negative value (e.g., –9999), or we might use something
    called exceptions, but they are beyond the scope of this book.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A player has a negative total number of yards across all games or a player
    has a single game with negative yards to ensure the code is properly handling
    negative values.* If you don’t follow American football, this can happen if a
    player catches a ball and is tackled behind the starting point (line of scrimmage).
    It’s unlikely a quarterback would have negative passing yards for an entire game,
    but it could happen if they throw one pass for a loss (negative yards) and get
    hurt at the same time, causing them to not play for the rest of the game.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have an idea of what we want to test, let’s build an artificial
    file that captures these test cases. We could have split these tests across multiple
    files, which would be a reasonable choice to make as well, but an advantage of
    putting them all in one file is that we can keep all of our test cases together.
    Table 6.3 is what we built and saved as test_file.csv.
  prefs: []
  type: TYPE_NORMAL
- en: Table 6.3 Our file to test the NFL passing yards function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| game_id | player_id | position | player | team | pass_cmp | pass_att | pass_yds
    | pass_td |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 201909050chi  | RodgAa00  | QB  | Aaron Rodgers  | GNB  | 20  | 30  | 200  |
    1  |'
  prefs: []
  type: TYPE_TB
- en: '| 201909080crd  | JohnKe06  | RB  | Kerryon Johnson  | DET  | 1  | 1  | 5  |
    0  |'
  prefs: []
  type: TYPE_TB
- en: '| 201909080crd  | PortLe00  | QB  | Leo Porter  | UCSD  | 0  | 1  | 0  | 0  |'
  prefs: []
  type: TYPE_TB
- en: '| 201909080car  | GoffJa00  | QB  | Jared Goff  | LAR  | 20  | 25  | 200  |
    1  |'
  prefs: []
  type: TYPE_TB
- en: '| 201909050chi  | RodgAa00  | QB  | Aaron Rodgers  | GNB  | 10  | 15  | 150  |
    1  |'
  prefs: []
  type: TYPE_TB
- en: '| 201909050chi  | RodgAa00  | QB  | Aaron Rodgers  | GNB  | 25  | 35  | 300  |
    1  |'
  prefs: []
  type: TYPE_TB
- en: '| 201909080car  | GoffJa00  | QB  | Jared Goff  | LAR  | 1  | 1  | –10  | 0  |'
  prefs: []
  type: TYPE_TB
- en: '| 201909080crd  | ZingDa00  | QB  | Dan Zingaro  | UT  | 1  | 1  | –10  | 0  |'
  prefs: []
  type: TYPE_TB
- en: '| 201909050chi  | RodgAa00  | QB  | Aaron Rodgers  | GNB  | 15  | 25  | 150  |
    0  |'
  prefs: []
  type: TYPE_TB
- en: Notice that the data here is entirely artificial. (These aren’t the real statistics
    for any player, as you can tell by the fact that Dan and Leo are now magically
    NFL quarterbacks.) We did keep the names of some real players as well as real
    `game_id`s and `player_id`s from the original dataset. It’s generally a good idea
    to make your artificial data be as close to real data as possible so that the
    tests are genuine and more apt to be representative of what will happen with real
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at how we incorporated all the test cases in this test file (table
    6.3). Aaron Rodgers occurs multiple times in the file, both consecutively and
    nonconsecutively, and as the last entry. Jared Goff appears multiple times, and
    we gave him an artificial –10 yards in a game (as an elite NFL QB, I hope he’s
    okay with us giving him an artificially bad single game). We kept Kerryon Johnson
    as a running back (RB) from the real dataset and gave him 5 passing yards to make
    sure the solution doesn’t filter for only QBs. Kerryon Johnson also only has one
    entry in the data. We added Leo Porter to the dataset and gave him 0 passing yards
    (he’s pretty sure he’d do anything to not get tackled by an NFL player). We also
    added Dan Zingaro and gave him a completed pass, but for –10 yards, covering the
    case that a single player’s total is negative. Table 6.4 shows what it *should*
    return per player when we run the query for each player.
  prefs: []
  type: TYPE_NORMAL
- en: Table 6.4 Passing yards per player in the test case
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Player | Passing yards in the test case |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Aaron Rodgers  | 800  |'
  prefs: []
  type: TYPE_TB
- en: '| Kerryon Johnson  | 5  |'
  prefs: []
  type: TYPE_TB
- en: '| Leo Porter  | 0  |'
  prefs: []
  type: TYPE_TB
- en: '| Jared Goff  | 190  |'
  prefs: []
  type: TYPE_TB
- en: '| Dan Zingaro  | –10  |'
  prefs: []
  type: TYPE_TB
- en: 6.6.2 Creating the function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s start with the function name and describe the problem in the docstring.
    This problem may be complicated enough that we’ll need to offer prompts within
    the function to help it know to open the file, process the data, and close the
    file, but let’s try to just describe the function first and see how Copilot does.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.9 Function to find total passing yards per player
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '#1 We abbreviated the function name.'
  prefs: []
  type: TYPE_NORMAL
- en: Before looking at the code produced, we want to point out that we didn’t include
    the test cases for this example because they likely wouldn’t help Copilot. We
    recommend having Copilot generate code prior to you adding test cases if your
    test cases will simply open and use artificial files. Each test case will just
    be a filename, a player, and the expected output. Copilot likely can’t read the
    file that corresponds to that filename and figure out that we’re summing the eighth
    column to get the desired result. This means that giving Copilot the test cases
    isn’t going to add to the quality of the prompt. In fact, the artificial nature
    of our data might even confuse it (e.g., in what scenario does a real NFL player
    have a negative total for passing yards?).
  prefs: []
  type: TYPE_NORMAL
- en: Now looking at the code that Copilot gave us, we can see that it’s quite reasonable
    and almost identical to the code we saw in chapter 2\. Given that it seems reasonable
    when we read it, let’s see how to test it.
  prefs: []
  type: TYPE_NORMAL
- en: 6.6.3 Testing the function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To test the function, we’ll augment the docstring to include our test cases
    for the full piece of code, as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.10 Testing the function to find player passing yards
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The additional test case for a player not in the dataset'
  prefs: []
  type: TYPE_NORMAL
- en: We ran this code, and all the test cases passed. (We know that all test cases
    passed because there was no output from doctest.) We now have additional evidence
    that the code is functioning properly!
  prefs: []
  type: TYPE_NORMAL
- en: 6.6.4 Common challenges with doctest
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s rewrite the previous prompt and add a really subtle error to the first
    test, as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.11 Mistake in doctest
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '#1 There is an extra space after the 800 that isn’t visible.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When we ran this code, we received this error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: On first glance, this seems really odd. The test case expects 800 and it got
    800, but it’s telling us it failed. Well, it turns out that we made a mistake
    in writing the test case and wrote “800 ” (with a space at the end) rather than
    “800”. This mistake causes Python to think the space is important and causes the
    test to fail. The bad news is that this is a really common problem working with
    doctest! We’ve made this mistake more often than we’d like to admit. The good
    news is it’s really easy to fix by just finding and deleting the space. If a test
    is failing but the output from doctest suggests that it should be passing, always
    check ends of lines for spaces or extra or missing spaces anywhere in your output
    compared to exactly what doctest is expecting.
  prefs: []
  type: TYPE_NORMAL
- en: Given that all our test cases passed, we can feel confident returning to the
    larger dataset and using the function we just created. The key thing from this
    example is that we can, and should, create artificial files to test functions
    that work with files. Again, testing is all about gaining confidence that the
    code is working properly, and you want to be sure you test any code you write
    or given to you by Copilot.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter as a whole, we learned about the importance of testing code,
    how to test code, and how to do it in two detailed examples. In our examples,
    we wrote and tested functions. But how do we decide which functions should be
    written to solve even larger problems? Well, we figure that out through a process
    known as problem decomposition that we’ll cover in detail in our next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 6.7 Exercises
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Given the following scenarios, identify whether closed-box testing or open-box
    testing would be more appropriate and explain why:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A tester is given a function specification and needs to ensure that the function
    behaves correctly without looking at the implementation.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: A tester needs to debug a function by writing tests that specifically target
    edge cases revealed by understanding the code implementation.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here is a function with some test cases. Identify which of the following test
    cases are common use cases and which are edge cases:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '3\. We’re building a program to analyze website traffic. Website traffic is
    represented as a list of dictionaries. Each dictionary has two keys: `"weekday"`
    (a string representing the day of the week) and `"visitors"` (an integer representing
    the number of visitors on that day). The same day of the week can appear in multiple
    dictionaries. Our goal is to find the day of the week with the highest number
    of visitors.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here’s the initial prompt we gave to an AI code-generation tool:'
  prefs: []
  type: TYPE_NORMAL
- en: 'def busiest_day(traffic_data): # traffic_data is a list of dictionaries. #
    Find the busiest day.'
  prefs: []
  type: TYPE_NORMAL
- en: The tool generated the following code, but it doesn’t seem quite right.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Can you explain why and how you would improve the prompt to get the desired
    functionality?
  prefs: []
  type: TYPE_NORMAL
- en: '4\. Imagine you have a function (`find_highest_grossing_book(filename)`) that
    analyzes book sales data and returns information about the book with the highest
    total revenue. Sales data is stored in a CSV file where each line represents a
    sale. The columns in the CSV file are as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`title`—The title of the book sold (string)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`author`—The author of the book (string)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`price`—The price of the book (float)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`quantity`—The number of copies sold for that particular sale (integer)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a Python script containing the `find_highest_grossing_book` function
    (implementation not provided here). Include a docstring explaining the function’s
    purpose, and add test cases using the doctest module.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Review the provided test cases for the `is_palindrome` function. Identify
    which test cases are incorrect, and explain why. Provide the correct version of
    the test cases.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 6\. Analyze the test coverage of the `find_max` function. Are there any scenarios
    not covered by the existing test cases? Suggest additional test cases if necessary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Testing is a critical skill when writing software using Copilot.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Closed-box and open-box testing are different approaches to ensuring the code
    is correct. In closed-box testing, we come up with test cases based on what we
    know about the problem; in open-box testing, we additionally examine the code
    itself.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doctest is a module that comes with Python that helps us test our code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To use doctest, we add test cases to the docstring description of a function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating artificial files is an effective way to test code that uses files.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
