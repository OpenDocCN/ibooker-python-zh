- en: Chapter 14\. MLOps and Machine learning Engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the hottest job titles in 2020 is machine learning engineer. Other hot
    job titles include data engineer, data scientist, and machine learning scientist.
    While you can be a DevOps specialist, DevOps is a behavior, and the principles
    of DevOps can be applied to any software project, including machine learning.
    Let’s look at the some core DevOps best practices: Continuous Integration, Continuous
    Delivery, Microservices, Infrastructure as Code, Monitoring and Logging, and Communication
    and Collaboration. Which of these doesn’t apply to machine learning?'
  prefs: []
  type: TYPE_NORMAL
- en: The more complex a software engineering project is, and machine learning is
    complex, the more you need DevOps principles. Is there a better example of a Microservice
    than an API that does machine learning prediction? In this chapter, let’s dive
    into the nitty-gritty of doing machine learning in a professional and repeatable
    way using a DevOps mindset.
  prefs: []
  type: TYPE_NORMAL
- en: What Is Machine Learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Machine learning is a method of using algorithms to automatically learn from
    data. There are four main types: supervised, semi-supervised, unsupervised, and
    reinforcement.'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised Machine Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In supervised machine learning, the correct answers are already known and labeled.
    For example, if you wanted to predict height from weight, you could collect examples
    of people’s heights and weights. The height would be the target and the weight
    would be the feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s walk through what an example of supervised machine learning looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original dataset](https://oreil.ly/jzWmI)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 25,000 synthetic records of heights and weights of 18-year-old children
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ingest
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`**In[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '`**In[7]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '`**Out[7]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Index | Height-Inches | Weight-Pounds |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `0` | `1` | `65.78331` | `112.9925` |'
  prefs: []
  type: TYPE_TB
- en: '| `1` | `2` | `71.51521` | `136.4873` |'
  prefs: []
  type: TYPE_TB
- en: '| `2` | `3` | `69.39874` | `153.0269` |'
  prefs: []
  type: TYPE_TB
- en: '| `3` | `4` | `68.21660` | `142.3354` |'
  prefs: []
  type: TYPE_TB
- en: '| `4` | `5` | `67.78781` | `144.2971` |'
  prefs: []
  type: TYPE_TB
- en: EDA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s look at the data and see what can be explored.
  prefs: []
  type: TYPE_NORMAL
- en: Scatterplot
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this example, seaborn, a popular plotting library in Python, is used to visualize
    the dataset. If you need to install it, you can always install with `!pip install
    seaborn` in your notebook. You can also install any other library in the section
    with the `!pip install <name of package>`. If you are using a Colab notebook,
    these libraries are installed for you. See the graph for the height/weight lm
    plot ([Figure 14-1](#Figure-14-1)).
  prefs: []
  type: TYPE_NORMAL
- en: '`**In[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '`**In[9]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![pydo 1401](assets/pydo_1401.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-1\. Height/weight lm plot
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Descriptive Statistics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, some descriptive statistics can be generated.
  prefs: []
  type: TYPE_NORMAL
- en: '`**In[10]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '`**Out[10]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Index | Height-Inches | Weight-Pounds |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `count` | `25000.000000` | `25000.000000` | `25000.000000` |'
  prefs: []
  type: TYPE_TB
- en: '| `mean` | `12500.500000` | `67.993114` | `127.079421` |'
  prefs: []
  type: TYPE_TB
- en: '| `std` | `7217.022701` | `1.901679` | `11.660898` |'
  prefs: []
  type: TYPE_TB
- en: '| `min` | `1.000000` | `60.278360` | `78.014760` |'
  prefs: []
  type: TYPE_TB
- en: '| `25%` | `6250.750000` | `66.704397` | `119.308675` |'
  prefs: []
  type: TYPE_TB
- en: '| `50%` | `12500.500000` | `67.995700` | `127.157750` |'
  prefs: []
  type: TYPE_TB
- en: '| `75%` | `18750.250000` | `69.272958` | `134.892850` |'
  prefs: []
  type: TYPE_TB
- en: '| `max` | `25000.000000` | `75.152800` | `170.924000` |'
  prefs: []
  type: TYPE_TB
- en: Kernel Density Distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A distribution for the density plot ([Figure 14-2](#Figure-14-2)) shows how
    the two variables relate to each other.
  prefs: []
  type: TYPE_NORMAL
- en: '`**In[11]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '`**Out[11]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '![pydo 1402](assets/pydo_1402.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-2\. Density plot
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Modeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now let’s review modeling. Machine learning modeling is when an algorithm learns
    from the data. The general idea is to use previous data to predict future data.
  prefs: []
  type: TYPE_NORMAL
- en: Sklearn Regression Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First the data is extracted into features and targets, and then it is split
    into a train and a test set. This allows the test set to be held aside to test
    the accuracy of the trained model.
  prefs: []
  type: TYPE_NORMAL
- en: '`**In[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Extract and inspect feature and target
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is a good idea to explicitly pull out the target and feature variables and
    reshape them in one cell. Afterwards you will want to check the shape to ensure
    it is the property dimension for doing machine learning with sklearn.
  prefs: []
  type: TYPE_NORMAL
- en: '`**In[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '`**In[14]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '`**Out[14]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Split the data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The data is split into an 80%/20% split.
  prefs: []
  type: TYPE_NORMAL
- en: '`**In[15]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '`**Out[15]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Fit the model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now the model is fit using a LinearRegression algorithm imported via sklearn.
  prefs: []
  type: TYPE_NORMAL
- en: '`**In[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Print accuracy of linear regression model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now the trained model can show what the accuracy is in predicting new data.
    This is performed by asking for the RMSE or root mean squared error of the predicted
    and the test data.
  prefs: []
  type: TYPE_NORMAL
- en: '`**In[18]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '`**Out[18]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Plot predicted height versus actual height
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now let’s plot the predicted height versus actual height ([Figure 14-3](#Figure-14-3))
    to see how well this model performs at predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '`**In[19]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![pydo 1403](assets/pydo_1403.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-3\. Predicted height versus actual height
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This is a very simple yet powerful example of a realistic workflow for creating
    a machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Python Machine learning Ecosystem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s take a quick look at the Python machine learning ecosystem ([Figure 14-4](#Figure-14-4)).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are really four main areas: deep learning, sklearn, AutoML, and Spark.
    In the area of deep learning, the most popular frameworks in order are: TensorFlow/Keras,
    PyTorch, and MXNet. Google is sponsoring TensorFlow, Facebook is sponsoring PyTorch,
    and MXNet comes from Amazon. You will see MXNet mentioned quite a bit by Amazon
    SageMaker. It is important to note that these deep learning frameworks target
    GPUs, giving them performance boosts over CPU targets of up to 50X.'
  prefs: []
  type: TYPE_NORMAL
- en: '![pydo 1404](assets/pydo_1404.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-4\. Python machine learning ecosystem
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The sklearn ecosystem often has Pandas and Numpy in the same projects. Sklearn
    also intentionally does not target GPUs. However, there is a project called Numba
    that does specifically target the GPU (both NVIDIA and AMD).
  prefs: []
  type: TYPE_NORMAL
- en: In AutoML, two of the leaders are Uber with Ludwig and H20, with H20 AutoML.
    Both can save significant time developing machine learning models and can also
    potentially optimize existing machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, there is the Spark ecosystem, which builds on the legacy of Hadoop.
    Spark can target GPUs and CPUs and does so via many different platforms: Amazon
    EMR, Databricks, GCP Dataproc, and more.'
  prefs: []
  type: TYPE_NORMAL
- en: Deep Learning with PyTorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that the ecosystem for machine learning with Python has been defined, let’s
    take a look at porting the simple linear regression example to PyTorch and run
    it on a CUDA GPU. One easy way to get access to an NVIDIA GPU is to use Colab
    notebooks. Colab notebooks are Google-hosted, Jupyter-compatible notebooks that
    give the user free access to both GPUs and tensor processing units (TPUs). You
    can run this code [in a GPU](https://oreil.ly/kQhKO).
  prefs: []
  type: TYPE_NORMAL
- en: Regression with PyTorch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First, convert data to `float32`.
  prefs: []
  type: TYPE_NORMAL
- en: '`**In[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Note that if you are not using Colab notebooks, you may have to install PyTorch.
    Also, if you use Colab notebooks, you can have an NVIDIA GPU and run this code.
    If you are not using Colab, you will need to run on a platform that has a GPU.
  prefs: []
  type: TYPE_NORMAL
- en: '`**In[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Now create a model with CUDA enabled (assuming you are running in Colab or on
    a machine with an NVIDIA GPU).
  prefs: []
  type: TYPE_NORMAL
- en: '`**In[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '`**Out[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Create the Stochastic Gradient Descent and Loss Function.
  prefs: []
  type: TYPE_NORMAL
- en: '`**In[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Now train the model.
  prefs: []
  type: TYPE_NORMAL
- en: '`**In[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The output over 1,000 runs is supressed to save space.
  prefs: []
  type: TYPE_NORMAL
- en: '`**Out[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Plot predicted height versus actual height
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now let’s plot the predicted height versus actual height ([Figure 14-5](#Figure-14-5))
    as in the simple model.
  prefs: []
  type: TYPE_NORMAL
- en: '`**In[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![pydo 1405](assets/pydo_1405.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-5\. Predicted height versus actual height
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Print RMSE
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Finally, let’s print out the RMSE and compare.
  prefs: []
  type: TYPE_NORMAL
- en: '`**In[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '`**Out[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: It does take a little more code to do deep learning, but the concepts are the
    same from the sklearn model. The big takeaway here is that GPUs are becoming an
    integral part of production pipelines. Even if you aren’t doing deep learning
    yourself, it is helpful to have a basic awareness of the process of building GPU-based
    machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud Machine learning Platforms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One aspect of machine learning that is becoming ubiquitious is cloud-based machine
    learning platforms. Google offers the GCP AI Platform ([Figure 14-6](#Figure-14-6)).
  prefs: []
  type: TYPE_NORMAL
- en: '![pydo 1406](assets/pydo_1406.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-6\. GCP AI platform
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The GCP platform has many high-level automation components, from data preparation
    to data labeling. The AWS platform offers Amazon SageMaker ([Figure 14-7](#Figure-14-7)).
  prefs: []
  type: TYPE_NORMAL
- en: '![pydo 1407](assets/pydo_1407.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-7\. Amazon SageMaker
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: SageMaker also has many high-level components, including training on spot instances
    and elastic prediction endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning Maturity Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the big challenges right now is a realization that transformational change
    is needed in companies that want to perform machine learning. The machine learning
    maturity model diagram ([Figure 14-8](#Figure-14-8)) represents some challenges
    and opportunities.
  prefs: []
  type: TYPE_NORMAL
- en: '![pydo 1408](assets/pydo_1408.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-8\. Machine learning maturity model
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Machine Learning Key Terminology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s define some key machine learning terminology that will be helpful througout
    the rest of the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning
  prefs: []
  type: TYPE_NORMAL
- en: A way of building mathmatical models based on sample or training data.
  prefs: []
  type: TYPE_NORMAL
- en: Model
  prefs: []
  type: TYPE_NORMAL
- en: This is what the product is in a machine learning application. A simple example
    is a linear equation, i.e., a straight line that predicts the relationship between
    an X and a Y.
  prefs: []
  type: TYPE_NORMAL
- en: Feature
  prefs: []
  type: TYPE_NORMAL
- en: A feature is a column in a spreadsheet that is used as a signal to create a
    machine learning model. A good example is points scored per game by an NBA team.
  prefs: []
  type: TYPE_NORMAL
- en: Target
  prefs: []
  type: TYPE_NORMAL
- en: The target is the column in a spreadsheet you are trying to predict. A good
    example is how many games an NBA team wins in a season.
  prefs: []
  type: TYPE_NORMAL
- en: Supersized machine learning
  prefs: []
  type: TYPE_NORMAL
- en: This is a type of machine learning that predicts future values based on known
    correct historical values. A good example would be predicting the amount of NBA
    wins in a season by using the feature points per game.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised machine learning
  prefs: []
  type: TYPE_NORMAL
- en: This is a type of machine learning that works with unlabeled data. Instead of
    predicting a future value, it finds hidden patterns by using tools such as clustering,
    which in turn could be used as labels. A good example would be to create clusters
    of NBA players that have similar points, rebounds, blocks, and assists. One cluster
    could be called “Tall, Top Players,” and another cluster could be called “Point
    guards who score a lot of points.”
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning
  prefs: []
  type: TYPE_NORMAL
- en: This is a type of machine learning that uses artificial neural networks that
    can be used for supervised or unsupervised machine learning. The most popular
    framework for deep learning is TensorFlow from Google.
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-learn
  prefs: []
  type: TYPE_NORMAL
- en: This is one of the most popular machine learning frameworks in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Pandas
  prefs: []
  type: TYPE_NORMAL
- en: This is one of the most popular libraries for doing data manipulation and analysis.
    It works well with scikit-learn and Numpy.
  prefs: []
  type: TYPE_NORMAL
- en: Numpy
  prefs: []
  type: TYPE_NORMAL
- en: This is the predominant Python library for doing low-level scientific computing.
    It has support for a large, multidimensional array and has a large collection
    of high-level mathmatical functions. It is used extensively with scikit-learn,
    Pandas, and TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: 'Level 1: Framing, Scope Identification, and Problem Definition'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s look at the first layer. When implementing machine learning at a company,
    it is important to consider what problems need solving and how they can be framed.
    One of the key reasons for failure of machine learning projects is that organizations
    haven’t first asked questions about what problems they need solved.
  prefs: []
  type: TYPE_NORMAL
- en: A good analogy to look at is building a mobile application for a restaurant
    chain in San Francisco. One naive approach would be to immediately start building
    a native iOS and a native Android app (using two development teams). A typical
    mobile team could be three full-time developers for each application. So this
    means hiring six developers at around two hundred thousand dollars each. The run
    rate for the project is about $1.2 million a year now. Will the mobile app deliver
    greater than $1.2 million in revenue each year? If not, is there an easier alternative?
    Perhaps a mobile-optimized web app that uses the existing web developers in the
    company is a better choice.
  prefs: []
  type: TYPE_NORMAL
- en: What about partnering with a company that specializes in food delivery and outsourcing
    this task completely? What are the pros and cons to this approach? The same type
    of thought process can and should be applied to machine learning and data science
    initiatives. For example, does your company need to hire six PhD-level machine
    learning researchers at, say, five hundred thousand dollars a year? What is the
    alternative? A little bit of scoping and problem definition goes a long way with
    machine learning and can ensure a higher chance of success.
  prefs: []
  type: TYPE_NORMAL
- en: 'Level 2: Continuous Delivery of Data'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the fundamentals of civilization is running water. Roman aqueducts carried
    water for miles to provide crowded cities with water as early as 312 B.C. Running
    water enabled the infrastructure necessary for a large city to succeed. It is
    estimated by UNICEF that worldwide in 2018, women and girls spend an estimated
    two hundred million hours daily, collecting water. The opportunity cost is substantial;
    less time to spend learning, taking care of children, working, or relaxing.
  prefs: []
  type: TYPE_NORMAL
- en: A popular expression is that “software is eating the world.” A corollary to
    that is that all software companies, which is every company in the future, need
    to have a machine learning and AI strategy. Part of that strategy is to think
    more seriously about continuous delivery of data. Just like running water, “running
    data” saves you hours per day. One potential solution is the concept of a *data
    lake*, as shown in [Figure 14-9](#Figure-14-9).
  prefs: []
  type: TYPE_NORMAL
- en: '![pydo 1409](assets/pydo_1409.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-9\. AWS data lake
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'At first glance, a data lake might seem like a solution in search of a problem,
    or too simple to do anything useful. Let’s look at some of the problems it solves,
    though:'
  prefs: []
  type: TYPE_NORMAL
- en: You can process the data without moving it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is cheap to store data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is straightforward to create life cycle policies to archive the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is straightforward to create life cycle policies that secure and audit the
    data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The production systems are decoupled from data processing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can have almost infinite and elastic storage and disk I/O.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The alternative to this architecture is often an ad hoc mess equivalent to
    walking four hours to a well and back simply to get some water. Security is also
    a major factor in a data lake architecture, just as security is in the water supply.
    By centralizing the architecture of data storage and delivery, preventing and
    monitoring data breaches becomes more straightforward. Here are a few ideas that
    may be helpful in preventing future data breaches:'
  prefs: []
  type: TYPE_NORMAL
- en: Is your data encrypted at rest? If so, who has the keys? Are decryption events
    logged and audited?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is your data moving out of your network logged and audited? For example, when
    is it ever a good idea for your entire customer database to move outside of your
    network? Why isn’t this monitored and audited?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do you have periodic data security audits? Why not?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are you storing personally identifiable information (PII)? Why?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You have monitoring for critical production events. Are you monitoring data
    security events? Why not?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why would we ever let data flow outside of the internal network? What if we
    designed critical data to be a literal square peg that couldn’t be transmitted
    outside of the host network without something like a nuclear launch code? Making
    it impossible to move data out of the environment does seem like a viable way
    to prevent these breaches. What if the external network itself could only transport
    “round peg” packets? Also, this could be a great “lock-in” feature for clouds
    providing this type of secure data lake.
  prefs: []
  type: TYPE_NORMAL
- en: 'Level 3: Continuous Delivery of Clean Data'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hopefully you are sold on the idea behind continuous delivery of data and how
    important it is to the success of a company’s plans for doing machine learning.
    One huge improvement to continuous delivery of data is continuous delivery of
    clean data. Why go through all of the trouble of delivering data that is a mess?
    The recent problem of contaminated water in Flint, Michigan, comes to mind. Around
    2014, Flint changed its water source from Lake Huron and the Detroit River to
    the Flint River. Officials failed to apply corrosion inhibitors, which allowed
    lead from aging pipes leak into the water supply. It is also possible that the
    water change caused an outbreak of Legionnaires’ disease that killed 12 people
    and sickened another 87.
  prefs: []
  type: TYPE_NORMAL
- en: One of the earliest success stories of data science involves dirty water from
    1849–1854\. John Snow was able to use data visualization to identify clusters
    of cholera cases ([Figure 14-10](#Figure-14-10)). This led to the discovery of
    the root cause of the outbreak. Sewage was being pumped directly into the drinking
    water supply!
  prefs: []
  type: TYPE_NORMAL
- en: '![pydo 1410](assets/pydo_1410.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-10\. Clusters of cholera cases
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Consider the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: Why isn’t data automatically processed to “clean” it?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can you visualize parts of your data pipeline that have “sewage” in them?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How much time is your company spending on data-cleaning tasks that are 100%
    automatible?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Level 4: Continuous Delivery of Exploratory Data Analysis'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If your only view of data science is Kaggle projects, it may seem like the
    whole point of data science is to generate the most accurate prediction possible.
    There is more to data science and machine learning than just making predictions.
    Data science is a multidisciplinary field and there are a few ways to look at
    it. e One perspective is to focus on causality. What are the underlying features
    that drive the model? Can you explain how the model is coming to the prediction
    it has generated? Several Python libraries help in this regard: ELI5, SHAP, and
    LIME. They all work to help explain what machine learning models are really doing.'
  prefs: []
  type: TYPE_NORMAL
- en: A prediction world view cares less about how you got to the answer, and more
    about whether the prediction is accurate. In a cloud-native, Big Data world, this
    approach has merits. Certain machine learning problems do very well with large
    quantities of data, such as image recognition using deep learning. The more data
    and the more computational power you have, the better your prediction accuracy
    will be.
  prefs: []
  type: TYPE_NORMAL
- en: Is what you created in production? Why not? If you are building machine learning
    models and they don’t get used, then why are you building machine learning models?
  prefs: []
  type: TYPE_NORMAL
- en: What don’t you know? What can you learn by looking at data? Often data science
    is more interested in the process than the outcome. If you are only looking for
    a prediction, then you may miss out on a completely different angle to the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Level 5: Continuous Delivery of Traditional ML and AutoML'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fighting automation is as old as human history. The Luddite movement was a secret
    organization of English textile workers who destroyed textile machinary as a form
    of protest from 1811 to 1816\. Ultimately, protesters were shot, the rebellion
    was put down with legal and military might, and the path of progress kept going.
  prefs: []
  type: TYPE_NORMAL
- en: If you look at the history of humanity, tools that automate tasks that humans
    once did are constantly being developed. In technological unemployment, lower-skilled
    workers are displaced and higher-skilled workers get an increased salary. A case
    in point is the systems administrator versus the DevOps professional. Yes, some
    systems administrators lost their jobs, for example, workers focused on tasks
    such as changing hard drives out of data centers, but new, higher-paying jobs
    such as cloud architects appeared.
  prefs: []
  type: TYPE_NORMAL
- en: 'It isn’t unusual to see job postings for machine learning and data science
    that command an annual salary of between three hundred thousand and one million
    dollars. Additionally these jobs often include many components that are essentially
    business rules: tweaking hyperparameters, removing null values, and distributing
    jobs to a cluster. The automator’s law (something I came up with) says, “If you
    talk about something being automated, it will eventually be automated.” There
    is a lot of talk around AutoML, so it is inevitable that large parts of machine
    learning will be automated.'
  prefs: []
  type: TYPE_NORMAL
- en: This will mean that, just like other automation examples, the nature of jobs
    will change. Some jobs will become even more skilled (imagine a person who can
    train thousands of machine learning models a day), and some jobs will become automated
    because a machine can do them much better (jobs that involve tweaking values in
    a JSON data structure, i.e., tuning hyperparameters).
  prefs: []
  type: TYPE_NORMAL
- en: 'Level 6: ML Operational Feedback Loop'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Why develop a mobile application? Presumably to have users on a mobile device
    use your application. What about machine learning? The point of machine learning,
    especially in comparison to data science or statistics, is to create a model and
    predict something. If the model isn’t in production, then what is it doing?
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, pushing the model to production is an opportunity to learn more.
    Does the model predict accurately when put into an environment where it gets new
    data? Does the model have the impact expected on the users: i.e., increasing purchases
    or staying on the site longer? These valuable insights can only be obtained when
    the model is actually deployed into a production environment.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another important concern is scalability and repeatability. A truly technologically
    mature organization can deploy software, including machine learning models, on
    demand. The best practices of DevOps for ML models are required here as well:
    continuous deployment, microservices, monitoring, and instrumentation.'
  prefs: []
  type: TYPE_NORMAL
- en: One easy way to inject more of this technological maturity into your organization
    is to apply the same logic as you do to choosing cloud computing over a physical
    data center. Rent the expertise of others and take advantage of economies of scale.
  prefs: []
  type: TYPE_NORMAL
- en: Sklearn Flask with Kubernetes and Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s walk through a real-world deployment of a sklearn-based machine learning
    model using Docker and Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a Dockerfile. Note, that this serves out a Flask application. The Flask
    application will host the sklearn application. Note that you might want to install
    Hadolint, which allows you to lint a Dockerfile: [*https://github.com/hadolint/hadolint*](https://github.com/hadolint/hadolint).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the `Makefile` and it serves as a central point of the application
    runtime:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the *requirements.txt* file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the *app.py* file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the *run_docker.sh* file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the *run_kubernetes.sh* file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Sklearn Flask with Kubernetes and Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You may be asking yourself how the model got created and then “pickled” out.
    You can see the [whole notebook here](https://oreil.ly/_pHz-).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import some libraries for machine learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '`**In[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '`**In[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '`**Out[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | `CRIM` | `ZN` | `INDUS` | `CHAS` | `NOX` | `RM` | `AGE` |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `0` | `0.00632` | `18.0` | `2.31` | `0` | `0.538` | `6.575` | `65.2` |'
  prefs: []
  type: TYPE_TB
- en: '| `1` | `0.02731` | `0.0` | `7.07` | `0` | `0.469` | `6.421` | `78.9` |'
  prefs: []
  type: TYPE_TB
- en: '| `2` | `0.02729` | `0.0` | `7.07` | `0` | `0.469` | `7.185` | `61.1` |'
  prefs: []
  type: TYPE_TB
- en: '| `3` | `0.03237` | `0.0` | `2.18` | `0` | `0.458` | `6.998` | `45.8` |'
  prefs: []
  type: TYPE_TB
- en: '| `4` | `0.06905` | `0.0` | `2.18` | `0` | `0.458` | `7.147` | `54.2` |'
  prefs: []
  type: TYPE_TB
- en: '|  | `DIS` | `RAD` | `TAX` | `PTRATIO` | `B` | `LSTAT` | `MEDV` |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `0` | `4.0900` | `1` | `296.0` | `15.3` | `396.90` | `4.98` | `24.0` |'
  prefs: []
  type: TYPE_TB
- en: '| `1` | `4.9671` | `2` | `242.0` | `17.8` | `396.90` | `9.14` | `21.6` |'
  prefs: []
  type: TYPE_TB
- en: '| `2` | `4.9671` | `2` | `242.0` | `17.8` | `392.83` | `4.03` | `34.7` |'
  prefs: []
  type: TYPE_TB
- en: '| `3` | `6.0622` | `3` | `222.0` | `18.7` | `394.63` | `2.94` | `33.4` |'
  prefs: []
  type: TYPE_TB
- en: '| `4` | `6.0622` | `3` | `222.0` | `18.7` | `396.90` | `5.33` | `36.2` |'
  prefs: []
  type: TYPE_TB
- en: EDA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'These are the features of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: CHAS
  prefs: []
  type: TYPE_NORMAL
- en: Charles River dummy variable (1 if tract bounds river; 0 otherwise)
  prefs: []
  type: TYPE_NORMAL
- en: RM
  prefs: []
  type: TYPE_NORMAL
- en: Average number of rooms per dwelling
  prefs: []
  type: TYPE_NORMAL
- en: TAX
  prefs: []
  type: TYPE_NORMAL
- en: Full-value property tax rate per $10,000
  prefs: []
  type: TYPE_NORMAL
- en: PTRATIO
  prefs: []
  type: TYPE_NORMAL
- en: Pupil-teacher ratio by town
  prefs: []
  type: TYPE_NORMAL
- en: Bk
  prefs: []
  type: TYPE_NORMAL
- en: The proportion of black people by town
  prefs: []
  type: TYPE_NORMAL
- en: LSTAT
  prefs: []
  type: TYPE_NORMAL
- en: '`%` lower status of the population'
  prefs: []
  type: TYPE_NORMAL
- en: MEDV
  prefs: []
  type: TYPE_NORMAL
- en: Median value of owner-occupied homes in $1000s
  prefs: []
  type: TYPE_NORMAL
- en: '`**In[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '`**Out[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | `CHAS` | `RM` | `TAX` | `PTRATIO` | `B` | `LSTAT` | `MEDV` |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `0` | `0` | `6.575` | `296.0` | `15.3` | `396.90` | `4.98` | `24.0` |'
  prefs: []
  type: TYPE_TB
- en: '| `1` | `0` | `6.421` | `242.0` | `17.8` | `396.90` | `9.14` | `21.6` |'
  prefs: []
  type: TYPE_TB
- en: '| `2` | `0` | `7.185` | `242.0` | `17.8` | `392.83` | `4.03` | `34.7` |'
  prefs: []
  type: TYPE_TB
- en: '| `3` | `0` | `6.998` | `222.0` | `18.7` | `394.63` | `2.94` | `33.4` |'
  prefs: []
  type: TYPE_TB
- en: '| `4` | `0` | `7.147` | `222.0` | `18.7` | `396.90` | `5.33` | `36.2` |'
  prefs: []
  type: TYPE_TB
- en: Modeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is where the modeling occurs in the notebook. One useful strategy is to
    always create four main sections of a notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: Ingestion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EDA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this modeling section, the data is extracted from the DataFrame and passed
    into the sklearn `train_test_split` module which does the heavy lifting of splitting
    the data into training and validation data.
  prefs: []
  type: TYPE_NORMAL
- en: Split Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`**In[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '`**In[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '`**Out[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Tune Scaled GBM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This model uses several advanced techniques that you can reference in many successful
    Kaggle projects. These techniques include GridSearch which can help find the optimal
    hyperparameters. Note, too, that scaling of the data is peformed. Most machine
    learning algorithms expect some type of scaling to create accurate predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '`**In[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '`**Out[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Fit Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This model is fit using the GradientBoostingRegressor. The final step after
    training the model is to fit the model and check for error using the data that
    was set aside. This data is scaled and passed into the model, and the accuracy
    is evaluated using the metric “Mean Squared Error.”
  prefs: []
  type: TYPE_NORMAL
- en: '`**In[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '`**Out[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Evaluate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the trickier aspects to machine learning is evaluating the model. This
    example shows how you can add predictions and the original home price to the same
    DataFrame. That DataFrame can be used to substract the differences.
  prefs: []
  type: TYPE_NORMAL
- en: '`**In[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The differences are shown here.
  prefs: []
  type: TYPE_NORMAL
- en: '`**Out[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | `Org house price` | `Pred house price` | `Difference` |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `0` | `21.7` | `21` | `0.7` |'
  prefs: []
  type: TYPE_TB
- en: '| `1` | `18.5` | `19` | `-0.5` |'
  prefs: []
  type: TYPE_TB
- en: '| `2` | `22.2` | `20` | `2.2` |'
  prefs: []
  type: TYPE_TB
- en: '| `3` | `20.4` | `19` | `1.4` |'
  prefs: []
  type: TYPE_TB
- en: '| `4` | `8.8` | `9` | `-0.2` |'
  prefs: []
  type: TYPE_TB
- en: Using the describe method on Pandas is a great way to see the distribution of
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: '`**In[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '`**Out[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | `Org house price` | `Pred house price` | `Difference` |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `count` | `102.000000` | `102.000000` | `102.000000` |'
  prefs: []
  type: TYPE_TB
- en: '| `mean` | `22.573529` | `22.117647` | `0.455882` |'
  prefs: []
  type: TYPE_TB
- en: '| `std` | `9.033622` | `8.758921` | `5.154438` |'
  prefs: []
  type: TYPE_TB
- en: '| `min` | `6.300000` | `8.000000` | `-34.100000` |'
  prefs: []
  type: TYPE_TB
- en: '| `25%` | `17.350000` | `17.000000` | `-0.800000` |'
  prefs: []
  type: TYPE_TB
- en: '| `50%` | `21.800000` | `20.500000` | `0.600000` |'
  prefs: []
  type: TYPE_TB
- en: '| `75%` | `24.800000` | `25.000000` | `2.200000` |'
  prefs: []
  type: TYPE_TB
- en: '| `max` | `50.000000` | `56.000000` | `22.000000` |'
  prefs: []
  type: TYPE_TB
- en: adhoc_predict
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s test this prediction model to see what the workflow would be after unpickling.
    When developing a web API for a machine learning model, it can be helpful to test
    out the sections of code that the API will perform in the notebook itself. It
    is much easier to debug and create functions inside the actual notebook than struggle
    to create the correct functions inside a web application.
  prefs: []
  type: TYPE_NORMAL
- en: '`**In[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '`**Out[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | `CHAS` | `RM` | `TAX` | `PTRATIO` | `B` | `LSTAT` | `MEDV` |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `0` | `0` | `6.575` | `296.0` | `15.3` | `396.9` | `4.98` | `24.0` |'
  prefs: []
  type: TYPE_TB
- en: '`**In[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '`**Out[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | `CHAS` | `RM` | `TAX` | `PTRATIO` | `B` | `LSTAT` |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `0` | `0` | `6.575` | `296.0` | `15.3` | `396.9` | `4.98` |'
  prefs: []
  type: TYPE_TB
- en: JSON Workflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is a section of the notebook that is useful for debugging Flask apps. As
    mentioned earlier, it is much more straightforward to develop the API code inside
    the machine learning project, make sure it works, then transport that code to
    a script. The alternative is trying to get the exact code syntax in a software
    project that doesn’t have the same interactive tools that Jupyter provides.
  prefs: []
  type: TYPE_NORMAL
- en: '`**In[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '`**Out[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Scale Input
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The data has to be scaled back to be predicted. This workflow needed to be flushed
    out in the notebook instead of struggling to get it to work in a web application
    that will be much tougher to debug. The section below shows the code that solves
    that portion of the machine learning prediction pipeline. It can then be used
    to create a function in a Flask application.
  prefs: []
  type: TYPE_NORMAL
- en: '`**In[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '`**Out[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '`**In[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '`**Out[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Pickling sklearn
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, let’s export this model.
  prefs: []
  type: TYPE_NORMAL
- en: '`**In[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '`**In[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '`**Out[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '`**In[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '`**Out[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Unpickle and predict
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`**In[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: adhoc_predict from Pickle
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`**In[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '`**Out[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | `CHAS` | `RM` | `TAX` | `PTRATIO` | `B` | `LSTAT` | `MEDV` |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `0` | `0` | `6.575` | `296.0` | `15.3` | `396.90` | `4.98` | `24.0` |'
  prefs: []
  type: TYPE_TB
- en: '| `1` | `0` | `6.421` | `242.0` | `17.8` | `396.90` | `9.14` | `21.6` |'
  prefs: []
  type: TYPE_TB
- en: '| `2` | `0` | `7.185` | `242.0` | `17.8` | `392.83` | `4.03` | `34.7` |'
  prefs: []
  type: TYPE_TB
- en: '| `3` | `0` | `6.998` | `222.0` | `18.7` | `394.63` | `2.94` | `33.4` |'
  prefs: []
  type: TYPE_TB
- en: '| `4` | `0` | `7.147` | `222.0` | `18.7` | `396.90` | `5.33` | `36.2` |'
  prefs: []
  type: TYPE_TB
- en: '`**In[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '`**Out[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | `CHAS` | `RM` | `TAX` | `PTRATIO` | `B` | `LSTAT` |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `0` | `0` | `6.575` | `296.0` | `15.3` | `396.9` | `4.98` |'
  prefs: []
  type: TYPE_TB
- en: Scale Input
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`**In[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '`**Out[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '`**In[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '`**Out[0]:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Finally, the pickled model is loaded back in and tested against a real dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are some key differences between scikit-learn and PyTorch?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is AutoML and why would you use it?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Change the scikit-learn model to use height to predict weight.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run the PyTorch example in Google Colab notebooks and toggle between CPU and
    GPU runtimes. Explain the performance difference if there is one.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is EDA and why is it so important in a data science project?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Case Study Question
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Go to the Kaggle website, take a popular notebook in Python, and convert it
    to a containerized Flask application that serves out predictions using the example
    shown in this chapter as a guide. Now deploy this to a cloud environment via a
    hosted Kubernetes service such as Amazon EKS.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning Assessments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Explain the different types of machine learning frameworks and ecosystems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run and debug a preexisting machine learning project in scikit-learn and PyTorch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Containerize a Flask scikit-learn model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand the production machine learning maturity model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
