["```py\nfrom dask_ml.preprocessing import StandardScaler\nimport dask.array as da\nimport numpy as np\n\ndf = dd.read_parquet(url)\ntrip_dist_df = df[[\"trip_distance\", \"total_amount\"]]\nscaler = StandardScaler()\n\nscaler.fit(trip_dist_df)\ntrip_dist_df_scaled = scaler.transform(trip_dist_df)\ntrip_dist_df_scaled.head()\n```", "```py\nfrom dask_ml.preprocessing import Categorizer\nfrom pandas.api.types import CategoricalDtype\n\npayment_type_amt_df = df[[\"payment_type\", \"total_amount\"]]\n\ncat = Categorizer(categories={\"payment_type\": CategoricalDtype([1, 2, 3, 4])})\ncategorized_df = cat.fit_transform(payment_type_amt_df)\ncategorized_df.dtypes\npayment_type_amt_df.head()\n```", "```py\ntrain = train.categorize(\"VendorID\")\ntrain = train.categorize(\"passenger_count\")\ntrain = train.categorize(\"store_and_fwd_flag\")\n\ntest = test.categorize(\"VendorID\")\ntest = test.categorize(\"passenger_count\")\ntest = test.categorize(\"store_and_fwd_flag\")\n```", "```py\nfrom dask_ml.preprocessing import DummyEncoder\n\ndummy = DummyEncoder()\ndummified_df = dummy.fit_transform(categorized_df)\ndummified_df.dtypes\ndummified_df.head()\n```", "```py\ntrain['Hour'] = train['tpep_pickup_datetime'].dt.hour\ntest['Hour'] = test['tpep_pickup_datetime'].dt.hour\n\ntrain['dayofweek'] = train['tpep_pickup_datetime'].dt.dayofweek\ntest['dayofweek'] = test['tpep_pickup_datetime'].dt.dayofweek\n\ntrain = train.categorize(\"dayofweek\")\ntest = test.categorize(\"dayofweek\")\n\ndom_train = dd.get_dummies(\n    train,\n    columns=['dayofweek'],\n    prefix='dom',\n    prefix_sep='_')\ndom_test = dd.get_dummies(\n    test,\n    columns=['dayofweek'],\n    prefix='dom',\n    prefix_sep='_')\n\nhour_train = dd.get_dummies(\n    train,\n    columns=['dayofweek'],\n    prefix='h',\n    prefix_sep='_')\nhour_test = dd.get_dummies(\n    test,\n    columns=['dayofweek'],\n    prefix='h',\n    prefix_sep='_')\n\ndow_train = dd.get_dummies(\n    train,\n    columns=['dayofweek'],\n    prefix='dow',\n    prefix_sep='_')\ndow_test = dd.get_dummies(\n    test,\n    columns=['dayofweek'],\n    prefix='dow',\n    prefix_sep='_')\n```", "```py\nfrom dask_ml.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    df['trip_distance'], df['total_amount'])\n```", "```py\nfrom dask_ml.linear_model import LinearRegression\nfrom dask_ml.model_selection import train_test_split\n\nregr_df = df[['trip_distance', 'total_amount']].dropna()\nregr_X = regr_df[['trip_distance']]\nregr_y = regr_df[['total_amount']]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    regr_X, regr_y)\n\nX_train = X_train.to_dask_array(lengths=[100]).compute()\nX_test = X_test.to_dask_array(lengths=[100]).compute()\ny_train = y_train.to_dask_array(lengths=[100]).compute()\ny_test = y_test.to_dask_array(lengths=[100]).compute()\n\nreg = LinearRegression()\nreg.fit(X_train, y_train)\ny_pred = reg.predict(X_test)\n\nr2_score(y_test, y_pred)\n```", "```py\nfrom sklearn.linear_model import LinearRegression as ScikitLinearRegression\nfrom sklearn.linear_model import SGDRegressor as ScikitSGDRegressor\n\nestimators = [ScikitLinearRegression(), ScikitSGDRegressor()]\nrun_tasks = [dask.delayed(estimator.fit)(X_train, y_train)\n             for estimator in estimators]\nrun_tasks\n```", "```py\nfrom dask.distributed import Client\nfrom joblib import parallel_backend\n\nclient = Client('127.0.0.1:8786')\n\nX, y = load_my_data()\nnet = get_that_net()\n\ngs = GridSearchCV(\n    net,\n    param_grid={'lr': [0.01, 0.03]},\n    scoring='accuracy',\n)\n\nXGBClassifier()\n\nwith parallel_backend('dask'):\n    gs.fit(X, y)\nprint(gs.cv_results_)\n```", "```py\nimport xgboost as xgb\nfrom dask_cuda import LocalCUDACluster\nfrom dask.distributed import Client\n\nn_workers = 4\ncluster = LocalCUDACluster(n_workers)\nclient = Client(cluster)\n\ndtrain = xgb.dask.DaskDMatrix(client, X_train, y_train)\n\nbooster = xgb.dask.train(\n    client,\n    {\"booster\": \"gbtree\", \"verbosity\": 2, \"nthread\": 4, \"eta\": 0.01, gamma=0,\n     \"max_depth\": 5, \"tree_method\": \"auto\", \"objective\": \"reg:squarederror\"},\n    dtrain,\n    num_boost_round=4,\n    evals=[(dtrain, \"train\")])\n```", "```py\nimport xgboost as xgb\n\ndtrain = xgb.DMatrix(X_train, label=y_train, feature_names=X_train.columns)\ndvalid = xgb.DMatrix(X_test, label=y_test, feature_names=X_test.columns)\nwatchlist = [(dtrain, 'train'), (dvalid, 'valid')]\nxgb_pars = {\n    'min_child_weight': 1,\n    'eta': 0.5,\n    'colsample_bytree': 0.9,\n    'max_depth': 6,\n    'subsample': 0.9,\n    'lambda': 1.,\n    'nthread': -1,\n    'booster': 'gbtree',\n    'silent': 1,\n    'eval_metric': 'rmse',\n    'objective': 'reg:linear'}\nmodel = xgb.train(xgb_pars, dtrain, 10, watchlist, early_stopping_rounds=2,\n                  maximize=False, verbose_eval=1)\nprint('Modeling RMSLE %.5f' % model.best_score)\n\nxgb.plot_importance(model, max_num_features=28, height=0.7)\n\npred = model.predict(dtest)\npred = np.exp(pred) - 1\n```", "```py\nimport xgboost as xgb\nfrom dask_cuda import LocalCUDACluster\nfrom dask.distributed import Client\n\nn_workers = 4\ncluster = LocalCUDACluster(n_workers)\nclient = Client(cluster)\n\ndef fit_model(client, X, y, X_valid, y_valid,\n              early_stopping_rounds=5) -> xgb.Booster:\n    Xy_valid = dxgb.DaskDMatrix(client, X_valid, y_valid)\n    # train the model\n    booster = xgb.dask.train(\n        client,\n        {\"booster\": \"gbtree\", \"verbosity\": 2, \"nthread\": 4, \"eta\": 0.01, gamma=0,\n         \"max_depth\": 5, \"tree_method\": \"gpu_hist\", \"objective\": \"reg:squarederror\"},\n        dtrain,\n        num_boost_round=500,\n        early_stopping_rounds=early_stopping_rounds,\n        evals=[(dtrain, \"train\")])[\"booster\"]\n    return booster\n\ndef predict(client, model, X):\n    predictions = xgb.predict(client, model, X)\n    assert isinstance(predictions, dd.Series)\n    return predictions\n```", "```py\nimport dask.dataframe as dd\nimport dask.datasets\nfrom dask_sql import Context\n\n# read dataset\ntaxi_df = dd.read_csv('./data/taxi_train_subset.csv')\ntaxi_test = dd.read_csv('./data/taxi_test.csv')\n\n# create a context to register tables\nc = Context()\nc.create_table(\"taxi_test\", taxi_test)\nc.create_table(\"taxicab\", taxi_df)\n```", "```py\nimport dask.dataframe as dd\nimport dask.datasets\nfrom dask_sql import Context\n\nc = Context()\n# define model\nc.sql(\n    \"\"\"\nCREATE MODEL fare_linreg_model WITH (\n model_class = 'LinearRegression',\n wrap_predict = True,\n target_column = 'fare_amount'\n) AS (\n SELECT passenger_count, fare_amount\n FROM taxicab\n LIMIT 1000\n)\n\"\"\"\n)\n\n# describe model\nc.sql(\n    \"\"\"\nDESCRIBE MODEL fare_linreg_model\n \"\"\"\n).compute()\n\n# run inference\nc.sql(\n    \"\"\"\nSELECT\n *\nFROM PREDICT(MODEL fare_linreg_model,\n SELECT * FROM taxi_test\n)\n \"\"\"\n).compute()\n```", "```py\nimport dask.dataframe as dd\nimport dask.datasets\nfrom dask_sql import Context\n\nc = Context()\n# define model\nc.sql(\n    \"\"\"\nCREATE MODEL classify_faretype WITH (\n model_class = 'XGBClassifier',\n target_column = 'fare_type'\n) AS (\n SELECT airport_surcharge, passenger_count, fare_type\n FROM taxicab\n LIMIT 1000\n)\n\"\"\"\n)\n\n# describe model\nc.sql(\n    \"\"\"\nDESCRIBE MODEL classify_faretype\n \"\"\"\n).compute()\n\n# run inference\nc.sql(\n    \"\"\"\nSELECT\n *\nFROM PREDICT(MODEL classify_faretype,\n SELECT airport_surcharge, passenger_count, FROM taxi_test\n)\n \"\"\"\n).compute()\n```", "```py\nfrom skimage.io import imread\nfrom skimage.io.collection import alphanumeric_key\nfrom dask import delayed\nimport dask.array as da\nimport os\n\nroot, dirs, filenames = os.walk(dataset_dir)\n# sample first file\nimread(filenames[0])\n\n@dask.delayed\ndef lazy_reader(file):\n    return imread(file)\n\n# we have a bunch of delayed readers attached to the files\nlazy_arrays = [lazy_reader(file) for file in filenames]\n\n# read individual files from reader into a dask array\n# particularly useful if each image is a large file like DICOM radiographs\n# mammography dicom tends to be extremely large\ndask_arrays = [\n    da.from_delayed(delayed_reader, shape=(4608, 5200,), dtype=np.float32)\n    for delayed_reader in lazy_arrays\n]\n```", "```py\nimport dask.dataframe as dd\nimport dask.bag as db\n\ndef rowwise_operation(row, arg *):\n    # row-wise compute\n    return result\n\ndef partition_operation(df):\n    # partition wise logic\n    result = df[col1].apply(rowwise_operation)\n    return result\n\nddf = dd.read_csv(“metadata_of_files”)\nresults = ddf.map_partitions(partition_operation)\nresults.compute()\n\n# An alternate way, but note the .apply() here becomes a pandas apply, not\n# Dask .apply(), and you must define axis = 1\nddf.map_partitions(\n    lambda partition: partition.apply(\n        lambda row: rowwise_operation(row), axis=1), meta=(\n            'ddf', object))\n```", "```py\ndef handle_batch(batch, conn, nlp_model):\n    # run_inference_here.\n    conn.commit()\n\ndef handle_partition(df):\n    worker = get_worker()\n    conn = connect_to_db()\n    try:\n        nlp_model = worker.roberta_model\n    except BaseException:\n        nlp_model = load_model()\n        worker.nlp_model = nlp_model\n    result, batch = [], []\n    for _, row in part.iterrows():\n        if len(batch) % batch_size == 0 and len(batch) > 0:\n            batch_results = handle_batch(batch, conn, nlp_model)\n            result.append(batch_results)\n            batch = []\n        batch.append((row.doc_id, row.sent_id, row.utterance))\n    if len(batch) > 0:\n        batch_results = handle_batch(batch, conn, nlp_model)\n        result.append(batch_results)\n    conn.close()\n    return result\n\nddf = dd.read_csv(\"metadata.csv”)\nresults = ddf.map_partitions(handle_partition)\nresults.compute()\n```"]