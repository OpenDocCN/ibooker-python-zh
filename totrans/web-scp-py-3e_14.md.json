["```py\nimport re\nimport string \n\ndef replace_newlines(text):\n    return text.replace('\\n', ' ')\n\ndef make_lowercase(text):\n    return text.lower()\n\ndef split_sentences(text):\n    return [s.strip() for s in text.split('. ')]\n\npuncts = [re.escape(c) for c in string.punctuation]\nPUNCTUATION_REGEX = re.compile('|'.join(puncts))\ndef remove_punctuation(text):\n    return re.sub(PUNCTUATION_REGEX, '', text)\n\n```", "```py\ncontent = str(\n    urlopen('http://pythonscraping.com/files/inaugurationSpeech.txt').read(),\n    'utf-8'\n)\n\ntext_operations = [\n    replace_newlines,\n    split_sentences,\n    make_lowercase,\n    remove_punctuation\n]\n\ncleaned = content\nfor op in text_operations:\n    if type(cleaned) == list:\n        cleaned = [op(c) for c in cleaned]\n    else:\n        cleaned = op(cleaned)\n\nprint(cleaned)\n\n```", "```py\ndef getNgrams(text, n):\n    text = text.split(' ')\n    return [' '.join(text[i:i+n]) for i in range(len(text)-n+1)]\n\ndef countNGramsFromSentences(sentences, n):\n    counts = Counter()\n    for sentence in sentences:\n        counts.update(getNgrams(sentence, n))\n    return counts\n\ncounts = countNGramsFromSentences(cleaned, 2)\nprint(counts.most_common())\n\n```", "```py\n[('of the', 213), ('in the', 65), ('to the', 61), ('by the', 41),\n('the constitution', 34), ('of our', 29), ('to be', 26),\n('the people', 24), ('from the', 24), ('that the', 23)...\n```", "```py\nCOMMON_WORDS = ['the', 'be', 'and', 'of', 'a', 'in', 'to', 'have',\n'it', 'i', 'that', 'for', 'you', 'he', 'with', 'on', 'do', 'say',\n'this', 'they', 'is', 'an', 'at', 'but', 'we', 'his', 'from', 'that',\n'not', 'by', 'she', 'or', 'as', 'what', 'go', 'their', 'can',\n'who', 'get', 'if', 'would', 'her', 'all', 'my', 'make', 'about',\n'know', 'will', 'as', 'up', 'one', 'time', 'has', 'been', 'there',\n'year', 'so', 'think', 'when', 'which', 'them', 'some', 'me',\n'people', 'take', 'out', 'into', 'just', 'see', 'him', 'your',\n'come', 'could', 'now', 'than', 'like', 'other', 'how', 'then',\n'its', 'our', 'two', 'more', 'these', 'want', 'way', 'look', 'first',\n'also', 'new', 'because', 'day', 'more', 'use', 'no', 'man', 'find',\n'here', 'thing', 'give', 'many', 'well']\n\ndef isCommon(ngram):\n  return any([w in COMMON_WORDS for w in ngram.split(' ')])\n\ndef filterCommon(counts):\n  return Counter({key: val for key, val in counts.items() if not isCommon(key)})\n\nfilterCommon(counts).most_common()\n\n```", "```py\n('united states', 10),\n('executive department', 4),\n('general government', 4),\n('called upon', 3),\n('chief magistrate', 3),\n('legislative body', 3),\n('same causes', 3),\n('government should', 3),\n('whole country', 3)\n```", "```py\nfrom urllib.request import urlopen\nfrom random import randint\nfrom collections import defaultdict\n\ndef retrieveRandomWord(wordList):\n    randIndex = randint(1, sum(wordList.values()))\n    for word, value in wordList.items():\n        randIndex -= value\n        if randIndex <= 0:\n            return word\n\ndef cleanAndSplitText(text):\n    # Remove newlines and quotes\n    text = text.replace('\\n', ' ').replace('\"', '');\n\n    # Make sure punctuation marks are treated as their own \"words,\"\n    # so that they will be included in the Markov chain\n    punctuation = [',','.',';',':']\n    for symbol in punctuation:\n        text = text.replace(symbol, f' {symbol} ');\n    # Filter out empty words\n    return [word for word in text.split(' ') if word != '']\n\ndef buildWordDict(text):\n    words = cleanAndSplitText(text)\n    wordDict = defaultdict(dict)\n    for i in range(1, len(words)):\n        wordDict[words[i-1]][words[i]] = \\\n        wordDict[words[i-1]].get(words[i], 0) + 1\n    return wordDict\n\ntext = str(urlopen('http://pythonscraping.com/files/inaugurationSpeech.txt')\n          .read(), 'utf-8')\nwordDict = buildWordDict(text)\n\n#Generate a Markov chain of length 100\nlength = 100\nchain = ['I']\nfor i in range(0, length):\n    newWord = retrieveRandomWord(wordDict[chain[-1]])\n    chain.append(newWord)\n\nprint(' '.join(chain))\n\n```", "```py\nI sincerely believe in Chief Magistrate to make all necessary sacrifices and\noppression of the remedies which we may have occurred to me in the arrangement \nand disbursement of the democratic claims them , consolatory to have been best \npolitical power in fervently commending every other addition of legislation , by\nthe interests which violate that the Government would compare our aboriginal \nneighbors the people to its accomplishment . The latter also susceptible of the \nConstitution not much mischief , disputes have left to betray . The maxim which \nmay sometimes be an impartial and to prevent the adoption or \n```", "```py\n{word_a : {word_b : 2, word_c : 1, word_d : 1},\n word_e : {word_b : 5, word_d : 2},...}\n```", "```py\nimport pymysql\n\nconn = pymysql.connect(host='127.0.0.1', unix_socket='/tmp/mysql.sock',\n    user='', passwd='', db='mysql', charset='utf8')\ncur = conn.cursor()\ncur.execute('USE wikipedia')\n\ndef getUrl(pageId):\n    cur.execute('SELECT url FROM pages WHERE id = %s', (int(pageId)))\n    return cur.fetchone()[0]\n\ndef getLinks(fromPageId):\n    cur.execute('SELECT toPageId FROM links WHERE fromPageId = %s',\n        (int(fromPageId)))\n    if cur.rowcount == 0:\n        return []\n    return [x[0] for x in cur.fetchall()]\n\ndef searchBreadth(targetPageId, paths=[[1]]):\n    newPaths = []\n    for path in paths:\n        links = getLinks(path[-1])\n        for link in links:\n            if link == targetPageId:\n                return path + [link]\n            else:\n                newPaths.append(path+[link])\n    return searchBreadth(targetPageId, newPaths)\n\nnodes = getLinks(1)\ntargetPageId = 28624\npageIds = searchBreadth(targetPageId)\nfor pageId in pageIds:\n    print(getUrl(pageId))\n\n```", "```py\n/wiki/Kevin_Bacon\n/wiki/Primetime_Emmy_Award_for_Outstanding_Lead_Actor_in_a_\nMiniseries_or_a_Movie\n/wiki/Gary_Gilmore\n/wiki/Eric_Idle\n\n```", "```py\n>>> import nltk\n>>> nltk.download()\n\n```", "```py\nNLTK Downloader\n---------------------------------------------------------------------------\n    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n---------------------------------------------------------------------------\n\nDownloader> l\n\nPackages:\n\n  [*] abc................. Australian Broadcasting Commission 2006\n  [ ] alpino.............. Alpino Dutch Treebank\n  [*] averaged_perceptron_tagger Averaged Perceptron Tagger\n  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n  [ ] basque_grammars..... Grammars for Basque\n  [ ] bcp47............... BCP-47 Language Tags\n  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n                           Extraction Systems in Biology)\n  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n  [*] book_grammars....... Grammars from NLTK Book\n  [*] brown............... Brown Corpus\n  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n  [ ] cess_cat............ CESS-CAT Treebank\n  [ ] cess_esp............ CESS-ESP Treebank\n  [*] chat80.............. Chat-80 Data Files\n  [*] city_database....... City Database\n  [*] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n  [ ] comparative_sentences Comparative Sentence Dataset\n  [ ] comtrans............ ComTrans Corpus Sample\n  [*] conll2000........... CONLL 2000 Chunking Corpus\n\nHit Enter to continue:\n```", "```py\nCollections:\n  [P] all-corpora......... All the corpora\n  [P] all-nltk............ All packages available on nltk_data gh-pages\n                           branch\n  [P] all................. All packages\n  [*] book................ Everything used in the NLTK Book\n  [P] popular............. Popular packages\n  [P] tests............... Packages for running tests\n  [ ] third-party......... Third-party data packages\n\n([*] marks installed packages; [P] marks partially installed collections)\n\n```", "```py\nnltk.download('book')\n```", "```py\nfrom nltk import word_tokenize\nfrom nltk import Text\n\ntokens = word_tokenize('Here is some not very interesting text')\ntext = Text(tokens)\n```", "```py\nfrom nltk.book import *\n```", "```py\n*** Introductory Examples for the NLTK Book ***\nLoading text1, ..., text9 and sent1, ..., sent9\nType the name of the text or sentence to view it.\nType: 'texts()' or 'sents()' to list the materials.\ntext1: Moby Dick by Herman Melville 1851\ntext2: Sense and Sensibility by Jane Austen 1811\ntext3: The Book of Genesis\ntext4: Inaugural Address Corpus\ntext5: Chat Corpus\ntext6: Monty Python and the Holy Grail\ntext7: Wall Street Journal\ntext8: Personals Corpus\ntext9: The Man Who Was Thursday by G . K . Chesterton 1908\n```", "```py\n>>> len(text6)/len(set(text6))\n7.833333333333333\n```", "```py\n>>> from nltk import FreqDist\n>>> fdist = FreqDist(text6)\n>>> fdist.most_common(10)\n[(':', 1197), ('.', 816), ('!', 801), (',', 731), (\"'\", 421), ('[', 3\n19), (']', 312), ('the', 299), ('I', 255), ('ARTHUR', 225)]\n>>> fdist[\"Grail\"]\n34\n```", "```py\n>>> from nltk import bigrams\n>>> bigrams = bigrams(text6)\n>>> bigramsDist = FreqDist(bigrams)\n>>> bigramsDist[('Sir', 'Robin')]\n18\n```", "```py\n>>> from nltk import ngrams\n>>> fourgrams = ngrams(text6, 4)\n>>> fourgramsDist = FreqDist(fourgrams)\n>>> fourgramsDist[('father', 'smelt', 'of', 'elderberries')]\n1\n```", "```py\nfrom nltk.book import *\nfrom nltk import ngrams\n\nfourgrams = ngrams(text6, 4)\n\n[f for f in fourgrams if f[0] == 'coconut']\n\n```", "```py\n>>> from nltk.book import *\n>>> from nltk import word_tokenize\n>>> text = word_tokenize('Strange women lying in ponds distributing swords'\\\n'is no basis for a system of government.')\n>>> from nltk import pos_tag\n>>> pos_tag(text)\n[('Strange', 'NNP'), ('women', 'NNS'), ('lying', 'VBG'), ('in', 'IN')\n, ('ponds', 'NNS'), ('distributing', 'VBG'), ('swords', 'NNS'), ('is'\n, 'VBZ'), ('no', 'DT'), ('basis', 'NN'), ('for', 'IN'), ('a', 'DT'), \n('system', 'NN'), ('of', 'IN'), ('government', 'NN'), ('.', '.')]\n```", "```py\n>>> text = word_tokenize('The dust was thick so he had to dust')\n>>> pos_tag(text)\n[('The', 'DT'), ('dust', 'NN'), ('was', 'VBD'), ('thick', 'JJ'), \n('so', 'RB'), ('he', 'PRP'), ('had', 'VBD'), ('to', 'TO'), ('dust', 'VB')]\n```", "```py\nfrom nltk import word_tokenize, sent_tokenize, pos_tag\nsentences = [\n    'Google is one of the best companies in the world.',\n    ' I constantly google myself to see what I\\'m up to.'\n]\nnouns = ['NN', 'NNS', 'NNP', 'NNPS']\n\nfor sentence in sentences:\n    for word, tag in pos_tag(word_tokenize(sentence)):\n        if word.lower() == 'google' and tag in nouns:\n            print(sentence)\n\n```"]