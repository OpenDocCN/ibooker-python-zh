["```py\n# Create the initial conditions\nu = vector of length N\nfor i in range(N):\n    u = 0 if there is water, 1 if there is dye\n\n# Evolve the initial conditions\nD = 1\nt = 0\ndt = 0.0001\nwhile True:\n    print(f\"Current time is: {t}\")\n    unew = vector of size N\n\n    # Update step for every cell\n    for i in range(N):\n        unew[i] = u[i] + D * dt * (u[(i+1)%N] + u[(i-1)%N] - 2 * u[i])\n    # Move the updated solution into u\n    u = unew\n\n    visualize(u)\n```", "```py\nfor i in range(N):\n    for j in range(M):\n        unew[i][j] = u[i][j] + dt * (\n            (u[(i + 1) % N][j] + u[(i - 1) % N][j] - 2 * u[i][j]) + # d^2 u / dx^2\n            (u[i][(j + 1) % M] + u[i][(j - 1) % M] - 2 * u[i][j])   # d^2 u / dy^2\n        )\n```", "```py\ngrid_shape = (640, 640)\n\ndef evolve(grid, dt, D=1.0):\n    xmax, ymax = grid_shape\n    new_grid = [[0.0] * ymax for x in range(xmax)]\n    for i in range(xmax):\n        for j in range(ymax):\n            grid_xx = (\n                grid[(i + 1) % xmax][j] + grid[(i - 1) % xmax][j] - 2.0 * grid[i][j]\n            )\n            grid_yy = (\n                grid[i][(j + 1) % ymax] + grid[i][(j - 1) % ymax] - 2.0 * grid[i][j]\n            )\n            new_grid[i][j] = grid[i][j] + D * (grid_xx + grid_yy) * dt\n    return new_grid\n```", "```py\ndef run_experiment(num_iterations):\n    # Setting up initial conditions ![1](Images/1.png)\n    xmax, ymax = grid_shape\n    grid = [[0.0] * ymax for x in range(xmax)]\n\n    # These initial conditions are simulating a drop of dye in the middle of our\n    # simulated region\n    block_low = int(grid_shape[0] * 0.4)\n    block_high = int(grid_shape[0] * 0.5)\n    for i in range(block_low, block_high):\n        for j in range(block_low, block_high):\n            grid[i][j] = 0.005\n\n    # Evolve the initial conditions\n    start = time.time()\n    for i in range(num_iterations):\n        grid = evolve(grid, 0.1)\n    return time.time() - start\n```", "```py\n$ kernprof -lv diffusion_python.py\nWrote profile results to diffusion_python.py.lprof\nTimer unit: 1e-06 s\n\nTotal time: 787.161 s\nFile: diffusion_python.py\nFunction: evolve at line 12\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n    12                                           @profile\n    13                                           def evolve(grid, dt, D=1.0):\n    14       500        843.0      1.7      0.0      xmax, ymax = grid_shape  ![1](Images/1.png)\n    15       500   24764794.0  49529.6      3.1      new_grid = [[0.0 for x in ...\n    16    320500     208683.0      0.7      0.0      for i in range(xmax):  ![2](Images/2.png)\n    17 205120000  128928913.0      0.6     16.4          for j in range(ymax):\n    18 204800000  222422192.0      1.1     28.3              grid_xx = ...\n    19 204800000  228660607.0      1.1     29.0              grid_yy = ...\n    20 204800000  182174957.0      0.9     23.1              new_grid[i][j] = ...\n    21       500        331.0      0.7      0.0      return new_grid  ![3](Images/3.png)\n```", "```py\nfrom math import sin\n\ndef loop_slow(num_iterations):\n    \"\"\"\n    >>> %timeit loop_slow(int(1e4))\n    1.68 ms ± 61.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n    \"\"\"\n    result = 0\n    for i in range(num_iterations):\n        result += i * sin(num_iterations)  ![1](Images/1.png)\n    return result\n\ndef loop_fast(num_iterations):\n    \"\"\"\n    >>> %timeit loop_fast(int(1e4))\n    551 µs ± 23.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n    \"\"\"\n    result = 0\n    factor = sin(num_iterations)\n    for i in range(num_iterations):\n        result += i\n    return result * factor\n```", "```py\ndef evolve(grid, dt, out, D=1.0):\n    xmax, ymax = grid_shape\n    for i in range(xmax):\n        for j in range(ymax):\n            grid_xx = (\n                grid[(i + 1) % xmax][j] + grid[(i - 1) % xmax][j] - 2.0 * grid[i][j]\n            )\n            grid_yy = (\n                grid[i][(j + 1) % ymax] + grid[i][(j - 1) % ymax] - 2.0 * grid[i][j]\n            )\n            out[i][j] = grid[i][j] + D * (grid_xx + grid_yy) * dt\n\ndef run_experiment(num_iterations):\n    # Setting up initial conditions\n    xmax, ymax = grid_shape\n    next_grid = [[0.0] * ymax for x in range(xmax)]\n    grid = [[0.0] * ymax for x in range(xmax)]\n\n    block_low = int(grid_shape[0] * 0.4)\n    block_high = int(grid_shape[0] * 0.5)\n    for i in range(block_low, block_high):\n        for j in range(block_low, block_high):\n            grid[i][j] = 0.005\n\n    start = time.time()\n    for i in range(num_iterations):\n        # evolve modifies grid and next_grid in-place\n        evolve(grid, 0.1, next_grid)\n        grid, next_grid = next_grid, grid\n    return time.time() - start\n```", "```py\n$ `kernprof` `-``lv` `diffusion_python_memory``.``py`\nWrote profile results to diffusion_python_memory.py.lprof\nTimer unit: 1e-06 s\n\nTotal time: 541.138 s\nFile: diffusion_python_memory.py\nFunction: evolve at line 12\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n    12                                           @profile\n    13                                           def evolve(grid, dt, out, D=1.0):\n    14       500        503.0      1.0      0.0      xmax, ymax = grid_shape\n    15    320500     131498.0      0.4      0.0      for i in range(xmax):\n    16 205120000   81105090.0      0.4     15.0          for j in range(ymax):\n    17 204800000  166271837.0      0.8     30.7              grid_xx = ...\n    18 204800000  169216352.0      0.8     31.3              grid_yy = ...\n    19 204800000  124412452.0      0.6     23.0              out[i][j] = ...\n\n```", "```py\n$ perf stat -e cycles,instructions,\\\n    cache-references,cache-misses,branches,branch-misses,task-clock,faults,\\\n    minor-faults,cs,migrations python diffusion_python_memory.py\n\n Performance counter stats for 'python diffusion_python_memory.py':\n\n   415,864,974,126      cycles                    #    2.889 GHz\n 1,210,522,769,388      instructions              #    2.91  insn per cycle\n       656,345,027      cache-references          #    4.560 M/sec\n       349,562,390      cache-misses              #   53.259 % of all cache refs\n   251,537,944,600      branches                  # 1747.583 M/sec\n     1,970,031,461      branch-misses             #    0.78% of all branches\n     143934.730837      task-clock (msec)         #    1.000 CPUs utilized\n            12,791      faults                    #    0.089 K/sec\n            12,791      minor-faults              #    0.089 K/sec\n               117      cs                        #    0.001 K/sec\n                 6      migrations                #    0.000 K/sec\n\n     143.935522122 seconds time elapsed\n```", "```py\nfrom array import array\nimport numpy\n\ndef norm_square_list(vector):\n    \"\"\"\n    >>> vector = list(range(1_000_000))\n    >>> %timeit norm_square_list(vector)\n    85.5 ms ± 1.65 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n    \"\"\"\n    norm = 0\n    for v in vector:\n        norm += v * v\n    return norm\n\ndef norm_square_list_comprehension(vector):\n    \"\"\"\n    >>> vector = list(range(1_000_000))\n    >>> %timeit norm_square_list_comprehension(vector)\n    80.3 ms ± 1.37 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n    \"\"\"\n    return sum([v * v for v in vector])\n\ndef norm_square_array(vector):\n    \"\"\"\n    >>> vector_array = array('l', range(1_000_000))\n    >>> %timeit norm_square_array(vector_array)\n    101 ms ± 4.69 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n    \"\"\"\n    norm = 0\n    for v in vector:\n        norm += v * v\n    return norm\n\ndef norm_square_numpy(vector):\n    \"\"\"\n    >>> vector_np = numpy.arange(1_000_000)\n    >>> %timeit norm_square_numpy(vector_np)\n    3.22 ms ± 136 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n    \"\"\"\n    return numpy.sum(vector * vector)  ![1](Images/1.png)\n\ndef norm_square_numpy_dot(vector):\n    \"\"\"\n    >>> vector_np = numpy.arange(1_000_000)\n    >>> %timeit norm_square_numpy_dot(vector_np)\n    960 µs ± 41.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n    \"\"\"\n    return numpy.dot(vector, vector)  ![2](Images/2.png)\n```", "```py\n>>> import numpy as np\n>>> np.roll([1,2,3,4], 1)\narray([4, 1, 2, 3])\n\n>>> np.roll([[1,2,3],[4,5,6]], 1, axis=1)\narray([[3, 1, 2],\n [6, 4, 5]])\n```", "```py\nfrom numpy import (zeros, roll)\n\ngrid_shape = (640, 640)\n\ndef laplacian(grid):\n    return (\n        roll(grid, +1, 0) +\n        roll(grid, -1, 0) +\n        roll(grid, +1, 1) +\n        roll(grid, -1, 1) -\n        4 * grid\n    )\n\ndef evolve(grid, dt, D=1):\n    return grid + dt * D * laplacian(grid)\n\ndef run_experiment(num_iterations):\n    grid = zeros(grid_shape)\n\n    block_low = int(grid_shape[0] * 0.4)\n    block_high = int(grid_shape[0] * 0.5)\n    grid[block_low:block_high, block_low:block_high] = 0.005\n\n    start = time.time()\n    for i in range(num_iterations):\n        grid = evolve(grid, 0.1)\n    return time.time() - start\n```", "```py\n$ perf stat -e cycles,instructions,\\\n    cache-references,cache-misses,branches,branch-misses,task-clock,faults,\\\n    minor-faults,cs,migrations python diffusion_numpy.py\n\n Performance counter stats for 'python diffusion_numpy.py':\n\n     8,432,416,866      cycles                    #    2.886 GHz\n     7,114,758,602      instructions              #    0.84  insn per cycle\n     1,040,831,469      cache-references          #  356.176 M/sec\n       216,490,683      cache-misses              #   20.800 % of all cache refs\n     1,252,928,847      branches                  #  428.756 M/sec\n         8,174,531      branch-misses             #    0.65% of all branches\n       2922.239426      task-clock (msec)         #    1.285 CPUs utilized\n           403,282      faults                    #    0.138 M/sec\n           403,282      minor-faults              #    0.138 M/sec\n                96      cs                        #    0.033 K/sec\n                 5      migrations                #    0.002 K/sec\n\n       2.274377105 seconds time elapsed\n```", "```py\n$ perf stat -e cycles,instructions,\\\n    cache-references,cache-misses,branches,branch-misses,task-clock,faults,\\\n    minor-faults,cs,migrations python diffusion_numpy.py\n\n Performance counter stats for 'python diffusion_numpy.py':\n\n    50,086,999,350      cycles                    #    2.888 GHz\n    53,611,608,977      instructions              #    1.07  insn per cycle\n     1,131,742,674      cache-references          #   65.266 M/sec\n       322,483,897      cache-misses              #   28.494 % of all cache refs\n     4,001,923,035      branches                  #  230.785 M/sec\n         6,211,101      branch-misses             #    0.16% of all branches\n      17340.464580      task-clock (msec)         #    1.000 CPUs utilized\n           403,193      faults                    #    0.023 M/sec\n           403,193      minor-faults              #    0.023 M/sec\n                74      cs                        #    0.004 K/sec\n                 6      migrations                #    0.000 K/sec\n\n      17.339656586 seconds time elapsed\n```", "```py\n>>> import numpy as np\n>>> array1 = np.random.random((10,10))\n>>> array2 = np.random.random((10,10))\n>>> id(array1)\n140199765947424 ![1](Images/1.png)\n>>> array1 += array2\n>>> id(array1)\n140199765947424 ![2](Images/2.png)\n>>> array1 = array1 + array2\n>>> id(array1)\n140199765969792 ![1](Images/3.png)\n```", "```py\n>>> import numpy as np\n\n>>> %%timeit array1, array2 = np.random.random((2, 100, 100))  ![1](Images/1.png) ![3](Images/3.png)\n... array1 = array1 + array2\n6.45 µs ± 53.3 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each) \n>>> %%timeit array1, array2 = np.random.random((2, 100, 100))  ![1](Images/1.png)\n... array1 += array2\n5.06 µs ± 78.7 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each) \n>>> %%timeit array1, array2 = np.random.random((2, 5, 5))  ![2](Images/2.png)\n... array1 = array1 + array2\n518 ns ± 4.88 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each) \n>>> %%timeit array1, array2 = np.random.random((2, 5, 5))  ![2](Images/2.png)\n... array1 += array2\n1.18 µs ± 6 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n```", "```py\ndef laplacian(grid, out):\n    np.copyto(out, grid)\n    out *= -4\n    out += np.roll(grid, +1, 0)\n    out += np.roll(grid, -1, 0)\n    out += np.roll(grid, +1, 1)\n    out += np.roll(grid, -1, 1)\n\ndef evolve(grid, dt, out, D=1):\n    laplacian(grid, out)\n    out *= D * dt\n    out += grid\n\ndef run_experiment(num_iterations):\n    next_grid = np.zeros(grid_shape)\n    grid = np.zeros(grid_shape)\n\n    block_low = int(grid_shape[0] * 0.4)\n    block_high = int(grid_shape[0] * 0.5)\n    grid[block_low:block_high, block_low:block_high] = 0.005\n\n    start = time.time()\n    for i in range(num_iterations):\n        evolve(grid, 0.1, next_grid)\n        grid, next_grid = next_grid, grid  ![1](Images/1.png)\n    return time.time() - start\n```", "```py\n$ perf stat -e cycles,instructions,\\\n    cache-references,cache-misses,branches,branch-misses,task-clock,faults,\\\n    minor-faults,cs,migrations python diffusion_numpy_memory.py\n\n Performance counter stats for 'python diffusion_numpy_memory.py':\n\n     6,880,906,446      cycles                    #    2.886 GHz\n     5,848,134,537      instructions              #    0.85  insn per cycle\n     1,077,550,720      cache-references          #  452.000 M/sec\n       217,974,413      cache-misses              #   20.229 % of all cache refs\n     1,028,769,315      branches                  #  431.538 M/sec\n         7,492,245      branch-misses             #    0.73% of all branches\n       2383.962679      task-clock (msec)         #    1.373 CPUs utilized\n            13,521      faults                    #    0.006 M/sec\n            13,521      minor-faults              #    0.006 M/sec\n               100      cs                        #    0.042 K/sec\n                 8      migrations                #    0.003 K/sec\n\n       1.736322099 seconds time elapsed\n\n```", "```py\nWrote profile results to diffusion_numpy_memory.py.lprof\nTimer unit: 1e-06 s\n\nTotal time: 1.58502 s\nFile: diffusion_numpy_memory.py\nFunction: evolve at line 21\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n    21                                           @profile\n    22                                           def evolve(grid, dt, out, D=1):\n    23       500    1327910.0   2655.8     83.8      laplacian(grid, out)\n    24       500     100733.0    201.5      6.4      out *= D * dt\n    25       500     156377.0    312.8      9.9      out += grid\n```", "```py\nimport numpy as np\n\ndef roll_add(rollee, shift, axis, out):\n    \"\"\"\n Given a matrix, a rollee, and an output matrix, out, this function will\n perform the calculation:\n\n >>> out += np.roll(rollee, shift, axis=axis)\n\n This is done with the following assumptions:\n * rollee is 2D\n * shift will only ever be +1 or -1\n * axis will only ever be 0 or 1 (also implied by the first assumption)\n\n Using these assumptions, we are able to speed up this function by avoiding\n extra machinery that numpy uses to generalize the roll function and also\n by making this operation intrinsically in-place.\n \"\"\"\n    if shift == 1 and axis == 0:\n        out[1:, :] += rollee[:-1, :]\n        out[0, :] += rollee[-1, :]\n    elif shift == -1 and axis == 0:\n        out[:-1, :] += rollee[1:, :]\n        out[-1, :] += rollee[0, :]\n    elif shift == 1 and axis == 1:\n        out[:, 1:] += rollee[:, :-1]\n        out[:, 0] += rollee[:, -1]\n    elif shift == -1 and axis == 1:\n        out[:, :-1] += rollee[:, 1:]\n        out[:, -1] += rollee[:, 0]\n\ndef test_roll_add():\n    rollee = np.asarray([[1, 2], [3, 4]])\n    for shift in (-1, +1):\n        for axis in (0, 1):\n            out = np.asarray([[6, 3], [9, 2]])\n            expected_result = np.roll(rollee, shift, axis=axis) + out\n            roll_add(rollee, shift, axis, out)\n            assert np.all(expected_result == out)\n\ndef laplacian(grid, out):\n    np.copyto(out, grid)\n    out *= -4\n    roll_add(grid, +1, 0, out)\n    roll_add(grid, -1, 0, out)\n    roll_add(grid, +1, 1, out)\n    roll_add(grid, -1, 1, out)\n```", "```py\n$ perf stat -e cycles,instructions,\\\n    cache-references,cache-misses,branches,branch-misses,task-clock,faults,\\\n    minor-faults,cs,migrations python diffusion_numpy_memory2.py\n\n Performance counter stats for 'python diffusion_numpy_memory2.py':\n\n     5,971,464,515      cycles                    #    2.888 GHz\n     5,893,131,049      instructions              #    0.99  insn per cycle\n     1,001,582,133      cache-references          #  484.398 M/sec\n        30,840,612      cache-misses              #    3.079 % of all cache refs\n     1,038,649,694      branches                  #  502.325 M/sec\n         7,562,009      branch-misses             #    0.73% of all branches\n       2067.685884      task-clock (msec)         #    1.456 CPUs utilized\n            11,981      faults                    #    0.006 M/sec\n            11,981      minor-faults              #    0.006 M/sec\n                95      cs                        #    0.046 K/sec\n                 3      migrations                #    0.001 K/sec\n\n       1.419869071 seconds time elapsed\n\n```", "```py\nfrom numexpr import evaluate\n\ndef evolve(grid, dt, next_grid, D=1):\n    laplacian(grid, next_grid)\n    evaluate(\"next_grid * D * dt + grid\", out=next_grid)\n```", "```py\n$ perf stat -e cycles,instructions,\\\n    cache-references,cache-misses,branches,branch-misses,task-clock,faults,\\\n    minor-faults,cs,migrations python diffusion_numpy_memory2_numexpr.py\n\n Performance counter stats for 'python diffusion_numpy_memory2_numexpr.py':\n\n     8,856,947,179      cycles                    #    2.872 GHz\n     9,354,357,453      instructions              #    1.06  insn per cycle\n     1,077,518,384      cache-references          #  349.423 M/sec\n        59,407,830      cache-misses              #    5.513 % of all cache refs\n     1,018,525,317      branches                  #  330.292 M/sec\n        11,941,430      branch-misses             #    1.17% of all branches\n       3083.709890      task-clock (msec)         #    1.991 CPUs utilized\n            15,820      faults                    #    0.005 M/sec\n            15,820      minor-faults              #    0.005 M/sec\n             8,671      cs                        #    0.003 M/sec\n             2,096      migrations                #    0.680 K/sec\n\n       1.548924090 seconds time elapsed\n\n```", "```py\nfrom scipy.ndimage.filters import laplace\n\ndef laplacian(grid, out):\n    laplace(grid, out, mode=\"wrap\")\n```", "```py\n$ perf stat -e cycles,instructions,\\\n    cache-references,cache-misses,branches,branch-misses,task-clock,faults,\\\n    minor-faults,cs,migrations python diffusion_scipy.py\n\n Performance counter stats for 'python diffusion_scipy.py':\n\n    10,051,801,725      cycles                    #    2.886 GHz\n    16,536,981,020      instructions              #    1.65  insn per cycle\n     1,554,557,564      cache-references          #  446.405 M/sec\n       126,627,735      cache-misses              #    8.146 % of all cache refs\n     2,673,416,633      branches                  #  767.696 M/sec\n         9,626,762      branch-misses             #    0.36% of all branches\n       3482.391211      task-clock (msec)         #    1.228 CPUs utilized\n            14,013      faults                    #    0.004 M/sec\n            14,013      minor-faults              #    0.004 M/sec\n                95      cs                        #    0.027 K/sec\n                 5      migrations                #    0.001 K/sec\n\n       2.835263796 seconds time elapsed\n\n```", "```py\n         0         1         2  ...       12        13\n0  1.016667  0.883333  1.033333 ...  1.016667  0.833333\n1  1.033333  1.016667  0.833333 ...  1.133333  0.883333\n2  0.966667  1.083333  1.183333 ...  1.000000  0.950000\n```", "```py\ndef ols_sklearn(row):\n    \"\"\"Solve OLS using scikit-learn's LinearRegression\"\"\"\n    est = LinearRegression()\n    X = np.arange(row.shape[0]).reshape(-1, 1) # shape (14, 1)\n    # note that the intercept is built inside LinearRegression\n    est.fit(X, row.values)\n    m = est.coef_[0] # note c is in est.intercept_\n    return m\n\ndef ols_lstsq(row):\n    \"\"\"Solve OLS using numpy.linalg.lstsq\"\"\"\n    # build X values for [0, 13]\n    X = np.arange(row.shape[0]) # shape (14,)\n    ones = np.ones(row.shape[0]) # constant used to build intercept\n    A = np.vstack((X, ones)).T # shape(14, 2)\n    # lstsq returns the coefficient and intercept as the first result\n    # followed by the residuals and other items\n    m, c = np.linalg.lstsq(A, row.values, rcond=-1)[0]\n    return m\n\ndef ols_lstsq_raw(row):\n    \"\"\"Variant of `ols_lstsq` where row is a numpy array (not a Series)\"\"\"\n    X = np.arange(row.shape[0])\n    ones = np.ones(row.shape[0])\n    A = np.vstack((X, ones)).T\n    m, c = np.linalg.lstsq(A, row, rcond=-1)[0]\n    return m\n```", "```py\n...\nlp = LineProfiler(est.fit)\nprint(\"Run on a single row\")\nlp.run(\"est.fit(X, row.values)\")\nlp.print_stats()\n\nLine #   % Time  Line Contents\n==============================\n   438               def fit(self, X, y, sample_weight=None):\n...\n   462      0.3          X, y = check_X_y(X, y,\n                                          accept_sparse=['csr', 'csc', 'coo'],\n   463     35.0                           y_numeric=True, multi_output=True)\n...\n   468      0.3          X, y, X_offset, y_offset, X_scale = \\\n                                 self._preprocess_data(\n   469      0.3                       X, y,\n                                      fit_intercept=self.fit_intercept,\n                                      normalize=self.normalize,\n   470      0.2                       copy=self.copy_X,\n                                      sample_weight=sample_weight,\n   471     29.3                       return_mean=True)\n...\n   502                       self.coef_, self._residues,\n                                      self.rank_, self.singular_ = \\\n   503     29.2                  linalg.lstsq(X, y)\n```", "```py\nms = []\nfor row_idx in range(df.shape[0]):\n    row = df.iloc[row_idx]\n    m = ols_lstsq(row)\n    ms.append(m)\nresults = pd.Series(ms)\n```", "```py\nms = []\nfor row_idx, row in df.iterrows():\n    m = ols_lstsq(row)\n    ms.append(m)\nresults = pd.Series(ms)\n```", "```py\nms = df.apply(ols_lstsq, axis=1)\nresults = pd.Series(ms)\n```", "```py\nms = df.apply(ols_lstsq_raw, axis=1, raw=True)\nresults = pd.Series(ms)\n```", "```py\nresults = None\nfor row_idx in range(df.shape[0]):\n    row = df.iloc[row_idx]\n    m = ols_lstsq(row)\n    if results is None:\n        results = pd.Series([m])\n    else:\n        results = pd.concat((results, pd.Series([m])))\n```", "```py\nIn [10]: df['0_as_str'] = df[0].apply(lambda v: str(v))\nOut[10]:\n              0            0_as_str\n0      1.016667  1.0166666666666666\n1      1.033333  1.0333333333333334\n2      0.966667  0.9666666666666667\n...\n\ndef find_9(s):\n    \"\"\"Return -1 if '9' not found else its location at position >= 0\"\"\"\n    return s.split('.')[1].find('9')\n\nIn [11]: df['0_as_str'].str.split('.', expand=True)[1].str.find('9')\nOut[11]:\n0       -1\n1       -1\n2        0\n\nIn [12]: %timeit df['0_as_str'].str.split('.', expand=True)[1].str.find('9')\nOut[12]: 183 ms ± 4.62 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\nIn [13]: %timeit df['0_as_str'].apply(find_9)\nOut[13]: 51 ms ± 987 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n```"]