<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 10. Dask with GPUs and Other &#10;Special Resources"><div class="chapter" id="ch10">
<h1><span class="label">Chapter 10. </span>Dask with GPUs and Other 
<span class="keep-together">Special Resources</span></h1>


<p>Sometimes the answer to our scaling problem isn’t throwing more computers at it; it’s throwing different types of resources at it. One example of this might be ten thousand monkeys trying to reproduce the works of Shakespeare, versus one Shakespeare.<sup><a data-type="noteref" id="id778-marker" href="ch10.xhtml#id778">1</a></sup> While performance varies, some benchmarks have shown <a href="https://oreil.ly/Iw3Sv">up to an 85% improvement in model training times</a> when using GPUs over CPUs. Continuing its modular tradition, the GPU logic of Dask is found in the libraries and ecosystem surrounding it. The libraries can either run on a collection of GPU workers or parallelize work over different GPUs on one host.</p>

<p>Most work we do on the computer <a data-type="indexterm" data-primary="TPUs (Tensor Processing Units)" id="id779"/><a data-type="indexterm" data-primary="Tensor Processing Units" data-see="TPUs" id="id780"/><a data-type="indexterm" data-primary="GPUs (graphics processing units)" id="id781"/>is done on the CPU. GPUs were created for displaying video but involve doing large amounts of vectorized floating point (e.g., non-integer) operations. With vectorized operations, the same operation is applied in parallel on large sets of data, like a <code>map</code>. Tensor Processing Units (TPUs) are similar to GPUs, except without also being used for graphics.</p>

<p>For our purposes, in Dask, we can think of GPUs and TPUs as specializing in offloading large vectorized computations, but there are many other kinds of accelerators. While much of this chapter is focused on GPUs, the same general techniques, albeit with different libraries, generally apply to other accelerators. Other kinds of specialized resources include NVMe drives, faster (or larger) RAM, TCP/IP offload, Just-a-Bunch-of-Disks expansion ports, and Intel’s OPTAIN memory. Special resources/accelerators can improve everything from network latency to writing large files to disk. What all these share is that Dask has no built-in understanding of these resources, and it’s up to you to provide that information to the Dask scheduler and also take advantage of it.</p>

<p>This chapter will look at the current state of accelerated analytics in Python and how to use these tools together with Dask. You will learn what kinds of problems are well suited to GPU acceleration, a bit about other kinds of accelerators, and how to apply this knowledge to your problems.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Cloud accounts and machines with access to GPUs are especially of interest to less-than-savory folks on the internet due to the relative ease of mining cryptocurrency. If you are used to working with only public data and lax security controls, take this as an opportunity to review your security process and restrict runtime access to only those who need it. Or be prepared for a really large cloud bill.</p>
</div>






<section data-type="sect1" data-pdf-bookmark="Transparent Versus Non-transparent Accelerators"><div class="sect1" id="id182">
<h1>Transparent Versus Non-transparent Accelerators</h1>

<p>Accelerators largely break down <a data-type="indexterm" data-primary="accelerators" data-secondary="transparent" id="id782"/><a data-type="indexterm" data-primary="accelerators" data-secondary="non-transparent" id="id783"/>into two categories: transparent (no code or change required) and non-transparent optimizers. Whether an accelerator is transparent or not largely depends on whether someone below us in the stack has made it transparent to us.</p>

<p>TCP/IP offloading is generally transparent at the user space level, which means the operating system takes care of it for us. NVMe drives are also generally transparent, generally appearing the same as spinning disks, except faster. It is still important to make Dask aware of transparent optimizers; for example, a disk-intensive workload should be scheduled on the machines with faster disks.</p>

<p>The non-transparent accelerators include GPUs, Optane, QAT, and many more. Using these requires changing our code to be able to take advantage of them. Sometimes this can be as simple as swapping in a different library, but not always. Many non-transparent accelerators require either copying our data or special formatting to be able to operate. This means that if an operation is relatively fast, moving to an optimizer could make it slower.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Understanding Whether GPUs or TPUs Can Help"><div class="sect1" id="id183">
<h1>Understanding Whether GPUs or TPUs Can Help</h1>

<p>Not every problem is a <a data-type="indexterm" data-primary="GPUs (graphics processing units)" data-secondary="benefits" id="id784"/><a data-type="indexterm" data-primary="TPUs (Tensor Processing Units)" data-secondary="benefits" id="id785"/>good fit for GPU acceleration. GPUs are especially good at performing the same calculation on a large number of data points at the same time. If a problem is well suited to vectorized computation, then there is a good chance that GPUs may be well suited to it.</p>

<p class="pagebreak-before">Some common problems that benefit from GPU acceleration include:</p>

<ul>
<li>
<p>Machine learning</p>
</li>
<li>
<p>Linear algebra</p>
</li>
<li>
<p>Physics simulations</p>
</li>
<li>
<p>Graphics (no surprise here)</p>
</li>
</ul>

<p>GPUs are not well suited to branch-heavy non-vectorized workflows, or workflows where the cost of copying the data is similar to or higher than the cost of the computation.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Making Dask Resource-Aware"><div class="sect1" id="id76">
<h1>Making Dask Resource-Aware</h1>

<p>If you have decided that your problem is well <a data-type="indexterm" data-primary="resource-awareness" id="rsrwr"/>suited to a specialized resource, the next step is to <a href="https://oreil.ly/EHFTr">make the scheduler aware of which machines and processes have the resource</a>. You can do this by adding either an environment variable or a command-line flag to the worker launch (e.g., <code>--resources "GPU=2"</code> or <code>DASK​_DIS⁠TRIBUTED__WORKER__RESOURCES__GPU=2</code>).</p>

<p>For NVIDIA users, the <code>dask-cuda</code> package <a data-type="indexterm" data-primary="dask-cuda package" id="id786"/>can launch one worker per GPU, pinning the GPU and thread together for performance. For example, on our Kubernetes cluster with GPU resources, we configure the workers to use the <code>dask-cuda-worker</code> launcher, as shown in <a data-type="xref" href="#ex_dask_cuda_k8s">Example 10-1</a>.</p>
<div id="ex_dask_cuda_k8s" data-type="example">
<h5><span class="label">Example 10-1. </span>Using the <code>dask-cuda-worker</code> package in the Dask Kubernetes template</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">worker_template</code> <code class="o">=</code> <code class="n">make_pod_spec</code><code class="p">(</code><code class="n">image</code><code class="o">=</code><code class="s1">'holdenk/dask:latest'</code><code class="p">,</code>
                                <code class="n">memory_limit</code><code class="o">=</code><code class="s1">'8G'</code><code class="p">,</code> <code class="n">memory_request</code><code class="o">=</code><code class="s1">'8G'</code><code class="p">,</code>
                                <code class="n">cpu_limit</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">cpu_request</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
<code class="n">worker_template</code><code class="o">.</code><code class="n">spec</code><code class="o">.</code><code class="n">containers</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">resources</code><code class="o">.</code><code class="n">limits</code><code class="p">[</code><code class="s2">"gpu"</code><code class="p">]</code> <code class="o">=</code> <code class="mi">1</code>
<code class="n">worker_template</code><code class="o">.</code><code class="n">spec</code><code class="o">.</code><code class="n">containers</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">resources</code><code class="o">.</code><code class="n">requests</code><code class="p">[</code><code class="s2">"gpu"</code><code class="p">]</code> <code class="o">=</code> <code class="mi">1</code>
<code class="n">worker_template</code><code class="o">.</code><code class="n">spec</code><code class="o">.</code><code class="n">containers</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">args</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code> <code class="o">=</code> <code class="s2">"dask-cuda-worker --resources 'GPU=1'"</code>
<code class="n">worker_template</code><code class="o">.</code><code class="n">spec</code><code class="o">.</code><code class="n">containers</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">env</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="s2">"NVIDIA_VISIBLE_DEVICES=ALL"</code><code class="p">)</code>
<code class="c1"># Or append --resources "GPU=2"</code></pre></div>

<p>Here you see we still add the <code>--resources</code> flag so that in a mixed environment we can select just the GPU workers.</p>

<p>If you’re using Dask to schedule work on multiple GPUs on a single computer (e.g., using Dask local mode with CUDA), the same <code>dask-cuda</code> package provides a <code>LocalCUDACluster</code>. As with <code>dask-cuda-worker</code>, you still need to add the resource tag manually, as shown in <a data-type="xref" href="#ex_dask_cuda_local">Example 10-2</a>, but <code>LocalCUDACluster</code> launches the correct workers and pins them to threads.</p>
<div id="ex_dask_cuda_local" data-type="example">
<h5><span class="label">Example 10-2. </span><code>LocalCUDACluster</code> with resource tagging</h5>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">dask_cuda</code> <code class="kn">import</code> <code class="n">LocalCUDACluster</code>
<code class="kn">from</code> <code class="nn">dask.distributed</code> <code class="kn">import</code> <code class="n">Client</code>
<code class="c1">#NOTE: The resources= flag is important; by default the </code>
<code class="c1"># LocalCUDACluster *does not* label any resources, which can make</code>
<code class="c1"># porting your code to a cluster where some workers have GPUs and </code>
<code class="c1"># some don't painful.</code>
<code class="n">cluster</code> <code class="o">=</code> <code class="n">LocalCUDACluster</code><code class="p">(</code><code class="n">resources</code><code class="o">=</code><code class="p">{</code><code class="s2">"GPU"</code><code class="p">:</code> <code class="mi">1</code><code class="p">})</code>
<code class="n">client</code> <code class="o">=</code> <code class="n">Client</code><code class="p">(</code><code class="n">cluster</code><code class="p">)</code></pre></div>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>For homogeneous clusters it may seem tempting to avoid labeling these resources, but unless you will always have a 1:1 mapping of worker process/thread to the accelerator (or the accelerator can be used by all workers at the same time), it is still beneficial to label these resources. This is important for non-shareable (or difficult-to-share) resources like GPUs/TPUs since Dask might schedule two tasks trying to access the GPU. But for shareable resources like NVMe drives, or TCP/IP offloading, if it’s present on every node in the cluster and will always be, you can probably skip it.</p>
</div>

<p>It’s important to note that Dask does not manage custom resources (including GPUs). If another process uses all of the GPU cores without asking Dask, there is no protection for this. In some ways, this is reminiscent of early computing, where we had “cooperative” multi-tasking; we depend on our neighbors being well behaved.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Dask depends on well-behaved Python code, which does not use resources it has not asked for and releases the resources when finished. This most commonly happens with memory leaks (both accelerated and not), often with specialized libraries like CUDA that allocate memory <a data-type="indexterm" data-primary="resource-awareness" data-startref="rsrwr" id="id787"/>outside of Python. These libraries often have special steps you need to call when you are done with the task you’ve asked to make the resources available for others.</p>
</div>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Installing the Libraries"><div class="sect1" id="id77">
<h1>Installing the Libraries</h1>

<p>Now that Dask is aware of the special resources on your cluster, it’s time to make sure that your code can take <a data-type="indexterm" data-primary="libraries" data-secondary="accelerator" id="lbrrcclr"/><a data-type="indexterm" data-primary="accelerator libraries" id="acclbrr"/>advantage of them. Often, but not always, these accelerators will require some kind of special library to be installed, which may involve long compile times. When possible, installing the acceleration libraries from conda and pre-installing on the workers (in the container or on the host) can help minimize this overhead.</p>

<p>For Kubernetes (or other Docker container users), you can do this by making a custom container with the accelerator libraries pre-installed, as seen in <a data-type="xref" href="#preinstall_gpu_docker">Example 10-3</a>.</p>
<div id="preinstall_gpu_docker" data-type="example">
<h5><span class="label">Example 10-3. </span>Pre-installing cuDF</h5>

<pre data-type="programlisting" data-code-language="python"><code class="c1"># Use the Dask base image; for arm64, though, we have to use custom built</code>
<code class="c1"># FROM ghcr.io/dask/dask</code>
<code class="n">FROM</code> <code class="n">holdenk</code><code class="o">/</code><code class="n">dask</code><code class="p">:</code><code class="n">latest</code>

<code class="c1"># arm64 channel</code>
<code class="n">RUN</code> <code class="n">conda</code> <code class="n">config</code> <code class="o">--</code><code class="n">add</code> <code class="n">channels</code> <code class="n">rpi</code>
<code class="c1"># Numba and conda-forge channels</code>
<code class="n">RUN</code> <code class="n">conda</code> <code class="n">config</code> <code class="o">--</code><code class="n">add</code> <code class="n">channels</code> <code class="n">numba</code>
<code class="n">RUN</code> <code class="n">conda</code> <code class="n">config</code> <code class="o">--</code><code class="n">add</code> <code class="n">channels</code> <code class="n">conda</code><code class="o">-</code><code class="n">forge</code>
<code class="c1"># Some CUDA-specific stuff</code>
<code class="n">RUN</code> <code class="n">conda</code> <code class="n">config</code> <code class="o">--</code><code class="n">add</code> <code class="n">channels</code> <code class="n">rapidsai</code>
<code class="c1"># Accelerator libraries often involve a lot of native code, so it's</code>
<code class="c1"># faster to install with conda</code>
<code class="n">RUN</code> <code class="n">conda</code> <code class="n">install</code> <code class="n">numba</code> <code class="o">-</code><code class="n">y</code>
<code class="c1"># GPU support (NV)</code>
<code class="n">RUN</code> <code class="n">conda</code> <code class="n">install</code> <code class="n">cudatoolkit</code> <code class="o">-</code><code class="n">y</code>
<code class="c1"># GPU support (AMD)</code>
<code class="n">RUN</code> <code class="n">conda</code> <code class="n">install</code> <code class="n">roctools</code> <code class="o">-</code><code class="n">y</code> <code class="o">||</code> <code class="n">echo</code> <code class="s2">"No roc tools on $(uname -a)"</code>
<code class="c1"># A lot of GPU acceleration libraries are in the rapidsai channel</code>
<code class="c1"># These are not installable with pip</code>
<code class="n">RUN</code> <code class="n">conda</code> <code class="n">install</code> <code class="n">cudf</code> <code class="o">-</code><code class="n">y</code></pre></div>

<p>Then, to build this, we run the script <a data-type="indexterm" data-primary="libraries" data-secondary="accelerator" data-startref="lbrrcclr" id="id788"/><a data-type="indexterm" data-primary="accelerator libraries" data-startref="acclbrr" id="id789"/>shown in <a data-type="xref" href="#build_custom_ch10_1686240447279">Example 10-4</a>.</p>
<div id="build_custom_ch10_1686240447279" data-type="example">
<h5><span class="label">Example 10-4. </span>Building custom Dask Docker containers</h5>

<pre data-type="programlisting" data-code-language="bash"><code class="c1">#/bin/bash</code>
<code class="nb">set</code><code class="w"> </code>-ex<code class="w"/>

docker<code class="w"> </code>buildx<code class="w"> </code>build<code class="w"> </code>-t<code class="w"> </code>holdenk/dask-extended<code class="w">  </code>--platform<code class="w"> </code><code class="se">\</code>
<code class="w">  </code>linux/arm64,linux/amd64<code class="w"> </code>--push<code class="w"> </code>.<code class="w"> </code>-f<code class="w"> </code>Dockerfile<code class="w"/>
docker<code class="w"> </code>buildx<code class="w"> </code>build<code class="w"> </code>-t<code class="w"> </code>holdenk/dask-extended-notebook<code class="w">  </code>--platform<code class="w"> </code><code class="se">\</code>
<code class="w">  </code>linux/arm64,linux/amd64<code class="w"> </code>--push<code class="w"> </code>.<code class="w"> </code>-f<code class="w"> </code>NotebookDockerfile<code class="w"/></pre></div>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Using Custom Resources Inside Your Dask Tasks"><div class="sect1" id="id184">
<h1>Using Custom Resources Inside Your Dask Tasks</h1>

<p>It is important that you make sure your tasks <a data-type="indexterm" data-primary="tasks" data-secondary="resources, custom" id="tsksrc"/>that need accelerators run on worker processes with the accelerator available. You can ask for special resources when scheduling tasks with Dask, either explicitly in <code>client.submit</code>, as seen in <a data-type="xref" href="#ex_submit_gpu">Example 10-5</a>, or by adding an annotation to your existing code, as seen in <a data-type="xref" href="#ex_annotate_gpu">Example 10-6</a>.</p>
<div id="ex_submit_gpu" data-type="example">
<h5><span class="label">Example 10-5. </span>Submitting a task asking for a GPU</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">future</code> <code class="o">=</code> <code class="n">client</code><code class="o">.</code><code class="n">submit</code><code class="p">(</code><code class="n">how_many_gpus</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="n">resources</code><code class="o">=</code><code class="p">{</code><code class="s1">'GPU'</code><code class="p">:</code> <code class="mi">1</code><code class="p">})</code></pre></div>
<div id="ex_annotate_gpu" data-type="example">
<h5><span class="label">Example 10-6. </span>Annotating a group of operations as needing a GPU</h5>

<pre data-type="programlisting" data-code-language="python"><code class="k">with</code> <code class="n">dask</code><code class="o">.</code><code class="n">annotate</code><code class="p">(</code><code class="n">resources</code><code class="o">=</code><code class="p">{</code><code class="s1">'GPU'</code><code class="p">:</code> <code class="mi">1</code><code class="p">}):</code>
    <code class="n">future</code> <code class="o">=</code> <code class="n">client</code><code class="o">.</code><code class="n">submit</code><code class="p">(</code><code class="n">how_many_gpus</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code></pre></div>

<p>If you move from a cluster with GPU resources to a cluster without, this code will hang indefinitely. The CPU Fallback design pattern covered later can mitigate this.</p>








<section data-type="sect2" data-pdf-bookmark="Decorators (Including Numba)"><div class="sect2" id="id78">
<h2>Decorators (Including Numba)</h2>

<p>Numba is a popular high-performance JIT compilation <a data-type="indexterm" data-primary="Numba" id="id790"/><a data-type="indexterm" data-primary="decorators" id="id791"/>library for Python, which also has support for various accelerators. Most JIT code, as well as many decorator functions, is generally not directly serializable, so attempting to directly Numba it with <code>dask.submit</code>, as seen in <a data-type="xref" href="#ex_dask_submit_numba_incorrect">Example 10-7</a>, does not work. Instead, the correct way is to wrap the function, as shown in <a data-type="xref" href="#ex_dask_submit_numba_correct">Example 10-8</a>.</p>
<div id="ex_dask_submit_numba_incorrect" data-type="example">
<h5><span class="label">Example 10-7. </span>Decorator difficulty</h5>

<pre data-type="programlisting" data-code-language="python"><code class="c1"># Works in local mode, but not distributed</code>
<code class="nd">@dask</code><code class="o">.</code><code class="n">delayed</code>
<code class="nd">@guvectorize</code><code class="p">([</code><code class="s1">'void(float64[:], intp[:], float64[:])'</code><code class="p">],</code>
             <code class="s1">'(n),()-&gt;(n)'</code><code class="p">)</code>
<code class="k">def</code> <code class="nf">delayed_move_mean</code><code class="p">(</code><code class="n">a</code><code class="p">,</code> <code class="n">window_arr</code><code class="p">,</code> <code class="n">out</code><code class="p">):</code>
    <code class="n">window_width</code> <code class="o">=</code> <code class="n">window_arr</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>
    <code class="n">asum</code> <code class="o">=</code> <code class="mf">0.0</code>
    <code class="n">count</code> <code class="o">=</code> <code class="mi">0</code>
    <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">window_width</code><code class="p">):</code>
        <code class="n">asum</code> <code class="o">+=</code> <code class="n">a</code><code class="p">[</code><code class="n">i</code><code class="p">]</code>
        <code class="n">count</code> <code class="o">+=</code> <code class="mi">1</code>
        <code class="n">out</code><code class="p">[</code><code class="n">i</code><code class="p">]</code> <code class="o">=</code> <code class="n">asum</code> <code class="o">/</code> <code class="n">count</code>
    <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">window_width</code><code class="p">,</code> <code class="nb">len</code><code class="p">(</code><code class="n">a</code><code class="p">)):</code>
        <code class="n">asum</code> <code class="o">+=</code> <code class="n">a</code><code class="p">[</code><code class="n">i</code><code class="p">]</code> <code class="o">-</code> <code class="n">a</code><code class="p">[</code><code class="n">i</code> <code class="o">-</code> <code class="n">window_width</code><code class="p">]</code>
        <code class="n">out</code><code class="p">[</code><code class="n">i</code><code class="p">]</code> <code class="o">=</code> <code class="n">asum</code> <code class="o">/</code> <code class="n">count</code>


<code class="n">arr</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">arange</code><code class="p">(</code><code class="mi">20</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="n">np</code><code class="o">.</code><code class="n">float64</code><code class="p">)</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">10</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">arr</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">dask</code><code class="o">.</code><code class="n">compute</code><code class="p">(</code><code class="n">delayed_move_mean</code><code class="p">(</code><code class="n">arr</code><code class="p">,</code> <code class="mi">3</code><code class="p">)))</code></pre></div>
<div id="ex_dask_submit_numba_correct" data-type="example">
<h5><span class="label">Example 10-8. </span>Decorator hack</h5>

<pre data-type="programlisting" data-code-language="python"><code class="nd">@guvectorize</code><code class="p">([</code><code class="s1">'void(float64[:], intp[:], float64[:])'</code><code class="p">],</code>
             <code class="s1">'(n),()-&gt;(n)'</code><code class="p">)</code>
<code class="k">def</code> <code class="nf">move_mean</code><code class="p">(</code><code class="n">a</code><code class="p">,</code> <code class="n">window_arr</code><code class="p">,</code> <code class="n">out</code><code class="p">):</code>
    <code class="n">window_width</code> <code class="o">=</code> <code class="n">window_arr</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>
    <code class="n">asum</code> <code class="o">=</code> <code class="mf">0.0</code>
    <code class="n">count</code> <code class="o">=</code> <code class="mi">0</code>
    <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">window_width</code><code class="p">):</code>
        <code class="n">asum</code> <code class="o">+=</code> <code class="n">a</code><code class="p">[</code><code class="n">i</code><code class="p">]</code>
        <code class="n">count</code> <code class="o">+=</code> <code class="mi">1</code>
        <code class="n">out</code><code class="p">[</code><code class="n">i</code><code class="p">]</code> <code class="o">=</code> <code class="n">asum</code> <code class="o">/</code> <code class="n">count</code>
    <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">window_width</code><code class="p">,</code> <code class="nb">len</code><code class="p">(</code><code class="n">a</code><code class="p">)):</code>
        <code class="n">asum</code> <code class="o">+=</code> <code class="n">a</code><code class="p">[</code><code class="n">i</code><code class="p">]</code> <code class="o">-</code> <code class="n">a</code><code class="p">[</code><code class="n">i</code> <code class="o">-</code> <code class="n">window_width</code><code class="p">]</code>
        <code class="n">out</code><code class="p">[</code><code class="n">i</code><code class="p">]</code> <code class="o">=</code> <code class="n">asum</code> <code class="o">/</code> <code class="n">count</code>


<code class="n">arr</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">arange</code><code class="p">(</code><code class="mi">20</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="n">np</code><code class="o">.</code><code class="n">float64</code><code class="p">)</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">10</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">arr</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">move_mean</code><code class="p">(</code><code class="n">arr</code><code class="p">,</code> <code class="mi">3</code><code class="p">))</code>


<code class="k">def</code> <code class="nf">wrapped_move_mean</code><code class="p">(</code><code class="o">*</code><code class="n">args</code><code class="p">):</code>
    <code class="k">return</code> <code class="n">move_mean</code><code class="p">(</code><code class="o">*</code><code class="n">args</code><code class="p">)</code>


<code class="n">a</code> <code class="o">=</code> <code class="n">dask</code><code class="o">.</code><code class="n">delayed</code><code class="p">(</code><code class="n">wrapped_move_mean</code><code class="p">)(</code><code class="n">arr</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code></pre></div>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p><a data-type="xref" href="#ex_dask_submit_numba_incorrect">Example 10-7</a> will work in local mode—but not when you go to scale.</p>
</div>
</div></section>








<section data-type="sect2" data-pdf-bookmark="GPUs"><div class="sect2" id="id79">
<h2>GPUs</h2>

<p>Like most tasks in Python, there are <a data-type="indexterm" data-primary="tasks" data-secondary="GPUs (graphics processing units)" id="id792"/><a data-type="indexterm" data-primary="GPUs (graphics processing units)" data-secondary="tasks" id="id793"/>many different libraries for working with GPUs. Many of these libraries support NVIDIA’s Compute Unified Device Architecture (CUDA) with <a data-type="indexterm" data-primary="CUDA (Compute Unified Device Architecture)" id="id794"/><a data-type="indexterm" data-primary="Compute Unified Device Architecture (CUDA)" id="id795"/><a data-type="indexterm" data-primary="HIP/Radeon Open Compute module (ROCm)" id="id796"/><a data-type="indexterm" data-primary="Radeon Open Compute module (ROCm)" id="id797"/><a data-type="indexterm" data-primary="ROCm (Radeon Open Compute module)" id="id798"/>experimental support for AMD’s new open HIP/Radeon Open Compute module (ROCm) interfaces. NVIDIA and CUDA were the first on the scene and have a much larger adoption than AMD’s Radeon Open Compute module—so much so that ROCm has a large focus on supporting ports of CUDA software to the ROCm platform.</p>

<p>We won’t dive deep into the world of <a data-type="indexterm" data-primary="Python" data-secondary="GPU libraries" id="id799"/>Python GPU libraries, but you may want to check out <a href="https://oreil.ly/i-FVO">Numba for GPUs</a>, <a href="https://oreil.ly/vChSG">TensorFlow GPU support</a>, and <a href="https://oreil.ly/sdLjo">PyTorch’s GPU support</a>.</p>

<p>Most of the libraries that have some form of GPU support require compiling large amounts of non-Python code. As such, it’s often best to install these libraries with conda, which frequently <a data-type="indexterm" data-primary="tasks" data-secondary="resources, custom" data-startref="tsksrc" id="id800"/>has more complete binary packaging, allowing you to skip the compile step.</p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="GPU Acceleration Built on Top of Dask"><div class="sect1" id="id272">
<h1>GPU Acceleration Built on Top of Dask</h1>

<p>The three main CUDA libraries extending Dask are cuDF (previously called dask-cudf), BlazingSQL, and cuML.<sup><a data-type="noteref" id="id801-marker" href="ch10.xhtml#id801">2</a></sup> Currently these libraries are focused on NVIDIA GPUs.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Dask does not currently have any libraries powering integrations with OpenCL or HIP. This does not preclude you in any way from using GPUs with libraries that support them, like TensorFlow, as previously illustrated.</p>
</div>








<section data-type="sect2" data-pdf-bookmark="cuDF"><div class="sect2" id="id80">
<h2>cuDF</h2>

<p><a href="https://oreil.ly/BZ9x2">cuDF</a> is a GPU-accelerated <a data-type="indexterm" data-primary="cuDF" id="id802"/><a data-type="indexterm" data-primary="libraries" data-secondary="cuDF" id="id803"/>version of Dask’s DataFrame library. Some <a href="https://oreil.ly/unpvl">benchmarking shows performance speed-ups of 7x~50x</a>. Not all DataFrame operations will have this same speed-up. For example, if you are operating row-by-row instead of in vectorized type operations, you may experience slower performance when using cuDF over Dask’s DataFrame library. cuDF supports most of the common data types you are likely to use, but not all.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Under the hood, cuDF frequently delegates work to the cuPY library, but since it is created by NVIDIA employees and their focus is on supporting NVIDIA hardware, cuDF does not have direct support for ROCm.</p>
</div>
</div></section>








<section data-type="sect2" data-pdf-bookmark="BlazingSQL"><div class="sect2" id="id81">
<h2>BlazingSQL</h2>

<p>BlazingSQL uses GPU acceleration to <a data-type="indexterm" data-primary="BlazingSQL" id="id804"/><a data-type="indexterm" data-primary="SQL (Structured Query Language)" data-secondary="BlazingSQL" id="id805"/>provide super-fast SQL queries. BlazingSQL operates on top of cuDF.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>While BlazingSQL is a wonderful tool, much of its documentation is broken. For example, at the time of this writing, none of the examples linked in the main README resolve correctly, and the documentation site is entirely offline.</p>
</div>
</div></section>








<section data-type="sect2" class="pagebreak-before less_space" data-pdf-bookmark="cuStreamz"><div class="sect2" id="id82">
<h2>cuStreamz</h2>

<p>Another GPU-accelerated library for <a data-type="indexterm" data-primary="libraries" data-secondary="cuStreamz" id="id806"/><a data-type="indexterm" data-primary="cuStreamz" id="id807"/>streaming on GPUs is cuStreamz, which is basically a combination of Dask streaming and cuDF; we cover it more in <a data-type="xref" href="app04.xhtml#appD">Appendix D</a>.</p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Freeing Accelerator Resources"><div class="sect1" id="id185">
<h1>Freeing Accelerator Resources</h1>

<p>Allocating memory on GPUs tends to be slow, so <a data-type="indexterm" data-primary="accelerators" data-secondary="freeing resources" id="id808"/>many libraries hold on to these resources. In most situations, if the Python VM exits, the resources will be cleared up. An option of last resort is to bounce all of the workers using <code>client.restart</code>. When possible, you will be best served by manually managing resources—which is library-dependent. For example, cuPY users can free the blocks once used by calling <code>free_all_blocks()</code>, as per the 
<span class="kturl"><a href="https://oreil.ly/hpxkg">memory management documentation</a></span>.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Design Patterns: CPU Fallback"><div class="sect1" id="id83">
<h1>Design Patterns: CPU Fallback</h1>

<p>CPU Fallback refers to attempting to use <a data-type="indexterm" data-primary="CPU Fallback" id="id809"/>an accelerator, like GPU or TPU, and falling back to the regular CPU code path if the accelerator is unavailable. In most cases, this is a good design pattern to follow, as accelerators (like GPUs) can be expensive and may not always be available. However, in some cases, the difference between CPU and GPU performance is so large that falling back to the CPU is unlikely to be able to succeed in a practical amount of time; this occurs most often with deep learning algorithms.</p>

<p>Object-oriented programming and duck-typing are somewhat well suited to this design pattern, since, provided that two classes implement the same parts of the interface you are using, you can swap them around. However, much like swapping in Dask DataFrames for pandas DataFrames, it is imperfect, especially when it comes to performance.</p>
<div data-type="warning" epub:type="warning" class="pagebreak-after"><h6>Warning</h6>
<p>In a better world, we could submit a task requesting GPU resources, and if that does not get scheduled, we could switch back to CPU-only resources. Unfortunately, Dask’s resources scheduling is closer to “best effort,”<sup><a data-type="noteref" id="id810-marker" href="ch10.xhtml#id810">3</a></sup> so we may be scheduled on nodes without the resources we request.</p>
</div>
</div></section>






<section data-type="sect1" class="less_space" data-pdf-bookmark="Conclusion"><div class="sect1" id="id273">
<h1>Conclusion</h1>

<p>Specialized accelerators, like GPUs, can make large differences in your workflows. Picking the right accelerator for your workflow is important, and some workflows are not well suited to acceleration. Dask does not automate the usage of any accelerators, but there are various libraries that you can use for GPU computation. Many of these libraries were not created with the idea of shared computation in mind, so it’s important to be on the lookout for accidental resource leaks, especially since GPU resources tend to be more expensive.</p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="id778"><sup><a href="ch10.xhtml#id778-marker">1</a></sup> Assuming Shakespeare were still alive, which he is not.</p><p data-type="footnote" id="id801"><sup><a href="ch10.xhtml#id801-marker">2</a></sup> BlazingSQL may be at the end of its life; there has not been a commit for an extended period of time, and the website is just a hard hat, like those 1990s GeoCities websites.</p><p data-type="footnote" id="id810"><sup><a href="ch10.xhtml#id810-marker">3</a></sup> This is not as <a href="https://oreil.ly/p1Ldf">documented</a>, and so may change in the future.</p></div></div></section></div></body></html>