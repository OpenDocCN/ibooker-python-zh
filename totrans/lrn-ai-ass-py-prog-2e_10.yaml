- en: 11 Creating an authorship identification program
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Writing an authorship identification program using top-down design
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning about refactoring code and why you would do it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In chapter 7, we learned about problem decomposition and top-down design when
    we wrote our Spelling Suggestions program. Here, we’re going to take top-down
    design to the next level and solve a much larger problem. We’re still doing the
    same thing as in chapter 7: dividing a problem into subproblems, and further dividing
    those subproblems into sub-subproblems as needed. And, just like before, we’re
    looking to design functions with a small number of parameters that return a meaningful
    and useful result to their caller. It’s also a good sign if we’re able to design
    functions that are called by multiple other functions—that helps reduce code repetition!'
  prefs: []
  type: TYPE_NORMAL
- en: We’re including this chapter because we wanted to provide a more authentic example
    than the Spelling Suggestions problem we solved in chapter 7\. We hope our example
    here is motivating and feels like a real problem that you could imagine yourself
    wanting to solve.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’re going to write a program that tries to identify the unknown
    author of a mystery book. It’ll be an example of a program that uses artificial
    intelligence (AI) to make a prediction. We couldn’t resist the opportunity to
    include an AI example in a book about programming with AI!
  prefs: []
  type: TYPE_NORMAL
- en: 11.1 Authorship identification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This problem is based on an assignment created by our colleague Michelle Craig
    [1]. Let’s start by taking a look at these two book excerpts:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Excerpt 1*—I have not yet described to you the most singular part. About six
    years ago—to be exact, upon the 4^(th) of May 1882—an advertisement appeared in
    the Times asking for the address of Miss Mary Morstan and stating that it would
    be to her advantage to come forward. There was no name or address appended. I
    had at that time just entered the family of Mrs. Cecil Forrester in the capacity
    of governess. By her advice I published my address in the advertisement column.
    The same day there arrived through the post a small card-board box addressed to
    me, which I found to contain a very large and lustrous pearl. No word of writing
    was enclosed. Since then, every year upon the same date there has always appeared
    a similar box, containing a similar pearl, without any clue as to the sender.
    They have been pronounced by an expert to be of a rare variety and of considerable
    value. You can see for yourselves that they are very handsome.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Excerpt 2*—It was the Dover Road that lay on a Friday night late in November,
    before the first of the persons with whom this history has business. The Dover
    Road lay, as to him, beyond the Dover mail, as it lumbered up Shooter’s Hill.
    He walked up hill in the mire by the side of the mail, as the rest of the passengers
    did; not because they had the least relish for walking exercise, under the circumstances,
    but because the hill, and the harness, and the mud, and the mail, were all so
    heavy, that the horses had three times already come to a stop, besides once drawing
    the coach across the road, with the mutinous intent of taking it back to Blackheath.
    Reins and whip and coachman and guard, however, in combination, had read that
    article of war which forbade a purpose otherwise strongly in favour of the argument,
    that some brute animals are endued with Reason; and the team had capitulated and
    returned to their duty.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suppose we asked you whether these two excerpts were likely written by the same
    author. One reasonable assumption you might make is that different authors write
    differently, and that these differences would show up in metrics that we could
    calculate about their texts.
  prefs: []
  type: TYPE_NORMAL
- en: For example, whoever wrote the first excerpt seems to use quite a few short
    sentences in terms of number of words compared to the second excerpt. We find
    short sentences like “There was no name or address appended” and “No word of writing
    was enclosed” in the first excerpt; those kinds of sentences are absent from the
    second. Similarly, the sentences from the first excerpt seem to be less complex
    than those in the second; look at all of those commas and semicolons in the second
    excerpt.
  prefs: []
  type: TYPE_NORMAL
- en: This analysis may lead you to believe that these texts are written by different
    authors, and, indeed, they are. The first is written by Sir Arthur Conan Doyle,
    and the second by Charles Dickens.
  prefs: []
  type: TYPE_NORMAL
- en: To be fair, we’ve absolutely cherry-picked these two excerpts. Doyle does use
    some long, complex sentences. Dickens does use some short ones. But, on average,
    at least for the two books that we took these excerpts from, Doyle does write
    shorter sentences than Dickens. More generally, if we look at two books written
    by different authors, we might expect to find some quantifiable differences on
    average.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that we have a bunch of books whose authors we know. We have one written
    by Doyle, one written by Dickens, and so on. Then, along comes a mystery book.
    Oh no! We don’t know who wrote it! Is it a lost Sherlock Holmes story from Doyle?
    A lost *Oliver Twist* sequel from Dickens? We want to find out who that unknown
    author is, and to do that, we’ll turn to a basic AI technique.
  prefs: []
  type: TYPE_NORMAL
- en: Our strategy will be to come up with a *signature* for each author, using one
    of the books we know they wrote. We’ll refer to these signatures as *known signatures*.
    Each of these signatures will capture metrics about the book text, such as the
    average number of words per sentence and the average sentence complexity. Then,
    we’ll come up with a signature for the mystery book with an unknown author. We’ll
    call this the *unknown signature*. We’ll look through all the known signatures,
    comparing each one to our unknown signature. We’ll use whichever is the closest
    as our guess for the unknown author.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, we have no idea if the unknown author is really one of the authors
    whose signatures we have. It could be a completely new author, for example. Even
    if the unknown author *is* one of the authors whose signature we have, we still
    might end up guessing wrong. After all, maybe the same author writes books in
    different styles (giving their books very different signatures), or we simply
    fail to capture what is most salient about how each of our authors writes. Indeed,
    we’re not after an industry-strength author identification program in this chapter.
    Still, considering the difficulty of this task, we think it’s impressive how well
    the approach that we’ll show you here works.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Authorship identification, as we’re doing here, is a *machine learning*(ML)
    task. ML is a branch of AI designed to help computers “learn” from data in order
    to make predictions. There are various forms of ML; the one we’re using here is
    called supervised learning. In supervised learning, we have access to training
    data, which consists of objects and their known categories (or labels). In our
    case, our objects are book texts, and the category for each book is the author
    who wrote it. We can train (i.e., learn) on the training set by calculating features—average
    number of words per sentence, average sentence complexity, and so on—for each
    book. Later, when we’re provided a book whose author we don’t know, we can use
    what we learned in the training to make our prediction (or guess).
  prefs: []
  type: TYPE_NORMAL
- en: 11.2 Authorship identification using top-down design
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Alright, we want to “write a program to determine the author of a book.” This
    seems like a daunting task, and it would be if we were trying to do this in one
    shot, using a single function. But just like in our Spelling Suggestions example
    in chapter 7, we’re not going to do that. We’re going to systematically break
    this problem down into subproblems that we can solve.
  prefs: []
  type: TYPE_NORMAL
- en: 'In chapter 7, we solved the Spelling Suggestions problem by using the model
    of reading input, processing that input, and producing an output result. We can
    think about our authorship identification program as following that model as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Input step* —For the input step, we need to ask the user for the filename
    of the mystery book.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Process step* —For the process step, we need to figure out the signature for
    the mystery book (that’s the unknown signature), as well as the signature for
    each of the books whose author we know (those are the known signatures). Creating
    the signature for each book is commonly called the training phase in ML. We also
    need to compare the unknown signature to each known signature to figure out which
    known signature is closest. These comparisons are the prediction phase in ML.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Output step* —For the output step, we need to report to the user the unknown
    signature that’s closest to the known signature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That is, to solve our overall authorship identification problem, we need to
    solve these three subproblems. We’re starting our top-down design!
  prefs: []
  type: TYPE_NORMAL
- en: We’ll name our top-level function `make_guess`. In it, we’ll solve each of the
    three subproblems we identified.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the input step, we’re simply asking the user for a filename. That’s something
    we can do in a small number of lines of code, so we probably don’t need a separate
    function for that. The output step seems similar: assuming that we already know
    which known signature is closest, we can just report that to the user. By contrast,
    the process step looks like a lot of work, and we’ll certainly want to break that
    subproblem down further. That’s what we’ll do next.'
  prefs: []
  type: TYPE_NORMAL
- en: 11.3 Breaking down the process subproblem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ll name our overall process function `process_data`. It will take the mystery
    book filename and the name of a directory of known-author books as parameters
    and return the name of the closest known signature.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at our description for the process step, it seems that we have three
    subproblems to solve here:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure out the signature for the mystery book.* That’s our unknown signature.
    We’ll name this function `make_signature`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure out the signature for each of the books whose author we know.* These
    are our known signatures. We’ll name this function `get_all_signatures`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Compare the unknown signature to each known signature to figure out which
    known signature is closest.* Because close signatures will have small differences,
    we’ll name this function `lowest_score`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ll work on our top-down design for each of these subproblems in turn. Figure
    11.1 shows a diagram of what we have so far.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/11-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 Functions diagram with the three subtasks of `process_data`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 11.3.1 Figuring out the signature for the mystery book
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The function for this task, `make_signature`, will take the text for our book
    as a parameter and return the book’s signature. At this point, we need to decide
    on the features that we’ll use to determine the signature for each book. Let’s
    break this down by thinking back to the previous example passages. We noticed
    there are differences in the authors’ passages based on the complexity and length
    of sentences. You may have also suspected that the authors may differ in the length
    of words used and ways they use words (e.g., some authors may be more repetitive
    than others). As such, we’ll want some features to be based on the structure of
    the author’s sentences, and we’ll want others based on the words that the author
    uses. We’ll look at each of these in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Features related to the structure of the author’s sentences
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In our earlier Doyle versus Dickens example, we talked about using the average
    number of words per sentence as one feature. We can calculate this by dividing
    the total number of words by the total number of sentences. For example, consider
    the following text:'
  prefs: []
  type: TYPE_NORMAL
- en: The same day there arrived through the post a small card-board box addressed
    to me, which I found to contain a very large and lustrous pearl. No word of writing
    was enclosed.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you count the words and sentences, you should find that there are 32 words
    (card-board counts as one word) and two sentences, so we’ll calculate the average
    words per sentence as 32/2 = 16\. This will be the*average number of words per
    sentence*feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also noticed that the complexity of sentences may vary between authors (i.e.,
    some authors have sentences with many more commas and semicolons compared to other
    authors), so it makes sense to use that as another feature. More complex sentences
    have more phrases, which are coherent pieces of sentences. Breaking a sentence
    into its component phrases is a tough challenge in its own right, and although
    we could try to do it more accurately, we’ll settle here for a simpler rule of
    thumb. Namely, we’ll say that a phrase is separated from other phrases in the
    sentence by a comma, semicolon, or colon. Looking at the previous text again,
    we find that there are three phrases. The first sentence has two phrases: “The
    same day there arrived through the post a small card-board box addressed to me”
    and “which I found to contain a very large and lustrous pearl.” The second sentence
    has no commas, semicolons, or colons, so it has only one phrase. As there are
    three phrases and two sentences, we’d say that the sentence complexity for this
    text is 3/2 = 1.5\. This will be the *average sentence complexity* feature.'
  prefs: []
  type: TYPE_NORMAL
- en: We hope that these sentence-level features intuitively make sense as things
    we could use to differentiate how authors write. Next, let’s start looking at
    the ways that authors may differ in their use of words.
  prefs: []
  type: TYPE_NORMAL
- en: Features related to the author’s word selection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You can probably think of your own metrics for word-level features, but we’ll
    use three here that in our experience work well. First, it’s likely that some
    authors use shorter words on average than other authors. To that end, we’ll use
    the average word length, which is just the average number of letters per word.
    Let’s consider this sample text that we created:'
  prefs: []
  type: TYPE_NORMAL
- en: A pearl! Pearl! Lustrous pearl! Rare. What a nice find.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you count the letters and words, you should find that there are 41 letters
    and 10 words. (Don’t count punctuation as letters here.) So, we’ll calculate the
    average word length as 41/10 = 4.1\. This will be the *average word length* feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, it may be that some authors use words more repetitively than others.
    To capture this, we’ll take the number of different words that the author uses
    and divide it by the total number of words. For our previous sample text, there
    are only seven different words used: *a*, *pearl*, *lustrous*, *rare*, *what*,
    *nice*, and *find*. There are 10 words in all, so our calculation for this metric
    would be 7/10 = 0.7\. This will be the*different words divided by total words*feature.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Third, it may be that some authors tend to use many words a single time, whereas
    other authors tend to use words multiple times. To calculate this one, we’ll take
    the number of words used exactly once and divide it by the total number of words.
    For our sample text, there are five words that are used exactly once: *lustrous*,
    *rare*, *what*, *nice*, and *find*. There are 10 words in all, so our calculation
    for this metric would be 5/10 = 0.5\. This will be the *number of words used exactly
    once divided by total words*feature.'
  prefs: []
  type: TYPE_NORMAL
- en: In all, we have five features that will make up each signature. We’ll need to
    store those numbers together in a single value, so we’ll end up using a list of
    five numbers for each signature.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s dig into how we’ll implement each of these features, starting with the
    word-level ones and proceeding to the sentence-level ones. We’ll go in this order:'
  prefs: []
  type: TYPE_NORMAL
- en: Average word length
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different words divided by total words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Words used exactly once divided by total words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Average number of words per sentence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Average sentence complexity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each one, we’ll ultimately end up writing a function. We have an updated
    diagram with function names for each of these five new functions that will help
    us implement `make_signature` in figure 11.2\. Do we need to further break down
    these problems, or are they OK as is? Let’s see!
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/11-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 Functions diagram with the additional five subtasks of `make_signature`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Average word length
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The function for this task, `average_word_length`, will take the text of the
    book as a parameter and return the average word length. We might start solving
    this task by using the split method on the text. As a reminder, the `split` method
    is used to split a string into a list of its pieces. By default, `split` will
    split around spaces. The book text is a string, and if we split around spaces,
    we’ll get its words! That’s exactly what we need here. We can then loop through
    that list of words, counting up the number of letters and number of words.
  prefs: []
  type: TYPE_NORMAL
- en: That’s a good start, but we need to be a little more careful here because we
    don’t want to end up counting nonletters as letters. For example, “pearl” has
    five letters. But so does “pearl.” or “pearl!!” or “(pearl)”. Aha—this sounds
    like a subtask to us! Namely, we can divide out the subtask of cleaning up a word
    into its own function to be used by `average_word_length`. We’ll call that cleanup
    function `clean_word`.
  prefs: []
  type: TYPE_NORMAL
- en: There’s another benefit to having our `clean_word` function, and that’s to help
    us identify when a “word” is actually not a word. For example, suppose one of
    our “words” in the text is . . . . When we pass this to `clean_word`, we’ll get
    an empty string back. That signifies that this in fact isn’t a word at all, so
    we won’t count it as such.
  prefs: []
  type: TYPE_NORMAL
- en: Different words divided by total words
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The function for this task, `different_to_total`, will take the text of the
    book as a parameter and will return the number of different words used divided
    by the total number of words.
  prefs: []
  type: TYPE_NORMAL
- en: As with `average_word_length`, we need to be careful to count only letters,
    not punctuation. But wait—we just talked about a `clean_word` function that we
    needed for `average_word_length`. We can use that function here as well! In fact,
    we’ll use `clean_ word` in most of our five feature tasks. This is the sign of
    a useful general-purpose function! Our top-down design is going well. We can see
    how the `clean_word` function will be called by both functions in our updated
    function diagram in figure 11.3.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/11-3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3 Functions diagram with two functions, both using our `clean_word`
    function to help
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: There’s one extra complication here, though, and it involves words like *pearl*,
    *Pearl*, and *PEARL*. We want to consider those to be the same words, but if we
    simply use string comparisons, they will be treated as different words. One solution
    here is to split this off into another subproblem to convert a string to all lowercase.
    We could also think of this as another part of cleaning up a word, right along
    with removing the punctuation. We’ll go with this second option. What we’ll do,
    then, is make our `clean_word` function not only remove punctuation but also convert
    the word to lowercase.
  prefs: []
  type: TYPE_NORMAL
- en: You might wonder whether we need to split off another subtask here, one that
    determines the number of different words. You could do that, and it wouldn’t be
    a mistake to do so. However, if we persevere without doing that, we’ll see that
    the function remains quite manageable without this additional subtask. Practice
    and experience over time will help you anticipate when a task needs to be further
    broken down.
  prefs: []
  type: TYPE_NORMAL
- en: Words used exactly once divided by total words
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The function for this task, `exactly_once_to_total`, will take the text of
    the book as a parameter and will return the number of words used exactly once
    divided by the total number of words. We’re going to need our `clean_word` function
    here as well for reasons similar to why we needed it in the prior two tasks: to
    make sure we’re working only with letters, not punctuation. Again, while we could
    split out a subtask to determine the number of words that are used exactly once,
    we’ll find that it doesn’t take much Python code to do this, so we’ll just leave
    this task alone without splitting it further.'
  prefs: []
  type: TYPE_NORMAL
- en: Average number of words per sentence
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The function for this task, `average_sentence_length`, will take the text of
    the book as a parameter and will return the average number of words per sentence.
    To split our text into words for the previous three tasks, we can use the string
    split method. How do we split our text into sentences? Is there a string method
    for that?
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, there isn’t. For that reason, it will be helpful to split out
    a task to break our text string into sentences. We’ll call the function for that
    subtask `get_ sentences`. The `get_sentences` function will take the text of the
    book as a parameter and will return a list of sentences from the text.
  prefs: []
  type: TYPE_NORMAL
- en: What’s a sentence? We’ll define a sentence as text that is separated by a period
    (.), question mark (?), or exclamation point (!). This rule, while convenient
    and simple, is going to make mistakes. For example, how many sentences are in
    this text?
  prefs: []
  type: TYPE_NORMAL
- en: I had at that time just entered the family of Mrs. Cecil Forrester in the capacity
    of governess.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The answer is one. Our program, though, is going to pull out two sentences,
    not one. It’ll get tricked by the word *Mrs.*, which has a period at the end.
    If you continue with authorship identification past this chapter, you could work
    on making your rules more robust or, use sophisticated natural language processing
    (NLP) software to do even better. For our purposes, however, we’ll be content
    with this rule that sometimes gets sentences wrong because most of the time we’ll
    get them right. If we’re only wrong once in a while, the errors won’t have an
    appreciable effect on our metric.
  prefs: []
  type: TYPE_NORMAL
- en: Average sentence complexity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We’ll name the function for this task `average_sentence_complexity`. It will
    take the text of a sentence as a parameter and return a measure of the sentence
    complexity.
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed previously, we’re interested in quantifying sentence complexity
    using the number of phrases in a sentence. Much as we used punctuation to separate
    sentences from each other, we’ll use different punctuation to separate phrases
    from each other. Namely, we’ll say that a phrase is separated by a comma (,),
    semicolon (;), or colon (:).
  prefs: []
  type: TYPE_NORMAL
- en: It would be nice to have a subtask to break a sentence into its phrases, just
    like we had a subtask to break text into its sentences. Let’s make that happen!
    We’ll call the function for that subtask `get_phrases`. The `get_phrases` function
    will take a sentence of the book as a parameter and return a list of phrases from
    the sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s pause for a moment and think about what we’re doing with our `get_sentences`
    and `get_phrases` functions. They’re both quite similar, come to think of it.
    All that distinguishes them is the characters that they use to make the splits.
    `get_sentences` cares about periods, question marks, and exclamation points, whereas
    `get_phrases` cares about commas, semicolons, and colons. We see an opportunity
    for a parent task that would simplify both of these tasks!
  prefs: []
  type: TYPE_NORMAL
- en: Namely, imagine that we had a `split_string` function that took two parameters,
    the text and a string of separator characters, and it returned a list of pieces
    of text separated by any of the separators. We could then call it with `'.?!'`
    to split into sentences and `',;:'` to split into phrases. That would make both
    `get_sentences` and `get_phrases` easier to implement and reduce code duplication.
    This is a win!
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we’ve fully fleshed out the functions necessary to support the
    higher-level function `make_signature`, as reflected in figure 11.4\. We’ll next
    turn to the `get_all_signatures` function.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/11-4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 Functions diagram with all the supporting functions for the `make_signature`
    function complete
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Figuring out each known signature
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We just worked hard to break down our `make_signature` function into five main
    tasks, one for each feature of our signatures. We designed that function to determine
    the unknown signature—the signature for the mystery text whose author we’re trying
    to identify.
  prefs: []
  type: TYPE_NORMAL
- en: Our next task is to figure out the signature for each of the books for which
    we know the author. In the resources for this book, under the ch11 folder, you’ll
    find a directory called `known_authors`. In there, you’ll find several files,
    each named as an author. Each file contains a book written by that author. For
    example, if you open Arthur_Conan_Doyle.txt, you’ll find the text of the book
    *A Study in Scarlet* by Arthur Conan Doyle. We need to determine the signature
    for each of these files.
  prefs: []
  type: TYPE_NORMAL
- en: Amazingly, we have far less work to do to solve this problem than it may seem.
    That’s because we can use that same `make_signature` function, the one we designed
    to determine the signature of the mystery book, to also determine the signature
    for any known book!
  prefs: []
  type: TYPE_NORMAL
- en: We’ll name the function for this task `get_all_signatures`. It wouldn’t make
    sense for this function to take the text of one book as a parameter because it’s
    supposed to be able to get the signature for all of our known books. Rather, it
    will take a directory of known books as a parameter. Its behavior will be to loop
    through the files in that directory, calculating the signature for each one.
  prefs: []
  type: TYPE_NORMAL
- en: We need the function to tell us which signature goes with which book. In other
    words, we need it to associate each book with its corresponding signature. This
    kind of association is precisely why Python has dictionaries! We’ll therefore
    have this function return a dictionary, where the keys are names of files, and
    the values are the corresponding signature. Our function diagram didn’t need any
    *new* functions to support the `get_all_signatures` function, so our updated diagram
    in figure 11.5 just shows how `get_all_signatures` calls `make_signature`.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/11-5.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.5 Functions diagram updated for `get_all_signatures` to call `make_signature`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Finding closest known signature
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let’s recap what we’ve designed so far:'
  prefs: []
  type: TYPE_NORMAL
- en: We’ve designed our `make_signature` function to get us the unknown signature
    for the mystery book.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ve designed our `get_all_signatures` function to get us all of our known
    signatures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, we need to design a function that tells us which of those known signatures
    is best; that is, which known signature is closest to our unknown signature. Each
    of our signatures will be a list of five numbers giving the quantity for each
    of our five features. The order of these numbers will be the same order we used
    before: average word length, different words divided by total words, words used
    exactly once divided by total words, average number of words per sentence, and
    average sentence complexity.'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that we have two signatures. The first one is `[4.6,` `0.1,` `0.05,`
    `10,` `2]`which means that the average word length for that book is 4.6, the different
    words divided by total words is 0.1, and so on. The second signature is `[4.3,`
    `0.1,` `0.04,` `16,` `4]`.
  prefs: []
  type: TYPE_NORMAL
- en: There are many ways to get an overall score giving the difference between signatures.
    The one we’ll use will give us a difference score for each feature, and then we’ll
    add up those scores to get our overall score.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the values of each signature for the first feature: 4.6 and 4.3\.
    If we subtract those, we get a difference of 4.6 – 4.3 = 0.3\. We could use 0.3
    as our answer for this feature, but it turns out to work better if we *weight*
    each difference using a different weight. Each weight gives the importance of
    that feature. We’ll use some weights(`[11,` `33,` `50,` `0.4,` `4]`) that in our
    experience have proven to work well. You might wonder where the heck these weights
    come from. But note that there’s no magic about them: in working with our students
    over the years, we’ve just found that these weights seem to work out. This would
    be only a starting point for a stronger authorship identification program. When
    doing this type of research, people routinely *tune* their training, which means
    to adjust weights to obtain stronger results.'
  prefs: []
  type: TYPE_NORMAL
- en: When we say that we’re using weights of `[11,` `33,` `50,` `0.4,` `4]`, it means
    that we’ll multiply the difference on the first feature by 11, the difference
    on the second feature by 33, and so on. So, rather than getting a difference of
    0.3 for the first feature, we’ll get 0.3 × 11 = 3.3.
  prefs: []
  type: TYPE_NORMAL
- en: We need to be careful with features like the fourth one, where the difference
    is negative. We don’t want to start with 10 – 16 = –6 because that’s a negative
    number, and that would *undo* some of the positive difference from other features.
    Instead, we need to first make this number positive and then multiply it by its
    weight. Removing the negative sign from a number is known as taking the absolute
    value, and the absolute value is denoted as `abs`. The full calculation for this
    fourth feature, then, is abs(10 – 16) × 0.4 = 2.4.
  prefs: []
  type: TYPE_NORMAL
- en: Table 11.1 gives the calculation for each feature. If we add up all five scores,
    we get an overall score of 14.2.
  prefs: []
  type: TYPE_NORMAL
- en: Table 11.1 Calculating the difference between two signatures
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Feature Number | Value of Feature in Signature 1 | Value of Feature in Signature
    2 | Weight of Feature | Contribution of Feature |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1  | 4.6  | 4.3  | 11  | abs(4.6 – 4.3) × 11 = 3.3  |'
  prefs: []
  type: TYPE_TB
- en: '| 2  | 0.1  | 0.1  | 33  | abs(0.1 – 0.1) × 33 = 0  |'
  prefs: []
  type: TYPE_TB
- en: '| 3  | 0.05  | 0.04  | 50  | abs(0.05 – 0.04) × 50 = .5  |'
  prefs: []
  type: TYPE_TB
- en: '| 4  | 10  | 16  | 0.4  | abs(10 – 16) × 0.4 = 2.4  |'
  prefs: []
  type: TYPE_TB
- en: '| 5  | 2  | 4  | 4  | abs(2 – 4) × 4 = 8  |'
  prefs: []
  type: TYPE_TB
- en: '| Sum  |  |  |  | 14.2  |'
  prefs: []
  type: TYPE_TB
- en: 'Remember where we are in the top-down design: we need a function that tells
    us which known signature is best. Well, now we know how to compare two signatures
    and get the score for that comparison. We’ll want to make that comparison between
    the unknown signature and each known signature to determine which known signature
    is best. The lower the score, the closer the signatures; the higher the score,
    the more different the signatures are. As such, we’ll want to ultimately choose
    the signature with the lowest comparison score.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll name the function for this task `lowest_score`. It will take three parameters:
    a dictionary mapping author names to their known signatures, an unknown signature,
    and a list of weights. The function will return the signature that has the lowest
    comparison score with our unknown signature.'
  prefs: []
  type: TYPE_NORMAL
- en: Think about the work that this function will need to do. It needs to loop through
    the known signatures. We can do that with a `for` loop—no need for a subtask there.
    It will need to compare the unknown signature to the current known signature.
    Oh! That’s a subtask right there, embodying the scoring mechanism that we outlined
    in table 11.1\. We’ll name the function for that subtask `get_score`. Our `get_score`
    function will take two signatures to compare and the list of weights and return
    the score for the comparison between these two signatures.
  prefs: []
  type: TYPE_NORMAL
- en: 11.4 Summary of our top-down design
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We did it! We’ve broken down our original big problem into several smaller problems
    that are amenable to being implemented as a function.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.6 depicts all the work that we’ve done during the process of decomposing
    the problem. Remember, we started with a `make_guess` function, which will solve
    the overall problem. To help us with `make_guess`, we created a `process_data`
    function that will do some of the work for `make_guess`. To help `process_data`,
    we created three more functions, `make_signature`, `get_all_signatures`, and `lowest_score`,
    that each have their own helper functions, and so forth. Having sketched out the
    functions we’ll need to solve our problem, our next step will be to implement
    them.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/11-6.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.6 Full functions diagram for `make_guess`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 11.5 Implementing our functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we’re ready to ask Copilot to implement each function that we need. We designed
    our functions by starting from the top—the biggest problem—and working down to
    smaller problems. But remember from chapter 7 that this isn’t the order that we
    implement the functions; instead, we implement the functions in the opposite order,
    from bottom to top (or right to left in figure 11.6).
  prefs: []
  type: TYPE_NORMAL
- en: Just as in our example in chapter 7, we’re not going to focus much on testing,
    prompt engineering, debugging, or code reading. We do encourage you to run doctest
    on the docstring tests that we’ve provided, and further encourage you to add additional
    tests for each function.
  prefs: []
  type: TYPE_NORMAL
- en: 11.5.1 clean_word
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ll start with our `clean_word` function. As usual, we provide the function
    header (the `def` line) and docstring, and we let Copilot fill in the code. We
    also provide some annotations to briefly illustrate how the code works.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that we want our `clean_word` function to remove punctuation that might
    show up around the word and to convert the word to lowercase. But we don’t want
    to mess with punctuation in the middle of the word, such as the “-” in *card-board*.
    We’ve written the docstring to make clear what we want.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.1 Clean words for analysis
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Converts the word to lowercase'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Uses the string module to strip punctuation from ends'
  prefs: []
  type: TYPE_NORMAL
- en: When working on our password functions in chapter 3, we saw Copilot using the
    string module, and we see Copilot doing that again here. We know from our work
    in chapter 3 that this won’t work unless we import string first, so add
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: above this function as we’ve done in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.2 Clean words for analysis, complete
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This completes the `clean_word` function, so we can mark this as complete in
    our functions diagram in figure 11.7.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/11-7.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.7 Full functions diagram with `clean_word` now finished
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 11.5.2 average_word_length
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now let’s tackle the first of our five signature feature functions: `average_word_length`.
    It needs to determine the average number of letters per word, but we don’t want
    to count surrounding punctuation as letters nor include words that don’t have
    any letters. We want to use our `clean_word` function here, as shown in the following
    listing. As always, we’ve written the docstring in a way that we hope directs
    Copilot to make these decisions.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.3 Average word length
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Splits string into its words'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 total will count the total number of letters across all words.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 count will count the number of words.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Loops through each word'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Copilot calls clean_word for us!'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Considers this word only if it isn’t empty'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Adds number of letters in word'
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Adds 1 to count this word'
  prefs: []
  type: TYPE_NORMAL
- en: '#9 Returns number of letters divided by number of words'
  prefs: []
  type: TYPE_NORMAL
- en: You’ll notice in the doctest here that we’ve split our string over two lines,
    ending the first line with a \ character. The reason we did this is that the string
    wouldn’t otherwise fit on one line in the book. We also needed to keep the second
    line without any indentation; otherwise, doctest would use that indentation as
    spaces in the string. On your computer, you can type the string on a single line
    and not worry about the \ or lack of indentation.
  prefs: []
  type: TYPE_NORMAL
- en: We can now mark `average_word_length` as done in our updated figure (figure
    11.8). Although satisfying, marking these off in the figure one by one might be
    a bit too much noise, so we’ll revisit the figure only periodically going forward.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/11-8.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.8 Full functions diagram with `average_word_length` now finished
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 11.5.3 different_to_total
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is the second of our signature features. We need this one to calculate
    the number of different words used divided by the total number of words. Again,
    we don’t want surrounding punctuation or empty words.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.4 Different words divided by total number of words
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Splits string into its words'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 total will count the total number of nonempty words.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 A set of the unique words found'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Copilot again calls clean_word for us!'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 A set is like a list but doesn’t accept duplicates.'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Returns the number of different words divided by total number of words'
  prefs: []
  type: TYPE_NORMAL
- en: This code uses a Python set, rather than a Python list, to store the unique
    words that it finds. A set is similar to a list except that it doesn’t accept
    duplicates, so if you try to add the same word multiple times, it retains only
    one copy of the word. That’s why we can use `len(unique)` in the return statement
    to get the total number of unique words.
  prefs: []
  type: TYPE_NORMAL
- en: We don’t need to know any more about Python sets to continue here. But if you’re
    interested in learning more, you can ask Copilot for a code explanation and/or
    do a Google search for information on Python sets.
  prefs: []
  type: TYPE_NORMAL
- en: 11.5.4 exactly_once_to_total
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’re ready to implement our third signature feature. This one calculates the
    number of words used exactly once divided by the total number of words, as shown
    in the following listing. We provided the prompt to Copilot and received the following
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 11.5 Fraction of words used just once: Try 1'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Removing a word from the unique set?'
  prefs: []
  type: TYPE_NORMAL
- en: When we skim that code, it looks like `unique` is a set that keeps track of
    the unique words. At the end, we divide that number of unique words by the total
    number of words.
  prefs: []
  type: TYPE_NORMAL
- en: However, run our test case using doctest, and you’ll see that our code fails
    the test. We need the result to be 0.5, but our code is unfortunately giving us
    0.6\. Our code is wrong. We need to stay vigilant!
  prefs: []
  type: TYPE_NORMAL
- en: Let’s review the code to see why it was wrong. Imagine that a word is used three
    times in a text. The first time our code finds it, the code will add it to `unique`.
    The second time, it will *remove* it from `unique`. That’s fine so far because
    the word isn’t being used exactly once anymore, so it shouldn’t be in there. But
    then the third time, it will add that word back to`unique`! So, our code is going
    to count some words as being used exactly once when in fact they were used more
    than once. We looked through the list that appears when you press Ctrl-Enter and
    found this alternate version, which is correct, as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 11.6 Fraction of words used just once: Try 2'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Keeps track of all words we’ve seen'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Keeps track of words used exactly once—what we ultimately care about'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 We’re seeing the word again.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 So the word has to be removed from once.'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 We’ve now seen this word.'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 So far, the word is used exactly once.'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Returns the number of words used exactly once divided by the total number
    of words'
  prefs: []
  type: TYPE_NORMAL
- en: This code is tricky! To understand it, start by focusing on the `else` code.
    That’s the code that runs the first time we see each word. That word gets added
    to both the `unique` and `once` sets. It’s the `once` set that’s going to keep
    track for us of the words used exactly once.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now imagine that we see a word for a second time. The `if` code is going to
    run when this happens because the word is already in `unique` (we added it there
    the first time we saw this word). Now, because we’ve seen the word more than once,
    we need it gone from the `once` set. That’s exactly what the `if` code does: it
    uses `once.discard(word)` to remove the word from `once`.'
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, the first time we see a word, it gets added to `once`. When we
    see it again, it gets removed from `once` with no way to ever have that word added
    back to `once`. The `once` set is correctly tracking the words used exactly once.
  prefs: []
  type: TYPE_NORMAL
- en: 11.5.5 split_string
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ve finished our three word-level signature feature functions. Before we can
    move on to our two sentence-level signature feature functions, we need to write
    `get_ sentences`. But to write `get_sentences`, we first need `split_string`,
    which is what we’ll work on now.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our `split_string` function is supposed to be able to split a string around
    any number of separators. It inherently has nothing to do with sentences or phrases.
    We’ve included one docstring test to highlight this fact: even though we’re going
    to use it to split sentences and phrases, it’s more general than that. Look at
    the following listing.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.7 Split a string around separators
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '******#1 A better variable name would be all_strings.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 A better variable name would be current_string.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Current string ends here.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Removes any space from beginning and end of current string'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 If the current string isn’t empty . . .'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 . . . saves this as one of the split strings.'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Clears the current string to get ready for the next one'
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Adds to the current string (don’t split yet)'
  prefs: []
  type: TYPE_NORMAL
- en: '#9 Handles the final split string by adding if not empty******  ******You might
    be curious about the code after the `for` loop and before the `return` statement.
    It seems to be duplicating some of the code from within the `for` loop, so what’s
    it doing there? This code is there because the loop only adds a split string to
    our list of strings when it finds one of the separator characters. If the text
    doesn’t end with a separator character, the loop won’t add the final split string.
    The code below the loop ensures that this final split string isn’t lost.'
  prefs: []
  type: TYPE_NORMAL
- en: It’s been a little while since we updated our diagram with functions we’ve completed.
    Time for an update! This also serves as a reminder that we’re finishing functions
    from the bottom up (right-to-left in the diagram). As such, figure 11.9 has our
    functions completed so far.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/11-9.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.9 Full functions diagram updated with `different_to_total`, `exactly_once_to_total`,
    and `split_string` now finished
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 11.5.6 get_sentences
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In our top-down design, we kicked most of the work for `get_sentences` off to
    the `split_string` function. Therefore, what we’re hoping for is that Copilot
    implements `get_sentences` by making a suitable call to `split_string`.
  prefs: []
  type: TYPE_NORMAL
- en: We don’t want our sentences to have any space at the beginning or end, and we
    don’t want any empty sentences. We initially had these requirements in the docstring,
    but even without them Copilot correctly calls `split_string`, and that function
    handles these edge cases anyway.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.8 Return list of sentences in text
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Calls split_string with the sentence separator characters'
  prefs: []
  type: TYPE_NORMAL
- en: 11.5.7 average_sentence_length
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have `get_sentences`, we can proceed with `average_sentence_length`,
    our fourth signature feature. This one calculates the average number of words
    per sentence. We provided the prompt to Copilot and received the function shown
    in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.9 Average number of words per sentence
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Gets a list of sentences'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Loops through the sentences'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Breaks current sentence into its words'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Loops through sentence’s words'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Returns number of words divided by number of sentences'
  prefs: []
  type: TYPE_NORMAL
- en: 11.5.8 get_phrases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Much liked we needed `get_sentences` before we could implement `average_sentence_
    length`, we need `get_phrases` before we can implement `average_sentence_complexity`.
  prefs: []
  type: TYPE_NORMAL
- en: As with `get_sentences`, we expect Copilot to call `split_string` to get the
    phrases. That is what it has done for us here, as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.10 Return list of phrases from a sentence
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Calls split_string with the phrase separator characters'
  prefs: []
  type: TYPE_NORMAL
- en: 11.5.9 average_sentence_complexity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With `get_phrases` completed, we can now prompt for an implementation of `average_
    sentence_complexity`. The code is shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.11 Average number of phrases per sentence
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '**#1 We changed a period to a comma to make this 5/4 = 1.25.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Gets a list of sentences'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Loops through the sentences'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Gets a list of phrases in the current sentence'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Adds the number of phrases in the current sentence'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Returns the number of phrases divided by the number of sentences**  **We’re
    really coming along now! We’ve finished all the functions needed to create `make_signature`,
    as shown in figure 11.10.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/11-10.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.10 Full functions diagram updated to show that we’re now ready to
    write `make_signature`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 11.5.10 make_signature
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ve written nine functions to this point, and while they’re all important,
    we may feel a little unsatisfied right now because we’re not even dealing with
    text signatures yet. We’ve got some functions that clean words, split strings
    in various ways, and calculate individual features of signatures, but no function
    to make a full signature.
  prefs: []
  type: TYPE_NORMAL
- en: That changes now because we’re finally ready to implement `make_signature` to
    give us the signature for a text. This function will take the text of a book and
    return a list of five numbers, each of which is the result of calling one of our
    five feature functions.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.12 Numeric signature for the text
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '******#1 Each of our five feature functions is called.******  ******Notice
    that this function can be implemented as nothing more than a call to each of our
    five feature functions. It’s important to pause now to think about just how messy
    this function would have been without having done a solid top-down design first.
    The code for all five of the functions that we’re calling here would have had
    to be in a single function, with all of their own variables and calculations mingled
    together into a real mess. Lucky for us, we’re using top-down design! Our function
    is therefore easier for us to read and easier to convince ourselves that it’s
    doing the right thing.'
  prefs: []
  type: TYPE_NORMAL
- en: 11.5.11 get_all_signatures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our `process_data` function has three subtasks for us to implement. We just
    finished with the first one (`make_signature`), so now we’ll move on to its second
    subtask, which is our `get_all_signatures` function.
  prefs: []
  type: TYPE_NORMAL
- en: From now on, we’ll assume that your working directory has your code and that
    it also has the subdirectory of books that we’ve provided. We need this function
    to return the signature for each file in our directory of known authors. We’re
    hoping for Copilot to call `make_signature` here to make this function far simpler
    than it otherwise would be.
  prefs: []
  type: TYPE_NORMAL
- en: Copilot did do that for us, but the code we got still had two problems. Our
    initial code is shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 11.13 Obtain all signatures from known authors: Try 1'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '**#1 Our dictionary, initially empty, maps filenames to signatures.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Loops through each file in the known authors directory'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Opens the current file'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Reads all text from the file'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Makes the signature for text and stores it in the dictionary**  **Try running
    this function from the Python prompt as'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'and you’ll get the following error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The error is telling us that the function is trying to use a module named os,
    but we don’t have this module available. This module is built-in to Python, and
    we know what to do in this case: import it! That is, we need to add'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'above this function. After that, we still get an error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: You might be wondering what a `UnicodeDecodeError` is. You could google it or
    ask ChatGPT if you’re interested in a technical explanation. What we need to know
    is that each file that we open is encoded in a specific way, and Python has chosen
    the wrong encoding to try to read this file.
  prefs: []
  type: TYPE_NORMAL
- en: We can, however, direct Copilot to fix it by adding a comment near the top of
    our function. (When you encounter errors like these, you can try placing a comment
    directly above the erroneous code that was generated. Then, once you delete the
    incorrect code, Copilot can often generate new code that is correct.) Once we
    do that, all is well, as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 11.14 Obtain all signatures from known authors: Try 2'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '**#1 This prompt tells Copilot to fix the error we saw previously.**  **Now,
    if you run this function, you should see a dictionary of authors and their signatures,
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: For simplicity, we haven’t added a test in the docstring for this function.
    If we did, however, we would create a fake, small book, along the lines of what
    we did in our second example in chapter 6\. We’d like to proceed here with our
    overall purpose of function decomposition, though, so we’ll leave that exercise
    to you if you’d like to pursue that. As shown in figure 11.11, we’ve gotten two
    `process_data` subtasks out of the way. Let’s keep going!
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/11-11.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.11 Full functions diagram updated to show that `make_signature` and
    `get_all_signatures` are finished
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 11.5.12 get_score
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s implement `get_score`, where we need to encode the way that we compare
    signatures. Remember the whole thing where we find the difference on each feature,
    multiply it by a weight, and then add everything together into an overall score?
    That’s what we want `get_score` to do.
  prefs: []
  type: TYPE_NORMAL
- en: 'It would be a challenge to explain this formula in the docstring. And we’re
    not even sure that it should go there: a docstring is supposed to explain how
    someone can use your function, not how it works internally. And, arguably, users
    of our function won’t care about this specific formula anyway. What we can do
    is use a general docstring, without our specific formula, and see what Copilot
    does with it. Here we go in the following listing.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.15 Compare two signatures
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '**#1 These weights, [11, 33, 50, 0.4, 4], worked well for us.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Loops through each signature index'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Adds the weighted difference to score**  **Copilot has implemented exactly
    the formula that we wanted. Now, before we start thinking that Copilot mind-melded
    us or anything like that, remember that the formula we’ve used here is a very
    common metric for comparing signatures. Many students and other programmers over
    the years have implemented authorship identification using this very formula.
    Copilot is just giving that back to us because it occurs so often in its training
    data. If Copilot happened to give us a different formula, we could have tried
    to describe what we want in a comment or, failing that, changed the code ourselves
    to get what we want.'
  prefs: []
  type: TYPE_NORMAL
- en: 11.5.13 lowest_score
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our `lowest_score` function will finally wrap up everything we need to implement
    `process_data`. The `get_score` function that we just implemented gives us the
    score between any two signatures. Our `lowest_score` function is going to call
    `get_score` once for each known signature to compare the unknown signature to
    each known signature. It will then return the known signature that has the lowest
    score with the unknown signature, as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.16 Closest known signature
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '****#1 Using variables in the doctest to make the test itself easier to read'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 This line is easier to read because we’re using our variables.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Loops through each author name'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Gets a score for comparing this known signature to the unknown signature'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 If this is the first comparison or we’ve found a lower score . . .'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 . . . this stores both the best key and score for that key.'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 lowest[0] is the best key.****  ****The first parameter, `signatures_dict`,
    is a dictionary that maps names of authors to their known signatures. That will
    ultimately come from the `get_all_signatures` function. The second parameter,
    `unknown_signature`, will ultimately come from calling `make_signature` on the
    mystery book. The third parameter, `weights`, will be hard-coded by us when we
    call this function.'
  prefs: []
  type: TYPE_NORMAL
- en: 11.5.14 process_data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Only two functions to go! One of them is `process_data`—it feels like it took
    us forever, but we’re finally ready for it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our `process_data` function is going to take two parameters in the following
    listing: the filename of a mystery book and the directory of known-author books.
    It will return the author that we think wrote the mystery book.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.17 Signature closest to the mystery author
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Gets all the known signatures'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Copilot uses our prior work to get the encoding right this time.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Reads text of the mystery book'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Gets the unknown signature'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Returns the signature with the lowest comparison score'
  prefs: []
  type: TYPE_NORMAL
- en: Again, notice how much we’re relying on our earlier functions. This massively
    useful `process_data` function is now really nothing more than a carefully sequenced
    list of function calls.
  prefs: []
  type: TYPE_NORMAL
- en: In the book resources for this chapter, we’ve included a few unknown author
    files, for example, unknown1.txt and unknown2.txt. Those should be in your current
    working directory along with your code (and the subdirectory of known author files).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s call `process_data` to guess who wrote `''unknown1.txt''`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Our program guesses that Arthur Conan Doyle wrote unknown1.txt. And if you peek
    at the text of unknown1.txt by opening the file, you’ll see that our guess is
    right. The book is called *The Sign of the Four*, which is a well-known Arthur
    Conan Doyle book.
  prefs: []
  type: TYPE_NORMAL
- en: 11.5.15 make_guess
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To guess the author of a book, we currently need to type the Python code to
    run `process_data`. That’s not very friendly to users; it would be nice if we
    could run the program and have it ask us which mystery book file we want to work
    with.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll put that finishing touch on our program by implementing `make_guess`,
    our top-most function! This function will ask the user for a filename of a mystery
    book, get the best guess using `process_data`, and tell the user about that guess,
    as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.18 Interacts with user and guesses text’s author
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Asks the user for the filename of the mystery book'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Calls process_data to do all the work and report our guess'
  prefs: []
  type: TYPE_NORMAL
- en: This completes all the functions from our diagram! Figure 11.12 shows that we’ve
    checked off every function from the bottom to the very top of our diagram.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/11-12.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.12 All the required functions for `make_guess` are now complete!
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'If you have all of our code in your Python file, you’ll be able to run it to
    guess the author of a mystery book after you add the following line of code at
    the bottom of that file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, here’s what happens when we run our program and type `unknown1.txt`
    as the unknown book:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: It correctly tells us that unknown1.txt is written by Arthur Conan Doyle! Try
    running it for each of the other unknown book files that we’ve provided. How many
    of those does it guess correctly? Which ones does it get wrong?
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You’ve completed your first real-world top-down design. And
    look at what we’ve managed to accomplish—an authorship identification program
    that any beginning programmer should be proud of. Your program uses AI to learn
    how individual authors write (do they use shorter or longer words on average,
    shorter or longer sentences on average, etc.?) by using the text of books in its
    training data. It then applies that learning to make a prediction on a mystery
    book by determining which author the mystery book most closely emulates—very cool!
    We managed to solve a very difficult problem, and we did it by breaking down the
    problem and letting Copilot write the code for each of the subproblems.
  prefs: []
  type: TYPE_NORMAL
- en: 11.6 Going further
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After people do a top-down design, they often see opportunities to refactor
    their code, which means making the code cleaner or better organized without changing
    its behavior. It’s possible to refactor our program in several ways. For example,
    you might notice that many of our signature feature functions split the string
    into words and then ignore empty words. This task (returning a list of nonempty
    words from a string) could be split off into its own subtask function, which would
    further simplify any function that calls it.
  prefs: []
  type: TYPE_NORMAL
- en: We might also decide that weights should be passed to `process_data`, rather
    than hard-coding the weights in that function. The weights would then be hard-coded
    in `make_guess`, moving the decision higher in the function hierarchy and therefore
    making it easier to find and change if needed.
  prefs: []
  type: TYPE_NORMAL
- en: It’s also possible to improve the program in terms of its features or efficiency.
    For features, right now, our program simply prints its best guess for the mystery
    book author. But we don’t know anything about that guess. Was there a second author
    that was very close to the one that was guessed? If so, we might want to know
    that. More generally, we might want to know the top few guesses rather than just
    the top guess. That way, we have useful information about who the author might
    be even if the top guess happens to be wrong. These are additional features that
    we could add to our program.
  prefs: []
  type: TYPE_NORMAL
- en: For efficiency, let’s think about that `get_all_signatures` function again.
    That function does a lot of work! If we have five books in our known directory,
    then it will read each of the five files and calculate each signature. Big deal,
    right? It’s only five files, and computers are really fast. But imagine if we
    had 100 files or 10,000 files. It may be acceptable to do all that work as a one-time-only
    thing, but that’s not what our program does. In fact, every time we run the program
    to get a guess for the author of a mystery book, it runs that `get_all_signatures`
    function, which means re-creating those signatures every single time. That’s a
    huge amount of wasted effort; it would be nice if we could just store those signatures
    somewhere, never having to calculate them again. Indeed, if we were to redesign
    the code for efficiency, a first step would be to ensure that the signature for
    a known text is only computed once and reused thereafter.
  prefs: []
  type: TYPE_NORMAL
- en: That’s exactly what tools like Copilot do! OpenAI trained GitHub Copilot just
    once on a huge corpus of code. That took thousands or millions of computer hours.
    But now that the training is done, it can keep writing code for us without having
    to train from scratch every time. The idea of doing the training once and then
    using that training for many subsequent predictions is a common paradigm throughout
    all of ML.
  prefs: []
  type: TYPE_NORMAL
- en: 11.7 Exercises
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Which of the following isn’t a step in the AI-based authorship identification
    process described in this chapter?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculating the average word length of the mystery book
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Comparing the mystery book’s signature to known signatures
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Asking the user for the filename of the mystery book
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Finding the total number of pages in the mystery book
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Build a classifier that can distinguish between spam and non-spam (ham) emails
    based on email content. Use features like word frequency, presence of certain
    keywords, and email length. Here are the steps you’ll need to take:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Collect a dataset of spam and non-spam emails. You can find publicly available
    datasets online, such as the Enron spam dataset.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Preprocess the emails (remove stop words, punctuation, etc.).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract features (e.g., word counts, presence of certain words).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Train a classifier using our labeled data (supervised learning). A simple and
    effective choice for the classifier is the Naïve Bayes classifier (feel free to
    use a Python library to help you).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Test the classifier with a separate set of emails to check its accuracy.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In this exercise, you’ll create a simple text generation program using n-grams.
    N-grams are contiguous sequences of *n* items from a given sample of text or speech.
    You’ll use these n-grams to generate new text that mimics the style of the input
    text. The key idea is to build a model that is trained to know which words commonly
    follow other words (i.e., “cat eats” makes sense, “tissue eats” does not) and
    then, among the possible choices, randomly select the next one. Feel free to look
    up n-grams for more information. Here are the steps you’ll need to take:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Choose input text that you can load into Python. You can use something like:
    “Pride and Prejudice” by Jane Austen.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Preprocess the text by converting it to lowercase and removing punctuation.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Create n-grams from the input text. An n-gram is a contiguous sequence of *n*
    items from a given text. For simplicity, we’ll use bigrams (*n* = 2) in this example.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the generated n-grams to produce new text. Start with a random n-gram, and
    keep adding new words based on the n-gram model until the desired length is reached.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Top-down design becomes more and more critical as the complexity of our programs
    increase.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Author identification is the process of guessing the author of a mystery book.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can use features about words (e.g., average word length) and sentences (e.g.,
    average number of words per sentence) to characterize how each known author writes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning is an important area of computer science that investigates
    how machines can learn from data and make predictions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In supervised learning, we have some training data in the form of objects (e.g.,
    books) and their categories (who wrote each book). We can learn from that data
    to make predictions about new objects.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A signature consists of a list of features, one signature per object.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refactoring code means to improve the design of the code (e.g., by reducing
    code repetition).************************
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
