["```py\n$ pip install Scrapy\n\n```", "```py\n`conda` `install` `-``c` `conda``-``forge` `scrapy`\n```", "```py\n$ scrapy startproject wikiSpider\n```", "```py\nfrom scrapy import Spider, Request\n\nclass ArticleSpider(Spider):\n    name='article'\n\n    def start_requests(self):\n        urls = [\n            'http://en.wikipedia.org/wiki/Python_%28programming_language%29',\n            'https://en.wikipedia.org/wiki/Functional_programming',\n            'https://en.wikipedia.org/wiki/Monty_Python']\n        return [Request(url=url, callback=self.parse) for url in urls]\n\n    def parse(self, response):\n        url = response.url\n        title = response.css('h1::text').extract_first()\n        print(f'URL is: {url}')\n        print(f'Title is: {title}')\n\n```", "```py\n$ scrapy runspider wikiSpider/spiders/article.py\n\n```", "```py\n2023-02-11 21:43:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wik\nipedia.org/robots.txt> (referer: None)\n2023-02-11 21:43:14 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (3\n01) to <GET https://en.wikipedia.org/wiki/Python_%28programming_language%29> from\n <GET http://en.wikipedia.org/wiki/Python_%28programming_language%29>\n2023-02-11 21:43:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wik\nipedia.org/wiki/Functional_programming> (referer: None)\n2023-02-11 21:43:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wik\nipedia.org/wiki/Monty_Python> (referer: None)\nURL is: https://en.wikipedia.org/wiki/Functional_programming\nTitle is: Functional programming\n2023-02-11 21:43:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wik\nipedia.org/wiki/Python_%28programming_language%29> (referer: None)\nURL is: https://en.wikipedia.org/wiki/Monty_Python\nTitle is: Monty Python\nURL is: https://en.wikipedia.org/wiki/Python_%28programming_language%29\nTitle is: Python (programming language)\n\n```", "```py\nfrom scrapy.linkextractors import LinkExtractor\nfrom scrapy.spiders import CrawlSpider, Rule\n\nclass ArticleSpider(CrawlSpider):\n    name = 'articles'\n    allowed_domains = ['wikipedia.org']\n    start_urls = ['https://en.wikipedia.org/wiki/Benevolent_dictator_for_life']\n    rules = [\n        Rule(\n            LinkExtractor(allow=r'.*'),\n​            callback='parse_items',\n​            follow=True\n​        )\n    ]\n\n    def parse_items(self, response):\n        url = response.url\n        title = response.css('span.mw-page-title-main::text').extract_first()\n        text = response.xpath('//div[@id=\"mw-content-text\"]//text()').extract()\n        lastUpdated = response.css(\n            'li#footer-info-lastmod::text'\n        ).extract_first()\n        lastUpdated = lastUpdated.replace('This page was last edited on ', '')\n        print(f'URL is: {url}')\n        print(f'Title is: {title} ')\n        print(f'Text is: {text}')\n        print(f'Last updated: {lastUpdated}')\n\n```", "```py\n$ scrapy runspider wikiSpider/spiders/articles.py\n```", "```py\n2023-02-11 22:13:34 [scrapy.spidermiddlewares.offsite] DEBUG: Filtered offsite\n request to 'drupal.org': <GET https://drupal.org/node/769>\n2023-02-11 22:13:34 [scrapy.spidermiddlewares.offsite] DEBUG: Filtered offsite\n request to 'groups.drupal.org': <GET https://groups.drupal.org/node/5434>\n2023-02-11 22:13:34 [scrapy.spidermiddlewares.offsite] DEBUG: Filtered offsite\n request to 'www.techrepublic.com': <GET https://www.techrepublic.com/article/\nopen-source-shouldnt-mean-anti-commercial-says-drupal-creator-dries-buytaert/>\n2023-02-11 22:13:34 [scrapy.spidermiddlewares.offsite] DEBUG: Filtered offsite\n request to 'www.acquia.com': <GET https://www.acquia.com/board-member/dries-b\nuytaert>\n\n```", "```py\ntitle is: Wikipedia:General disclaimer\n\n```", "```py\nrules = [Rule(LinkExtractor(allow=r'.*'), callback='parse_items',\n​    follow=True)]\n```", "```py\nfrom scrapy.linkextractors import LinkExtractor\nfrom scrapy.spiders import CrawlSpider, Rule\n\nclass ArticleSpider(CrawlSpider):\n    name = 'articles'\n    allowed_domains = ['wikipedia.org']\n    start_urls = ['https://en.wikipedia.org/wiki/Benevolent_dictator_for_life']\n    rules = [\n        Rule(\n            LinkExtractor(allow='(/wiki/)((?!:).)*$'),\n            callback='parse_items',\n            follow=True,\n            cb_kwargs={'is_article': True}\n        ),\n        Rule(\n            LinkExtractor(allow='.*'),\n            callback='parse_items',\n            cb_kwargs={'is_article': False}\n        )\n    ]\n\n    def parse_items(self, response, is_article):\n        print(response.url)\n        title = response.css('span.mw-page-title-main::text').extract_first()\n        if is_article:\n            url = response.url\n            text = response.xpath(\n                '//div[@id=\"mw-content-text\"]//text()'\n            ).extract()\n            lastUpdated = response.css(\n                'li#footer-info-lastmod::text'\n            ).extract_first()\n            lastUpdated = lastUpdated.replace(\n                'This page was last edited on ',\n                ''\n            )\n            print(f'URL is: {url}')\n            print(f'Title is: {title}')\n            print(f'Text is: {text}')\n        else:\n            print(f'This is not an article: {title}')\n\n```", "```py\n# -*- coding: utf-8 -*-\n\n# Define here the models for your scraped items\n#\n# See documentation in:\n# http://doc.scrapy.org/en/latest/topics/items.html\n\nimport scrapy\n\nclass WikispiderItem(scrapy.Item):\n    # define the fields for your item here like:\n    # name = scrapy.Field()\n    pass\n\n```", "```py\nimport scrapy\n\nclass Article(scrapy.Item):\n    url = scrapy.Field()\n    title = scrapy.Field()\n    text = scrapy.Field()\n    lastUpdated = scrapy.Field()\n\n```", "```py\nfrom scrapy.linkextractors import LinkExtractor\nfrom scrapy.spiders import CrawlSpider, Rule\nfrom wikiSpider.items import Article\n\nclass ArticleSpider(CrawlSpider):\n    name = 'articleItems'\n    allowed_domains = ['wikipedia.org']\n    start_urls = ['https://en.wikipedia.org/wiki/Benevolent'\n​    ​    '_dictator_for_life']\n    rules = [\n        Rule(LinkExtractor(allow='(/wiki/)((?!:).)*$'),\n​    ​    ​    callback='parse_items', follow=True),\n    ]\n\n    def parse_items(self, response):\n        article = Article()\n        article['url'] = response.url\n        article['title'] = response.css('h1::text').extract_first()\n        article['text'] = response.xpath('//div[@id='\n​    ​    ​    '\"mw-content-text\"]//text()').extract()\n        lastUpdated = response.css('li#footer-info-lastmod::text'\n​    ​    ​    ).extract_first()\n        article['lastUpdated'] = lastUpdated.replace('This page was '\n​    ​    ​    'last edited on ', '')\n        return article\n\n```", "```py\n$ scrapy runspider wikiSpider/spiders/articleItems.py\n\n```", "```py\n2023-02-11 22:52:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wik\nipedia.org/wiki/Benevolent_dictator_for_life#bodyContent> (referer: https://en.wi\nkipedia.org/wiki/Benevolent_dictator_for_life)\n2023-02-11 22:52:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wik\nipedia.org/wiki/OCaml> (referer: https://en.wikipedia.org/wiki/Benevolent_dictato\nr_for_life)\n2023-02-11 22:52:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wik\nipedia.org/wiki/Xavier_Leroy> (referer: https://en.wikipedia.org/wiki/Benevolent_\ndictator_for_life)\n2023-02-11 22:52:26 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wik\nipedia.org/wiki/Benevolent_dictator_for_life>\n{'lastUpdated': ' 7 February 2023, at 01:14',\n'text': ['Title given to a small number of open-source software development '\n          'leaders',\n          ...\n\n```", "```py\n$ scrapy runspider articleItems.py -o articles.csv -t csv\n$ scrapy runspider articleItems.py -o articles.json -t json\n$ scrapy runspider articleItems.py -o articles.xml -t xml\n```", "```py\n<items>\n<item>\n    <url>https://en.wikipedia.org/wiki/Benevolent_dictator_for_life</url>\n    <title>Benevolent dictator for life</title>\n    <text>\n        <value>For the political term, see </value>\n        <value>Benevolent dictatorship</value>\n        ...\n    </text>\n    <lastUpdated> 7 February 2023, at 01:14.</lastUpdated>\n</item>\n....\n\n```", "```py\n# Configure item pipelines\n# See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html\n#ITEM_PIPELINES = {\n#    'wikiSpider.pipelines.WikispiderPipeline': 300,\n#}\n\n```", "```py\nITEM_PIPELINES = {\n    'wikiSpider.pipelines.WikispiderPipeline': 300,\n}\n\n```", "```py\n    def parse_items(self, response):\n        return response\n\n```", "```py\nfrom scrapy.linkextractors import LinkExtractor\nfrom scrapy.spiders import CrawlSpider, Rule\nfrom wikiSpider.items import Article\n\nclass ArticleSpider(CrawlSpider):\n    name = 'articlePipelines'\n    allowed_domains = ['wikipedia.org']\n    start_urls = ['https://en.wikipedia.org/wiki/Benevolent_dictator_for_life']\n    rules = [\n        Rule(LinkExtractor(allow='(/wiki/)((?!:).)*$'),\n​    ​    ​    callback='parse_items', follow=True),\n    ]\n\n    def parse_items(self, response):\n        article = Article()\n        article['url'] = response.url\n        article['title'] = response.css('h1::text').extract_first()\n        article['text'] = response.xpath('//div[@id='\n​    ​    ​    '\"mw-content-text\"]//text()').extract()\n        article['lastUpdated'] = response.css('li#'\n​    ​    ​    'footer-info-lastmod::text').extract_first()\n        return article\n\n```", "```py\n# -*- coding: utf-8 -*-\n\n# Define your item pipelines here\n#\n# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n# See: http://doc.scrapy.org/en/latest/topics/item-pipeline.html\n\nclass WikispiderPipeline(object):\n    def process_item(self, item, spider):\n        return item\n```", "```py\nfrom datetime import datetime\nfrom wikiSpider.items import Article\nfrom string import whitespace\n\nclass WikispiderPipeline(object):\n    def process_item(self, article, spider):\n        dateStr = article['lastUpdated']\n        article['lastUpdated'] = article['lastUpdated']\n​    ​    ​    .replace('This page was last edited on', '')\n        article['lastUpdated'] = article['lastUpdated'].strip()\n        article['lastUpdated'] = datetime.strptime(\n​    ​    ​    article['lastUpdated'], '%d %B %Y, at %H:%M.')\n        article['text'] = [line for line in article['text']\n​    ​    ​    if line not in whitespace]\n        article['text'] = ''.join(article['text'])\n        return article\n\n```", "```py\ndef process_item(self, item, spider):    \n    if isinstance(item, Article):\n        # Article-specific processing here\n\n```", "```py\nLOG_LEVEL = 'ERROR'\n```", "```py\n$ scrapy crawl articles -s LOG_FILE=wiki.log\n```"]