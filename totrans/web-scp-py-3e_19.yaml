- en: Chapter 17\. Avoiding Scraping Traps
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第17章 避免爬虫陷阱
- en: Few things are more frustrating than scraping a site, viewing the output, and
    not seeing the data that’s so clearly visible in your browser. Or submitting a
    form that should be perfectly fine but gets denied by the web server. Or getting
    your IP address blocked by a site for unknown reasons.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 很少有什么比爬取一个网站、查看输出，并没有看到浏览器中清晰可见的数据更令人沮丧的了。或者提交一个应该完全正常但被网络服务器拒绝的表单。或者因未知原因被一个网站阻止
    IP 地址。
- en: These are some of the most difficult bugs to solve, not only because they can
    be so unexpected (a script that works just fine on one site might not work at
    all on another, seemingly identical, site), but because they purposefully don’t
    have any telltale error messages or stack traces to use. You’ve been identified
    as a bot, rejected, and you don’t know why.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是一些最难解决的 bug，不仅因为它们可能是如此意外（在一个网站上运行良好的脚本在另一个看似相同的网站上可能根本不起作用），而且因为它们故意不提供任何显眼的错误消息或堆栈跟踪供使用。你被识别为机器人，被拒绝了，而你却不知道为什么。
- en: 'In this book, I’ve written about a lot of ways to do tricky things on websites,
    including submitting forms, extracting and cleaning difficult data, and executing
    JavaScript. This chapter is a bit of a catchall in that the techniques stem from
    a wide variety of subjects. However, they all have something in common: they are
    meant to overcome an obstacle put in place for the sole purpose of preventing
    automated scraping of a site.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我写了很多在网站上执行棘手任务的方法，包括提交表单、提取和清理困难数据以及执行 JavaScript。这一章有点像一个杂项章节，因为这些技术来自各种各样的学科。然而，它们都有一个共同点：它们旨在克服一个唯一目的是防止自动爬取网站的障碍。
- en: Regardless of how immediately useful this information is to you at the moment,
    I highly recommend you at least skim this chapter. You never know when it might
    help you solve a difficult bug or prevent a problem altogether.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 无论这些信息对你目前有多么重要，我强烈建议你至少浏览一下这一章。你永远不知道它何时会帮助你解决一个难题或完全防止一个问题的发生。
- en: A Note on Ethics
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于伦理的一点说明
- en: In the first few chapters of this book, I discussed the legal gray area that
    web scraping inhabits, as well as some of the ethical and legal guidelines to
    scrape by. To be honest, this chapter is, ethically, perhaps the most difficult
    one for me to write. My websites have been plagued by bots, spammers, web scrapers,
    and all manner of unwanted virtual guests, as perhaps yours have been. So why
    teach people how to build better bots?
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的前几章中，我讨论了网络爬虫所处的法律灰色地带，以及一些伦理和法律指南。说实话，对我来说，这一章在伦理上可能是最难写的一章。我的网站一直受到机器人、垃圾邮件发送者、网络爬虫和各种不受欢迎的虚拟访客的困扰，也许你的网站也是如此。那么为什么要教人们如何构建更好的机器人呢？
- en: 'I believe this chapter is important to include for a few reasons:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为包括这一章有几个重要原因：
- en: There are perfectly ethical and legally sound reasons to scrape some websites
    that do not want to be scraped. In a previous job I had as a web scraper, I performed
    automated collection of information from websites that were publishing clients’
    names, addresses, telephone numbers, and other personal information to the internet
    without their consent. I used the scraped information to make legal requests to
    the websites to remove this information. To avoid competition, these sites vigilantly
    guarded this information from scrapers. However, my work to ensure the anonymity
    of my company’s clients (some of whom had stalkers, were the victims of domestic
    violence, or had other very good reasons to want to keep a low profile) made a
    compelling case for web scraping, and I was grateful that I had the skills necessary
    to do the job.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有一些完全符合道德和法律的理由可以爬取一些不希望被爬取的网站。在我以前的一份工作中，我作为一个网络爬虫，自动收集网站上发布客户姓名、地址、电话号码和其他个人信息的信息，而这些客户并未同意发布这些信息到互联网上。我使用爬取的信息向网站提出合法请求，要求其删除这些信息。为了避免竞争，这些网站密切关注着这些信息不被爬取。然而，我的工作确保了我公司客户（其中一些人有骚扰者，是家庭暴力的受害者，或者有其他非常充分的理由希望保持低调）的匿名性，这为网络爬取提供了一个令人信服的理由，我很感激我有必要的技能来完成这项工作。
- en: Although it is almost impossible to build a “scraper proof” site (or at least
    one that can still be easily accessed by legitimate users), I hope that the information
    in this chapter will help those wanting to defend their websites against malicious
    attacks. Throughout, I will point out some of the weaknesses in each web scraping
    technique, which you can use to defend your own site. Keep in mind that most bots
    on the web today are merely doing a broad scan for information and vulnerabilities;
    employing even a couple of simple techniques described in this chapter likely
    will thwart 99% of them. However, they are getting more sophisticated every month,
    and it’s best to be prepared.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管几乎不可能建立一个“抓取器免疫”的网站（或者至少是一个仍然容易被合法用户访问的网站），但我希望本章的信息能帮助那些希望捍卫其网站免受恶意攻击的人。在整本书中，我将指出每种网页抓取技术的一些弱点，你可以用来保护自己的网站。请记住，今天网络上的大多数机器人仅仅是在进行广泛的信息和漏洞扫描；即使使用本章描述的几种简单技术，也很可能会阻挡其中99%。然而，它们每个月都在变得更加复杂，最好做好准备。
- en: Like most programmers, I don’t believe that withholding any sort of educational
    information is a net positive thing to do.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像大多数程序员一样，我不认为隐瞒任何教育信息是一件积极的事情。
- en: 'While you’re reading this chapter, keep in mind that many of these scripts
    and described techniques should not be run against every site you can find. Not
    only is it not a nice thing to do, but you could wind up receiving a cease and
    desist letter or worse (for more information about what to do if you receive such
    a letter, see [Chapter 2](ch02.html#c-2)). But I’m not going to pound you over
    the head with this every time I discuss a new technique. So, for the rest of this
    chapter, as the philosopher Gump once said: “That’s all I have to say about that.”'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读本章节时，请记住，许多这些脚本和描述的技术不应该在你能找到的每个网站上运行。这不仅仅是不好的做法，而且你可能最终会收到停止和停止信函或者更糟糕的后果（如果你收到这样的信函，详细信息请参阅[第2章](ch02.html#c-2)）。但我不会每次讨论新技术时都敲打你的头。所以，在本章剩余部分，就像哲学家Gump曾经说过的：“这就是我要说的一切。”
- en: Looking Like a Human
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 仿人行为
- en: The fundamental challenge for sites that do not want to be scraped is figuring
    out how to tell bots from humans. Although many of the techniques sites use (such
    as CAPTCHAs) can be difficult to fool, you can do a few fairly easy things to
    make your bot look more human.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于不想被抓取的网站而言，基本挑战在于如何区分机器人和人类。虽然许多网站使用的技术（如CAPTCHA）很难欺骗，但你可以通过一些相对简单的方法使你的机器人看起来更像人类。
- en: Adjust Your Headers
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调整你的标头
- en: 'Throughout the book, you’ve used the Python Requests library to create, send,
    and receive HTTP requests, such as handling forms on a website in [Chapter 13](ch13.html#c-13).
    The Requests library is also excellent for setting headers. HTTP headers are lists
    of attributes, or preferences, sent by you every time you make a request to a
    web server. HTTP defines dozens of obscure header types, most of which are not
    commonly used. The following seven fields, however, are consistently used by most
    major browsers when initiating any connection (shown with example data from my
    own browser):'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，你已经使用Python Requests库来创建、发送和接收HTTP请求，比如在[第13章](ch13.html#c-13)处理网站上的表单。Requests库也非常适合设置标头。HTTP标头是每次向Web服务器发出请求时由你发送的属性或偏好列表。HTTP定义了几十种晦涩的标头类型，其中大多数不常用。然而，大多数主流浏览器在发起任何连接时一直使用以下七个字段（显示了来自我的浏览器示例数据）：
- en: '| `Host` | https://www.google.com/ |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| `Host` | https://www.google.com/ |'
- en: '| `Connection` | keep-alive |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| `Connection` | keep-alive |'
- en: '| `Accept` | text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7
    |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| `Accept` | text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7
    |'
- en: '| `User-Agent` | Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36
    (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36 |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| `User-Agent` | Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36
    (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36 |'
- en: '| `Referrer` | https://www.google.com/ |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| `Referrer` | https://www.google.com/ |'
- en: '| `Accept-Encoding` | gzip, deflate, sdch |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| `Accept-Encoding` | gzip, deflate, sdch |'
- en: '| `Accept-Language` | en-US,en;q=0.8 |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| `Accept-Language` | en-US,en;q=0.8 |'
- en: 'And here are the headers that a typical Python scraper using the default urllib
    library might send:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个使用默认的urllib库进行典型Python网页抓取的标头：
- en: '| `Accept-Encoding` | identity |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| `Accept-Encoding` | identity |'
- en: '| `User-Agent` | Python-urllib/3.9 |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| `User-Agent` | Python-urllib/3.9 |'
- en: If you’re a website administrator trying to block scrapers, which one are you
    more likely to let through?
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是一个试图阻止爬虫的网站管理员，你更可能放行哪一个？
- en: 'Fortunately, headers can be completely customized using the Requests library.
    The website [*https://www.whatismybrowser.com*](https://www.whatismybrowser.com)
    is great for testing browser properties viewable by servers. You’ll scrape this
    website to verify your cookie settings with the following script:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，使用Requests库可以完全自定义头部。[*https://www.whatismybrowser.com*](https://www.whatismybrowser.com)这个网站非常适合测试服务器可见的浏览器属性。你将使用以下脚本爬取这个网站以验证你的Cookie设置。
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The output should show that the headers are now the same ones set in the `headers`
    dictionary object in the code.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应显示标题现在与代码中的`headers`字典对象中设置的相同。
- en: Although it is possible for websites to check for “humanness” based on any of
    the properties in HTTP headers, I’ve found that typically the only setting that
    really matters is the `User-Agent`. It’s a good idea to keep this one set to something
    more inconspicuous than `Python-urllib/3.9`, regardless of what project you are
    working on. In addition, if you ever encounter an extremely suspicious website,
    populating one of the commonly used but rarely checked headers such as `Accept-Language`
    might be the key to convincing it you’re a human.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管网站可以基于HTTP头部的任何属性检查“人类性”，但我发现通常唯一重要的设置是`User-Agent`。无论你在做什么项目，将其设置为比`Python-urllib/3.9`更不引人注意的内容都是个好主意。此外，如果你遇到一个极为可疑的网站，填充诸如`Accept-Language`等常用但很少检查的头部之一可能是说服它你是人类的关键。
- en: Handling Cookies with JavaScript
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用JavaScript处理Cookie
- en: Handling cookies correctly can alleviate many scraping problems, although cookies
    can also be a double-edged sword. Websites that track your progression through
    a site using cookies might attempt to cut off scrapers that display abnormal behavior,
    such as completing forms too quickly, or visiting too many pages. Although these
    behaviors can be disguised by closing and reopening connections to the site, or
    even changing your IP address (see [Chapter 20](ch20.html#c-20) for more information
    on how to do that), if your cookie gives your identity away, your efforts at disguise
    might be futile.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 正确处理Cookie可以缓解许多爬取问题，尽管Cookie也可能是双刃剑。使用Cookie跟踪你在网站上的活动进度的网站可能会试图阻止显示异常行为的爬虫，如过快地完成表单或访问过多页面。尽管这些行为可以通过关闭和重新打开与网站的连接，甚至更改你的IP地址来掩饰，但如果你的Cookie暴露了你的身份，你的伪装努力可能会徒劳无功（详见[第20章](ch20.html#c-20)了解更多关于如何做到这一点的信息）。
- en: Cookies can also be necessary to scrape a site. As shown in [Chapter 13](ch13.html#c-13),
    staying logged in on a site requires that you be able to hold and present a cookie
    from page to page. Some websites don’t even require that you actually log in and
    get a new version of a cookie every time—merely holding an old copy of a “logged-in”
    cookie and visiting the site is enough.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Cookies有时也是爬取网站必需的。如[第13章](ch13.html#c-13)所示，在网站上保持登录状态需要能够保存并呈现页面到页面的Cookie。有些网站甚至不要求你真正登录并获得新版本的Cookie，只需持有一个旧的“已登录”Cookie并访问网站即可。
- en: If you are scraping a single targeted website or a small number of targeted
    sites, I recommend examining the cookies generated by those sites and considering
    which ones you might want your scraper to handle. Various browser plug-ins can
    show you how cookies are being set as you visit and move around a site. [EditThisCookie](http://www.editthiscookie.com),
    a Chrome extension, is one of my favorites.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在爬取一个或少数几个特定的网站，我建议检查这些网站生成的Cookie，并考虑你的爬虫可能需要处理哪些Cookie。各种浏览器插件可以在你访问和浏览网站时显示Cookie的设置方式。[EditThisCookie](http://www.editthiscookie.com)，一款Chrome扩展，是我喜欢的工具之一。
- en: Check out the code samples in [“Handling Logins and Cookies”](ch13.html#handling_logins_cookies)
    in [Chapter 13](ch13.html#c-13) for more information about handling cookies by
    using the Requests library. Of course, because it is unable to execute JavaScript,
    the Requests library will be unable to handle many of the cookies produced by
    modern tracking software, such as Google Analytics, which are set only after the
    execution of client-side scripts (or sometimes based on page events, such as button
    clicks, that happen while browsing the page). To handle these, you need to use
    the Selenium and Chrome WebDriver packages (I covered their installation and basic
    usage in [Chapter 14](ch14.html#c-14)).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 若要了解有关使用 Requests 库处理 cookies 的更多信息，请查看[“处理登录和 cookies”](ch13.html#handling_logins_cookies)中的代码示例在[第
    13 章](ch13.html#c-13)。当然，由于它无法执行 JavaScript，因此 Requests 库将无法处理许多现代跟踪软件生成的 cookies，例如
    Google Analytics，这些 cookies 仅在客户端脚本执行后（或有时基于页面事件，如按钮点击，在浏览页面时发生）设置。为了处理这些问题，您需要使用
    Selenium 和 Chrome WebDriver 包（我在[第 14 章](ch14.html#c-14)中介绍了它们的安装和基本用法）。
- en: 'You can view cookies by visiting any site ([*http://pythonscraping.com*](http://pythonscraping.com),
    in this example) and calling `get_cookies()` on the webdriver:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过访问任何站点（[*http://pythonscraping.com*](http://pythonscraping.com)，例如）并在 webdriver
    上调用 `get_cookies()` 查看 cookies：
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This provides the fairly typical array of Google Analytics cookies:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这提供了相当典型的 Google Analytics cookies 数组：
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To manipulate cookies, you can call the `delete_cookie()`, `add_cookie()`,
    and `delete_all_cookies()` functions. In addition, you can save and store cookies
    for use in other web scrapers. Here’s an example to give you an idea of how these
    functions work together:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 要操作 cookies，您可以调用 `delete_cookie()`、`add_cookie()` 和 `delete_all_cookies()`
    函数。此外，您可以保存和存储 cookies 以供其他网络爬虫使用。以下示例让您了解这些函数如何协同工作：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In this example, the first webdriver retrieves a website, prints the cookies,
    and then stores them in the variable `savedCookies`. The second webdriver loads
    the same website, deletes its own cookies, and adds the cookies from the first
    webdriver.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，第一个 webdriver 检索一个网站，打印 cookies，然后将它们存储在变量 `savedCookies` 中。第二个 webdriver
    加载同一个网站，删除自己的 cookies，并添加第一个 webdriver 的 cookies。
- en: Note that the second webdriver must load the website first before the cookies
    are added. This is so Selenium knows which domain the cookies belong to, even
    if the act of loading the website does nothing useful for the scraper.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 注意第二个 webdriver 必须先加载网站，然后再添加 cookies。这样 Selenium 才知道 cookies 属于哪个域，即使加载网站本身对爬虫没有实际用处。
- en: After this is done, the second webdriver should have the same cookies as the
    first. According to Google Analytics, this second webdriver is now identical to
    the first one, and they will be tracked in the same way. If the first webdriver
    was logged in to a site, the second webdriver will be as well.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，第二个 webdriver 应该有与第一个相同的 cookies。根据 Google Analytics 的说法，这第二个 webdriver
    现在与第一个完全相同，并且它们将以相同的方式被跟踪。如果第一个 webdriver 已登录到一个站点，第二个 webdriver 也将是如此。
- en: TLS Fingerprinting
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TLS 指纹识别
- en: 'In the early 2000s, many large tech companies liked to ask prospective programmers
    riddles during job interviews. This mostly fell out of fashion when hiring managers
    realized two things: candidates were sharing and memorizing riddle solutions,
    and the “ability to solve riddles” doesn’t correlate well to job performance.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在 2000 年代初，许多大型科技公司喜欢在面试程序员时提出经典谜题。当招聘经理意识到两件事情时，这种做法大多数已经淡出：候选人共享和记忆谜题解决方案，以及“解决谜题的能力”与工作表现之间的关联并不紧密。
- en: 'However, one of these classic job interview riddles still has value as a metaphor
    for the Transport Layer Security protocol. It goes like this:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些经典的面试谜题之一仍然作为传输层安全协议的隐喻是有价值的。它是这样的：
- en: You need to ship a top secret message to a friend via a dangerous route where
    any unlocked message-containing boxes are intercepted by spies (padlocked message
    boxes, however, are safe if the spies do not have the keys). You place the message
    in a box that can be locked with multiple padlocks. While you have padlocks with
    corresponding keys and your friend also has their own padlocks with corresponding
    keys, none of your friend’s keys work on your padlocks and vice versa. How do
    you ensure that your friend is able to unlock the box on their end and receive
    the message securely?
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要通过危险的路线向朋友发送一条绝密消息，如果解锁的包含消息的盒子被间谍拦截（但是，如果间谍没有钥匙，则带锁的消息盒是安全的）。您将消息放入可以用多个挂锁锁定的盒子中。虽然您有相应钥匙的挂锁和您的朋友也有自己的挂锁及其相应的钥匙，但是您的朋友的钥匙不适用于您的挂锁，反之亦然。如何确保您的朋友能够在其端解锁盒子并安全地接收消息？
- en: Note that shipping a key that unlocks your padlock, even as a separate shipment,
    will not work. The spies will intercept and make copies of these keys and save
    them for later use. Also, shipping a key afterwards will not work (although this
    is where “riddle as metaphor” breaks down a bit) because the spies can make copies
    of the *box itself* and, if a key is sent later, unlock their box copy.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，即使作为单独的运输发送解锁您的挂锁的密钥也不起作用。间谍会拦截并复制这些钥匙并保存以备将来使用。此外，稍后发送钥匙也不起作用（尽管这是“谜题作为隐喻”有点崩溃的地方），因为间谍可以复制*盒子本身*，如果稍后发送一个钥匙，则可以解锁他们的盒子副本。
- en: 'One solution is this: you place your padlock on the box and ship it to your
    friend. Your friend receives the locked box, places their own padlock on it (so
    now the box has two padlocks), and ships it back. You remove your padlock and
    ship it to your friend with only their padlock remaining. Your friend receives
    the box and unlocks it.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一个解决方案是这样的：你把你的挂锁放在盒子上并将其寄给你的朋友。你的朋友收到了锁上的盒子，把他们自己的挂锁放在上面（这样盒子上就有两个挂锁），然后把它寄回来。你移走你的挂锁，只剩下他们的挂锁寄给你的朋友。你的朋友收到盒子并解锁它。
- en: This is, essentially, how secure communications are established over an untrusted
    network. Over a secure communication protocol, like HTTPS, all messages are encrypted
    and decrypted with a key. If an attacker obtains the key (represented by the secret
    message in the riddle), then they are able to read any messages being sent.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，这是如何在不可信网络上建立安全通信的方法。在像HTTPS这样的安全通信协议上，所有消息都使用密钥进行加密和解密。如果攻击者获得了密钥（谜题中的秘密消息表示的），则能够读取发送的任何消息。
- en: So how do you send your friend the key that you’re going to use to encrypt and
    decrypt future messages without that key being intercepted and used by attackers?
    Encrypt it with your own “padlock,” send it to the friend, the friend adds their
    own “padlock,” you remove your “padlock,” and send it back for the friend to “unlock.”
    In this way, the secret key is exchanged securely.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 那么如何将您将用于加密和解密未来消息的密钥发送给朋友，而不会被攻击者拦截和使用？用您自己的“挂锁”加密它，发送给朋友，朋友添加他们自己的“挂锁”，您移除您的“挂锁”，然后发送回来供朋友“解锁”。通过这种方式，秘密密钥安全地交换。
- en: This entire process of “locking,” sending, adding another “lock,” etc., is handled
    by the Transport Layer Security protocol, or TLS. This process of securely establishing
    a mutually known key is called the *TLS handshake*.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这个“锁定”、发送、添加另一个“锁定”等整个过程由传输层安全协议（TLS）处理。安全地建立双方共知的密钥的这一过程称为*TLS握手*。
- en: 'In addition to a establishing a mutually known key, or *master secret*, many
    other things are established during the handshake:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 除了建立一个双方共知的密钥或*主秘密*外，握手期间还建立了许多其他事情：
- en: The highest version of the TLS protocol supported by both parties (which will
    be the version used during the rest of the handshake)
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双方支持的TLS协议的最高版本（在握手的其余部分中将使用的版本）
- en: Which encryption library will be used
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将使用的加密库是哪个
- en: Which compression method will be used
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将使用的压缩方法
- en: The identity of the server, represented by its public certificate
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务器的身份，由其公共证书表示
- en: Verifications that the master secret is working for both parties and that the
    communication is now secure
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证主秘密对双方都有效并且通信现在是安全的
- en: This entire TLS handshake is done every time you contact a new web server and
    any time you need to establish a new HTTP session with that web server (see [Chapter 1](ch01.html#c-1)
    for more information about sessions). The exact messages that are sent by your
    computer for the TLS handshake are determined by the application that is making
    the connection. For example, Chrome may support slightly different TLS versions
    or encryption libraries than Microsoft Edge, so the messages sent by it in the
    TLS handshake will be different.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 每次与新的Web服务器联系和建立新的HTTP会话时，都会执行整个TLS握手过程（有关会话的更多信息，请参见[第1章](ch01.html#c-1)）。由你的计算机发送的确切TLS握手消息由进行连接的应用程序确定。例如，Chrome可能支持略有不同的TLS版本或加密库，因此在TLS握手中发送的消息将不同。
- en: Because the TLS handshake is so lengthy, and the variables involved in its negotiations
    so numerous, clever server administrators realized that the messages sent by clients
    during the TLS handshake were somewhat unique to each application. The messages
    created a sort of *TLS fingerprint* that revealed whether the messages were being
    generated by Chrome, Microsoft Edge, Safari, or even the Python Requests library.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 由于TLS握手过程非常复杂，并涉及的变量很多，聪明的服务器管理员意识到，在TLS握手期间由客户端发送的消息在某种程度上对每个应用程序都是独一无二的。这些消息形成了一种*TLS指纹*，可以显示出消息是由Chrome、Microsoft
    Edge、Safari甚至Python Requests库生成的。
- en: You can see some of the information generated by your TLS handshake by visiting
    (or scraping) [*https://tools.scrapfly.io/api/fp/ja3?extended=1*](https://tools.scrapfly.io/api/fp/ja3?extended=1).
    To make TLS fingerprints more manageable and easy to compare, a hashing method
    called JA3 is often used, the results of which are shown in this API response.
    JA3 hashed fingerprints are catalogued in large databases and used for lookup
    when an application needs to be identified later.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过访问（或爬取）[*https://tools.scrapfly.io/api/fp/ja3?extended=1*](https://tools.scrapfly.io/api/fp/ja3?extended=1)来查看由您的TLS握手生成的一些信息。为了使TLS指纹更易于管理和比较，通常会使用称为JA3的哈希方法，其结果显示在此API响应中。JA3哈希指纹被编入大型数据库，并在以后需要识别应用程序时进行查找。
- en: A TLS fingerprint is a bit like a User-Agent cookie in that it is a long string
    that identifies the application you’re using to send data. But, unlike a User-Agent
    cookie, it’s not easy to modify. In Python, TLS is controlled by the [SSL library](https://github.com/python/cpython/blob/3.11/Lib/ssl.py).
    In theory, perhaps you could rewrite the SSL library. With hard work and dedication,
    you may be able to modify the TLS fingerprint that Python is sending from your
    computer just enough to make the JA3 hash unrecognizable to servers seeking to
    block Python bots. With harder work and more dedication, you might impersonate
    an innocuous browser! Some projects, such as [*https://github.com/lwthiker/curl-impersonate*](https://github.com/lwthiker/curl-impersonate), 
    are seeking to do just that.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: TLS指纹有点像用户代理cookie，它是一个长字符串，用于标识您用于发送数据的应用程序。但与用户代理cookie不同的是，它不容易修改。在Python中，TLS由[SSL库](https://github.com/python/cpython/blob/3.11/Lib/ssl.py)控制。理论上，也许你可以重写SSL库。通过努力和奉献，也许你能够修改Python发送的TLS指纹，使其足够不同，以致于服务器无法识别JA3哈希以阻止Python机器人。通过更加努力和奉献，你甚至可能冒充一个无害的浏览器！一些项目，如[*https://github.com/lwthiker/curl-impersonate*](https://github.com/lwthiker/curl-impersonate)，正在试图做到这一点。
- en: 'Unfortunately the nature of this TLS fingerprint problem means that any impersonation
    libraries will require frequent maintenance by volunteers and are prone to quick
    degradation. Until these projects gain more mainstream traction and reliability,
    there is a much easier way to subvert TLS fingerprinting and blocking: Selenium.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，TLS指纹问题的本质意味着任何仿冒库都需要由志愿者进行频繁维护，并且容易快速退化。在这些项目获得更广泛的关注和可靠性之前，有一种更简单的方法可以规避TLS指纹识别和阻断：Selenium。
- en: Throughout this book, I’ve warned against using an automated browser to solve
    your problems when alternative solutions exist. Browsers use lots of memory, often
    load unnecessary pages, and require extra dependencies that all need upkeep and
    maintenance to keep your web crawler running. But when it comes to TLS fingerprinting,
    it just makes sense to avoid the headache and use a browser.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，我都警告过不要在存在替代解决方案时使用自动化浏览器来解决问题。浏览器使用大量内存，经常加载不必要的页面，并需要额外的依赖项来保持您的网络爬虫运行。但是，当涉及到TLS指纹时，避免麻烦并使用浏览器是很合理的选择。
- en: Keep in mind that your TLS fingerprint will be the same whether you’re using
    the headless or nonheadless version of your browser. So feel free to turn off
    the graphics and use best practices to load only the data that you need—the target
    server isn’t going to know (based on your TLS data, at least)!
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，无论您使用浏览器的无头版本还是非无头版本，您的 TLS 指纹都将是相同的。因此，可以关闭图形并使用最佳实践仅加载您需要的数据——目标服务器不会知道（至少根据您的
    TLS 数据）！
- en: Timing Is Everything
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间至关重要
- en: Some well-protected websites might prevent you from submitting forms or interacting
    with the site if you do it too quickly. Even if these security features aren’t
    in place, downloading lots of information from a website significantly faster
    than a normal human might is a good way to get yourself noticed and blocked.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 一些受到良好保护的网站可能会阻止您提交表单或与网站进行交互，如果您操作得太快的话。即使这些安全功能没有启用，从网站下载大量信息比正常人类快得多也是一个被注意并被封锁的好方法。
- en: 'Therefore, although multithreaded programming might be a great way to load
    pages faster—allowing you to process data in one thread while repeatedly loading
    pages in another—it’s a terrible policy for writing good scrapers. You should
    always try to keep individual page loads and data requests to a minimum. If possible,
    try to space them out by a few seconds, even if you have to add in an extra:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，虽然多线程编程可能是加快页面加载速度的好方法——允许您在一个线程中处理数据，同时在另一个线程中反复加载页面——但对于编写良好的爬虫来说却是一种糟糕的策略。您应该始终尽量减少单个页面加载和数据请求。如果可能的话，尽量将它们间隔几秒钟，即使您必须添加额外的：
- en: '[PRE4]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Whether or not you need this extra few seconds between page loads is often found
    experimentally. Many times I’ve struggled to scrape data from a website, having
    to prove myself as “not a robot” every few minutes (solving the CAPTCHA by hand,
    pasting my newly obtained cookies back over to the scraper so the website viewed
    the scraper itself as having “proven its humanness”), but adding a `time.sleep`
    solved my problems and allowed me to scrape indefinitely.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 是否需要在页面加载之间增加这几秒钟的额外时间通常需要通过实验来确定。我曾多次为了从网站中抓取数据而苦苦挣扎，每隔几分钟就要证明自己“不是机器人”（手动解决
    CAPTCHA，将新获取的 cookie 粘贴回到爬虫中，以便网站将爬虫本身视为“已证明其人类性”），但添加了 `time.sleep` 解决了我的问题，并使我可以无限期地进行抓取。
- en: Sometimes you have to slow down to go fast!
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候你必须放慢脚步才能更快地前进！
- en: Common Form Security Features
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常见的安全特性
- en: Many litmus tests have been used over the years, and continue to be used, with
    varying degrees of success, to separate web scrapers from browser-using humans.
    Although it’s not a big deal if a bot downloads some articles and blog posts that
    were available to the public anyway, it is a big problem if a bot creates thousands
    of user accounts and starts spamming all of your site’s members. Web forms, especially
    forms that deal with account creation and logins, pose a significant threat to
    security and computational overhead if they’re vulnerable to indiscriminate use
    by bots, so it’s in the best interest of many site owners (or at least they think
    it is) to try to limit access to the site.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来一直使用并继续使用许多试金石测试，以不同程度的成功将网络爬虫与使用浏览器的人类分开。虽然如果机器人下载了一些对公众可用的文章和博客文章并不是什么大事，但如果机器人创建了数千个用户帐户并开始向您网站的所有成员发送垃圾邮件，则这是一个大问题。网络表单，特别是处理帐户创建和登录的表单，如果容易受到机器人的不加区分使用，那么对于安全和计算开销来说，它们对许多站点所有者的最大利益（或至少他们认为是）是限制对站点的访问。
- en: These antibot security measures centered on forms and logins can pose a significant
    challenge to web scrapers.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这些以表单和登录为中心的反机器人安全措施可能对网络爬虫构成重大挑战。
- en: Keep in mind that this is only a partial overview of some of the security measures
    you might encounter when creating automated bots for these forms. Review [Chapter 16](ch16.html#c-16),
    on dealing with CAPTCHAs and image processing, as well as [Chapter 13](ch13.html#c-13),
    on dealing with headers and IP addresses, for more information on dealing with
    well-protected forms.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，这只是创建这些表单的自动化机器人时可能遇到的一些安全措施的部分概述。请参阅[第16章](ch16.html#c-16)，有关处理 CAPTCHA
    和图像处理的内容，以及[第13章](ch13.html#c-13)，有关处理标头和 IP 地址的内容，获取有关处理受良好保护的表单的更多信息。
- en: Hidden Input Field Values
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 隐藏的输入字段值
- en: '“Hidden” fields in HTML forms allow the value contained in the field to be
    viewable by the browser but invisible to the user (unless they look at the site’s
    source code). With the increase in use of cookies to store variables and pass
    them around on websites, hidden fields fell out of a favor for a while before
    another excellent purpose was discovered for them: preventing scrapers from submitting
    forms.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: HTML表单中的“隐藏”字段允许浏览器查看字段中包含的值，但用户看不到它们（除非他们查看站点的源代码）。随着使用cookie在网站上存储变量并在之间传递它们的增加，隐藏字段在一段时间内不再受欢迎，然后发现了它们的另一个出色用途：防止网页抓取程序提交表单。
- en: '[Figure 17-1](#facebook_login_form) shows an example of these hidden fields
    at work on a LinkedIn login page. Although the form has only three visible fields
    (Username, Password, and a Submit button), it conveys a great deal of information
    to the server behind the scenes.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 17-1](#facebook_login_form)显示了LinkedIn登录页面上这些隐藏字段的示例。虽然表单只有三个可见字段（用户名、密码和提交按钮），但它向服务器传递了大量信息。'
- en: '![Alt Text](assets/wsp3_1701.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![Alt Text](assets/wsp3_1701.png)'
- en: Figure 17-1\. The LinkedIn login form has a few hidden fields
  id: totrans-81
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 17-1\. LinkedIn登录表单有几个隐藏字段。
- en: 'Hidden fields are used to prevent web scraping in two main ways: a field can
    be populated with a randomly generated variable on the form page that the server
    is expecting to be posted to the form-processing page. If this value is not present
    in the form, the server can reasonably assume that the submission did not originate
    organically from the form page but was posted by a bot directly to the processing
    page. The best way to get around this measure is to scrape the form page first,
    collect the randomly generated variable, and then post to the processing page
    from there.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏字段主要用于防止Web抓取程序的两种主要方式：字段可以在表单页面上使用随机生成的变量填充，服务器期望在处理页面上提交该变量。如果表单中没有这个值，服务器可以合理地认为提交不是源自表单页面的有机操作，而是直接由机器人发布到处理页面。绕过此措施的最佳方法是首先抓取表单页面，收集随机生成的变量，然后从那里发布到处理页面。
- en: The second method is a “honeypot” of sorts. If a form contains a hidden field
    with an innocuous name, such as Username or Email Address, a poorly written bot
    might fill out the field and attempt to submit it, regardless of whether it is
    hidden to the user. Any hidden fields with actual values (or values that are different
    from their defaults on the form submission page) should be disregarded, and the
    user may even be blocked from the site.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法有点像“蜜罐”。如果表单包含一个隐藏字段，其名称看似无害，比如用户名或电子邮件地址，那么一个编写不良的机器人可能会填写该字段并尝试提交它，而不管它对用户是否隐藏。任何带有实际值的隐藏字段（或者在表单提交页面上与其默认值不同的值）都应被忽略，甚至可能会阻止用户访问该站点。
- en: 'In short: it is sometimes necessary to check the page that the form is on to
    see whether you missed anything that the server might be expecting. If you see
    several hidden fields, often with large, randomly generated string variables,
    the web server likely will be checking for their existence on form submission.
    In addition, there might be other checks to ensure that the form variables have
    been used only once, are recently generated (this eliminates the possibility of
    simply storing them in a script and using them over and over again over time),
    or both.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之：有时需要检查表单所在的页面，看看服务器可能期望您漏掉的任何内容。如果看到几个隐藏字段，通常带有大量随机生成的字符串变量，那么Web服务器可能会在表单提交时检查它们的存在。此外，可能还会有其他检查，以确保表单变量仅被使用一次，是最近生成的（这消除了仅仅将它们存储在脚本中并随时使用的可能性），或两者兼而有之。
- en: Avoiding Honeypots
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 避免蜜罐
- en: Although CSS for the most part makes life extremely easy when it comes to differentiating
    useful information from nonuseful information (e.g., by reading the `id` and `class`
    tags), it can occasionally be problematic for web scrapers. If a field on a web
    form is hidden from a user via CSS, it is reasonable to assume that the average
    user visiting the site will not be able to fill it out because it doesn’t show
    up in the browser. If the form *is* populated, there is likely a bot at work and
    the post will be discarded.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管CSS在区分有用信息和无用信息（例如通过读取`id`和`class`标签）方面大多数情况下使生活变得极为简单，但在Web抓取程序方面有时可能会出现问题。如果通过CSS从用户隐藏网页上的字段，可以合理地假设平均访问该网站的用户将无法填写它，因为它在浏览器中不显示。如果表单被填充，很可能是有机器人在操作，并且该帖子将被丢弃。
- en: This applies not only to forms but to links, images, files, and any other item
    on the site that can be read by a bot but is hidden from the average user visiting
    the site through a browser. A page visit to a “hidden” link on a site can easily
    trigger a server-side script that will block the user’s IP address, log that user
    out of the site, or take some other action to prevent further access. In fact,
    many business models have been based on exactly this concept.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这不仅适用于表单，还适用于链接、图像、文件和站点上的任何其他项目，这些项目可以被机器人读取，但对于通过浏览器访问站点的普通用户而言是隐藏的。访问站点上的“隐藏”链接可能很容易触发一个服务器端脚本，该脚本将阻止用户的IP地址，将该用户从站点注销，或者采取其他措施防止进一步访问。事实上，许多商业模型都是基于这个概念的。
- en: 'Take, for example, the page located at [*http://pythonscraping.com/pages/itsatrap.html*](http://pythonscraping.com/pages/itsatrap.html).
    This page contains two links, one hidden by CSS and another visible. In addition,
    it contains a form with two hidden fields:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，位于[*http://pythonscraping.com/pages/itsatrap.html*](http://pythonscraping.com/pages/itsatrap.html)的页面。这个页面包含两个链接，一个被CSS隐藏，另一个可见。此外，它包含一个带有两个隐藏字段的表单：
- en: '[PRE5]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'These three elements are hidden from the user in three ways:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个元素以三种方式对用户隐藏：
- en: The first link is hidden with a simple CSS `display:none` attribute.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个链接使用简单的CSS `display:none`属性隐藏。
- en: The phone field is a hidden input field.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电话字段是一个隐藏的输入字段。
- en: The email field is hidden by moving it 50,000 pixels to the right (presumably
    off the screen of everyone’s monitors) and hiding the telltale scroll bar.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电子邮件字段通过将其向右移动50,000像素（可能超出所有人的显示器屏幕）并隐藏显眼的滚动条来隐藏它。
- en: Fortunately, because Selenium renders the pages it visits, it is able to distinguish
    between elements that are visually present on the page and those that aren’t.
    Whether the element is present on the page can be determined by the `is_displayed()`
    function.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，因为Selenium呈现它访问的页面，它能够区分页面上视觉上存在的元素和不存在的元素。元素是否存在于页面上可以通过`is_displayed()`函数确定。
- en: 'For example, the following code retrieves the previously described page and
    looks for hidden links and form input fields:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，以下代码检索先前描述的页面，并查找隐藏链接和表单输入字段：
- en: '[PRE6]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Selenium catches each hidden field, producing the following output:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Selenium捕获每个隐藏字段，产生以下输出：
- en: '[PRE7]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Although you probably don’t want to visit any hidden links you find, you will
    want to make sure that you submit any prepopulated hidden form values (or have
    Selenium submit them for you) with the rest of the form. To sum up, it is dangerous
    to simply ignore hidden fields, although you must be careful when interacting
    with them.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然你可能不想访问你发现的任何隐藏链接，但你会想确保你提交了任何预填充的隐藏表单值（或者让Selenium为你提交），并与其他表单一起提交。总之，简单忽略隐藏字段是危险的，尽管在与它们交互时必须小心。
- en: The Human Checklist
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人类检查清单
- en: 'There’s a lot of information in this chapter, and indeed in this book, about
    how to build a scraper that looks less like a scraper and more like a human. If
    you keep getting blocked by websites and you don’t know why, here’s a checklist
    you can use to remedy the problem:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这一章节以及这本书中有很多关于如何构建一个看起来不像爬虫而更像人类的爬虫的信息。如果你不断被网站阻止而又不知道原因，这里有一个你可以用来解决问题的检查清单：
- en: First, if the page you are receiving from the web server is blank, missing information,
    or is otherwise not what you expect (or have seen in your own browser), it is
    likely caused by JavaScript being executed on the site to create the page. Review
    [Chapter 14](ch14.html#c-14).
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，如果你从Web服务器接收的页面是空白的、缺少信息的，或者与你期望的（或者在你自己的浏览器中看到的）不同，很可能是由于JavaScript在站点上执行以创建页面。查看[第14章](ch14.html#c-14)。
- en: If you are submitting a form or making a `POST` request to a website, check
    the page to make sure that everything the website is expecting you to submit is
    being submitted and in the correct format. Use a tool such as Chrome’s Inspector
    panel to view an actual `POST` request sent to the site to make sure you have
    everything, and that an “organic” request looks the same as the ones your bot
    is sending.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你正在向网站提交表单或进行`POST`请求，请检查页面以确保网站期望你提交的一切都被提交并且格式正确。使用诸如Chrome的Inspector面板之类的工具查看发送到网站的实际`POST`请求，确保你拥有一切，并且“有机”的请求看起来与你的机器人发送的请求相同。
- en: If you are trying to log in to a site and can’t make the login “stick,” or the
    website is experiencing other strange “state” behavior, check your cookies. Make
    sure that cookies are being persisted correctly between each page load and that
    your cookies are sent to the site for every request.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您尝试登录网站但无法保持登录状态，或者网站出现其他奇怪的“状态”行为，请检查您的 cookies。确保 cookies 在每次页面加载之间都被正确保存，并且您的
    cookies 被发送到该网站以处理每个请求。
- en: 'If you are getting HTTP errors from the client, especially 403 Forbidden errors,
    it might indicate that the website has identified your IP address as a bot and
    is unwilling to accept any more requests. You will need to either wait until your
    IP address is removed from the list or obtain a new IP address (either move to
    a Starbucks or see [Chapter 20](ch20.html#c-20)). To make sure you don’t get blocked
    again, try the following:'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您从客户端收到 HTTP 错误，特别是 403 Forbidden 错误，这可能表示网站已将您的 IP 地址识别为机器人，不愿再接受任何请求。您需要等待直到您的
    IP 地址从列表中移除，或者获取一个新的 IP 地址（要么去星巴克，要么参考[第 20 章](ch20.html#c-20)）。为了确保您不会再次被阻止，请尝试以下方法：
- en: 'Make sure that your scrapers aren’t moving through the site too quickly. Fast
    scraping is a bad practice that places a heavy burden on the web administrator’s
    servers, can land you in legal trouble, and is the number one cause of scrapers
    getting blacklisted. Add delays to your scrapers and let them run overnight. Remember:
    being in a rush to write programs or gather data is a sign of bad project management;
    plan ahead to avoid messes like this in the first place.'
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保您的抓取器不要过快地浏览网站。快速抓取是一种不良实践，会给网站管理员的服务器带来沉重负担，可能会让您陷入法律麻烦，并且是抓取器被列入黑名单的头号原因。为您的抓取器添加延迟，并让它们在夜间运行。记住：匆忙编写程序或收集数据是糟糕项目管理的表现；提前计划以避免出现这种混乱。
- en: 'The obvious one: change your headers! Some sites will block anything that advertises
    itself as a scraper. Copy your own browser’s headers if you’re unsure about what
    some reasonable header values are.'
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最明显的一种：更改您的 headers！一些网站会阻止任何宣称自己是抓取器的内容。如果您对一些合理的 header 值感到不确定，请复制您自己浏览器的
    headers。
- en: Make sure you’re not clicking on or accessing anything that a human normally
    would not be able to (refer to [“Avoiding Honeypots”](#avoiding_honeypots) for
    more information).
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保您不要点击或访问任何人类通常无法访问的内容（更多信息请参考[“避免蜜罐”](#avoiding_honeypots)）。
- en: If you find yourself jumping through a lot of difficult hoops to gain access,
    consider contacting the website administrator to let them know what you’re doing.
    Try emailing *webmaster@<domain name>* or *admin@<domain name>* for permission
    to use your scrapers. Admins are people too, and you might be surprised at how
    amenable they can be to sharing their data.
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您发现自己需要跨越许多困难障碍才能获得访问权限，请考虑联系网站管理员，让他们知道您的操作。尝试发送电子邮件至*webmaster@<domain name>*或*admin@<domain
    name>*，请求使用您的抓取器。管理员也是人，您可能会惊讶地发现他们对分享数据的态度是多么的乐意。
