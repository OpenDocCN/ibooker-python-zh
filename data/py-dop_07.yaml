- en: Chapter 7\. Monitoring and Logging
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第七章。监控和日志记录
- en: When Noah worked in startups in San Francisco, he used his lunch break to exercise.
    He would play basketball, run up to Coit Tower, or practice Brazilian Jiu-Jitsu.
    Most of the startups Noah worked at would have a catered lunch.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当诺亚在旧金山的初创公司工作时，他利用午餐时间锻炼。他会打篮球，跑到科伊特塔上，或者练巴西柔术。诺亚工作过的大多数初创公司都提供午餐。
- en: He discovered a very unusual pattern coming back from lunch. There was never
    anything unhealthy left to eat. The leftovers were often full salads, fruit, vegetables,
    or healthy lean meats. The hordes of startup workers ate all of the unhealthy
    options while he was exercising, leaving zero temptation to eat bad food. There
    is something to be said for not following the crowd.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 他发现一个非常不寻常的模式，在午饭后回来。从来没有剩下任何不健康的东西可吃。剩下的东西通常是完整的沙拉，水果，蔬菜或健康的瘦肉。一群群的初创公司工作人员在他锻炼时吃光了所有不健康的选择，不留一丝诱惑吃坏的食物。不随波逐流确实有一番道理。
- en: Likewise, the easy path is to ignore operations when developing machine learning
    models, mobile apps, and web apps. Ignoring operations is so typical it is like
    eating the chips, soda, and ice cream at the catered lunch. Being normal isn’t
    necessarily preferred, though. In this chapter, the “salad and lean meats” approach
    to software development is described.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，当开发机器学习模型，移动应用程序和Web应用程序时，忽视操作是一条容易的路。忽视操作是如此典型，就像在提供午餐时吃薯片，苏打水和冰淇淋一样。虽然成为正常人并不一定是首选。在本章中，描述了软件开发的“沙拉和瘦肉”方法。
- en: Key Concepts in Building Reliable Systems
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建可靠系统的关键概念
- en: Having built companies for a while, it’s fun to look at what worked on the software
    engineering portion versus what didn’t. One of the best antipatterns out there
    is “trust me.” Any sane DevOps professional will not trust humans. They are flawed,
    make emotional mistakes, and can destroy entire companies on a whim. Especially
    if they are the company’s founder.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 经过一段时间的公司建设，看看在软件工程部分起作用和不起作用的东西是有趣的。其中一个最好的反模式是“相信我”。任何理智的DevOps专业人员都不会相信人类。他们是有缺陷的，会犯情感上的错误，并且可以随心所欲地摧毁整个公司。特别是如果他们是公司的创始人。
- en: Instead of a hierarchy based on complete nonsense, a better approach to building
    reliable systems is to build them piece by piece. In additional, when creating
    the platform, failure should be expected regularly. The only thing that will affect
    this truism is if a powerful person is involved in building the architecture.
    In that case, this truism will be exponentially increased.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 不是基于完全胡言乱语的层次结构，构建可靠系统的更好方法是逐步构建它们。此外，在创建平台时，应该经常预期失败。唯一会影响这个真理的事情是如果有一个有权势的人参与建立架构。在那种情况下，这个真理将呈指数增长。
- en: You may have heard of the chaos monkey from Netflix, but why bother with that?
    Instead, let the founders of your company, the CTO, or the VP of Engineering do
    drive-by coding and second-guess your architecture and codebase. The human chaos
    monkey will run circles around Netflix. Better yet, let them compile jar files
    in the middle of production outages and put them on nodes one by one, via SSH,
    all the while yelling, “This will do the trick!” In this way, the harmonic mean
    of chaos and ego is achieved.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能听说过Netflix的混沌猴，但为什么要费心呢？相反，让你公司的创始人，首席技术官或工程副总裁做随意编码并对你的架构和代码库提出质疑。人类混沌猴将在Netflix周围打转。更好的是，让他们在生产中断期间编译jar文件，并逐个将它们放在节点上，通过SSH，同时喊着，“这会奏效的！”通过这种方式，混沌和自我之间的谐波均值被实现。
- en: What is the action item for a sane DevOps professional? Automation is greater
    than hierarchy. The only solution to the chaos of startups is automation, skepticism,
    humility, and immutable DevOps principles.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对于理智的DevOps专业人员来说，行动项目是什么？自动化大于层次结构。解决初创公司混乱的唯一方法是自动化，怀疑，谦卑和不可变的DevOps原则。
- en: Immutable DevOps Principles
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不可变的DevOps原则
- en: It is hard to imagine a better place to start to build a reliable system than
    this immutable principle. If the CTO is building Java `.jar` files from a laptop
    to fix production fires, you should just quit your job. There is nothing that
    will save your company. We should know—we’ve been there!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 很难想象有比这个不可改变的原则更好的地方来开始构建可靠的系统。如果首席技术官正在从笔记本电脑构建Java的`.jar`文件来解决生产问题，你应该辞职。没有什么能够拯救你的公司。我们应该知道——我们曾经在那里！
- en: No matter how smart/powerful/charismatic/creative/rich a person is, if they
    are manually applying critical changes in a crisis to your software platform,
    you are already dead. You just don’t know it yet. The alternative to this monstrous
    existence is automation.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 无论一个人有多聪明/强大/有魅力/有创意/有钱，如果他们在软件平台危机中手动应用关键更改，你已经死了。只是你还不知道。摆脱这种可怕状态的替代方案是自动化。
- en: 'Humans cannot be involved in deploying software in the long term. This is the
    #1 antipattern that exists in the software industry. It is essentially a back
    door for hooligans to wreak havoc on your platform. Instead, deploying software,
    testing software, and building software needs to be 100% automated.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 长期来看，人类不能参与软件部署。这是软件行业存在的头号反模式。它本质上是暴徒对你的平台造成严重破坏的后门。相反，部署软件、测试软件和构建软件需要100%自动化。
- en: The most significant initial impact you can have in a company is to set up continuous
    integration and continuous delivery. Everything else pales in comparison.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在公司中，你可以产生最显著的初始影响是建立持续集成和持续交付。其他所有事情都相形见绌。
- en: Centralized Logging
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集中日志记录
- en: Logging follows close behind automation in importance. In large-scale, distributed
    systems, logging isn’t optional. Special attention must be paid to logging at
    both the application level and the environment level.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在自动化之后，日志记录在重要性上紧随其后。在大规模分布式系统中，日志记录不是可选的。必须特别关注应用程序级别和环境级别的日志记录。
- en: For example, exceptions should always be sent to the centralized logging system.
    On the other hand, while developing software, it is often a good idea to create
    debug logging instead of print statements. Why do this? Many hours are spent developing
    heuristics to debug the source code. Why not capture that so it can be turned
    on if a problem surfaces in production again?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，异常应始终发送到集中日志记录系统。另一方面，在开发软件时，通常创建调试日志而不是打印语句是一个好主意。为什么这样做？花费了很多时间开发调试源代码的启发式算法。为什么不捕获它，以便在生产中再次出现问题时可以打开它？
- en: The trick here is logging levels. By creating debug log levels that only appear
    in the nonproduction environment, it allows the logic of debugging to be kept
    in the source code. Likewise, instead of having overly verbose logs appear in
    production and add confusion, they can be toggled on and off.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的诀窍在于日志记录级别。通过创建仅出现在非生产环境中的调试日志级别，可以将调试逻辑保留在源代码中。同样，不要让过于冗长的日志在生产中出现并引起混乱，可以随时开启或关闭它们。
- en: 'An example of logging in large-scale distributed systems is what [Ceph](https://ceph.com)
    uses: daemons can have up to 20 debug levels! All of these levels are handled
    in code, allowing the system to fine-tune the amount of logging. Ceph furthers
    this strategy by being able to restrict the amount of logging per daemon. The
    system has a few daemons, and the logging can be increased for one or all of them.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在大规模分布式系统中，日志记录的一个例子是[Ceph](https://ceph.com)的使用：守护程序可以拥有高达20个调试级别！所有这些级别都在代码中处理，允许系统精细调节日志记录量。Ceph通过能够限制每个守护程序的日志记录量进一步推动了这一策略。系统有几个守护程序，可以增加一个或所有守护程序的日志记录量。
- en: 'Case Study: Production Database Kills Hard Drives'
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 案例研究：生产数据库损坏硬盘
- en: Another vital strategy for logging is solving scalability. Once the application
    is large enough, it might not be feasible to store all logs in a file anymore.
    Alfredo was once tasked to debug a problem with the primary database of an extensive
    web application that hosted about one hundred newspaper, radio station, and television
    station sites. These sites generated lots of traffic and produced massive amounts
    of logs. So much log output was created that the logging for PostgreSQL was set
    to a minimum, and he couldn’t debug the problem because it required raising the
    log level. If the log level was raised, the application would stop working from
    the intense I/O generated. Every day at around five o’clock in the morning, the
    database load spiked. It had gotten progressively worse.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 日志记录的另一个关键策略是解决可扩展性问题。一旦应用程序足够庞大，可能不再可行将所有日志存储在文件中。阿尔弗雷多曾经被指派解决一个广泛网络应用的主数据库问题，这个应用托管大约一百家报纸、广播电台和电视台的站点。这些站点产生了大量流量并生成了大量日志。产生了如此多的日志输出，以至于PostgreSQL的日志记录被设置到最低，他无法调试问题，因为需要提高日志级别。如果提高日志级别，应用程序将因所产生的强烈I/O而停止工作。每天早上五点左右，数据库负载就会急剧上升。情况越来越糟。
- en: 'The database administrators were opposed to raising the log levels to see the
    most expensive queries (PostgreSQL can include query information in the logs)
    for a whole day, so we compromised: fifteen minutes at around five o’clock in
    the morning. As soon as he was able to get those logs, Alfredo immediately got
    to rate the slowest queries and how often they were run. There was a clear winner:
    a `SELECT *` query that was taking so long to complete that the fifteen-minute
    window was not enough to capture its runtime. The application didn’t do any queries
    that selected everything from any table; what could it be?'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库管理员反对提高日志级别以查看最昂贵的查询（PostgreSQL可以在日志中包含查询信息）整整一天，因此我们妥协了：每天早晨五点左右十五分钟。一旦能够获取这些日志，阿尔弗雷多立即开始评估最慢的查询及其运行频率。有一个明显的优胜者：一个耗时如此之长的`SELECT
    *`查询，以至于十五分钟的时间窗口无法捕获其运行时间。应用程序并未对任何表进行全表查询；那么问题出在哪里呢？
- en: 'After much persuasion, we got access to the database server. If the spike in
    load happens at around five o’clock in the morning *every single day*, is it possible
    that there is some recurrent script? We looked into `crontab` (the program that
    keeps track of what runs at given intervals), which showed a suspicious script:
    *backup.sh*. The contents of the script had a few SQL statements that included
    several `SELECT *`. The database administrators were using this to back up the
    primary database, and as the size of the database grew, so did the load until
    it was no longer tolerable. The solution? Stop using the script, and back up one
    of the four secondary (replica) databases.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 经过多次劝说，我们终于获得了对数据库服务器的访问权限。如果每天清晨五点左右出现负载激增，是否可能存在某种定期脚本？我们调查了`crontab`（用于记录定时运行任务的程序），发现了一个可疑的脚本：*backup.sh*。脚本内容包含几条SQL语句，其中包括多个`SELECT
    *`。数据库管理员使用此脚本备份主数据库，随着数据库规模的增长，负载也随之增加，直至无法忍受。解决方案？停止使用此脚本，开始备份四个次要（副本）数据库中的一个。
- en: That saved the backup problem, but it didn’t help the inability to have access
    to logs. Thinking ahead on distributing logging output is the right approach.
    Tools like [`rsyslog`](https://www.rsyslog.com) are meant to solve this, and if
    added from the start, it can save you from being unable to solve a production
    outage.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这解决了备份问题，但并没有解决无法访问日志的问题。提前考虑分发日志输出是正确的做法。像[`rsyslog`](https://www.rsyslog.com)这样的工具就是为解决这类问题而设计的，如果从一开始就添加，可以避免在生产中遇到问题时束手无策。
- en: Did You Build It or Buy It?
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 是否自建还是购买？
- en: It is incredible how much *vendor lock-in* gets press. Vendor lock-in, though,
    is in the eye of the beholder. You can pretty much throw a rock in any direction
    in downtown San Francisco and hit someone preaching the evils of vendor lock-in.
    When you dig a little deeper though, you wonder what their alternative is.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 令人难以置信的是*供应商锁定*如何备受关注。然而，供应商锁定是因人而异的。在旧金山市中心，你几乎可以随处一掷石头就能碰到有人大声疾呼供应商锁定的罪恶。然而，深入挖掘之后，你会想知道他们的替代方案是什么。
- en: In Economics, there is a principle called comparative advantage. In a nutshell,
    it means there is an economic benefit to focusing on what you are best at and
    outsourcing other tasks to other people. With the cloud, in particular, there
    is continuous improvement, and end-users benefit from that improvement without
    an aggregated cost—and most of the time with less complexity than before.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在经济学中，有一个叫做比较优势的原则。简而言之，它意味着将精力集中在自己最擅长的事务上，并将其他任务外包给其他人是经济上有利的。尤其是在云计算领域，不断的改进使最终用户从中受益，而且通常比以前更少复杂。
- en: 'Unless your company is operating at the scale of one of the giants in technology,
    there is almost no way to implement, maintain, and improve a private cloud and
    be able to save money and improve the business at the same time. For example,
    in 2017, Amazon released the ability to deploy multimaster databases with automatic
    failover across multiple *Availability Zones*. As someone who has attempted this,
    I can say that it is borderline impossible and tremendously hard to add the ability
    of automatic failover in such a scenario. One of the golden questions to consider
    regarding outsourcing is: *“Is this a core competency of the business?”* A company
    that runs its own mail server but its core competency is selling car parts is
    playing with fire and probably already losing money.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 除非你的公司运营规模达到技术巨头之一，否则几乎不可能实施、维护和改进私有云，并能在同时节省成本和提升业务。例如，2017年，亚马逊发布了跨多个*可用区*具有自动故障切换功能的多主数据库部署能力。作为曾尝试过这一点的人，我可以说这几乎是不可能的，而在这种情况下添加自动故障切换功能非常困难。在考虑外包时要考虑的一个关键问题是：“这是否是业务的核心竞争力？”一个运营自己邮件服务器但核心竞争力是销售汽车零件的公司，正在玩火并且可能已经在亏钱。
- en: Fault Tolerance
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 容错性
- en: Fault tolerance is a fascinating topic but it can be very confusing. What is
    meant by fault tolerance, and how can you achieve it? A great place to learn more
    about fault tolerance is to read as [many white papers from AWS](https://oreil.ly/zYuls)
    as you can.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 容错性是一个迷人的主题，但它可能非常令人困惑。容错性的含义是什么，如何实现？学习更多关于容错性的信息的好地方是阅读尽可能多的来自AWS的[白皮书](https://oreil.ly/zYuls)。
- en: 'When designing fault-tolerant systems, it’s useful to start by trying to answer
    the following question: when this service goes down, what can I implement to eliminate
    (or reduce) manual interaction? Nobody likes getting notifications that a critical
    system is down, especially when this means numerous steps to recover, even less
    when it takes communication with other services to ensure everything is back to
    normal. Note that the question is not being framed as an improbable event, it
    explicitly acknowledges that the service will go down and that some work will
    needed done to get it up and running.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计容错系统时，最好从尝试回答以下问题开始：当此服务宕机时，我可以实施什么来消除（或减少）手动交互？没有人喜欢收到关键系统宕机的通知，尤其是这意味着需要多个步骤来恢复，更不用说还需要与其他服务进行沟通，确保一切恢复正常。需要注意的是，这个问题并不是将宕机视为不太可能发生的事件，而是明确承认该服务会宕机，并且需要进行一些工作来使其恢复运行。
- en: 'A while ago, a full redesign of a complex build system was planned. The build
    system did several things, most of them related to packaging and releasing the
    software: dependencies had to be in check, `make` and other tooling built binaries,
    RPM and Debian packages were produced, and repositories for different Linux distributions
    (like CentOS, Debian, and Ubuntu) were created and hosted. The main requirement
    for this build system was that it had to be fast.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 不久之前，计划对复杂的构建系统进行全面重新设计。该构建系统完成了多项任务，其中大部分与软件打包和发布相关：必须检查依赖关系，使用`make`和其他工具构建二进制文件，生成RPM和Debian软件包，并创建和托管不同Linux发行版（如CentOS、Debian和Ubuntu）的软件库。这个构建系统的主要要求是速度快。
- en: Although speed was one of the primary objectives, when designing a system that
    involves several steps and different components, it is useful to address the known
    pain points and try hard to prevent new ones. There are always going to be unknowns
    in large systems, but using the right strategies for logging (and logging aggregation),
    monitoring, and recovery are crucial.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管速度是主要目标之一，但在设计涉及多个步骤和不同组件的系统时，解决已知的痛点并努力防止新问题的出现非常有用。大型系统中总会存在未知因素，但使用适当的日志记录（以及日志聚合）、监控和恢复策略至关重要。
- en: 'Turning our attention back to the build system, one of the problems was that
    the machines that created the repositories were somewhat complex: an HTTP API
    received packages for a particular project at a specific version, and repositories
    were generated automatically. This process involved a database, a RabbitMQ service
    for asynchronous task handling, and massive amounts of storage to keep repositories
    around, served by Nginx. Finally, some status reporting would be sent to a central
    dashboard, so developers could check where in the build process their branch was.
    It was *crucial* to design everything around the possibility of this service going
    down.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在回到构建系统，其中一个问题是创建仓库的机器有些复杂：一个 HTTP API 接收特定项目特定版本的软件包，并自动生成仓库。 这个过程涉及数据库、RabbitMQ
    服务用于异步任务处理，以及大量的存储空间来保存仓库，由 Nginx 提供服务。 最后，一些状态报告将发送到中央仪表板，以便开发人员可以查看其分支在构建过程中的位置。
    设计所有东西都围绕这个服务可能出现故障的可能性是*至关重要*的。
- en: 'A big note was added to the whiteboard that read: “ERROR: the repo service
    is down because the disk is full.” The task was not necessarily to prevent a disk
    being full, but to create a system that can continue working with a full disk,
    and when the problem is solved, almost no effort is required to put it back into
    the system. The “full disk” error was a fictional one that could’ve been anything,
    like RabbitMQ not running, or a DNS problem, but exemplified the task at hand
    perfectly.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在白板上添加了一张大字条，上面写着：“错误：仓库服务因磁盘已满而停止。” 任务并不是要防止磁盘满，而是要创建一个能在磁盘满的情况下继续工作，并在问题解决后几乎不费力就能重新将其纳入系统的系统。
    “磁盘已满”错误是一个虚构的错误，可能是任何事情，比如 RabbitMQ 没有运行或者 DNS 问题，但它完美地说明了当前的任务。
- en: It is difficult to understand the importance of monitoring, logging, and sound
    design patterns until a piece of the puzzle doesn’t work, and it is impossible
    to determine the *why* and the *how*. You need to know why it went down so that
    prevention steps can be instrumented (alerts, monitoring, and self-recovery) to
    avoid this problem from repeating itself in the future.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 直到某个关键部分失效时，理解监控、日志记录和良好设计模式的重要性才变得困难，并且几乎无法确定*为什么*以及*如何*。 你需要知道为什么它会崩溃，以便可以采取预防措施（警报、监控和自我恢复）来避免将来出现类似问题。
- en: 'To allow this system to continue to work, we split the load into five machines,
    all of them equal and doing the same work: creating and hosting repositories.
    The nodes creating the binaries would query an API for a healthy repo machine,
    which in turn would send an HTTP request to query the `/health/` endpoint of the
    next build server on its list. If the server reported healthy, the binaries would
    go there; otherwise, the API would pick the next one on the list. If a node failed
    a health check three times in a row, it was put out of rotation. All a sysadmin
    had to do to get it back into rotation after a fix was to restart the repository
    service. (The repository service had a self-health assessment that if successful
    would notify the API that it was ready to do work.)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这个系统能够继续工作，我们将负载分成了五台机器，它们都是相同的，做着相同的工作：创建和托管仓库。 生成二进制文件的节点将查询一个健康的仓库机器的
    API，然后该机器会发送一个 HTTP 请求来查询其列表中下一个构建服务器的 `/health/` 终端。 如果服务器报告健康，则二进制文件会发送到那里；否则，API
    将选择列表中的下一个服务器。 如果一个节点连续三次未通过健康检查，它将被移出轮换。 系统管理员只需在修复后重新启动仓库服务即可将其重新纳入轮换。（仓库服务有一个自我健康评估，如果成功将通知
    API 准备好进行工作。）
- en: Although the implementation was not bulletproof (work was still needed to get
    a server up, and notifications weren’t always accurate), it had a tremendous impact
    on the maintenance of the service when restoration was required, and kept everything
    still functioning in a degraded state. This is what fault tolerance is all about!
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然实现并非完全可靠（仍需努力使服务器恢复运行，并且通知并不总是准确），但在需要恢复服务时对服务维护产生了巨大影响，并保持所有内容在降级状态下继续运行。
    这就是容错性的全部意义所在！
- en: Monitoring
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控
- en: Monitoring is one of those things where one can do almost nothing and still
    claim that a monitoring system is in place (as an intern, Alfredo once used a
    `curl` cronjob to check a production website), but can grow to be so complicated
    that the production environment seems nimble in comparison. When done right, monitoring
    and reporting, in general, can help answer the most difficult questions of a production
    life cycle. It’s crucial to have and difficult to get right. This is why there
    are many companies that specialize in monitoring, alerting, and metric visualization.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 监控是这样一种事情，你几乎什么都不做也可以声称已经建立了一个监控系统（作为实习生，Alfredo 曾经使用一个`curl`定时任务来检查一个生产网站），但是它可能变得非常复杂，以至于生产环境看起来像是灵活的。当监控和报告做得正确时，通常可以帮助回答生产生命周期中最困难的问题。拥有它是至关重要的，但要做到这一点却很困难。这就是为什么有许多公司专门从事监控、警报和指标可视化的原因。
- en: 'At its core, there are two paradigms most services fall into: pull and push.
    This chapter will cover Prometheus (pull) and Graphite with StatsD (push). Understanding
    when one is a better choice than the other, and the caveats, is useful when adding
    monitoring to environments. Most importantly, it’s practical to know both and
    have the ability to deploy whichever services that will work best in a given scenario.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，大多数服务遵循两种范式：拉取和推送。本章将涵盖 Prometheus（拉取）和 Graphite 与 StatsD（推送）。了解在何时选择其中一种更为合适以及其中的注意事项，在为环境添加监控时是非常实用的。更重要的是，了解两者并具备根据特定场景选择最佳服务的能力，这一点非常实际。
- en: Reliable time-series software has to withstand incredibly high rates of incoming
    transactional information, be able to store that information, correlate it with
    time, support querying, and offer a graphical interface that can be customized
    with filters and queries. In essence, it must almost be like a high-performing
    database, but specific to time, data manipulation, and visualization.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 可靠的时间序列软件必须能够承受极高的事务信息输入速率，能够存储这些信息，与时间相关联，支持查询，并提供一个可以根据过滤器和查询自定义的图形界面。本质上，它几乎必须像一个高性能数据库，但专门用于时间、数据操作和可视化。
- en: Graphite
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Graphite
- en: 'Graphite is a data store for numerical time-based data: it keeps numeric information
    that correlates to the time it was captured and saves it according to customizable
    rules. It offers a *very powerful* API that can be queried for information about
    its data, with time ranges, and also apply *functions* that can transform or perform
    computations in data.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Graphite 是一个用于数字时间数据的数据存储：它保存与捕获时间相关的数值信息，并根据可自定义的规则保存这些信息。它提供了一个*非常强大*的 API，可以查询有关其数据的信息，包括时间范围，并且还可以应用*函数*对数据进行转换或计算。
- en: An important aspect of [Graphite](https://oreil.ly/-0YEs) is that it *does not
    collect data*; instead, it concentrates on its API and ability to handle immense
    amounts of data for periods of time. This forces users to think about what collecting
    software to deploy along with Graphite. There are quite a few options to choose
    from for shipping metrics over to Graphite; in this chapter we will cover one
    of those options, StatsD.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[Graphite](https://oreil.ly/-0YEs)的一个重要方面是它*不会收集数据*；相反，它集中精力于其 API 和处理大量数据的能力。这迫使用户考虑与
    Graphite 部署的收集软件。有很多选择可供将指标发送到 Graphite；在本章中，我们将介绍其中一种选择，StatsD。'
- en: Another interesting aspect of Graphite is that although it comes with a web
    application that can render graphs on demand, it is common to deploy a different
    service that can use Graphite directly as a backend for graphs. An excellent example
    of this is the fantastic [Grafana project](https://grafana.com), which provides
    a fully-featured web application for rendering metrics.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Graphite 的另一个有趣之处在于，虽然它自带了一个可以按需呈现图形的 Web 应用程序，但通常会部署一个不同的服务，该服务可以直接使用 Graphite
    作为图形的后端。一个很好的例子就是优秀的[Grafana 项目](https://grafana.com)，它提供了一个功能齐全的 Web 应用程序用于呈现指标。
- en: StatsD
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: StatsD
- en: Graphite allows you to push metrics to it via TCP or UDP, but using something
    like StatsD is especially nice because there are instrumentation options in Python
    that allow workflows such as aggregating metrics over UDP and then shipping them
    over to Graphite. This type of setup makes sense for Python applications that
    shouldn’t block to send data over (TCP connections will block until a response
    is received; UDP will not). If a very time-expensive Python loop is being captured
    for metrics, it wouldn’t make sense to add the time it takes to communicate to
    a service that is capturing metrics.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Graphite 允许您通过 TCP 或 UDP 将指标推送到它，但是使用类似 StatsD 的工具特别方便，因为 Python 中有仪表化选项，允许通过
    UDP 聚合指标然后将其发送到 Graphite。这种设置对于不应阻塞发送数据的 Python 应用程序很合理（TCP 连接会阻塞，直到收到响应；UDP 则不会）。如果捕获的非常耗时的
    Python 循环用于指标，那么将其通信时间加入到捕获指标的服务中是没有意义的。
- en: In short, sending metrics to a StatsD service feels like no cost at all (as
    it should be!). With the Python instrumentation available, measuring everything
    is very straightforward. Once a StatsD service has enough metrics to ship to Graphite,
    it will start the process to send the metrics over. All of this occurs in a completely
    asynchronous fashion, helping the application to continue. Metrics, monitoring,
    and logging should never impact a production application in any way!
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，将指标发送到 StatsD 服务感觉就像没有成本一样（应该是这样！）。利用可用的 Python 仪器，测量一切非常直接。一旦 StatsD 服务有足够的指标要发送到
    Graphite，它就会开始发送指标的过程。所有这些都是完全异步进行的，有助于应用程序的继续运行。指标、监控和日志记录绝不能以任何方式影响生产应用程序！
- en: 'When using StatsD, the data pushed to it is aggregated and flushed to a configurable
    backend (such as Graphite) at given intervals (which default to 10 seconds). Having
    deployed a combination of Graphite and StatsD in several production environments,
    it has been easier to use one StatsD instance on every application server instead
    of a single instance for all applications. That type of deployment allows more
    straightforward configuration and tighter security: configuration on all app servers
    will point to the `localhost` StatsD service and no external ports will need to
    be opened. At the end, StatsD will ship the metrics over to Graphite in an outbound
    UDP connection. This also helps by spreading the load by pushing the scalability
    further down the pipeline into Graphite.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 StatsD 时，推送到它的数据在给定间隔（默认为 10 秒）内聚合并刷新到可配置的后端（如 Graphite）。在几个生产环境中部署了 Graphite
    和 StatsD 的组合后，发现在每个应用程序服务器上使用一个 StatsD 实例比为所有应用程序使用单个实例更容易。这种部署方式允许更简单的配置和更紧密的安全性：所有应用服务器上的配置将指向
    `localhost` 的 StatsD 服务，不需要打开外部端口。最后，StatsD 将通过出站 UDP 连接将指标发送到 Graphite。这也通过将可扩展性进一步推向
    Graphite 的管道下游来分担负载。
- en: Note
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: StatsD is a `Node.js` daemon, so installing it means pulling in the `Node.js`
    dependency. It is certainly *not* a Python project!
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: StatsD 是一个 `Node.js` 守护程序，因此安装它意味着引入 `Node.js` 依赖。它绝对 *不是* 一个 Python 项目！
- en: Prometheus
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Prometheus
- en: 'In a lot of ways, [Prometheus](https://prometheus.io) is very similar to Graphite
    (powerful queries and visualization). The main difference is that it *pulls* information
    from sources, and it does this over HTTP. This requires services to expose HTTP
    endpoints to allow Prometheus to gather metric data. Another significant difference
    from Graphite is that it has baked-in alerting, where rules can be configured
    to trigger alerts or make use of the `Alertmanager`: a component in charge of
    dealing with alerts, silencing them, aggregating them, and relaying them to different
    systems such as email, chat, and on-call platforms.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多方面，[Prometheus](https://prometheus.io) 与 Graphite 非常相似（强大的查询和可视化）。主要区别在于它从源头
    *拉取* 信息，并且通过 HTTP 进行。这需要服务公开 HTTP 端点以允许 Prometheus 收集指标数据。与 Graphite 的另一个显著区别是，它内置了警报功能，可以配置规则来触发警报或使用
    `Alertmanager`：一个负责处理警报、静音、聚合并将其中继到不同系统（如电子邮件、聊天和值班平台）的组件。
- en: Some projects like [Ceph](https://ceph.com) already have configurable options
    to enable Prometheus to scrape information at specific intervals. It’s great when
    this type of integration is offered out of the box; otherwise, it requires running
    an HTTP instance somewhere that can expose metric data for a service. For example,
    in the case of the [PostgreSQL database](https://www.postgresql.org), the Prometheus
    exporter is a container that runs an HTTP service exposing data. This might be
    *fine* in a lot of cases, but if there are already integrations to gather data
    with something, such as [collectd](https://collectd.org), then running HTTP services
    might not work that well.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 一些项目（比如 [Ceph](https://ceph.com)）已经具有可配置选项，以便让Prometheus在特定间隔内抓取信息。当这种类型的集成被提供时，这是很好的；否则，它需要在某个地方运行一个可以公开服务的HTTP实例来暴露度量数据。例如，在
    [PostgreSQL数据库](https://www.postgresql.org) 的情况下，Prometheus导出器是运行一个公开数据的HTTP服务的容器。在许多情况下，这可能是*fine*的，但是如果已经有集成可以使用诸如[collectd](https://collectd.org)之类的东西收集数据，那么运行HTTP服务可能效果不是那么好。
- en: Prometheus is a great choice for short-lived data or time data that frequently
    changes, whereas Graphite is better suited for long-term historical information.
    Both offer a very advanced query language, but Prometheus is more powerful.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus是短期数据或频繁变化的时间数据的绝佳选择，而Graphite更适合长期历史信息。两者都提供非常先进的查询语言，但Prometheus更为强大。
- en: For Python, the [`prometheus_client`](https://oreil.ly/t9NtW) is an excellent
    utility to start shipping metrics over to Prometheus; if the application is already
    web-based, the client has integrations for lots of different Python webservers,
    such as Twisted, WSGI, Flask, and even Gunicorn. Aside from that, it can also
    export all its data to expose it directly at a defined endpoint (versus doing
    it on a separate HTTP instance). If you want your web application exposing in
    `/metrics/`, then adding a handler that calls `prometheus_client.generate_latest()`
    will return the contents in the format that the Prometheus parser understands.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Python来说，[`prometheus_client`](https://oreil.ly/t9NtW)是一个很好的实用工具，可以开始将指标发送到Prometheus；如果应用程序已经是基于Web的，该客户端还具有许多不同Python
    Web服务器的集成，例如Twisted、WSGI、Flask，甚至Gunicorn。除此之外，它还可以导出所有数据以直接在定义的端点公开（而不是在单独的HTTP实例上执行）。如果你想让你的Web应用程序在
    `/metrics/` 暴露出来，那么添加一个调用 `prometheus_client.generate_latest()` 的处理程序将以Prometheus解析器理解的格式返回内容。
- en: 'Create a small Flask application (save it to `web.py`) to get an idea how simple
    `generate_latest()` is to use, and make sure to install the `prometheus_client`
    package:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个小的Flask应用程序（保存到`web.py`中），以了解`generate_latest()`的使用是多么简单，并确保安装了`prometheus_client`包：
- en: '[PRE0]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Run the app with the development server:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 使用开发服务器运行应用程序：
- en: '[PRE1]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'While the application is running, open a web browser and type in the URL `[*http://localhost:5000/metrics*](http://localhost:5000/metrics)`.
    It starts generating output that Prometheus can collect, even if there is nothing
    really significant:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用程序运行时，打开一个Web浏览器，输入网址`[*http://localhost:5000/metrics*](http://localhost:5000/metrics)`。它开始生成Prometheus可以收集的输出，即使没有什么真正重要的东西：
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Most production-grade web servers like Nginx and Apache can produce extensive
    metrics on response times and latency. For example, if adding that type of metric
    data to a Flask application, the middleware, where all requests can get recorded,
    would be a good fit. Apps will usually do other interesting things in a request,
    so let’s add two more endpoints—one with a counter, and the other with a timer.
    These two new endpoints will generate metrics that will get processed by the `prometheus_client`
    library and reported when the `/metrics/` endpoint gets requested over HTTP.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数生产级Web服务器（如Nginx和Apache）可以生成有关响应时间和延迟的详尽指标。例如，如果向Flask应用程序添加这种类型的度量数据，那么中间件，其中所有请求都可以记录，将是一个很好的选择。应用程序通常会在请求中执行其他有趣的操作，所以让我们添加另外两个端点——一个带有计数器，另一个带有计时器。这两个新端点将生成度量数据，该数据将由
    `prometheus_client` 库处理，并在通过HTTP请求 `/metrics/` 端点时报告。
- en: 'Adding a counter to our small app involves a couple of small changes. Create
    a new index endpoint:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 向我们的小应用程序添加一个计数器涉及一些小的更改。创建一个新的索引端点：
- en: '[PRE3]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now define the `Counter` object. Add the name of the counter (`requests`),
    a short description (`Application Request Count`), and at least one useful label
    (such as `endpoint`). This label will help identify where this counter is coming
    from:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在定义 `Counter` 对象。添加计数器的名称（`requests`）、一个简短的描述（`Application Request Count`）和至少一个有用的标签（比如
    `endpoint`）。这个标签将帮助识别这个计数器来自哪里：
- en: '[PRE4]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'With the `REQUESTS` counter defined, include it in the `index()` function,
    restart the application, and make a couple of requests. If `/metrics/` is then
    requested, the output should show some new activity we’ve created:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了 `REQUESTS` 计数器后，将其包含在 `index()` 函数中，重新启动应用程序并进行几次请求。然后如果请求 `/metrics/`，输出应显示我们创建的一些新活动：
- en: '[PRE5]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now add a `Histogram` object to capture details on an endpoint that sometimes
    takes a bit longer to reply. The code simulated this by sleeping for a randomized
    amount of time. Just like the index function, a new endpoint is needed as well,
    where the `Histogram` object is used:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在添加一个 `Histogram` 对象来捕获一个端点的详细信息，有时回复时间较长。代码通过随机休眠一段时间来模拟这一情况。与 `index` 函数一样，还需要一个新的端点，其中使用了
    `Histogram` 对象：
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The simulated expensive operation will use a function that tracks the start
    time and end time and then passes that information to the histogram object:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟的昂贵操作将使用一个函数来跟踪开始时间和结束时间，然后将这些信息传递给直方图对象：
- en: '[PRE7]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Two new modules are needed: `time` and `random`. Those will help calculate
    the time passed onto the histogram and simulate the expensive operation being
    performed in the database. Running the application once again and requesting the
    `/database/` endpoint will start producing content when `/metrics/` is polled.
    Several items that are measuring our simulation times should now appear:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 需要两个新模块：`time` 和 `random`。它们将帮助计算传递给直方图的时间，并模拟在数据库中执行的昂贵操作。再次运行应用程序并请求 `/database/`
    端点将在 `/metrics/` 被轮询时开始生成内容。现在应该能看到几个测量我们模拟时间的项目：
- en: '[PRE8]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The `Histogram` object is flexible enough that it can operate as a context manager,
    a decorator, or take in values directly. Having this flexibility is incredibly
    powerful and helps produce instrumentation that can work in most environments
    with little effort.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`Histogram` 对象非常灵活，可以作为上下文管理器、装饰器或直接接收值。拥有这种灵活性非常强大，有助于在大多数环境中轻松生成可用的仪器设备。'
- en: Instrumentation
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 仪器设备
- en: At a company we are familiar with, there was this massive application that was
    used by several different newspapers—a giant monolithic web application that had
    no runtime monitoring. The ops team was doing a great job keeping an eye on system
    resources such as memory and CPU usage, but there was nothing checking how many
    API calls per second were going to the third-party video vendor, and how expensive
    these calls were. One could argue that that type of measurement is achievable
    with logging, and that wouldn’t be wrong, but again, this is a massive monolithic
    application with absurd amounts of logging already.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们熟悉的某家公司，有一个由多家不同报纸使用的大型应用程序——一个巨大的单体网络应用程序，没有运行时监控。运维团队在监视系统资源如内存和 CPU 使用率方面做得很好，但没有任何方式来检查每秒向第三方视频供应商发出多少
    API 调用，以及这些调用有多昂贵。可以说，通过日志记录可以实现这种类型的测量，这并不是错误，但再次强调，这是一个有着荒谬数量日志记录的大型单体应用程序。
- en: The question here was how to introduce robust metrics with easy visualization
    and querying that wouldn’t take three days of implementation training for developers
    and make it as easy as adding a logging statement in the code. Instrumentation
    of any technology at runtime has to be as close to the previous statement as possible.
    Any solution that drifts from that premise will have trouble being successful.
    If it is hard to query and visualize, then few people will care or pay attention.
    If it is hard to implement (and maintain!), then it might get dropped. If it is
    cumbersome for developers to add at runtime, then it doesn’t matter if all the
    infrastructure and services are ready to receive metrics; nothing will get shipped
    over (or at least, nothing meaningful).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的问题是如何引入具有简单可视化和查询的健壮指标，而不需要开发人员进行三天的实施培训，并使其像在代码中添加日志语句一样简单。运行时的任何技术仪器设备必须尽可能接近前述声明。任何偏离这一前提的解决方案都将难以成功。如果查询和可视化困难，那么很少有人会关心或注意。如果实施（和维护！）困难，那么它可能会被放弃。如果开发人员在运行时添加这些内容复杂，那么无论基础设施和服务是否准备好接收指标，都不会发货（或至少不会是有意义的）。
- en: The `python-statsd` is an excellent (and tiny) library that pushes metrics over
    to StatsD (which later can be relayed to Graphite) and can help you understand
    how metrics can be instrumented easily. Having a dedicated module in an application
    that wraps the library is useful, because you will need to add customization that
    would become tedious if repeated everywhere.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '`python-statsd`是一个出色的（而且很小）库，用于将指标推送到StatsD（稍后可以中继到Graphite），可以帮助您轻松地进行指标仪表化。在应用程序中有一个专用模块包装此库非常有用，因为您需要添加定制化内容，如果到处重复将会很繁琐。'
- en: Tip
  id: totrans-81
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: The Python client for StatsD has a few packages available on PyPI. For the purposes
    of these examples, use the `python-statsd` package. Install it in a virtual environment
    with `pip install python-statsd`. Failing to use the right client might cause
    import errors!
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: StatsD的Python客户端在PyPI上有几个可用的包。为了这些示例的目的，请使用`python-statsd`包。在虚拟环境中安装它，命令为`pip
    install python-statsd`。如果未使用正确的客户端可能会导致导入错误！
- en: 'One of the simplest use cases is a counter, and the examples for `python-statsd`
    show something similar to this:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的用例之一是计数器，`python-statsd`的示例显示类似于以下内容：
- en: '[PRE9]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This example assumes a StatsD is running locally. Therefore there is no need
    to create a connection; the defaults work great here. But the call to the `Counter`
    class is passing a name (`*app*`) that will not work in a production environment.
    As described in [“Naming Conventions”](#Metrics_namespaces), it is crucial to
    have a good scheme that helps identify environments and locations of the stats,
    but it would be very repetitive if you had to do this everywhere. In some Graphite
    environments, a *secret* has to prefix the namespace on all metrics being sent
    as a means of authentication. This adds another layer that needs to be abstracted
    away so that it isn’t needed when instrumenting metrics.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例假定本地正在运行StatsD。因此无需创建连接；在这里默认设置非常有效。但是`Counter`类的调用传递了一个在生产环境中不起作用的名称（`*app*`）。如[“命名约定”](#Metrics_namespaces)所述，具有帮助识别统计信息的环境和位置的良好方案至关重要，但如果到处都这样做将会非常重复。在某些Graphite环境中，作为认证手段，所有发送的指标必须在命名空间前加上一个*secret*。这增加了另一个需要抽象化的层次，以便在仪表化指标时不需要它。
- en: 'Some parts of the namespace, such as the secret, have to be configurable, and
    others can be programmatically assigned. Assuming there is a way to optionally
    prefix the namespace with a function called `get_prefix()`, this is how the `Counter`
    would get wrapped to provide a smooth interaction in a separate module. For the
    examples to work, create the new module, name it *metrics.py*, and add the following:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 命名空间的某些部分（如密钥）必须是可配置的，而其他部分可以以编程方式分配。假设有一种方法可以选择性地使用函数`get_prefix()`作为命名空间前缀，那么`Counter`将被包装以在单独的模块中提供平滑交互。为使示例生效，请创建新模块，命名为*metrics.py*，并添加以下内容：
- en: '[PRE10]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'By following the same example used in [“Naming Conventions”](#Metrics_namespaces)
    for a small Python application that calls to the Amazon S3 API in a path such
    as *web/api/aws.py*, the `Counter` can get instantiated like this:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循[“命名约定”](#Metrics_namespaces)中使用的同一示例，用于调用Amazon S3 API的小型Python应用程序的某个路径，例如*web/api/aws.py*，`Counter`可以这样实例化：
- en: '[PRE11]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'By using `__name__`, the `Counter` object is created with the full Python namespace
    of the module that will appear on the receiving side as `web.api.aws.Counter`.
    This works nicely, but it isn’t flexible enough if we need more than one counter
    in loops that happen in different places. We must modify the wrapper so that a
    suffix is allowed:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`__name__`，`Counter`对象将以模块的完整Python命名空间（在接收端显示为`web.api.aws.Counter`）创建。这很好地实现了功能，但如果我们需要在不同位置的循环中有多个计数器，这种方式就不够灵活了。我们必须修改包装器，以允许使用后缀：
- en: '[PRE12]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'If the *aws.py* file contains two places that require a counter, say a read
    and a write function for S3, then we can easily suffix them:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*aws.py*文件包含两个需要计数器的地方，例如用于S3读取和写入功能，则可以轻松添加后缀：
- en: '[PRE13]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: These two helpers now have unique counters from the same wrapper, and if configured
    in a production environment, the metrics appear in a namespace similar to `secret.app1.web.api.aws.s3.write.Counter`.
    This level of granularity is helpful when trying to identify metrics per operation.
    Even if there are cases where granularity isn’t needed, it is always better to
    have it and ignore it than to need it and not have it. Most metric dashboards
    allow customization for grouping metrics.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个辅助工具现在从同一个包装中具有独特的计数器，如果在生产环境中配置，则度量数据将显示在类似于 `secret.app1.web.api.aws.s3.write.Counter`
    的命名空间中。在尝试识别每个操作的指标时，这种粒度是有帮助的。即使有些情况下不需要粒度，拥有而不使用总比需要而没有要好。大多数度量仪表板允许自定义分组指标。
- en: 'The suffix is useful when added to function names (or class methods) that hardly
    represent what they are or what they do, so improving the naming by using something
    meaningful is another benefit to this added flexibility:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在函数名称（或类方法）后添加后缀对于几乎不代表它们是什么或它们所做的功能的函数名称来说是有用的，因此通过使用有意义的内容来改进命名是增加灵活性的另一个好处：
- en: '[PRE14]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Counters and other metric types such as gauges can be so easy to add that it
    might be tempting to include them in a loop, but for performance-critical code
    blocks that run thousands of times per second, it might be impactful to add these
    types of instrumentation. Limiting the metrics that are being sent or sending
    them later are good options to consider.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 计数器和其他指标类型，比如计量表，添加起来可能很容易，可以诱人地将它们包含在循环中，但对于每秒运行数千次的性能关键代码块来说，添加这些类型的仪表可能会产生影响。考虑限制发送的指标或稍后发送它们是不错的选择。
- en: This section showed how to instrument metrics for a local StatsD service. This
    instance will end up relaying its metric data to a configured backend like Graphite,
    but these simplistic examples aren’t meant to be a StatsD-only solution. To the
    contrary, they demonstrate that adding helpers and utilities to wrap common usage
    is a must, and when easy instrumentation exists, developers will want to add it
    everywhere. The problem of having too much metric data is preferable to having
    no metrics at all.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 本节展示了如何为本地的 StatsD 服务添加仪表指标。此实例最终将其指标数据中继到像 Graphite 这样的配置后端，但这些简单的示例并不意味着仅适用于
    StatsD。相反，它们表明添加帮助程序和实用工具来包装常见用法是必不可少的，当存在简单的仪表时，开发人员将希望在各处添加它们。拥有过多的度量数据的问题，比完全没有度量数据要好。
- en: Naming Conventions
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 命名惯例
- en: 'In most monitoring and metric services such as Graphite, Grafana, Prometheus,
    and even StatsD, there is a notion of namespaces. Namespaces are very important,
    and it is worth thinking carefully on a convention that will allow easy identification
    of system components, while at the same time allow enough flexibility to accommodate
    for growth or even change. These namespaces are similar to how Python uses them:
    a dot separates each name, and each separated part represents a step in a hierarchy
    from left to right. The first item from the left is the parent, and every subsequent
    part is a child.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数监控和度量服务中，如 Graphite、Grafana、Prometheus，甚至 StatsD 中，都有命名空间的概念。命名空间非常重要，值得仔细考虑一个约定，以便轻松识别系统组件，同时足够灵活以适应增长甚至变化。这些命名空间类似于
    Python 的使用方式：每个名称之间用点分隔，每个分隔的部分表示从左到右层次结构中的一步。从左边开始的第一项是父级，每个后续部分都是子级。
- en: 'For example, let’s assume we have some API calls to AWS in a nimble Python
    application that serves images on a website. The Python module where we have the
    spot we want metrics is in a path like this: *web/api/aws.py*. The natural namespace
    choice for this path could be: `web.api.aws`, but what if we have more than one
    production app server? Once the metrics ship with one namespace, it’s hard (almost
    impossible!) to change to a different scheme. Let’s improve the namespace to help
    identify production servers: `{server_name}.web.api.aws`'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有一个灵活的 Python 应用程序，通过 AWS 进行一些 API 调用，在网站上提供图像服务。我们要关注的 Python 模块路径如下：*web/api/aws.py*。此路径的自然命名空间选择可能是：`web.api.aws`，但如果我们有多个生产应用服务器怎么办？一旦指标使用了一个命名空间，要改成其他方案就很困难（几乎不可能！）。我们改进命名空间以帮助识别生产服务器：`{server_name}.web.api.aws`
- en: 'Much better! But can you see another issue? When shipping metrics, a trailing
    name is sent over. In the case of counters example, the name would be something
    like: `{server_name}.web.api.aws.counter`. That is a problem because our little
    application does several calls to AWS like S3 and we may want to talk to other
    AWS services in the future. It is easier to fix child naming than parent naming,
    so in this case, it just requires developers to match metrics as granularly as
    possible to what is measured. For example, if we had an S3 module inside the *aws.py*
    file, it would make sense to include it to distinguish it from other pieces. The
    child portion of that metric would look like `aws.s3`, and a counter metric would
    end up looking like `aws.s3.counter`.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 好多了！ 但是你能看到另一个问题吗？ 在发送指标时，会发送一个尾随名称。 在计数器示例中，名称将类似于：`{server_name}.web.api.aws.counter`。
    这是一个问题，因为我们的小应用程序对 AWS 进行了几次调用，比如 S3，而且我们将来可能还想与其他 AWS 服务交互。 修复子命名比修复父命名更容易，因此在这种情况下，它只需要开发人员尽可能精确地匹配所测量的指标。
    例如，如果我们在 *aws.py* 文件中有一个 S3 模块，那么将其包含进去以区分它与其他部分是有意义的。 该指标的子部分将类似于 `aws.s3`，计数器指标最终将看起来像
    `aws.s3.counter`。
- en: Having so many variables for namespaces can feel cumbersome, but most established
    metric services allow easy combinations, such as “show me count average for all
    S3 calls last week but only from production servers in the East Coast.” Pretty
    powerful, huh?
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 有这么多命名空间变量可能会感到繁琐，但是大多数已建立的度量服务都允许轻松组合，例如“显示上周所有来自东海岸生产服务器的S3调用的平均计数”。 很强大，对吧？
- en: 'There is another potential issue here. What do we do with production and staging
    environments? What if I’m developing and testing in a virtual machine somewhere?
    The `{server_name}` part might not help much here if everyone calls their development
    machine `srv1`. If deploying to different regions, or even if there is a plan
    to scale beyond a single region or country, an extra addition to the namespace
    can make sense as well. There are numerous ways to expand on namespaces to better
    fit an environment, but something like this is a suitable prefix: `{region}.{prod|staging|dev}.{server_name}`'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这里还有另一个潜在问题。 我们在生产和分级环境中该怎么办？ 如果我在某个虚拟机中进行开发和测试怎么办？ 如果每个人都将他们的开发机器命名为 `srv1`，那么`{server_name}`部分可能不会太有用。
    如果部署到不同的地区，甚至如果计划超越单个地区或国家进行扩展，则额外添加到命名空间也是有意义的。 有许多方法可以扩展命名空间以更好地适应环境，但类似这样的适当前缀是合适的：`{region}.{prod|staging|dev}.{server_name}`
- en: Logging
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 日志记录
- en: 'It can be daunting to configure logging in Python correctly. The logging module
    is very performant and has several different outlets that can receive its output.
    Once you grasp its initial configuration, it isn’t that complicated to add to
    it. We’ve been guilty of rewriting an alternative way of logging because of the
    dislike for configuring the logging module properly. This was a mistake, because
    it almost never accounted for all the things that the standard library module
    does well: multithreaded environments, unicode, and supporting multiple destinations
    other than `STDOUT`, just to name a few.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 配置 Python 日志记录可能会让人望而生畏。 日志记录模块性能很高，有几种不同的输出方式可以接收其输出。 一旦掌握了其初始配置，向其中添加内容就不那么复杂了。
    由于不喜欢正确配置日志记录模块而曾经重写了另一种日志记录方式，我们曾经犯过错误。 这是一个错误，因为它几乎从未考虑过标准库模块擅长的所有事情：多线程环境、Unicode
    和支持除 `STDOUT` 之外的多个目的地，仅举几例。
- en: Python’s logging module is so big and can be made to accommodate so many different
    uses (just like almost all software in this very distilled chapter), that not
    even a whole chapter would be sufficient to cover it all. This section will offer
    short examples for the simplest use cases, and then move progressively toward
    more complex uses. Once a few scenarios are well understood, it isn’t hard to
    keep expanding logging into other configurations.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Python 的日志记录模块非常庞大，可以用来适应许多不同的用途（就像本章中所述的几乎所有软件一样），甚至一个完整的章节都不足以涵盖所有内容。 本节将为最简单的用例提供简短的示例，然后逐渐向更复杂的用法发展。
    一旦几个场景被充分理解，将日志记录扩展到其他配置就不难了。
- en: Even though it is complex and can take some time to fully comprehend, it is
    one of the *crucial pillars* of DevOps. You cannot be a successful DevOps person
    without it.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 即使它很复杂，可能需要一些时间才能完全理解，但它是 DevOps 的 *重要支柱* 之一。 没有它，你就不能成为成功的 DevOps 人员。
- en: Why Is It Hard?
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么这么难？
- en: Python applications, like command-line tools and one-shot types of tools, usually
    have a top-to-bottom design and are very procedural. When you start learning development
    with something like Python (or perhaps Bash), it is reasonable to get used to
    that flow. Even when moving to more object-oriented programming and using more
    classes and modules, there is still this sense of declaring what you need, instantiating
    objects to use them, and so on. Modules and objects aren’t usually preconfigured
    at import time, and it is not common to see some imported module be configured
    globally for the whole project even before being instantiated.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Python 应用程序，如命令行工具和一次性工具，通常采用自顶向下的设计，非常程序化。当你开始用像 Python（或者可能是 Bash）这样的东西学习开发时，习惯于这种流程是合理的。即使转向更面向对象的编程并使用更多的类和模块，仍然存在声明所需内容、实例化对象以使用它们等这种感觉。模块和对象通常不会在导入时预先配置，并且在实例化之前全局配置某些导入的模块并不常见。
- en: There is this sense of *“somehow this is configured and how is it possible if
    I haven’t even called it yet.”* Logging is like that; once configured at runtime,
    the module somehow persists this configuration regardless of where it is imported
    and used, and before creating loggers. This is all very convenient, but it is
    hard to get used to when almost nothing else works this way in the Python standard
    library!
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 有这种感觉*“不知何故已经配置了，如果我甚至还没有调用它，这怎么可能。”* 日志记录就像这样；一旦在运行时配置，模块会在任何地方导入和使用之前持久化这种配置，并且在创建记录器之前。这一切都非常方便，但是当几乎没有其他东西以这种方式在
    Python 标准库中运作时，要习惯这种方式是困难的！
- en: The basicconfig
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基本配置
- en: 'The easiest way out of logging configuration misery is to simply use `basicconfig`.
    It is a straightforward way to get logging working, with lots of defaults and
    about three lines worth of work:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 摆脱日志配置困扰的最简单方法就是简单地使用`basicconfig`。这是一个直接的方法来让日志记录工作，具有许多默认值，大约三行的工作量：
- en: '[PRE15]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'With almost no need to understand anything about logging, messages appear and
    the module appears to be configured correctly. It is also good that it can support
    more customization and a few options that are well-suited for small applications
    that don’t need highly customized logging interfaces. The format of the log messages
    and setting the verbosity are achieved with ease:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎不需要理解任何关于日志记录的东西，消息就会出现，模块似乎配置正确。这也很好，它可以支持更多的定制选项，并且适合于不需要高度定制化日志接口的小型应用程序。日志消息的格式和设置详细程度都可以轻松实现：
- en: '[PRE16]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This example was configured to set the minimum level at `INFO`, which is why
    the debug message didn’t emit anything. Formatting was passed into the `basicConfig`
    call to set the time, the name of the logging (more on that later in this section),
    the level name, and finally, the message. That is plenty for most applications,
    and it is good to know that a simple entry into logging can do so much already.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例配置为将最低级别设置为`INFO`，这就是为什么调试消息没有发出任何内容。格式化信息传递到`basicConfig`调用中以设置时间、日志名称（本节后面会详述）、级别名称和最后消息。对于大多数应用程序来说，这已经足够了，并且了解到日志记录的简单入门已经可以做这么多事情。
- en: The problem with this type of configuration is that it will not be sufficient
    to take advantage of more complex scenarios. This configuration has a lot of defaults
    that might not be acceptable and will be cumbersome to change. If there is any
    possibility that the application will need something a bit more complicated, it
    is recommended to fully configure logging and go through the pain of understanding
    how to do that.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的配置问题在于它不足以应对更复杂的场景。这种配置有很多默认值，可能不被接受，并且更改起来很麻烦。如果应用程序有可能需要更复杂的内容，建议完全配置日志记录并努力理解如何做到这一点。
- en: Deeper Configuration
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更深层次的配置
- en: The logging module has several different *loggers*; these loggers can be configured
    independently, and they can also inherit configuration from a *parent* logger.
    The top-most logger is the `root` logger, and all other loggers are child loggers
    (`root` is the parent). When configuring the `root` logger, you are essentially
    setting the configuration for *everything* globally. This way of organizing logging
    makes sense when different applications or different parts of a single application
    require different types of logging interfaces and settings.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 日志模块有几种不同的*loggers*；这些记录器可以独立配置，也可以从*parent*记录器继承配置。最顶层的记录器是`root`记录器，所有其他记录器都是子记录器（`root`是父记录器）。当配置`root`记录器时，实际上是为*全局*设置了配置。在不同应用程序或单个应用程序的不同部分需要不同类型的日志记录接口和设置时，这种组织日志记录的方式是有意义的。
- en: If a web application wants to send WSGI server errors over email but log everything
    else to a file, this would be impossible to do if a single root-level logger was
    being configured. This is similar to [“Naming Conventions”](#Metrics_namespaces)
    in that the names are separated by dots, and every dot indicates a new child level.
    It means that `app.wsgi` could be configured to send error logs over email, while
    `app.requests` can be set separately with file-based logging.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个 web 应用程序想要将 WSGI 服务器错误发送到电子邮件，但将其他所有内容记录到文件中，如果配置了单个根级别记录器，则这将是不可能的。这与[“命名约定”](#Metrics_namespaces)类似，名称之间用点分隔，每个点表示一个新的子级别。这意味着可以配置`app.wsgi`以通过电子邮件发送错误日志，而`app.requests`可以单独设置为基于文件的记录。
- en: Tip
  id: totrans-123
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: A nice way of dealing with this namespace is by using the same namespace as
    Python instead of using something custom. Do this by using `*name*` to create
    loggers in modules. Using the same namespace for both the project and logging
    prevents confusion.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 处理这个命名空间的一个好方法是使用与 Python 相同的命名空间，而不是使用自定义的命名空间。通过在模块中使用`*name*`来创建记录器来实现这一点。对项目和日志记录使用相同的命名空间可以防止混淆。
- en: 'Configuration of logging should be set as *early as possible*. If the application
    is a command-line tool, then the right place for it is at the main entry point,
    probably even before parsing arguments. For web applications, the logging configuration
    is usually through the framework’s helpers. Most popular web frameworks today
    have a facility for logging configuration: Django, Flask, Pecan, and Pyramid all
    offer an interface for early logging configuration. Take advantage of it!'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 日志记录的配置应尽可能早地设置。如果应用程序是一个命令行工具，那么正确的位置是在主入口点，可能甚至在解析参数之前。对于 web 应用程序，日志记录配置通常是通过框架的辅助工具进行的。今天最流行的
    web 框架都提供了日志记录配置的接口：Django、Flask、Pecan 和 Pyramid 都提供了早期日志记录配置的接口。利用它！
- en: 'This example shows how to configure a command-line tool; you can see that there
    are a few similarities to `basicConfig`:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例显示了如何配置一个命令行工具；你可以看到与`basicConfig`有一些相似之处：
- en: '[PRE17]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: A lot is happening here. The `root` logger is requested by calling `getLogger()`
    without any arguments, and the level is set at `DEBUG`. This is a good default
    to have, since other child loggers can modify the level. Next, the file logger
    gets configured. In this case, it tries to create the file logger that will fall
    back to a temporary location if it can’t write to it. It then gets set at the
    `INFO` level, and its message format is changed to include a timestamp (useful
    for file-based log files).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生了很多事情。通过调用`getLogger()`而不带任何参数来请求`root`记录器，并将级别设置为`DEBUG`。这是一个很好的默认设置，因为其他子记录器可以修改级别。接下来，配置文件记录器。在这种情况下，它尝试创建文件记录器，如果无法写入文件，则会回退到临时位置。然后将其设置为`INFO`级别，并更改其消息格式以包含时间戳（对于基于文件的日志文件很有用）。
- en: 'Note how at the end, the file logger gets added to the `root_logger`. It feels
    counter-intuitive, but in this case, the root configuration is being set to handle
    everything. Adding a *stream handler* to the root logger will make the application
    send logs to both the file and standard error at the same time:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在最后，文件记录器被添加到`root_logger`中。这感觉有些违反直觉，但在这种情况下，根配置被设置为处理所有内容。向根记录器添加*stream
    handler*将使应用程序同时将日志发送到文件和标准错误输出：
- en: '[PRE18]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In this case, the `BASE_FORMAT` was used, because it’s going to the terminal,
    where timestamps can cause excessive noise. As you can see, it takes quite a bit
    of configuration and settings, and it gets very complicated once we start dealing
    with different loggers. To minimize this, a separate module with a helper that
    sets all these options is preferable. As an alternative to this type of configuration,
    the `logging` module offers a dictionary-based configuration, where the settings
    are set in a key-value interface. The example below shows how the configuration
    would look for the same example.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，使用`BASE_FORMAT`，因为它要去到终端，而时间戳可能会引起过多的噪音。正如你所看到的，它需要相当多的配置和设置，一旦我们开始处理不同的记录器，情况就会变得非常复杂。为了最小化这一点，最好使用一个带有设置所有这些选项的辅助程序的单独模块。作为这种类型配置的替代方案，`logging`模块提供了一种基于字典的配置，其中设置以键值接口设置。下面的示例显示了相同示例的配置会是什么样子。
- en: 'To see it in action, add a couple of log calls at the end of the file, execute
    directly with Python, and save it in a file called *log_test.py*:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 要看它如何运作，请在文件末尾添加几个日志调用，直接用Python执行，并将其保存在名为*log_test.py*的文件中：
- en: '[PRE19]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The root logger is the parent, and a new logger called `my-app` is introduced.
    Executing the file directly gives output in the terminal as well as in a file
    called *application.log*:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 根记录器是父记录器，并引入了一个名为`my-app`的新记录器。直接执行文件将在终端以及名为*application.log*的文件中输出：
- en: '[PRE20]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The output is repeated because we configured both, but that doesn’t mean they
    need to be. The formatting changed for the file-based logger, allowing a cleaner
    view in the console:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 输出重复了，因为我们都配置了，但这并不意味着它们必须这样。文件日志记录器的格式已更改，允许在控制台中获得更清晰的视图：
- en: '[PRE21]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Using `dictConfig` helps better visualize where things go and how they all tie
    together versus the more manual example earlier. For complicated setups where
    more than one logger is needed, the `dictConfig` way is better. Most of the web
    frameworks use the dictionary-based configuration exclusively.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`dictConfig`有助于更好地可视化事物的走向以及它们如何相互联系，相对于早期更手动的示例。对于需要多个记录器的复杂设置，`dictConfig`方法更好。大多数Web框架都专门使用基于字典的配置。
- en: 'Sometimes, the logging format is overlooked. It’s often seen as something cosmetic
    that provides visual appeal to a human reading the logs. While that is partly
    true, it is nice to have some square brackets to designate the logging level (for
    example, `[CRITICAL]`), but it can also be instrumental when other specifics of
    the environment, such as production, staging, or development, need separation.
    It might be immediately clear to a developer that the logs are from the development
    version, but it is extremely important to identify them if they are being forwarded
    around or collected in a central place. Dynamically applying this is done with
    environment variables and the use of `logging.Filter` in the `dictConfig`:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，日志格式被忽视了。它通常被视为为人阅读日志提供视觉吸引力的一种表面装饰。虽然这在某种程度上是正确的，但加上一些方括号来指定日志级别（例如，`[CRITICAL]`）也是很好的，但在其他环境细节（例如，生产、暂存或开发）需要分离时也可以起到一定作用。对于开发人员来说，日志来自开发版本可能会立即清晰，但如果它们被转发或集中收集，识别它们则非常重要。动态应用此操作是通过环境变量和在`dictConfig`中使用`logging.Filter`完成的：
- en: '[PRE22]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: There is a lot going on in this example. It might be easy to miss a few things
    that got updated. First, a new class called `EnvironFilter`, which uses the `logging.Filter`
    as a base class, was added, and it defined a method called `filter` that accepts
    a `record` argument. This is how the base class wants this method to be defined.
    The `record` argument gets extended to include the `APP_ENVIRON` environment variable
    that defaults to `*DEVEL*`.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中发生了很多事情。可能会容易忽略一些已更新的内容。首先，添加了一个名为`EnvironFilter`的新类，它将`logging.Filter`作为基类，并定义了一个名为`filter`的方法，该方法接受一个`record`参数。这是基类要求定义此方法的方式。`record`参数被扩展以包括默认为`*DEVEL*`的`APP_ENVIRON`环境变量。
- en: 'Then, in the `dictConfig`, a new key is added (`filters`) that names this filter
    the `environ_filter`, pointing to the `EnvironFilter` class. Finally, in the `handlers`
    key, we added the `filters` key that accepts a list, and in this case it will
    only have a single filter added: `environ_filter`.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在`dictConfig`中，添加了一个新的键（`filters`），将这个过滤器命名为`environ_filter`，指向`EnvironFilter`类。最后，在`handlers`键中，我们添加了`filters`键，它接受一个列表，在这种情况下，它只会添加一个单独的过滤器：`environ_filter`。
- en: The defining and naming of the filter feels cumbersome, but this is because
    our example is trivial. In more complex environments, it allows you to extend
    and configure without *boilerplate* filling the dictionary, making it easier to
    update or extend further.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 定义和命名过滤器感觉很笨重，但这是因为我们的例子太简单了。在更复杂的环境中，它允许您扩展和配置，而无需填充字典中的*样板文件*，使得更新或进一步扩展变得更加容易。
- en: 'A quick test in the command line indicates how the new filter shows the environment.
    In this example, a basic [Pecan](https://www.pecanpy.org) application is used:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在命令行中进行快速测试表明新过滤器如何显示环境。在这个例子中，使用了基本的[Pecan](https://www.pecanpy.org)应用程序：
- en: '[PRE23]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The default environment of `DEVEL` works, and changing it to production is
    one environment variable away:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`DEVEL`的默认环境有效，将其更改为生产环境只需一个环境变量：'
- en: '[PRE24]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Common Patterns
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 常见模式
- en: 'The logging module offers a few good patterns that aren’t immediately obvious
    but are quite good to use as much as possible. One of these patterns is using
    the `logging.exception` helper. A common workflow looks like this:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 日志模块提供了一些非常好用但并非立即显而易见的模式。其中一个模式是使用`logging.exception`助手。常见的工作流程如下：
- en: '[PRE25]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This type of workflow is problematic on several fronts: it primarily *eats*
    the exception and just reports the string representation of it. If the exception
    isn’t obvious or if it happens in a location that is not immediately evident,
    then reporting `TypeError` is useless. When string replacement fails, you can
    get a ValueError, but if the code is obscuring the traceback, then the error doesn’t
    help:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这种工作流程在几个方面存在问题：它主要是*吞噬*异常，并仅报告其字符串表示。如果异常不明显或发生在不明显的位置，则报告`TypeError`是无用的。当字符串替换失败时，可能会得到一个ValueError，但如果代码模糊了回溯，则该错误毫无帮助：
- en: '[PRE26]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Where did that happen? We know it happens when `expensive_operation()` is called,
    but where? In what function, class, or file? This type of logging is not only
    unhelpful, it is infuriating! The logging module can help us log the full exception
    traceback:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 发生在哪里？我们知道在调用`expensive_operation()`时会发生，但是在哪里？在哪个函数、类或文件中？这种记录方式不仅无益，而且令人恼火！日志模块可以帮助我们记录完整的异常回溯：
- en: '[PRE27]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The `logging.exception` helper will push the full traceback to the log output
    magically. The implementation doesn’t need to worry about capturing `error` as
    it was doing before, or even try to retrieve useful information from the exception.
    The logging module is taking care of everything.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`logging.exception`助手会神奇地将完整的回溯推送到日志输出。实现不需要再担心像以前那样捕获`error`，甚至尝试从异常中检索有用信息。日志模块会处理一切。
- en: 'Another useful pattern is to use the built-in capabilities of the logging module
    for string interpolation. Take this piece of code as an example:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种有用的模式是利用日志模块的内置能力进行字符串插值。以这段代码为例：
- en: '[PRE28]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The statement is requiring two string replacements, and it is assuming that
    `arguments` is going to have two items. If `arguments` fails to have two arguments,
    the above statement will break the production code. You *never* want to break
    production code because of logging. The module has a helper to catch this, report
    it as a problem, and allow the program to continue:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 该语句需要两个字符串替换，并且假设`arguments`将有两个项目。如果`arguments`没有两个参数，上述语句将破坏生产代码。您*绝对不希望*因为日志记录而破坏生产代码。该模块有一个助手来捕获这种情况，将其报告为问题，并允许程序继续运行：
- en: '[PRE29]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: This is safe, and it is the recommended way to pass items to the statement.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这是安全的，并且是向语句传递项目的推荐方式。
- en: The ELK Stack
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ELK堆栈
- en: 'Just like Linux, Apache, MySQL, and PHP were known as *LAMP*, you will often
    hear about the *ELK* stack: Elasticsearch, Logstash, and Kibana. This stack allows
    you to extract information from logs, capture useful metadata, and send it to
    a document store (Elasticsearch) which then uses a powerful dashboard (Kibana)
    to display the information. Understanding each piece is crucial for an effective
    strategy when consuming logs. Each component of the stack is equally essential,
    and although you may find similar applications for each, this section concentrates
    on their actual roles in an example application.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 就像Linux、Apache、MySQL和PHP被称为*LAMP*一样，您经常会听到*ELK*堆栈：Elasticsearch、Logstash和Kibana。该堆栈允许您从日志中提取信息，捕获有用的元数据，并将其发送到文档存储（Elasticsearch），然后使用强大的仪表板（Kibana）显示信息。了解每个部分对于有效地消费日志至关重要。该堆栈的每个组件同样重要，尽管您可能会发现每个组件都有类似的应用，但本节重点介绍它们在示例应用程序中的实际角色。
- en: 'Most production systems have been around for a while, and you will rarely get
    the chance to redo infrastructure from scratch. Even if you are lucky enough to
    get to design infrastructure from the ground up, it is possible to overlook the
    importance of log structure. A proper log structure is as important as capturing
    useful information, but when the structure is lacking, Logstash can help. When
    installing Nginx, the default logging output is similar to this:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数生产系统已经运行了一段时间，你很少有机会从头开始重新设计基础架构。即使你有幸能够从头开始设计基础架构，也可能会忽视日志结构的重要性。适当的日志结构和捕获有用信息一样重要，但当结构不完整时，Logstash可以提供帮助。在安装Nginx时，默认的日志输出类似于这样：
- en: '[PRE30]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Some parts of the log statement are straightforward, such as the HTTP method
    (a `GET`) and the timestamp. If you can control the information, drop what is
    not meaningful, or include needed data, that is fine as long as you have a clear
    understanding of what all of these components are. The configuration for the HTTP
    server has these details in */etc/nginx/nginx.conf*:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 日志语句的某些部分很简单，比如HTTP方法（一个`GET`）和时间戳。如果你可以控制信息，丢弃无意义的内容或包含所需的数据，只要你清楚所有这些组件的含义即可。HTTP服务器的配置在*/etc/nginx/nginx.conf*中包含了这些细节：
- en: '[PRE31]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: When you first look at the output, you might think that the dashes were for
    missing information, but this is not entirely correct. In the log output example,
    *two* dashes follow the IP; one is just cosmetic, and the second one is for missing
    information. The configuration tells us that a single dash follows the IP and
    then by `$remote_user`, which is useful when authentication is involved so that
    the authenticated user is captured. If this is an HTTP server that doesn’t have
    authentication enabled, the `$remote_user` can be dropped from the configuration
    (if you have access and permission to change the *nginx.conf* file), or it can
    be ignored with rules that extract the metadata from the logs. Let’s see in the
    next section how Logstash can help with its vast number of input plug-ins.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 当你第一次看输出时，可能会认为破折号是表示缺失信息的，但这并不完全正确。在日志输出示例中，*两个*破折号跟在IP后面；一个只是装饰用的，第二个用于缺失信息。配置告诉我们，IP后面跟着一个破折号，然后是`$remote_user`，在涉及认证时很有用，以便捕获已认证的用户。如果这是一个未启用认证的HTTP服务器，可以在有权限和访问权限的情况下从*nginx.conf*文件中删除`$remote_user`，或者可以使用从日志中提取元数据的规则来忽略它。让我们在下一节看看Logstash如何利用其大量的输入插件来帮助。
- en: Tip
  id: totrans-168
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Elasticsearch, Logstash, and Kibana are usually *not* available in Linux distributions.
    Depending on the flavor of the distribution, the proper signing keys need to get
    imported, and the package manager needs to be configured to pull from the right
    repositories. [Refer to the install sections on the official documentation](https://oreil.ly/A-EwN).
    Make sure that the Filebeat package is also installed. This is a lightweight (yet
    powerful) utility for log forwarding. It will be used to send logs to Logstash
    later.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch、Logstash和Kibana通常在Linux发行版中*不*可用。根据发行版的特性，需要导入正确的签名密钥，并配置包管理器以从正确的存储库获取。[请参阅官方文档上的安装部分](https://oreil.ly/A-EwN)。确保也安装了Filebeat包。这是一个轻量级（但功能强大）的日志转发工具。稍后将用它将日志发送到Logstash。
- en: Logstash
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Logstash
- en: The first step after deciding to go with the ELK stack is to hack some Logstash
    rules to extract information from a given source, filter it, and then ship it
    to a service (like Elasticsearch in this case). After Logstash gets installed,
    the path */etc/logstash/* becomes available with a useful *conf.d* directory where
    we can add multiple configurations for different services. Our use case is capturing
    Nginx information, filtering it, and then shipping it over to a local Elasticsearch
    service that should already be installed and running.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 决定采用ELK堆栈后的第一步是对一些Logstash规则进行修改，以从给定源中提取信息，过滤它，然后将其发送到一个服务（比如这种情况下的Elasticsearch）。安装Logstash后，路径*/etc/logstash/*将可用，并且有一个有用的*conf.d*目录，我们可以在其中为不同的服务添加多个配置。我们的用例是捕获Nginx信息，对其进行过滤，然后将其发送到本地已安装并运行的Elasticsearch服务。
- en: To consume logs, the `filebeat` utility needs to be installed. This is available
    from the same repositories that you enabled to install Elasticsearch, Kibana,
    and Logstash. Before configuring Logstash, we need to ensure that Filebeat is
    configured for the Nginx log files and the location of Logstash.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 要消费日志，需要安装 `filebeat` 实用程序。这可以从安装 Elasticsearch、Kibana 和 Logstash 的相同软件仓库中获取。在配置
    Logstash 之前，我们需要确保 Filebeat 已为 Nginx 日志文件和 Logstash 的位置进行了配置。
- en: 'After installing Filebeat, add the log paths for Nginx and the default Logstash
    port for localhost (`5044`). The configuration in */etc/filebeat/filebeat.yml*
    should have these lines defined (or uncommented):'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 Filebeat 后，在 */etc/filebeat/filebeat.yml* 文件中定义（或取消注释）以下行，添加 Nginx 的日志路径以及
    localhost (`5044`) 的默认 Logstash 端口：
- en: '[PRE32]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This allows Filebeat to look at every single path in */var/log/nginx/* and
    then forward it to the `localhost` instance of Logstash. If a separate log file
    for another Nginx application is required, it gets added here. Other defaults
    in the configuration file might be present, and those should be fine to leave
    as is. Now start the service:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得 Filebeat 可以查看 */var/log/nginx/* 中的每个路径，并将其转发到 Logstash 的 `localhost` 实例。如果需要为另一个
    Nginx 应用程序添加单独的日志文件，则在此处添加。配置文件中可能存在其他默认值，这些值应该保持不变。现在启动服务：
- en: '[PRE33]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now create a new file in the Logstash configuration directory (in */etc/logstash/conf.d/*),
    and name it *nginx.conf*. The first section that you should add handles the input:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在在 Logstash 配置目录（在 */etc/logstash/conf.d/* 中）创建一个新文件，命名为 *nginx.conf*。首先要添加的部分是处理输入：
- en: '[PRE34]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The `input` section indicates that the source of the information will come from
    the Filebeat service using port `5044`. Since all the file path configuration
    is done in the Filbeat configuration, not much else is necessary here.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '`input` 部分表明信息来源将通过 Filebeat 服务，使用 `5044` 端口。由于所有文件路径配置都在 Filebeat 配置中完成，这里不需要其他配置。'
- en: 'Next, we need to extract the information and map it to keys (or fields). Some
    parsing rules need to be put in place to make sense of the unstructured data we
    are dealing with. For this type of parsing, use the `grok` plug-in; append the
    following configuration to the same file:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要提取信息并映射到键（或字段）。需要设置一些解析规则以理解我们处理的非结构化数据。对于这种类型的解析，请使用 `grok` 插件；将以下配置追加到同一文件中：
- en: '[PRE35]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The `filter` section now defines the usage of the `grok` plug-in that takes
    the incoming line and applies the powerful `COMBINEDAPACHELOG`, a collection of
    regular expressions that can accurately find and map all the components of the
    web server logs coming from Nginx.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '`filter` 部分现在定义了使用 `grok` 插件来接收传入行并应用强大的 `COMBINEDAPACHELOG`，这是一组正则表达式，可以准确地查找并映射来自
    Nginx 的 Web 服务器日志的所有组件。'
- en: 'Finally, the output section needs to set where the newly structured data should
    go:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，输出部分需要设置新结构化数据的目标位置：
- en: '[PRE36]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: This means that all structured data gets sent over to the local instance of
    Elasticsearch. As you can see, the configuration for Logstash (and the Filebeat
    service) was very minimal. There are several plug-ins and configuration options
    that can be added to further fine-tune the log collection and parsing. This *batteries
    included* approach is excellent for getting started without having to figure out
    extensions or plug-ins. If you are curious, browse through the Logstash source
    code and search for the *grok-patterns* file that contains `COMBINEDAPACHELOG`;
    the collection of regular expressions is quite the sight.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着所有结构化数据都会发送到 Elasticsearch 的本地实例。正如你所看到的，对于 Logstash（和 Filebeat 服务），配置非常简单。可以添加多个插件和配置选项来进一步微调日志收集和解析。这种“一应俱全”的方法非常适合初学者，无需探索扩展或插件。如果你感兴趣，可以浏览
    Logstash 源代码，并搜索包含 `COMBINEDAPACHELOG` 的 *grok-patterns* 文件，其中包含一组相当不错的正则表达式。
- en: Elasticsearch and Kibana
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Elasticsearch 和 Kibana
- en: 'After installing the `elasticsearch` package, there is little that you need
    to do to have a local setup running and ready to receive structured data from
    Logstash. Make sure the service is started and running without problems:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完 `elasticsearch` 包后，你几乎不需要做其他操作就可以在本地搭建一个可以接收来自 Logstash 的结构化数据的运行环境。确保服务已启动并正常运行：
- en: '[PRE37]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Similarly, install the `kibana` package and start the service up:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，安装 `kibana` 包并启动服务：
- en: '[PRE38]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Even though Kibana is a dashboard, and the ELK stack is not built with Python,
    these services are so well integrated that it demonstrates what great platform
    design and architecture truly are. After starting Kibana for the first time, while
    browsing through the log output it immediately starts looking for an instance
    of Elasticsearch running on the host. This is the default behavior of its own
    Elasticsearch plug-in with no extra configuration. The behavior is transparent,
    and the messaging tells you that is was able to initialize the plug-in and reach
    Elasticsearch:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次启动 Kibana 后，在浏览日志输出时，它立即开始寻找运行在主机上的 Elasticsearch 实例。这是其自身 Elasticsearch
    插件的默认行为，无需额外配置。行为是透明的，消息告诉你它已经成功初始化插件并连接到 Elasticsearch：
- en: '[PRE39]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'After changing the configuration to an incorrect port, the logs are very clear
    that the automatic behavior is not quite working:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在将配置更改为不正确的端口后，日志非常清楚地表明自动行为并未完全起作用：
- en: '[PRE40]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Once Kibana is up and running, along with Elasticsearch (on the correct port!),
    Filebeat, and Logstash, you will be greeted with a full-featured dashboard and
    plenty of options to get started, as in [Figure 7-1](#Figure-7-1).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 Kibana 正常运行，以及 Elasticsearch（在正确的端口上！）、Filebeat 和 Logstash，你将看到一个功能齐全的仪表板，并有很多选项可供开始使用，就像
    [图 7-1](#Figure-7-1) 中展示的那样。
- en: '![pydo 0701](assets/pydo_0701.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![pydo 0701](assets/pydo_0701.png)'
- en: Figure 7-1\. Kibana’s landing dashboard page
  id: totrans-197
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-1\. Kibana 的起始仪表板页面
- en: 'Hit the local instance of Nginx to get some activity in the logs and start
    the data processing. For this example, the Apache Benchmarking Tool (`ab`) is
    used, but you can try it with your browser or directly with `curl`:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 访问本地的 Nginx 实例以在日志中生成一些活动并启动数据处理。在这个例子中，使用了 Apache 基准测试工具 (`ab`)，但你也可以用你的浏览器或直接使用
    `curl` 测试：
- en: '[PRE41]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Without doing anything specific to configure Kibana further, open the default
    URL and port where it is running: `[*http://localhost:5601*](http://localhost:5601)`.
    The default view offers lots of options to add. In the *discover* section, you
    will see all the structured information from the requests. This is an example
    JSON fragment that Logstash processed and is available in Kibana (which is sourcing
    the data from Elasticsearch):'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 要打开 Kibana 的默认 URL 和端口（运行在 `[*http://localhost:5601*](http://localhost:5601)`），无需做任何特定的配置。默认视图提供了大量添加选项。在
    *discover* 部分，你将看到所有请求的结构化信息。这是 Logstash 处理的示例 JSON 片段，可以在 Kibana 中使用（Kibana 从
    Elasticsearch 获取数据）：
- en: '[PRE42]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Critical keys like `verb`, `timestamp`, `request`, and `response` have been
    parsed and captured by Logstash. There is lots of work to do with this initial
    setup to convert it into something more useful and practical. The captured metadata
    can help render traffic (including geolocation), and Kibana can even set threshold
    alerts for data, for when specific metrics go above or below a determined value.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的键，如 `verb`、`timestamp`、`request` 和 `response`，已被 Logstash 解析并捕获。在这个初始设置中有很多工作要做，以将其转换为更有用和实用的内容。捕获的元数据可以帮助呈现流量（包括地理位置），而且
    Kibana 甚至可以为数据设置阈值警报，用于在特定指标超出或低于设定值时进行警报。
- en: In the dashboard, this structured data is available to be picked apart and used
    to create meaningful graphs and representations, as shown in [Figure 7-2](#Figure-7-2).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在仪表板中，可以拆解这些结构化数据并用于创建有意义的图表和展示，如 [图 7-2](#Figure-7-2) 所示。
- en: '![pydo 0702](assets/pydo_0702.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: 即使 Kibana 是一个仪表板，并且 ELK 堆栈并非构建于 Python 上，这些服务集成得非常好，展示了出色的平台设计和架构。
- en: Figure 7-2\. Structured data in Kibana
  id: totrans-205
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-2\. Kibana 中的结构化数据
- en: As we’ve seen, the ELK stack can get you started capturing and parsing through
    logs with minimal configuration and almost no effort. The examples are trivial,
    but should already demonstrate the immense capabilities of its components. More
    often than we would like to admit, we’ve been faced with infrastructure with a
    `cron` entry that is *tailing* logs and *grepping* for some pattern to send an
    email or submit an alert to Nagios. Using capable software components and understanding
    how much they can do for you, even in their simplest forms, is essential to better
    infrastructure, and in this case, better visibility of what that infrastructure
    is doing.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，ELK 堆栈可以让您开始捕获和解析日志，几乎不需要配置和努力。这些示例可能很简单，但已经演示了其组件的巨大能力。我们往往会面对一些基础设施，其中有一个
    `cron` 记录正在 *tail* 日志并 *grep* 某些模式以发送电子邮件或向 Nagios 提交警报。使用功能强大的软件组件，并理解它们即使在最简单的形式下也能为您做多少，对于更好的基础设施至关重要，而在这种情况下，能够更好地看到基础设施正在做什么也非常重要。
- en: Exercises
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: What is fault tolerance and how can it help infrastructure systems?
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是容错性，它如何帮助基础设施系统？
- en: What can be done for systems that produce huge amounts of logging?
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对产生大量日志的系统可以采取哪些措施？
- en: Explain why UDP might be preferred when pushing metrics to other systems. Why
    would TCP be problematic?
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释为什么在将指标推送到其他系统时可能更喜欢 UDP。为什么 TCP 可能会有问题？
- en: Describe the differences between *pull* and *push* systems. When would one be
    better than the other?
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述 *拉取* 和 *推送* 系统之间的区别。在什么情况下哪种更好？
- en: Produce a naming convention for storing metrics that accommodates production
    environments, web and database servers, and different application names.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 制定一个命名规范，用于存储适用于生产环境、Web 和数据库服务器以及不同应用程序名称的指标。
- en: Case Study Question
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究问题
- en: Create a Flask application that fully implements logging at different levels
    (info, debug, warning, and error) and sends a metric (like a Counter) to a remote
    Graphite instance via a StatsD service when an exception is produced.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个 Flask 应用程序，完全实现不同级别（info、debug、warning 和 error）的日志记录，并在产生异常时通过 StatsD 服务将指标（如计数器）发送到远程
    Graphite 实例。
