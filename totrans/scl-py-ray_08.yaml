- en: Chapter 7\. Implementing Microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Initially, Ray was created as a framework for implementing [reinforcement learning](https://oreil.ly/xRIXk)
    but gradually morphed into a full-fledged serverless platform. Similarly, initially
    introduced as a [better way to serve ML models](https://oreil.ly/tFneS), [Ray
    Serve](https://oreil.ly/970jH) has recently evolved into a full-fledged microservices
    framework. In this chapter, you will learn how to use Ray Serve for implementing
    a general-purpose microservice framework and how to use this framework for model
    serving.
  prefs: []
  type: TYPE_NORMAL
- en: Complete code of all examples used in this chapter can be found in the folder
    [*/ray_examples/serving*](https://oreil.ly/truUQ) in the book’s GitHub repo.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Microservice Architecture in Ray
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Ray microservice architecture (Ray Serve) is implemented on top of Ray by leveraging
    [Ray actors](https://oreil.ly/12pG5). Three kinds of actors are created to make
    up a Serve instance:'
  prefs: []
  type: TYPE_NORMAL
- en: Controller
  prefs: []
  type: TYPE_NORMAL
- en: A global actor unique to each Serve instance that manages the control plane.
    It is responsible for creating, updating, and destroying other actors. All of
    the Serve API calls (e.g., creating or getting a deployment) use the controller
    for their execution.
  prefs: []
  type: TYPE_NORMAL
- en: Router
  prefs: []
  type: TYPE_NORMAL
- en: There is one router per node. Each router is a [Uvicorn HTTP server](https://oreil.ly/IexLX)
    that accepts incoming requests, forwards them to replicas, and responds after
    they are completed.
  prefs: []
  type: TYPE_NORMAL
- en: Worker replica
  prefs: []
  type: TYPE_NORMAL
- en: Worker replicas execute the user-defined code in response to a request. Each
    replica processes individual requests from the routers.
  prefs: []
  type: TYPE_NORMAL
- en: User-defined code is implemented using a Ray [deployment](https://oreil.ly/6UnLe),
    an extension of a Ray actor with additional features. We will start by examining
    the deployment itself.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The central concept in Ray Serve is the deployment, defining business logic
    that will handle incoming requests and the way this logic is exposed over HTTP
    or in Python. Let’s start with a simple deployment implementing a temperature
    controller ([Example 7-1](#tc_control_basic)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-1\. [Temperature controller deployment](https://oreil.ly/8pbXQ)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The implementation is decorated by an `@serve.deployment` annotation, telling
    Ray that this is a deployment. This deployment implements a single method, `call`,
    which has a special meaning in deployment: it is invoked via HTTP. It is a class
    method taking a [`starlette` request](https://oreil.ly/F76fs), which provides
    a convenient interface for the incoming HTTP request. In the case of the temperature
    controller, the request contains two parameters: the temperature and the conversion
    type.'
  prefs: []
  type: TYPE_NORMAL
- en: Once the deployment is defined, you need to deploy it using `Converter.deploy`,
    similar to `.remote` when deploying an actor. You can then immediately access
    it via an HTTP interface ([Example 7-2](#call_http)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-2\. [Accessing converter over HTTP](https://oreil.ly/8pbXQ)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Note here that we are using URL parameters (query strings) to specify parameters.
    Also, because the services are exposed externally via HTTP, the requester can
    run anywhere, including in code that is running outside Ray.
  prefs: []
  type: TYPE_NORMAL
- en: '[Example 7-3](#output_ex1) shows the results of this invocation.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-3\. Results of HTTP invocations of deployment
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In addition to being able to invoke a deployment over HTTP, you can invoke it
    directly using Python. To do this, you need to get a *handle* to the deployment
    and then use it for invocation, as shown in [Example 7-4](#direct_invoke).
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-4\. [Invoking a deployment via a handle](https://oreil.ly/8pbXQ)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note that in this code, we are manually creating `starlette` requests by specifying
    the request type and a query string.
  prefs: []
  type: TYPE_NORMAL
- en: Once executed, this code returns the same results as in [Example 7-3](#output_ex1).
    This example uses the same `call` method for both HTTP and Python requests. Although
    this works, the [best practice](https://oreil.ly/LqkPK) is to implement additional
    methods for Python invocation to avoid the usage of `Request` objects in the Python
    invocation. In our example, we can extend our initial deployment in [Example 7-1](#tc_control_basic)
    with additional methods for Python invocations in [Example 7-5](#tc_control_ext).
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-5\. [Implementing additional methods for Python invocation](https://oreil.ly/8pbXQ)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: With these additional methods in place, Python invocations can be significantly
    simplified ([Example 7-6](#simpler_call)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-6\. [Using additional methods for handle-based invocation](https://oreil.ly/8pbXQ)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Unlike [Example 7-4](#direct_invoke), which uses the default `call` method,
    these invocation methods are explicitly specified (instead of putting the request
    type in the request itself, the request type here is implicit—​it’s a method name).
  prefs: []
  type: TYPE_NORMAL
- en: 'Ray offers synchronous and asynchronous handles. A *sync* flag, `Deployment.get​_han⁠dle(…​,
    sync=True|False)`, can be used to specify a handle type:'
  prefs: []
  type: TYPE_NORMAL
- en: The default handle is synchronous. In this case, calling `handle.remote` returns
    a Ray `ObjectRef`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To create an asynchronous handle, set `sync=False`. As its name indicates, async
    handle invocation is asynchronous, and you will have to use `await` to get a Ray
    `ObjectRef`. To use `await`, you have to run `deployment.get_handle` and `handle.remote`
    in the Python `asyncio` event loop.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will demonstrate the use of async handles later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, deployments can be updated by simply modifying the code or configuration
    options and calling `deploy` again. In addition to HTTP and direct Python invocation,
    described here, you can use the Python APIs to invoke deployment with Kafka (see
    [Chapter 6](ch06.html#ch06) for the Kafka integration approach).
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know the basics of deployment, let’s take a look at additional
    capabilities available for deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Additional Deployment Capabilities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Additional deployment capabilities are provided in three ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Adding parameters to annotations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using FastAPI HTTP deployments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using deployment composition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, you can combine all three to achieve your goals. Let’s take a close
    look at the options provided by each approach.
  prefs: []
  type: TYPE_NORMAL
- en: Adding parameters to annotations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `@serve.deployment` annotation can take several [parameters](https://oreil.ly/mcUML).
    The most widely used is the number of replicas and resource requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Improving scalability with resource replicas
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By default, `deployment.deploy` creates a single instance of a deployment. By
    specifying the number of replicas in `@serve.deployment`, you can scale out a
    deployment to many processes. When the requests are sent to such a replicated
    deployment, Ray uses round-robin scheduling to invoke individual replicas. You
    can modify [Example 7-1](#tc_control_basic) to add multiple replicas and IDs for
    individual instances ([Example 7-7](#tc_control_scaled)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-7\. [Scaled deployment](https://oreil.ly/zImkU)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now the usage of either HTTP or handle-based invocation produces the result
    in [Example 7-8](#invoke_scaled_result).
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-8\. Invoking scaled deployment
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Looking at this result, you can see that every request is processed by a different
    deployment instance (a different ID).
  prefs: []
  type: TYPE_NORMAL
- en: This is manual scaling of deployment. What about autoscaling? Similar to the
    autoscaling of Kafka listeners (discussed in [Chapter 6](ch06.html#ch06)), Ray’s
    approach to autoscaling is different from the one taken by Kubernetes natively—​see,
    for example, [Knative](https://oreil.ly/2Tj0l). Instead of creating a new instance,
    Ray’s autoscaling approach is to create more Ray nodes and redistribute deployments
    appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: 'If your deployments begin to exceed about three thousand requests per second,
    you should also scale the HTTP ingress to Ray. By default, the ingress HTTP server
    is started on only the head node, but you can also start an HTTP server on every
    node by using `serve.start(http_options=\{"location": "EveryNode"})`. If you scale
    the number of HTTP ingresses, you will also need to deploy a load balancer, available
    from your cloud provider or installed locally.'
  prefs: []
  type: TYPE_NORMAL
- en: Resource requirements for deployments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can request specific resource requirements in `@serve.deployment`. For
    example, two CPUs and one GPU would be indicated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Another useful parameter of `@serve.deployment` is `route_prefix`. As you can
    see from [Example 7-2](#call_http), the default prefix is the name of the Python
    class used in this deployment. Using `route_prefix`, for example, allows you to
    explicitly specify a prefix used by HTTP requests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: For descriptions of additional configuration parameters, refer to the [“Ray
    Core API” documentation](https://oreil.ly/3NWgQ).
  prefs: []
  type: TYPE_NORMAL
- en: Implementing request routing with FastAPI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although the initial example of a temperature converter deployment in [Example 7-1](#tc_control_basic)
    works fine, it is not convenient to use. You need to specify the transformation
    type with every request. A better approach is to have two separate endpoints (URLs)
    for the API—​one for Celsius-to-Fahrenheit transformation and one for Fahrenheit-to-Celsius
    transformation. You can achieve this by leveraging [Serve integration](https://oreil.ly/ue4Mz)
    with [FastAPI](https://oreil.ly/h8QKh). With this, you can rewrite [Example 7-1](#tc_control_basic)
    as shown in [Example 7-9](#tc_control_http).
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-9\. [Implementing multiple HTTP APIs in a deployment](https://oreil.ly/OuurE)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Note that here, we have introduced two HTTP-accessible APIs with two different
    URLs (effectively converting the second query string parameter to a set of URLs)—one
    per conversion type. (We also leverage the `route_prefix` parameter described
    previously.) This can simplify HTTP access; compare [Example 7-10](#call_multi)
    to the original in [Example 7-2](#call_http).
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-10\. [Invoking deployment with multiple HTTP endpoints](https://oreil.ly/OuurE)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Additional features provided through FastAPI implementation include variable
    routes, automatic type validation, dependency injection (e.g., for database connections),
    [security support](https://oreil.ly/atwGv), and more. Refer to the [FastAPI documentation](https://oreil.ly/3pw0k)
    on how to use these features.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment Composition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deployments can be built as a composition of other deployments. This allows
    for building powerful deployment pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at the specific example: [canary deployment](https://oreil.ly/YT27x).
    In this deployment strategy, you deploy a new version of your code or model in
    a limited fashion to see how it behaves. You can easily build this type of deployment
    by using deployment composition. We will start by defining and deploying two simple
    deployments in [Example 7-11](#versioned_deploy).'
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-11\. [Two basic deployments](https://oreil.ly/EaD6e)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'These deployments take any data and return a string: `"result": "version1"`
    for deployment 1 and `"result": “version2"` for deployment 2\. You can combine
    these two deployments by implementing a canary deployment ([Example 7-12](#canary_deploy)).'
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-12\. [Canary deployment](https://oreil.ly/EaD6e)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This deployment illustrates several points. First, it demonstrates a constructor
    with parameters, which is useful for deployment, allowing a single definition
    to be deployed with different parameters. Second, we define the `call` function
    as `async`, to process queries concurrently. The implementation of the `call`
    function is simple: generate a new random number and, depending on its value and
    a value of `canary_percent`, you will invoke either the version 1 or version 2
    deployment.'
  prefs: []
  type: TYPE_NORMAL
- en: Once the `Canary` class is deployed (by using `Canary.deploy(.3)`, you can invoke
    it using HTTP. The result of invoking canary deployment 10 times is shown in [Example 7-13](#res_canary).
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-13\. Results of the canary deployment invocation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: As you can see here, the canary model works fairly well and does exactly what
    you need. Now that you know how to build and use Ray-based microservices, let’s
    see how you can use them for model serving.
  prefs: []
  type: TYPE_NORMAL
- en: Using Ray Serve for Model Serving
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a nutshell, serving a model is no different from invoking any other microservice
    (we will talk about specific model-serving requirements later in this chapter).
    As long as you can get an ML-generated model in some shape or form compatible
    with Ray’s runtime—e.g., in [pickle format](https://oreil.ly/DWVE7), straight
    Python code, or binary format along with a Python library for its processing—you
    can use this model to process inference requests. Let’s start with a simple example
    of model serving.
  prefs: []
  type: TYPE_NORMAL
- en: Simple Model Service Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One popular model-learning application is predicting the quality of red wine,
    based on the [Kaggle Red Wine Quality dataset](https://oreil.ly/yvPRp). Numerous
    blog posts use this dataset to build ML implementations of wine quality—​for example,
    see articles by [Mayur Badole](https://oreil.ly/9yLZ9) and [Dexter Nguyen](https://oreil.ly/JWwgO).
    For our example, we have built several classification models for the Red Wine
    Quality dataset, based on Terence Shin’s [“Predicting Wine Quality with Several
    Classification Techniques”](https://oreil.ly/M6lc4); the actual code can be found
    in the book’s [GitHub repo](https://oreil.ly/ChZtD). The code uses several techniques
    for building a classification model of the red wine quality, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Decision trees](https://oreil.ly/Qnx8W)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Random forest](https://oreil.ly/yXqZz)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[AdaBoost](https://oreil.ly/gerOD)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Gradient boost](https://oreil.ly/bTNNZ)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[XGBoost](https://oreil.ly/csAzq)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All implementations leverage the scikit-learn Python library, which allows you
    to generate a model and export it using pickle. When validating the models, we
    saw the best results from the random forest, gradient boost, and XGBoost classifications,
    so we saved only these models locally—​generated models are available in the book’s
    [GitHub repo](https://oreil.ly/VY9NE). With the models in place, you can use a
    simple deployment that allows serving the red wine quality model using random
    forest classification ([Example 7-14](#rf_serve)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-14\. [Implementing model serving using random forest classification](https://oreil.ly/52qR4)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This deployment has three methods:'
  prefs: []
  type: TYPE_NORMAL
- en: The constructor
  prefs: []
  type: TYPE_NORMAL
- en: Loads a model and stores it locally. We are using model location as a parameter
    so we can redeploy this deployment when a model changes.
  prefs: []
  type: TYPE_NORMAL
- en: '`call`'
  prefs: []
  type: TYPE_NORMAL
- en: Invoked by HTTP requests, this method retrieves the features (as a dictionary)
    and invokes the `serve` method for the actual processing. By defining it as async,
    it can process multiple requests simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: '`serve`'
  prefs: []
  type: TYPE_NORMAL
- en: Can be used to invoke deployment via a handle. It converts the incoming dictionary
    into a vector and calls the underlying model for inference.
  prefs: []
  type: TYPE_NORMAL
- en: Once the implementation is deployed, it can be used for model serving. If invoked
    via HTTP, it takes a JSON string as a payload; for direct invocation, the request
    is in the form of a dictionary. Implementations for [XGBoost](https://oreil.ly/Kc2oH)
    and [gradient boost](https://oreil.ly/UbrD7) look pretty much the same, with the
    exception that a generated model in these cases takes a two-dimensional array
    instead of a vector, so you need to do this transformation before invoking the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, you can take a look at Ray’s documentation for [serving other
    types of models](https://oreil.ly/qouwp), including TensorFlow and PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know how to build a simple model-serving implementation, the question
    is whether Ray-based microservices are a good platform for model serving.
  prefs: []
  type: TYPE_NORMAL
- en: Considerations for Model-Serving Implementations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When it comes to model serving, a few specific requirements are important.
    A good definition of requirements specific to model serving can be found in [*Kubeflow
    for Machine Learning*](https://oreil.ly/sikPG) by Trevor Grant et al. (O’Reilly).
    These requirements are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The implementation has to be flexible. It should allow for your training to
    be implementation agnostic (i.e., TensorFlow versus PyTorch, versus scikit-learn).
    For an inference service invocation, it should not matter if the underlying model
    was trained using PyTorch, scikit-learn, or TensorFlow: the service interface
    should be shared so that the user’s API remains consistent.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is sometimes advantageous to be able to batch requests in a variety of settings
    in order to realize better throughput. The implementation should make it simple
    to support batching of model-serving requests.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The implementation should provide the ability to leverage hardware optimizers
    that match the needs of the algorithm. Sometimes in the evaluation phase, you
    would benefit from hardware optimizers like GPUs to infer the models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The implementation should be able to seamlessly include additional components
    of an [inference graph](https://oreil.ly/9Gr2X). An inference graph could comprise
    feature transformers, predictors, explainers, and drift detectors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implementation should allow scaling of serving instances, both explicitly and
    using autoscalers, regardless of the underlying hardware.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It should be possible to expose model-serving functionality via different protocols
    including HTTP and Kafka.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ML models traditionally do not extrapolate well outside the training data distribution.
    As a result, if data drift occurs, the model performance can deteriorate, and
    it should be retrained and redeployed. Implementation should support an easy redeployment
    of models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Flexible deployment strategy implementations (including canary deployment, blue-green
    deployments, and A/B testing) are required to ensure that new versions of models
    will not behave worse than the existing ones.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s see how these requirements are satisfied by Ray’s microservice framework:'
  prefs: []
  type: TYPE_NORMAL
- en: Ray’s deployment cleanly separates deployment APIs from model APIs. Thus, Ray
    “standardizes” deployment APIs and provides support for converting incoming data
    to the format required for the model. See [Example 7-14](#rf_serve).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ray’s deployment makes it easy to implement request batching. Refer to the Ray
    [“Batching Tutorial” guide](https://oreil.ly/K3up4) for details on how to implement
    and deploy a Ray Serve deployment that accepts batches, configure the batch size,
    and query the model in Python.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As described earlier in this chapter, deployments support configurations that
    allow specifying hardware resources (CPU/GPU) required for its execution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deployment composition, described earlier in this chapter, allows for easy creation
    of the model-serving graphs, mixing and matching plain Python code and existing
    deployments. We will present an additional example of deployment compositions
    later in this chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As described earlier in this chapter, deployments support setting the number
    of replicas, thus easily scaling deployments. Coupled with Ray’s autoscaling and
    the ability to define the number of HTTP servers, the microservice framework allows
    for very efficient scaling of model serving.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As we’ve described, deployments can be exposed via HTTP or straight Python.
    The latter option allows for integration with any required transport.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As described earlier in this chapter, a simple redeployment of deployment allows
    you to update models without restarting the Ray cluster and interrupting applications
    that are leveraging model serving.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As shown in [Example 7-12](#canary_deploy), using deployment composition allows
    for easy implementation of any deployment strategy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As we have shown here, the Ray microservice framework is a solid foundation
    for model serving that satisfies all of the main requirements for model serving.
  prefs: []
  type: TYPE_NORMAL
- en: The last thing that you are going to learn in this chapter is the implementation
    of one of the advanced model-serving techniques—[speculative model serving](https://oreil.ly/KH8EZ)—using
    the Ray microservices framework.
  prefs: []
  type: TYPE_NORMAL
- en: Speculative Model Serving Using the Ray Microservice Framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Speculative model serving is an application of [speculative execution](https://oreil.ly/RRvzK).
    In this optimization technique, a computer system performs a task that may not
    be needed. The work is done before knowing whether it is actually required. This
    allows getting results up front, so if they are actually needed, they will be
    available with no delay. Speculative execution is important in model serving because
    it provides the following features for machine-serving applications:'
  prefs: []
  type: TYPE_NORMAL
- en: Guaranteed execution time
  prefs: []
  type: TYPE_NORMAL
- en: Assuming that you have several models, with the fastest providing a fixed execution
    time, it is possible to provide a model-serving implementation with a fixed upper
    limit on execution time, as long as that time is larger than the execution time
    of the simplest model.
  prefs: []
  type: TYPE_NORMAL
- en: Consensus-based model serving
  prefs: []
  type: TYPE_NORMAL
- en: Assuming that you have several models, you can implement model serving in such
    a way that prediction is the one returned by the majority of the models.
  prefs: []
  type: TYPE_NORMAL
- en: Quality-based model serving
  prefs: []
  type: TYPE_NORMAL
- en: Assuming that you have a metric allowing you to evaluate the quality of model-serving
    results, this approach allows you to pick the result with the best quality.
  prefs: []
  type: TYPE_NORMAL
- en: Here you will learn how to implement consensus-based model serving using Ray’s
    microservice framework.
  prefs: []
  type: TYPE_NORMAL
- en: 'You learned earlier in this chapter how to implement quality scoring of red
    wine using three models: random forest, gradient boost, and XGBoost. Now let’s
    try to produce an implementation that returns a result on which at least two models
    agree. The basic implementation is shown in [Example 7-15](#spec_serve).'
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-15\. [Consensus-based model serving](https://oreil.ly/2RyYu)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The constructor of this deployment creates handles for all of your deployments
    implementing individual models. Note that here we are creating async handles that
    allow parallel execution of each deployment.
  prefs: []
  type: TYPE_NORMAL
- en: The `call` method gets the payload and starts executing all three models in
    parallel and then waits for all to complete—see [“Waiting in asyncio”](https://oreil.ly/pKovE)
    by Hynek Schlawack for information on using `asyncio` for the execution of many
    coroutines and running them concurrently. Once you have all the results, you implement
    the consensus calculations and return the result (along with methods that voted
    for it).^([1](ch07.html#idm45354770971824))
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned Ray’s implementation of the microservice framework
    and how this framework can be used by model serving. We started by describing
    a basic microservices deployment and extensions allowing for better control, scale,
    and extending of the deployment’s execution. We then showed an example of how
    this framework can be used to implement model serving, analyzed typical model-serving
    requirements, and showed how they can be satisfied by Ray. Finally, you learned
    how to implement an advanced model-serving example—consensus-based model serving—allowing
    you to improve the quality of individual model-serving methods. The article [“Building
    Highly Available and Scalable Online Applications on Ray at Ant Group”](https://oreil.ly/y9BuV)
    by Tengwei Cai et al. shows how to bring together the basic building blocks described
    here into more complex implementations.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn about workflow implementation in Ray and
    how to use workflows to automate your application execution.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch07.html#idm45354770971824-marker)) You can also implement different
    policies for waiting for the model’s execution. You could, for example, use at
    least one model’s result via `asyncio.wait(tasks). return_when=asyncio.FIRST_COMPLETED)`
    or just wait for a given time interval by using `asyncio.wait(tasks, interval)`.
  prefs: []
  type: TYPE_NORMAL
